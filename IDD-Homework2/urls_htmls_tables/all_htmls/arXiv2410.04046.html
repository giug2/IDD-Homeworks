<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Lane Detection System for Driver Assistance in Vehicles</title>
<!--Generated on Sat Oct  5 05:50:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Lane Detection,  Computer Vision,  Autonomous Driving,  Road Safety
" lang="en" name="keywords"/>
<base href="/html/2410.04046v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S1" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">INTRODUCTION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S2" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">THEORETICA FOUNDATION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">METHODOLOGY</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS1" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Camera Calibration and Distortion Correction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS2" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Perspective Transformation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS3" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Image Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS4" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Lane Detection</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS5" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-E</span> </span><span class="ltx_text ltx_font_italic">Conversion to the Real-World Space</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S3.SS6" title="In III METHODOLOGY ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-F</span> </span><span class="ltx_text ltx_font_italic">Visualization and Feedback</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S4" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">RESULTS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S4.SS1" title="In IV RESULTS ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Lane Detection Accuracy</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S4.SS2" title="In IV RESULTS ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Performance in Adverse Conditions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S4.SS3" title="In IV RESULTS ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Quantitative Evaluation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S4.SS4" title="In IV RESULTS ‣ Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Observed Limitations</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S5" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">DISCUSSION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.04046v1#S6" title="In Lane Detection System for Driver Assistance in Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusão</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Lane Detection System for Driver Assistance in Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">1<sup class="ltx_sup" id="id1.1.id1">st</sup> Kauan Divino Pouso Mariano
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.2.id1">Institute of Informatics</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id3.3.id2">Federal University of Goiás
<br class="ltx_break"/></span>Goiânia, Goiás 
<br class="ltx_break"/>kauan@discente.ufg.br
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">2<sup class="ltx_sup" id="id4.1.id1">nd</sup> Fernanda de Castro Fernandes
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id5.2.id1">School of Electrical, Mechanical, and Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id6.3.id2">Federal University of Goiás
<br class="ltx_break"/></span>Goiânia, Goiás 
<br class="ltx_break"/>castro.fernanda@discente.ufg.br
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">3<sup class="ltx_sup" id="id7.1.id1">rd</sup> Luan Gabriel Silva Oliveira
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.2.id1">Institute of Informatics</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id9.3.id2">Federal University of Goiás
<br class="ltx_break"/></span>Goiânia, Goiás 
<br class="ltx_break"/>luangabriel@discente.ufg.br
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">4<sup class="ltx_sup" id="id10.1.id1">th</sup> Lyan Eduardo Sakuno Rodrigues
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id11.2.id1">Institute of Informatics</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id12.3.id2">Federal University of Goiás
<br class="ltx_break"/></span>Goiânia, Goiás 
<br class="ltx_break"/>lyan@discente.ufg.br
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">5<sup class="ltx_sup" id="id13.1.id1">th</sup> Matheus Andrade Brandão
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id14.2.id1">Institute of Informatics</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id15.3.id2">Federal University of Goiás
<br class="ltx_break"/></span>Goiânia, Goiás 
<br class="ltx_break"/>matheus_brandao@discente.ufg.br
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1">This work presents the development of a lane detection system aimed at assisting the driving of conventional and autonomous vehicles. The system was implemented using traditional computer vision techniques, focusing on robustness and efficiency to operate in real-time, even under adverse conditions such as worn-out lanes and weather variations. The methodology employs an image processing pipeline that includes camera calibration, distortion correction, perspective transformation, and binary image generation. Lane detection is performed using sliding window techniques and segmentation based on gradients and color channels, enabling the precise identification of lanes in various road scenarios. The results indicate that the system can effectively detect and track lanes, performing well under different lighting conditions and road surfaces. However, challenges were identified in extreme situations, such as intense shadows and sharp curves. It is concluded that, despite its limitations, the traditional computer vision approach shows significant potential for application in driver assistance systems and autonomous navigation, with room for future improvements.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Lane Detection, Computer Vision, Autonomous Driving, Road Safety

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">INTRODUCTION</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The increasing number of vehicles on the roads and the growing demand for safer and more efficient transportation systems have driven the development of driver assistance and autonomous driving technologies. Among these technologies, accurate lane detection plays a crucial role in ensuring that vehicles remain within designated lanes, directly contributing to road safety. The ability to precisely identify and track road lanes is a fundamental requirement for Advanced Driver Assistance Systems (ADAS) and autonomous vehicles.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, lane detection presents significant challenges in dynamic environments and under various lighting, weather, and lane wear conditions. These factors complicate the task and demand robust, real-time solutions. Computer vision-based approaches have shown promise in addressing these difficulties by providing algorithms capable of interpreting the environment in a manner similar to human vision.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this context, the present work aims to develop a lane detection system using traditional computer vision techniques. Unlike recent approaches based on machine learning, the proposed method seeks to explore techniques that are less dependent on large volumes of data and have lower computational costs, making them suitable for devices with limited capabilities, such as cameras embedded in passenger vehicles. The developed system aims to overcome obstacles such as adverse weather conditions, lane wear, and different types of roads, offering an efficient solution for real-time detection.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The methodology adopted for lane detection includes steps such as camera calibration, distortion correction, perspective transformation to obtain a bird’s-eye view, lane segmentation through thresholding techniques, and the application of sliding window methods for tracking detected lanes. The results indicate that the system successfully performs lane detection under a variety of road conditions, with satisfactory accuracy in scenarios involving varied lighting and worn surfaces. Thus, this article contributes to the field of computer vision by proposing a practical and efficient approach to lane detection, with potential applications in autonomous driving and driver assistance systems.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">THEORETICA FOUNDATION</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Computer vision has emerged as one of the most promising fields for the development of driver assistance systems and autonomous vehicles. The central objective of this field is to automate the human capability of interpreting and making decisions based on visual information. For assisted driving systems, lane detection is an essential task, as it enables the precise guidance of the vehicle, improving road safety and traffic efficiency.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">However, lane detection presents specific challenges, particularly in dynamic environments, such as roads with worn lanes, variable lighting, and adverse weather conditions. Traditional computer vision approaches, such as perspective transformations and image segmentation, have been widely employed to overcome these challenges. These techniques involve image pre-processing steps, such as camera calibration for distortion correction, segmentation through color and gradient thresholding, and the use of sliding window-based methods for lane line detection.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">One study that has influenced the development of lane detection systems is the work by Hou (2019), which proposed an agnostic lane detection method, focusing on a generalizable solution for different types of roads and driving conditions. Hou’s methodology is based on pixel segmentation in road images, emphasizing the robustness of detection in various environments. Although advanced machine learning techniques were utilized, Hou highlights the importance of solutions that can be applied to embedded systems with limited resources.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Additionally, the article ”Ultra Fast Structure-aware Deep Lane Detection” by Zequn et al. (2020) introduced an innovative technique that integrates deep learning with a structural understanding of lane markings. Although this work did not directly adopt deep learning techniques, the concept of structural understanding was adapted for traditional computer vision approaches. This concept aids in the precise modeling of lanes, especially in scenarios where lighting is variable and lanes are partially worn.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Traditional computer vision techniques have been extensively studied in the literature. According to Abualsaud et al. (2021), methods based on perspective transformations and color segmentation can be efficiently implemented in real-time, even in systems with hardware limitations, making these approaches attractive for passenger vehicles. The use of perspective transformations, such as the so-called ”bird’s-eye view,” simplifies the lane detection problem by projecting road lanes as approximately parallel lines in the image, facilitating their detection and tracking.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">For camera calibration and distortion correction, classical techniques described by Zhang (2000) are still widely used in computer vision projects. Precise camera calibration is essential to ensure that the projected image is an accurate representation of the three-dimensional environment. In lane detection systems, distortion correction allows lanes to be detected more accurately by eliminating the image deformation caused by the camera’s lenses.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">The use of thresholding and segmentation algorithms in different color spaces has also proven effective in lane detection under varying lighting conditions. Luo et al. (2023) demonstrated that the combination of color spaces such as RGB, HLS, and LAB improves the robustness of segmentation, enabling the precise detection of white and yellow lanes. These techniques of color channel fusion are employed in systems such as the one proposed in this article, contributing to lane detection even under adverse conditions, such as poorly lit roads or highly reflective surfaces.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">Therefore, the theoretical foundation of this work aligns with the most established approaches in the literature, but it adapts these traditional computer vision techniques to a specific context of lane detection. The decision not to use deep neural networks was driven by the pursuit of lighter and lower computational cost solutions, as pointed out by previous studies. However, recent advances in deep learning, as seen in the work of Zequn et al., serve as inspiration for potential future improvements.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">METHODOLOGY</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.6.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.7.2">Camera Calibration and Distortion Correction</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The first step of the pipeline is camera calibration, an essential procedure to eliminate optical distortions that can compromise detection accuracy. Distortions are common in cameras embedded in vehicles due to the projection of three-dimensional objects onto a two-dimensional image. To correct this issue, a set of chessboard images was used, a standard pattern widely employed for camera calibration, as described by Zhang (2000).</p>
</div>
<div class="ltx_logical-block" id="S3.SS1.2">
<div class="ltx_para" id="S3.SS1.2.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S3.SS1.1.g1" src="extracted/5903373/calibration1.jpg" width="314"/>
<p class="ltx_p ltx_align_center" id="S3.SS1.2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.2.p1.1.1">Figure 1</span> Chessboard used for camera calibration.</p>
</div>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The calibration process was conducted using functions from the OpenCV library, such as cv2.findChessboardCorners to identify the corners of the chessboard, cv2.calibrateCamera to calculate the calibration matrix, and cv2.undistort to correct distortions. The parameters obtained from this process, such as the intrinsic matrix and distortion coefficients, were stored and applied to all images processed by the system, ensuring that any subsequent frame was adjusted to remove distortions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Camera calibration is a fundamental step to ensure accurate projection of the lane markings in the processed image. Any remaining distortion can result in significant errors in lane detection, particularly in situations where the lanes are distant or under adverse lighting conditions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Perspective Transformation</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">After distortion correction, the image is subjected to a perspective transformation to simulate a bird’s-eye view. This technique is widely used in lane detection, as the orthogonal projection of the road facilitates the identification of lanes as approximately parallel lines, simplifying the detection and tracking process.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">The perspective transformation was performed using the cv2.getPerspectiveTransform function, which requires defining four source points in the original image and their corresponding coordinates in the transformed image. These points were selected empirically, considering that lanes in the original images may appear distorted depending on the camera position and road geometry. The perspective transformation aims to align the lanes so they can be detected more precisely, especially on straight road sections.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">However, this technique presents limitations on roads with sharp curves, where the orthogonal projection may not accurately represent the actual lane trajectory. This issue is addressed in the subsequent step of lane detection and tracking, with the introduction of algorithms that handle variations in road geometry.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">Image Segmentation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The third step of the pipeline is image segmentation, a crucial process to isolate the pixels corresponding to the lane markings. For this, a combination of thresholding techniques based on gradients and color channels was employed. The segmentation aims to highlight the lane markings, differentiating them from the rest of the road and the surrounding scene.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">The approach used explores different color spaces, such as RGB, HLS, HSV, and LAB, to maximize the system’s robustness under various lighting and surface conditions. Each of these color spaces provides distinct characteristics that help isolate the white and yellow lane markings commonly found on roads.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">In the HLS space, for example, yellow and white lanes are detected using adaptive thresholding. One of the challenges encountered was the adaptation of the detection threshold according to road conditions, especially in cases of wear or highly reflective surfaces. The cv2.inRange function was applied to create binary masks that highlight the lanes of interest, combining them to generate the final binary image.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">Lane Detection</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">After generating the binary image, the next step is lane detection. For this, the sliding windows method was used, a technique widely employed in line segmentation in images. The sliding windows algorithm works by identifying regions in the image with a high density of pixels corresponding to the lanes. Once the initial points are identified, the algorithm expands these windows along the image, tracking the pixels associated with the lanes and fitting them to a second-degree polynomial that describes the trajectory of the lanes along the road.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">This method offers the advantage of being efficient in real-time and operating robustly on straight roads. However, on curved roads or where lanes are partially worn, detection may be less precise. To mitigate this issue, an adaptive search technique was introduced, allowing the algorithm to automatically adjust the detection windows based on the geometric characteristics of the lanes detected in previous frames.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">Additionally, lane detection was refined by combining different color and gradient segmentation methods, ensuring system robustness under various lighting and road texture conditions. The pixels corresponding to the lanes were then organized into a logical sequence, enabling continuous lane tracking across subsequent frames during video processing.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS5.4.1.1">III-E</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS5.5.2">Conversion to the Real-World Space</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Once the lanes are detected in the image, it is necessary to convert this information from the pixel domain to the real-world space in order to calculate relevant metrics, such as the curvature of the road and the vehicle’s relative position with respect to the center of the lane. The coordinate conversion is performed based on empirical factors derived from traffic regulations and the scale of the processed images.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">To calculate the lane curvature, it was necessary to adopt a second-degree polynomial model, fitted to the points of the detected lane in the image. From this model, it was possible to derive the road curvature at different points, which provides essential information for controlling autonomous or assisted vehicles. Additionally, the vehicle’s lateral position relative to the lane center was calculated based on the detected lane positions in the processed frames.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">The coefficients used in the conversion were adjusted according to U.S. traffic regulations, the country of origin of the sample videos used in the project. However, it should be noted that these parameters can be adapted for different regions and regulations as needed, making the system flexible and applicable in various contexts.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS6.4.1.1">III-F</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS6.5.2">Visualization and Feedback</span>
</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">The final step of the pipeline involves projecting the detected lanes back onto the original image, allowing a clear and direct visualization of the road lanes. This was done using the cv2.warpPerspective function, which reverses the perspective transformation previously applied. As a result, the detected lanes are overlaid on the original image, providing an accurate visualization of the position and curvature of the lanes.</p>
</div>
<div class="ltx_para" id="S3.SS6.p2">
<p class="ltx_p" id="S3.SS6.p2.1">Additionally, extra metrics such as road curvature and the vehicle’s position relative to the lane center were displayed on the image, offering continuous visual feedback to the system. This real-time visualization can be used by both driver assistance systems and autonomous vehicles, providing crucial data for safe and efficient navigation.</p>
</div>
<div class="ltx_para" id="S3.SS6.p3">
<p class="ltx_p" id="S3.SS6.p3.1">The entire pipeline was implemented in an interactive Jupyter Notebook environment, allowing for iterative exploration of the process steps. The code was structured in a modular way, facilitating the optimization of each part of the system and enabling the integration of future improvements.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">RESULTS</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.6.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.7.2">Lane Detection Accuracy</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The algorithm demonstrated robust performance in identifying lane markings under a variety of road conditions. When tested on videos from the CULane and TuSimple datasets, the system was able to detect and track lanes accurately, both on straight sections and on slightly curved roads. The perspective transformation technique, combined with color and gradient segmentation, proved effective in separating the lanes from the rest of the image, facilitating continuous tracking throughout the processed frames.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Specifically, the white and yellow lane markings, predominant on the roads in the evaluated videos, were consistently identified, even under variable lighting conditions such as direct sunlight and partial shadows. In nighttime lighting scenarios, color segmentation was also effective, provided the lanes were well-defined and exhibited sufficient contrast relative to the road surface.</p>
</div>
<div class="ltx_logical-block" id="S4.SS1.2">
<div class="ltx_para" id="S4.SS1.2.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="144" id="S4.SS1.1.g1" src="extracted/5903373/imagem_exemplo1.jpeg" width="314"/>
<p class="ltx_p ltx_align_center" id="S4.SS1.2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.2.p1.1.1">Figure 2</span> Lane detection output showing the vehicle’s position relative to the center of the lane.</p>
</div>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Performance in Adverse Conditions</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Although the system exhibited satisfactory accuracy in normal driving conditions, some challenges were identified in adverse scenarios. On roads with heavily worn lane markings or in extreme weather conditions, such as fog or intense sunlight reflections, the algorithm showed a reduction in detection accuracy. This occurred mainly due to the low distinction between the lane markings and the rest of the road, which made it difficult to precisely segment the pixels corresponding to the lanes.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Additionally, on sharp curved sections, the perspective transformation caused distortions in the lane projections, compromising tracking accuracy. This phenomenon is due to the orthogonal projection used in the perspective transformation, which tends to inadequately represent the curved road geometries.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Quantitative Evaluation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To quantitatively assess the system’s performance, common metrics from lane detection literature were used, such as accuracy rate and average detection error rate. In the experiments conducted, the system achieved an accuracy rate of 92% for lane detection in daytime lighting scenarios, with a drop to 85% in nighttime or low visibility conditions. The average error rate, in turn, remained below 10%, indicating stable performance in most tests.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Processing times were also measured, confirming that the pipeline can run in real-time, with an average processing rate of 24 frames per second in HD resolution videos. This performance reinforces the system’s viability for use in embedded vehicles with limited processing capacity.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Observed Limitations</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Despite the encouraging results, some limitations were observed. On roads with heavily deteriorated lane markings or under extreme lighting conditions, such as direct sunlight reflections on the camera, the system’s accuracy was reduced. The segmentation method, which relies on color and gradient thresholds, proved sensitive to these variations, resulting in occasional failures in lane detection.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">Moreover, on sharply curved road sections, the orthogonal projection used in the perspective transformation proved inadequate, as it assumes a flat and linear surface. This limitation suggests that more sophisticated approaches, such as the use of 3D models or machine learning-based techniques, could improve accuracy in more challenging scenarios.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Despite the mentioned limitations, the results obtained demonstrate the potential of the lane detection system for application in driver assistance systems and autonomous vehicles. The implemented pipeline offers a lightweight, low computational cost solution suitable for urban and rural roads with moderate traffic and weather conditions. With improvements in the segmentation and perspective transformation steps, the system can be expanded to support more complex and challenging scenarios.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">DISCUSSION</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The results obtained throughout this work indicate that the approach based on traditional computer vision techniques for lane detection performs satisfactorily in most of the tested scenarios. The use of camera calibration, perspective transformation, and color segmentation proved effective in identifying lanes, especially under daytime lighting conditions and on straight road sections. However, some limitations observed in more adverse situations highlight the need for future improvements.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Compared to machine learning-based approaches, which require large datasets and high computational power, the proposed method offers a lighter solution with faster implementation, making it viable for embedded systems with limited resources. Although traditional computer vision techniques may not offer the same flexibility and adaptability as deep neural network models, their computational efficiency and real-time robustness reinforce their relevance for passenger vehicles and Advanced Driver Assistance Systems (ADAS).</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The limitations found in adverse conditions, such as worn lanes or intense lighting variations, reveal the sensitivity of the segmentation method used. The reliance on fixed thresholds, both in color space and gradients, proved vulnerable to extreme environmental variations. To address these limitations, future improvements could consider the implementation of adaptive algorithms that dynamically adjust the segmentation thresholds based on local lighting and contrast conditions. Such enhancements would allow greater flexibility for the system in challenging scenarios, such as poorly lit roads or roads with faded lane markings.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Additionally, the results indicate that while perspective transformation is effective on straight sections, it faces difficulties on sharply curved roads. The orthogonal projection used in the transformation process tends to distort lanes when they are in curved regions, compromising detection accuracy. A possible solution to this problem would be the incorporation of more complex geometric models, such as the adaptation of 3D road models or the use of depth maps, which could improve projection in scenarios with greater geometric variation.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Compared to more modern approaches, such as those using deep learning, the proposed system also lacks predictive capability in scenarios where lanes are completely invisible, such as in heavy fog or rain. Neural network-based approaches have proven more effective in handling such scenarios, as they can learn more complex patterns from large datasets. However, the focus of this work was to develop a simpler and more accessible solution that does not rely on extensive datasets or high-capacity computational infrastructure, yet remains efficient in common traffic situations.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">The quantitative performance, with an accuracy exceeding 90% in most scenarios, confirms that the implemented pipeline can be used in driver assistance applications, providing a solid foundation for the development of advanced navigation systems. The modularity of the pipeline also allows for future extensions and integrations with more advanced image processing techniques, such as the fusion with additional sensors (e.g., LiDAR or radar), which could complement lane detection, increasing accuracy and robustness in complex environments.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">It is concluded that the traditional computer vision methodology applied in this work meets the objective of offering a practical and efficient solution for lane detection. However, there is significant room for improvement, both in the robustness of the algorithms used and in adapting the system to more challenging scenarios. Future work could explore the integration of deep learning techniques, dynamic adjustment algorithms, and the fusion of data from different sensors to enhance the accuracy and adaptability of the proposed system.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusão</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This work developed and implemented a lane detection system using traditional computer vision techniques. The main objective was to create an efficient, low computational cost solution capable of operating in real-time and under a variety of road conditions. The adopted methodology, based on camera calibration, perspective transformation, and color and gradient segmentation, proved effective in accurately detecting lanes under normal driving conditions, achieving an accuracy rate of over 90% in daytime scenarios.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Although the system demonstrated robust performance in most tests, limitations were observed under adverse conditions, such as worn lanes, severe weather variations, and sharp curves. These limitations highlight the need for future improvements, particularly in adapting segmentation thresholds for scenarios with high environmental variability and improving perspective projection for roads with more complex geometries.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">The modularity of the implemented pipeline allows for iterative enhancements, making it possible to integrate more advanced computer vision or deep learning techniques to handle more challenging scenarios. Additionally, fusion with other sensors, such as LiDAR or radar, can increase the system’s robustness, making it more adaptable to different traffic and environmental conditions.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In summary, the proposed system shows significant potential for application in driver assistance systems and autonomous vehicles, especially in contexts that require efficient, low-cost solutions. Despite the observed limitations, the traditional computer vision approach provides a solid foundation for future extensions and refinements, with possibilities for application in various areas of assisted driving and vehicle automation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Hou, Y. (2019). Agnostic Lane Detection. arXiv preprint arXiv:1905.03704.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Q. Zequn, W. Huanyu, L. Xi. “Ultra Fast Structure-aware Deep Lane Detection,” Computer Vision – ECCV 2020, vol 12369, pp 276–291, Novembro 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Q. Zequn, W. Huanyu, L. Xi. ”Ultra Fast Deep Lane Detection With Hybrid Anchor Driven Ordinal Classification”, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-14, Junho 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Luo, Y., Zheng, C., Yan, X., Kun, T., Zheng, C., Cui, S., Li, Z. (2023). LATR: 3D Lane Detection from Monocular Images with Transformer. arXiv preprint arXiv:2308.04583.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Abualsaud, H., Liu, S., Lu, D., Situ, K., Rangesh, A., Trivedi, M. M. (2021). LaneAF: Robust Multi-Lane Detection with Affinity Fields. arXiv preprint arXiv:2103.12040.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Zhang, Z. (2000). A flexible new technique for camera calibration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(11), 1330–1334.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">J. Neven, B. De Brabandere, S. Georgoulis, M. Proesmans, L. Van Gool. ”Towards End-to-End Lane Detection: An Instance Segmentation Approach.” 2018 IEEE Intelligent Vehicles Symposium (IV), pp. 286-291, 2018. DOI: 10.1109/IVS.2018.8500484.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">G. Ghiasi, T.-Y. Lin, Q. V. Le. ”NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7036-7045, 2019. DOI: 10.1109/CVPR.2019.00721.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">P. Zhou, Q. Zhang, F. Gao, X. Pan, X. Yang. ”Spatial CNN for Traffic Scene Understanding.” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 07, pp. 12801-12808, 2020. DOI: 10.1609/aaai.v34i07.6886..

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Oct  5 05:50:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
