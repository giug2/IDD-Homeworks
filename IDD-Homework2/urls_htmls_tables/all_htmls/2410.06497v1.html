<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System</title>
<!--Generated on Wed Oct  9 02:47:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="cache,  user representation,  personalization,  online advertising" lang="en" name="keywords"/>
<base href="/html/2410.06497v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S1" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Motivation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.SS1" title="In 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.SS2" title="In 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Model Serving Triangle</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.SS3" title="In 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Opportunities</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Design and Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS1" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Architecture of ERCache.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS2" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>ERCache functionalities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS3" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Customized cache configurations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS4" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Update combination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS5" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Asynchronous write</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS6" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Regional consistency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.SS7" title="In 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Reliability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS1" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS2" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Computational resource evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS3" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Service SLAs evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS4" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Impact of cache TTL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS5" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Serving cost of ERCache</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS6" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Reliability of  ERCache</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.SS7" title="In 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Key takeaways</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S5" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S6" title="In ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">ERCache: An Efficient and Reliable Caching Framework for Large-Scale
User Representations in Meta’s Ads System</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fang Zhou, Yaning Huang, Dong Liang, Dai Li, Zhongke Zhang, Kai Wang,
Xiao Xin, Abdallah Aboelela,
Zheliang Jiang, Yang Wang, Jeff Song, Wei Zhang, Chen Liang,
Huayu Li, ChongLin Sun, Hang Yang, Lei Qu, Zhan Shu, Mindi Yuan, Emanuele Maccherani,
Taha Hayat, John Guo, Varna Puvvada, and Uladzimir Pashkevich
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Meta Platforms, Inc.</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Menlo Park</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">CA</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id5.id1">The increasing complexity of deep learning models
used for calculating user representations presents
significant challenges,
particularly with limited computational resources
and strict service-level agreements (SLAs).
Previous research efforts have focused on
optimizing model inference but
have overlooked a critical question:
is it necessary to perform user model
inference for every ad request
in large-scale social networks?</p>
<p class="ltx_p" id="id6.id2">To address this question and these challenges,
we first analyze user access patterns at Meta and
find that most user model inferences occur
within a short timeframe. T
his observation reveals a triangular relationship
among model complexity, embedding freshness,
and service SLAs.</p>
<p class="ltx_p" id="id7.id3">Building on this insight,
we designed, implemented, and evaluated  ERCache,
an efficient and robust caching framework
for large-scale user representations
in ads recommendation systems
on social networks.
 ERCache categorizes cache
into direct and failover types and
applies customized settings and eviction policies
for each model,
effectively balancing model complexity,
embedding freshness, and service SLAs,
even considering the staleness introduced by caching.</p>
<p class="ltx_p" id="id8.id4">ERCache has been deployed at Meta for over six months,
supporting more than 30 ranking models
while efficiently conserving computational resources
and complying with service SLA requirements.</p>
</div>
<div class="ltx_keywords">cache, user representation, personalization, online advertising
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Online advertising</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep learning techniques have been shown to significantly improve user representation in recommendation systems
<cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib2" title="">2016</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib5" title="">2017</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib16" title="">2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib17" title="">2021</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib6" title="">2017a</a>)</cite>.
By leveraging neural networks and other deep learning architectures,
these models can learn complex patterns and relationships between users and items,
resulting in more accurate recommendations.
Therefore, there has been a growing trend towards developing
increasingly complex deep learning models
to enhance the performance of user representation
 <cite class="ltx_cite ltx_citemacro_citep">(Cheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib2" title="">2016</a>; Rendle, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib13" title="">2010</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib14" title="">2012</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib7" title="">2017b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib16" title="">2017</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib17" title="">2021</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib5" title="">2017</a>; Grbovic and Cheng, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib4" title="">2018</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib24" title="">2020</a>; Pancha et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib11" title="">2022</a>; Pi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib12" title="">2019</a>; Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib21" title="">2018</a>; El-Kishky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib3" title="">2022</a>; Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib18" title="">2023</a>; Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib22" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib23" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib25" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Since user representation is inferred through online serving,
the increasing complexity of models in ads recommendation systems
has significant challenges: constrained computational resources
and service SLA limitations.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These challenges necessitate the development of more efficient computational strategies
and robust system architectures to ensure that
the deployment of complex models does not
compromise user experience, recommendation performance, and system reliability.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Prior to our work, researchers have focuses more on
how to speed up the model inference requests,
using scalable embedding structures <cite class="ltx_cite ltx_citemacro_citep">(Kurniawan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib8" title="">2023</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib10" title="">2023</a>)</cite>,
heterogeneous caching embeddings <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib19" title="">2022</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib15" title="">2023</a>)</cite>, etc.
However, no prior work has attempted to investigate and understand
whether it is necessary to perform model inference for every ads request
in large-scale social network.
Our investigation into user access patterns reveals that
76% of consecutive user tower inferences occur within ten minutes,
and 52% occur within one minute.
This observation highlights the potential benefits of
using cached user embeddings to reduce the
number of requests for model inference.
In addition, it reveals a crucial triangular relationship
between user embedding freshness, model complexity,
and service SLAs in ads recommendation systems.
This interplay highlights the need for a balanced approach
that takes into account the trade-offs
between these factors to achieve
optimal system performance and efficiency.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges, we propose ERCache,
an efficient and reliable caching framework specifically
designed for large-scale user representation within ads recommendation systems.
Our approach is based on the observation that consecutive user tower inferences
often occur within a short time frame.
The primary goal of ERCache is
to achieve an optimal balance between user embedding freshness,
model complexity, and service SLAs.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">ERCache includes two components: direct cache and failover cache.
The direct cache stores generated user tower embeddings to bypass user tower inference requests when cached embeddings are valid.
Conversely, the failover cache is employed to recover from failed requests.
We carefully design cache requests and
eviction policies with customized settings for each model.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">ERCache has been deployed at Meta for more than half a year and
supported more than 30 ranking models at Meta successfully,
significantly unblocking computational resources limitations
while adhering to strict service SLAs.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In summary, this paper makes the following contributions:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">It shows the observation of user access pattern of
large-scale social network in ads recommendation,
which highlights the user access pattern in ads recommendation systems, revealing that a significant portion of consecutive user tower inferences occur within a short time frame.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">It reveals a crucial triangular relationship
between user embedding freshness, model complexity,
and service SLAs in ads recommendation systems.
This interplay highlights the need for a balanced approach
that takes into account the trade-offs
between these factors to achieve
optimal system performance and efficiency.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">It proposes ERCache, a novel caching framework
specifically designed for large-scale user representations
in ads recommender systems.
ERCache is a comprehensive solution that addresses
the challenges of constrained computational resources
and service SLA limitations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.ix4.p1">
<p class="ltx_p" id="S1.I1.ix4.p1.1">We integrate ERCache with
a diverse range of ranking models,
demonstrating its versatility and adaptability
in various ads recommendation scenarios.
Through extensive experiments,
we show the effectiveness of ERCache in
improving system efficiency and reliability.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Motivation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section presents some challenges and opportunities
that motivates our work.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Challenges</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The complexity of models used in ad recommendation systems
is increasing at a faster rate than the available computational resources,
creating challenges in terms of
constrained computational resources
and
service SLA limitations.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="132" id="S2.F1.g1" src="x1.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Model Serving Triangle.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">Constrained computational resources:</span>
Increased model complexity raises demand for
computational resources (e.g., CPUs, GPUs).
However, the availability of these resources is limited in reality,
thus not all models’ computational needs can be satisfied.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Service SLA limitations:</span>
Incorporating complex models may
increase e2e latency and are vulnerable to failures
due to computational demands.
This could potentially violate service SLAs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Model Serving Triangle</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.F1" title="Figure 1 ‣ 2.1. Challenges ‣ 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">1</span></a>, we have observed
a triangular relationship in model serving practice that
it is impossible for a model serving system to
simultaneously provide all three of the following guarantees:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Model complexity: the computation resource required
by ML models used in the model serving system.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Embedding freshness: how up-to-date the embeddings
(i.e., user tower embeddings) are in the model serving system.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Service SLAs: the requirements of important system metrics,
like e2e latency, model fallback rate, etc.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In other words, if a model serving system is designed to
handle complex models and provide fresh embeddings,
it may compromise on service SLAs.
Similarly, if the system prioritizes embedding freshness
and meets the requirements of service SLAs,
it may sacrifice model complexity.
Alternatively, if the system aims to achieve
both model complexity and service SLAs,
it may not be able to maintain embedding freshness.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Since model complexity tends to increase over time,
and service SLAs remain unchanged in production,
it is essential to explore opportunities for improvement
in embedding freshness. This can help maintain a balance
between the three factors and ensure that the model serving
system continues to perform optimally.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Opportunities</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S2.F2.g1" src="x2.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>CDF of consecutive inference time interval.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="328" id="S2.F3.g1" src="extracted/5912070/flow.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Sequence diagram of ERCache.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To find the opportunities,
we review the access pattern for online users interacting
with ads recommendation systems at Meta.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.F2" title="Figure 2 ‣ 2.3. Opportunities ‣ 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">2</span></a>,
there is a significant likelihood of multiple user tower inferences
occurring at a short time.
These findings have motivated us to design a caching system
to balancing user embedding freshness
and model performance
that takes advantage of user access patterns.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Specifically, 88% of consecutive user tower inferences
occur within an hour,
while 76% occur within ten minutes and 52% occur within one minute.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Our observations indicate that
even short-lived caches, such as one minute,
can significantly reduce computational resource usage.
Additionally, mid-lived caches lasting about an hour
can cover the majority of requests and
serve as a potential source for failure recovery.
These findings have motivated us to
design a caching system
that balances user embedding freshness,
model complexity, and service SLAs
by leveraging user access patterns.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Design and Implementation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we introduce the design details of ERCache.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Architecture of ERCache.</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">ERCache is a caching system independent from
ads ranking systems, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.F4" title="Figure 4 ‣ 3.1. Architecture of ERCache. ‣ 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">4</span></a>.
ERCache consists of two components:
direct cache and failover cache.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The direct cache stores generated user embeddings to
bypass user tower inference requests when cached embeddings are valid.
The failover cache applies the cached user embeddings
to recover from failed requests.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="96" id="S3.F4.g1" src="x3.png" width="374"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Architecture of the ERCache system.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>ERCache functionalities</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">ERCache offers three functionalities to
enhance the efficiency and robustness of ads ranking systems:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">Direct Cache Check: System checks if model’s direct cache
is valid before sending requests to inference;
uses cached embedding if valid, otherwise continues normally.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.ix2.p1">
<p class="ltx_p" id="S3.I1.ix2.p1.1">Failover Cache Assistance: For failed inference requests,
system checks failover cache for valid embeddings;
replaces failed requests with valid cached embeddings,
otherwise reports failure.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.ix3.p1">
<p class="ltx_p" id="S3.I1.ix3.p1.1">Cache update: Upon receiving the latest embeddings
from the model inference requests,
we will update the cache in ERCache
by issuing a write request.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">The sequence diagram of ERCache is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S2.F3" title="Figure 3 ‣ 2.3. Opportunities ‣ 2. Motivation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1">Parameter</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2">Parameter type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.3">Description</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.1">model_id</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.2">INT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1.3">It is a unique identifier for a specific ads ranking model.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.1">model_type</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.2">STRING</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.3.2.3">It is a unique identifier for a specific ads ranking model type.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.1">enable_flag</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.2">BOOL</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3.3">It determines whether or not the cache is enabled.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.4.1">cache_ttl</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.4.2">INT</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.5.4.3">It is used that specifies the duration for which embeddings are valid in the cache.</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>ERCache Configuration Parameters</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="236" id="S3.F5.g1" src="x4.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Combination technique of ERCache.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Customized cache configurations</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We chose the TTL-based eviction policy due to its alignment with user access patterns and time-based prioritization, which is more suitable for this cache design compared to LRU or other policy-driven approaches.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">This approach ensures that items are evicted from the cache based on their age, which aligns with the natural decay of user interest in content over time. Additionally, it allows us to prioritize items based on their recency, ensuring that the most recently accessed items are kept in the cache for longer periods of time. This approach also simplifies the cache management process, as it eliminates the need for complex heuristics or algorithms to determine which items to evict.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Overall, the TTL-based eviction policy provides a simple and effective solution for managing the cache and ensuring that it remains relevant and useful to users.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">ERCache offers caching capabilities for
individual model IDs or model types.
Customers have the flexibility to
enable caching based on their specific needs.
The ERCache configuration includes various parameters, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.T1" title="Table 1 ‣ 3.2. ERCache functionalities ‣ 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Update combination</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">ERCache employs a two-layer update combination mechanism
to minimize the number of cache write requests per user
across multiple ranking stages, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S3.F5" title="Figure 5 ‣ 3.2. ERCache functionalities ‣ 3. Design and Implementation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">5</span></a>.
By consolidating user embeddings from various ranking models
across multiple ranking stages
into a single request,
rather than having one request per model embeddings per stage,
we significantly reduce the write QPS on the ERCache.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Asynchronous write</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">After grouping all cache write requests into one single request,
we send the write request to  ERCache asynchronously.
The asynchronous operation moves write
out ot the critical path and
does not impact the e2e latency.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Regional consistency</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">ERCache guarantees the regional consistency
through its internal memcache system.
Since most requests are routed to the same region
as their previous serving for good locality,
both the request and cache remain in the same region most of the time,
ensuring efficient data access
and minimizing latency.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7. </span>Reliability</h3>
<div class="ltx_para" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">ERCache may face cascading effects due to
traffic oscillations, regional outages, and site events,
leading to increased load and reduced performance.
To enhance system reliability, a rate limiter has been implemented.
This rate limiter filters requests based
on regional thresholds if there is a sudden spike in QPS.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Evaluation</h2>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1">Predictor task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">Ranking stage</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">Direct cache TTL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4">Computation resource savings</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.5">E2E p99 latency diff</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.1">CVR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">First</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">5 minutes</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">44%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5">-0.4%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.1">CVR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">5 minute</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">51%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5">-0.11%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3">5 minutes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4">43%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5">-0.04%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.2">Second</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3">5 minutes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4">64%</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5">-0.03%</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.1">CVR</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.2">Second</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.3">1 minutes</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.4">52%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.6.5.5">-0.4%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>The ERCache (direct cache) performance on ads ranking models at Meta.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1">Predictor task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">Ranking stage</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">Failover cache TTL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">Fallback rate w/o cache</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5">Fallback rate w/ cache</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.1">CVR</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">Retrival</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">1 hour</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">0.7%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5">0.3%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">Retrival</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">1 hour</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">0.6%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5">0.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.1">CVR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.3">1 hour</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.4">5.9%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.4.3.5">0.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.1">CVR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.3">1 hour</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.4">6.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.5.4.5">0.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.3">1 hour</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.4">1.5%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.6.5.5">0.5%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.2">First</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.3">1 hour</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.4">1.4%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.7.6.5">0.1%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.1">CTR</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.2">Second</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.3">2 hours</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.4">0.05%</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.8.7.5">0.01%</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8.1">CVR</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8.2">Second</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8.3">2 hours</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8.4">0.1%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.9.8.5">0.04%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>The ERCache (failover cache) performance on ads ranking models at Meta.</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we evaluate ERCacheto answer the following questions:</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.ix1.p1">
<p class="ltx_p" id="S4.I1.ix1.p1.1">How much computational resources can ERCache save?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.ix2.p1">
<p class="ltx_p" id="S4.I1.ix2.p1.1">What’s the impact of ERCache on service SLAs?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I1.ix3.p1">
<p class="ltx_p" id="S4.I1.ix3.p1.1">How can ERCache affect model performance with
different cache TTL?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I1.ix4.p1">
<p class="ltx_p" id="S4.I1.ix4.p1.1">What is the effect of varying cache TTL settings on cache performance?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I1.ix5.p1">
<p class="ltx_p" id="S4.I1.ix5.p1.1">What are the serving costs of ERCache?</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="S4.I1.ix6.p1">
<p class="ltx_p" id="S4.I1.ix6.p1.1">Is ERCache reliable when faced with cascading effects, such as sudden changes in traffic or system failures?</p>
</div>
</li>
</ol>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental setup</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this study, all experiments were conducted on industrial datasets
using A/B testing in our production system.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">To measure the effcetiveness of ERCache,
we compare the computational resources and
key service SLAs for
enabling and disabling ads ranking models at Meta.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Specifically, we measure computational resources
by the power consumed during model inference
using both CPU and GPU.
In terms of service SLAs,
we focus on the key metrics that
impact our system’s performance,
including end-to-end (e2e) p99 latency
and model fallback rate.
Additionally, we evaluate model performance based on
Normalized Cross Entropy (NE).</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">To further understand the performance of ERCache,
we measure the cache hit rate with
different cache TTL settings.
We also evaluate the serving cost of ERCache
and its reliability during cascading effects.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Computational resource evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As mentioned earlier, we measure the power usage
for a model and compare the change with and without
direct cache to evaluate the effectiveness of ERCache
in reducing computational resources.
From Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T2" title="Table 2 ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">2</span></a>,
we find ERCache can significantly reduce computational resource
usage by 42% to 64%,
depending on the cache TTL settings.
Furthermore, we note that
the power savings achieved by ERCache
vary across different models,
due to their distinct access patterns and model profiles.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Service SLAs evaluation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To understand the imapct of ERCache on service SLAs,
we evaluate e2e p99 latency and model fallback rate
with ERCache enabled.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">E2E p99 latency.</span>
According to Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T2" title="Table 2 ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">2</span></a>,
we achieved an average reduction of 0.2%
in end-to-end p99 latency.
This improvement is attributed to
the decrease in the number of model inference requests
and reduced workload in the ads recommendation systems.
Notably, we did not observe
any NE loss for the models with direct cache enabled,
using the cache TTL shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T2" title="Table 2 ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">2</span></a>.
Notably, we did not observe any NE loss
for the models with direct cache enabled,
using the cache TTL shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T2" title="Table 2 ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Model fallback rate.</span>
Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T3" title="Table 3 ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">3</span></a> shows that
the failover cache effectively reduces
the fallback rate for ads ranking models,
with an average reduction of 79.6%.
The most notable improvement is observed
in the CVR model at the first ranking stage,
where the fallback rate decreased from 6.5% to 0.1%.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Impact of cache TTL</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To further understand the impact of cache TTL,
we evaluate model performance and cache performance
using different cache TTL settings.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Model performance evaluation.</span></p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.1">Direct cache TTL</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.2">NE difference</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.1">30 seconds</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.2">0.002%</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.1">1 minute</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2.2">-0.001%</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.1">2 minutes</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.3.2">-0.007%</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.1">5 minutes</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4.2">0.003%</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.5.1">10 minutes</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.6.5.2">0.06%</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>The impact of cache TTL on model performance.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">We investigate the relationship between model performance (NE)
and cache TTL by comparing the NE difference
between enabling and disabling direct cache with
varying cache TTLs, ranging from 30 seconds to 10 minutes.
We find that the model’s performance starts
to degrade when the cache TTL is set to 10 minutes or higher,
as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.T4" title="Table 4 ‣ 4.4. Impact of cache TTL ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">4</span></a>.
It is important to note that a lower NE value
indicates better model performance.</p>
</div>
<div class="ltx_para" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.1.1">Impact of cache TTL on cache performance.</span></p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Figure  <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.F6" title="Figure 6 ‣ 4.4. Impact of cache TTL ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">6</span></a> shows the impact of cache TTL over direct cache.
On average, a 1-minute TTL yields a 51.6% cache hit rate,
while a 5-minute TTL results in a 68.7% cache hit rate.
A 1-hour TTL achieves an impressive 89.7% cache hit rate,
and extending the TTL to 6 hours leads to a remarkable 97.1% cache hit rate.
Furthermore, a 12-hour TTL can achieve an outstanding 97.9% cache hit rate.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S4.F6.g1" src="x5.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Impact of cache TTL on direct cache.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p6.1.1">Optimizing cache TTL settings in production.</span>
In practice, we typically set a shorter TTL for the direct cache
and a longer TTL for the failover cache.
This is because we prioritize maintaining model performance
by using a short TTL in the direct cache,
while the failover cache is designed to compensate
for failed model inference requests
and is less concerned with data freshness,
so a longer TTL can be used.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Serving cost of ERCache</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">We evaluate the serving cost of ERCache from
QPS, latency, and bandwidth.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">QPS</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S4.F7.g1" src="x6.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>ERCache QPS over a week period.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.F7" title="Figure 7 ‣ 4.5. Serving cost of ERCache ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">7</span></a> shows the write QPS,
which ranges from 0.93 M/s to 1.63 M/s,
and the read QPS, which varies between 2.43 M/s and 3.778 M/s.
By applying the caching grouping technique,
we have successfully reduced the number of cache reads and writes.
If we were to support 30 models without this grouping technique,
the QPS would increase by at least 30x.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">Latency</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S4.F8.g1" src="x7.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>CDF of ERCache read latency.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.F8" title="Figure 8 ‣ 4.5. Serving cost of ERCache ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">8</span></a> displays the CDF of read latency in ERCache.
The results show that 50% of read requests
have a latency of less than 1 ms,
while 80% have a latency of less than 2 ms.
The p50 read latency is 0.77 ms and
the p99 read latency is 8.47 ms.
Most read requests can be completed within 10 ms.
The low latency of ERCache
does not hurt the performance of ads recommendation systems.</p>
</div>
<div class="ltx_para" id="S4.SS5.p6">
<p class="ltx_p" id="S4.SS5.p6.1">Since we use asynchronous write to ERCache,
we do not prioritize the write latency.</p>
</div>
<div class="ltx_para" id="S4.SS5.p7">
<p class="ltx_p" id="S4.SS5.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.p7.1.1">Write bandwidth</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="280" id="S4.F9.g1" src="x8.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Bytes bandwidth of ERCache over a week period.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p8">
<p class="ltx_p" id="S4.SS5.p8.1">The write bandwidth of ERCache varies between
7.26 GB/s and 12.43 GB/s, with an average of 9.16 GB/s, shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.F9" title="Figure 9 ‣ 4.5. Serving cost of ERCache ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">9</span></a>.
We do not discuss the read throughput
as it is relatively inexpensive in memory.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Reliability of  ERCache</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">To assess the reliability of  ERCache,
we conducted a drain test on one region and
monitored its performance during this special situation.
The drain test involved intentionally taking down a data center/region
to simulate a disaster scenario,
such as a fire or power outage.</p>
</div>
<div class="ltx_para" id="S4.SS6.p2">
<p class="ltx_p" id="S4.SS6.p2.1">We ran a 6-hour drain test on one region out
of 13 main regions.
Figure<a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#S4.F10" title="Figure 10 ‣ 4.7. Key takeaways ‣ 4. Evaluation ‣ ERCache: An Efficient and Reliable Caching Framework for Large-Scale User Representations in Meta’s Ads System"><span class="ltx_text ltx_ref_tag">10</span></a> presents the results
of the reliability test.
The drain test began at hour 21 and ended at hour 26.
During the test period, we did not observe any unusual changes
in ERCache’s primary metrics.
The cache hit rate of  ERCache remained stable throughout the period.
The results demonstrate that  ERCache
can withstand severe situations,
such as cascading effects, and exhibits good reliability.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7. </span>Key takeaways</h3>
<div class="ltx_para" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">ERCache has been deployed at Meta for over two years,
providing support to more than 30 ranking models
and ensuring improved model performance
in accordance with service SLAs.</p>
</div>
<div class="ltx_para" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">The success of ERCache demonstrates that</p>
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.ix1.p1">
<p class="ltx_p" id="S4.I2.ix1.p1.1">Model inference is not necessary for every ads request,
despite the importance of embedding freshness to model performance.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.ix2.p1">
<p class="ltx_p" id="S4.I2.ix2.p1.1">The triangular relationship between model complexity, embedding freshness,
and service SLAs is useful and reasonable.
It can serve as a reference for other researchers and engineers developing models and systems.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.ix3.p1">
<p class="ltx_p" id="S4.I2.ix3.p1.1">The serving cost of ERCache is not expensive
due to its low QPS, latency, and bandwidth.</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I2.ix4.p1">
<p class="ltx_p" id="S4.I2.ix4.p1.1">ERCache can be easily applied to other ads recommendation systems in large-scale social networks or other areas with similar access patterns to Meta.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="285" id="S4.F10.g1" src="x9.png" width="374"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>6-hour drain test for ERCache</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>related work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Training Optimization</span>
Recent publications focus on optimizing DRAM cache
and GPU resident cache utilization for training purposes.
HierPS <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib26" title="">2020</a>)</cite> is a
distributed GPU hierarchical parameter server
for massive scale deep learning ads systems
with 3-layer hierarchical storage
including GPU HBM, CPU memory and SSD.
AIBox <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib27" title="">2019</a>)</cite> is
a centralized system to train CTR
models with tens-of-terabytes-gb
by SSDs and GPUs.
While they prioritize training optimization,
our focus is on design of caching systems for efficient
and reliable model inference.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Embedding optimization</span>
AdaEmbed <cite class="ltx_cite ltx_citemacro_citep">(Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib9" title="">2023</a>)</cite> is a complementary system,
to reduce the size of embeddings needed for the same accuracy
via in-training embedding pruning.
It prioritizes embeddings with high runtime access
frequencies and large training gradients,
dynamically pruning less important ones to optimize per-feature embeddings.
AdaEmbed targets embedding optimization during the training phase,
which is a distinct area of caching research within our work.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Inference optimization</span>
Fleche <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib19" title="">2022</a>)</cite> presents a comprehensive cache
scheme with detailed designs
for efficient GPU-resident embedding caching.
UGACHE <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib15" title="">2023</a>)</cite> introduces
a novel factored extraction mechanism that
mitigates bandwidth congestion to
fully utilize high-speed cross-GPU interconnects.
RECom <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib10" title="">2023</a>)</cite>
proposes the first ML compiler designed to optimize
the massive embedding columns in recommendation models on the GPU.
EVStore <cite class="ltx_cite ltx_citemacro_citep">(Kurniawan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib8" title="">2023</a>)</cite> is a 3-layer table lookup
system using both structural regularity
in inference operations and
domain-specific approximations
to provide optimized caching.
However, these works focus on optimizing the performance of model inference,
whereas ERCache targets the caching system
before sending requests to model inference.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1"><span class="ltx_text ltx_font_bold" id="S5.p4.1.1">Cache case study</span>
Twitter has published an analysis of its internal
caching system <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.06497v1#bib.bib20" title="">2021</a>)</cite>.
The paper aims to characterize cache workloads
based on traffic patterns, TTL,
popularity distribution, and size distribution.
However, this analysis is too broad and not specifically
tailored to ads recommendation systems.
As a result, ads recommendation systems may not
find much value in such a general analysis.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We introduce ERCache,
a caching framework specifically
designed to efficiently and reliably
manage large-scale user representations.
ERCache helps alleviate computational resource limitations
for increasingly complex models while
ensuring that onboarding complex models
meets SLAs.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">By utilizing a direct and failover cache system
alongside customized eviction policies,
 ERCache effectively balances model complexity,
embedding freshness, and SLAs,
despite the inherent staleness introduced by caching.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">ERCache has been successfully deployed
in Meta’s production systems for over half a year,
supporting more than 30 ad ranking models.
This deployment has significantly reduced
computational resource requirements
while maintaining service SLAs.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">Apart from the practical contributions,
the triangular relationship identified in this study,
along with the success of ERCache,
provides a valuable reference for the
design and research of ad recommendation systems.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah. 2016.

</span>
<span class="ltx_bibblock">Wide &amp; Deep Learning for Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</em> (Boston, MA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(DLRS 2016)</em>. Association for Computing Machinery, New York, NY, USA, 7–10.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2988450.2988454" title="">https://doi.org/10.1145/2988450.2988454</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El-Kishky et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ahmed El-Kishky, Thomas Markovich, Serim Park, Chetan Verma, Baekjin Kim, Ramy Eskander, Yury Malkov, Frank Portman, Sofía Samaniego, Ying Xiao, and Aria Haghighi. 2022.

</span>
<span class="ltx_bibblock">TwHIN: Embedding the Twitter Heterogeneous Information Network for Personalized Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (Washington DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib3.4.2">(KDD ’22)</em>. Association for Computing Machinery, New York, NY, USA, 2842–2850.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534678.3539080" title="">https://doi.org/10.1145/3534678.3539080</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grbovic and Cheng (2018)</span>
<span class="ltx_bibblock">
Mihajlo Grbovic and Haibin Cheng. 2018.

</span>
<span class="ltx_bibblock">Real-Time Personalization Using Embeddings for Search Ranking at Airbnb. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (London, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">(KDD ’18)</em>. Association for Computing Machinery, New York, NY, USA, 311–320.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3219819.3219885" title="">https://doi.org/10.1145/3219819.3219885</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.

</span>
<span class="ltx_bibblock">DeepFM: A Factorization-Machine Based Neural Network for CTR Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 26th International Joint Conference on Artificial Intelligence</em> (Melbourne, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(IJCAI’17)</em>. AAAI Press, 1725–1731.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2017a)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017a.

</span>
<span class="ltx_bibblock">Neural Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 26th International Conference on World Wide Web</em> (Perth, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(WWW ’17)</em>. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 173–182.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3038912.3052569" title="">https://doi.org/10.1145/3038912.3052569</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2017b)</span>
<span class="ltx_bibblock">
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017b.

</span>
<span class="ltx_bibblock">Neural Collaborative Filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 26th International Conference on World Wide Web</em> (Perth, Australia) <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(WWW ’17)</em>. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 173–182.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3038912.3052569" title="">https://doi.org/10.1145/3038912.3052569</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kurniawan et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniar H Kurniawan, Ruipu Wang, Kahfi S Zulkifli, Fandi A Wiranata, John Bent, Ymir Vigfusson, and Haryadi S Gunawi. 2023.

</span>
<span class="ltx_bibblock">EVStore: Storage and Caching Capabilities for Scaling Embedding Tables in Deep Recommendation Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2</em>. 281–294.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib9.6.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fan Lai, Wei Zhang, Rui Liu, William Tsai, Xiaohan Wei, Yuxi Hu, Sabin Devkota, Jianyu Huang, Jongsoo Park, Xing Liu, et al<span class="ltx_text" id="bib.bib9.7.1">.</span> 2023.

</span>
<span class="ltx_bibblock"><math alttext="\{" class="ltx_Math" display="inline" id="bib.bib9.1.m1.1"><semantics id="bib.bib9.1.m1.1a"><mo id="bib.bib9.1.m1.1.1" stretchy="false" xref="bib.bib9.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.1.m1.1b"><ci id="bib.bib9.1.m1.1.1.cmml" xref="bib.bib9.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.1.m1.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.1.m1.1d">{</annotation></semantics></math>AdaEmbed<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib9.2.m2.1"><semantics id="bib.bib9.2.m2.1a"><mo id="bib.bib9.2.m2.1.1" stretchy="false" xref="bib.bib9.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.2.m2.1b"><ci id="bib.bib9.2.m2.1.1.cmml" xref="bib.bib9.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.2.m2.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.2.m2.1d">}</annotation></semantics></math>: Adaptive Embedding for <math alttext="\{" class="ltx_Math" display="inline" id="bib.bib9.3.m3.1"><semantics id="bib.bib9.3.m3.1a"><mo id="bib.bib9.3.m3.1.1" stretchy="false" xref="bib.bib9.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.3.m3.1b"><ci id="bib.bib9.3.m3.1.1.cmml" xref="bib.bib9.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.3.m3.1c">\{</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.3.m3.1d">{</annotation></semantics></math>Large-Scale<math alttext="\}" class="ltx_Math" display="inline" id="bib.bib9.4.m4.1"><semantics id="bib.bib9.4.m4.1a"><mo id="bib.bib9.4.m4.1.1" stretchy="false" xref="bib.bib9.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib9.4.m4.1b"><ci id="bib.bib9.4.m4.1.1.cmml" xref="bib.bib9.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib9.4.m4.1c">\}</annotation><annotation encoding="application/x-llamapun" id="bib.bib9.4.m4.1d">}</annotation></semantics></math> Recommendation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.8.1">17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)</em>. 817–831.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zaifeng Pan, Zhen Zheng, Feng Zhang, Ruofan Wu, Hao Liang, Dalin Wang, Xiafei Qiu, Junjie Bai, Wei Lin, and Xiaoyong Du. 2023.

</span>
<span class="ltx_bibblock">RECom: A Compiler Approach to Accelerating Recommendation Model Inference with Massive Embedding Columns. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 4</em>. 268–286.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pancha et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022.

</span>
<span class="ltx_bibblock">PinnerFormer: Sequence Modeling for User Representation at Pinterest. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (Washington DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(KDD ’22)</em>. Association for Computing Machinery, New York, NY, USA, 3702–3712.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3534678.3539156" title="">https://doi.org/10.1145/3534678.3539156</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pi et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2019.

</span>
<span class="ltx_bibblock">Practice on Long Sequential User Behavior Modeling for Click-Through Rate Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (Anchorage, AK, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.2">(KDD ’19)</em>. Association for Computing Machinery, New York, NY, USA, 2671–2679.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3292500.3330666" title="">https://doi.org/10.1145/3292500.3330666</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle (2010)</span>
<span class="ltx_bibblock">
Steffen Rendle. 2010.

</span>
<span class="ltx_bibblock">Factorization Machines. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">2010 IEEE International Conference on Data Mining</em>. 995–1000.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICDM.2010.127" title="">https://doi.org/10.1109/ICDM.2010.127</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rendle (2012)</span>
<span class="ltx_bibblock">
Steffen Rendle. 2012.

</span>
<span class="ltx_bibblock">Factorization Machines with LibFM.

</span>
<span class="ltx_bibblock">3, 3, Article 57 (may 2012), 22 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2168752.2168771" title="">https://doi.org/10.1145/2168752.2168771</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoniu Song, Yiwen Zhang, Rong Chen, and Haibo Chen. 2023.

</span>
<span class="ltx_bibblock">UGACHE: A Unified GPU Cache for Embedding-based Deep Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 29th Symposium on Operating Systems Principles</em> (Koblenz, Germany) <em class="ltx_emph ltx_font_italic" id="bib.bib15.4.2">(SOSP ’23)</em>. Association for Computing Machinery, New York, NY, USA, 627–641.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3600006.3613169" title="">https://doi.org/10.1145/3600006.3613169</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017.

</span>
<span class="ltx_bibblock">Deep &amp; Cross Network for Ad Click Predictions. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the ADKDD’17</em> (Halifax, NS, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.2">(ADKDD’17)</em>. Association for Computing Machinery, New York, NY, USA, Article 12, 7 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3124749.3124754" title="">https://doi.org/10.1145/3124749.3124754</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021.

</span>
<span class="ltx_bibblock">DCN V2: Improved Deep &amp; Cross Network and Practical Lessons for Web-Scale Learning to Rank Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Proceedings of the Web Conference 2021</em> (Ljubljana, Slovenia) <em class="ltx_emph ltx_font_italic" id="bib.bib17.4.2">(WWW ’21)</em>. Association for Computing Machinery, New York, NY, USA, 1785–1797.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3442381.3450078" title="">https://doi.org/10.1145/3442381.3450078</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, Po-Wei Wang, Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, and Andrew Zhai. 2023.

</span>
<span class="ltx_bibblock">TransAct: Transformer-Based Realtime User Action Model for Recommendation at Pinterest. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (Long Beach, CA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib18.4.2">(KDD ’23)</em>. Association for Computing Machinery, New York, NY, USA, 5249–5259.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3580305.3599918" title="">https://doi.org/10.1145/3580305.3599918</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Minhui Xie, Youyou Lu, Jiazhen Lin, Qing Wang, Jian Gao, Kai Ren, and Jiwu Shu. 2022.

</span>
<span class="ltx_bibblock">Fleche: an efficient GPU embedding cache for personalized recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the Seventeenth European Conference on Computer Systems</em> (Rennes, France) <em class="ltx_emph ltx_font_italic" id="bib.bib19.4.2">(EuroSys ’22)</em>. Association for Computing Machinery, New York, NY, USA, 402–416.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3492321.3519554" title="">https://doi.org/10.1145/3492321.3519554</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Juncheng Yang, Yao Yue, and KV Rashmi. 2021.

</span>
<span class="ltx_bibblock">A large-scale analysis of hundreds of in-memory key-value cache clusters at twitter.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">ACM Transactions on Storage (TOS)</em> 17, 3 (2021), 1–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec. 2018.

</span>
<span class="ltx_bibblock">Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (London, United Kingdom) <em class="ltx_emph ltx_font_italic" id="bib.bib21.4.2">(KDD ’18)</em>. Association for Computing Machinery, New York, NY, USA, 974–983.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3219819.3219890" title="">https://doi.org/10.1145/3219819.3219890</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, and Liguang Zhang. 2020.

</span>
<span class="ltx_bibblock">Parameter-Efficient Transfer from Sequential Behaviors for User Modeling and Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (Virtual Event, China) <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.2">(SIGIR ’20)</em>. Association for Computing Machinery, New York, NY, USA, 1469–1478.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3397271.3401156" title="">https://doi.org/10.1145/3397271.3401156</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Fajie Yuan, Guoxiao Zhang, Alexandros Karatzoglou, Joemon Jose, Beibei Kong, and Yudong Li. 2021.

</span>
<span class="ltx_bibblock">One Person, One Model, One World: Learning Continual User Representation without Forgetting. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (¡conf-loc¿, ¡city¿Virtual Event¡/city¿, ¡country¿Canada¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib23.4.2">(SIGIR ’21)</em>. Association for Computing Machinery, New York, NY, USA, 696–705.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3404835.3462884" title="">https://doi.org/10.1145/3404835.3462884</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Junqi Zhang, Bing Bai, Ye Lin, Jian Liang, Kun Bai, and Fei Wang. 2020.

</span>
<span class="ltx_bibblock">General-Purpose User Embeddings Based on Mobile App Usage. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em> (Virtual Event, CA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib24.4.2">(KDD ’20)</em>. Association for Computing Machinery, New York, NY, USA, 2831–2840.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3394486.3403334" title="">https://doi.org/10.1145/3394486.3403334</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei Zhang, Dai Li, Chen Liang, Fang Zhou, Zhongke Zhang, Xuewei Wang, Ru Li, Yi Zhou, Yaning Huang, Dong Liang, et al<span class="ltx_text" id="bib.bib25.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Scaling User Modeling: Large-scale Online User Representations for Ads Personalization in Meta. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.4.1">Companion Proceedings of the ACM on Web Conference 2024</em>. 47–55.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Weijie Zhao, Deping Xie, Ronglai Jia, Yulei Qian, Ruiquan Ding, Mingming Sun, and Ping Li. 2020.

</span>
<span class="ltx_bibblock">Distributed hierarchical gpu parameter server for massive scale deep learning ads systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of Machine Learning and Systems</em> 2 (2020), 412–428.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Weijie Zhao, Jingyuan Zhang, Deping Xie, Yulei Qian, Ronglai Jia, and Ping Li. 2019.

</span>
<span class="ltx_bibblock">AIBox: CTR prediction model training on a single node. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the 28th ACM International Conference on Information and Knowledge Management</em>. 319–328.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  9 02:47:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
