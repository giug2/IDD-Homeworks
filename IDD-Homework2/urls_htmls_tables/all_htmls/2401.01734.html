<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2401.01734] Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data</title><meta property="og:description" content="Assistive robots should be able to wash, fold or iron clothes.
However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging. Sy…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2401.01734">

<!--Generated on Tue Feb 27 10:37:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Deep Learning for Visual Perception,  Simulation and Animation,  Data Sets for Robotic Vision
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Lips<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Victor-Louis De Gusseme<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">1</span></sup> and Francis wyffels<sup id="id7.7.id3" class="ltx_sup"><span id="id7.7.id3.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"><sup id="id8.8.id1" class="ltx_sup"><span id="id8.8.id1.1" class="ltx_text ltx_font_italic">1</span></sup>AI and Robotics Lab, Ghent University - imec 
<br class="ltx_break">Technologiepark 126, 9052 Zwijnaarde, Belgiumcorresponding author: Thomas.Lips@UGent.be</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Assistive robots should be able to wash, fold or iron clothes.
However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging. Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap.
To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items. To test its performance, we have also collected a real-world dataset.
</p>
<p id="id10.id2" class="ltx_p">We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%. Fine-tuning on real-world data improves performance to 74.2%. Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials.
We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap. The code, dataset and trained models are available at <a target="_blank" href="https://github.com/tlpss/synthetic-cloth-data" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tlpss/synthetic-cloth-data</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Deep Learning for Visual Perception, Simulation and Animation, Data Sets for Robotic Vision

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Clothes are everywhere in our living environments. Therefore assistive robots should be able to interact with cloth items and perform tasks such as washing, folding or ironing. However, due to their variety in shape and appearance, complex deformations and self-occlusions, cloth manipulation and perception are challenging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Progress has been made in recent years thanks to the continued integration of data-driven techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and the development of new hardware <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, but we do not yet have robots that can reliably flatten, fold or iron arbitrary pieces of cloth.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Part of the reason is that we lack the required amount of data to make these systems generalize. The use of synthetic data has proven an effective direction to overcome this data limit in various domains of robotics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and beyond <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Synthetic data can provide arbitrary amounts of perfectly annotated data, but the main concern is to bridge the gap between synthetic and real data, as perfectly covering the real data distribution is often infeasible. Various approaches have been explored to overcome this <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, yet a great deal of engineering effort is usually involved in making sim-to-real work.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we want to enable generic robot folding. To this end, we generate synthetic data to learn keypoints on various cloth items. We focus on detecting non-occluded keypoints from an RGB image of almost flattened clothes for various cloth categories. Using these keypoints, tasks such as folding flattened clothes can be achieved with scripted motions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Several prior works use synthetic data for cloth manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> but to the best of our knowledge, none have tackled this specific setting. Furthermore, we are also the first to compare the performance of different procedures to obtain cloth meshes and materials.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2401.01734/assets/images/examples-combined.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>In this work we learn to detect semantic keypoints on <span id="S1.F1.2.1" class="ltx_text ltx_font_italic">almost-flattened</span> clothes in everyday environments. To tackle the large diversity in cloth states, cloth materials and environments, we generate synthetic data to train these keypoint detectors. </figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We generate synthetic data for three different cloth categories: T-shirts, towels and shorts. This process has three stages: We first procedurally create cloth meshes. In the second stage, these meshes are dropped and deformed to mimic the output of current unfolding systems, for which we use Nvidia Flex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Finally, we render images of the clothes and generate the corresponding annotations. To provide real-world training data and to thoroughly evaluate the performance of the keypoint detectors, we have also gathered a dataset of almost 2,000 images containing more than 100 cloth items.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We find that keypoint detectors trained on our synthetic data have a mean average precision of 64.3 % on our test dataset, which is an improvement over the baseline trained only on real data. Fine-tuning using real data further improves performance to 74.2%. These results show that our synthetic data pipeline can be used to build state estimators for robotic cloth folding systems. Additionally, we compare several different procedures to generate cloth meshes and materials where we find that the best results are obtained by using random materials and single-layer cloth meshes, even though this decreases fidelity. Finally, we quantify the remaining sim-to-real gap and argue that this gap can only be closed by further advancements in cloth asset generation and simulation.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To summarize, our contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We build a synthetic data pipeline and use it to train keypoint detectors that enable cloth folding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We compare different procedures to obtain cloth meshes and materials, to gain more insight into synthetic data generation for cloth manipulation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide a real-world dataset of 2,000 images and corresponding annotations of almost-flatted clothes in everyday environments.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Robotic Cloth Manipulation</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Robotic cloth manipulation has been extensively studied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Most works in robotic laundering focus on unfolding (also called flattening) and folding. Despite the considerable success demonstrated by unfolding pipelines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, they frequently yield clothes that aren’t perfectly flattened and have yet to fully demonstrate the desired generalization across various cloth instances and environments</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Given flattened clothes, tasks such as folding can be tackled using an appropriate representation and a scripted policy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Several methods have been explored to create such state representations for clothes, including template fitting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, edge detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and semantic keypoint detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Many of these works use depth images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, as depth images are in theory invariant to cloth materials. However, they also discard useful information (such as seams) and additionally can lack the required precision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In this work, we focus on learning keypoints on RGB images of almost-flattened clothes that are lying on a surface. By doing so, we aim to facilitate the creation of cloth manipulation systems that operate on the output of an unfolding system and can generalize to arbitrary clothes and environments.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Synthetic data for Robotic Cloth Manipulation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Synthetic data has been used extensively in robotic cloth manipulation research to learn representations and end-to-end policies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
A variety of cloth simulators have been used, including Pybullet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, Nvidia Flex<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">As always when learning from synthetic data, obtaining appropriate 3D assets is a key challenge. Many works use a limited number of pre-made cloth meshes which they manually annotate if needed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Others generate single-layer meshes procedurally <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The authors of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> have focused on generating a large dataset of cloth meshes, which is used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, though these meshes do not come with annotations of semantic locations or edges.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">In this work, we further explore the use of synthetic data for keypoint detection. Our goal is to create a pipeline from which models can be trained that generalize to arbitrary clothes, deformations and environments.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps"> almost-ready-to-fold Clothes Dataset</span>
</h2>

<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The aRTF Clothes dataset in numbers. Both scenes and cloth items are distinct for the train and test splits to measure the generalization performance of trained models.
</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold"># scenes</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold"># cloth items</span></th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold"># Images</span></th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S3.T1.1.2.2.1.1" class="ltx_text ltx_font_bold">Cloth Category</span></th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Train</th>
<th id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Test</th>
<th id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Train</th>
<th id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Test</th>
<th id="S3.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">Train</th>
<th id="S3.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Test</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<th id="S3.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Towels</th>
<td id="S3.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">6</td>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">8</td>
<td id="S3.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">15</td>
<td id="S3.T1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S3.T1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t">210</td>
<td id="S3.T1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t">400</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<th id="S3.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">T-shirts</th>
<td id="S3.T1.1.4.2.2" class="ltx_td ltx_align_center">6</td>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.4.2.4" class="ltx_td ltx_align_center">15</td>
<td id="S3.T1.1.4.2.5" class="ltx_td ltx_align_center">20</td>
<td id="S3.T1.1.4.2.6" class="ltx_td ltx_align_center">210</td>
<td id="S3.T1.1.4.2.7" class="ltx_td ltx_align_center">400</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr">
<th id="S3.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Shorts</th>
<td id="S3.T1.1.5.3.2" class="ltx_td ltx_align_center">6</td>
<td id="S3.T1.1.5.3.3" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.5.3.4" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.5.3.5" class="ltx_td ltx_align_center">9</td>
<td id="S3.T1.1.5.3.6" class="ltx_td ltx_align_center">112</td>
<td id="S3.T1.1.5.3.7" class="ltx_td ltx_align_center">180</td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<th id="S3.T1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Boxershorts</th>
<td id="S3.T1.1.6.4.2" class="ltx_td ltx_align_center">6</td>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_center">8</td>
<td id="S3.T1.1.6.4.4" class="ltx_td ltx_align_center">11</td>
<td id="S3.T1.1.6.4.5" class="ltx_td ltx_align_center">11</td>
<td id="S3.T1.1.6.4.6" class="ltx_td ltx_align_center">154</td>
<td id="S3.T1.1.6.4.7" class="ltx_td ltx_align_center">220</td>
</tr>
<tr id="S3.T1.1.7.5" class="ltx_tr">
<th id="S3.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Total</th>
<td id="S3.T1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">6</td>
<td id="S3.T1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">8</td>
<td id="S3.T1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">49</td>
<td id="S3.T1.1.7.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">60</td>
<td id="S3.T1.1.7.5.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">686</td>
<td id="S3.T1.1.7.5.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">1200</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To train and evaluate the keypoint detectors, we have created a dataset of (almost) flattened clothes. We aim to enable folding clothes that have been previously unfolded (flattened) and because current unfolding pipelines do not yet produce perfectly flattened results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we have applied small deformations to the clothes. We refer to this dataset as the aRTF Clothes dataset, for almost-ready-to-fold clothes. There have already been some efforts to collect datasets for robotic cloth manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, but to the best of our knowledge, this dataset is the first to provide labeled images of almost-flattened cloth items in household settings.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In the next sections, we briefly describe the data capturing and labeling procedure.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Dataset Capturing</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To collect the images we listed 14 household scenes containing a flat surface that can be used to fold clothes.
We then collected several clothing items by asking lab members to bring them. For the towels, we included some pieces from the household cloth dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For each image, we started by laying the cloth item on the folding surface.
Based on examples from state-of-the-art unfolding pipelines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, we distilled a number of failure modes and linked these to deformations that we randomly applied to the flat cloth items. These deformations include a random pinch to create wrinkles, or folding a side or corner of the cloth upwards or downwards.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We collected the images using a ZED2i RGB-D camera. For a number of scenes we used a smartphone alongside the ZED camera, to incorporate camera and lens diversity.
The camera was positioned to mimic an ego-centric perspective. The distance to the folding surface is between 0.5 and 1 m.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">The dataset contains a total of 1,896 images and is summarized in Table <a href="#S3.T1" title="TABLE I ‣ III almost-ready-to-fold Clothes Dataset ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. Collecting these images took 2 persons about 15 hours. Some example images from the dataset can be found in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Annotating</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">For each cloth category, we defined a number of semantic locations (keypoints) and annotated these on each image. We mostly use the same semantic locations as Miller et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. The semantic locations can be seen in the examples in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We also labeled the segmentation mask and bounding boxes, for which we use Segment Anything (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. We have found SAM regularly fails with a single point, but produces satisfactory masks when provided with or 2 to 3 points on the cloth surface.
In total, this labeling effort took a single person about 3 days.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Procedural Data Generation Pipeline</span>
</h2>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2401.01734/assets/images/sim-examples.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="296" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of the generated synthetic images. Though these single-layer meshes with random materials look unrealistic, they have the best performance out of all evaluated procedures.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe the synthetic data pipeline we have created to generate synthetic images of clothes. In general, generating synthetic data entails two steps: obtaining the assets (meshes, materials…) and specifying how these assets should be used to create 3D scenes, from which the desired images and their corresponding annotations can then be generated. Powerful tools such as Blender<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> have enabled the latter step. The main challenge typically lies in gathering or creating the required assets, and this typically comes with a trade-off between realism and diversity. As we are dealing with deformable objects, generating the assets is even more complicated: we need not only to obtain cloth meshes but also to generate the desired distribution of configurations for them, i.e., we need to deform them. The next sections will discuss in more detail how we generate cloth meshes and materials, deform the clothes using Nvidia Flex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and use Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> to generate images and the corresponding annotations.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Obtaining Cloth Assets</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Cloth meshes</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">The first step is to obtain flat cloth meshes. We generate a series of 2D boundary vertices using a template for each cloth type. These templates are inspired on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.
We then connect the vertices using bezier curves to create single-layer meshes. These bezier curves are used to create the neck of a T-shirt and to better mimic real cloth items by slightly curving edges. For the same reason, we also round the corners of the single-layer mesh. The parameters of the skeleton, the bezier curves and the rounding radii are sampled from manually tuned ranges. Finally, the meshes are triangulated such that they have edge lengths of at most 1cm and UV maps are generated. We keep track of the vertices that correspond to each semantic location to automatically label the keypoints later on.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Cloth Materials</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">Faithfully reproducing cloth materials requires both mimicking fabrics and producing realistic color patterns. At the same time, we also need these materials to provide sufficient diversity to enable the generalization of the models trained on the data. This combination tends to be infeasible. We have experimented with various procedures (see Section <a href="#S6.SS2" title="VI-B Comparing procedures for obtaining cloth meshes ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-B</span></span></a>) to generate the cloth materials and have found that using random textures from PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> provides the best results. To increase diversity, we mix these textures with a random color.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Deforming the Meshes</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To generate diverse mesh configurations from the flattened meshes, we deform them using the Nvidia Flex cloth simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, which is also used in several previous works on robotic cloth manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. We have also experimented with the Blender cloth simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and Pybullet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, but found Nvidia Flex to perform better.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">To deform the meshes, we randomize their orientation and drop them on a surface to create wrinkles. Afterwards, we sometimes grasp a point on the cloth and use a circular trajectory to create folds. To create both visible and invisible folds, we sometimes flip the cloth by lifting it entirely, rotating it and dropping it again. This procedure differs from the drop-pick-drop procedure used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, as we focus on <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">almost-flattened</span> clothes instead of arbitrary crumpled states. The parameters of the procedure have been tuned manually to maximize performance.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">To load a mesh into Nvidia Flex, we convert all vertices of the mesh to particles and add both stretch stiffness and bending stiffness constraints using the 1-ring and 2-ring neighbors of each vertex, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Additionally, similar to previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we randomize various physics parameters, including bending stiffness, stretch stiffness, friction and drag to increase diversity.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance of the keypoint detector for various data sources. Training on synthetic data outperforms the baseline trained only on real data. The performance is significantly improved by training on synthetic data and then fine-tuning on real data.</figcaption>
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T2.1.1.1.1" class="ltx_text ltx_font_bold">mAP<sub id="S4.T2.1.1.1.1.1" class="ltx_sub">2,4,8</sub> (<math id="S4.T2.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo stretchy="false" id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
<th id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T2.2.2.2.1" class="ltx_text ltx_font_bold">mAP<sub id="S4.T2.2.2.2.1.1" class="ltx_sub">2</sub> (<math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mo stretchy="false" id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><ci id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T2.2.3.1.1.1" class="ltx_text ltx_font_bold">Cloth Type</span></th>
<th id="S4.T2.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">real-to-real</th>
<th id="S4.T2.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-real</th>
<th id="S4.T2.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(sim+real)-to-real</th>
<th id="S4.T2.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">real-to-real</th>
<th id="S4.T2.2.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-real</th>
<th id="S4.T2.2.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">(sim+real)-to-real</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.2.4.1" class="ltx_tr">
<th id="S4.T2.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">T-shirt</th>
<td id="S4.T2.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">54.4</td>
<td id="S4.T2.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S4.T2.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">69.1</td>
<td id="S4.T2.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">30.5</td>
<td id="S4.T2.2.4.1.6" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td id="S4.T2.2.4.1.7" class="ltx_td ltx_align_center ltx_border_t">51.9</td>
</tr>
<tr id="S4.T2.2.5.2" class="ltx_tr">
<th id="S4.T2.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Towel</th>
<td id="S4.T2.2.5.2.2" class="ltx_td ltx_align_center">74.4</td>
<td id="S4.T2.2.5.2.3" class="ltx_td ltx_align_center">83.2</td>
<td id="S4.T2.2.5.2.4" class="ltx_td ltx_align_center">88.6</td>
<td id="S4.T2.2.5.2.5" class="ltx_td ltx_align_center">65.9</td>
<td id="S4.T2.2.5.2.6" class="ltx_td ltx_align_center">77.7</td>
<td id="S4.T2.2.5.2.7" class="ltx_td ltx_align_center">83.5</td>
</tr>
<tr id="S4.T2.2.6.3" class="ltx_tr">
<th id="S4.T2.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Shorts</th>
<td id="S4.T2.2.6.3.2" class="ltx_td ltx_align_center">50.7</td>
<td id="S4.T2.2.6.3.3" class="ltx_td ltx_align_center">51.4</td>
<td id="S4.T2.2.6.3.4" class="ltx_td ltx_align_center">64.9</td>
<td id="S4.T2.2.6.3.5" class="ltx_td ltx_align_center">37.5</td>
<td id="S4.T2.2.6.3.6" class="ltx_td ltx_align_center">44.0</td>
<td id="S4.T2.2.6.3.7" class="ltx_td ltx_align_center">55.7</td>
</tr>
<tr id="S4.T2.2.7.4" class="ltx_tr">
<th id="S4.T2.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">All</th>
<td id="S4.T2.2.7.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">59.8</td>
<td id="S4.T2.2.7.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.3</td>
<td id="S4.T2.2.7.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">74.2</td>
<td id="S4.T2.2.7.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">44.6</td>
<td id="S4.T2.2.7.4.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.2</td>
<td id="S4.T2.2.7.4.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">63.7</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Scene Composition</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Using the deformed cloth meshes and the cloth materials described before, the procedure to generate a synthetic image and its annotations is explained below.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">We sample an environment texture from PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to create complex scene lighting. At the same time, we use this texture to create a scene background. Thereafter, a rectangular surface is added to the scene to mimic the folding surface. We sample a material from PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> for this plane and randomize its base color to increase diversity and avoid a bias towards certain colors. We then put a cloth mesh on top of the table and solidify the mesh to add some thickness to the layers. Afterwards, we sample a cloth material and apply it to the mesh.
To make the keypoint detectors more robust, we place a number of distractor objects on the surface. We sample the objects from the Google Scanned Objects dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Next, we randomize the camera position in a spherical cap around the cloth center and point the camera towards the cloth.
At this point we render a single image using Cycles, Blender’s Physically Based Renderer, and create a bounding box, segmentation mask and 2D keypoint annotations. For the keypoints, we use raycasting to determine if the vertex that corresponds to the keypoint is occluded. Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, we consider a keypoint as visible if any vertex in its N-ring neighborhood is visible. This corresponds better to the human labels on the aRTF dataset.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Dataset Generation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The 3-stage procedure explained in Sections <a href="#S4.SS2" title="IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>, <a href="#S4.SS2" title="IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a> and  <a href="#S4.SS3" title="IV-C Scene Composition ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a> is used to generate the desired amount of images for all cloth categories. Creating a mesh template is very fast. Deforming a mesh takes about 6 seconds. Rendering takes about 5 seconds for a 512x256 image on our workstation with an Nvidia RTX 3090, 32GB of RAM and an Intel i7 CPU. More details on the procedural data generation pipeline can be found in the accompanying <a target="_blank" href="https://github.com/tlpss/synthetic-cloth-data" title="" class="ltx_ref ltx_href">codebase</a>.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Keypoint Detection</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This section provides more details about the keypoint detectors. We describe the model used to detect keypoints and the average precision metrics we use to quantify the performance of the keypoint detectors. We also describe how we ordered the keypoints in image space to deal with the symmetries of cloth objects.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Model and Training</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We formulate keypoint detection as regression on 2D heatmaps as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. With this formulation, keypoint coordinates are converted into Gaussian blobs on a 2D heatmap, and these heatmaps are then used target for pixel-wise regression. The keypoint model uses a Unet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> inspired architecture. The encoder of the Unet is a pretrained MaxViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">nano</span> model, obtained from the timm library <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">As is common  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we employ a set of image augmentations during training: random cropping, random Gaussian blurring and noise, and random color, contrast and brightness augmentations. Note that some of these effects could be added to the synthetic data pipeline, but we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> by adding them as augmentations to increase flexibility.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">We train models on synthetic and/or the train split of the aRTF dataset (see Table <a href="#S3.T1" title="TABLE I ‣ III almost-ready-to-fold Clothes Dataset ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>). As the synthetic datasets tend to have an order of magnitude more data compared to our aRTF dataset, we use a different set of hyperparameters for each setting. These were obtained using a random hyperparameter search.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">We train each model until convergence and use a subset of the aRTF train split as validation set for this purpose. The performance of each model is tested on the test split of the aRTF dataset, using the checkpoint with the best validation performance.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Evaluation Metric</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this work, we want to enable folding. Hence, we only detect keypoints that are not occluded, as a robot can only interact with those parts of the cloth. This implies a varying number of detections for each keypoint category and hence we cannot directly use adistance-based metric such as the average pixel distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Instead, we use the Average Precision (AP) to quantify the performance of the keypoint detectors. To distinguish between false positive and false negative keypoint predictions we use the L2 pixel distance<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>COCO uses a non-linear transform of the L2 pixel distance: the object-similarity score (OKS). We directly use the L2 distance, since it relates more directly to real-world distances, which are typically the metric of interest for robotic manipulation.</span></span></span>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">As thresholds for the L2 distance, we use 2, 4 and 8 pixels. For the ZED 2i with an image resolution of 512x256 and given that the maximal distance between a cloth and the camera in the aRTF dataset is 1 meter (see Section <a href="#S3.T1" title="TABLE I ‣ III almost-ready-to-fold Clothes Dataset ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>), this corresponds to real-world discrepancies between real and predicted keypoints of at most 1, 2 and 4 cm. The thresholds were chosen as follows: 1cm and 2cm are relevant for grasping as they are the half-width of regular fingertips and of custom fingertips used in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. 4cm is relevant for tasks such as ironing and serves as upper bound to distinguish between lack of precision and lack of semantic knowledge for the keypoint detectors. Similar in spirit to the COCO metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, we report the mean average precision over all thresholds (mAP<sub id="S5.SS2.p2.1.1" class="ltx_sub">2,4,8</sub> ) and the AP on the most strict threshold (mAP<sub id="S5.SS2.p2.1.2" class="ltx_sub">2</sub> ).</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.5.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.6.2" class="ltx_text ltx_font_italic">Dealing with symmetry</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">Clothes have (apparent) symmetries that need to be handled carefully when detecting keypoints, as the model cannot be expected to distinguish between a motion of the camera versus a motion of the cloth object. Therefore we need to handle all symmetries in the Special Euclidean group SE(3) (which describes all rigid motions and hence all possible camera movements). One possible solution is to not distinguish between symmetric keypoints by labeling them identically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, but this results in some loss of information: we can no longer infer the connectivity of the keypoints, which is useful if you want to e.g. grasp the two corners of an edge. Other solutions include using a loss function that is invariant across all symmetries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, or selecting an arbitrary ordering in the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. We have taken the latter approach in this work and have adapted the labels of the synthetic data and the aRTF dataset as follows:</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">For towels, we have taken the keypoint closest to the top left corner of the bounding box to be the first and its physically adjacent corner that is closest to the top right corner of the bounding box as the second keypoint. The others follow from this convention.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">T-shirts or shorts have no symmetries in SE3 (which does not include reflections). However, detecting the front/back side can be very ambiguous, even on real data, and therefore we treat this as a (rotational) symmetry. For T-shirts we take as <span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_italic">left</span> side, the side of which the waist keypoint is closest to the bottom left corner of the T-shirt’s bounding box. For shorts, we use the top left corner of the bounding box.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we train several keypoint detectors on different data sources. We evaluate the performance of all models on the test split of the aRTF dataset (see Table <a href="#S3.T1" title="TABLE I ‣ III almost-ready-to-fold Clothes Dataset ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>) using the mean Average Precision (mAP) as metric. In Section <a href="#S6.SS1" title="VI-A Main Results ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a> we show that training on the synthetic data for three cloth categories results in better performance than training on the aRTF train split, and how fine-tuning on the real data improves performance significantly. The cloth mesh procedure and cloth materials described in Section <a href="#S4.F2" title="Figure 2 ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> are compared against other alternatives in Sections <a href="#S6.SS2" title="VI-B Comparing procedures for obtaining cloth meshes ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-B</span></span></a> and <a href="#S6.SS3" title="VI-C Comparing different cloth materials ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-C</span></span></a>.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Main Results</span>
</h3>

<figure id="S6.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2401.01734/assets/images/imperfect_example1_combined.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="592" height="74" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2401.01734/assets/images/imperfect_example2_combined.png" id="S6.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="592" height="74" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2401.01734/assets/images/imperfect_example4_combined.png" id="S6.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="592" height="74" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Illustration of failure modes of the detectors. From left to right, each row shows ground truth keypoints and the output of detectors trained on real data, synthetic data and on both. The first row illustrates how the real baseline produces incomplete and inconsistent keypoints, whereas the second row shows how the detectors still struggle with folds. In the third row, the detectors are confused by an open zipper and mistake it for a leg of the shorts.</figcaption>
</figure>
<figure id="S6.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>reality gap of the synthetic data as measured by the difference between sim-to-sim and sim-to-real performance.</figcaption>
<table id="S6.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T3.2.2" class="ltx_tr">
<th id="S6.T3.2.2.3" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S6.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S6.T3.1.1.1.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T3.1.1.1.1.1" class="ltx_sub">2,4,8</sub> (<math id="S6.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T3.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T3.1.1.1.1.m1.1.1" xref="S6.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.1.1.1.1.m1.1b"><ci id="S6.T3.1.1.1.1.m1.1.1.cmml" xref="S6.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
<th id="S6.T3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S6.T3.2.2.2.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T3.2.2.2.1.1" class="ltx_sub">2</sub> (<math id="S6.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T3.2.2.2.1.m1.1a"><mo stretchy="false" id="S6.T3.2.2.2.1.m1.1.1" xref="S6.T3.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T3.2.2.2.1.m1.1b"><ci id="S6.T3.2.2.2.1.m1.1.1.cmml" xref="S6.T3.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T3.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
<tr id="S6.T3.2.3.1" class="ltx_tr">
<th id="S6.T3.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S6.T3.2.3.1.1.1" class="ltx_text ltx_font_bold">cloth type</span></th>
<th id="S6.T3.2.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-real</th>
<th id="S6.T3.2.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-sim</th>
<th id="S6.T3.2.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-real</th>
<th id="S6.T3.2.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">sim-to-sim</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T3.2.4.1" class="ltx_tr">
<th id="S6.T3.2.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">T-shirt</th>
<td id="S6.T3.2.4.1.2" class="ltx_td ltx_align_center ltx_border_t">58.2</td>
<td id="S6.T3.2.4.1.3" class="ltx_td ltx_align_center ltx_border_t">81.8</td>
<td id="S6.T3.2.4.1.4" class="ltx_td ltx_align_center ltx_border_t">43.8</td>
<td id="S6.T3.2.4.1.5" class="ltx_td ltx_align_center ltx_border_t">69.5</td>
</tr>
<tr id="S6.T3.2.5.2" class="ltx_tr">
<th id="S6.T3.2.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Towel</th>
<td id="S6.T3.2.5.2.2" class="ltx_td ltx_align_center">83.2</td>
<td id="S6.T3.2.5.2.3" class="ltx_td ltx_align_center">84.7</td>
<td id="S6.T3.2.5.2.4" class="ltx_td ltx_align_center">77.7</td>
<td id="S6.T3.2.5.2.5" class="ltx_td ltx_align_center">78.5</td>
</tr>
<tr id="S6.T3.2.6.3" class="ltx_tr">
<th id="S6.T3.2.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Shorts</th>
<td id="S6.T3.2.6.3.2" class="ltx_td ltx_align_center">51.4</td>
<td id="S6.T3.2.6.3.3" class="ltx_td ltx_align_center">79.8</td>
<td id="S6.T3.2.6.3.4" class="ltx_td ltx_align_center">44.0</td>
<td id="S6.T3.2.6.3.5" class="ltx_td ltx_align_center">71.4</td>
</tr>
<tr id="S6.T3.2.7.4" class="ltx_tr">
<th id="S6.T3.2.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">All</th>
<td id="S6.T3.2.7.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">64.3</td>
<td id="S6.T3.2.7.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">82.1</td>
<td id="S6.T3.2.7.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">55.2</td>
<td id="S6.T3.2.7.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">73.1</td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We have trained keypoint detectors for both T-shirts, shorts and towels. For each cloth type, we trained three models using different data sources: the corresponding train split of the aRTF dataset (which we use as a baseline), a dataset of synthetic images and a combination of both by fine-tuning on the aRTF data after training on the synthetic data. We use 10,000 synthetic images per category as we have observed that surpassing this amount does not lead to a significant improvement.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.4" class="ltx_p">The performance of all models, as measured on the aRTF test splits, is given in Table <a href="#S4.T2" title="TABLE II ‣ IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. Over all cloth types, the baseline model that is only trained on real data obtains an mAP<sub id="S6.SS1.p2.4.1" class="ltx_sub">2,4,8</sub> of <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="59.8" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">59.8</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><cn type="float" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">59.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">59.8</annotation></semantics></math> and an mAP<sub id="S6.SS1.p2.4.2" class="ltx_sub">2</sub> of <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="44.6" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><mn id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml">44.6</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><cn type="float" id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">44.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">44.6</annotation></semantics></math>. Training on synthetic data outperforms this baseline by 4 and 9 percentage points (<span id="S6.SS1.p2.4.3" class="ltx_text ltx_font_italic">pp</span> ) respectively. Fine-tuning on the aRTF data results in an mAP<sub id="S6.SS1.p2.4.4" class="ltx_sub">2,4,8</sub> of <math id="S6.SS1.p2.3.m3.1" class="ltx_Math" alttext="74.2" display="inline"><semantics id="S6.SS1.p2.3.m3.1a"><mn id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">74.2</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><cn type="float" id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">74.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">74.2</annotation></semantics></math> and an mAP<sub id="S6.SS1.p2.4.5" class="ltx_sub">2</sub> of <math id="S6.SS1.p2.4.m4.1" class="ltx_Math" alttext="63.7" display="inline"><semantics id="S6.SS1.p2.4.m4.1a"><mn id="S6.SS1.p2.4.m4.1.1" xref="S6.SS1.p2.4.m4.1.1.cmml">63.7</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m4.1b"><cn type="float" id="S6.SS1.p2.4.m4.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1">63.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m4.1c">63.7</annotation></semantics></math>, which is an improvement of 10 <span id="S6.SS1.p2.4.6" class="ltx_text ltx_font_italic">pp</span> compared to training on synthetic data only. The performance on T-shirts and shorts is rather similar, whereas the results for towels are respectively 10 and 20 <span id="S6.SS1.p2.4.7" class="ltx_text ltx_font_italic">pp</span> above average for both metrics.
A few examples of the output of the models trained on a combination of the aRTF train split and synthetic data can be found in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Using these keypoints, folding policies can be scripted <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">The above results clearly show the added value of the synthetic data. At the same time, the results in Table <a href="#S4.T2" title="TABLE II ‣ IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> also indicate that the keypoints are still not always detected correctly, as this would result in an mAP close to 100%. To provide more insight into the failures of the detectors, we show predictions of the models trained on each data source on a number of samples from the aRTF test split in Fig. <a href="#S6.F3" title="Figure 3 ‣ VI-A Main Results ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Each example illustrates a common failure mode: The first row illustrates how training only on the aRTF train split results in models that confuse semantic locations and return many false positives, which is also visible in the second example. The keypoint detectors still struggle with folds, as can be seen in the second example. Despite many efforts to tailor the distribution of mesh deformations in the synthetic data (see Section <a href="#S4.SS2" title="IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>), we have not managed to completely solve this problem. The final example in Fig. <a href="#S6.F3" title="Figure 3 ‣ VI-A Main Results ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates how all models have difficulties with a particular configuration of a short in which the opened zipper is confused for a leg. Similar confusions occur for T-shirts as well.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">The lack of further improvement when adding more synthetic data and the performance improvement when fine-tuning on the aRTF data already suggest that there is a reality gap for the synthetic data. To further quantify this gap, we have evaluated both the sim-to-sim performance and the sim-to-real performance of the models trained on synthetic data. These results can be found in table <a href="#S6.T3" title="TABLE III ‣ VI-A Main Results ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Across all categories, the gap is about 20<span id="S6.SS1.p4.1.1" class="ltx_text ltx_font_italic">pp</span> , which confirms there is a substantial reality gap. Interestingly, for towels the sim and real performance are very close, suggesting that the gap is smaller for this category. This also corresponds with the results from Table <a href="#S4.T2" title="TABLE II ‣ IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, where the performance increase associated with fine-tuning on the aRTF data was lower than for the other categories.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.5.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.6.2" class="ltx_text ltx_font_italic">Comparing procedures for obtaining cloth meshes</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In this experiment, we motivate the use of single-layer meshes for synthetic data generation by comparing three different procedures to generate the cloth meshes:</p>
<ol id="S6.I1" class="ltx_enumerate">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">Inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> we have taken 5 meshes from the Cloth3D dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. We have triangulated the meshes and cleaned up their UV maps. Using Nvidia Flex, we generated a flattened version of these meshes, by dropping them from a certain height. We then manually labeled all keypoints by selecting the corresponding vertices on the mesh. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we have created 10 variants of each mesh by rescaling it, leading to a total of 50 different meshes. Using these meshes, we generate deformed configurations with the procedure discussed in Section <a href="#S4.SS2" title="IV-B Deforming the Meshes ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-B</span></span></a>.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">We procedurally generate single-layer meshes using a 2D template as described in Section <a href="#S4.SS1.SSS1" title="IV-A1 Cloth meshes ‣ IV-A Obtaining Cloth Assets ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span>1</span></a>. We then create deformations using the same procedure as above. As these meshes and their annotations are generated procedurally, we can generate arbitrary amounts of meshes, unlike for the cloth3d procedure.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">We use the same 2D templates as above but do not deform them, which results in perfectly flat meshes, similarly to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</li>
</ol>
<p id="S6.SS2.p1.2" class="ltx_p">An example of each mesh procedure can be found in Fig. <a href="#S6.F4" title="Figure 4 ‣ VI-B Comparing procedures for obtaining cloth meshes ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
For each procedure, we generate a dataset of 5,000 T-shirt meshes which we then use to generate 5,000 synthetic images using the pipeline described in Section <a href="#S4.SS3" title="IV-C Scene Composition ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>. The performance on the aRTF test split of the keypoint detectors trained on these synthetic datasets can be found in table <a href="#S6.T4" title="TABLE IV ‣ VI-B Comparing procedures for obtaining cloth meshes ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>. Surprisingly, the more realistic meshes from the Cloth3D dataset perform worse than the less realistic but more diverse single-layer meshes. The undeformed single-layer meshes also perform surprisingly well. Based on these results, we use the single-layer mesh procedure in this paper.</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/2401.01734/assets/images/sim-mesh-comparison.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of the different cloth mesh procedures that were considered. Left to right: undeformed single-layer mesh, cloth3d mesh, single-layer mesh. Though less realistic, using the single-layer meshes results in the best performance.</figcaption>
</figure>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Comparison of different procedures to obtain cloth meshes. The single-layer procedure results in the best performance.</figcaption>
<table id="S6.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.2.2" class="ltx_tr">
<th id="S6.T4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S6.T4.2.2.3.1" class="ltx_text ltx_font_bold">Cloth Mesh Procedure</span></th>
<th id="S6.T4.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T4.1.1.1.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T4.1.1.1.1.1" class="ltx_sub">2,4,8</sub> (<math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><ci id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
<th id="S6.T4.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T4.2.2.2.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T4.2.2.2.1.1" class="ltx_sub">2</sub> (<math id="S6.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T4.2.2.2.1.m1.1a"><mo stretchy="false" id="S6.T4.2.2.2.1.m1.1.1" xref="S6.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.1.m1.1b"><ci id="S6.T4.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.2.3.1" class="ltx_tr">
<th id="S6.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Cloth3D subset</th>
<td id="S6.T4.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">43.4</td>
<td id="S6.T4.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">29.1</td>
</tr>
<tr id="S6.T4.2.4.2" class="ltx_tr">
<th id="S6.T4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">single-layer</th>
<td id="S6.T4.2.4.2.2" class="ltx_td ltx_align_center"><span id="S6.T4.2.4.2.2.1" class="ltx_text ltx_font_bold">54.3</span></td>
<td id="S6.T4.2.4.2.3" class="ltx_td ltx_align_center">38.7</td>
</tr>
<tr id="S6.T4.2.5.3" class="ltx_tr">
<th id="S6.T4.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">single-layer undeformed</th>
<td id="S6.T4.2.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">53.8</td>
<td id="S6.T4.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S6.T4.2.5.3.3.1" class="ltx_text ltx_font_bold">39.0</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.5.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.6.2" class="ltx_text ltx_font_italic">Comparing different cloth materials</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In this experiment, we compare several different cloth material configurations, to motivate the use of random materials in Section <a href="#S4.F2" title="Figure 2 ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>:</p>
<ol id="S6.I2" class="ltx_enumerate">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.1" class="ltx_p">For the first configuration, we simply add random materials from PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> to the cloth and mix the base color map with a random color to create more diversity.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.1" class="ltx_p">In the second configuration, we apply a uniform color to the clothes as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p id="S6.I2.i3.p1.1" class="ltx_p">We also compare against a more tailored procedural material: next to uniform colors, we also add striped color maps. We also add random images to the clothes to mimic logos and prints. Furthermore, we procedurally generate a normal map that mimics fabric materials and generates additional wrinkles, similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</li>
</ol>
<p id="S6.SS3.p1.2" class="ltx_p">An example of each material can be found in Fig. <a href="#S6.F5" title="Figure 5 ‣ VI-C Comparing different cloth materials ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We have generated datasets of 5,000 images for these materials, using the procedure from Section <a href="#S4.SS3" title="IV-C Scene Composition ‣ IV Procedural Data Generation Pipeline ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-C</span></span></a>. The performance of the keypoint detectors trained on these datasets can be found in table <a href="#S6.T5" title="TABLE V ‣ VI-C Comparing different cloth materials ‣ VI Experiments ‣ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>. We observe that uniform color materials result in the lowest performance. This is not surprising as the aRTF dataset contains many non-uniform colored cloth items. It is also consistent with findings in our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, where we also observed a decreased performance for non-uniform cloth pieces. Perhaps more surprisingly, the random materials perform better than the tailored materials even though the latter look more plausible.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/2401.01734/assets/images/sim-materials.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="592" height="162" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of the cloth different materials that were considered. From left to right: uniform colors, a cloth-tailored procedural material and random PolyHaven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> textures. Though random materials are less plausible, they result in the best performance.</figcaption>
</figure>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Comparison of different Cloth materials. Adding random materials to the cloth meshes leads to the highest performance on the aRTF test split.</figcaption>
<table id="S6.T5.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.2.2" class="ltx_tr">
<th id="S6.T5.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S6.T5.2.2.3.1" class="ltx_text ltx_font_bold">Cloth Materials</span></th>
<th id="S6.T5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.1.1.1.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T5.1.1.1.1.1" class="ltx_sub">2,4,8</sub> (<math id="S6.T5.1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T5.1.1.1.1.m1.1a"><mo stretchy="false" id="S6.T5.1.1.1.1.m1.1.1" xref="S6.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
<th id="S6.T5.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S6.T5.2.2.2.1" class="ltx_text ltx_font_bold">mAP<sub id="S6.T5.2.2.2.1.1" class="ltx_sub">2</sub> (<math id="S6.T5.2.2.2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S6.T5.2.2.2.1.m1.1a"><mo stretchy="false" id="S6.T5.2.2.2.1.m1.1.1" xref="S6.T5.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S6.T5.2.2.2.1.m1.1b"><ci id="S6.T5.2.2.2.1.m1.1.1.cmml" xref="S6.T5.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.2.2.2.1.m1.1c">\uparrow</annotation></semantics></math>)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.2.3.1" class="ltx_tr">
<th id="S6.T5.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">PolyHaven random</th>
<td id="S6.T5.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.2.3.1.2.1" class="ltx_text ltx_font_bold">54.3</span></td>
<td id="S6.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S6.T5.2.3.1.3.1" class="ltx_text ltx_font_bold">38.7</span></td>
</tr>
<tr id="S6.T5.2.4.2" class="ltx_tr">
<th id="S6.T5.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">cloth-tailored</th>
<td id="S6.T5.2.4.2.2" class="ltx_td ltx_align_center">49.0</td>
<td id="S6.T5.2.4.2.3" class="ltx_td ltx_align_center">36.5</td>
</tr>
<tr id="S6.T5.2.5.3" class="ltx_tr">
<th id="S6.T5.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">uniform color</th>
<td id="S6.T5.2.5.3.2" class="ltx_td ltx_align_center ltx_border_bb">35.1</td>
<td id="S6.T5.2.5.3.3" class="ltx_td ltx_align_center ltx_border_bb">25.5</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We have created a synthetic data pipeline for clothes and used it to generate images and corresponding keypoint annotations of cloth items that are <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">almost</span> flattened. The improved performance when using this synthetic data to train keypoint detectors clearly shows its benefits.
At the same time, we stress that the amount of real-world data of the baseline is limited. Scaling up real-world data collection would improve the results of the baseline. However, due to the huge diversity in cloth meshes, their configurations, cloth materials and environments, we argue that synthetic data will still be needed to obtain models that can generalize to all these aspects.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p">We have found that performance when training on the synthetic data increased as we traded diversity for fidelity for both cloth meshes and materials. However, we also determined that there is a reality gap that limits the sim-to-real performance. We argue that the current lack of realism in favor of diversity is a local optimum, as we do not expect more randomizations to further improve performance. Further increasing the realism of the data generation pipeline will allow to escape from the observed performance limit.</p>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Improving performance will hence require more accurate cloth meshes that contain fine-grained elements such as seams, zippers, pockets and labels, which are all important cues to determine the semantic locations. To apply materials realistically, the meshes also need to contain accurate UV maps. Furthermore, the meshes also need to be automatically annotated to reduce engineering effort. Leveraging recent advances in generative text-to-3D models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> could provide an exciting alternative to manually engineering all the above.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.1" class="ltx_p">Not only do the assets need to be improved, particle-based simulators such as Nvidia Flex <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> have fundamental limitations. Next to the limited realism of the cloth-physics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, we have also found the inherent intertwining of mesh resolution with cloth thickness and deformation granularity hindering. Simulators such as C-IPC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> provide improved realism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, though at the cost of additional computational complexity.</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Using generative models for end-to-end data augmentation as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> is an alternative to manual engineering of 3D assets and physics simulators, though we have found in some initial experiments that keeping the labels consistent when augmenting real data is a challenge.</p>
</div>
<div id="S7.p6" class="ltx_para">
<p id="S7.p6.1" class="ltx_p">Next to the sim-to-real gap, we have observed that the keypoint detectors still struggle with folds and severe deformations. This results in predictions that are between themselves not consistent and false positives on the apparent cloth edges. It is very well possible that these issues can be resolved with improved data generation, but we think that the models would benefit from more geometric reasoning capabilities. On the other hand, we are not convinced that forcing a model to predict keypoints from a static image of heavily deformed clothes lying on a surface is the best way forward: this might be an artificially hard task. Using more interactive perception as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> would allow the system to collect more information. Improvements to the generalization of unfolding systems could also reduce the need to handle deformed pieces in the first place.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this work, we have described a pipeline to generate synthetic images of clothes to advance the generalization of robotic cloth manipulation. By training a keypoint detector on images generated by this pipeline we have validated its usefulness. The resulting models and the aRTF dataset can be used to enable various tasks such as folding or ironing. Furthermore, the pipeline can be easily adapted to train models for other tasks or cloth types.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">We have found that models trained on our data generation pipeline, which prioritizes diversity over fidelity, have a decreased performance when transferred to the real world. We argue that future work should focus on increasing fidelity to bridge this reality gap.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">A limitation of our work is that we assume the cloth type is already known in advance, which cannot be taken for granted in real-world scenarios. We believe our data generation pipeline can be used for learning to classify clothes as well, but consider this to be out of scope.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors wish to thank Maxim Bonnaerens and Peter De Roovere for their valuable feedback. This project is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG) and by the euROBIn Project (EU grant number 101070596).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. Jiménez, “Visual grasp point localization, classification and state
recognition in robotic manipulation of cloth: An overview,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Robotics
and Autonomous Systems</em>, vol. 92, pp. 107–125, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
H. Yin, A. Varava, and D. Kragic, “Modeling, learning, perception, and control
methods for deformable object manipulation,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Science Robotics</em>,
vol. 6, no. 54, p. eabd8803, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Ha and S. Song, “Flingbot: The unreasonable effectiveness of dynamic
manipulation for cloth unfolding,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Conference on Robot
Learning</em>.   PMLR, 2022, pp. 24–33.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Avigal, L. Berscheid, T. Asfour, T. Kröger, and K. Goldberg,
“Speedfolding: Learning efficient bimanual folding of garments,” in
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS)</em>.   IEEE, 2022, pp. 1–8.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Canberk, C. Chi, H. Ha, B. Burchfiel, E. Cousineau, S. Feng, and S. Song,
“Cloth funnels: Canonicalized-alignment for multi-purpose garment
manipulation,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Robotics and
Automation (ICRA)</em>.   IEEE, 2023, pp.
5872–5879.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
R. Proesmans, A. Verleysen, and F. wyffels, “Unfoldir: Tactile robotic
unfolding of cloth,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference on intelligent
robots and systems (IROS)</em>.   IEEE,
2017, pp. 23–30.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birchfield,
“Deep object pose estimation for semantic robotic grasping of household
objects,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>.   PMLR, 2018, pp. 306–316.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
N. Rudin, D. Hoeller, P. Reist, and M. Hutter, “Learning to walk in minutes
using massively parallel deep reinforcement learning,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Conference
on Robot Learning</em>.   PMLR, 2022, pp.
91–100.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
E. Wood, T. Baltrušaitis, C. Hewitt, S. Dziadzio, T. J. Cashman, and
J. Shotton, “Fake it till you make it: face analysis in the wild using
synthetic data alone,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF international
conference on computer vision</em>, 2021, pp. 3681–3691.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2223–2232.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Doumanoglou, J. Stria, G. Peleka, I. Mariolis, V. Petrik, A. Kargakos,
L. Wagner, V. Hlaváč, T.-K. Kim, and S. Malassiotis, “Folding
clothes autonomously: A complete pipeline,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Robotics</em>, vol. 32, no. 6, pp. 1461–1478, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
V.-L. De Gusseme and F. wyffels, “Effective cloth folding trajectories in
simulation with only two parameters,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Frontiers in Neurorobotics</em>,
vol. 16, p. 989702, 2022.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. Corona, G. Alenya, A. Gabas, and C. Torras, “Active garment recognition and
target grasping point detection using deep learning,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Pattern
Recognition</em>, vol. 74, pp. 629–641, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
D. Seita, A. Ganapathi, R. Hoque, M. Hwang, E. Cen, A. K. Tanwani,
A. Balakrishna, B. Thananjeyan, J. Ichnowski, N. Jamali, <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Deep imitation learning of sequential fabric smoothing from an algorithmic
supervisor,” in <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference on Intelligent
Robots and Systems (IROS)</em>.   IEEE,
2020, pp. 9651–9658.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
“Nvidia flex,”
<a target="_blank" href="https://docs.nvidia.com/gameworks/content/gameworkslibrary/physx/flex/index.html" title="" class="ltx_ref ltx_url">https://docs.nvidia.com/gameworks/content/gameworkslibrary/physx/flex/index.html</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Zhu, A. Cherubini, C. Dune, D. Navarro-Alarcon, F. Alambeigi, D. Berenson,
F. Ficuciello, K. Harada, J. Kober, X. Li, <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Challenges and
outlook in robotic manipulation of deformable objects,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">IEEE Robotics
&amp; Automation Magazine</em>, vol. 29, no. 3, pp. 67–77, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Qian, T. Weng, L. Zhang, B. Okorn, and D. Held, “Cloth region segmentation
for robust grasp selection,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/RSJ International Conference
on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2020, pp. 9553–9560.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
T. Lips and F. De Gusseme, Victor-Louis wyffels, “Learning keypoints from
synthetic data for robotic cloth folding,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">RMDO Workshop ICRA</em>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Seita, N. Jamali, M. Laskey, A. K. Tanwani, R. Berenstein, P. Baskaran,
S. Iba, J. Canny, and K. Goldberg, “Deep transfer learning of pick points on
fabric for robot bed-making,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">The International Symposium of
Robotics Research</em>.   Springer, 2019,
pp. 275–290.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
J. Matas, S. James, and A. J. Davison, “Sim-to-real reinforcement learning for
deformable object manipulation,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Conference on Robot
Learning</em>.   PMLR, 2018, pp. 734–743.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. Coumans and Y. Bai, “Pybullet, a python module for physics simulation for
games, robotics and machine learning,” <a target="_blank" href="http://pybullet.org" title="" class="ltx_ref ltx_url">http://pybullet.org</a>,
2016–2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Blender Online Community, “Blender - a 3d modelling and rendering package,”
http://www.blender.org, Blender Foundation.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
A. Ganapathi, P. Sundaresan, B. Thananjeyan, A. Balakrishna, D. Seita,
J. Grannen, M. Hwang, R. Hoque, J. E. Gonzalez, N. Jamali, <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>,
“Learning dense visual correspondences in simulation to smooth and fold real
fabrics,” in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Robotics and
Automation (ICRA)</em>.   IEEE, 2021, pp.
11 515–11 522.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
H. Bertiche, M. Madadi, and S. Escalera, “Cloth3d: clothed 3d humans,” in
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2020, pp. 344–359.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Verleysen, M. Biondina, and F. wyffels, “Video dataset of human
demonstrations of folding clothing for robotic folding,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">The
International Journal of Robotics Research</em>, vol. 39, no. 9, pp. 1031–1036,
2020.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
T. Ziegler, J. Butepage, M. C. Welle, A. Varava, T. Novkovic, and D. Kragic,
“Fashion landmark detection and category classification for robotics,” in
<em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Autonomous Robot Systems and
Competitions (ICARSC)</em>.   IEEE, 2020,
pp. 81–88.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
I. Garcia-Camacho, J. Borràs, B. Calli, A. Norton, and G. Alenyà,
“Household cloth object set: Fostering benchmarking in deformable object
manipulation,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 7, no. 3,
pp. 5866–5873, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Miller, M. Fritz, T. Darrell, and P. Abbeel, “Parametrized shape models for
clothing,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2011 IEEE International Conference on Robotics and
Automation</em>.   IEEE, 2011, pp.
4861–4868.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
S. Whitehead, A. C. Berg, W.-Y. Lo, <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Segment anything,”
<em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02643</em>, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
PolyHaven Team, “Polyhaven: The public 3d assets library,”
<a target="_blank" href="https://polyhaven.com/" title="" class="ltx_ref ltx_url">https://polyhaven.com/</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X. Lin, Y. Wang, J. Olkin, and D. Held, “Softgym: Benchmarking deep
reinforcement learning for deformable object manipulation,” in
<em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>.   PMLR, 2021, pp. 432–448.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B.
McHugh, and V. Vanhoucke, “Google scanned objects: A high-quality dataset of
3d scanned household items,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2022 International Conference on
Robotics and Automation (ICRA)</em>.   IEEE,
2022, pp. 2553–2560.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
T. Jakab, A. Gupta, H. Bilen, and A. Vedaldi, “Unsupervised learning of object
landmarks through conditional image generation,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 31, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and
Computer-Assisted Intervention–MICCAI 2015: 18th International Conference,
Munich, Germany, October 5-9, 2015, Proceedings, Part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Z. Tu, H. Talebi, H. Zhang, F. Yang, P. Milanfar, A. Bovik, and Y. Li,
“Maxvit: Multi-axis vision transformer,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">European conference on
computer vision</em>.   Springer, 2022, pp.
459–479.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
R. Wightman, “Pytorch image models,”
<a target="_blank" href="https://github.com/rwightman/pytorch-image-models" title="" class="ltx_ref ltx_url">https://github.com/rwightman/pytorch-image-models</a>, 2019.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “Dreamfusion: Text-to-3d
using 2d diffusion,” <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2209.14988</em>, 2022.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
D. Blanco-Mulero, O. Barbany, G. Alcan, A. Colomé, C. Torras, and V. Kyrki,
“Benchmarking the sim-to-real gap in cloth manipulation,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:2310.09543</em>, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Li, D. M. Kaufman, and C. Jiang, “Codimensional incremental potential
contact,” <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Graph.</em>, vol. 40, no. 4, jul 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan,
J. Peralta, B. Ichter, <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Scaling robot learning with
semantically imagined experience,” <em id="bib.bib42.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.11550</em>,
2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2401.01733" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2401.01734" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2401.01734">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2401.01734" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2401.01735" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 10:37:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
