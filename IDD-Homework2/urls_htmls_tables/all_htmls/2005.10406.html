<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2005.10406] Training Keyword Spotting Models on Non-IID Data with Federated Learning</title><meta property="og:description" content="We demonstrate that a production-quality keyword-spotting model can be trained
on-device using federated learning and achieve comparable false accept and
false reject rates to a centrally-trained model. To overcome the…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Training Keyword Spotting Models on Non-IID Data with Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Training Keyword Spotting Models on Non-IID Data with Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2005.10406">

<!--Generated on Thu Mar 14 14:17:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:90%;">Training Keyword Spotting Models on Non-IID Data with Federated Learning</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text" style="font-size:90%;">We demonstrate that a production-quality keyword-spotting model can be trained
on-device using federated learning and achieve comparable false accept and
false reject rates to a centrally-trained model. To overcome the algorithmic
constraints associated with fitting on-device data (which are inherently
non-independent and identically distributed), we conduct thorough empirical
studies of optimization algorithms and hyperparameter configurations using
large-scale federated simulations. To overcome resource constraints, we
replace memory-intensive MTR data augmentation with SpecAugment, which reduces
the false reject rate by 56%. Finally, to label examples (given the zero
visibility into on-device data), we explore teacher-student training.</span></p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Index Terms</span><span id="p1.1.2" class="ltx_text" style="font-size:90%;">: federated learning, on-device learning, keyword
spotting, wake word detection, non-iid data, data augmentation</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p"><span id="S1.p1.1.1" class="ltx_text" style="font-size:90%;">Keyword spotting has become an essential access point for virtual assistants.
Vocalized keywords such as </span><span id="S1.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Alexa</span><span id="S1.p1.1.3" class="ltx_text" style="font-size:90%;">, </span><span id="S1.p1.1.4" class="ltx_text ltx_font_italic" style="font-size:90%;">Hey Google</span><span id="S1.p1.1.5" class="ltx_text" style="font-size:90%;">, or
</span><span id="S1.p1.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">Hey Siri</span><span id="S1.p1.1.7" class="ltx_text" style="font-size:90%;"> can be used to initiate search queries and issue commands to
mobile phones and smart speakers. The underlying algorithms must process
streaming audio—the majority of which must be ignored—and trigger quickly
and reliably when needed.</span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p"><span id="S1.p2.1.1" class="ltx_text" style="font-size:90%;">Neural networks have achieved state-of-the-art performance in automatic speech
recognition tasks </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="S1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.4" class="ltx_text" style="font-size:90%;">. Applications of neural
networks to keyword spotting have also been explored, particularly within the
contexts of quality improvement and latency reduction for low-resource
environments </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a><span id="S1.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.7" class="ltx_text" style="font-size:90%;"> and end-to-end model training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p2.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S1.p2.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p2.1.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p"><span id="S1.p3.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Federated Learning</span><span id="S1.p3.1.2" class="ltx_text" style="font-size:90%;"> (FL) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S1.p3.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.5" class="ltx_text" style="font-size:90%;"> is a decentralized
computation paradigm that can be used to train neural networks directly
on-device. In FL, all model updates shared by devices with the server are
</span><span id="S1.p3.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">ephemeral</span><span id="S1.p3.1.7" class="ltx_text" style="font-size:90%;"> (only stored temporarily in server memory), </span><span id="S1.p3.1.8" class="ltx_text ltx_font_italic" style="font-size:90%;">focused</span><span id="S1.p3.1.9" class="ltx_text" style="font-size:90%;">
(only relevant to a specific training task), and </span><span id="S1.p3.1.10" class="ltx_text ltx_font_italic" style="font-size:90%;">aggregated</span><span id="S1.p3.1.11" class="ltx_text" style="font-size:90%;"> (only
processed collectively with updates from other devices across the population).
In conjunction with techniques such as differential privacy </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.12.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S1.p3.1.13.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.14" class="ltx_text" style="font-size:90%;"> and
secure aggregation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p3.1.15.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="S1.p3.1.16.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p3.1.17" class="ltx_text" style="font-size:90%;">, FL can integrate strong anonymity and privacy
guarantees into the neural network training process.</span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text" style="font-size:90%;">Federated learning provides a path to train keyword models at the edge, on real
user data, as opposed to proxy data. In contrast, centrally-trained
keyword-spotting models use proxy data, since false accepts (in which the
keyword-spotter accidentally triggers) are not logged.</span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text" style="font-size:90%;">Multiple production models have been trained with federated learning, including
next-word prediction </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="S1.p5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.4" class="ltx_text" style="font-size:90%;">, emoji prediction </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S1.p5.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.7" class="ltx_text" style="font-size:90%;">, n-gram
language models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="S1.p5.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.10" class="ltx_text" style="font-size:90%;">, and query suggestions </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="S1.p5.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.13" class="ltx_text" style="font-size:90%;"> for mobile
keyboards. Many of these models achieve better performance as a result of the
additional signals and unbiased data available on-device. Recently, the
feasibility of training keyword-spotting algorithms with FL has been explored
with smaller datasets </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p5.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S1.p5.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p5.1.16" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text" style="font-size:90%;">On-device training comes with challenges, including the fact that the quantities
and characteristics of training examples vary considerably from device to
device. Centrally-trained models benefit from the ability to sample data in a
controlled, independent and identically distributed (IID) manner, resulting in
gradient updates that are unbiased estimates of the total gradient for the
dataset. This is not true on-device, where client updates are biased
representations of the gradient across the entire population </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p6.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S1.p6.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p6.1.4" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text" style="font-size:90%;">Non-IID data adversely affect convergence </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S1.p7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.4" class="ltx_text" style="font-size:90%;">, and have been
identified as a fundamental challenge to FL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="S1.p7.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.7" class="ltx_text" style="font-size:90%;">. Proposals to fit
non-IID data better include optimizers that account for client
drift </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="S1.p7.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.10" class="ltx_text" style="font-size:90%;">, data sharing between client devices </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S1.p7.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.13" class="ltx_text" style="font-size:90%;">,
and adaptive server optimizers with client learning rate
decay </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.14.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S1.p7.1.15.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.16" class="ltx_text" style="font-size:90%;">, among others </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.p7.1.17.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S1.p7.1.18.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S1.p7.1.19" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text" style="font-size:90%;">The primary contribution of this paper is to demonstrate that keyword-spotting
models can be trained on large-scale datasets using FL, and can achieve false
accept and false reject rates that rival those of centralized training. Using
simulated federated learning experiments on large-scale datasets consisting
of thousands of speakers and millions of utterances, we address the algorithmic
challenges associated with training on non-IID data, the visibility challenges
associated with labeling on-device data, and the physical constraints that limit
augmentation capabilities on-device.</span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Model</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text" style="font-size:90%;">This paper used the end-to-end architecture described in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.4" class="ltx_text" style="font-size:90%;">.
End-to-end trainable neural architectures have demonstrated state-of-the-art
performance in terms of accuracy as well as lowered resource requirements while
providing a highly optimizable system design </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="S2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.7" class="ltx_text" style="font-size:90%;">. The model consisted
of an encoder-decoder architecture in which both the encoder and decoder made
use of efficiently parameterized SVDF (single value decomposition filter)
layers—originally introduced in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="S2.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p1.1.10" class="ltx_text" style="font-size:90%;">—to approximate fully-connected
layers with low rank approximations. Dense bottleneck layers were used to
further reduce computational costs and keep the model size down at only 320,778
parameters (Figure </span><a href="#S2.F1" title="Figure 1 ‣ 2 Model ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S2.p1.1.11" class="ltx_text" style="font-size:90%;">).</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.3" class="ltx_p"><span id="S2.p2.3.1" class="ltx_text" style="font-size:90%;">The encoder took spectral domain features </span><math id="S2.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{X}_{t}" display="inline"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">𝐗</mi><mi mathsize="90%" id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">𝐗</ci><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\mathbf{X}_{t}</annotation></semantics></math><span id="S2.p2.3.2" class="ltx_text" style="font-size:90%;"> as input and generated
outputs </span><math id="S2.p2.2.m2.1" class="ltx_Math" alttext="Y^{E}" display="inline"><semantics id="S2.p2.2.m2.1a"><msup id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">Y</mi><mi mathsize="90%" id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">E</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">superscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">𝑌</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">Y^{E}</annotation></semantics></math><span id="S2.p2.3.3" class="ltx_text" style="font-size:90%;"> corresponding to phoneme-like sound units. The decoder
model used the encoder output as input and generated binary output </span><math id="S2.p2.3.m3.1" class="ltx_Math" alttext="Y^{D}" display="inline"><semantics id="S2.p2.3.m3.1a"><msup id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S2.p2.3.m3.1.1.2" xref="S2.p2.3.m3.1.1.2.cmml">Y</mi><mi mathsize="90%" id="S2.p2.3.m3.1.1.3" xref="S2.p2.3.m3.1.1.3.cmml">D</mi></msup><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><apply id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.p2.3.m3.1.1.1.cmml" xref="S2.p2.3.m3.1.1">superscript</csymbol><ci id="S2.p2.3.m3.1.1.2.cmml" xref="S2.p2.3.m3.1.1.2">𝑌</ci><ci id="S2.p2.3.m3.1.1.3.cmml" xref="S2.p2.3.m3.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">Y^{D}</annotation></semantics></math><span id="S2.p2.3.4" class="ltx_text" style="font-size:90%;"> that
predicted the existence of a keyword. The model was fed with acoustic input
features at each frame (generated every 10ms), and generated prediction labels
at each frame in a streaming manner.</span></p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2005.10406/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>End-to-end topology trained to predict the keyword likelihood
score <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The encoder consists of 4 SVDF-plus-bottleneck
layers, while 3 SVDF layers comprise the decoder.</figcaption>
</figure>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text" style="font-size:90%;">Training such an architecture traditionally required frame-level labels
generated by LVCSR systems </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S2.p3.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.4" class="ltx_text" style="font-size:90%;"> to provide accurate timing
information. This approach to label generation is not possible on-device, due to
the large computational resources required to store and run a LVCSR system.
Therefore, while we retained the same architecture, in our experiments we train
the system with just a binary cross entropy loss for keyword presence and did
not present any supervised targets to train the encoder. Recent
work </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S2.p3.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S2.p3.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S2.p3.1.7" class="ltx_text" style="font-size:90%;"> suggests a better approach to train the encoder without
LVCSR targets and will be the focus of future work.</span></p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Federated Optimization</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text" style="font-size:90%;">In federated learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p1.1.4" class="ltx_text" style="font-size:90%;">, a central server sends neural
models to many client devices (such as phones). These clients process local
caches of data in parallel and send updated model weights back to the server.
The server aggregates the updates, produces a new global model, and repeats this
cycle (called a federated training round) until the model converges.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.8" class="ltx_p"><span id="S3.p2.8.1" class="ltx_text" style="font-size:90%;">Federated Averaging (</span><span id="S3.p2.8.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S3.p2.8.3" class="ltx_text" style="font-size:90%;">) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p2.8.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S3.p2.8.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p2.8.6" class="ltx_text" style="font-size:90%;"> was used as a baseline
optimization algorithm. During each training round, indexed by </span><math id="S3.p2.1.m1.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S3.p2.1.m1.1a"><mi mathsize="90%" id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">t</annotation></semantics></math><span id="S3.p2.8.7" class="ltx_text" style="font-size:90%;">, a subset of
</span><math id="S3.p2.2.m2.1" class="ltx_Math" alttext="K=400" display="inline"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">K</mi><mo mathsize="90%" id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1.cmml">=</mo><mn mathsize="90%" id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><eq id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"></eq><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">𝐾</ci><cn type="integer" id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">K=400</annotation></semantics></math><span id="S3.p2.8.8" class="ltx_text" style="font-size:90%;"> client devices in the experiment population downloaded a global model,
</span><math id="S3.p2.3.m3.1" class="ltx_Math" alttext="w_{t}" display="inline"><semantics id="S3.p2.3.m3.1a"><msub id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml"><mi mathsize="90%" id="S3.p2.3.m3.1.1.2" xref="S3.p2.3.m3.1.1.2.cmml">w</mi><mi mathsize="90%" id="S3.p2.3.m3.1.1.3" xref="S3.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><apply id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p2.3.m3.1.1.1.cmml" xref="S3.p2.3.m3.1.1">subscript</csymbol><ci id="S3.p2.3.m3.1.1.2.cmml" xref="S3.p2.3.m3.1.1.2">𝑤</ci><ci id="S3.p2.3.m3.1.1.3.cmml" xref="S3.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">w_{t}</annotation></semantics></math><span id="S3.p2.8.9" class="ltx_text" style="font-size:90%;">, from the server. Each client </span><math id="S3.p2.4.m4.1" class="ltx_Math" alttext="k\in K" display="inline"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi mathsize="90%" id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">k</mi><mo mathsize="90%" id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">∈</mo><mi mathsize="90%" id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><in id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></in><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">𝑘</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">k\in K</annotation></semantics></math><span id="S3.p2.8.10" class="ltx_text" style="font-size:90%;"> had a local data cache
consisting of </span><math id="S3.p2.5.m5.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S3.p2.5.m5.1a"><msub id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi mathsize="90%" id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">n</mi><mi mathsize="90%" id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1">subscript</csymbol><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">𝑛</ci><ci id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">n_{k}</annotation></semantics></math><span id="S3.p2.8.11" class="ltx_text" style="font-size:90%;"> examples. The clients used stochastic gradient descent
(SGD) to train over their local examples and derive an average gradient,
</span><math id="S3.p2.6.m6.1" class="ltx_Math" alttext="g_{k}" display="inline"><semantics id="S3.p2.6.m6.1a"><msub id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mi mathsize="90%" id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">g</mi><mi mathsize="90%" id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1">subscript</csymbol><ci id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">𝑔</ci><ci id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">g_{k}</annotation></semantics></math><span id="S3.p2.8.12" class="ltx_text" style="font-size:90%;">. For a client learning rate </span><math id="S3.p2.7.m7.1" class="ltx_Math" alttext="{\eta}_{c}" display="inline"><semantics id="S3.p2.7.m7.1a"><msub id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mi mathsize="90%" id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">η</mi><mi mathsize="90%" id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">subscript</csymbol><ci id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">𝜂</ci><ci id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">{\eta}_{c}</annotation></semantics></math><span id="S3.p2.8.13" class="ltx_text" style="font-size:90%;">, the local client step,
</span><math id="S3.p2.8.m8.1" class="ltx_Math" alttext="{w}_{t+1}^{k}" display="inline"><semantics id="S3.p2.8.m8.1a"><msubsup id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><mi mathsize="90%" id="S3.p2.8.m8.1.1.2.2" xref="S3.p2.8.m8.1.1.2.2.cmml">w</mi><mrow id="S3.p2.8.m8.1.1.2.3" xref="S3.p2.8.m8.1.1.2.3.cmml"><mi mathsize="90%" id="S3.p2.8.m8.1.1.2.3.2" xref="S3.p2.8.m8.1.1.2.3.2.cmml">t</mi><mo mathsize="90%" id="S3.p2.8.m8.1.1.2.3.1" xref="S3.p2.8.m8.1.1.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.p2.8.m8.1.1.2.3.3" xref="S3.p2.8.m8.1.1.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml">k</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1">superscript</csymbol><apply id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.2.1.cmml" xref="S3.p2.8.m8.1.1">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.2.cmml" xref="S3.p2.8.m8.1.1.2.2">𝑤</ci><apply id="S3.p2.8.m8.1.1.2.3.cmml" xref="S3.p2.8.m8.1.1.2.3"><plus id="S3.p2.8.m8.1.1.2.3.1.cmml" xref="S3.p2.8.m8.1.1.2.3.1"></plus><ci id="S3.p2.8.m8.1.1.2.3.2.cmml" xref="S3.p2.8.m8.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.p2.8.m8.1.1.2.3.3.cmml" xref="S3.p2.8.m8.1.1.2.3.3">1</cn></apply></apply><ci id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">{w}_{t+1}^{k}</annotation></semantics></math><span id="S3.p2.8.14" class="ltx_text" style="font-size:90%;">, was defined:</span></p>
</div>
<div id="S3.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="{w}_{t+1}^{k}={w}_{t}-{\eta}_{c}{g}_{k}." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.2.2.2" xref="S3.E1.m1.1.1.1.1.2.2.2.cmml">w</mi><mrow id="S3.E1.m1.1.1.1.1.2.2.3" xref="S3.E1.m1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.2.2.3.2" xref="S3.E1.m1.1.1.1.1.2.2.3.2.cmml">t</mi><mo mathsize="90%" id="S3.E1.m1.1.1.1.1.2.2.3.1" xref="S3.E1.m1.1.1.1.1.2.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E1.m1.1.1.1.1.2.2.3.3" xref="S3.E1.m1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo mathsize="90%" id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">w</mi><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml">t</mi></msub><mo mathsize="90%" id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">−</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.cmml">η</mi><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml">c</mi></msub><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2.cmml">g</mi><mi mathsize="90%" id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.cmml">k</mi></msub></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.2">𝑤</ci><apply id="S3.E1.m1.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3"><plus id="S3.E1.m1.1.1.1.1.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S3.E1.m1.1.1.1.1.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><minus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></minus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">𝑤</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3">𝑡</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">𝜂</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3">𝑐</ci></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">𝑔</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">{w}_{t+1}^{k}={w}_{t}-{\eta}_{c}{g}_{k}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text" style="font-size:90%;">This equation represents a single step of SGD, but client training typically
involved multiple steps of SGD with a batch size of 1. Updated client weights
were sent back to the server, which aggregated them to compute a global model
update:</span></p>
</div>
<div id="S3.p5" class="ltx_para">
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\Delta_{t}=\sum_{k=1}^{K}\frac{{n}_{k}}{N}({w}_{t}-{w}_{t+1}^{k})." display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">Δ</mi><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">t</mi></msub><mo mathsize="90%" rspace="0.111em" id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><munderover id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E2.m1.1.1.1.1.1.2.2.2" xref="S3.E2.m1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S3.E2.m1.1.1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.1.1.2.2.3.2.cmml">k</mi><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.2.3.cmml">K</mi></munderover><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mfrac id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.2.cmml">n</mi><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.3.2.3.cmml">k</mi></msub><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msub><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">w</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">t</mi><mo mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">k</mi></msubsup></mrow><mo maxsize="90%" minsize="90%" id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">Δ</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E2.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.2"></sum><apply id="S3.E2.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.2">𝑘</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.2.3">𝐾</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><divide id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"></divide><apply id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2.2">𝑛</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2.3">𝑘</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><minus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1"></minus><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.2">𝑤</ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3"><plus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.1"></plus><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.2">𝑡</ci><cn type="integer" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\Delta_{t}=\sum_{k=1}^{K}\frac{{n}_{k}}{N}({w}_{t}-{w}_{t+1}^{k}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.3" class="ltx_p"><span id="S3.p6.3.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.p6.1.m1.1" class="ltx_Math" alttext="N=\sum_{k}{n}_{k}" display="inline"><semantics id="S3.p6.1.m1.1a"><mrow id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi mathsize="90%" id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">N</mi><mo mathsize="90%" rspace="0.111em" id="S3.p6.1.m1.1.1.1" xref="S3.p6.1.m1.1.1.1.cmml">=</mo><mrow id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml"><msub id="S3.p6.1.m1.1.1.3.1" xref="S3.p6.1.m1.1.1.3.1.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S3.p6.1.m1.1.1.3.1.2" xref="S3.p6.1.m1.1.1.3.1.2.cmml">∑</mo><mi mathsize="90%" id="S3.p6.1.m1.1.1.3.1.3" xref="S3.p6.1.m1.1.1.3.1.3.cmml">k</mi></msub><msub id="S3.p6.1.m1.1.1.3.2" xref="S3.p6.1.m1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.p6.1.m1.1.1.3.2.2" xref="S3.p6.1.m1.1.1.3.2.2.cmml">n</mi><mi mathsize="90%" id="S3.p6.1.m1.1.1.3.2.3" xref="S3.p6.1.m1.1.1.3.2.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><eq id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1.1"></eq><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">𝑁</ci><apply id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3"><apply id="S3.p6.1.m1.1.1.3.1.cmml" xref="S3.p6.1.m1.1.1.3.1"><csymbol cd="ambiguous" id="S3.p6.1.m1.1.1.3.1.1.cmml" xref="S3.p6.1.m1.1.1.3.1">subscript</csymbol><sum id="S3.p6.1.m1.1.1.3.1.2.cmml" xref="S3.p6.1.m1.1.1.3.1.2"></sum><ci id="S3.p6.1.m1.1.1.3.1.3.cmml" xref="S3.p6.1.m1.1.1.3.1.3">𝑘</ci></apply><apply id="S3.p6.1.m1.1.1.3.2.cmml" xref="S3.p6.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.p6.1.m1.1.1.3.2.1.cmml" xref="S3.p6.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.p6.1.m1.1.1.3.2.2.cmml" xref="S3.p6.1.m1.1.1.3.2.2">𝑛</ci><ci id="S3.p6.1.m1.1.1.3.2.3.cmml" xref="S3.p6.1.m1.1.1.3.2.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">N=\sum_{k}{n}_{k}</annotation></semantics></math><span id="S3.p6.3.2" class="ltx_text" style="font-size:90%;">. For a server learning rate </span><math id="S3.p6.2.m2.1" class="ltx_Math" alttext="\eta_{s}" display="inline"><semantics id="S3.p6.2.m2.1a"><msub id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml"><mi mathsize="90%" id="S3.p6.2.m2.1.1.2" xref="S3.p6.2.m2.1.1.2.cmml">η</mi><mi mathsize="90%" id="S3.p6.2.m2.1.1.3" xref="S3.p6.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.1.1.cmml" xref="S3.p6.2.m2.1.1">subscript</csymbol><ci id="S3.p6.2.m2.1.1.2.cmml" xref="S3.p6.2.m2.1.1.2">𝜂</ci><ci id="S3.p6.2.m2.1.1.3.cmml" xref="S3.p6.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\eta_{s}</annotation></semantics></math><span id="S3.p6.3.3" class="ltx_text" style="font-size:90%;">, the updated
global model weights, </span><math id="S3.p6.3.m3.1" class="ltx_Math" alttext="{w}_{t+1}" display="inline"><semantics id="S3.p6.3.m3.1a"><msub id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml"><mi mathsize="90%" id="S3.p6.3.m3.1.1.2" xref="S3.p6.3.m3.1.1.2.cmml">w</mi><mrow id="S3.p6.3.m3.1.1.3" xref="S3.p6.3.m3.1.1.3.cmml"><mi mathsize="90%" id="S3.p6.3.m3.1.1.3.2" xref="S3.p6.3.m3.1.1.3.2.cmml">t</mi><mo mathsize="90%" id="S3.p6.3.m3.1.1.3.1" xref="S3.p6.3.m3.1.1.3.1.cmml">+</mo><mn mathsize="90%" id="S3.p6.3.m3.1.1.3.3" xref="S3.p6.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><apply id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.1.1.1.cmml" xref="S3.p6.3.m3.1.1">subscript</csymbol><ci id="S3.p6.3.m3.1.1.2.cmml" xref="S3.p6.3.m3.1.1.2">𝑤</ci><apply id="S3.p6.3.m3.1.1.3.cmml" xref="S3.p6.3.m3.1.1.3"><plus id="S3.p6.3.m3.1.1.3.1.cmml" xref="S3.p6.3.m3.1.1.3.1"></plus><ci id="S3.p6.3.m3.1.1.3.2.cmml" xref="S3.p6.3.m3.1.1.3.2">𝑡</ci><cn type="integer" id="S3.p6.3.m3.1.1.3.3.cmml" xref="S3.p6.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">{w}_{t+1}</annotation></semantics></math><span id="S3.p6.3.4" class="ltx_text" style="font-size:90%;"> were computed according to:</span></p>
</div>
<div id="S3.p7" class="ltx_para">
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E3.m1.1" class="ltx_Math" alttext="{w}_{t+1}={w}_{t}-\eta_{s}\Delta_{t}." display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.2.2.cmml">w</mi><mrow id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.2.3.2.cmml">t</mi><mo mathsize="90%" id="S3.E3.m1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E3.m1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo mathsize="90%" id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml"><msub id="S3.E3.m1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.3.2.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.3.2.2" xref="S3.E3.m1.1.1.1.1.3.2.2.cmml">w</mi><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.3.2.3" xref="S3.E3.m1.1.1.1.1.3.2.3.cmml">t</mi></msub><mo mathsize="90%" id="S3.E3.m1.1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.1.3.1.cmml">−</mo><mrow id="S3.E3.m1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.3.3.cmml"><msub id="S3.E3.m1.1.1.1.1.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.2.cmml"><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.3.3.2.2" xref="S3.E3.m1.1.1.1.1.3.3.2.2.cmml">η</mi><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.3.3.2.3" xref="S3.E3.m1.1.1.1.1.3.3.2.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.1.1.1.3.3.1" xref="S3.E3.m1.1.1.1.1.3.3.1.cmml">​</mo><msub id="S3.E3.m1.1.1.1.1.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E3.m1.1.1.1.1.3.3.3.2" xref="S3.E3.m1.1.1.1.1.3.3.3.2.cmml">Δ</mi><mi mathsize="90%" id="S3.E3.m1.1.1.1.1.3.3.3.3" xref="S3.E3.m1.1.1.1.1.3.3.3.3.cmml">t</mi></msub></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1"></eq><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2">𝑤</ci><apply id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"><plus id="S3.E3.m1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.2.3.1"></plus><ci id="S3.E3.m1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.E3.m1.1.1.1.1.2.3.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"><minus id="S3.E3.m1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.1"></minus><apply id="S3.E3.m1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.2.2">𝑤</ci><ci id="S3.E3.m1.1.1.1.1.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.2.3">𝑡</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3"><times id="S3.E3.m1.1.1.1.1.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.1"></times><apply id="S3.E3.m1.1.1.1.1.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.2">𝜂</ci><ci id="S3.E3.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.2.3">𝑠</ci></apply><apply id="S3.E3.m1.1.1.1.1.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.2">Δ</ci><ci id="S3.E3.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E3.m1.1.1.1.1.3.3.3.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">{w}_{t+1}={w}_{t}-\eta_{s}\Delta_{t}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p8" class="ltx_para ltx_noindent">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text" style="font-size:90%;">When phrased in the form of Equation </span><a href="#S3.E3" title="In 3 Federated Optimization ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p8.1.2" class="ltx_text" style="font-size:90%;">, </span><span id="S3.p8.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S3.p8.1.4" class="ltx_text" style="font-size:90%;">
clearly consists of an </span><span id="S3.p8.1.5" class="ltx_text ltx_font_italic" style="font-size:90%;">inner optimizer loop</span><span id="S3.p8.1.6" class="ltx_text" style="font-size:90%;"> (SGD over gradients on the
clients) and an </span><span id="S3.p8.1.7" class="ltx_text ltx_font_italic" style="font-size:90%;">outer optimizer loop</span><span id="S3.p8.1.8" class="ltx_text" style="font-size:90%;"> (SGD on averaged weight deltas on
the server).</span></p>
</div>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.1" class="ltx_p"><span id="S3.p9.1.1" class="ltx_text" style="font-size:90%;">Momentum-based variants of </span><span id="S3.p9.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S3.p9.1.3" class="ltx_text" style="font-size:90%;"> were explored as in
Ref. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p9.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a><span id="S3.p9.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p9.1.6" class="ltx_text" style="font-size:90%;">, in which Nesterov accelerated
gradients (NAG) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p9.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib29" title="" class="ltx_ref">29</a><span id="S3.p9.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p9.1.9" class="ltx_text" style="font-size:90%;"> were applied to the server updates. The server
update in Equation </span><a href="#S3.E3" title="In 3 Federated Optimization ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p9.1.10" class="ltx_text" style="font-size:90%;"> was replaced by:</span></p>
</div>
<div id="S3.p10" class="ltx_para">
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E4.m1.1" class="ltx_Math" alttext="{w}_{t+1}={w}_{t}-\eta_{s}(\gamma{v}_{t+1}+\Delta_{t})." display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">w</mi><mrow id="S3.E4.m1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.3.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.2" xref="S3.E4.m1.1.1.1.1.3.3.2.cmml">t</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.1" xref="S3.E4.m1.1.1.1.1.3.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.3.3.3" xref="S3.E4.m1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.3.2.cmml">w</mi><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.3.3.cmml">t</mi></msub><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.3.2.cmml">η</mi><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.3.3.cmml">s</mi></msub><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml">​</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">v</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml"><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml">t</mi><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml">+</mo><mn mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">Δ</mi><mi mathsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo lspace="0em" mathsize="90%" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">𝑤</ci><apply id="S3.E4.m1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3"><plus id="S3.E4.m1.1.1.1.1.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.3.1"></plus><ci id="S3.E4.m1.1.1.1.1.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.3.2">𝑡</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.3.3.3">1</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><minus id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"></minus><apply id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.3.2">𝑤</ci><ci id="S3.E4.m1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3.3">𝑡</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"></times><apply id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.2">𝜂</ci><ci id="S3.E4.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3.3">𝑠</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><plus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1"></plus><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1"></times><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2">𝛾</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2">𝑣</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3"><plus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.1"></plus><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.2">𝑡</ci><cn type="integer" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2">Δ</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">{w}_{t+1}={w}_{t}-\eta_{s}(\gamma{v}_{t+1}+\Delta_{t}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p11" class="ltx_para ltx_noindent">
<p id="S3.p11.2" class="ltx_p"><span id="S3.p11.2.1" class="ltx_text" style="font-size:90%;">where </span><math id="S3.p11.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S3.p11.1.m1.1a"><mi mathsize="90%" id="S3.p11.1.m1.1.1" xref="S3.p11.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S3.p11.1.m1.1b"><ci id="S3.p11.1.m1.1.1.cmml" xref="S3.p11.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.1.m1.1c">\gamma</annotation></semantics></math><span id="S3.p11.2.2" class="ltx_text" style="font-size:90%;"> is the momentum hyperparameter and
</span><math id="S3.p11.2.m2.1" class="ltx_Math" alttext="{v}_{t+1}=\gamma{v}_{t}+\Delta_{t}" display="inline"><semantics id="S3.p11.2.m2.1a"><mrow id="S3.p11.2.m2.1.1" xref="S3.p11.2.m2.1.1.cmml"><msub id="S3.p11.2.m2.1.1.2" xref="S3.p11.2.m2.1.1.2.cmml"><mi mathsize="90%" id="S3.p11.2.m2.1.1.2.2" xref="S3.p11.2.m2.1.1.2.2.cmml">v</mi><mrow id="S3.p11.2.m2.1.1.2.3" xref="S3.p11.2.m2.1.1.2.3.cmml"><mi mathsize="90%" id="S3.p11.2.m2.1.1.2.3.2" xref="S3.p11.2.m2.1.1.2.3.2.cmml">t</mi><mo mathsize="90%" id="S3.p11.2.m2.1.1.2.3.1" xref="S3.p11.2.m2.1.1.2.3.1.cmml">+</mo><mn mathsize="90%" id="S3.p11.2.m2.1.1.2.3.3" xref="S3.p11.2.m2.1.1.2.3.3.cmml">1</mn></mrow></msub><mo mathsize="90%" id="S3.p11.2.m2.1.1.1" xref="S3.p11.2.m2.1.1.1.cmml">=</mo><mrow id="S3.p11.2.m2.1.1.3" xref="S3.p11.2.m2.1.1.3.cmml"><mrow id="S3.p11.2.m2.1.1.3.2" xref="S3.p11.2.m2.1.1.3.2.cmml"><mi mathsize="90%" id="S3.p11.2.m2.1.1.3.2.2" xref="S3.p11.2.m2.1.1.3.2.2.cmml">γ</mi><mo lspace="0em" rspace="0em" id="S3.p11.2.m2.1.1.3.2.1" xref="S3.p11.2.m2.1.1.3.2.1.cmml">​</mo><msub id="S3.p11.2.m2.1.1.3.2.3" xref="S3.p11.2.m2.1.1.3.2.3.cmml"><mi mathsize="90%" id="S3.p11.2.m2.1.1.3.2.3.2" xref="S3.p11.2.m2.1.1.3.2.3.2.cmml">v</mi><mi mathsize="90%" id="S3.p11.2.m2.1.1.3.2.3.3" xref="S3.p11.2.m2.1.1.3.2.3.3.cmml">t</mi></msub></mrow><mo mathsize="90%" id="S3.p11.2.m2.1.1.3.1" xref="S3.p11.2.m2.1.1.3.1.cmml">+</mo><msub id="S3.p11.2.m2.1.1.3.3" xref="S3.p11.2.m2.1.1.3.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S3.p11.2.m2.1.1.3.3.2" xref="S3.p11.2.m2.1.1.3.3.2.cmml">Δ</mi><mi mathsize="90%" id="S3.p11.2.m2.1.1.3.3.3" xref="S3.p11.2.m2.1.1.3.3.3.cmml">t</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p11.2.m2.1b"><apply id="S3.p11.2.m2.1.1.cmml" xref="S3.p11.2.m2.1.1"><eq id="S3.p11.2.m2.1.1.1.cmml" xref="S3.p11.2.m2.1.1.1"></eq><apply id="S3.p11.2.m2.1.1.2.cmml" xref="S3.p11.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p11.2.m2.1.1.2.1.cmml" xref="S3.p11.2.m2.1.1.2">subscript</csymbol><ci id="S3.p11.2.m2.1.1.2.2.cmml" xref="S3.p11.2.m2.1.1.2.2">𝑣</ci><apply id="S3.p11.2.m2.1.1.2.3.cmml" xref="S3.p11.2.m2.1.1.2.3"><plus id="S3.p11.2.m2.1.1.2.3.1.cmml" xref="S3.p11.2.m2.1.1.2.3.1"></plus><ci id="S3.p11.2.m2.1.1.2.3.2.cmml" xref="S3.p11.2.m2.1.1.2.3.2">𝑡</ci><cn type="integer" id="S3.p11.2.m2.1.1.2.3.3.cmml" xref="S3.p11.2.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S3.p11.2.m2.1.1.3.cmml" xref="S3.p11.2.m2.1.1.3"><plus id="S3.p11.2.m2.1.1.3.1.cmml" xref="S3.p11.2.m2.1.1.3.1"></plus><apply id="S3.p11.2.m2.1.1.3.2.cmml" xref="S3.p11.2.m2.1.1.3.2"><times id="S3.p11.2.m2.1.1.3.2.1.cmml" xref="S3.p11.2.m2.1.1.3.2.1"></times><ci id="S3.p11.2.m2.1.1.3.2.2.cmml" xref="S3.p11.2.m2.1.1.3.2.2">𝛾</ci><apply id="S3.p11.2.m2.1.1.3.2.3.cmml" xref="S3.p11.2.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.p11.2.m2.1.1.3.2.3.1.cmml" xref="S3.p11.2.m2.1.1.3.2.3">subscript</csymbol><ci id="S3.p11.2.m2.1.1.3.2.3.2.cmml" xref="S3.p11.2.m2.1.1.3.2.3.2">𝑣</ci><ci id="S3.p11.2.m2.1.1.3.2.3.3.cmml" xref="S3.p11.2.m2.1.1.3.2.3.3">𝑡</ci></apply></apply><apply id="S3.p11.2.m2.1.1.3.3.cmml" xref="S3.p11.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.p11.2.m2.1.1.3.3.1.cmml" xref="S3.p11.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.p11.2.m2.1.1.3.3.2.cmml" xref="S3.p11.2.m2.1.1.3.3.2">Δ</ci><ci id="S3.p11.2.m2.1.1.3.3.3.cmml" xref="S3.p11.2.m2.1.1.3.3.3">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p11.2.m2.1c">{v}_{t+1}=\gamma{v}_{t}+\Delta_{t}</annotation></semantics></math><span id="S3.p11.2.3" class="ltx_text" style="font-size:90%;"> is the forward-looking Nesterov
accelerated gradient. The advantages of Nesterov momentum over classical
momentum </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p11.2.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="S3.p11.2.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p11.2.6" class="ltx_text" style="font-size:90%;"> have been demonstrated in the central training
setting </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p11.2.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="S3.p11.2.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p11.2.9" class="ltx_text" style="font-size:90%;">, and were expected to translate to FL.</span></p>
</div>
<div id="S3.p12" class="ltx_para">
<p id="S3.p12.1" class="ltx_p"><span id="S3.p12.1.1" class="ltx_text" style="font-size:90%;">Finally, adaptive variants of </span><span id="S3.p12.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S3.p12.1.3" class="ltx_text" style="font-size:90%;"> were investigated, in which the
server optimizer function was replaced by Adam </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p12.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S3.p12.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p12.1.6" class="ltx_text" style="font-size:90%;">, Yogi </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p12.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib33" title="" class="ltx_ref">33</a><span id="S3.p12.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p12.1.9" class="ltx_text" style="font-size:90%;">, or
LAMB </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p12.1.10.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib34" title="" class="ltx_ref">34</a><span id="S3.p12.1.11.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p12.1.12" class="ltx_text" style="font-size:90%;">. Prior works shown that adaptive per-coordinate updates can
improve convergence for FL </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p12.1.13.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S3.p12.1.14.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p12.1.15" class="ltx_text" style="font-size:90%;">. Adaptive methods have
shown particular strength in environments with heavy-tailed stochastic gradient
noise distributions </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p12.1.16.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="S3.p12.1.17.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p12.1.18" class="ltx_text" style="font-size:90%;">—a common property of non-IID data in
FL.</span></p>
</div>
<div id="S3.p13" class="ltx_para">
<p id="S3.p13.1" class="ltx_p"><span id="S3.p13.1.1" class="ltx_text" style="font-size:90%;">Adaptive methods replace the server optimizer loop, shown in
Equation </span><a href="#S3.E3" title="In 3 Federated Optimization ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S3.p13.1.2" class="ltx_text" style="font-size:90%;">, with an adaptive optimization step. As with
</span><span id="S3.p13.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S3.p13.1.4" class="ltx_text" style="font-size:90%;">, the classical momentum terms of Adam or Yogi can be replaced
with NAG as in NAdam </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.p13.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib36" title="" class="ltx_ref">36</a><span id="S3.p13.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S3.p13.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Data and Simulations</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><span id="S4.p1.1.1" class="ltx_text" style="font-size:90%;">Experiments were conducted using simulated federated learning with
vendor-collected datasets. A total of 1.5M utterances (1,650 hours in total)
were recorded by 8,000
English-speaking participants. Each lasted a few seconds, and most contained one
of two spoken keyword phrases. The dataset was divided into a train set (1.3
million utterances) and an eval set (180,000 utterances) with non-overlapping
groups of users. IID and non-IID configurations of the training data were
prepared, while the eval set was always used in an IID configuration.</span></p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text" style="font-size:90%;">Utterances were grouped into non-IID simulated clients in three steps. First,
data were clustered according to speaker. The resulting clusters varied
significantly in size: though speakers provided a median of 108 utterances each,
a few speakers were associated with nearly a thousand unique examples. Next,
clusters were further divided on the basis of labels. Individual clusters
contained either positive utterances (which contained the keyword) or negative
utterances (which lacked the keyword) exclusively. It should be noted that
training labels were specified on a per-frame basis, and even positive
utterances contained numerous frames with negative targets. Finally, the
clusters were randomly subdivided to enforce an exponential distribution of
utterances per client (</span><math id="S4.p2.1.m1.1" class="ltx_Math" alttext="n_{k}" display="inline"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi mathsize="90%" id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">n</mi><mi mathsize="90%" id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝑛</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">n_{k}</annotation></semantics></math><span id="S4.p2.1.2" class="ltx_text" style="font-size:90%;">). The final step was motivated by a desire to
match the training data distribution to the inference distribution, coupled with
observations of power-law feature usage among the general population.</span></p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text" style="font-size:90%;">The resulting non-IID training dataset consisted of 160,000 clusters and a
median of 6.5 utterances per cluster. Individual clusters were affiliated with
individual simulated client devices for federated learning.</span></p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text" style="font-size:90%;">For IID simulated clients, the data were randomly divided into clusters
consisting of 50 utterances. Each uniformly-sized cluster thus included a mix of
speakers and labels. The IID training set contained 26,000 clusters of 50
utterances each. 3,675 clusters comprised the IID eval set.</span></p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text" style="font-size:90%;">This section describes experiments to address the various constraints of
on-device training. Specifically, we discuss optimization techniques to overcome
the algorithmic challenge of fitting non-IID data, lightweight data augmentation
techniques that run with constrained on-device resources, and teacher-student
training to provide labels given the inability to peek at federated data.</span></p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Training and evaluation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">Model checkpoints generated by the training tasks were periodically saved and
sent to a held-out set of client devices for federated eval tasks. The train and
eval tasks ran over orthogonal datasets, which were constructed following the
description in Section </span><a href="#S4" title="4 Data and Simulations ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S5.SS1.p1.1.2" class="ltx_text" style="font-size:90%;">. Metrics including the frame-level
accuracy and cross-entropy loss were computed on each client and averaged on the
server.</span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">Hyperparameters were tuned and checkpoints were selected based on the criterion
of minimizing eval loss for non-IID data. Eval loss was measured on the IID
dataset described in Section </span><a href="#S4" title="4 Data and Simulations ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S5.SS1.p2.1.2" class="ltx_text" style="font-size:90%;">. Tasks were also trained on IID
datasets in order to compare training under the different data distributions.</span></p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">Models were evaluated using false accept (FA) and false reject (FR) rates, which
were computed offline on large tests sets consisting of negative utterances and
positive utterances, respectively. The triggering thresholds were tuned to have
FA=0.2%, approximately. FA and FR are more relevant to quality than loss, since
they directly correspond to inference performance.</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Optimizers and learning rate schedules</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.9" class="ltx_p"><span id="S5.SS2.p1.9.1" class="ltx_text" style="font-size:90%;">Optimization techniques were explored for non-IID training. First, the
algorithms described in Section </span><a href="#S3" title="3 Federated Optimization ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS2.p1.9.2" class="ltx_text" style="font-size:90%;"> were tuned via grid
searches. For </span><span id="S5.SS2.p1.9.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S5.SS2.p1.9.4" class="ltx_text" style="font-size:90%;">, </span><math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="\eta_{s}=1.0" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><msub id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p1.1.m1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.2.2.cmml">η</mi><mi mathsize="90%" id="S5.SS2.p1.1.m1.1.1.2.3" xref="S5.SS2.p1.1.m1.1.1.2.3.cmml">s</mi></msub><mo mathsize="90%" id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">1.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><eq id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></eq><apply id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.2.1.cmml" xref="S5.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2.2">𝜂</ci><ci id="S5.SS2.p1.1.m1.1.1.2.3.cmml" xref="S5.SS2.p1.1.m1.1.1.2.3">𝑠</ci></apply><cn type="float" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">1.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">\eta_{s}=1.0</annotation></semantics></math><span id="S5.SS2.p1.9.5" class="ltx_text" style="font-size:90%;"> and a momentum value of </span><math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="0.99" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mn mathsize="90%" id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml">0.99</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><cn type="float" id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1">0.99</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">0.99</annotation></semantics></math><span id="S5.SS2.p1.9.6" class="ltx_text" style="font-size:90%;"> was
found to work best. </span><span id="S5.SS2.p1.9.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAdam</span><span id="S5.SS2.p1.9.8" class="ltx_text" style="font-size:90%;"> and </span><span id="S5.SS2.p1.9.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.SS2.p1.9.10" class="ltx_text" style="font-size:90%;"> both converged well
with </span><math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\beta_{1}=0.9" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><msub id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p1.3.m3.1.1.2.2" xref="S5.SS2.p1.3.m3.1.1.2.2.cmml">β</mi><mn mathsize="90%" id="S5.SS2.p1.3.m3.1.1.2.3" xref="S5.SS2.p1.3.m3.1.1.2.3.cmml">1</mn></msub><mo mathsize="90%" id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><eq id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></eq><apply id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.3.m3.1.1.2.1.cmml" xref="S5.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.3.m3.1.1.2.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2.2">𝛽</ci><cn type="integer" id="S5.SS2.p1.3.m3.1.1.2.3.cmml" xref="S5.SS2.p1.3.m3.1.1.2.3">1</cn></apply><cn type="float" id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\beta_{1}=0.9</annotation></semantics></math><span id="S5.SS2.p1.9.11" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="\beta_{2}=0.999" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><msub id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p1.4.m4.1.1.2.2" xref="S5.SS2.p1.4.m4.1.1.2.2.cmml">β</mi><mn mathsize="90%" id="S5.SS2.p1.4.m4.1.1.2.3" xref="S5.SS2.p1.4.m4.1.1.2.3.cmml">2</mn></msub><mo mathsize="90%" id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><eq id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></eq><apply id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.4.m4.1.1.2.1.cmml" xref="S5.SS2.p1.4.m4.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.4.m4.1.1.2.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2.2">𝛽</ci><cn type="integer" id="S5.SS2.p1.4.m4.1.1.2.3.cmml" xref="S5.SS2.p1.4.m4.1.1.2.3">2</cn></apply><cn type="float" id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\beta_{2}=0.999</annotation></semantics></math><span id="S5.SS2.p1.9.12" class="ltx_text" style="font-size:90%;">, though Adam worked with the default
</span><math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="\epsilon=10^{-8}" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mrow id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">ϵ</mi><mo mathsize="90%" id="S5.SS2.p1.5.m5.1.1.1" xref="S5.SS2.p1.5.m5.1.1.1.cmml">=</mo><msup id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml"><mn mathsize="90%" id="S5.SS2.p1.5.m5.1.1.3.2" xref="S5.SS2.p1.5.m5.1.1.3.2.cmml">10</mn><mrow id="S5.SS2.p1.5.m5.1.1.3.3" xref="S5.SS2.p1.5.m5.1.1.3.3.cmml"><mo mathsize="90%" id="S5.SS2.p1.5.m5.1.1.3.3a" xref="S5.SS2.p1.5.m5.1.1.3.3.cmml">−</mo><mn mathsize="90%" id="S5.SS2.p1.5.m5.1.1.3.3.2" xref="S5.SS2.p1.5.m5.1.1.3.3.2.cmml">8</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><eq id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1.1"></eq><ci id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">italic-ϵ</ci><apply id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p1.5.m5.1.1.3.1.cmml" xref="S5.SS2.p1.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS2.p1.5.m5.1.1.3.2.cmml" xref="S5.SS2.p1.5.m5.1.1.3.2">10</cn><apply id="S5.SS2.p1.5.m5.1.1.3.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3.3"><minus id="S5.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S5.SS2.p1.5.m5.1.1.3.3"></minus><cn type="integer" id="S5.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S5.SS2.p1.5.m5.1.1.3.3.2">8</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\epsilon=10^{-8}</annotation></semantics></math><span id="S5.SS2.p1.9.13" class="ltx_text" style="font-size:90%;"> and </span><math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="\eta_{s}=10^{-3}" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><mrow id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml"><msub id="S5.SS2.p1.6.m6.1.1.2" xref="S5.SS2.p1.6.m6.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p1.6.m6.1.1.2.2" xref="S5.SS2.p1.6.m6.1.1.2.2.cmml">η</mi><mi mathsize="90%" id="S5.SS2.p1.6.m6.1.1.2.3" xref="S5.SS2.p1.6.m6.1.1.2.3.cmml">s</mi></msub><mo mathsize="90%" id="S5.SS2.p1.6.m6.1.1.1" xref="S5.SS2.p1.6.m6.1.1.1.cmml">=</mo><msup id="S5.SS2.p1.6.m6.1.1.3" xref="S5.SS2.p1.6.m6.1.1.3.cmml"><mn mathsize="90%" id="S5.SS2.p1.6.m6.1.1.3.2" xref="S5.SS2.p1.6.m6.1.1.3.2.cmml">10</mn><mrow id="S5.SS2.p1.6.m6.1.1.3.3" xref="S5.SS2.p1.6.m6.1.1.3.3.cmml"><mo mathsize="90%" id="S5.SS2.p1.6.m6.1.1.3.3a" xref="S5.SS2.p1.6.m6.1.1.3.3.cmml">−</mo><mn mathsize="90%" id="S5.SS2.p1.6.m6.1.1.3.3.2" xref="S5.SS2.p1.6.m6.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><apply id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1"><eq id="S5.SS2.p1.6.m6.1.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1.1"></eq><apply id="S5.SS2.p1.6.m6.1.1.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.6.m6.1.1.2.1.cmml" xref="S5.SS2.p1.6.m6.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.6.m6.1.1.2.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2.2">𝜂</ci><ci id="S5.SS2.p1.6.m6.1.1.2.3.cmml" xref="S5.SS2.p1.6.m6.1.1.2.3">𝑠</ci></apply><apply id="S5.SS2.p1.6.m6.1.1.3.cmml" xref="S5.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p1.6.m6.1.1.3.1.cmml" xref="S5.SS2.p1.6.m6.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS2.p1.6.m6.1.1.3.2.cmml" xref="S5.SS2.p1.6.m6.1.1.3.2">10</cn><apply id="S5.SS2.p1.6.m6.1.1.3.3.cmml" xref="S5.SS2.p1.6.m6.1.1.3.3"><minus id="S5.SS2.p1.6.m6.1.1.3.3.1.cmml" xref="S5.SS2.p1.6.m6.1.1.3.3"></minus><cn type="integer" id="S5.SS2.p1.6.m6.1.1.3.3.2.cmml" xref="S5.SS2.p1.6.m6.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">\eta_{s}=10^{-3}</annotation></semantics></math><span id="S5.SS2.p1.9.14" class="ltx_text" style="font-size:90%;"> while Yogi worked best with a larger
</span><math id="S5.SS2.p1.7.m7.1" class="ltx_Math" alttext="\epsilon=10^{-3}" display="inline"><semantics id="S5.SS2.p1.7.m7.1a"><mrow id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p1.7.m7.1.1.2" xref="S5.SS2.p1.7.m7.1.1.2.cmml">ϵ</mi><mo mathsize="90%" id="S5.SS2.p1.7.m7.1.1.1" xref="S5.SS2.p1.7.m7.1.1.1.cmml">=</mo><msup id="S5.SS2.p1.7.m7.1.1.3" xref="S5.SS2.p1.7.m7.1.1.3.cmml"><mn mathsize="90%" id="S5.SS2.p1.7.m7.1.1.3.2" xref="S5.SS2.p1.7.m7.1.1.3.2.cmml">10</mn><mrow id="S5.SS2.p1.7.m7.1.1.3.3" xref="S5.SS2.p1.7.m7.1.1.3.3.cmml"><mo mathsize="90%" id="S5.SS2.p1.7.m7.1.1.3.3a" xref="S5.SS2.p1.7.m7.1.1.3.3.cmml">−</mo><mn mathsize="90%" id="S5.SS2.p1.7.m7.1.1.3.3.2" xref="S5.SS2.p1.7.m7.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><apply id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1"><eq id="S5.SS2.p1.7.m7.1.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1.1"></eq><ci id="S5.SS2.p1.7.m7.1.1.2.cmml" xref="S5.SS2.p1.7.m7.1.1.2">italic-ϵ</ci><apply id="S5.SS2.p1.7.m7.1.1.3.cmml" xref="S5.SS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.p1.7.m7.1.1.3.1.cmml" xref="S5.SS2.p1.7.m7.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS2.p1.7.m7.1.1.3.2.cmml" xref="S5.SS2.p1.7.m7.1.1.3.2">10</cn><apply id="S5.SS2.p1.7.m7.1.1.3.3.cmml" xref="S5.SS2.p1.7.m7.1.1.3.3"><minus id="S5.SS2.p1.7.m7.1.1.3.3.1.cmml" xref="S5.SS2.p1.7.m7.1.1.3.3"></minus><cn type="integer" id="S5.SS2.p1.7.m7.1.1.3.3.2.cmml" xref="S5.SS2.p1.7.m7.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">\epsilon=10^{-3}</annotation></semantics></math><span id="S5.SS2.p1.9.15" class="ltx_text" style="font-size:90%;">, </span><math id="S5.SS2.p1.8.m8.1" class="ltx_Math" alttext="\eta_{s}=0.1" display="inline"><semantics id="S5.SS2.p1.8.m8.1a"><mrow id="S5.SS2.p1.8.m8.1.1" xref="S5.SS2.p1.8.m8.1.1.cmml"><msub id="S5.SS2.p1.8.m8.1.1.2" xref="S5.SS2.p1.8.m8.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p1.8.m8.1.1.2.2" xref="S5.SS2.p1.8.m8.1.1.2.2.cmml">η</mi><mi mathsize="90%" id="S5.SS2.p1.8.m8.1.1.2.3" xref="S5.SS2.p1.8.m8.1.1.2.3.cmml">s</mi></msub><mo mathsize="90%" id="S5.SS2.p1.8.m8.1.1.1" xref="S5.SS2.p1.8.m8.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p1.8.m8.1.1.3" xref="S5.SS2.p1.8.m8.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.8.m8.1b"><apply id="S5.SS2.p1.8.m8.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1"><eq id="S5.SS2.p1.8.m8.1.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1.1"></eq><apply id="S5.SS2.p1.8.m8.1.1.2.cmml" xref="S5.SS2.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.8.m8.1.1.2.1.cmml" xref="S5.SS2.p1.8.m8.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.8.m8.1.1.2.2.cmml" xref="S5.SS2.p1.8.m8.1.1.2.2">𝜂</ci><ci id="S5.SS2.p1.8.m8.1.1.2.3.cmml" xref="S5.SS2.p1.8.m8.1.1.2.3">𝑠</ci></apply><cn type="float" id="S5.SS2.p1.8.m8.1.1.3.cmml" xref="S5.SS2.p1.8.m8.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.8.m8.1c">\eta_{s}=0.1</annotation></semantics></math><span id="S5.SS2.p1.9.16" class="ltx_text" style="font-size:90%;">, and initial accumulator value of </span><math id="S5.SS2.p1.9.m9.1" class="ltx_Math" alttext="10^{-6}" display="inline"><semantics id="S5.SS2.p1.9.m9.1a"><msup id="S5.SS2.p1.9.m9.1.1" xref="S5.SS2.p1.9.m9.1.1.cmml"><mn mathsize="90%" id="S5.SS2.p1.9.m9.1.1.2" xref="S5.SS2.p1.9.m9.1.1.2.cmml">10</mn><mrow id="S5.SS2.p1.9.m9.1.1.3" xref="S5.SS2.p1.9.m9.1.1.3.cmml"><mo mathsize="90%" id="S5.SS2.p1.9.m9.1.1.3a" xref="S5.SS2.p1.9.m9.1.1.3.cmml">−</mo><mn mathsize="90%" id="S5.SS2.p1.9.m9.1.1.3.2" xref="S5.SS2.p1.9.m9.1.1.3.2.cmml">6</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.9.m9.1b"><apply id="S5.SS2.p1.9.m9.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.9.m9.1.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1">superscript</csymbol><cn type="integer" id="S5.SS2.p1.9.m9.1.1.2.cmml" xref="S5.SS2.p1.9.m9.1.1.2">10</cn><apply id="S5.SS2.p1.9.m9.1.1.3.cmml" xref="S5.SS2.p1.9.m9.1.1.3"><minus id="S5.SS2.p1.9.m9.1.1.3.1.cmml" xref="S5.SS2.p1.9.m9.1.1.3"></minus><cn type="integer" id="S5.SS2.p1.9.m9.1.1.3.2.cmml" xref="S5.SS2.p1.9.m9.1.1.3.2">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.9.m9.1c">10^{-6}</annotation></semantics></math><span id="S5.SS2.p1.9.17" class="ltx_text" style="font-size:90%;">.
Experiments were also performed in which Nesterov accelerated gradients were
substituted for classical momenta.</span></p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S5.T1" title="Table 1 ‣ 5.2 Optimizers and learning rate schedules ‣ 5 Experiments ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S5.SS2.p2.1.2" class="ltx_text" style="font-size:90%;"> compares non-IID training with each server
optimization algorithm. Exponentially decayed client learning rates were used
with </span><span id="S5.SS2.p2.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAdam</span><span id="S5.SS2.p2.1.4" class="ltx_text" style="font-size:90%;"> and </span><span id="S5.SS2.p2.1.5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.SS2.p2.1.6" class="ltx_text" style="font-size:90%;">, while </span><span id="S5.SS2.p2.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S5.SS2.p2.1.8" class="ltx_text" style="font-size:90%;"> worked better
with constant client learning rates. The adaptive optimizers had a decisive
advantage over </span><span id="S5.SS2.p2.1.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S5.SS2.p2.1.10" class="ltx_text" style="font-size:90%;"> on FR. Replacing classical momentum with NAG
benefitted </span><span id="S5.SS2.p2.1.11" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S5.SS2.p2.1.12" class="ltx_text" style="font-size:90%;"> and </span><span id="S5.SS2.p2.1.13" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAdam</span><span id="S5.SS2.p2.1.14" class="ltx_text" style="font-size:90%;">, but </span><span id="S5.SS2.p2.1.15" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.SS2.p2.1.16" class="ltx_text" style="font-size:90%;"> with
classical momentum had the lowest FR overall.</span></p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparisons of offline false accept and reject rates for various
optimizers on non-IID data.</figcaption>
<table id="S5.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.3.1.1" class="ltx_tr">
<th id="S5.T1.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Optimizer</span></th>
<th id="S5.T1.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.3.1.1.2.1" class="ltx_text" style="font-size:90%;">FA [%]</span></th>
<th id="S5.T1.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T1.3.1.1.3.1" class="ltx_text" style="font-size:90%;">FR [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.3.2.1" class="ltx_tr">
<td id="S5.T1.3.2.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T1.3.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span></td>
<td id="S5.T1.3.2.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S5.T1.3.2.1.2.1" class="ltx_text" style="font-size:90%;">0.21</span></td>
<td id="S5.T1.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.3.2.1.3.1" class="ltx_text" style="font-size:90%;">8.76</span></td>
</tr>
<tr id="S5.T1.3.3.2" class="ltx_tr">
<td id="S5.T1.3.3.2.1" class="ltx_td ltx_align_left">
<span id="S5.T1.3.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAvg</span><span id="S5.T1.3.3.2.1.2" class="ltx_text" style="font-size:90%;"> + NAG</span>
</td>
<td id="S5.T1.3.3.2.2" class="ltx_td ltx_align_left"><span id="S5.T1.3.3.2.2.1" class="ltx_text" style="font-size:90%;">0.21</span></td>
<td id="S5.T1.3.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.3.2.3.1" class="ltx_text" style="font-size:90%;">4.09</span></td>
</tr>
<tr id="S5.T1.3.4.3" class="ltx_tr">
<td id="S5.T1.3.4.3.1" class="ltx_td ltx_align_left"><span id="S5.T1.3.4.3.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAdam</span></td>
<td id="S5.T1.3.4.3.2" class="ltx_td ltx_align_left"><span id="S5.T1.3.4.3.2.1" class="ltx_text" style="font-size:90%;">0.19</span></td>
<td id="S5.T1.3.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.4.3.3.1" class="ltx_text" style="font-size:90%;">1.95</span></td>
</tr>
<tr id="S5.T1.3.5.4" class="ltx_tr">
<td id="S5.T1.3.5.4.1" class="ltx_td ltx_align_left">
<span id="S5.T1.3.5.4.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedAdam</span><span id="S5.T1.3.5.4.1.2" class="ltx_text" style="font-size:90%;"> + NAG</span>
</td>
<td id="S5.T1.3.5.4.2" class="ltx_td ltx_align_left"><span id="S5.T1.3.5.4.2.1" class="ltx_text" style="font-size:90%;">0.21</span></td>
<td id="S5.T1.3.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.5.4.3.1" class="ltx_text" style="font-size:90%;">1.68</span></td>
</tr>
<tr id="S5.T1.3.6.5" class="ltx_tr">
<td id="S5.T1.3.6.5.1" class="ltx_td ltx_align_left"><span id="S5.T1.3.6.5.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span></td>
<td id="S5.T1.3.6.5.2" class="ltx_td ltx_align_left"><span id="S5.T1.3.6.5.2.1" class="ltx_text" style="font-size:90%;">0.19</span></td>
<td id="S5.T1.3.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T1.3.6.5.3.1" class="ltx_text" style="font-size:90%;">1.39</span></td>
</tr>
<tr id="S5.T1.3.7.6" class="ltx_tr">
<td id="S5.T1.3.7.6.1" class="ltx_td ltx_align_left ltx_border_bb">
<span id="S5.T1.3.7.6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.T1.3.7.6.1.2" class="ltx_text" style="font-size:90%;"> + NAG</span>
</td>
<td id="S5.T1.3.7.6.2" class="ltx_td ltx_align_left ltx_border_bb"><span id="S5.T1.3.7.6.2.1" class="ltx_text" style="font-size:90%;">0.20</span></td>
<td id="S5.T1.3.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T1.3.7.6.3.1" class="ltx_text" style="font-size:90%;">2.11</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.7" class="ltx_p"><span id="S5.SS2.p3.7.1" class="ltx_text" style="font-size:90%;">Theoretical and empirical results indicate that client learning rate (LR) decay
improves convergence on non-IID
data </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS2.p3.7.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S5.SS2.p3.7.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS2.p3.7.4" class="ltx_text" style="font-size:90%;">. Fixed client learning
rates (with </span><math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="{\eta}_{c}=0.02" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><msub id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p3.1.m1.1.1.2.2" xref="S5.SS2.p3.1.m1.1.1.2.2.cmml">η</mi><mi mathsize="90%" id="S5.SS2.p3.1.m1.1.1.2.3" xref="S5.SS2.p3.1.m1.1.1.2.3.cmml">c</mi></msub><mo mathsize="90%" id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><eq id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1"></eq><apply id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p3.1.m1.1.1.2.1.cmml" xref="S5.SS2.p3.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS2.p3.1.m1.1.1.2.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2.2">𝜂</ci><ci id="S5.SS2.p3.1.m1.1.1.2.3.cmml" xref="S5.SS2.p3.1.m1.1.1.2.3">𝑐</ci></apply><cn type="float" id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">{\eta}_{c}=0.02</annotation></semantics></math><span id="S5.SS2.p3.7.5" class="ltx_text" style="font-size:90%;">) were compared with exponentially-decayed client
learning rate schedules, in which </span><math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="{\eta}_{c}" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><msub id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">η</mi><mi mathsize="90%" id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">𝜂</ci><ci id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">{\eta}_{c}</annotation></semantics></math><span id="S5.SS2.p3.7.6" class="ltx_text" style="font-size:90%;"> was reduced by a constant factor,
</span><math id="S5.SS2.p3.3.m3.2" class="ltx_Math" alttext="{\Gamma}_{\eta,c}" display="inline"><semantics id="S5.SS2.p3.3.m3.2a"><msub id="S5.SS2.p3.3.m3.2.3" xref="S5.SS2.p3.3.m3.2.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S5.SS2.p3.3.m3.2.3.2" xref="S5.SS2.p3.3.m3.2.3.2.cmml">Γ</mi><mrow id="S5.SS2.p3.3.m3.2.2.2.4" xref="S5.SS2.p3.3.m3.2.2.2.3.cmml"><mi mathsize="90%" id="S5.SS2.p3.3.m3.1.1.1.1" xref="S5.SS2.p3.3.m3.1.1.1.1.cmml">η</mi><mo mathsize="90%" id="S5.SS2.p3.3.m3.2.2.2.4.1" xref="S5.SS2.p3.3.m3.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S5.SS2.p3.3.m3.2.2.2.2" xref="S5.SS2.p3.3.m3.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.2b"><apply id="S5.SS2.p3.3.m3.2.3.cmml" xref="S5.SS2.p3.3.m3.2.3"><csymbol cd="ambiguous" id="S5.SS2.p3.3.m3.2.3.1.cmml" xref="S5.SS2.p3.3.m3.2.3">subscript</csymbol><ci id="S5.SS2.p3.3.m3.2.3.2.cmml" xref="S5.SS2.p3.3.m3.2.3.2">Γ</ci><list id="S5.SS2.p3.3.m3.2.2.2.3.cmml" xref="S5.SS2.p3.3.m3.2.2.2.4"><ci id="S5.SS2.p3.3.m3.1.1.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1.1.1">𝜂</ci><ci id="S5.SS2.p3.3.m3.2.2.2.2.cmml" xref="S5.SS2.p3.3.m3.2.2.2.2">𝑐</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.2c">{\Gamma}_{\eta,c}</annotation></semantics></math><span id="S5.SS2.p3.7.7" class="ltx_text" style="font-size:90%;">, after every </span><math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="N_{\Gamma}" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><msub id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p3.4.m4.1.1.2" xref="S5.SS2.p3.4.m4.1.1.2.cmml">N</mi><mi mathsize="90%" mathvariant="normal" id="S5.SS2.p3.4.m4.1.1.3" xref="S5.SS2.p3.4.m4.1.1.3.cmml">Γ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><apply id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.p3.4.m4.1.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S5.SS2.p3.4.m4.1.1.2.cmml" xref="S5.SS2.p3.4.m4.1.1.2">𝑁</ci><ci id="S5.SS2.p3.4.m4.1.1.3.cmml" xref="S5.SS2.p3.4.m4.1.1.3">Γ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">N_{\Gamma}</annotation></semantics></math><span id="S5.SS2.p3.7.8" class="ltx_text" style="font-size:90%;"> steps. Hyperparameter scans found
that the eval loss was minimized with an initial learning rate
</span><math id="S5.SS2.p3.5.m5.2" class="ltx_Math" alttext="{\eta}_{c,0}=0.02" display="inline"><semantics id="S5.SS2.p3.5.m5.2a"><mrow id="S5.SS2.p3.5.m5.2.3" xref="S5.SS2.p3.5.m5.2.3.cmml"><msub id="S5.SS2.p3.5.m5.2.3.2" xref="S5.SS2.p3.5.m5.2.3.2.cmml"><mi mathsize="90%" id="S5.SS2.p3.5.m5.2.3.2.2" xref="S5.SS2.p3.5.m5.2.3.2.2.cmml">η</mi><mrow id="S5.SS2.p3.5.m5.2.2.2.4" xref="S5.SS2.p3.5.m5.2.2.2.3.cmml"><mi mathsize="90%" id="S5.SS2.p3.5.m5.1.1.1.1" xref="S5.SS2.p3.5.m5.1.1.1.1.cmml">c</mi><mo mathsize="90%" id="S5.SS2.p3.5.m5.2.2.2.4.1" xref="S5.SS2.p3.5.m5.2.2.2.3.cmml">,</mo><mn mathsize="90%" id="S5.SS2.p3.5.m5.2.2.2.2" xref="S5.SS2.p3.5.m5.2.2.2.2.cmml">0</mn></mrow></msub><mo mathsize="90%" id="S5.SS2.p3.5.m5.2.3.1" xref="S5.SS2.p3.5.m5.2.3.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p3.5.m5.2.3.3" xref="S5.SS2.p3.5.m5.2.3.3.cmml">0.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.5.m5.2b"><apply id="S5.SS2.p3.5.m5.2.3.cmml" xref="S5.SS2.p3.5.m5.2.3"><eq id="S5.SS2.p3.5.m5.2.3.1.cmml" xref="S5.SS2.p3.5.m5.2.3.1"></eq><apply id="S5.SS2.p3.5.m5.2.3.2.cmml" xref="S5.SS2.p3.5.m5.2.3.2"><csymbol cd="ambiguous" id="S5.SS2.p3.5.m5.2.3.2.1.cmml" xref="S5.SS2.p3.5.m5.2.3.2">subscript</csymbol><ci id="S5.SS2.p3.5.m5.2.3.2.2.cmml" xref="S5.SS2.p3.5.m5.2.3.2.2">𝜂</ci><list id="S5.SS2.p3.5.m5.2.2.2.3.cmml" xref="S5.SS2.p3.5.m5.2.2.2.4"><ci id="S5.SS2.p3.5.m5.1.1.1.1.cmml" xref="S5.SS2.p3.5.m5.1.1.1.1">𝑐</ci><cn type="integer" id="S5.SS2.p3.5.m5.2.2.2.2.cmml" xref="S5.SS2.p3.5.m5.2.2.2.2">0</cn></list></apply><cn type="float" id="S5.SS2.p3.5.m5.2.3.3.cmml" xref="S5.SS2.p3.5.m5.2.3.3">0.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.5.m5.2c">{\eta}_{c,0}=0.02</annotation></semantics></math><span id="S5.SS2.p3.7.9" class="ltx_text" style="font-size:90%;">, </span><math id="S5.SS2.p3.6.m6.2" class="ltx_Math" alttext="{\Gamma}_{\eta,c}=0.9" display="inline"><semantics id="S5.SS2.p3.6.m6.2a"><mrow id="S5.SS2.p3.6.m6.2.3" xref="S5.SS2.p3.6.m6.2.3.cmml"><msub id="S5.SS2.p3.6.m6.2.3.2" xref="S5.SS2.p3.6.m6.2.3.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S5.SS2.p3.6.m6.2.3.2.2" xref="S5.SS2.p3.6.m6.2.3.2.2.cmml">Γ</mi><mrow id="S5.SS2.p3.6.m6.2.2.2.4" xref="S5.SS2.p3.6.m6.2.2.2.3.cmml"><mi mathsize="90%" id="S5.SS2.p3.6.m6.1.1.1.1" xref="S5.SS2.p3.6.m6.1.1.1.1.cmml">η</mi><mo mathsize="90%" id="S5.SS2.p3.6.m6.2.2.2.4.1" xref="S5.SS2.p3.6.m6.2.2.2.3.cmml">,</mo><mi mathsize="90%" id="S5.SS2.p3.6.m6.2.2.2.2" xref="S5.SS2.p3.6.m6.2.2.2.2.cmml">c</mi></mrow></msub><mo mathsize="90%" id="S5.SS2.p3.6.m6.2.3.1" xref="S5.SS2.p3.6.m6.2.3.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p3.6.m6.2.3.3" xref="S5.SS2.p3.6.m6.2.3.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.6.m6.2b"><apply id="S5.SS2.p3.6.m6.2.3.cmml" xref="S5.SS2.p3.6.m6.2.3"><eq id="S5.SS2.p3.6.m6.2.3.1.cmml" xref="S5.SS2.p3.6.m6.2.3.1"></eq><apply id="S5.SS2.p3.6.m6.2.3.2.cmml" xref="S5.SS2.p3.6.m6.2.3.2"><csymbol cd="ambiguous" id="S5.SS2.p3.6.m6.2.3.2.1.cmml" xref="S5.SS2.p3.6.m6.2.3.2">subscript</csymbol><ci id="S5.SS2.p3.6.m6.2.3.2.2.cmml" xref="S5.SS2.p3.6.m6.2.3.2.2">Γ</ci><list id="S5.SS2.p3.6.m6.2.2.2.3.cmml" xref="S5.SS2.p3.6.m6.2.2.2.4"><ci id="S5.SS2.p3.6.m6.1.1.1.1.cmml" xref="S5.SS2.p3.6.m6.1.1.1.1">𝜂</ci><ci id="S5.SS2.p3.6.m6.2.2.2.2.cmml" xref="S5.SS2.p3.6.m6.2.2.2.2">𝑐</ci></list></apply><cn type="float" id="S5.SS2.p3.6.m6.2.3.3.cmml" xref="S5.SS2.p3.6.m6.2.3.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.6.m6.2c">{\Gamma}_{\eta,c}=0.9</annotation></semantics></math><span id="S5.SS2.p3.7.10" class="ltx_text" style="font-size:90%;">, and </span><math id="S5.SS2.p3.7.m7.1" class="ltx_Math" alttext="N_{\Gamma}=1000" display="inline"><semantics id="S5.SS2.p3.7.m7.1a"><mrow id="S5.SS2.p3.7.m7.1.1" xref="S5.SS2.p3.7.m7.1.1.cmml"><msub id="S5.SS2.p3.7.m7.1.1.2" xref="S5.SS2.p3.7.m7.1.1.2.cmml"><mi mathsize="90%" id="S5.SS2.p3.7.m7.1.1.2.2" xref="S5.SS2.p3.7.m7.1.1.2.2.cmml">N</mi><mi mathsize="90%" mathvariant="normal" id="S5.SS2.p3.7.m7.1.1.2.3" xref="S5.SS2.p3.7.m7.1.1.2.3.cmml">Γ</mi></msub><mo mathsize="90%" id="S5.SS2.p3.7.m7.1.1.1" xref="S5.SS2.p3.7.m7.1.1.1.cmml">=</mo><mn mathsize="90%" id="S5.SS2.p3.7.m7.1.1.3" xref="S5.SS2.p3.7.m7.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.7.m7.1b"><apply id="S5.SS2.p3.7.m7.1.1.cmml" xref="S5.SS2.p3.7.m7.1.1"><eq id="S5.SS2.p3.7.m7.1.1.1.cmml" xref="S5.SS2.p3.7.m7.1.1.1"></eq><apply id="S5.SS2.p3.7.m7.1.1.2.cmml" xref="S5.SS2.p3.7.m7.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p3.7.m7.1.1.2.1.cmml" xref="S5.SS2.p3.7.m7.1.1.2">subscript</csymbol><ci id="S5.SS2.p3.7.m7.1.1.2.2.cmml" xref="S5.SS2.p3.7.m7.1.1.2.2">𝑁</ci><ci id="S5.SS2.p3.7.m7.1.1.2.3.cmml" xref="S5.SS2.p3.7.m7.1.1.2.3">Γ</ci></apply><cn type="integer" id="S5.SS2.p3.7.m7.1.1.3.cmml" xref="S5.SS2.p3.7.m7.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.7.m7.1c">N_{\Gamma}=1000</annotation></semantics></math><span id="S5.SS2.p3.7.11" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text" style="font-size:90%;">Learning rate comparisons are shown in Table </span><a href="#S5.T2" title="Table 2 ‣ 5.2 Optimizers and learning rate schedules ‣ 5 Experiments ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS2.p4.1.2" class="ltx_text" style="font-size:90%;">, for the
</span><span id="S5.SS2.p4.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.SS2.p4.1.4" class="ltx_text" style="font-size:90%;"> optimizer. LR decay significantly improves both IID and non-IID
training. The difference is most pronounced for non-IID training, where the FR
decreases from 2.35% to 1.39% given a fixed FA=0.2%.</span></p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>A comparison of client learning rate schedules.</figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<th id="S5.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T2.3.1.1.1.1" class="ltx_text" style="font-size:90%;">LR schedule</span></th>
<th id="S5.T2.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.3.1.1.2.1" class="ltx_text" style="font-size:90%;">FR (IID) [%]</span></th>
<th id="S5.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.3.1.1.3.1" class="ltx_text" style="font-size:90%;">FR (Non-IID) [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.2.1" class="ltx_tr">
<th id="S5.T2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T2.3.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Constant</span></th>
<td id="S5.T2.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.3.2.1.2.1" class="ltx_text" style="font-size:90%;">2.14</span></td>
<td id="S5.T2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T2.3.2.1.3.1" class="ltx_text" style="font-size:90%;">2.35</span></td>
</tr>
<tr id="S5.T2.3.3.2" class="ltx_tr">
<th id="S5.T2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.3.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Exponential</span></th>
<td id="S5.T2.3.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.3.3.2.2.1" class="ltx_text" style="font-size:90%;">1.73</span></td>
<td id="S5.T2.3.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.3.3.2.3.1" class="ltx_text" style="font-size:90%;">1.39</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Data Augmentation</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p"><span id="S5.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">Two common speech data augmentation methods—MTR </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="S5.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p1.1.4" class="ltx_text" style="font-size:90%;"> and
SpecAugment </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S5.SS3.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p1.1.7" class="ltx_text" style="font-size:90%;">—were tuned for non-IID training.</span></p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">MTR is an acoustic room simulator that generates noise files which can be
applied to spectrogram inputs. Based on </span><span id="S5.SS3.p2.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">a priori</span><span id="S5.SS3.p2.1.3" class="ltx_text" style="font-size:90%;"> distributions, MTR
generates random room sizes and dimensions, speaker and noise source positions,
signal to noise ratios, and reverberation times </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p2.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S5.SS3.p2.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p2.1.6" class="ltx_text" style="font-size:90%;">. The technique is
effective for far-field speech recognition and has been used previously for
keyword spotting </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p2.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a><span id="S5.SS3.p2.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p2.1.9" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text" style="font-size:90%;">In the simulation experiments, MTR was used to create up to 100 noised replica
of each clean utterance from vendor data. In order to keep a constant number of
training examples per simulated client, MTR configurations were randomly sampled
every time a given simulated client device was used for training.</span></p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p"><span id="S5.SS3.p4.1.1" class="ltx_text" style="font-size:90%;">Unfortunately, MTR is infeasible for on-device training: users would have to
download additional noise data (for additive noise) and room simulation
configurations (for reverberations). The extra data processing would also
lengthen client training times.</span></p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p"><span id="S5.SS3.p5.1.1" class="ltx_text" style="font-size:90%;">Spectrum augmentation (SpecAugment) is a fast and lightweight alternative for
speech data augmentation. It has been used previously for keyword
spotting </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="S5.SS3.p5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p5.1.4" class="ltx_text" style="font-size:90%;">, and has been used to achieve state-of-the-art ASR
performance </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS3.p5.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="S5.SS3.p5.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS3.p5.1.7" class="ltx_text" style="font-size:90%;">. The augmentation policy broadly consists of
three components: (1) </span><span id="S5.SS3.p5.1.8" class="ltx_text ltx_font_italic" style="font-size:90%;">Time Masking</span><span id="S5.SS3.p5.1.9" class="ltx_text" style="font-size:90%;">, in which consecutive time frames in
the spectrogram are masked and replaced with Gaussian-distributed noise, (2)
</span><span id="S5.SS3.p5.1.10" class="ltx_text ltx_font_italic" style="font-size:90%;">Frequency Masking</span><span id="S5.SS3.p5.1.11" class="ltx_text" style="font-size:90%;">, in which adjacent bins of the spectrogram are
zeroed, and (3) </span><span id="S5.SS3.p5.1.12" class="ltx_text ltx_font_italic" style="font-size:90%;">Time Warping</span><span id="S5.SS3.p5.1.13" class="ltx_text" style="font-size:90%;">, in which features are linearly displaced
along the temporal axis.</span></p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p"><span id="S5.SS3.p6.1.1" class="ltx_text" style="font-size:90%;">SpecAugment is an ideal on-device alternative to MTR, as it requires no config
files and minimally increases training time. Tuning in non-IID data simulations
found an optimal configuration of 2 time masks of up to 60 frames along with 2
frequency masks of up to 15 bins. TimeWarp was not used.</span></p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>FA and FR comparisons for models trained on IID and non-IID data with
different data augmentations.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.3.1.1" class="ltx_tr">
<th id="S5.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Data Augmentation</span></th>
<th id="S5.T3.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T3.3.1.1.2.1" class="ltx_text" style="font-size:90%;">Data type</span></th>
<th id="S5.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.3.1.1.3.1" class="ltx_text" style="font-size:90%;">FA [%]</span></th>
<th id="S5.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T3.3.1.1.4.1" class="ltx_text" style="font-size:90%;">FR [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.3.2.1" class="ltx_tr">
<th id="S5.T3.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T3.3.2.1.1.1" class="ltx_text" style="font-size:90%;">No augmentation</span></th>
<th id="S5.T3.3.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T3.3.2.1.2.1" class="ltx_text" style="font-size:90%;">IID</span></th>
<td id="S5.T3.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.3.2.1.3.1" class="ltx_text" style="font-size:90%;">0.17</span></td>
<td id="S5.T3.3.2.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T3.3.2.1.4.1" class="ltx_text" style="font-size:90%;">4.20</span></td>
</tr>
<tr id="S5.T3.3.3.2" class="ltx_tr">
<th id="S5.T3.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.3.2.1.1" class="ltx_text" style="font-size:90%;">No augmentation</span></th>
<th id="S5.T3.3.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.3.2.2.1" class="ltx_text" style="font-size:90%;">Non-IID</span></th>
<td id="S5.T3.3.3.2.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.3.2.3.1" class="ltx_text" style="font-size:90%;">0.20</span></td>
<td id="S5.T3.3.3.2.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.3.2.4.1" class="ltx_text" style="font-size:90%;">3.19</span></td>
</tr>
<tr id="S5.T3.3.4.3" class="ltx_tr">
<th id="S5.T3.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.4.3.1.1" class="ltx_text" style="font-size:90%;">MTR</span></th>
<th id="S5.T3.3.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.4.3.2.1" class="ltx_text" style="font-size:90%;">IID</span></th>
<td id="S5.T3.3.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.4.3.3.1" class="ltx_text" style="font-size:90%;">0.13</span></td>
<td id="S5.T3.3.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.4.3.4.1" class="ltx_text" style="font-size:90%;">6.96</span></td>
</tr>
<tr id="S5.T3.3.5.4" class="ltx_tr">
<th id="S5.T3.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.5.4.1.1" class="ltx_text" style="font-size:90%;">MTR</span></th>
<th id="S5.T3.3.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.5.4.2.1" class="ltx_text" style="font-size:90%;">Non-IID</span></th>
<td id="S5.T3.3.5.4.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.5.4.3.1" class="ltx_text" style="font-size:90%;">0.18</span></td>
<td id="S5.T3.3.5.4.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.5.4.4.1" class="ltx_text" style="font-size:90%;">6.15</span></td>
</tr>
<tr id="S5.T3.3.6.5" class="ltx_tr">
<th id="S5.T3.3.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.6.5.1.1" class="ltx_text" style="font-size:90%;">SpecAugment</span></th>
<th id="S5.T3.3.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T3.3.6.5.2.1" class="ltx_text" style="font-size:90%;">IID</span></th>
<td id="S5.T3.3.6.5.3" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.5.3.1" class="ltx_text" style="font-size:90%;">0.20</span></td>
<td id="S5.T3.3.6.5.4" class="ltx_td ltx_align_center"><span id="S5.T3.3.6.5.4.1" class="ltx_text" style="font-size:90%;">1.73</span></td>
</tr>
<tr id="S5.T3.3.7.6" class="ltx_tr">
<th id="S5.T3.3.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.3.7.6.1.1" class="ltx_text" style="font-size:90%;">SpecAugment</span></th>
<th id="S5.T3.3.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T3.3.7.6.2.1" class="ltx_text" style="font-size:90%;">Non-IID</span></th>
<td id="S5.T3.3.7.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.3.7.6.3.1" class="ltx_text" style="font-size:90%;">0.19</span></td>
<td id="S5.T3.3.7.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T3.3.7.6.4.1" class="ltx_text" style="font-size:90%;">1.39</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p"><span id="S5.SS3.p7.1.1" class="ltx_text" style="font-size:90%;">Augmentation strategies for IID and non-IID FL are compared in
Table </span><a href="#S5.T3" title="Table 3 ‣ 5.3 Data Augmentation ‣ 5 Experiments ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S5.SS3.p7.1.2" class="ltx_text" style="font-size:90%;">. SpecAugment reduced the FR with respect to MTR and
no augmentation on both data distributions. Thus, we can reduce communication
costs and on-device processing time with SpecAugment while also improving
performance.</span></p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Labeling</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p"><span id="S5.SS4.p1.1.1" class="ltx_text" style="font-size:90%;">High-quality labeling can be difficult to obtain on-device, since peeking at
data is impossible by design in FL, and user feedback signals are unreliable or
infrequent. Given the obstacles to on-device labeling, teacher student training
can be used to adapt a model trained on the server (with manually labeled data)
to the on-device unlabeled data domain </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS4.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a><span id="S5.SS4.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS4.p1.1.4" class="ltx_text" style="font-size:90%;">.
Models were trained on both IID and non-IID data with supervised and
teacher-generated labels. For the semi-supervised setting, the teacher model
architecture was identical to the student, but was trained on additional data in
a centralized setting.</span></p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>FR comparisons for on-device labeling strategies.</figcaption>
<table id="S5.T4.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.3.1.1" class="ltx_tr">
<th id="S5.T4.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S5.T4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Labeling</span></th>
<th id="S5.T4.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.3.1.1.2.1" class="ltx_text" style="font-size:90%;">FR (IID) [%]</span></th>
<th id="S5.T4.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T4.3.1.1.3.1" class="ltx_text" style="font-size:90%;">FR (Non-IID) [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.3.2.1" class="ltx_tr">
<th id="S5.T4.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S5.T4.3.2.1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Supervised</span></th>
<td id="S5.T4.3.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T4.3.2.1.2.1" class="ltx_text" style="font-size:90%;">1.73</span></td>
<td id="S5.T4.3.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T4.3.2.1.3.1" class="ltx_text" style="font-size:90%;">1.39</span></td>
</tr>
<tr id="S5.T4.3.3.2" class="ltx_tr">
<th id="S5.T4.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T4.3.3.2.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">Teacher</span></th>
<td id="S5.T4.3.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.3.2.2.1" class="ltx_text" style="font-size:90%;">2.12</span></td>
<td id="S5.T4.3.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T4.3.3.2.3.1" class="ltx_text" style="font-size:90%;">2.07</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#S5.T4" title="Table 4 ‣ 5.4 Labeling ‣ 5 Experiments ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S5.SS4.p2.1.2" class="ltx_text" style="font-size:90%;"> compares teacher student training with supervised
training. While the FR increases when moving to semi-supervised labels, it is
expected that the matched data available in true on-device data, coupled with
a limited number of samples labeled with user feedback signals, will close the
performance gap.</span></p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Central training comparison and ablation studies</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text" style="font-size:90%;">The previously-discussed techniques were applied and then removed individually
in an ablation study, and the results were compared with a model trained in the
centralized setting on the exact same vendor dataset using </span><math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="4\times 10^{8}" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mrow id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml"><mn mathsize="90%" id="S5.SS5.p1.1.m1.1.1.2" xref="S5.SS5.p1.1.m1.1.1.2.cmml">4</mn><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="S5.SS5.p1.1.m1.1.1.1" xref="S5.SS5.p1.1.m1.1.1.1.cmml">×</mo><msup id="S5.SS5.p1.1.m1.1.1.3" xref="S5.SS5.p1.1.m1.1.1.3.cmml"><mn mathsize="90%" id="S5.SS5.p1.1.m1.1.1.3.2" xref="S5.SS5.p1.1.m1.1.1.3.2.cmml">10</mn><mn mathsize="90%" id="S5.SS5.p1.1.m1.1.1.3.3" xref="S5.SS5.p1.1.m1.1.1.3.3.cmml">8</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><apply id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1"><times id="S5.SS5.p1.1.m1.1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1.1"></times><cn type="integer" id="S5.SS5.p1.1.m1.1.1.2.cmml" xref="S5.SS5.p1.1.m1.1.1.2">4</cn><apply id="S5.SS5.p1.1.m1.1.1.3.cmml" xref="S5.SS5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.SS5.p1.1.m1.1.1.3.1.cmml" xref="S5.SS5.p1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S5.SS5.p1.1.m1.1.1.3.2.cmml" xref="S5.SS5.p1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S5.SS5.p1.1.m1.1.1.3.3.cmml" xref="S5.SS5.p1.1.m1.1.1.3.3">8</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">4\times 10^{8}</annotation></semantics></math><span id="S5.SS5.p1.1.2" class="ltx_text" style="font-size:90%;"> steps
of asynchronous SGD. This provided a direct comparison of centralized training
and FL on IID and non-IID data.</span></p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text" style="font-size:90%;">Two additional techniques were explored to fit non-IID data. Client update
clipping, based on </span><math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="\|{w}_{t+1}^{k}-{w}_{t}\|^{2}" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><msup id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml"><mrow id="S5.SS5.p2.1.m1.1.1.1.1" xref="S5.SS5.p2.1.m1.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.2" xref="S5.SS5.p2.1.m1.1.1.1.2.1.cmml">‖</mo><mrow id="S5.SS5.p2.1.m1.1.1.1.1.1" xref="S5.SS5.p2.1.m1.1.1.1.1.1.cmml"><msubsup id="S5.SS5.p2.1.m1.1.1.1.1.1.2" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.cmml"><mi mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.2" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.2.cmml">w</mi><mrow id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.cmml"><mi mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.2" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.2.cmml">t</mi><mo mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.1" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.1.cmml">+</mo><mn mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.3" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.3" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.3.cmml">k</mi></msubsup><mo mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.1" xref="S5.SS5.p2.1.m1.1.1.1.1.1.1.cmml">−</mo><msub id="S5.SS5.p2.1.m1.1.1.1.1.1.3" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.3.2" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3.2.cmml">w</mi><mi mathsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.1.3.3" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3.3.cmml">t</mi></msub></mrow><mo maxsize="90%" minsize="90%" id="S5.SS5.p2.1.m1.1.1.1.1.3" xref="S5.SS5.p2.1.m1.1.1.1.2.1.cmml">‖</mo></mrow><mn mathsize="90%" id="S5.SS5.p2.1.m1.1.1.3" xref="S5.SS5.p2.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><apply id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS5.p2.1.m1.1.1.2.cmml" xref="S5.SS5.p2.1.m1.1.1">superscript</csymbol><apply id="S5.SS5.p2.1.m1.1.1.1.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS5.p2.1.m1.1.1.1.2.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.2">norm</csymbol><apply id="S5.SS5.p2.1.m1.1.1.1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1"><minus id="S5.SS5.p2.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.1"></minus><apply id="S5.SS5.p2.1.m1.1.1.1.1.1.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.2">𝑤</ci><apply id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3"><plus id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.1"></plus><ci id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.3.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.SS5.p2.1.m1.1.1.1.1.1.2.3.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.2.3">𝑘</ci></apply><apply id="S5.SS5.p2.1.m1.1.1.1.1.1.3.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS5.p2.1.m1.1.1.1.1.1.3.1.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.SS5.p2.1.m1.1.1.1.1.1.3.2.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3.2">𝑤</ci><ci id="S5.SS5.p2.1.m1.1.1.1.1.1.3.3.cmml" xref="S5.SS5.p2.1.m1.1.1.1.1.1.3.3">𝑡</ci></apply></apply></apply><cn type="integer" id="S5.SS5.p2.1.m1.1.1.3.cmml" xref="S5.SS5.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">\|{w}_{t+1}^{k}-{w}_{t}\|^{2}</annotation></semantics></math><span id="S5.SS5.p2.1.2" class="ltx_text" style="font-size:90%;">, was tuned for
non-IID fitting. Multiple client training epochs were also studied, and have
been shown to improve convergence </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS5.p2.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib24" title="" class="ltx_ref">24</a><span id="S5.SS5.p2.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS5.p2.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p"><span id="S5.SS5.p3.1.1" class="ltx_text" style="font-size:90%;">The following settings were used for the ablation study FL baseline: data were
augmented with SpecAugment, 10 client epochs were used, the client LR was
decayed, client L2 weight norms were clipped to 20, and the </span><span id="S5.SS5.p3.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S5.SS5.p3.1.3" class="ltx_text" style="font-size:90%;">
optimizer was used to train with supervised labels.</span></p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p"><span id="S5.SS5.p4.1.1" class="ltx_text" style="font-size:90%;">Ablation results are shown in Figure </span><a href="#S5.F2" title="Figure 2 ‣ 5.5 Central training comparison and ablation studies ‣ 5 Experiments ‣ Training Keyword Spotting Models on Non-IID Data with Federated Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">2</span></a><span id="S5.SS5.p4.1.2" class="ltx_text" style="font-size:90%;">. The metrics favor
non-IID data because the hyperparameters were tuned to minimize non-IID eval
loss. Had the hyperparameters been tuned for IID data, the IID FR would be lower
than the FR tuned for non-IID data.</span></p>
</div>
<div id="S5.SS5.p5" class="ltx_para">
<p id="S5.SS5.p5.1" class="ltx_p"><span id="S5.SS5.p5.1.1" class="ltx_text" style="font-size:90%;">FL achieves comparable FR performance with the centrally-trained model. In
absolute terms, SpecAugment and multiple client epochs provided the largest
contributions to both IID and non-IID performance. Interestingly, decayed client
learning rates were more important to non-IID training than IID training. And
contrary to prior studies </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS5.p5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="S5.SS5.p5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS5.p5.1.4" class="ltx_text" style="font-size:90%;">, we found that additional client
training epochs benefitted non-IID training.</span></p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2005.10406/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="451" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>FA and FR for the ablation study, with models trained on IID and
non-IID data compared with centralized training (dashed line).</figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">Empirical studies were conducted to train a keyword-spotting model using FL on
non-IID data. Adaptive server optimizers like </span><span id="S6.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">FedYogi</span><span id="S6.p1.1.3" class="ltx_text" style="font-size:90%;"> helped train a
model with a lower false reject rate in fewer training rounds. We also
demonstrated the necessity and utility of replacing MTR with SpecAugment for
on-device training. Ablation studies revealed the importance of multiple
client epochs and reduced client clipping. And we provided strong empirical
evidence in favor of client learning rate decay for training with non-IID data.
Finally, we overcome the visiblity limitations of on-device training by
demonstrating that, in the absence of high-quality on-device labels,
teacher-student training can achieve comparable performance.</span></p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p"><span id="S7.p1.1.1" class="ltx_text" style="font-size:90%;">The authors would like to thank Google Research colleagues for providing the FL
framework, Manzil Zaheer for his optimizer expertise, and Daniel Park for
SpecAugment discussions.</span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
G. Hinton, L. Deng, D. Yu, G. Dahl, A. rahman Mohamed, N. Jaitly, A. Senior,
V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury, “Deep neural networks
for acoustic modeling in speech recognition,” </span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Signal Processing
Magazine</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, “Application of pretrained
deep neural networks to large vocabulary speech recognition,” in
</span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Interspeech 2012</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Y. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao,
D. Rybach, A. Kannan, Y. Wu, R. Pang, and et al., “Streaming end-to-end
speech recognition for mobile devices,” </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
G. Chen, C. Parada, and G. Heigold, “Small-footprint keyword spotting using
deep neural networks,” in </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
S. Panchapagesan, M. Sun, A. Khare, S. Matsoukas, A. Mandal, B. Hoffmeister,
and S. Vitaladevuni, “Multi-task learning and weighted cross-entropy for
DNN-based keyword spotting,” in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
S. Team, “Hey siri: An on-device dnn-powered voice trigger for apple’s
personal assistant,”
https://machinelearning.apple.com/2017/10/01/hey-siri.html, accessed:
2020-04-30.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
M. Sun, D. Snyder, Y. Gao, V. K. Nagaraja, M. Rodehorst, S. Panchapagesan,
N. Strom, S. Matsoukas, and S. Vitaladevuni, “Compressed time delay neural
network for small-footprint keyword spotting,” in </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
H.-J. Park, P. Violette, and N. Subrahmanya, “Learning to detect keyword parts
and whole by smoothed max pooling,” </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
A. Gruenstein, R. Alvarez, C. Thornton, and M. Ghodrat, “A cascade
architecture for keyword spotting on mobile devices,” 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
R. Alvarez and H.-J. Park, “End-to-end streaming keyword spotting,”
</span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP)</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017,
Fort Lauderdale, FL, USA</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
C. Dwork, “Differential privacy,” in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">33rd International Colloquium on
Automata, Languages and Programming, part II (ICALP 2006)</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, vol. 4052.   Venice, Italy: Springer Verlag.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” </span><em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of
the 2016 ACM SIGSAC Conference on Computer and Communications Security</em><span id="bib.bib13.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth, “Practical secure aggregation for
federated learning on user-held data,” in </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS Workshop on Private
Multi-Party Machine Learning</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augenstein,
H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for mobile
keyboard prediction,” 2018.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
S. Ramaswamy, R. Mathews, K. Rao, and F. Beaufays, “Federated learning for
emoji prediction in a mobile keyboard,” 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
M. Chen, A. T. Suresh, R. Mathews, A. Wong, C. Allauzen, F. Beaufays, and
M. Riley, “Federated learning of n-gram language models,” in
</span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd Conference on Computational Natural Language
Learning, CoNLL 2019, Hong Kong, China, November 3-4, 2019</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong, D. Ramage, and
F. Beaufays, “Applied federated learning: Improving google keyboard query
suggestions,” 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Federated
learning for keyword spotting,” </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated learning
with non-IID data,” 2018.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “On the convergence of fedavg
on non-IID data,” 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
P. K. et al., “Advances and open problems in federated learning,” 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T.
Suresh, “Scaffold: Stochastic controlled averaging for federated learning,”
2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
S. Reddi, Z. Charles, M. Zaheer, Z. Garrett, K. Rush,
J. Konečný, S. Kumar, and H. B. McMahan, “Adaptive
Federated Optimization,” </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv e-prints</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, Feb. 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
M. Dundar, B. Krishnapuram, J. Bi, and R. B. Rao, “Learning classifiers when
the training data is not iid,” in </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 20th
International Joint Conference on Artifical Intelligence</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, ser.
IJCAI’07.   San Francisco, CA, USA:
Morgan Kaufmann Publishers Inc., 2007.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
P. Nakkiran, R. Alvarez, R. Prabhavalkar, and C. Parada, “Compressing deep
neural networks using a rank-constrained topology,” in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of
Annual Conference of the International Speech Communication Association
(Interspeech)</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
T. Sainath and C. Parada, “Convolutional neural networks for small-footprint
keyword spotting.” in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Annual Conference of the
International Speech Communication Association (Interspeech)</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
T.-M. H. Hsu, H. Qi, and M. Brown, “Measuring the effects of non-identical
data distribution for federated visual classification,” 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">
Y. Nesterov, “A method for solving the convex programming problem with
convergence rate </span><math id="bib.bib29.1.m1.1" class="ltx_Math" alttext="o(1/{k}^{2})" display="inline"><semantics id="bib.bib29.1.m1.1a"><mrow id="bib.bib29.1.m1.1.1" xref="bib.bib29.1.m1.1.1.cmml"><mi mathsize="90%" id="bib.bib29.1.m1.1.1.3" xref="bib.bib29.1.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="bib.bib29.1.m1.1.1.2" xref="bib.bib29.1.m1.1.1.2.cmml">​</mo><mrow id="bib.bib29.1.m1.1.1.1.1" xref="bib.bib29.1.m1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="bib.bib29.1.m1.1.1.1.1.2" xref="bib.bib29.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="bib.bib29.1.m1.1.1.1.1.1" xref="bib.bib29.1.m1.1.1.1.1.1.cmml"><mn mathsize="90%" id="bib.bib29.1.m1.1.1.1.1.1.2" xref="bib.bib29.1.m1.1.1.1.1.1.2.cmml">1</mn><mo maxsize="90%" minsize="90%" stretchy="true" symmetric="true" id="bib.bib29.1.m1.1.1.1.1.1.1" xref="bib.bib29.1.m1.1.1.1.1.1.1.cmml">/</mo><msup id="bib.bib29.1.m1.1.1.1.1.1.3" xref="bib.bib29.1.m1.1.1.1.1.1.3.cmml"><mi mathsize="90%" id="bib.bib29.1.m1.1.1.1.1.1.3.2" xref="bib.bib29.1.m1.1.1.1.1.1.3.2.cmml">k</mi><mn mathsize="90%" id="bib.bib29.1.m1.1.1.1.1.1.3.3" xref="bib.bib29.1.m1.1.1.1.1.1.3.3.cmml">2</mn></msup></mrow><mo maxsize="90%" minsize="90%" id="bib.bib29.1.m1.1.1.1.1.3" xref="bib.bib29.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="bib.bib29.1.m1.1b"><apply id="bib.bib29.1.m1.1.1.cmml" xref="bib.bib29.1.m1.1.1"><times id="bib.bib29.1.m1.1.1.2.cmml" xref="bib.bib29.1.m1.1.1.2"></times><ci id="bib.bib29.1.m1.1.1.3.cmml" xref="bib.bib29.1.m1.1.1.3">𝑜</ci><apply id="bib.bib29.1.m1.1.1.1.1.1.cmml" xref="bib.bib29.1.m1.1.1.1.1"><divide id="bib.bib29.1.m1.1.1.1.1.1.1.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.1"></divide><cn type="integer" id="bib.bib29.1.m1.1.1.1.1.1.2.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.2">1</cn><apply id="bib.bib29.1.m1.1.1.1.1.1.3.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="bib.bib29.1.m1.1.1.1.1.1.3.1.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.3">superscript</csymbol><ci id="bib.bib29.1.m1.1.1.1.1.1.3.2.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.3.2">𝑘</ci><cn type="integer" id="bib.bib29.1.m1.1.1.1.1.1.3.3.cmml" xref="bib.bib29.1.m1.1.1.1.1.1.3.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib29.1.m1.1c">o(1/{k}^{2})</annotation></semantics></math><span id="bib.bib29.3.2" class="ltx_text" style="font-size:90%;">,” </span><em id="bib.bib29.4.3" class="ltx_emph ltx_font_italic" style="font-size:90%;">Dokl. Akad. Nauk SSSR</em><span id="bib.bib29.5.4" class="ltx_text" style="font-size:90%;">, vol. 269,
1983.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
B. Polyak, “Some methods of speeding up the convergence of iteration
methods,” </span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">USSR Computational Mathematics and Mathematical Physics</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">,
vol. 4, no. 5, 1964.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
I. Sutskever, J. Martens, G. Dahl, and G. Hinton, “On the importance of
initialization and momentum in deep learning,” in </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the
30th International Conference on Machine Learning</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">.   PMLR, Jun 2013.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”
</span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv e-prints</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">, Dec. 2014.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
M. Zaheer, S. J. Reddi, D. Sachan, S. Kale, and S. Kumar, “Adaptive methods
for nonconvex optimization,” in </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 32nd International
Conference on Neural Information Processing Systems</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:90%;">, ser. NIPS’18.   Red Hook, NY, USA: Curran Associates Inc.,
2018.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
J. Demmel, K. Keutzer, and C.-J. Hsieh, “Large batch optimization for deep
learning: Training bert in 76 minutes,” 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. J. Reddi, S. Kumar, and
S. Sra, “Why adam beats sgd for attention models,” 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
T. Dozat, “Incorporating nesterov momentum into adam,” 2016.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
C. Kim, A. Misra, K. K. Chin, T. Hughes, A. Narayanan, T. N. Sainath, and
M. Bacchiani, “Generation of large-scale simulated utterances in virtual
rooms to train deep-neural networks for far-field speech recognition in
google home,” in </span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">INTERSPEECH</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,
“Specaugment: A simple data augmentation method for automatic speech
recognition,” </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interspeech 2019</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
R. Prabhavalkar, R. Alvarez, C. Parada, P. Nakkiran, and T. Sainath,
“Automatic gain control and multi-style training for robust small-footprint
keyword spotting with deep neural networks,” in </span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
J. Hou, Y. Shi, M. Ostendorf, M. Hwang, and L. Xie, “Mining
effective negative training samples for keyword spotting,” in </span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP
2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
D. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu,
“Specaugment on large scale datasets,” </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2020 - 2020 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
network,” 2015.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
J. Li, M. L. Seltzer, X. Wang, R. Zhao, and Y. Gong, “Large-scale domain
adaptation via teacher-student learning,” </span><em id="bib.bib43.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Interspeech 2017</em><span id="bib.bib43.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
S. H. Krishnan Parthasarathi and N. Strom, “Lessons from building acoustic
models with a million hours of speech,” </span><em id="bib.bib44.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICASSP 2019 - 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em><span id="bib.bib44.3.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2005.10405" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2005.10406" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2005.10406">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2005.10406" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2005.10407" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 14:17:42 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
