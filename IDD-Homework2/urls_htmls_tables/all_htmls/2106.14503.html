<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2106.14503] Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data</title><meta property="og:description" content="Federated Learning allows training of data stored in distributed devices without the need for centralizing training data, thereby maintaining data privacy. Addressing the ability to handle data heterogeneity (non-identâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2106.14503">

<!--Generated on Tue Mar 19 12:16:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pravin Chandran
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raghavendra Bhat
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Avinash Chakravarthy
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Srikanth Chandar
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Federated Learning allows training of data stored in distributed devices without the need for centralizing training data, thereby maintaining data privacy. Addressing the ability to handle data heterogeneity (non-identical and independent distribution or non-IID) is a key enabler for the wider deployment of Federated Learning. In this paper, we propose a novel Divide-and-Conquer training methodology that enables the use of the popular FedAvg aggregation algorithm by overcoming the acknowledged FedAvg limitations in non-IID environments. We propose a novel use of Cosine-distance based Weight Divergence metric to determine the exact point where a Deep Learning network can be divided into class agnostic initial layers and class-specific deep layers for performing a Divide and Conquer training. We show that the methodology achieves trained model accuracy at par (and in certain cases exceeding) with numbers achieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc. Also, we show that this methodology leads to compute and bandwidth optimizations under certain documented conditions.</p>
</div>
<div id="id1" class="ltx_logical-block">
<div id="id1.p1" class="ltx_para">
<p id="id1.p1.1" class="ltx_p ltx_align_center">Intel Technology India Pvt. Ltd, Bengaluru, KA, India</p>
<p id="id1.p1.2" class="ltx_p ltx_align_center">(pravin.chandran, raghavendra.bhat, avinash.chakravarthi,srikanth.chandar)@intel.com</p>
</div>
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning has been proposed as a new learning paradigm to overcome the privacy regulations and communication overheads associated with central training <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite><cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. In Federated Learning, a central server shares a global model with participating client devices and the model is trained on the local datasets available at the client device. The local dataset is never shared with the server, instead, local updates to the global model are shared with the server. The server combines the local updates from the participating clients using an Optimization (or Aggregation) Algorithm and creates a new version of the global model. This process is repeated for the required number of communication rounds until the desired convergence criteria are achieved.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning differs significantly from traditional learning approaches in terms of optimization in a distributed setting, privacy preserving learning, and communication latency during the learning process <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2019</a>)</cite>. Optimization in Distributed setting differs from the traditional learning approach due to statistical and systems heterogeneity <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite>. The statistical heterogeneity manifests itself in the form of non-independent and identical distribution (non-IID) of training data across participating clients. The non-IID condition arises due to a host of reasons that is specific to the local environment and usage patterns at the client. Causes for the skewed data distribution have been surveyed extensively and it has been proven that any real-world scale deployment of Federated Learning should address the challenges around non-IID data. A good example specific to the medical domain can be found in <cite class="ltx_cite ltx_citemacro_citep">(Xu &amp; Wang, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>. Several approaches have been studied to address the non-IID heterogeneity. Data Distillation which involves sharing of client data with central server <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, Client specific local models or Personalization layers to customize the last few layers of the global model specific to the client data <cite class="ltx_cite ltx_citemacro_citep">(Fallah etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Ghosh etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Hanzely etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Dinh etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Hanzely &amp; RichtÃ¡rik, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, Novel optimization algorithms <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> etc. are some of these most researched approaches.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Data Distillation techniques violate the strict privacy requirements. Client specific model approach results in multiple models, which does not cater to any specific requirement for a single model for deployment. In this paper, we focus on the Optimization Algorithm approach to address the non-IID challenge. While there are numerous state-of-the-art algorithms like FedProx <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>, FedMA <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>, FedMAX <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> etc., these approaches are not productized in a large scale to the best of knowledge of the authors. Hence, we focus on the most widely deployed FedAvg algorithm <cite class="ltx_cite ltx_citemacro_citep">(McMahan etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2017</a>)</cite> and investigate improving its ability to handle non-IID data to the same level as state-of-the-art algorithms like FedMA, FedProx, FedMAX, etc.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The primary contribution of this paper is proposing a novel Divide-and-Conquer training methodology which in combination with FedAvg is able to meet state-of-the-art performance in simulated environment. Another contribution of this paper is the novel use of the Cosine Distance based Weight Divergence metric to partition the global model into class agnostic initial layers and class-specific deep layers. The two parts of the global model are trained in a mutually exclusive manner while freezing the other part. Under certain documented conditions, this approach also leads to better compute and bandwidth optimization.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of the paper is organized as follows. Section II discusses the limitation with vanilla FedAvg algorithm while section III explains the Divide-and-Conquer methodology. We document the simulation environment, experiments, and results in the simulated environment in section IV establishing the state-of-the-art credentials of the approach. Finally, we conclude the paper and discuss possible future work in section V.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>FedAvg and its Challenges</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Federated Learning (FL) methods are designed to train over multiple devices, each holding their own data, with a central server driving the global learning objective across the entire network. The standard formulation of FL aims to find the minimizer of the overall population loss <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite> shown below.</p>
</div>
<figure id="S2.1" class="ltx_figure">
<p id="S2.1.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S2.1.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/dnc_eqaution.png" id="S2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="129" alt="[Uncaptioned image]"></span></p>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In general, the local objectives measure the local empirical risk over possibly differing data distributions with samples available at each device. In a non-IID environment, the assumption of a global minimizer being representative of the overall population is not valid as every client has its own data distribution which differs from other clients and the overall population. Hence, on each client, a <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">local objective function</span> based on the clientâ€™s data is used as a surrogate for the global objective function. At each outer iteration, a subset of devices are selected and <span id="S2.p2.1.2" class="ltx_text ltx_font_italic">local solvers</span> are used to optimize the local objective functions of the selected client. Each client then communicates its local model updates to the central server, which aggregates them and updates the global model accordingly. In addition to the usual hyper-parameters of traditional learning like batch size, optimizer, etc., Federated Learning has additional hyper-parameters like epochs per round (<math id="S2.p2.1.m1.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S2.p2.1.m1.1a"><msub id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">E</mi><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">ğ¸</ci><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">E_{p}</annotation></semantics></math>), number of communication rounds, number of participants in each round, and optimization algorithm which can be tweaked for optimal performance.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.7" class="ltx_p">In FedAvg, the local objective function at client <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">k</annotation></semantics></math> is <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="F_{k}(\cdot)" display="inline"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.2" xref="S2.p3.2.m2.1.2.cmml"><msub id="S2.p3.2.m2.1.2.2" xref="S2.p3.2.m2.1.2.2.cmml"><mi id="S2.p3.2.m2.1.2.2.2" xref="S2.p3.2.m2.1.2.2.2.cmml">F</mi><mi id="S2.p3.2.m2.1.2.2.3" xref="S2.p3.2.m2.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S2.p3.2.m2.1.2.1" xref="S2.p3.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S2.p3.2.m2.1.2.3.2" xref="S2.p3.2.m2.1.2.cmml"><mo stretchy="false" id="S2.p3.2.m2.1.2.3.2.1" xref="S2.p3.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">â‹…</mo><mo stretchy="false" id="S2.p3.2.m2.1.2.3.2.2" xref="S2.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.2.cmml" xref="S2.p3.2.m2.1.2"><times id="S2.p3.2.m2.1.2.1.cmml" xref="S2.p3.2.m2.1.2.1"></times><apply id="S2.p3.2.m2.1.2.2.cmml" xref="S2.p3.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.2.2.1.cmml" xref="S2.p3.2.m2.1.2.2">subscript</csymbol><ci id="S2.p3.2.m2.1.2.2.2.cmml" xref="S2.p3.2.m2.1.2.2.2">ğ¹</ci><ci id="S2.p3.2.m2.1.2.2.3.cmml" xref="S2.p3.2.m2.1.2.2.3">ğ‘˜</ci></apply><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">F_{k}(\cdot)</annotation></semantics></math>, and the local solver is the stochastic gradient descent (SGD), with the same learning rate (<math id="S2.p3.3.m3.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">\eta</annotation></semantics></math>) and number of local epochs used on each client. At each round, a subset <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">K</annotation></semantics></math> <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.p3.5.m5.1a"><mo id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><lt id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">&lt;</annotation></semantics></math> <math id="S2.p3.6.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p3.6.m6.1a"><mi id="S2.p3.6.m6.1.1" xref="S2.p3.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p3.6.m6.1b"><ci id="S2.p3.6.m6.1.1.cmml" xref="S2.p3.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.6.m6.1c">N</annotation></semantics></math> of the total clients are selected and run SGD locally for <math id="S2.p3.7.m7.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S2.p3.7.m7.1a"><msub id="S2.p3.7.m7.1.1" xref="S2.p3.7.m7.1.1.cmml"><mi id="S2.p3.7.m7.1.1.2" xref="S2.p3.7.m7.1.1.2.cmml">E</mi><mi id="S2.p3.7.m7.1.1.3" xref="S2.p3.7.m7.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p3.7.m7.1b"><apply id="S2.p3.7.m7.1.1.cmml" xref="S2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.p3.7.m7.1.1.1.cmml" xref="S2.p3.7.m7.1.1">subscript</csymbol><ci id="S2.p3.7.m7.1.1.2.cmml" xref="S2.p3.7.m7.1.1.2">ğ¸</ci><ci id="S2.p3.7.m7.1.1.3.cmml" xref="S2.p3.7.m7.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.7.m7.1c">E_{p}</annotation></semantics></math> number of epochs, and then the resulting model updates are averaged. The details are summarized below.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated Averaging Algorithm</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">Â Â <span id="alg1.l1.1" class="ltx_text ltx_font_bold">Input:</span> <math id="alg1.l1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg1.l1.m1.1a"><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">K</annotation></semantics></math>, <math id="alg1.l1.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="alg1.l1.m2.1a"><mi id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">T</annotation></semantics></math>, <math id="alg1.l1.m3.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.l1.m3.1a"><mi id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><ci id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">\eta</annotation></semantics></math>, <math id="alg1.l1.m4.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="alg1.l1.m4.1a"><msub id="alg1.l1.m4.1.1" xref="alg1.l1.m4.1.1.cmml"><mi id="alg1.l1.m4.1.1.2" xref="alg1.l1.m4.1.1.2.cmml">E</mi><mi id="alg1.l1.m4.1.1.3" xref="alg1.l1.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m4.1b"><apply id="alg1.l1.m4.1.1.cmml" xref="alg1.l1.m4.1.1"><csymbol cd="ambiguous" id="alg1.l1.m4.1.1.1.cmml" xref="alg1.l1.m4.1.1">subscript</csymbol><ci id="alg1.l1.m4.1.1.2.cmml" xref="alg1.l1.m4.1.1.2">ğ¸</ci><ci id="alg1.l1.m4.1.1.3.cmml" xref="alg1.l1.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m4.1c">E_{p}</annotation></semantics></math>, <math id="alg1.l1.m5.1" class="ltx_Math" alttext="w^{0}" display="inline"><semantics id="alg1.l1.m5.1a"><msup id="alg1.l1.m5.1.1" xref="alg1.l1.m5.1.1.cmml"><mi id="alg1.l1.m5.1.1.2" xref="alg1.l1.m5.1.1.2.cmml">w</mi><mn id="alg1.l1.m5.1.1.3" xref="alg1.l1.m5.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="alg1.l1.m5.1b"><apply id="alg1.l1.m5.1.1.cmml" xref="alg1.l1.m5.1.1"><csymbol cd="ambiguous" id="alg1.l1.m5.1.1.1.cmml" xref="alg1.l1.m5.1.1">superscript</csymbol><ci id="alg1.l1.m5.1.1.2.cmml" xref="alg1.l1.m5.1.1.2">ğ‘¤</ci><cn type="integer" id="alg1.l1.m5.1.1.3.cmml" xref="alg1.l1.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m5.1c">w^{0}</annotation></semantics></math>, <math id="alg1.l1.m6.1" class="ltx_Math" alttext="N" display="inline"><semantics id="alg1.l1.m6.1a"><mi id="alg1.l1.m6.1.1" xref="alg1.l1.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m6.1b"><ci id="alg1.l1.m6.1.1.cmml" xref="alg1.l1.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m6.1c">N</annotation></semantics></math>, <math id="alg1.l1.m7.1" class="ltx_Math" alttext="p_{k}" display="inline"><semantics id="alg1.l1.m7.1a"><msub id="alg1.l1.m7.1.1" xref="alg1.l1.m7.1.1.cmml"><mi id="alg1.l1.m7.1.1.2" xref="alg1.l1.m7.1.1.2.cmml">p</mi><mi id="alg1.l1.m7.1.1.3" xref="alg1.l1.m7.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l1.m7.1b"><apply id="alg1.l1.m7.1.1.cmml" xref="alg1.l1.m7.1.1"><csymbol cd="ambiguous" id="alg1.l1.m7.1.1.1.cmml" xref="alg1.l1.m7.1.1">subscript</csymbol><ci id="alg1.l1.m7.1.1.2.cmml" xref="alg1.l1.m7.1.1.2">ğ‘</ci><ci id="alg1.l1.m7.1.1.3.cmml" xref="alg1.l1.m7.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m7.1c">p_{k}</annotation></semantics></math>, <math id="alg1.l1.m8.3" class="ltx_Math" alttext="k=1,...,N" display="inline"><semantics id="alg1.l1.m8.3a"><mrow id="alg1.l1.m8.3.4" xref="alg1.l1.m8.3.4.cmml"><mi id="alg1.l1.m8.3.4.2" xref="alg1.l1.m8.3.4.2.cmml">k</mi><mo id="alg1.l1.m8.3.4.1" xref="alg1.l1.m8.3.4.1.cmml">=</mo><mrow id="alg1.l1.m8.3.4.3.2" xref="alg1.l1.m8.3.4.3.1.cmml"><mn id="alg1.l1.m8.1.1" xref="alg1.l1.m8.1.1.cmml">1</mn><mo id="alg1.l1.m8.3.4.3.2.1" xref="alg1.l1.m8.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="alg1.l1.m8.2.2" xref="alg1.l1.m8.2.2.cmml">â€¦</mi><mo id="alg1.l1.m8.3.4.3.2.2" xref="alg1.l1.m8.3.4.3.1.cmml">,</mo><mi id="alg1.l1.m8.3.3" xref="alg1.l1.m8.3.3.cmml">N</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m8.3b"><apply id="alg1.l1.m8.3.4.cmml" xref="alg1.l1.m8.3.4"><eq id="alg1.l1.m8.3.4.1.cmml" xref="alg1.l1.m8.3.4.1"></eq><ci id="alg1.l1.m8.3.4.2.cmml" xref="alg1.l1.m8.3.4.2">ğ‘˜</ci><list id="alg1.l1.m8.3.4.3.1.cmml" xref="alg1.l1.m8.3.4.3.2"><cn type="integer" id="alg1.l1.m8.1.1.cmml" xref="alg1.l1.m8.1.1">1</cn><ci id="alg1.l1.m8.2.2.cmml" xref="alg1.l1.m8.2.2">â€¦</ci><ci id="alg1.l1.m8.3.3.cmml" xref="alg1.l1.m8.3.3">ğ‘</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m8.3c">k=1,...,N</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">Â Â <span id="alg1.l2.1" class="ltx_text ltx_font_bold">for</span>Â <math id="alg1.l2.m1.1" class="ltx_Math" alttext="t=1" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">t</mi><mo id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">=</mo><mn id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><eq id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1"></eq><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">ğ‘¡</ci><cn type="integer" id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">t=1</annotation></semantics></math> <span id="alg1.l2.2" class="ltx_text ltx_font_bold">to</span> <math id="alg1.l2.m2.1" class="ltx_Math" alttext="T-1" display="inline"><semantics id="alg1.l2.m2.1a"><mrow id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml"><mi id="alg1.l2.m2.1.1.2" xref="alg1.l2.m2.1.1.2.cmml">T</mi><mo id="alg1.l2.m2.1.1.1" xref="alg1.l2.m2.1.1.1.cmml">âˆ’</mo><mn id="alg1.l2.m2.1.1.3" xref="alg1.l2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><apply id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1"><minus id="alg1.l2.m2.1.1.1.cmml" xref="alg1.l2.m2.1.1.1"></minus><ci id="alg1.l2.m2.1.1.2.cmml" xref="alg1.l2.m2.1.1.2">ğ‘‡</ci><cn type="integer" id="alg1.l2.m2.1.1.3.cmml" xref="alg1.l2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">T-1</annotation></semantics></math>Â <span id="alg1.l2.3" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="alg1.l3" class="ltx_listingline">Â Â Â Â Â Server selects a subset <math id="alg1.l3.m1.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="alg1.l3.m1.1a"><msub id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">S</mi><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><csymbol cd="ambiguous" id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1">subscript</csymbol><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">ğ‘†</ci><ci id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">S_{t}</annotation></semantics></math> of <math id="alg1.l3.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg1.l3.m2.1a"><mi id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">K</annotation></semantics></math> clients at random.

</div>
<div id="alg1.l4" class="ltx_listingline">Â Â Â Â Â Server sends <math id="alg1.l4.m1.1" class="ltx_Math" alttext="w^{t}" display="inline"><semantics id="alg1.l4.m1.1a"><msup id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">w</mi><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><csymbol cd="ambiguous" id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1">superscript</csymbol><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">ğ‘¤</ci><ci id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">w^{t}</annotation></semantics></math> to all chosen clients.

</div>
<div id="alg1.l5" class="ltx_listingline">Â Â Â Â Â Each client <math id="alg1.l5.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l5.m1.1a"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">k</annotation></semantics></math> <math id="alg1.l5.m2.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="alg1.l5.m2.1a"><mo id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">âˆˆ</mo><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><in id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">\in</annotation></semantics></math> <math id="alg1.l5.m3.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="alg1.l5.m3.1a"><msub id="alg1.l5.m3.1.1" xref="alg1.l5.m3.1.1.cmml"><mi id="alg1.l5.m3.1.1.2" xref="alg1.l5.m3.1.1.2.cmml">S</mi><mi id="alg1.l5.m3.1.1.3" xref="alg1.l5.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m3.1b"><apply id="alg1.l5.m3.1.1.cmml" xref="alg1.l5.m3.1.1"><csymbol cd="ambiguous" id="alg1.l5.m3.1.1.1.cmml" xref="alg1.l5.m3.1.1">subscript</csymbol><ci id="alg1.l5.m3.1.1.2.cmml" xref="alg1.l5.m3.1.1.2">ğ‘†</ci><ci id="alg1.l5.m3.1.1.3.cmml" xref="alg1.l5.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m3.1c">S_{t}</annotation></semantics></math> updates <math id="alg1.l5.m4.1" class="ltx_Math" alttext="w^{t}" display="inline"><semantics id="alg1.l5.m4.1a"><msup id="alg1.l5.m4.1.1" xref="alg1.l5.m4.1.1.cmml"><mi id="alg1.l5.m4.1.1.2" xref="alg1.l5.m4.1.1.2.cmml">w</mi><mi id="alg1.l5.m4.1.1.3" xref="alg1.l5.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="alg1.l5.m4.1b"><apply id="alg1.l5.m4.1.1.cmml" xref="alg1.l5.m4.1.1"><csymbol cd="ambiguous" id="alg1.l5.m4.1.1.1.cmml" xref="alg1.l5.m4.1.1">superscript</csymbol><ci id="alg1.l5.m4.1.1.2.cmml" xref="alg1.l5.m4.1.1.2">ğ‘¤</ci><ci id="alg1.l5.m4.1.1.3.cmml" xref="alg1.l5.m4.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m4.1c">w^{t}</annotation></semantics></math> for <math id="alg1.l5.m5.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="alg1.l5.m5.1a"><msub id="alg1.l5.m5.1.1" xref="alg1.l5.m5.1.1.cmml"><mi id="alg1.l5.m5.1.1.2" xref="alg1.l5.m5.1.1.2.cmml">E</mi><mi id="alg1.l5.m5.1.1.3" xref="alg1.l5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m5.1b"><apply id="alg1.l5.m5.1.1.cmml" xref="alg1.l5.m5.1.1"><csymbol cd="ambiguous" id="alg1.l5.m5.1.1.1.cmml" xref="alg1.l5.m5.1.1">subscript</csymbol><ci id="alg1.l5.m5.1.1.2.cmml" xref="alg1.l5.m5.1.1.2">ğ¸</ci><ci id="alg1.l5.m5.1.1.3.cmml" xref="alg1.l5.m5.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m5.1c">E_{p}</annotation></semantics></math> epochs of SGD on <math id="alg1.l5.m6.1" class="ltx_Math" alttext="F_{k}" display="inline"><semantics id="alg1.l5.m6.1a"><msub id="alg1.l5.m6.1.1" xref="alg1.l5.m6.1.1.cmml"><mi id="alg1.l5.m6.1.1.2" xref="alg1.l5.m6.1.1.2.cmml">F</mi><mi id="alg1.l5.m6.1.1.3" xref="alg1.l5.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l5.m6.1b"><apply id="alg1.l5.m6.1.1.cmml" xref="alg1.l5.m6.1.1"><csymbol cd="ambiguous" id="alg1.l5.m6.1.1.1.cmml" xref="alg1.l5.m6.1.1">subscript</csymbol><ci id="alg1.l5.m6.1.1.2.cmml" xref="alg1.l5.m6.1.1.2">ğ¹</ci><ci id="alg1.l5.m6.1.1.3.cmml" xref="alg1.l5.m6.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m6.1c">F_{k}</annotation></semantics></math> with step size <math id="alg1.l5.m7.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="alg1.l5.m7.1a"><mi id="alg1.l5.m7.1.1" xref="alg1.l5.m7.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="alg1.l5.m7.1b"><ci id="alg1.l5.m7.1.1.cmml" xref="alg1.l5.m7.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m7.1c">\eta</annotation></semantics></math> to obtain <math id="alg1.l5.m8.1" class="ltx_Math" alttext="w_{k}^{t+1}" display="inline"><semantics id="alg1.l5.m8.1a"><msubsup id="alg1.l5.m8.1.1" xref="alg1.l5.m8.1.1.cmml"><mi id="alg1.l5.m8.1.1.2.2" xref="alg1.l5.m8.1.1.2.2.cmml">w</mi><mi id="alg1.l5.m8.1.1.2.3" xref="alg1.l5.m8.1.1.2.3.cmml">k</mi><mrow id="alg1.l5.m8.1.1.3" xref="alg1.l5.m8.1.1.3.cmml"><mi id="alg1.l5.m8.1.1.3.2" xref="alg1.l5.m8.1.1.3.2.cmml">t</mi><mo id="alg1.l5.m8.1.1.3.1" xref="alg1.l5.m8.1.1.3.1.cmml">+</mo><mn id="alg1.l5.m8.1.1.3.3" xref="alg1.l5.m8.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="alg1.l5.m8.1b"><apply id="alg1.l5.m8.1.1.cmml" xref="alg1.l5.m8.1.1"><csymbol cd="ambiguous" id="alg1.l5.m8.1.1.1.cmml" xref="alg1.l5.m8.1.1">superscript</csymbol><apply id="alg1.l5.m8.1.1.2.cmml" xref="alg1.l5.m8.1.1"><csymbol cd="ambiguous" id="alg1.l5.m8.1.1.2.1.cmml" xref="alg1.l5.m8.1.1">subscript</csymbol><ci id="alg1.l5.m8.1.1.2.2.cmml" xref="alg1.l5.m8.1.1.2.2">ğ‘¤</ci><ci id="alg1.l5.m8.1.1.2.3.cmml" xref="alg1.l5.m8.1.1.2.3">ğ‘˜</ci></apply><apply id="alg1.l5.m8.1.1.3.cmml" xref="alg1.l5.m8.1.1.3"><plus id="alg1.l5.m8.1.1.3.1.cmml" xref="alg1.l5.m8.1.1.3.1"></plus><ci id="alg1.l5.m8.1.1.3.2.cmml" xref="alg1.l5.m8.1.1.3.2">ğ‘¡</ci><cn type="integer" id="alg1.l5.m8.1.1.3.3.cmml" xref="alg1.l5.m8.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m8.1c">w_{k}^{t+1}</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">Â Â Â Â Â Each client <math id="alg1.l6.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l6.m1.1a"><mi id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><ci id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">k</annotation></semantics></math> <math id="alg1.l6.m2.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="alg1.l6.m2.1a"><mo id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">âˆˆ</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><in id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">\in</annotation></semantics></math> <math id="alg1.l6.m3.1" class="ltx_Math" alttext="S_{t}" display="inline"><semantics id="alg1.l6.m3.1a"><msub id="alg1.l6.m3.1.1" xref="alg1.l6.m3.1.1.cmml"><mi id="alg1.l6.m3.1.1.2" xref="alg1.l6.m3.1.1.2.cmml">S</mi><mi id="alg1.l6.m3.1.1.3" xref="alg1.l6.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m3.1b"><apply id="alg1.l6.m3.1.1.cmml" xref="alg1.l6.m3.1.1"><csymbol cd="ambiguous" id="alg1.l6.m3.1.1.1.cmml" xref="alg1.l6.m3.1.1">subscript</csymbol><ci id="alg1.l6.m3.1.1.2.cmml" xref="alg1.l6.m3.1.1.2">ğ‘†</ci><ci id="alg1.l6.m3.1.1.3.cmml" xref="alg1.l6.m3.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m3.1c">S_{t}</annotation></semantics></math> sends <math id="alg1.l6.m4.1" class="ltx_Math" alttext="w_{k}^{t+1}" display="inline"><semantics id="alg1.l6.m4.1a"><msubsup id="alg1.l6.m4.1.1" xref="alg1.l6.m4.1.1.cmml"><mi id="alg1.l6.m4.1.1.2.2" xref="alg1.l6.m4.1.1.2.2.cmml">w</mi><mi id="alg1.l6.m4.1.1.2.3" xref="alg1.l6.m4.1.1.2.3.cmml">k</mi><mrow id="alg1.l6.m4.1.1.3" xref="alg1.l6.m4.1.1.3.cmml"><mi id="alg1.l6.m4.1.1.3.2" xref="alg1.l6.m4.1.1.3.2.cmml">t</mi><mo id="alg1.l6.m4.1.1.3.1" xref="alg1.l6.m4.1.1.3.1.cmml">+</mo><mn id="alg1.l6.m4.1.1.3.3" xref="alg1.l6.m4.1.1.3.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="alg1.l6.m4.1b"><apply id="alg1.l6.m4.1.1.cmml" xref="alg1.l6.m4.1.1"><csymbol cd="ambiguous" id="alg1.l6.m4.1.1.1.cmml" xref="alg1.l6.m4.1.1">superscript</csymbol><apply id="alg1.l6.m4.1.1.2.cmml" xref="alg1.l6.m4.1.1"><csymbol cd="ambiguous" id="alg1.l6.m4.1.1.2.1.cmml" xref="alg1.l6.m4.1.1">subscript</csymbol><ci id="alg1.l6.m4.1.1.2.2.cmml" xref="alg1.l6.m4.1.1.2.2">ğ‘¤</ci><ci id="alg1.l6.m4.1.1.2.3.cmml" xref="alg1.l6.m4.1.1.2.3">ğ‘˜</ci></apply><apply id="alg1.l6.m4.1.1.3.cmml" xref="alg1.l6.m4.1.1.3"><plus id="alg1.l6.m4.1.1.3.1.cmml" xref="alg1.l6.m4.1.1.3.1"></plus><ci id="alg1.l6.m4.1.1.3.2.cmml" xref="alg1.l6.m4.1.1.3.2">ğ‘¡</ci><cn type="integer" id="alg1.l6.m4.1.1.3.3.cmml" xref="alg1.l6.m4.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m4.1c">w_{k}^{t+1}</annotation></semantics></math> back to server

</div>
<div id="alg1.l7" class="ltx_listingline">Â Â Â Â Â Server aggregates the <math id="alg1.l7.m1.1" class="ltx_Math" alttext="w" display="inline"><semantics id="alg1.l7.m1.1a"><mi id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><ci id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1">ğ‘¤</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">w</annotation></semantics></math>â€™s as <math id="alg1.l7.m2.1" class="ltx_Math" alttext="w^{t+1}=\frac{1}{K}\sum_{k\in S_{t}}w_{k}^{t+1}" display="inline"><semantics id="alg1.l7.m2.1a"><mrow id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml"><msup id="alg1.l7.m2.1.1.2" xref="alg1.l7.m2.1.1.2.cmml"><mi id="alg1.l7.m2.1.1.2.2" xref="alg1.l7.m2.1.1.2.2.cmml">w</mi><mrow id="alg1.l7.m2.1.1.2.3" xref="alg1.l7.m2.1.1.2.3.cmml"><mi id="alg1.l7.m2.1.1.2.3.2" xref="alg1.l7.m2.1.1.2.3.2.cmml">t</mi><mo id="alg1.l7.m2.1.1.2.3.1" xref="alg1.l7.m2.1.1.2.3.1.cmml">+</mo><mn id="alg1.l7.m2.1.1.2.3.3" xref="alg1.l7.m2.1.1.2.3.3.cmml">1</mn></mrow></msup><mo id="alg1.l7.m2.1.1.1" xref="alg1.l7.m2.1.1.1.cmml">=</mo><mrow id="alg1.l7.m2.1.1.3" xref="alg1.l7.m2.1.1.3.cmml"><mfrac id="alg1.l7.m2.1.1.3.2" xref="alg1.l7.m2.1.1.3.2.cmml"><mn id="alg1.l7.m2.1.1.3.2.2" xref="alg1.l7.m2.1.1.3.2.2.cmml">1</mn><mi id="alg1.l7.m2.1.1.3.2.3" xref="alg1.l7.m2.1.1.3.2.3.cmml">K</mi></mfrac><mo lspace="0em" rspace="0em" id="alg1.l7.m2.1.1.3.1" xref="alg1.l7.m2.1.1.3.1.cmml">â€‹</mo><mrow id="alg1.l7.m2.1.1.3.3" xref="alg1.l7.m2.1.1.3.3.cmml"><msub id="alg1.l7.m2.1.1.3.3.1" xref="alg1.l7.m2.1.1.3.3.1.cmml"><mo id="alg1.l7.m2.1.1.3.3.1.2" xref="alg1.l7.m2.1.1.3.3.1.2.cmml">âˆ‘</mo><mrow id="alg1.l7.m2.1.1.3.3.1.3" xref="alg1.l7.m2.1.1.3.3.1.3.cmml"><mi id="alg1.l7.m2.1.1.3.3.1.3.2" xref="alg1.l7.m2.1.1.3.3.1.3.2.cmml">k</mi><mo id="alg1.l7.m2.1.1.3.3.1.3.1" xref="alg1.l7.m2.1.1.3.3.1.3.1.cmml">âˆˆ</mo><msub id="alg1.l7.m2.1.1.3.3.1.3.3" xref="alg1.l7.m2.1.1.3.3.1.3.3.cmml"><mi id="alg1.l7.m2.1.1.3.3.1.3.3.2" xref="alg1.l7.m2.1.1.3.3.1.3.3.2.cmml">S</mi><mi id="alg1.l7.m2.1.1.3.3.1.3.3.3" xref="alg1.l7.m2.1.1.3.3.1.3.3.3.cmml">t</mi></msub></mrow></msub><msubsup id="alg1.l7.m2.1.1.3.3.2" xref="alg1.l7.m2.1.1.3.3.2.cmml"><mi id="alg1.l7.m2.1.1.3.3.2.2.2" xref="alg1.l7.m2.1.1.3.3.2.2.2.cmml">w</mi><mi id="alg1.l7.m2.1.1.3.3.2.2.3" xref="alg1.l7.m2.1.1.3.3.2.2.3.cmml">k</mi><mrow id="alg1.l7.m2.1.1.3.3.2.3" xref="alg1.l7.m2.1.1.3.3.2.3.cmml"><mi id="alg1.l7.m2.1.1.3.3.2.3.2" xref="alg1.l7.m2.1.1.3.3.2.3.2.cmml">t</mi><mo id="alg1.l7.m2.1.1.3.3.2.3.1" xref="alg1.l7.m2.1.1.3.3.2.3.1.cmml">+</mo><mn id="alg1.l7.m2.1.1.3.3.2.3.3" xref="alg1.l7.m2.1.1.3.3.2.3.3.cmml">1</mn></mrow></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><apply id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1"><eq id="alg1.l7.m2.1.1.1.cmml" xref="alg1.l7.m2.1.1.1"></eq><apply id="alg1.l7.m2.1.1.2.cmml" xref="alg1.l7.m2.1.1.2"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.2.1.cmml" xref="alg1.l7.m2.1.1.2">superscript</csymbol><ci id="alg1.l7.m2.1.1.2.2.cmml" xref="alg1.l7.m2.1.1.2.2">ğ‘¤</ci><apply id="alg1.l7.m2.1.1.2.3.cmml" xref="alg1.l7.m2.1.1.2.3"><plus id="alg1.l7.m2.1.1.2.3.1.cmml" xref="alg1.l7.m2.1.1.2.3.1"></plus><ci id="alg1.l7.m2.1.1.2.3.2.cmml" xref="alg1.l7.m2.1.1.2.3.2">ğ‘¡</ci><cn type="integer" id="alg1.l7.m2.1.1.2.3.3.cmml" xref="alg1.l7.m2.1.1.2.3.3">1</cn></apply></apply><apply id="alg1.l7.m2.1.1.3.cmml" xref="alg1.l7.m2.1.1.3"><times id="alg1.l7.m2.1.1.3.1.cmml" xref="alg1.l7.m2.1.1.3.1"></times><apply id="alg1.l7.m2.1.1.3.2.cmml" xref="alg1.l7.m2.1.1.3.2"><divide id="alg1.l7.m2.1.1.3.2.1.cmml" xref="alg1.l7.m2.1.1.3.2"></divide><cn type="integer" id="alg1.l7.m2.1.1.3.2.2.cmml" xref="alg1.l7.m2.1.1.3.2.2">1</cn><ci id="alg1.l7.m2.1.1.3.2.3.cmml" xref="alg1.l7.m2.1.1.3.2.3">ğ¾</ci></apply><apply id="alg1.l7.m2.1.1.3.3.cmml" xref="alg1.l7.m2.1.1.3.3"><apply id="alg1.l7.m2.1.1.3.3.1.cmml" xref="alg1.l7.m2.1.1.3.3.1"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.3.3.1.1.cmml" xref="alg1.l7.m2.1.1.3.3.1">subscript</csymbol><sum id="alg1.l7.m2.1.1.3.3.1.2.cmml" xref="alg1.l7.m2.1.1.3.3.1.2"></sum><apply id="alg1.l7.m2.1.1.3.3.1.3.cmml" xref="alg1.l7.m2.1.1.3.3.1.3"><in id="alg1.l7.m2.1.1.3.3.1.3.1.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.1"></in><ci id="alg1.l7.m2.1.1.3.3.1.3.2.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.2">ğ‘˜</ci><apply id="alg1.l7.m2.1.1.3.3.1.3.3.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.3"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.3.3.1.3.3.1.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.3">subscript</csymbol><ci id="alg1.l7.m2.1.1.3.3.1.3.3.2.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.3.2">ğ‘†</ci><ci id="alg1.l7.m2.1.1.3.3.1.3.3.3.cmml" xref="alg1.l7.m2.1.1.3.3.1.3.3.3">ğ‘¡</ci></apply></apply></apply><apply id="alg1.l7.m2.1.1.3.3.2.cmml" xref="alg1.l7.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.3.3.2.1.cmml" xref="alg1.l7.m2.1.1.3.3.2">superscript</csymbol><apply id="alg1.l7.m2.1.1.3.3.2.2.cmml" xref="alg1.l7.m2.1.1.3.3.2"><csymbol cd="ambiguous" id="alg1.l7.m2.1.1.3.3.2.2.1.cmml" xref="alg1.l7.m2.1.1.3.3.2">subscript</csymbol><ci id="alg1.l7.m2.1.1.3.3.2.2.2.cmml" xref="alg1.l7.m2.1.1.3.3.2.2.2">ğ‘¤</ci><ci id="alg1.l7.m2.1.1.3.3.2.2.3.cmml" xref="alg1.l7.m2.1.1.3.3.2.2.3">ğ‘˜</ci></apply><apply id="alg1.l7.m2.1.1.3.3.2.3.cmml" xref="alg1.l7.m2.1.1.3.3.2.3"><plus id="alg1.l7.m2.1.1.3.3.2.3.1.cmml" xref="alg1.l7.m2.1.1.3.3.2.3.1"></plus><ci id="alg1.l7.m2.1.1.3.3.2.3.2.cmml" xref="alg1.l7.m2.1.1.3.3.2.3.2">ğ‘¡</ci><cn type="integer" id="alg1.l7.m2.1.1.3.3.2.3.3.cmml" xref="alg1.l7.m2.1.1.3.3.2.3.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">w^{t+1}=\frac{1}{K}\sum_{k\in S_{t}}w_{k}^{t+1}</annotation></semantics></math>

</div>
<div id="alg1.l8" class="ltx_listingline">Â Â <span id="alg1.l8.1" class="ltx_text ltx_font_bold">end</span>Â <span id="alg1.l8.2" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">Tuning of the hyper-parameters is a critical requirement for optimal performance of FedAvg. The number of epochs plays a critical role in convergence as more number of epochs leads to faster convergence. This comes at the cost of higher compute on client devices but with the benefit of lower communication. However, the high number of epochs has diminishing returns on the speed of convergence in non-IID conditions. For FedAvg, there is a significant drop in reduction of accuracy due to weight divergence <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>. The trade-off between high number of epochs and convergence speed for FedAvg has been addressed in other optimization algorithms like FedProx, FedMA, FedMAX etc. FedProx is very similar to FedAvg but addresses the limitations of the latter by adding a proximal term to client cost functions to limit the impact of local updates within a particular range of global model. This approach allows the number of epochs to be tuned based on the non-IIDness of the client data. While it address the weight divergence issue with FedAvg, the convergence speed is slower at higher number of epochs when compared to other state-of-the-art algorithms <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Shoham etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. FedMA offers the best accuracy and convergence speed in comparison to others but comes with significant compute cost on the client devices. The complexity of this algorithm is also high in comparison with FedAvg or FedProx leading to restrictions on its applicability on certain NN models.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Tuning of the hyper-parameters is a critical requirement for optimal performance of FedAvg. The number of epochs plays a critical role in convergence as more number of epochs leads to faster convergence. This comes at the cost of higher compute on client devices but with the benefit of lower communication. However, the high number of epochs has diminishing returns on the speed of convergence in non-IID conditions. For FedAvg, there is a significant drop in reduction of accuracy due to weight divergence <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite>. The trade-off between high number of epochs and convergence speed for FedAvg has been addressed in other optimization algorithms like FedProx, FedMA, FedMAX etc. FedProx is very similar to FedAvg but addresses the limitations of the latter by adding a proximal term to client cost functions to limit the impact of local updates within a particular range of global model. This approach allows the number of epochs to be tuned based on the non-IIDness of the client data. While it address the weight divergence issue with FedAvg, the convergence speed is slower at higher number of epochs when compared to other state-of-the-art algorithms <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2020</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> <cite class="ltx_cite ltx_citemacro_citep">(Shoham etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite>. FedMA offers the best accuracy and convergence speed in comparison to others but comes with significant compute cost on the client devices. The complexity of this algorithm is also high in comparison with FedAvg or FedProx leading to restrictions on its applicability on certain NN models.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">An ideal optimization algorithm should come with the simplicity and elegance of FedAvg, allow for state-of-the-art accuracy in non-IID environments with comparable or better convergence speed. In this work, we present a novel Federated Training methodology that is well suited to handle non-IID challenges using the simple FedAvg algorithm. Our methodology eliminates performance overheads associated with methods like FedMA while achieving comparable accuracy. Since FedAvg is the de-facto standard in majority production deployments, the proposed method can be easily integrated to offer significant accuracy and convergence benefits with little performance overhead.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Note on the terminology: In the rest of the document, clients will be referred to as <span id="S2.p7.1.1" class="ltx_text ltx_font_italic">collaborators</span> and the server will be referred to as aggregator reflecting the role they play in the overall federation.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Divide-and-Conquer Training Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The impact of non-IIDness of data in Federated Learning is well researched in literature. A non-IID data environment leads to over-fitting of local models to the skewed training data at individual collaborators resulting in distortion of previously aggregated feature detectors and descent of SGD optimizer to different local minima at different collaborators.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<br class="ltx_break">
<p id="S3.F1.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F1.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/ticktock_overview.png" id="S3.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="375" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>DNN Layer Significance - VGG9 Image Classification Topology</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Typically, the initial layers of a Deep Neural Network (DNN) learn low level or class agnostic features and deeper layers are responsible for learning high level or class-specific features <cite class="ltx_cite ltx_citemacro_citep">(Zeiler &amp; Fergus, <a href="#bib.bib20" title="" class="ltx_ref">2014</a>)</cite>, as illustrated for a vision architecture, VGG9 <cite class="ltx_cite ltx_citemacro_citep">(Simonyan &amp; Zisserman, <a href="#bib.bib16" title="" class="ltx_ref">2015</a>)</cite>, in Figure <a href="#S3.F1" title="Figure 1 â€£ 3 Divide-and-Conquer Training Methodology â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. For training paradigms like Transfer Learning <cite class="ltx_cite ltx_citemacro_citep">(Reyes etÂ al., <a href="#bib.bib13" title="" class="ltx_ref">2015</a>)</cite>, data scarcity mandates the use of special training methods that learn class agnostic features from generic datasets and learn class specific features for any new tasks by freezing the initial layers. This process of decoupling <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">task-learning</span> has been successfully applied to multiple training tasks including recent advances like Few Shot Learning <cite class="ltx_cite ltx_citemacro_citep">(Yan etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2020</a>)</cite>. This work extends the idea to Federated Learning to address the challenges with non-IID. Our methodology involves splitting the given DNN into two parts, namely</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Class Agnostic Layers</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Class Specific Layers</p>
</div>
</li>
</ul>
<p id="S3.p2.2" class="ltx_p">The two parts are trained separately. Federated Learning is typically performed using several Communication Rounds (CR), where trained weights from individual collaborators are aggregated together in a central Aggregator. Our proposed method configures collaborators to perform <span id="S3.p2.2.1" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S3.p2.2.2" class="ltx_text ltx_font_italic">task-learning</span> or <span id="S3.p2.2.3" class="ltx_text ltx_font_italic">fine-tuning</span> in alternate rounds as shown in Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Divide-and-Conquer Training Methodology â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Weights corresponding to relevant trained layers alone are transferred over to the Aggregator, which results in communication bandwidth reduction. Communication saving is realized during model transfers in both directions (i) Transfer of global models to Collaborators and (ii) Transfer of local trained models from Collaborator to the Aggregator.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<br class="ltx_break">
<p id="S3.F2.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F2.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/ticktock_fig.png" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="152" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Divide and Conquer Training Methodology using alternate Feature-Learning and Fine-Tuning rounds. CR1, C2,. represent the communication rounds.</figcaption>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Class Agnostic layers, comprised of initial layers of the DNN architecture, are trained more aggressively as compared to Class-Specific layers. Class Agnostic layer training is treated similar to <span id="S3.p3.1.1" class="ltx_text ltx_font_italic">feature-learning</span>. Class Specific layers, consisting of deep layers are trained similar to <span id="S3.p3.1.2" class="ltx_text ltx_font_italic">fine-tuning</span>. This ensures that weight divergence across different collaborators, due to non-IIDness of constituent data is minimal and features are insulated from distortion that would otherwise occur due to combined learning of all layers.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">While methods like FedProx limit weight divergence, they penalize all layers of the network and hinder learning in Class Agnostic layers. Our approach addresses this by allowing different layers of a network to train differently after grouping initial layers separately from deep layers. Training rounds are configured to alternate between <span id="S3.p4.1.1" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S3.p4.1.2" class="ltx_text ltx_font_italic">fine-tuning</span> to facilitate learning under non-IID conditions by freezing relevant layers of DNN architecture. At the beginning of a communication round, the aggregator broadcasts the desired hyper-parameter configurations to collaborators, together with specifications for layers to be frozen. The exact point at which a DNN architecture has to be broken into two parts is decided based on <span id="S3.p4.1.3" class="ltx_text ltx_font_italic">weight divergence</span> observed from the pre-pass round of training. The key contributions of our paper can be summarized to the following two key points:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Novel methodology, called <span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span>, to train topology in pairs of <span id="S3.I2.i1.p1.1.2" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S3.I2.i1.p1.1.3" class="ltx_text ltx_font_italic">fine-tuning</span> steps to handle non-IID conditions.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Novel use of <span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">weight-divergence</span> metric, observed from the pre-pass round of training, to split the given DNN topology into Class Agnostic and Class Specific layers. This metric provides a measure of non-IIDness across participating collaborators as a mapping of the layers of DNN architecture they impact the most.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">Choice of layers that are chosen for base class <span id="S3.p5.1.1" class="ltx_text ltx_font_italic">feature-learning</span> as against novel class <span id="S3.p5.1.2" class="ltx_text ltx_font_italic">fine-tuning</span> is a hyper-parameter in <span id="S3.p5.1.3" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> training methodology. Few options for splitting the VGG9 topology is shown in Figure <a href="#S3.F3" title="Figure 3 â€£ 3 Divide-and-Conquer Training Methodology â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For instance, Divide at layer3 assigns layers 1 to 3 for learning class agnostic features and remaining layers for learning class or task specific features. This hyper-parameter is dependent on the <span id="S3.p5.1.4" class="ltx_text ltx_font_italic">weight-divergence</span> metric which in turn reflects the non-IIDness of data.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<br class="ltx_break">
<p id="S3.F3.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S3.F3.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/LayerSplitMethodology.png" id="S3.F3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="275" height="310" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>VGG9 : Topology Division and assignment of layers to Feature-Learning and Fine-Tuning Groups</figcaption>
</figure>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.4" class="ltx_p">After determining an optimal split, <span id="S3.p6.4.1" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S3.p6.4.2" class="ltx_text ltx_font_italic">fine-tuning</span> is achieved by control of other hyper-parameters like number of Epochs (<math id="S3.p6.1.m1.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S3.p6.1.m1.1a"><msub id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml"><mi id="S3.p6.1.m1.1.1.2" xref="S3.p6.1.m1.1.1.2.cmml">E</mi><mi id="S3.p6.1.m1.1.1.3" xref="S3.p6.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><apply id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p6.1.m1.1.1.1.cmml" xref="S3.p6.1.m1.1.1">subscript</csymbol><ci id="S3.p6.1.m1.1.1.2.cmml" xref="S3.p6.1.m1.1.1.2">ğ¸</ci><ci id="S3.p6.1.m1.1.1.3.cmml" xref="S3.p6.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">E_{p}</annotation></semantics></math>) and Learning rate (<math id="S3.p6.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.p6.2.m2.1a"><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\eta</annotation></semantics></math>). Fine-Tuning round of learning is scheduled using lower <math id="S3.p6.3.m3.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S3.p6.3.m3.1a"><msub id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml"><mi id="S3.p6.3.m3.1.1.2" xref="S3.p6.3.m3.1.1.2.cmml">E</mi><mi id="S3.p6.3.m3.1.1.3" xref="S3.p6.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><apply id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.1.1.1.cmml" xref="S3.p6.3.m3.1.1">subscript</csymbol><ci id="S3.p6.3.m3.1.1.2.cmml" xref="S3.p6.3.m3.1.1.2">ğ¸</ci><ci id="S3.p6.3.m3.1.1.3.cmml" xref="S3.p6.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">E_{p}</annotation></semantics></math> and <math id="S3.p6.4.m4.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S3.p6.4.m4.1a"><mi id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><ci id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">\eta</annotation></semantics></math>, which is aligned with the conditions under which FedAvg performs the best in non-IID conditions. Federated Learning at a faster pace is achieved by alternating low-level <span id="S3.p6.4.3" class="ltx_text ltx_font_italic">feature-learning</span> and high-level <span id="S3.p6.4.4" class="ltx_text ltx_font_italic">fine-tuning</span> along with appropriate hyper-parameters as described in the next section.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Divide-and-Conquer: Experiments, Results, and Discussion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes the simulation environment, experiments done, and results. The comparison with other state-of-the-art approaches is also captured in the results section to establish the state-of-the-art credentials of our proposed approach.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We present observations from <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Divid-and-Conquer</span> on VGG9 topology using 3 different non-IID conditions as in <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>, which includes coverage for convolutional layers and LSTMs. Classification and NLP models used were also same as <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Classification using Color Skewed CIFAR10 Dataset <cite class="ltx_cite ltx_citemacro_citep">(Krizhevsky, <a href="#bib.bib9" title="" class="ltx_ref">2009</a>)</cite>: CIFAR10 dataset is split into two groups of 5 classes each, with each class assigned uniquely to the two collaborators. To skew the data further using a 95-5% skew pattern, 95% of images in the first group are converted to gray-scale and 5% of images in the second group are converted to gray-scale. This results in the first collaborator holding gray-scale dominant data and the second collaborator holding color dominant data.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">Classification using Class Imbalanced CIFAR10 Data: Data is distributed non-uniformly across different collaborators to create non-IID conditions from the perspective of total training data per collaborator as well as the number of records per class.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p">Next Character prediction model on Shakespeare dataset <cite class="ltx_cite ltx_citemacro_citep">(Caldas etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> leveraging non-IIDness in speaking-roles: Data corresponding to each speaking-role in the play is grouped to create unique collaborators, to simulate natural non-IID condition. For the trial, we selected only clients with a minimum of 10k data points and sampled a random subset of 66 clients.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Hyper-Parameter Selection</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Fine Tuning Epoch and Learning Rate</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.4" class="ltx_p">Divide-and-Conquer allows the use of variable hyper-parameters for different parts of the network. As discussed earlier, we train <span id="S4.SS2.SSS1.p1.4.1" class="ltx_text ltx_font_italic">feature-learning</span> group more aggressively than <span id="S4.SS2.SSS1.p1.4.2" class="ltx_text ltx_font_italic">fine-tuning</span> group by control of parameters like <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><msub id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS1.p1.1.m1.1.1.2" xref="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml">E</mi><mi id="S4.SS2.SSS1.p1.1.m1.1.1.3" xref="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><apply id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.2">ğ¸</ci><ci id="S4.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">E_{p}</annotation></semantics></math> and <math id="S4.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m2.1a"><mi id="S4.SS2.SSS1.p1.2.m2.1.1" xref="S4.SS2.SSS1.p1.2.m2.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m2.1b"><ci id="S4.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m2.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m2.1c">\eta</annotation></semantics></math>. Use of lower <math id="S4.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m3.1a"><msub id="S4.SS2.SSS1.p1.3.m3.1.1" xref="S4.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS1.p1.3.m3.1.1.2" xref="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml">E</mi><mi id="S4.SS2.SSS1.p1.3.m3.1.1.3" xref="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m3.1b"><apply id="S4.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.2">ğ¸</ci><ci id="S4.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS1.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m3.1c">E_{p}</annotation></semantics></math> for <span id="S4.SS2.SSS1.p1.4.3" class="ltx_text ltx_font_italic">fine-tuning</span> rounds results in slightly better accuracy compared to higher epochs. This is because the local models are skewed by over-fitting to non-IID data at the individual collaborators. By using lower values for epoch and learning rate for <span id="S4.SS2.SSS1.p1.4.4" class="ltx_text ltx_font_italic">fine-tuning</span> rounds, we achieve better accuracy while simultaneously reducing compute requirements needed for <span id="S4.SS2.SSS1.p1.4.5" class="ltx_text ltx_font_italic">fine-tuning</span> rounds. Data from Color Skewed distribution is presented in Figure <a href="#S4.F4" title="Figure 4 â€£ 4.2.1 Fine Tuning Epoch and Learning Rate â€£ 4.2 Hyper-Parameter Selection â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. This observation is in alignment with the behavior of FedAvg where a high number of <math id="S4.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S4.SS2.SSS1.p1.4.m4.1a"><msub id="S4.SS2.SSS1.p1.4.m4.1.1" xref="S4.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S4.SS2.SSS1.p1.4.m4.1.1.2" xref="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml">E</mi><mi id="S4.SS2.SSS1.p1.4.m4.1.1.3" xref="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m4.1b"><apply id="S4.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.2">ğ¸</ci><ci id="S4.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S4.SS2.SSS1.p1.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m4.1c">E_{p}</annotation></semantics></math> leads to lower training accuracy due to weight divergence.
<br class="ltx_break"></p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F4.1.1" class="ltx_p ltx_align_center ltx_figure_panel ltx_align_center"><span id="S4.F4.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/epochs_exp.png" id="S4.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption"></span></p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Effect of Training Epochs: Ep0.1 corresponds to fine-tuning epoch that is 10% of the value used for feature-learning</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<p id="S4.F4.2" class="ltx_p ltx_figure_panel ltx_align_center">epoch</p>
</div>
</div>
</figure>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Topology Division</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.2" class="ltx_p">Depending on the nature and magnitude of non-IIDness, the Class Agnostic and Class Specific layers in a given model will diverge across different collaborators. We explored, weight divergence in the learned model, to guide <span id="S4.SS2.SSS2.p1.2.1" class="ltx_text ltx_font_italic">Divide-and-Conquer (Divide-and-Conquer)</span> methodology. The metric given below was explored in <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> .</p>
<table id="S4.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex1.m1.2" class="ltx_Math" alttext="\ W_{d}=||W_{1}-W_{2}||/||W_{1}||" display="block"><semantics id="S4.Ex1.m1.2a"><mrow id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml"><msub id="S4.Ex1.m1.2.2.4" xref="S4.Ex1.m1.2.2.4.cmml"><mi id="S4.Ex1.m1.2.2.4.2" xref="S4.Ex1.m1.2.2.4.2.cmml">W</mi><mi id="S4.Ex1.m1.2.2.4.3" xref="S4.Ex1.m1.2.2.4.3.cmml">d</mi></msub><mo id="S4.Ex1.m1.2.2.3" xref="S4.Ex1.m1.2.2.3.cmml">=</mo><mrow id="S4.Ex1.m1.2.2.2" xref="S4.Ex1.m1.2.2.2.cmml"><mrow id="S4.Ex1.m1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.2.1.cmml">â€–</mo><mrow id="S4.Ex1.m1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.cmml"><msub id="S4.Ex1.m1.1.1.1.1.1.1.2" xref="S4.Ex1.m1.1.1.1.1.1.1.2.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.2.2" xref="S4.Ex1.m1.1.1.1.1.1.1.2.2.cmml">W</mi><mn id="S4.Ex1.m1.1.1.1.1.1.1.2.3" xref="S4.Ex1.m1.1.1.1.1.1.1.2.3.cmml">1</mn></msub><mo id="S4.Ex1.m1.1.1.1.1.1.1.1" xref="S4.Ex1.m1.1.1.1.1.1.1.1.cmml">âˆ’</mo><msub id="S4.Ex1.m1.1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex1.m1.1.1.1.1.1.1.3.2" xref="S4.Ex1.m1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S4.Ex1.m1.1.1.1.1.1.1.3.3" xref="S4.Ex1.m1.1.1.1.1.1.1.3.3.cmml">2</mn></msub></mrow><mo stretchy="false" id="S4.Ex1.m1.1.1.1.1.1.3" xref="S4.Ex1.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mo id="S4.Ex1.m1.2.2.2.3" xref="S4.Ex1.m1.2.2.2.3.cmml">/</mo><mrow id="S4.Ex1.m1.2.2.2.2.1" xref="S4.Ex1.m1.2.2.2.2.2.cmml"><mo stretchy="false" id="S4.Ex1.m1.2.2.2.2.1.2" xref="S4.Ex1.m1.2.2.2.2.2.1.cmml">â€–</mo><msub id="S4.Ex1.m1.2.2.2.2.1.1" xref="S4.Ex1.m1.2.2.2.2.1.1.cmml"><mi id="S4.Ex1.m1.2.2.2.2.1.1.2" xref="S4.Ex1.m1.2.2.2.2.1.1.2.cmml">W</mi><mn id="S4.Ex1.m1.2.2.2.2.1.1.3" xref="S4.Ex1.m1.2.2.2.2.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S4.Ex1.m1.2.2.2.2.1.3" xref="S4.Ex1.m1.2.2.2.2.2.1.cmml">â€–</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.2b"><apply id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2"><eq id="S4.Ex1.m1.2.2.3.cmml" xref="S4.Ex1.m1.2.2.3"></eq><apply id="S4.Ex1.m1.2.2.4.cmml" xref="S4.Ex1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.4.1.cmml" xref="S4.Ex1.m1.2.2.4">subscript</csymbol><ci id="S4.Ex1.m1.2.2.4.2.cmml" xref="S4.Ex1.m1.2.2.4.2">ğ‘Š</ci><ci id="S4.Ex1.m1.2.2.4.3.cmml" xref="S4.Ex1.m1.2.2.4.3">ğ‘‘</ci></apply><apply id="S4.Ex1.m1.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2"><divide id="S4.Ex1.m1.2.2.2.3.cmml" xref="S4.Ex1.m1.2.2.2.3"></divide><apply id="S4.Ex1.m1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S4.Ex1.m1.1.1.1.1.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S4.Ex1.m1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1"><minus id="S4.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.1"></minus><apply id="S4.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.2.2">ğ‘Š</ci><cn type="integer" id="S4.Ex1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.2.3">1</cn></apply><apply id="S4.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex1.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex1.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.3.2">ğ‘Š</ci><cn type="integer" id="S4.Ex1.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex1.m1.1.1.1.1.1.1.3.3">2</cn></apply></apply></apply><apply id="S4.Ex1.m1.2.2.2.2.2.cmml" xref="S4.Ex1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S4.Ex1.m1.2.2.2.2.2.1.cmml" xref="S4.Ex1.m1.2.2.2.2.1.2">norm</csymbol><apply id="S4.Ex1.m1.2.2.2.2.1.1.cmml" xref="S4.Ex1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.2.2.2.2.1.1.1.cmml" xref="S4.Ex1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S4.Ex1.m1.2.2.2.2.1.1.2.cmml" xref="S4.Ex1.m1.2.2.2.2.1.1.2">ğ‘Š</ci><cn type="integer" id="S4.Ex1.m1.2.2.2.2.1.1.3.cmml" xref="S4.Ex1.m1.2.2.2.2.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.2c">\ W_{d}=||W_{1}-W_{2}||/||W_{1}||</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">where <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="W_{d}" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><msub id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">W</mi><mi id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">ğ‘Š</ci><ci id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">W_{d}</annotation></semantics></math> is Weight Divergence.
We modified the divergence computation as below, to capture direction aware divergence to guide our <span id="S4.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> methodology.</p>
<table id="S4.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.Ex2.m1.3" class="ltx_Math" alttext="\ W_{d}=CosineDist(W_{1},W_{2})/||W_{1}||" display="block"><semantics id="S4.Ex2.m1.3a"><mrow id="S4.Ex2.m1.3.3" xref="S4.Ex2.m1.3.3.cmml"><msub id="S4.Ex2.m1.3.3.5" xref="S4.Ex2.m1.3.3.5.cmml"><mi id="S4.Ex2.m1.3.3.5.2" xref="S4.Ex2.m1.3.3.5.2.cmml">W</mi><mi id="S4.Ex2.m1.3.3.5.3" xref="S4.Ex2.m1.3.3.5.3.cmml">d</mi></msub><mo id="S4.Ex2.m1.3.3.4" xref="S4.Ex2.m1.3.3.4.cmml">=</mo><mrow id="S4.Ex2.m1.3.3.3" xref="S4.Ex2.m1.3.3.3.cmml"><mrow id="S4.Ex2.m1.2.2.2.2" xref="S4.Ex2.m1.2.2.2.2.cmml"><mi id="S4.Ex2.m1.2.2.2.2.4" xref="S4.Ex2.m1.2.2.2.2.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.5" xref="S4.Ex2.m1.2.2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3a" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.6" xref="S4.Ex2.m1.2.2.2.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3b" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.7" xref="S4.Ex2.m1.2.2.2.2.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3c" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.8" xref="S4.Ex2.m1.2.2.2.2.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3d" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.9" xref="S4.Ex2.m1.2.2.2.2.9.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3e" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.10" xref="S4.Ex2.m1.2.2.2.2.10.cmml">D</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3f" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.11" xref="S4.Ex2.m1.2.2.2.2.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3g" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.12" xref="S4.Ex2.m1.2.2.2.2.12.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3h" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mi id="S4.Ex2.m1.2.2.2.2.13" xref="S4.Ex2.m1.2.2.2.2.13.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.Ex2.m1.2.2.2.2.3i" xref="S4.Ex2.m1.2.2.2.2.3.cmml">â€‹</mo><mrow id="S4.Ex2.m1.2.2.2.2.2.2" xref="S4.Ex2.m1.2.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.Ex2.m1.2.2.2.2.2.2.3" xref="S4.Ex2.m1.2.2.2.2.2.3.cmml">(</mo><msub id="S4.Ex2.m1.1.1.1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex2.m1.1.1.1.1.1.1.1.2" xref="S4.Ex2.m1.1.1.1.1.1.1.1.2.cmml">W</mi><mn id="S4.Ex2.m1.1.1.1.1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S4.Ex2.m1.2.2.2.2.2.2.4" xref="S4.Ex2.m1.2.2.2.2.2.3.cmml">,</mo><msub id="S4.Ex2.m1.2.2.2.2.2.2.2" xref="S4.Ex2.m1.2.2.2.2.2.2.2.cmml"><mi id="S4.Ex2.m1.2.2.2.2.2.2.2.2" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2.cmml">W</mi><mn id="S4.Ex2.m1.2.2.2.2.2.2.2.3" xref="S4.Ex2.m1.2.2.2.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S4.Ex2.m1.2.2.2.2.2.2.5" xref="S4.Ex2.m1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.3.3.3.4" xref="S4.Ex2.m1.3.3.3.4.cmml">/</mo><mrow id="S4.Ex2.m1.3.3.3.3.1" xref="S4.Ex2.m1.3.3.3.3.2.cmml"><mo stretchy="false" id="S4.Ex2.m1.3.3.3.3.1.2" xref="S4.Ex2.m1.3.3.3.3.2.1.cmml">â€–</mo><msub id="S4.Ex2.m1.3.3.3.3.1.1" xref="S4.Ex2.m1.3.3.3.3.1.1.cmml"><mi id="S4.Ex2.m1.3.3.3.3.1.1.2" xref="S4.Ex2.m1.3.3.3.3.1.1.2.cmml">W</mi><mn id="S4.Ex2.m1.3.3.3.3.1.1.3" xref="S4.Ex2.m1.3.3.3.3.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S4.Ex2.m1.3.3.3.3.1.3" xref="S4.Ex2.m1.3.3.3.3.2.1.cmml">â€–</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.3b"><apply id="S4.Ex2.m1.3.3.cmml" xref="S4.Ex2.m1.3.3"><eq id="S4.Ex2.m1.3.3.4.cmml" xref="S4.Ex2.m1.3.3.4"></eq><apply id="S4.Ex2.m1.3.3.5.cmml" xref="S4.Ex2.m1.3.3.5"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.5.1.cmml" xref="S4.Ex2.m1.3.3.5">subscript</csymbol><ci id="S4.Ex2.m1.3.3.5.2.cmml" xref="S4.Ex2.m1.3.3.5.2">ğ‘Š</ci><ci id="S4.Ex2.m1.3.3.5.3.cmml" xref="S4.Ex2.m1.3.3.5.3">ğ‘‘</ci></apply><apply id="S4.Ex2.m1.3.3.3.cmml" xref="S4.Ex2.m1.3.3.3"><divide id="S4.Ex2.m1.3.3.3.4.cmml" xref="S4.Ex2.m1.3.3.3.4"></divide><apply id="S4.Ex2.m1.2.2.2.2.cmml" xref="S4.Ex2.m1.2.2.2.2"><times id="S4.Ex2.m1.2.2.2.2.3.cmml" xref="S4.Ex2.m1.2.2.2.2.3"></times><ci id="S4.Ex2.m1.2.2.2.2.4.cmml" xref="S4.Ex2.m1.2.2.2.2.4">ğ¶</ci><ci id="S4.Ex2.m1.2.2.2.2.5.cmml" xref="S4.Ex2.m1.2.2.2.2.5">ğ‘œ</ci><ci id="S4.Ex2.m1.2.2.2.2.6.cmml" xref="S4.Ex2.m1.2.2.2.2.6">ğ‘ </ci><ci id="S4.Ex2.m1.2.2.2.2.7.cmml" xref="S4.Ex2.m1.2.2.2.2.7">ğ‘–</ci><ci id="S4.Ex2.m1.2.2.2.2.8.cmml" xref="S4.Ex2.m1.2.2.2.2.8">ğ‘›</ci><ci id="S4.Ex2.m1.2.2.2.2.9.cmml" xref="S4.Ex2.m1.2.2.2.2.9">ğ‘’</ci><ci id="S4.Ex2.m1.2.2.2.2.10.cmml" xref="S4.Ex2.m1.2.2.2.2.10">ğ·</ci><ci id="S4.Ex2.m1.2.2.2.2.11.cmml" xref="S4.Ex2.m1.2.2.2.2.11">ğ‘–</ci><ci id="S4.Ex2.m1.2.2.2.2.12.cmml" xref="S4.Ex2.m1.2.2.2.2.12">ğ‘ </ci><ci id="S4.Ex2.m1.2.2.2.2.13.cmml" xref="S4.Ex2.m1.2.2.2.2.13">ğ‘¡</ci><interval closure="open" id="S4.Ex2.m1.2.2.2.2.2.3.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2"><apply id="S4.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex2.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.2">ğ‘Š</ci><cn type="integer" id="S4.Ex2.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex2.m1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S4.Ex2.m1.2.2.2.2.2.2.2.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex2.m1.2.2.2.2.2.2.2.1.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.Ex2.m1.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.2">ğ‘Š</ci><cn type="integer" id="S4.Ex2.m1.2.2.2.2.2.2.2.3.cmml" xref="S4.Ex2.m1.2.2.2.2.2.2.2.3">2</cn></apply></interval></apply><apply id="S4.Ex2.m1.3.3.3.3.2.cmml" xref="S4.Ex2.m1.3.3.3.3.1"><csymbol cd="latexml" id="S4.Ex2.m1.3.3.3.3.2.1.cmml" xref="S4.Ex2.m1.3.3.3.3.1.2">norm</csymbol><apply id="S4.Ex2.m1.3.3.3.3.1.1.cmml" xref="S4.Ex2.m1.3.3.3.3.1.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.3.3.3.3.1.1.1.cmml" xref="S4.Ex2.m1.3.3.3.3.1.1">subscript</csymbol><ci id="S4.Ex2.m1.3.3.3.3.1.1.2.cmml" xref="S4.Ex2.m1.3.3.3.3.1.1.2">ğ‘Š</ci><cn type="integer" id="S4.Ex2.m1.3.3.3.3.1.1.3.cmml" xref="S4.Ex2.m1.3.3.3.3.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.3c">\ W_{d}=CosineDist(W_{1},W_{2})/||W_{1}||</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Weight divergence from VGG9 model for Color Skewed non-IID simulation described in Section 4.1 is shown in Figure <a href="#S4.F5" title="Figure 5 â€£ 4.2.2 Topology Division â€£ 4.2 Hyper-Parameter Selection â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. A pre-pass training is initially performed for 5 rounds and layer-wise divergence is computed for future rounds using the pre-pass model as a reference. Model at end of pre-pass comprising 5 rounds is M4. L1, L2 represents different layers of VGG9 while M5, M6, etc., corresponding to models from future communication rounds. Compared to prepass model M4, the divergence is low for the initial set of layers and starts to increase around Layer5. Divide-and-Conquer can be applied around this layer to split the topology for creating feature-training and fine-tuning groups.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<br class="ltx_break">
<p id="S4.F5.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F5.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/wdiv1.png" id="S4.F5.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>LayerWise Weight Divergence for Color Skewed distribution at different communication rounds. L1,L2 corresponds to layers and M5,M6 corresponds to model at end of successive communication rounds. Divergence is low for initial layers suggesting opportunities for Divide-and-Conquer</figcaption>
</figure>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">To validate the efficacy of <math id="S4.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="W_{d}" display="inline"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><msub id="S4.SS2.SSS2.p3.1.m1.1.1" xref="S4.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p3.1.m1.1.1.2" xref="S4.SS2.SSS2.p3.1.m1.1.1.2.cmml">W</mi><mi id="S4.SS2.SSS2.p3.1.m1.1.1.3" xref="S4.SS2.SSS2.p3.1.m1.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p3.1.m1.1b"><apply id="S4.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.2">ğ‘Š</ci><ci id="S4.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p3.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">W_{d}</annotation></semantics></math>, Accuracy and convergence behavior for VGG9 under different layer division schemes were checked using a brute force sweep across different splits. Accuracy for different division schemes is presented in Figure <a href="#S4.F6" title="Figure 6 â€£ 4.2.2 Topology Division â€£ 4.2 Hyper-Parameter Selection â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. As discussed in Section 2, Divide5 corresponds to division after layer5. From the figure, Divide5 offers the best accuracy and convergence speed under the given non-IID condition. All runs used 20 epochs for <span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_italic">feature-learning</span> and 4 epochs for <span id="S4.SS2.SSS2.p3.1.2" class="ltx_text ltx_font_italic">fine-tuning</span>. Likewise, learning rate for <span id="S4.SS2.SSS2.p3.1.3" class="ltx_text ltx_font_italic">fine-tuning</span> round was half that of <span id="S4.SS2.SSS2.p3.1.4" class="ltx_text ltx_font_italic">feature-learning</span>. Learning Rate Decay was also applied across the communication rounds starting from 0.001 and reducing by 10% for every round.</p>
</div>
<div id="S4.SS2.SSS2.p4" class="ltx_para">
<p id="S4.SS2.SSS2.p4.1" class="ltx_p">For certain division schemes (ex: Divide7), large spread is seen in accuracy between <span id="S4.SS2.SSS2.p4.1.1" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S4.SS2.SSS2.p4.1.2" class="ltx_text ltx_font_italic">fine-tuning</span> rounds suggesting that the layer assignment strategy for the two groups is sub-optimal. For Divide7, Divide6, etc., we find that accuracy is higher for <span id="S4.SS2.SSS2.p4.1.3" class="ltx_text ltx_font_italic">fine-tuning</span> rounds (CR=2,4,6,etc.,) and drops for <span id="S4.SS2.SSS2.p4.1.4" class="ltx_text ltx_font_italic">feature-learning</span> rounds (CR=1,3,5,etc.). The trend however reverses for Divide5 where the accuracy is higher for <span id="S4.SS2.SSS2.p4.1.5" class="ltx_text ltx_font_italic">feature-learning</span> rounds and marginally drops for <span id="S4.SS2.SSS2.p4.1.6" class="ltx_text ltx_font_italic">fine-tuning</span> group. The divergence between <span id="S4.SS2.SSS2.p4.1.7" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S4.SS2.SSS2.p4.1.8" class="ltx_text ltx_font_italic">fine-tuning</span> is also minimal in this split.</p>
</div>
<div id="S4.SS2.SSS2.p5" class="ltx_para">
<p id="S4.SS2.SSS2.p5.1" class="ltx_p">When fewer layers are present in <span id="S4.SS2.SSS2.p5.1.1" class="ltx_text ltx_font_italic">feature-learning</span> group as in Divide4, we find that the rate of learning starts to fall and accuracy spread between the two learning groups increases again. This suggests that Divide5 is an optimal split for this topology for this non-IID dataset thereby validating the usage of weight divergence metric to determine the point in a model where the layer split can be performed.
<br class="ltx_break"></p>
</div>
<figure id="S4.F6" class="ltx_figure">
<br class="ltx_break">
<p id="S4.F6.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F6.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/layersplit_exp.png" id="S4.F6.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="236" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Training Accuracy under different Layer Division Schemes. Divide5 offers optimal results in-line with weight divergence</figcaption>
</figure>
<div id="S4.SS2.SSS2.p6" class="ltx_para">
<p id="S4.SS2.SSS2.p6.1" class="ltx_p">For the Class Imbalanced non-IID condition, the weight-divergence is high across all the layers of the topology [Figure <a href="#S4.F7" title="Figure 7 â€£ 4.2.2 Topology Division â€£ 4.2 Hyper-Parameter Selection â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>], suggesting that <span id="S4.SS2.SSS2.p6.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> might not offer significant benefits. Accuracy improvements was not seen in brute force sweep across different layer splits as well. For experiment sake, we chose to divide after layer8 inline with traditional <span id="S4.SS2.SSS2.p6.1.2" class="ltx_text ltx_font_italic">fine-tuning</span> strategies where the last layer is used for <span id="S4.SS2.SSS2.p6.1.3" class="ltx_text ltx_font_italic">fine-tuning</span>. The Next Character prediction model has 3 layers and we again use the last layer for <span id="S4.SS2.SSS2.p6.1.4" class="ltx_text ltx_font_italic">fine-tuning</span>.</p>
</div>
<figure id="S4.F7" class="ltx_figure">
<br class="ltx_break">
<p id="S4.F7.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F7.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/wdiv2.png" id="S4.F7.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="197" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>LayerWise Weight Divergence for class-imbalanced distribution at different communication rounds. L1,L2 corresponds to layers and M5,M6 corresponds to model at end of successive communication rounds. Divergence is high for all layers.</figcaption>
</figure>
<div id="S4.SS2.SSS2.p7" class="ltx_para">
<p id="S4.SS2.SSS2.p7.1" class="ltx_p">In the current work, layer division is determined using a pre-pass run and the scheme is fixed for the entire duration of training. Future work will extend this to explore a dynamic scheme assignment where layers from a group can be reassigned to other group based on observed trend in <span id="S4.SS2.SSS2.p7.1.1" class="ltx_text ltx_font_italic">feature-learning</span> accuracy vs <span id="S4.SS2.SSS2.p7.1.2" class="ltx_text ltx_font_italic">fine-tuning</span> accuracy over few communication rounds.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Results from the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> methodology under different non-IID scenario is presented in this section. For training we use 20 epochs for feature-learning and 4 epochs for fine-tuning. Learning rate was initialized to 0.001 and allowed to decay by 10% for every communication round. Learning rate for fine-tuning was 50% of learning-rate for feature-learning.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> uses half the network bandwidth for data transfers compared to FedAvg, as the full model is transferred for every two communication rounds. For FedMA, results from equivalent matched averaged round is presented based on equivalency established in <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2020</a>)</cite>. Though FedMA uses much lower communication bandwidth, compute overhead for layer matching increases with model depth as well as width, making it less desirable for practical deployments. We show that our methodology yields similar accuracy levels as more complex algorithms like FedMA in acceptable rounds of communication.
<br class="ltx_break">Note: In the tables providing the comparison across different approaches, <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> is captured under <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">D&amp;C</span>.
<br class="ltx_break"></p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Image Classification : Color Skewed Distribution</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.2" class="ltx_p">Training accuracy and convergence profile for different aggregation algorithms using Color Skewed 95-5% CIFAR10 data are shown in Figure <a href="#S4.F8" title="Figure 8 â€£ 4.3.1 Image Classification : Color Skewed Distribution â€£ 4.3 Results â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. It can be seen that for this category of non-IIDness, the model reaches high accuracy with much smaller communication rounds compared to FedAvg. Divide5 was used for this analysis as described in the earlier section along with the same values for <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="E_{p}" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><msub id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.1.m1.1.1.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml">E</mi><mi id="S4.SS3.SSS1.p1.1.m1.1.1.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2">ğ¸</ci><ci id="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">E_{p}</annotation></semantics></math> and <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\eta" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><mi id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml">Î·</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><ci id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">ğœ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">\eta</annotation></semantics></math>. Results for additional levels of Color Skew is presented in Table <a href="#S4.T1" title="Table 1 â€£ 4.3.1 Image Classification : Color Skewed Distribution â€£ 4.3 Results â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We chose 18 rounds of communication for the comparison to align with FedMA.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<br class="ltx_break">
<p id="S4.F8.1.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S4.F8.1.1.1" class="ltx_text"><img src="/html/2106.14503/assets/figures/sec3_sota_comparison.png" id="S4.F8.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="430" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Accuracy Comparison for Color Skewed Distribution with 95-5% skew. Accuracy and Convergence rate for Divide-and-Conquer (using FedAvg) is higher than FedAvg</figcaption>
</figure>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Accuracy for Color Skewed Distribution for 18 communication rounds under different levels of skew for 2 collaborators. D&amp;C (using FedAvg) delivers high classification accuracy under this non-iidness</figcaption>
<br class="ltx_break">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">#Col</span></th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">Skew</span></th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedAvg</span></th>
<th id="S4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedProx</span></th>
<th id="S4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedMA</span></th>
<th id="S4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.1.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">D&amp;C</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">2</span></th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">95-5%</span></td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">53.1%</span></td>
<td id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">56.2%</span></td>
<td id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">81.0%</span></td>
<td id="S4.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.2.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">80.1%</span></td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">2</span></th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">75-25%</span></td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">52.8%</span></td>
<td id="S4.T1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">74.6%</span></td>
<td id="S4.T1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">78.8%</span></td>
<td id="S4.T1.1.3.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.3.2.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">79.2%</span></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">2</span></th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.4.3.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">50-50%</span></td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.4.3.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">49.1%</span></td>
<td id="S4.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.4.3.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">67.2%</span></td>
<td id="S4.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.4.3.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">79.9%</span></td>
<td id="S4.T1.1.4.3.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.4.3.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">81.8%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Image Classification : Class Imbalance Distribution</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">For Class Imbalanced data, the observed weight divergence from the pre-pass run was high for most layers. This indicates that <span id="S4.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> does not offer any advantages over FedAvg. As an experiment, we divided the topology at layer8 similar to traditional <span id="S4.SS3.SSS2.p1.1.2" class="ltx_text ltx_font_italic">fine-tuning</span>. <span id="S4.SS3.SSS2.p1.1.3" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> yields slightly lower accuracy compared to FedAvg and FedMA as documented in table [<a href="#S4.T2" title="Table 2 â€£ 4.3.2 Image Classification : Class Imbalance Distribution â€£ 4.3 Results â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>]. However, if bandwidth saving is not considered as a requirement and <span id="S4.SS3.SSS2.p1.1.4" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> is run for additional rounds to get a similar amount of model transfer as FedAvg, the performance of <span id="S4.SS3.SSS2.p1.1.5" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> is marginally better. This is captured in the table under the column <span id="S4.SS3.SSS2.p1.1.6" class="ltx_text ltx_font_italic">D&amp;Câ€™</span>. Though FedMA achieves its accuracy levels using much lower communication bandwidth, compute overhead for layer matching increases with model depth as well as width, as discussed earlier, making it less desirable for practical deployments.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Given the results, it is clear that in cases where weight divergence suggests no clear split layer, it is recommended not to adopt <span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span>. As collaborator count increases, training data per collaborator decreases in the simulation environment, as the data is divided across the collaborators. This could also lead to increased divergence when <span id="S4.SS3.SSS2.p2.1.2" class="ltx_text ltx_font_italic">feature-learning</span> is done aggressively on sparse data. In a truly federated set up with a large training corpus across collaborators, we expect our methodology to offer better accuracy improvements.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Accuracy for Class-Imbalanced Distribution for 18 communication rounds using 5 &amp; 10 collaborators. Accuracy from D&amp;C (using FedAvg) is inline with other algorithms.</figcaption>
<br class="ltx_break">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">#Col</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedAvg</span></th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedProx</span></th>
<th id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedMA</span></th>
<th id="S4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">D&amp;C</span></th>
<th id="S4.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">D&amp;Câ€™</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">5</span></th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">88.5%</span></td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">87.5%</span></td>
<td id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">87.5%</span></td>
<td id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">87.1%</span></td>
<td id="S4.T2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.2.1.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">89.3%</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T2.1.3.2.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">10</span></th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.3.2.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">83.5%</span></td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.3.2.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">80.0%</span></td>
<td id="S4.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.3.2.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">82.5%</span></td>
<td id="S4.T2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.3.2.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">76.8%</span></td>
<td id="S4.T2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.3.2.6.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">82.2%</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">An extreme case of Class Imbalance based heterogeneity is when each collaborator exclusively holds data from one unique class. All the tested algorithms performed poorly (accuracy less than 15%) under this scenario, suggesting a need for more research in this area.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Next Character Prediction: speaker-role based non-IID distribution</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Results from application of <span id="S4.SS3.SSS3.p1.1.1" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> to a character prediction model is shown in table <a href="#S4.T3" title="Table 3 â€£ 4.3.3 Next Character Prediction: speaker-role based non-IID distribution â€£ 4.3 Results â€£ 4 Divide-and-Conquer: Experiments, Results, and Discussion â€£ Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. At end of 9 communication rounds, the accuracy from <span id="S4.SS3.SSS3.p1.1.2" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> is comparable to other algorithms while only requiring half the amount of data transfer as FedAvg. 9 rounds of communication was chosen to align with FedMA.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy for next-character prediction lstm model for 9 communication rounds using 66 collaborators. Accuracy from D&amp;C (using FedAvg) is inline with other algorithms</figcaption>
<br class="ltx_break">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">#Col</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedAvg</span></th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedProx</span></th>
<th id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">FedMA</span></th>
<th id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T3.1.1.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">D&amp;C</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S4.T3.1.2.1.1.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">66</span></th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.2.1.2.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">50.8%</span></td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.2.1.3.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">44.6%</span></td>
<td id="S4.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.2.1.4.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">47.4%</span></td>
<td id="S4.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.2.1.5.1" class="ltx_text ltx_font_smallcaps" style="font-size:90%;">49.6%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we presented a weight divergence based <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">Divide and Conquer</span> algorithm which builds on popular FedAvg algorithm to achieve state-of-the-art accuracy under non-IIDness. By training network in parts, our novel methodology is shown to a) Achieve faster convergence when low-level features are well-represented b) Reduce communication by half, as a consequence of training and weight exchange in parts, and c) Require less compute compared to state-of-the-art techniques like FedMA, which has performance overheads from weight matching. A static topology splitting strategy is adapted in this work, where the topology is divided at the beginning of training using a pre-pass run. Future work can explore a dynamic <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">Divide-and-Conquer</span> strategy where layers are moved between <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">feature-learning</span> and <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">fine-tuning</span> groups based on accuracy observations during training. Future work can also explore the application of Divide-and-Conquer methodology to learning paradigms like Few Shot Learning to identify Class Agnostic layers for the backbone network.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz etÂ al. (2019)</span>
<span class="ltx_bibblock">
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C., KonecnÃ½, J., Mazzocchi, S., McMahan, H., Overveldt, T.Â V.,
Petrou, D., Ramage, D., and Roselander, J.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1902.01046, 2019.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas etÂ al. (2018)</span>
<span class="ltx_bibblock">
Caldas, S., Wu, P., Li, T., KonecnÃ½, J., McMahan, H., Smith, V., and
Talwalkar, A.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1812.01097, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2020)</span>
<span class="ltx_bibblock">
Chen, W., Bhardwaj, K., and Marculescu, R.

</span>
<span class="ltx_bibblock">Fedmax: Mitigating activation divergence for accurate and
communication-efficient federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2004.03657, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinh etÂ al. (2020)</span>
<span class="ltx_bibblock">
Dinh, C.Â T., Tran, N.Â H., and Nguyen, T.Â D.

</span>
<span class="ltx_bibblock">Personalized federated learning with moreau envelopes.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2006.08848, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fallah etÂ al. (2020)</span>
<span class="ltx_bibblock">
Fallah, A., Mokhtari, A., and Ozdaglar, A.

</span>
<span class="ltx_bibblock">Personalized federated learning with theoretical guarantees: A
model-agnostic meta-learning approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh etÂ al. (2020)</span>
<span class="ltx_bibblock">
Ghosh, A., Chung, J., Yin, D., and Ramchandran, K.

</span>
<span class="ltx_bibblock">An efficient framework for clustered federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2006.04088, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely &amp; RichtÃ¡rik (2020)</span>
<span class="ltx_bibblock">
Hanzely, F. and RichtÃ¡rik, P.

</span>
<span class="ltx_bibblock">Federated learning of a mixture of global and local models.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2002.05516, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hanzely etÂ al. (2020)</span>
<span class="ltx_bibblock">
Hanzely, F., Hanzely, S., Horvath, S., and RichtÃ¡rik, P.

</span>
<span class="ltx_bibblock">Lower bounds and optimal algorithms for personalized federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2010.02372, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krizhevsky (2009)</span>
<span class="ltx_bibblock">
Krizhevsky, A.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2020)</span>
<span class="ltx_bibblock">
Li, T., Sahu, A.Â K., Talwalkar, A., and Smith, V.

</span>
<span class="ltx_bibblock">Federated learning: Challenges, methods, and future directions.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Processing Magazine</em>, 37:50â€“60, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Lin, T., Kong, L., Stich, S., and Jaggi, M.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2006.07242, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan etÂ al. (2017)</span>
<span class="ltx_bibblock">
McMahan, H., Moore, E., Ramage, D., Hampson, S., and yÂ Arcas, B.Â A.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">AISTATS</em>, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reyes etÂ al. (2015)</span>
<span class="ltx_bibblock">
Reyes, A.Â K., Caicedo, J.Â C., and Camargo, J.

</span>
<span class="ltx_bibblock">Fine-tuning deep convolutional networks for plant recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">CLEF</em>, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Sahu, A.Â K., Li, T., Sanjabi, M., Zaheer, M., Talwalkar, A., and Smith, V.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv: Learning</em>, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shoham etÂ al. (2019)</span>
<span class="ltx_bibblock">
Shoham, N., Avidor, T., Keren, A., Israel, N., Benditkis, D., Mor-Yosef, L.,
and Zeitak, I.

</span>
<span class="ltx_bibblock">Overcoming forgetting in federated learning on non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1910.07796, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan &amp; Zisserman (2015)</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A.

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1409.1556, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2020)</span>
<span class="ltx_bibblock">
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.

</span>
<span class="ltx_bibblock">Federated learning with matched averaging.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2002.06440, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu &amp; Wang (2020)</span>
<span class="ltx_bibblock">
Xu, J. and Wang, F.

</span>
<span class="ltx_bibblock">Federated learning for healthcare informatics.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of Healthcare Informatics Research</em>, pp.Â  1 â€“ 19,
2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yan, X., Chen, Z., Xu, A., Wang, X., Liang, X., and Lin, L.

</span>
<span class="ltx_bibblock">Meta r-cnn : Towards general solver for instance-level few-shot
learning.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeiler &amp; Fergus (2014)</span>
<span class="ltx_bibblock">
Zeiler, M.Â D. and Fergus, R.

</span>
<span class="ltx_bibblock">Visualizing and understanding convolutional networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2018)</span>
<span class="ltx_bibblock">
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.

</span>
<span class="ltx_bibblock">Federated learning with non-iid data.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1806.00582, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2106.14501" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2106.14503" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2106.14503">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2106.14503" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2106.14504" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 12:16:09 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
