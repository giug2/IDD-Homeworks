<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives</title>
<!--Generated on Mon Jun 10 17:16:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Generative AI,  Recommender System,  Large Language Model,  LLMOps,  Retrieval-augmented Generation,  Autonomous Agent,  Evaluation,  Human-AI Alignment,  Trust and Safety,  Responsible AI" lang="en" name="keywords"/>
<base href="/html/2406.06475v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S1" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S1.SS1" title="In 1. Introduction ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Outline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S1.SS2" title="In 1. Introduction ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Key Contributions and Distinguishing from Related Works</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Fundamentals</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS1" title="In 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Industrial Recsys in a Nutshell</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS2" title="In 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Major Deficiencies of Existing Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS3" title="In 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Building GAI Foundation and LLMOps</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS3.SSS1" title="In 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>GAI ecosystem.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS3.SSS2" title="In 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>LLMOps.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.SS4" title="In 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Classification Framework of GAI in Recsys</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>GAI-enhanced data, feature, and modeling for Recsys</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3.SS1" title="In 3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Research Progress</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3.SS2" title="In 3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Solution Framework and Practical Considerations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Augmenting the Curation Capability of Recsys</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS1" title="In 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Research Progress</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2" title="In 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Solutions Framework and Practical Considerations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2.SSS1" title="In 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Evaluating RAG performance offline.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2.SSS2" title="In 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Development.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2.SSS3" title="In 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Deployment.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2.SSS4" title="In 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.4 </span>Iteration.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Facilitating Action and Reasoning for Interactive Recsys</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.SS1" title="In 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Research Progress</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.SS2" title="In 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Solution Framework and Practical Considerations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.SS2.SSS1" title="In 5.2. Solution Framework and Practical Considerations ‣ 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Agent with pre-defined reasoning graph.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.SS2.SSS2" title="In 5.2. Solution Framework and Practical Considerations ‣ 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Agent that uses GAI for planning.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Responsible GAI and Human-AI Alignment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.SS1" title="In 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Trust and Responsible AI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.SS2" title="In 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>GAI response risk assessment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.SS3" title="In 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Human-AI alignment for GAI in Recsys</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S7" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Open Problems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S8" title="In Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Da Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id3.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id4.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dxu2@linkedin.com">dxu2@linkedin.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Danqing Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id6.2.id2">Palo Alto</span><span class="ltx_text ltx_affiliation_state" id="id7.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id8.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:danqinz@amazon.com">danqinz@amazon.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangyu Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id9.1.id1">Tiktok</span><span class="ltx_text ltx_affiliation_city" id="id10.2.id2">Santa Clara</span><span class="ltx_text ltx_affiliation_state" id="id11.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id12.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:guangyu.yang@tiktok.com">guangyu.yang@tiktok.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Yang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Amazon</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Palo Alto</span><span class="ltx_text ltx_affiliation_state" id="id15.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id16.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:byyng@amazon.com">byyng@amazon.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuyuan Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id17.1.id1">Rutgers University</span><span class="ltx_text ltx_affiliation_city" id="id18.2.id2">New Brunswick</span><span class="ltx_text ltx_affiliation_state" id="id19.3.id3">New Jersey</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shuyuan.xu@rutgers.edu">shuyuan.xu@rutgers.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lingling Zheng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">Microsoft</span><span class="ltx_text ltx_affiliation_city" id="id22.2.id2">Redmond</span><span class="ltx_text ltx_affiliation_state" id="id23.3.id3">Washington</span><span class="ltx_text ltx_affiliation_country" id="id24.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:linzheng@microsoft.com">linzheng@microsoft.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cindy Liang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">LinkedIn</span><span class="ltx_text ltx_affiliation_city" id="id26.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_state" id="id27.3.id3">California</span><span class="ltx_text ltx_affiliation_country" id="id28.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:cliang@linkedin.com">cliang@linkedin.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id29.id1">Recently, generative AI (GAI), with their emerging capabilities, have presented unique opportunities for augmenting and revolutionizing industrial recommender systems (Recsys).
Despite growing research efforts at the intersection of these fields, the integration of GAI into industrial Recsys remains in its infancy, largely due to the intricate nature of modern industrial Recsys infrastructure, operations, and product sophistication.
Drawing upon our experiences in successfully integrating GAI into several major social and e-commerce platforms, this survey aims to comprehensively examine the underlying system and AI foundations, solution frameworks, connections to key research advancements, as well as summarize the practical insights and challenges encountered in the endeavor to integrate GAI into industrial Recsys. As pioneering work in this domain<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Subsequent versions of this manuscript will be released with additional real-world examples and solution details after clearing the review process.</span></span></span>, we hope outline the representative developments of relevant fields, shed lights on practical GAI adoptions in the industry, and motivate future research.</p>
</div>
<div class="ltx_keywords">Generative AI, Recommender System, Large Language Model, LLMOps, Retrieval-augmented Generation, Autonomous Agent, Evaluation, Human-AI Alignment, Trust and Safety, Responsible AI
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.p1.1.1">Motivation</span> – the last two years have been a thrilling journey for generative AI (GAI) and its emerging capabilities, revolutionizing and reshaping the technology landscape across major domains <cite class="ltx_cite ltx_citemacro_citep">(Bommasani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib9" title="">2021</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib147" title="">2023</a>)</cite>. Recommendation systems (Recsys) have not been left behind, as the field experiences a surge of innovative ideas and research works that leverage GAI to augment or replace existing system components and layers <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib85" title="">2023</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib139" title="">2023c</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib36" title="">2023</a>)</cite>.
Nonetheless, landing these innovations in the real world systems, especially those for social and e-commerce recommendations <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib119" title="">2013</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib133" title="">2007</a>)</cite>, poses significant challenges.
In particular, industrial Recsys are powered by sophisticated and compound AI systems that encompass not only AI models but also infrastructure, operational processes, as well as business and product considerations. Effectively integrating new technologies requires a holistic approach in practice.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Our work emerges at a crucial juncture to provide an <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">up-to-date</em>, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">application-centric</em>, and <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">interdisciplinary</em> survey tailored for the industry and Recsys community.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>Outline</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">The first part of the survey aims to offer a concise overview of existing industrial recsys and several key deficiencies, and the major advancements in GAI with a specific focus on GAI production fundamentals and LLMOps. Expanding upon these fundamentals, the structure of our survey can be depicted as in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S1.F1" title="Figure 1 ‣ 1.1. Outline ‣ 1. Introduction ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="340" id="S1.F1.g1" src="extracted/5655486/images/outline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Structure of the survey.</figcaption>
</figure>
<div class="ltx_para" id="S1.SS1.p2">
<p class="ltx_p" id="S1.SS1.p2.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3" title="3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a>, we start with mapping the opportunity landscape of using GAI usecases to enhance <span class="ltx_text ltx_font_bold" id="S1.SS1.p2.1.1">personalized recommendation</span> – the cornerstone of user satisfaction in the existing Recsys paradigm <cite class="ltx_cite ltx_citemacro_citep">(Ricci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib110" title="">2010</a>)</cite>.
After exploring the opportunities and initiatives, we address practical considerations and solutions for integrating the promising ones into real-world production systems.</p>
</div>
<div class="ltx_para" id="S1.SS1.p3">
<p class="ltx_p" id="S1.SS1.p3.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>, we delve deeper beyond personalized retrieval and ranking, focusing on the utilization of GAI for <span class="ltx_text ltx_font_bold" id="S1.SS1.p3.1.1">Recsys curation</span>. This new landscape entails such as re-purposing raw contents, curating from external knowledge, and generating explanations to precisely address users’ varied information need and enhance transparency and trustworthiness – the other crucial dimensions of user satisfaction in social and e-commerce Recsys <cite class="ltx_cite ltx_citemacro_citep">(Nilashi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib94" title="">2016</a>; Hassan, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib46" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib126" title="">2022b</a>)</cite>. Drawing from our experience in deploying these strategies on real-world platforms, we present systematic approaches to facilitate <span class="ltx_text ltx_font_bold" id="S1.SS1.p3.1.2">retrieval-augmented generation</span> (RAG) within Recsys accompanied by detailed solutions pertaining to AI modeling, serving, LLMOps, and other practical considerations.</p>
</div>
<div class="ltx_para" id="S1.SS1.p4">
<p class="ltx_p" id="S1.SS1.p4.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5" title="5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">5</span></a> is devoted to equipping Recsys with <span class="ltx_text ltx_font_bold" id="S1.SS1.p4.1.1">AI agents</span> to facilitate <span class="ltx_text ltx_font_bold" id="S1.SS1.p4.1.2">interactive recommendation</span> and <span class="ltx_text ltx_font_bold" id="S1.SS1.p4.1.3">active feedback</span> loops, marking a departure from the prevailing passive feedback paradigm. By harnessing more advanced tool-using and sequential reasoning-acting capabilities <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib124" title="">2023e</a>; Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib149" title="">2022</a>)</cite>, Recsys agents can be engineered to not only suggest items and offer content curation and explanation, but also processing and responding to the explicit, free-form user asks and preference statements. This approach serves to bridge existing gaps and potentially exceed the capabilities and user experiences offered by contemporary conversational Recsys <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib57" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS1.p5">
<p class="ltx_p" id="S1.SS1.p5.1">In Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6" title="6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">6</span></a>, we review two pivotal challenges associated with deploying GAI in customer-facing applications: <span class="ltx_text ltx_font_bold" id="S1.SS1.p5.1.1">responsible GAI</span> and <span class="ltx_text ltx_font_bold" id="S1.SS1.p5.1.2">human-AI alignment</span> <cite class="ltx_cite ltx_citemacro_citep">(Kenton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib66" title="">2021</a>; Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib164" title="">2023</a>)</cite>. We present the strategies employed by industry applications to navigate the intricate landscape and address the multifaceted aspects of these challenges.
Subsequently, in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S7" title="7. Open Problems ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">7</span></a>, we summarize and discuss the practical challenges and problems that have surfaced during our endeavors to land GAI in Recsys.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Key Contributions and Distinguishing from Related Works</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">While several academic surveys have been conducted on the research intersection between LLM/GAI and Recsys <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib85" title="">2023</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib82" title="">2023a</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib139" title="">2023c</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib36" title="">2023</a>)</cite>, there has <em class="ltx_emph ltx_font_italic" id="S1.SS2.p1.1.1">not</em> yet been a review addressing the integration and development of GAI within real-world Recsys. This survey work goes beyond previous works by not only covering new aspects such as <em class="ltx_emph ltx_font_italic" id="S1.SS2.p1.1.2">system architecture</em>, <em class="ltx_emph ltx_font_italic" id="S1.SS2.p1.1.3">LLMOps</em>, <em class="ltx_emph ltx_font_italic" id="S1.SS2.p1.1.4">GAI production framework design</em>, but also establishes links between practical application and a broader spectrum of GAI topics such as human-AI alignment and responsible AI practices. Additionally, we explore the unresolved challenges faced in real-world practice outside the confines of research labs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background and Fundamentals</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section aims to offer a concise overview of the foundational components of industrial Recsys and the merging GAI ecosystem, with an emphasis on the practical <em class="ltx_emph ltx_font_italic" id="S2.p1.1.1">operational</em> aspects inherent in real-world compound AI systems.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Industrial Recsys in a Nutshell</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">For all major social and e-commerce platforms, user satisfaction and commercial value growth often hinge upon the efficacy of Recsys in meeting increasingly challenging business goals <cite class="ltx_cite ltx_citemacro_citep">(Aggarwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib3" title="">2016</a>; Naumov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib93" title="">2019</a>; Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib151" title="">2018</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib157" title="">2019</a>)</cite>. As we illustrate in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F2" title="Figure 2 ‣ 2.1. Industrial Recsys in a Nutshell ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">2</span></a>, modern industrial Recsys have evolved into <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">compound AI systems</em>, comprising of multiple components that exhibit intricate interplay. The necessity of using compound AI systems stems from several factors.</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">The business and performance goals of industrial Recsys are <em class="ltx_emph ltx_font_italic" id="S2.I1.i1.p1.1.1">dynamic</em> and <em class="ltx_emph ltx_font_italic" id="S2.I1.i1.p1.1.2">vary widely</em>. Compound AI systems can adapt quickly due to their modular structure and operational flexibility.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">For a wide array of real-world tasks, constructing components and managing their interplay within the existing system often lead to <em class="ltx_emph ltx_font_italic" id="S2.I1.i2.p1.1.1">simpler</em> and more <em class="ltx_emph ltx_font_italic" id="S2.I1.i2.p1.1.2">cost-effective</em> solutions.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Compound AI systems offer <em class="ltx_emph ltx_font_italic" id="S2.I1.i3.p1.1.1">finer levels of control</em> throughout the product lifecycle, including development, integration, testing, trouble shooting, and maintenance.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="404" id="S2.F2.g1" src="extracted/5655486/images/recsys.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Overview of the key components employed by modern industrial social and e-commerce Recsys. The modeling stack typically consists of data, representation, modeling, objective, and product layers. The infra stack often comprises diverse serving, streaming, backend serveries orchestrated using MLOps techniques.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">As subsequently demonstrated in this survey, incorporating GAI solutions also necessitates a holistic approach with <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">system thinking</em>. This entails considering the current system architecture, infrastructural limitations, operational capabilities, and product/service-level agreements to avoid unintended consequences. Conversely, GAI solutions can capitalize on the mature solutions including reusable components and design patterns inherent in existing Recsys.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Major Deficiencies of Existing Systems</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">GAI and foundation models can effectively address several major Deficiencies presented in many existing industrial Recsys:</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Insufficient open-world memorization and generalization</span> – the memorization and generalization (which are the foundations of personalization) of most social and e-comm Recsys relies primarily on members’ in-app/web data and engagements.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">Lack of active content curation</span> – industrial social and e-comm Recsys are typically designed to passively retrieve and rank raw contents, with minimum focus on repurposing and curating contents to meet user’s highly diversified information indeed.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">Absence of interactive reasoning</span> – the majority of industrial Recsys operate within the one-shot engagement paradigm and are unable to facilitate interactive reasoning regarding the scene, context, user need, explanation, etc.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Building GAI Foundation and LLMOps</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Developing and serving GAI can involve <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">modeling</em>, <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.2">infrastructure</em>, and <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.3">operational</em> components that are not readily available in Recsys. We provide a comprehensive overview in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F3" title="Figure 3 ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F4" title="Figure 4 ‣ 2.3.1. GAI ecosystem. ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a> that is in accordance with the scope of this tutorial.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="547" id="S2.F3.g1" src="extracted/5655486/images/GAIEcosystem.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Illustration of the GAI foundations that will be the focus areas of our tutorial.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1. </span>GAI ecosystem.</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">Prompt engineering is an effective approach to direct LLM towards producing desired outcome for specific tasks.
Prompting LLM and applying relevant techniques like in-context learning, chain-of-thought, and other enhancements has rapidly become essential for adopting LLM for specific tasks <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib132" title="">2022</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib85" title="">2023</a>; Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib92" title="">2022</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib159" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib128" title="">2022a</a>)</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F3" title="Figure 3 ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a> provides a summary of the elements required for implementing prompt engineering solutions in production, which we will elucidate throughout the remainder of this survey.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">Prompting LLMs can be further endowed with <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS1.p2.1.1">tool-using</em> capabilities <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib114" title="">2024</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib146" title="">2024</a>)</cite>, enabling them to address compositional tasks through chains of <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS1.p2.1.2">reasoning and action</em> <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib149" title="">2022</a>)</cite>. This also catalyzed the development of LLM-based autonomous agents that integrate planning, memory (including content comprehension), and tool utilization <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib124" title="">2023e</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib141" title="">2023a</a>; Bubeck et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib13" title="">2023</a>)</cite>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F3" title="Figure 3 ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a>, we illustrate the requisite services and functions for accessing the internal and external services, tools, and memory within production systems.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p3">
<p class="ltx_p" id="S2.SS3.SSS1.p3.1">While prompt engineering has demonstrated reasonable proof-of-concept results across various tasks, scaling up and ensuring sustainability of GAI solutions often requires <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS1.p3.1.1">GAI model optimization</span>, which is a key direction to address cost, latency, and the effective usage of proprietary data and domain knowledge. Fortunately, task-specific GAI model development is often simpler through effective fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib50" title="">2021</a>; Li and Liang, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib80" title="">2021</a>)</cite> and learning from human feedback <cite class="ltx_cite ltx_citemacro_citep">(Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib97" title="">2022</a>; Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib105" title="">2024</a>)</cite>. Training GAI solutions from scratch is often less undesired <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib12" title="">2020</a>; Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib106" title="">2020</a>)</cite>. Serving up GAI deployment usually involves a collaborative refinement of the algorithm (e.g. through model compression, sparsity, operator fusion, distillation, pruning, efficient decoding techniques <cite class="ltx_cite ltx_citemacro_citep">(Zafrir et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib153" title="">2019</a>; Jiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib61" title="">2019</a>; Pope et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib103" title="">2023</a>; Niu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib95" title="">2021</a>)</cite>) and the AI system architecture (e.g. with quantization, PagedAttention, parallel computation <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib72" title="">2023</a>; Miao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib91" title="">2023</a>)</cite>). Finally, we mention that non-GAI models will remain essential in the ecosystem as <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS1.p3.1.2">supplement and edge functions</em>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="263" id="S2.F4.g1" src="extracted/5655486/images/LLMOps.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Overview of DevOps, MLOps, and LLMOps. The key LLMOps terms will be clarified in the rest of this survey.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2. </span>LLMOps.</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Similar to other internet applications, delivering GAI products that can meet client expectations remains a significant challenge unless the workflows can be effectively automated and operationalized. This is addressed within the domain of LLMOps (as we outlined in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F4" title="Figure 4 ‣ 2.3.1. GAI ecosystem. ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>), which represents an evolution of <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">development operations</em> (DevOps) and <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.1.2">machine learning operations</em> (MLOps) <cite class="ltx_cite ltx_citemacro_citep">(Kreuzberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib71" title="">2023</a>; Jabbari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib56" title="">2016</a>)</cite>.
In addition to leveraging insights from the previous generation of software and machine learning product development, there are several new practices and concepts crucial for harnessing and managing the power of GAI. The most critical ones include <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.1.3">GAI evaluation</em> and the <em class="ltx_emph ltx_font_italic" id="S2.SS3.SSS2.p1.1.4">trust and responsible GAI</em> <cite class="ltx_cite ltx_citemacro_citep">(Baxter and Schlesinger, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib7" title="">2023</a>; Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib19" title="">2023</a>)</cite>, which will be further elaborated in the next sections.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Classification Framework of GAI in Recsys</h3>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="190" id="S2.F5.g1" src="extracted/5655486/images/taxonomy.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Real-world product taxonomy of social/e-commerce Recsys and they best fit into the major solution categories of GAI in Recsys solutions.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Emphasizing <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.1">practicality</em> and <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.2">feasibility</em> in real-world deployment, our taxonomy and classification framework (depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F5" title="Figure 5 ‣ 2.4. Classification Framework of GAI in Recsys ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">5</span></a>) is organized by: 1). real-world production tasks of social and e-commerce Recsys, 2). their <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.3">optimal realization</em> through three primary categories of GAI methods.
Notably, our framework is <em class="ltx_emph ltx_font_italic" id="S2.SS4.p1.1.4">application-centric</em>, thus facilitating a comprehensive examination of the deployment of GAI in real-world Recsys.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>GAI-enhanced data, feature, and modeling for Recsys</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">For most existing social and e-commerce Recsys, the goal of recommendation is to identify good matches and tailor contents and experiences to best align with <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">personalized preferences</em> <cite class="ltx_cite ltx_citemacro_citep">(Ricci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib110" title="">2010</a>)</cite>.
Given GAI’s unprecedented capability to process complex multi-modal user-item information/contexts/sequences <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib152" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib76" title="">2023b</a>; Sarkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib113" title="">2023</a>)</cite>, solve cold-start/few-shot problems <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib34" title="">2021</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib48" title="">2024</a>)</cite>, and serve as various other components outlined in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F2" title="Figure 2 ‣ 2.1. Industrial Recsys in a Nutshell ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">2</span></a>, several preliminary studies have recognized the potential of GAI to augment personalized recommendation.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Research Progress</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3.F6" title="Figure 6 ‣ 3.1. Research Progress ‣ 3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">6</span></a>, we present a comprehensive overview of the predominant trends in existing efforts, where the <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.1">data and feedback processing</em> and <em class="ltx_emph ltx_font_italic" id="S3.SS1.p1.1.2">feature engineering</em> stages are also taken into account.
The key advancements in applying GAI for data and feedback processing include enriching user/item profile extraction and tagging <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib74" title="">2023a</a>; Brinkmann et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib11" title="">2023</a>)</cite>, comprehending user intent and interests <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib24" title="">2023</a>)</cite>, condensing and augmenting data <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib138" title="">2023b</a>)</cite>, integrating external knowledge <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib21" title="">2023b</a>; Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib150" title="">2023</a>)</cite>, and simulating and generating records <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib125" title="">2023f</a>)</cite>. We note that there is a growing body of literature on using Agent as Recsys user simulator <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib154" title="">2023d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib156" title="">b</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib125" title="">2023f</a>)</cite>, which represents an alternative application of agents to the one we will present in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5" title="5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S3.F6.g1" src="extracted/5655486/images/GAIinRecsys.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Overview of the predominant trends of using GAI to improve personalized retrieval and ranking in social and e-commerce Recsys. We also include rendering and interaction stages in the circle for completeness.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.1.1">feature engineering</em>, Recsys primarily utilize sparse categorical features and their representations. Through not entirely GAI-driven, the advent of LLM has presented opportunities to further improve representation learning especially for textual features and multi-modal data <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib161" title="">2023</a>; Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib109" title="">2023</a>; Sarkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib113" title="">2023</a>)</cite>. Graph data has also been introduced to enhance LLM’s capability to generate high-quality representations <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib134" title="">2024b</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib142" title="">2023</a>)</cite>. Recent studies have proposed using LLM for ID-based representation of users and items <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib42" title="">2022</a>; Hua et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib51" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">GAI techniques have also been explored for <em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.1">retrieval and ranking tasks</em>. Generative retrieval and recommendation has emerged as a promising domain where GAI directly generates item IDs as output <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib118" title="">2024</a>; Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib107" title="">2024</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib143" title="">2023</a>)</cite>. Some studies also follow NLP approaches to use GAI for top-K recommendation <cite class="ltx_cite ltx_citemacro_citep">(Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib28" title="">2023</a>)</cite>. Other major directions include context-aware recommendation where GAI’s world knowledge can serve as important background information <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib140" title="">2023b</a>; Harte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib45" title="">2023</a>)</cite>. For a more detailed examination of this subject, please refer to the comprehensive surveys of <cite class="ltx_cite ltx_citemacro_citet">Lin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib82" title="">2023a</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib139" title="">2023c</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Solution Framework and Practical Considerations</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since the subject matter addressed in this section usually represent the initial attempts of adopting GAI to the offline development of real-world Recsys, investing in GAI foundation and LLMOps (especially the offline elements described in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F3" title="Figure 3 ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2.F4" title="Figure 4 ‣ 2.3.1. GAI ecosystem. ‣ 2.3. Building GAI Foundation and LLMOps ‣ 2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>) can substantially expedite GAI-in-the-loop endeavors in the future. During this stage, latency is typically not a significant obstacle for batch offline inference, and prompt engineering/fine-tuning LLM can often deliver tangible improvements across various tasks <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib85" title="">2023</a>)</cite>. Furthermore, the existing metrics and frameworks for offline and online Recsys evaluation are still applicable <cite class="ltx_cite ltx_citemacro_citep">(Castells and Moffat, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib18" title="">2022</a>; Kohavi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib70" title="">2020</a>)</cite>, as the final forms of recommendation remains unchanged.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S3.F7.g1" src="extracted/5655486/images/GAI4Recsys-flow.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>A standard workflow for productionizing prompt engineering solution for enhancing Recsys training.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Nevertheless, while leveraging prompt engineering for the development of novel features may seem straightforward, ensuring the seamless transition of temporary prototypes to large-scale production necessitates <em class="ltx_emph ltx_font_italic" id="S3.SS2.p2.1.1">operational strategies</em> (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3.F7" title="Figure 7 ‣ 3.2. Solution Framework and Practical Considerations ‣ 3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">7</span></a>). This process entails the construction of an end-to-end pipeline that facilitates effectively prompt tuning and evaluation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib87" title="">2021</a>)</cite>, versioning and publishing prompts <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib137" title="">2023a</a>)</cite>, and grounding and monitoring to ensure the correct services and contexts are triggered and utilized.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In practice, we have observed that using ”small” in-house and task-specific GAI models can offer cost and performance advantage over using the open ”large-model” solutions.
This finding is substantiated by the fact that many smaller-scale applications usually do not require GAI excelling at both task specialization and generation <cite class="ltx_cite ltx_citemacro_citep">(Schick and Schütze, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib115" title="">2020</a>; Bommasani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib9" title="">2021</a>)</cite>. Nevertheless, ensuring the efficacy of ”small” models often require model fine-tuning and alignment with proprietary data, as well as the development of instruction, demonstration, and in-context learning strategies <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib85" title="">2023</a>; Min et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib92" title="">2022</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib87" title="">2021</a>)</cite>. These important aspects are also revealed from the recent industrial efforts <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib27" title="">2022</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib36" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Lastly, we note that personalized recommendation has been extensively studied in the past two decades, leading to many practical and effective domain methods <cite class="ltx_cite ltx_citemacro_citep">(Perugini and Gonçalves, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib100" title="">2002</a>; Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib68" title="">2022</a>)</cite>.
Considering the maturity and efficacy of many time-tested industrial Recsys,
the potential value of integrating GAI may be more pronounced in domains where existing RecSys exhibit limitations. These areas will be explored in detail in the subsequent sections.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Augmenting the Curation Capability of Recsys</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Beyond enhancing personalized retrieval and ranking, GAI-powered curator allows Recsys to transcend the limitation of only showing human-generated contents. It enables <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">re-purposing</span>, <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">explaining</span>, and <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">curating contents using external knowledge</span> to meet diversified needs and elevate transparency and trust <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib129" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib127" title="">d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib126" title="">2022b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">However, using GAI to directly generate customer-facing contents can suffer from <em class="ltx_emph ltx_font_italic" id="S4.p2.1.1">inadequate knowledge</em> of both the user and subject matter, <em class="ltx_emph ltx_font_italic" id="S4.p2.1.2">limited task expertise</em>, <em class="ltx_emph ltx_font_italic" id="S4.p2.1.3">lack of control of the output</em>, and various other <em class="ltx_emph ltx_font_italic" id="S4.p2.1.4">trust and safety issues</em> that cannot be fully addressed through improving prompting or model tuning techniques <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib53" title="">2023b</a>; Petroni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib101" title="">2019</a>; Carlini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib16" title="">2021</a>)</cite>.
As our earlier discussions suggest, building compound AI system can be a pragmatic approach to systematically address the above challenges, which leads to our introduction of the <em class="ltx_emph ltx_font_italic" id="S4.p2.1.5">retrieval-augmented generation</em> (RAG) – a technique that integrates external data retrieval into the generative process <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib75" title="">2022</a>; Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib73" title="">2020</a>)</cite>.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Research Progress</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Until not long ago, Recsys’ curation capability (e.g. explanation generation) primarily relies on the efficacy of templates and manual curation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib78" title="">2020</a>)</cite>, which often lack richness and in-depth context comprehension and knowledge.
RAG systems combine the strengths of retrieval-based and GAI-based methods to enhance the accuracy, credibility, and relevance of the GAI outputs for various Recsys usages <cite class="ltx_cite ltx_citemacro_citep">(Di Palma, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib33" title="">2023</a>)</cite>. The scope of retrieval can extend beyond user-generated content to, for example, behavioral sequences <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib83" title="">2023b</a>)</cite>. We provide a concise overview of RAG system in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.F8" title="Figure 8 ‣ 4.1. Research Progress ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S4.F8.g1" src="extracted/5655486/images/RAG-naive.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Illustration of a typical RAG system marked with the areas that can be optimization within the system.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">RAG is a rapidly developing technique (with variants such as self-RAG, auto-RAG, Corrective RAG) where the workflow is further improved by adding components like retrieval result verification and re-ranking <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib144" title="">2024</a>; Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib162" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib79" title="">2023c</a>)</cite>, adaptive retrieval <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib60" title="">2023b</a>)</cite>, and query rewrite <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib89" title="">2023</a>)</cite>.
Typically, incoming requests are first parsed into queries and prompts. The queries are directed to the retrieval and re-ranking service (with pre-trained encoder and datastore index) to retrieve the most relevant contents. The contents are subsequently leveraged to improve generation by such as <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.1">enriching/augmenting</em> the input prompts and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">correcting</em> LLM’s output.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">It is noteworthy that the output of RAG can be <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.1">multi-modal</em>, including images (including attribute change of existing images and stylized images) and videos, through the integration of diffusion and CLIP models <cite class="ltx_cite ltx_citemacro_citep">(Rombach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib111" title="">2022</a>; Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib104" title="">2021</a>)</cite>. The <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.2">adaptability</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS1.p3.1.3">versatility</em> of RAG have positioned it as a prominent GAI solution, particularly suited for tasks requiring integration with excessive knowledge. <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib39" title="">2023</a>)</cite>. As we will demonstrate in the subsequent section, RAG also holds promise for effective utilization by AI agents.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Solutions Framework and Practical Considerations</h3>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="121" id="S4.F9.g1" src="extracted/5655486/images/RAG-landing.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Taxonomies and overview of the solution framework for building RAG as GAI curator in Recsys.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Despite that RAG shares several common building blocks with Recsys and holds promising benchmark results <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib20" title="">2023a</a>)</cite>, implementing and integrating RAG into Recsys presents significant challenges in practice. We summarize them into three folds as below, and present an overview of the solution framework in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.F9" title="Figure 9 ‣ 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">9</span></a>. Generally, addressing these challenges requires system-level solutions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">evaluation</span> – unlike evaluating item recommendations, assessing curation outcome in Recsys is often feasible only during runtime, and the metric taxonomy for <em class="ltx_emph ltx_font_italic" id="S4.I1.i1.p1.1.2">natural language generation</em> (NLG) differs from that of RecSys <cite class="ltx_cite ltx_citemacro_citep">(Sai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib112" title="">2022</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib160" title="">2022</a>)</cite>;</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">deployment</span> – deploying RAG within the service-level agreement (SLA) of Recsys necessitate optimizations across algorithm, AI system, and the serving infrastructure;</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">building and refining RAG</span> involve fostering synergies among multiple interacting components, both offline and online.</p>
</div>
</li>
</ol>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Evaluating RAG performance offline.</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">RAG system is difficult to evaluate offline because of the dependency on real-time and operation contexts. We have found the application of <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p1.1.1">canary testing</em>, a technique derived from DevOps practices <cite class="ltx_cite ltx_citemacro_citep">(Tarvo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib120" title="">2015</a>)</cite>, along with synthetic generation to be effective. Canary testing enables the real-time performance tracking of RAG by replicating online requests within the same context without impacting user experience. Leveraging the tracked events, we can synthesize records and conduct various NLG evaluations, such as employing BLEU, ROUGE, and other hallucination metrics <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib58" title="">2023</a>; Gatt and Krahmer, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib40" title="">2018</a>)</cite>, and performing risk assessment (refer to Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6" title="6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">6</span></a> for further detail).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">In contrast to traditional NLG tasks like translation, summarization, and open-book Q&amp;A, there lacks well-established evaluation solutions for such as <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">explanation generation</em> for Recsys. Our practical experience suggests that expert evaluation, even on a modest scale, proves highly effective. Nonetheless, it is crucial to establish explicit standards and guidelines to minimize ambiguities and misunderstandings during expert evaluation <cite class="ltx_cite ltx_citemacro_citep">(Howcroft et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib49" title="">2020</a>)</cite>. Lastly, standard relevance metrics such as precision@k and recall@k can be helpful for specific tasks where ground truth is available.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Development.</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">For <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p1.1.1">building</span> the initial RAG solution, a typical roadmap involves two phases:</p>
<ul class="ltx_itemize" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">offline</span> – constructing data stores along with the development of the encoder and index for each data store <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib65" title="">2020</a>)</cite>;</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">online</span> – setting up execution flows for query processing, chunk re-rank/process/consolidate, and prompting LLM <cite class="ltx_cite ltx_citemacro_citep">(Guu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib44" title="">2020</a>; Borgeaud et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib10" title="">2022</a>)</cite>.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S4.SS2.SSS2.p1.2">Following these phases, the RAG system can be encapsulated as a standalone service module, responsible for managing incoming requests from the RecSys (as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.F9" title="Figure 9 ‣ 4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Deployment.</h4>
<div class="ltx_para" id="S4.SS2.SSS3.p1">
<p class="ltx_p" id="S4.SS2.SSS3.p1.1">Given that social and e-commerce Recsys often adhere to stringent SLA <cite class="ltx_cite ltx_citemacro_citep">(Kersbergen and Schelter, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib67" title="">2021</a>)</cite>, may encounter challenges in meeting these SLAs when handling inputs or outputs of moderate lengths. As elaborated in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S2" title="2. Background and Fundamentals ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">2</span></a>, optimizing LLM inference typically involves a combination of algorithmic and AI system enhancements <cite class="ltx_cite ltx_citemacro_citep">(Miao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib91" title="">2023</a>)</cite>. Each of these methods entails its own trade-offs, so their efficacy should be discussed case-by-case. In practice, we have found adopting traditional web service optimization techniques, such as dynamic caching and pagination, proves valuable for circumventing the SLA <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib15" title="">1998</a>; Fredrich, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib37" title="">2012</a>)</cite>. Furthermore, these techniques have been adapted to specifically support LLM applications <cite class="ltx_cite ltx_citemacro_citep">(Bang, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib6" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>Iteration.</h4>
<div class="ltx_para" id="S4.SS2.SSS4.p1">
<p class="ltx_p" id="S4.SS2.SSS4.p1.1">Iterating and refining RAG solution post-launch often involve encoder and index update through sequential and asyncornized/batch training, improving the data store, and optimizing the querying process, chunk processing logic, and prompting strategies <cite class="ltx_cite ltx_citemacro_citep">(Izacard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib55" title="">2022</a>)</cite>. These components are also highlighted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.F8" title="Figure 8 ‣ 4.1. Research Progress ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS4.p2">
<p class="ltx_p" id="S4.SS2.SSS4.p2.1">However, it is noteworthy that directly optimizing RAG based on <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS4.p2.1.1">implicit</em> feedback from the recommendations, such as clicks, can be challenging due to the feedback being attributed to a combination of raw and curated contents.
Consequently, establishing explicit and active feedback loops for <em class="ltx_emph ltx_font_italic" id="S4.SS2.SSS4.p2.1.2">richer</em> user feedback can benefit the optimization of GAI curator in Recsys. This topic will be further explored in the next two sections.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Facilitating Action and Reasoning for Interactive Recsys</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Within the current Recsys paradigm, user engagement is generally facilitated via single-round, passive interactions through a constrained set of predefined mechanisms, including item icon clicks or search bar queries <cite class="ltx_cite ltx_citemacro_citep">(Aggarwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib3" title="">2016</a>; Ricci et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib110" title="">2010</a>)</cite>. This framework is tailored for scenarios in which passive feedback is considered sufficient for both user satisfaction and business objectives.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">However, such Recsys often lack ability to drive active interactions or leverage external tools (e.g. to access external knowledge bases) to further engage and satisfy users. Consequently, their applicability is restricted, and cannot generalize across recommendation scenarios or to meet emerging user demands for <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">interactivity</span> and participating in <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">active feedback</span> loops.
To address these shortcomings, we advocates for the augmenting RecSys with autonomous AI agent, harnessing their robust natural language reasoning capabilities and tool utilization proficiency <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib124" title="">2023e</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib141" title="">2023a</a>)</cite>, in conjunction with the GAI solution frameworks discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3" title="3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Research Progress</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">From Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S3" title="3. GAI-enhanced data, feature, and modeling for Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">3</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>, we see that GAI can indeed enable interactive recommendations by curating human-like responses for various tasks such as direct recommendation, preference refinement, recommended follow-up questions, and recommendation justification <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib57" title="">2021</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib52" title="">2024</a>)</cite>. AI agents can be employed to manage and orchestrate these interactive components <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib31" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib130" title="">2023c</a>)</cite>. In fact, the RAG system introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a> can be considered a simple AI agent with retrieval and generation modules.</p>
</div>
<figure class="ltx_figure" id="S5.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="S5.F10.g1" src="extracted/5655486/images/agent-overview.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Illustration of the components and capabilities of an AI agent for typical social and e-commerce scenarios.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">LLM-powered AI agents can utilize LLM to reason through problems, create plans, produce text-based outputs and actions, and interact with external tools via API calls.
The <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">planning</em> aspect can be facilitated by either providing the agent with a predefined reasoning graph, as discussed in <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib145" title="">2022</a>)</cite>, by harnessing LLM’s reasoning abilities to dynamically generate the reasoning path <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib132" title="">2022</a>)</cite>.
For <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.2">actions</em>, the agent can generate textual outputs following the decision-making processes regarding whether and how to engage with a particular tool <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib114" title="">2024</a>)</cite>. Recent research has delved into both of these capabilities for Recsys <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib130" title="">2023c</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib54" title="">2023a</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Solution Framework and Practical Considerations</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We start by conceptualizing an AI Agent (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.F10" title="Figure 10 ‣ 5.1. Research Progress ‣ 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">10</span></a>) that is equipped with a diverse array of capabilities to facilitate interactive recommendation processes.
Depending on the product design and requirement, the planning module can be one of the two types, where the main difference lies in how planning is executed: <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.1">predefined</em> or <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.2">generated in real time</em>.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>Agent with pre-defined reasoning graph.</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">We outline one example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.F11" title="Figure 11 ‣ 5.2.1. Agent with pre-defined reasoning graph. ‣ 5.2. Solution Framework and Practical Considerations ‣ 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">11</span></a> where the pre-defined reasoning graph is applied, and the role of LLM is to make decisions on how to navigate the graph and use tools to execute each step along the sequence.
This type of Agent is designed to minimize latency, cost, and <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS1.p1.1.1">operation overhead</em> – instead relying on LLM to generate outputs for a function call, we apply rules and light-weight supplement models for decision-making. However, this approach is mostly suitable for simpler applications with small action spaces. Also, creating a predefined reasoning graph also requires domain expertise, and many not be applicable in certain scenarios.</p>
</div>
<figure class="ltx_figure" id="S5.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="201" id="S5.F11.g1" src="extracted/5655486/images/Type1Agent.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Example of Type-1 GAI agent with pre-defined reasoning graph for in-session recommendation tasks.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Agent that uses GAI for planning.</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Techniques like chain-of-thoughts <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib132" title="">2022</a>)</cite> or tree-of-thoughts <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib148" title="">2024</a>)</cite> can be adopted to prompt LLM to generate reasoning paths on-the-fly. GAI planning can effectively <em class="ltx_emph ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">supplement</em> pre-defined reasoning for the long-tail behaviors and events. For further illustration, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S5.F12" title="Figure 12 ‣ 5.2.2. Agent that uses GAI for planning. ‣ 5.2. Solution Framework and Practical Considerations ‣ 5. Facilitating Action and Reasoning for Interactive Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">12</span></a> and the following paragraphs, we outline an interactive Recsys agent that enables seamless user engagements and active feedback.</p>
</div>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S5.F12.g1" src="extracted/5655486/images/Type2Agent.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Overview of a real-world Recsys agent for multi-round interactive social recommendation. The Responsible GAI component will be discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6" title="6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">6</span></a>.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Depending on the product and task requirements, there are practical considerations for setting up the action phase.
</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">Database query and Recsys API call</span> –
we discovered significant efficacy in developing LLM-oriented domain-specifc language (DSL) <cite class="ltx_cite ltx_citemacro_citep">(Mernik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib90" title="">2005</a>)</cite> that can be effectively translated into appropriate text2sql queries and API calls. When combined with standard few-shot learning and in-context learning/retrieval in run-time, the tool-using performance notably surpasses alternative options for getting data and recommendations.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.1.1">LLM tools</span> – to enable input augmentation, text summarization, and response generations, with ”small” LLM such as <em class="ltx_emph ltx_font_italic" id="S5.I1.i2.p1.1.2">Llama-7B</em> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib122" title="">2023</a>)</cite> and <em class="ltx_emph ltx_font_italic" id="S5.I1.i2.p1.1.3">Mistral-7B</em> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib59" title="">2023a</a>)</cite> typically being sufficient for these tasks.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.1.1">RAG tools</span> – to facilitate the various Recsys curation functionalities described in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4" title="4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i4.p1">
<p class="ltx_p" id="S5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i4.p1.1.1">Intent and ambiguity classification models</span> –
ambiguities in user input can lead to failures in agents’ planning process in practice. Using supplement models to characterize intents and ambiguity can enable agents to communicate with users for precise instructions.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i5.p1">
<p class="ltx_p" id="S5.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.I1.i5.p1.1.1">Internal workflow and session data models</span> – for real-time accessing and acting on the serving infrastructure such as data store (cache), schemas, streaming services, and more.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Lastly, the evaluation and continual optimization of Recsys agents involve more elements than what we outlined for RAG in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S4.SS2" title="4.2. Solutions Framework and Practical Considerations ‣ 4. Augmenting the Curation Capability of Recsys ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">4.2</span></a>. Many of the aspects require thorough examination, and we will elaborated them in the next sections.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Responsible GAI and Human-AI Alignment</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In social and e-commerce platforms, <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">trust</em> and <em class="ltx_emph ltx_font_italic" id="S6.p1.1.2">safety</em> stand out as the top product requirements for incorporating and serving LLM, RAG, and AI agents in Recsys.
Subsequent changes are introduced to the offline evaluation and AI improvement strategies. In particular, <em class="ltx_emph ltx_font_italic" id="S6.p1.1.3">risk assessment</em> of the generative outputs adds more dimensions to the traditional metrics like relevance, and <em class="ltx_emph ltx_font_italic" id="S6.p1.1.4">aligning</em> GAI to user preferences also necessities a considerable expansion of the current serving and offline training strategies of Recsys.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Trust and Responsible AI</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Several identified attributes of GAI outputs can violate the trust and responsible AI principles, especially concerning <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.1">bias</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.2">toxicity</em> <cite class="ltx_cite ltx_citemacro_citep">(Gehman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib41" title="">2020</a>)</cite>, <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.3">privacy</em> <cite class="ltx_cite ltx_citemacro_citep">(Carlini et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib16" title="">2021</a>)</cite>, and <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.4">hallucination</em> <cite class="ltx_cite ltx_citemacro_citep">(Rawte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib108" title="">2023</a>)</cite>. It has also been found that LLM are subject to adversarial attacks with crafted inputs, e.g. <em class="ltx_emph ltx_font_italic" id="S6.SS1.p1.1.5">jailbreak prompts</em>, to trigger undesired output <cite class="ltx_cite ltx_citemacro_citep">(Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib164" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">There are active investigations on the causes and modeling and data solutions of these issues <cite class="ltx_cite ltx_citemacro_citep">(Schick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib116" title="">2021</a>; Dathathri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib29" title="">2019</a>; Welbl et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib136" title="">2021</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib131" title="">2024a</a>)</cite>. But as we mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S1" title="1. Introduction ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">1</span></a>, compound AI system has natural advantages in improving control and trust, and our tutorial will focus on introducing <span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">system-level</span> solutions, especially:</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<ul class="ltx_itemize" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">Adopting <span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Red Teaming</span>, which uses a ”Red LLM” to regulate the behavior of the target LLM <cite class="ltx_cite ltx_citemacro_citep">(Perez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib99" title="">2022</a>)</cite>, has been demonstrated highly effective to alleviate both the bias and toxicity issues as well as preventing adversarial attacks and privacy leakage <cite class="ltx_cite ltx_citemacro_citep">(Ganguli et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib38" title="">2022</a>; Casper et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib17" title="">[n. d.]</a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">Incorporating system layers that use RAG (with knowledge base) or domain LLM and expert tools can often provide practical solutions to mitigate the hallucination problems <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib158" title="">2023c</a>; Tonmoy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib121" title="">2024</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1">In practice, the above solutions and related components constitute the <span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Trust &amp; Responsible GAI service</span>, which we elaborate in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.F13" title="Figure 13 ‣ 6.1. Trust and Responsible AI ‣ 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">13</span></a>. After integrated with the existing API and environment manager, the service will apply to both the input and output stage of the generation process and is responsible for communicating with the clients through response handling.</p>
</div>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="200" id="S6.F13.g1" src="extracted/5655486/images/RAI.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>Core components of the Trust &amp; Responsible GAI service to support the various system-level solutions to bias, toxicity, privacy and adversarial attach issues.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>GAI response risk assessment</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Alongside the service-time guardrails solutions to ensure responsible GAI, offline risk assessment of the generated contents is also critical for safeguarding prompting, RAG, and agent solutions in Recsys by ensuring <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.1">reliability</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p1.1.2">trustworthiness</em> <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib19" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib155" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">We specifically focus on assessing the risk perspective of GAI including ethical and bias issues and trust violations, given their crucial importance as product safeguards. Both conventional NLP testing approaches and innovative multifaceted exploration has been conducted to evaluate GAI’s toxicity, social bias, and trustworthiness vulnerabilities <cite class="ltx_cite ltx_citemacro_citep">(Zhuo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib163" title="">2023</a>; Dhamala et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib32" title="">2021</a>; Gehman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib41" title="">2020</a>; Parrish et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib98" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib123" title="">2023a</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="S6.F14.g1" src="extracted/5655486/images/GET.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Workflow of human-in-the-loop response risk assessment for RAG and AI agent in Recsys. It can be applied to generic natural language generation evaluation <cite class="ltx_cite ltx_citemacro_citep">(Howcroft et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib49" title="">2020</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib160" title="">2022</a>)</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">While those research probe the problem from different angles, a common consensus underscores the necessity of <em class="ltx_emph ltx_font_italic" id="S6.SS2.p3.1.1">dedicated</em> evaluation datasets and <em class="ltx_emph ltx_font_italic" id="S6.SS2.p3.1.2">human-in-the-loop</em> assessments <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib19" title="">2023</a>)</cite>. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.F14" title="Figure 14 ‣ 6.2. GAI response risk assessment ‣ 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">14</span></a>, we characterize a typical industrial service workflow for human-in-the-loop evaluation of GAI response risk assessment.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Human-AI alignment for GAI in Recsys</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In the course to machine learning, AI alignment has shown up explicitly before in the field of Recsys and information retrieval where learning-to-rank from bandit feedback and user preference have been studied intensively <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib77" title="">2010</a>; Afsar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib2" title="">2022</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib86" title="">2009</a>; Joachims et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib62" title="">2017</a>)</cite>. As we summarized in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#S6.F15" title="Figure 15 ‣ 6.3. Human-AI alignment for GAI in Recsys ‣ 6. Responsible GAI and Human-AI Alignment ‣ Survey for Landing Generative AI in Social and E-commerce Recsys – the Industry Perspectives"><span class="ltx_text ltx_ref_tag">15</span></a>, there is non-trivial overlap between the Recsys and GAI methodological topics in the broader human-AI alignment domain.</p>
</div>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="169" id="S6.F15.g1" src="extracted/5655486/images/alignment-topics.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15. </span>Some key directions for improving Human-AI alignment for GAI in Recsys.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">However, there are fundamental differences – for GAI, alignment problems are generally refer to <em class="ltx_emph ltx_font_italic" id="S6.SS3.p2.1.1">behavior alignment</em>, aiming create an agent that behaves in accordance with human instructions and values <cite class="ltx_cite ltx_citemacro_citep">(Kenton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib66" title="">2021</a>; Ouyang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib97" title="">2022</a>)</cite>. For Recsys, the alignment emphasizes more on matching and exploring human preferences and interests.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Clearly, introducing new criteria, objectives, and techniques from GAI will be beneficial. For example, the <em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.1">Helpful, Honest, Harmless</em> criteria proposed by <cite class="ltx_cite ltx_citemacro_citet">Lin et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib84" title="">2021</a>)</cite>, the adoption of <em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.2">reinforcement learning from human feedback</em> (RLHF) and <em class="ltx_emph ltx_font_italic" id="S6.SS3.p3.1.3">direct preference optimization</em> (DPO) to achieve these criteria <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib105" title="">2024</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib5" title="">2022</a>)</cite>, and the various technical directions outlined in <cite class="ltx_cite ltx_citemacro_citet">Bai et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib5" title="">2022</a>)</cite>. Another crucial aspect lies in <span class="ltx_text ltx_font_bold" id="S6.SS3.p3.1.4">user interface design</span> and <span class="ltx_text ltx_font_bold" id="S6.SS3.p3.1.5">communication strategies</span> <cite class="ltx_cite ltx_citemacro_citep">(Weisz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib135" title="">2024</a>)</cite>, as they can influence, regulate, incentivize, and direct users towards providing explicit preferences for continual learning through the aforementioned methods.</p>
</div>
<div class="ltx_para" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1">Understanding the cause of <em class="ltx_emph ltx_font_italic" id="S6.SS3.p4.1.1">misalignment</em> in GAI can also go a long way. Specifically, the emerging GAI capabilities are often attributed to the scaling law and extensive pre-training corpus <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib64" title="">2020</a>; Bubeck et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib13" title="">2023</a>)</cite>, so building insights to the data and model scaling properties can be critical for diagnosing issues and identifying opportunities <cite class="ltx_cite ltx_citemacro_citep">(Bender et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib8" title="">2021</a>; Dodge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib35" title="">2021</a>; Henighan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib47" title="">2020</a>)</cite>. A comprehensive solution can therefore incorporate both the traditional wisdom of <em class="ltx_emph ltx_font_italic" id="S6.SS3.p4.1.2">data cleaning</em>, <em class="ltx_emph ltx_font_italic" id="S6.SS3.p4.1.3">debiasing</em>, <em class="ltx_emph ltx_font_italic" id="S6.SS3.p4.1.4">multi-task learning</em>, as well as contemporary GAI techniques like calibration of prompting <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib159" title="">2021</a>)</cite> and reasoning with self-consistency <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib128" title="">2022a</a>)</cite>. Additionally, enforcing <em class="ltx_emph ltx_font_italic" id="S6.SS3.p4.1.5">controlled generation</em> by such as Red Teaming is also a practical approach for human-AI alignment.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Open Problems</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In the course of landing GAI in Recsys, we have faced several universal challenges on top of the application-specific problems outlined in the previous sections. Here, we emphasize a selection of these that hold particular practical significance and opportunity.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Effective in-house GAI serving stack</span> – for industrial systems, GAI <em class="ltx_emph ltx_font_italic" id="S7.p2.1.2">cost</em> and <em class="ltx_emph ltx_font_italic" id="S7.p2.1.3">latency</em> (especially those from the long input and token generation) significantly impact the experience of both developers and users when adopting GAI at the scale of Recsys, and within the constraints of existing service-level agreements. <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib96" title="">2023</a>; Pope et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib103" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib22" title="">2023c</a>; Community, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib26" title="">2023</a>)</cite>. Developing a systematic solution remains challenging despite the recent research advances in LLM serving <cite class="ltx_cite ltx_citemacro_citep">(Miao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib91" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">Imitating LLM on proprietary data and models</span> – constructing compact models that strikes a balance between <em class="ltx_emph ltx_font_italic" id="S7.p3.1.2">task imitation</em> with proprietary model and structured data, alongside LLM’s <em class="ltx_emph ltx_font_italic" id="S7.p3.1.3">generation capability</em>, can be instrumental for overcoming the computational and resource challenges while ensuring optimal efficiency and effectiveness for specific applications <cite class="ltx_cite ltx_citemacro_citep">(Gudibande et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib43" title="">2023</a>; Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib25" title="">2022</a>; Kaddour et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib63" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">High-quality human data</span>
– high quality data is the fuel for all stages in the GAI and Recsys development cycles. Unlike Recsys which can leverage a wide range of user feedback, the majority of task-specific labeled data for GAI training and evaluation is obtained through <em class="ltx_emph ltx_font_italic" id="S7.p4.1.2">human annotation</em>, including tasks such as classification or RLHF labeling for LLM alignment training. While numerous machine learning techniques can contribute to enhancing data quality, human data collection fundamentally relies on attention to detail and careful execution <cite class="ltx_cite ltx_citemacro_citep">(Callison-Burch, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib14" title="">2009</a>; Davani and Prabhakaran, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib30" title="">2022</a>; Aroyo and Welty, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib4" title="">2015</a>; Pleiss, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib102" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Online evaluation and monitoring for GAI in Recsys</span> – while the Recsys community has developed mature and sophisticated solutions for online controlled experiments <cite class="ltx_cite ltx_citemacro_citep">(Kohavi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib70" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib69" title="">2007</a>; Kreuzberger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib71" title="">2023</a>)</cite>, with the introduction of GAI, measuring online performance is confronted with challenges such as overcoming brittle metrics, measuring generative task performance at scale, ensuring reproducibity, and coming up with the appropriate monitoring and experimental designs for both user satisfaction and system efficiency <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib23" title="">2023d</a>; Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib81" title="">2022</a>; Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib88" title="">2021</a>; Srivastava et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2406.06475v1#bib.bib117" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this survey, we provide a comprehensive review of the promising initial attempts at integrating GAI into social and e-commerce Recsys. We traverse the landscape of industrial Recsys and GAI fundamentals, existing solution frameworks, their connections to research advancements, and the practical insights and challenges encountered. As a pioneering contribution in this field, our survey also serves as a guide for incorporating GAI into a broader range of industrial Recsys. Moreover, it benefits scholars and practitioners in both the GAI and Recsys communities by delineating clear practical roadmaps for embedding GAI into real-world applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Afsar et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
M Mehdi Afsar, Trafford
Crump, and Behrouz Far.
2022.

</span>
<span class="ltx_bibblock">Reinforcement learning based recommender systems: A
survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Comput. Surveys</em> 55,
7 (2022), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aggarwal et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Charu C Aggarwal et al<span class="ltx_text" id="bib.bib3.3.1">.</span>
2016.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.4.1">Recommender systems</em>.
Vol. 1.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aroyo and Welty (2015)</span>
<span class="ltx_bibblock">
Lora Aroyo and Chris
Welty. 2015.

</span>
<span class="ltx_bibblock">Truth is a lie: Crowd truth and the seven myths of
human annotation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">AI Magazine 36.1</em> (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Andy Jones,
Kamal Ndousse, Amanda Askell,
Anna Chen, Nova DasSarma,
Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan,
et al<span class="ltx_text" id="bib.bib5.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Training a helpful and harmless assistant with
reinforcement learning from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.4.1">arXiv preprint arXiv:2204.05862</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang (2023)</span>
<span class="ltx_bibblock">
Fu Bang. 2023.

</span>
<span class="ltx_bibblock">GPTCache: An open-source semantic cache for LLM
applications enabling faster answers and cost savings. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 3rd Workshop for Natural
Language Processing Open Source Software (NLP-OSS 2023)</em>.
212–218.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baxter and Schlesinger (2023)</span>
<span class="ltx_bibblock">
Kathy Baxter and Yoav
Schlesinger. 2023.

</span>
<span class="ltx_bibblock">Managing the Risks of Generative AI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hbr.org/2023/06/managing-the-risks-of-generative-ai" title="">https://hbr.org/2023/06/managing-the-risks-of-generative-ai</a>.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Emily M Bender, Timnit
Gebru, Angelina McMillan-Major, and
Shmargaret Shmitchell. 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language
models be too big?. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 2021 ACM
conference on fairness, accountability, and transparency</em>.
610–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A
Hudson, Ehsan Adeli, Russ Altman,
Simran Arora, Sydney von Arx,
Michael S Bernstein, Jeannette Bohg,
Antoine Bosselut, Emma Brunskill,
et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">arXiv preprint arXiv:2108.07258</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur
Mensch, Jordan Hoffmann, Trevor Cai,
Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark,
et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from
trillions of tokens. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">International conference
on machine learning</em>. PMLR, 2206–2240.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brinkmann et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Alexander Brinkmann, Roee
Shraga, Reng Chiz Der, and Christian
Bizer. 2023.

</span>
<span class="ltx_bibblock">Product Information Extraction using ChatGPT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">arXiv preprint arXiv:2306.14921</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann,
Nick Ryder, Melanie Subbiah,
Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell,
et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">Advances in neural information processing
systems</em> 33 (2020),
1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck,
Varun Chandrasekaran, Ronen Eldan,
Johannes Gehrke, Eric Horvitz,
Ece Kamar, Peter Lee,
Yin Tat Lee, Yuanzhi Li,
Scott Lundberg, et al<span class="ltx_text" id="bib.bib13.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early
experiments with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.4.1">arXiv preprint arXiv:2303.12712</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Callison-Burch (2009)</span>
<span class="ltx_bibblock">
Chris Callison-Burch.
2009.

</span>
<span class="ltx_bibblock">Fast, cheap, and creative: Evaluating translation
quality using Amazon’s Mechanical Turk.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of conference on empirical
methods in natural language processing</em> (2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (1998)</span>
<span class="ltx_bibblock">
Pei Cao, Jin Zhang, and
Kevin Beach. 1998.

</span>
<span class="ltx_bibblock">Active cache: Caching dynamic contents on the web.
In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Middleware’98: IFIP International Conference
on Distributed Systems Platforms and Open Distributed Processing</em>. Springer,
373–388.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Florian
Tramer, Eric Wallace, Matthew Jagielski,
Ariel Herbert-Voss, Katherine Lee,
Adam Roberts, Tom Brown,
Dawn Song, Ulfar Erlingsson,
et al<span class="ltx_text" id="bib.bib16.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Extracting training data from large language
models. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.1">30th USENIX Security Symposium (USENIX
Security 21)</em>. 2633–2650.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casper et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> ([n. d.])</span>
<span class="ltx_bibblock">
Stephen Casper, Jason
Lin, Joe Kwon, Gatlen Culp, and
Dylan Hadfield-Menell.
[n. d.].

</span>
<span class="ltx_bibblock">Explore, Establish, Exploit: Red Teaming Language
Models from Scratch, June 2023b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">URL http://arxiv. org/abs/2306.09442</em>
([n. d.]).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Castells and Moffat (2022)</span>
<span class="ltx_bibblock">
Pablo Castells and
Alistair Moffat. 2022.

</span>
<span class="ltx_bibblock">Offline recommender system evaluation: Challenges
and new directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">AI magazine</em> 43,
2 (2022), 225–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yupeng Chang, Xu Wang,
Jindong Wang, Yuan Wu,
Linyi Yang, Kaijie Zhu,
Hao Chen, Xiaoyuan Yi,
Cunxiang Wang, Yidong Wang,
et al<span class="ltx_text" id="bib.bib19.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">A survey on evaluation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.4.1">ACM Transactions on Intelligent Systems and
Technology</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin,
Xianpei Han, and Le Sun.
2023a.

</span>
<span class="ltx_bibblock">Benchmarking large language models in
retrieval-augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2309.01431</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiao Chen, Luyi Ma,
Xiaohan Li, Nikhil Thakurdesai,
Jianpeng Xu, Jason HD Cho,
Kaushiki Nag, Evren Korpeoglu,
Sushant Kumar, and Kannan Achan.
2023b.

</span>
<span class="ltx_bibblock">Knowledge graph completion models are few-shot
learners: An empirical study of relation labeling in e-commerce with llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">arXiv preprint arXiv:2305.09858</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Lingjiao Chen, Matei
Zaharia, and James Zou.
2023c.

</span>
<span class="ltx_bibblock">Frugalgpt: How to use large language models while
reducing cost and improving performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">arXiv preprint arXiv:2305.05176</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Lingjiao Chen, Matei
Zaharia, and James Zou.
2023d.

</span>
<span class="ltx_bibblock">How is ChatGPT’s behavior changing over time?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2307.09009</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou,
Alberto Lalama, Cj Adams,
Iris Qu, Yifat Amir,
Samer Chucri, Pierce Vollucci,
Fabio Soldo, Dina Bseiso,
Sarah Scodel, et al<span class="ltx_text" id="bib.bib24.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Large language models for user interest journeys.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">arXiv preprint arXiv:2305.15498</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou,
Shayne Longpre, Barret Zoph,
Yi Tay, William Fedus,
Yunxuan Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma,
et al<span class="ltx_text" id="bib.bib25.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.4.1">arXiv preprint arXiv:2210.11416</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Community (2023)</span>
<span class="ltx_bibblock">
MLOps Community.
2023.

</span>
<span class="ltx_bibblock">LLM in production responses.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.google.com/spreadsheets/d/13wdBwkX8vZrYKuvF4h2egPh0LYSn2GQSwUaLV4GUNaU/edit#gid=501618501" title="">https://docs.google.com/spreadsheets/d/13wdBwkX8vZrYKuvF4h2egPh0LYSn2GQSwUaLV4GUNaU/edit#gid=501618501</a>.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeyu Cui, Jianxin Ma,
Chang Zhou, Jingren Zhou, and
Hongxia Yang. 2022.

</span>
<span class="ltx_bibblock">M6-rec: Generative pretrained language models are
open-ended recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2205.08084</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao,
Haiyuan Zhao, Weijie Yu,
Zihua Si, Chen Xu,
Zhongxiang Sun, Xiao Zhang, and
Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Uncovering chatgpt’s capabilities in recommender
systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Proceedings of the 17th ACM Conference
on Recommender Systems</em>. 1126–1132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dathathri et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Sumanth Dathathri, Andrea
Madotto, Janice Lan, Jane Hung,
Eric Frank, Piero Molino,
Jason Yosinski, and Rosanne Liu.
2019.

</span>
<span class="ltx_bibblock">Plug and play language models: A simple approach to
controlled text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:1912.02164</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davani and Prabhakaran (2022)</span>
<span class="ltx_bibblock">
Mark Díaz Davani, Aida Mostafazadeh and
Vinodkumar Prabhakaran. 2022.

</span>
<span class="ltx_bibblock">Dealing with disagreements: Looking beyond the
majority vote in subjective annotations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">ransactions of the Association for
Computational Linguistics 10</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yang Deng, An Zhang,
Yankai Lin, Xu Chen,
Ji-Rong Wen, and Tat-Seng Chua.
2024.

</span>
<span class="ltx_bibblock">Large Language Model Powered Agents in the Web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">learning</em> 2
(2024), 20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhamala et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun,
Varun Kumar, Satyapriya Krishna,
Yada Pruksachatkun, Kai-Wei Chang, and
Rahul Gupta. 2021.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in
open-ended language generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings of
the 2021 ACM conference on fairness, accountability, and transparency</em>.
862–872.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Palma (2023)</span>
<span class="ltx_bibblock">
Dario Di Palma.
2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented recommender system: Enhancing
recommender systems with large language models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 17th ACM Conference on
Recommender Systems</em>. 1369–1373.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hao Ding, Yifei Ma,
Anoop Deoras, Yuyang Wang, and
Hao Wang. 2021.

</span>
<span class="ltx_bibblock">Zero-shot recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2105.08318</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dodge et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jesse Dodge, Maarten Sap,
Ana Marasović, William Agnew,
Gabriel Ilharco, Dirk Groeneveld,
Margaret Mitchell, and Matt Gardner.
2021.

</span>
<span class="ltx_bibblock">Documenting large webtext corpora: A case study on
the colossal clean crawled corpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2104.08758</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenqi Fan, Zihuai Zhao,
Jiatong Li, Yunqing Liu,
Xiaowei Mei, Yiqi Wang,
Jiliang Tang, and Qing Li.
2023.

</span>
<span class="ltx_bibblock">Recommender systems in the era of large language
models (llms).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">arXiv preprint arXiv:2307.02046</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fredrich (2012)</span>
<span class="ltx_bibblock">
Todd Fredrich.
2012.

</span>
<span class="ltx_bibblock">Restful service best practices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Recommendations for Creating Web Services</em>
(2012), 1–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ganguli et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Deep Ganguli, Liane
Lovitt, Jackson Kernion, Amanda Askell,
Yuntao Bai, Saurav Kadavath,
Ben Mann, Ethan Perez,
Nicholas Schiefer, Kamal Ndousse,
et al<span class="ltx_text" id="bib.bib38.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms:
Methods, scaling behaviors, and lessons learned.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.4.1">arXiv preprint arXiv:2209.07858</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong,
Xinyu Gao, Kangxiang Jia,
Jinliu Pan, Yuxi Bi, Yi
Dai, Jiawei Sun, and Haofen Wang.
2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language
models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">arXiv preprint arXiv:2312.10997</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gatt and Krahmer (2018)</span>
<span class="ltx_bibblock">
Albert Gatt and Emiel
Krahmer. 2018.

</span>
<span class="ltx_bibblock">Survey of the state of the art in natural language
generation: Core tasks, applications and evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Journal of Artificial Intelligence Research</em>
61 (2018), 65–170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gehman et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Samuel Gehman, Suchin
Gururangan, Maarten Sap, Yejin Choi,
and Noah A Smith. 2020.

</span>
<span class="ltx_bibblock">Realtoxicityprompts: Evaluating neural toxic
degeneration in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">arXiv preprint arXiv:2009.11462</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang
Liu, Zuohui Fu, Yingqiang Ge, and
Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as language processing (rlp): A
unified pretrain, personalized prompt &amp; predict paradigm (p5). In
<em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the 16th ACM Conference on
Recommender Systems</em>. 299–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gudibande et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Arnav Gudibande, Eric
Wallace, Charlie Snell, Xinyang Geng,
Hao Liu, Pieter Abbeel,
Sergey Levine, and Dawn Song.
2023.

</span>
<span class="ltx_bibblock">The false promise of imitating proprietary llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">arXiv preprint arXiv:2305.15717</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee,
Zora Tung, Panupong Pasupat, and
Mingwei Chang. 2020.

</span>
<span class="ltx_bibblock">Retrieval augmented language model pre-training.
In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">International conference on machine learning</em>.
PMLR, 3929–3938.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jesse Harte, Wouter
Zorgdrager, Panos Louridas, Asterios
Katsifodimos, Dietmar Jannach, and
Marios Fragkoulis. 2023.

</span>
<span class="ltx_bibblock">Leveraging large language models for sequential
recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 17th ACM
Conference on Recommender Systems</em>. 1096–1102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hassan (2019)</span>
<span class="ltx_bibblock">
Taha Hassan.
2019.

</span>
<span class="ltx_bibblock">Trust and trustworthiness in social recommender
systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Companion proceedings of the 2019
world wide web conference</em>. 529–532.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henighan et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Henighan, Jared
Kaplan, Mor Katz, Mark Chen,
Christopher Hesse, Jacob Jackson,
Heewoo Jun, Tom B Brown,
Prafulla Dhariwal, Scott Gray,
et al<span class="ltx_text" id="bib.bib47.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Scaling laws for autoregressive generative
modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.4.1">arXiv preprint arXiv:2010.14701</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang,
Zihan Lin, Hongyu Lu,
Ruobing Xie, Julian McAuley, and
Wayne Xin Zhao. 2024.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for
recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">European Conference on
Information Retrieval</em>. Springer, 364–381.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howcroft et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
David M Howcroft, Anya
Belz, Miruna Clinciu, Dimitra Gkatzia,
Sadid A Hasan, Saad Mahamood,
Simon Mille, Emiel Van Miltenburg,
Sashank Santhanam, and Verena Rieser.
2020.

</span>
<span class="ltx_bibblock">Twenty years of confusion in human evaluation: NLG
needs evaluation sheets and standardised definitions. In
<em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">13th International Conference on Natural Language
Generation 2020</em>. Association for Computational Linguistics,
169–182.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Edward J Hu, Yelong Shen,
Phillip Wallis, Zeyuan Allen-Zhu,
Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen.
2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2106.09685</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hua et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenyue Hua, Shuyuan Xu,
Yingqiang Ge, and Yongfeng Zhang.
2023.

</span>
<span class="ltx_bibblock">How to index item ids for recommendation foundation
models. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Proceedings of the Annual International
ACM SIGIR Conference on Research and Development in Information Retrieval in
the Asia Pacific Region</em>. 195–204.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Chengkai Huang, Tong Yu,
Kaige Xie, Shuai Zhang,
Lina Yao, and Julian McAuley.
2024.

</span>
<span class="ltx_bibblock">Foundation Models for Recommender Systems: A Survey
and New Perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">arXiv preprint arXiv:2402.11143</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Lei Huang, Weijiang Yu,
Weitao Ma, Weihong Zhong,
Zhangyin Feng, Haotian Wang,
Qianglong Chen, Weihua Peng,
Xiaocheng Feng, Bing Qin,
et al<span class="ltx_text" id="bib.bib53.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models:
Principles, taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.4.1">arXiv preprint arXiv:2311.05232</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xu Huang, Jianxun Lian,
Yuxuan Lei, Jing Yao,
Defu Lian, and Xing Xie.
2023a.

</span>
<span class="ltx_bibblock">Recommender ai agent: Integrating large language
models for interactive recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">arXiv preprint arXiv:2308.16505</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick
Lewis, Maria Lomeli, Lucas Hosseini,
Fabio Petroni, Timo Schick,
Jane Dwivedi-Yu, Armand Joulin,
Sebastian Riedel, and Edouard Grave.
2022.

</span>
<span class="ltx_bibblock">Atlas: Few-shot learning with retrieval augmented
language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">arXiv preprint arXiv:2208.03299</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabbari et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Ramtin Jabbari, Nauman bin
Ali, Kai Petersen, and Binish
Tanveer. 2016.

</span>
<span class="ltx_bibblock">What is DevOps? A systematic mapping study on
definitions and practices. In <em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of the
scientific workshop proceedings of XP2016</em>. 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jannach et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dietmar Jannach, Ahtsham
Manzoor, Wanling Cai, and Li Chen.
2021.

</span>
<span class="ltx_bibblock">A survey on conversational recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">ACM Computing Surveys (CSUR)</em>
54, 5 (2021),
1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee,
Rita Frieske, Tiezheng Yu,
Dan Su, Yan Xu, Etsuko
Ishii, Ye Jin Bang, Andrea Madotto,
and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Comput. Surveys</em> 55,
12 (2023), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre
Sablayrolles, Arthur Mensch, Chris
Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna
Lengyel, Guillaume Lample, Lucile
Saulnier, et al<span class="ltx_text" id="bib.bib59.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.4.1">arXiv preprint arXiv:2310.06825</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zhengbao Jiang, Frank F
Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu,
Yiming Yang, Jamie Callan, and
Graham Neubig. 2023b.

</span>
<span class="ltx_bibblock">Active Retrieval Augmented Generation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 2023 Conference on Empirical
Methods in Natural Language Processing</em>. 7969–7992.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Xiaoqi Jiao, Yichun Yin,
Lifeng Shang, Xin Jiang,
Xiao Chen, Linlin Li,
Fang Wang, and Qun Liu.
2019.

</span>
<span class="ltx_bibblock">Tinybert: Distilling bert for natural language
understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">arXiv preprint arXiv:1909.10351</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joachims et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Thorsten Joachims, Adith
Swaminathan, and Tobias Schnabel.
2017.

</span>
<span class="ltx_bibblock">Unbiased learning-to-rank with biased feedback. In
<em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the tenth ACM international
conference on web search and data mining</em>. 781–789.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaddour et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jean Kaddour, Joshua
Harris, Maximilian Mozes, Herbie
Bradley, Roberta Raileanu, and Robert
McHardy. 2023.

</span>
<span class="ltx_bibblock">Challenges and applications of large language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2307.10169</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam
McCandlish, Tom Henighan, Tom B Brown,
Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford,
Jeffrey Wu, and Dario Amodei.
2020.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">arXiv preprint arXiv:2001.08361</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas
Oğuz, Sewon Min, Patrick Lewis,
Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih.
2020.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question
answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">arXiv preprint arXiv:2004.04906</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zachary Kenton, Tom
Everitt, Laura Weidinger, Iason Gabriel,
Vladimir Mikulik, and Geoffrey Irving.
2021.

</span>
<span class="ltx_bibblock">Alignment of language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">arXiv preprint arXiv:2103.14659</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kersbergen and Schelter (2021)</span>
<span class="ltx_bibblock">
Barrie Kersbergen and
Sebastian Schelter. 2021.

</span>
<span class="ltx_bibblock">Learnings from a Retail Recommendation System on
Billions of Interactions at bol. com. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">2021 IEEE
37th International Conference on Data Engineering (ICDE)</em>. IEEE,
2447–2452.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyeyoung Ko, Suyeon Lee,
Yoonseo Park, and Anna Choi.
2022.

</span>
<span class="ltx_bibblock">A survey of recommendation systems: recommendation
models, techniques, and application fields.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Electronics</em> 11,
1 (2022), 141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kohavi et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Ron Kohavi, Randal M
Henne, and Dan Sommerfield.
2007.

</span>
<span class="ltx_bibblock">Practical guide to controlled experiments on the
web: listen to your customers not to the hippo. In
<em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Proceedings of the 13th ACM SIGKDD international
conference on Knowledge discovery and data mining</em>.
959–967.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kohavi et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ron Kohavi, Diane Tang,
and Ya Xu. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">Trustworthy online controlled experiments:
A practical guide to a/b testing</em>.

</span>
<span class="ltx_bibblock">Cambridge University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kreuzberger et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dominik Kreuzberger,
Niklas Kühl, and Sebastian
Hirschl. 2023.

</span>
<span class="ltx_bibblock">Machine learning operations (mlops): Overview,
definition, and architecture.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">IEEE access</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li,
Siyuan Zhuang, Ying Sheng,
Lianmin Zheng, Cody Hao Yu,
Joseph Gonzalez, Hao Zhang, and
Ion Stoica. 2023.

</span>
<span class="ltx_bibblock">Efficient memory management for large language
model serving with pagedattention. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Proceedings
of the 29th Symposium on Operating Systems Principles</em>.
611–626.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan
Perez, Aleksandra Piktus, Fabio Petroni,
Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis,
Wen-tau Yih, Tim Rocktäschel,
et al<span class="ltx_text" id="bib.bib73.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for
knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.4.1">Advances in Neural Information Processing
Systems</em> 33 (2020),
9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Chen Li, Yixiao Ge,
Jiayong Mao, Dian Li, and
Ying Shan. 2023a.

</span>
<span class="ltx_bibblock">Taggpt: Large language models are zero-shot
multimodal taggers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">arXiv preprint arXiv:2304.03022</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Huayang Li, Yixuan Su,
Deng Cai, Yan Wang, and
Lemao Liu. 2022.

</span>
<span class="ltx_bibblock">A survey on retrieval-augmented text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">arXiv preprint arXiv:2202.01110</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiacheng Li, Ming Wang,
Jin Li, Jinmiao Fu, Xin
Shen, Jingbo Shang, and Julian
McAuley. 2023b.

</span>
<span class="ltx_bibblock">Text is all you need: Learning language
representations for sequential recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining</em>. 1258–1267.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Lihong Li, Wei Chu,
John Langford, and Robert E Schapire.
2010.

</span>
<span class="ltx_bibblock">A contextual-bandit approach to personalized news
article recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Proceedings of the 19th
international conference on World wide web</em>. 661–670.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lei Li, Yongfeng Zhang,
and Li Chen. 2020.

</span>
<span class="ltx_bibblock">Generate neural template explanations for
recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">Proceedings of the 29th ACM
International Conference on Information &amp; Knowledge Management</em>.
755–764.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Xiaonan Li, Changtai Zhu,
Linyang Li, Zhangyue Yin,
Tianxiang Sun, and Xipeng Qiu.
2023c.

</span>
<span class="ltx_bibblock">Llatrieval: Llm-verified retrieval for verifiable
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">arXiv preprint arXiv:2311.07838</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Liang (2021)</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy
Liang. 2021.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2101.00190</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi
Bommasani, Tony Lee, Dimitris Tsipras,
Dilara Soylu, Michihiro Yasunaga,
Yian Zhang, Deepak Narayanan,
Yuhuai Wu, Ananya Kumar, et al<span class="ltx_text" id="bib.bib81.3.1">.</span>
2022.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.4.1">arXiv preprint arXiv:2211.09110</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jianghao Lin, Xinyi Dai,
Yunjia Xi, Weiwen Liu,
Bo Chen, Xiangyang Li,
Chenxu Zhu, Huifeng Guo,
Yong Yu, Ruiming Tang, et al<span class="ltx_text" id="bib.bib82.3.1">.</span>
2023a.

</span>
<span class="ltx_bibblock">How can recommender systems benefit from large
language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.4.1">arXiv preprint arXiv:2306.05817</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jianghao Lin, Rong Shan,
Chenxu Zhu, Kounianhua Du,
Bo Chen, Shigang Quan,
Ruiming Tang, Yong Yu, and
Weinan Zhang. 2023b.

</span>
<span class="ltx_bibblock">Rella: Retrieval-enhanced large language models for
lifelong sequential behavior comprehension in recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">arXiv preprint arXiv:2308.11131</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob
Hilton, and Owain Evans.
2021.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human
falsehoods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">arXiv preprint arXiv:2109.07958</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan,
Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig.
2023.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey
of prompting methods in natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Comput. Surveys</em> 55,
9 (2023), 1–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu et al<span class="ltx_text" id="bib.bib86.3.1">.</span>
2009.

</span>
<span class="ltx_bibblock">Learning to rank for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.4.1">Foundations and Trends® in
Information Retrieval</em> 3, 3
(2009), 225–331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiao Liu, Kaixuan Ji,
Yicheng Fu, Weng Lam Tam,
Zhengxiao Du, Zhilin Yang, and
Jie Tang. 2021.

</span>
<span class="ltx_bibblock">P-tuning v2: Prompt tuning can be comparable to
fine-tuning universally across scales and tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">arXiv preprint arXiv:2110.07602</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yao Lu, Max Bartolo,
Alastair Moore, Sebastian Riedel, and
Pontus Stenetorp. 2021.

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find
them: Overcoming few-shot prompt order sensitivity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">arXiv preprint arXiv:2104.08786</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong,
Pengcheng He, Hai Zhao, and
Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting in Retrieval-Augmented Large
Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">Proceedings of the 2023
Conference on Empirical Methods in Natural Language Processing</em>.
5303–5315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mernik et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Marjan Mernik, Jan
Heering, and Anthony M Sloane.
2005.

</span>
<span class="ltx_bibblock">When and how to develop domain-specific languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">ACM computing surveys (CSUR)</em>
37, 4 (2005),
316–344.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xupeng Miao, Gabriele
Oliaro, Zhihao Zhang, Xinhao Cheng,
Hongyi Jin, Tianqi Chen, and
Zhihao Jia. 2023.

</span>
<span class="ltx_bibblock">Towards efficient generative large language model
serving: A survey from algorithms to systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">arXiv preprint arXiv:2312.15234</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Sewon Min, Xinxi Lyu,
Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Rethinking the role of demonstrations: What makes
in-context learning work?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">arXiv preprint arXiv:2202.12837</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Naumov et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Maxim Naumov, Dheevatsa
Mudigere, Hao-Jun Michael Shi, Jianyu
Huang, Narayanan Sundaraman, Jongsoo
Park, Xiaodong Wang, Udit Gupta,
Carole-Jean Wu, Alisson G Azzolini,
et al<span class="ltx_text" id="bib.bib93.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Deep learning recommendation model for
personalization and recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.4.1">arXiv preprint arXiv:1906.00091</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilashi et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Mehrbakhsh Nilashi,
Dietmar Jannach, Othman bin Ibrahim,
Mohammad Dalvi Esfahani, and Hossein
Ahmadi. 2016.

</span>
<span class="ltx_bibblock">Recommendation quality, transparency, and website
quality for trust-building in recommendation agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">Electronic Commerce Research and
Applications</em> 19 (2016),
70–84.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Wei Niu, Jiexiong Guan,
Yanzhi Wang, Gagan Agrawal, and
Bin Ren. 2021.

</span>
<span class="ltx_bibblock">DNNFusion: accelerating deep neural networks
execution with advanced operator fusion. In
<em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">Proceedings of the 42nd ACM SIGPLAN International
Conference on Programming Language Design and Implementation</em>.
883–898.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">Chat gpt 4 painfully slow.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://community.openai.com/t/chat-gpt-4-painfully-slow/117996" title="">https://community.openai.com/t/chat-gpt-4-painfully-slow/117996</a>.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu,
Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal,
Katarina Slama, Alex Ray,
et al<span class="ltx_text" id="bib.bib97.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions
with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.4.1">Advances in neural information processing
systems</em> 35 (2022),
27730–27744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parrish et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alicia Parrish, Angelica
Chen, Nikita Nangia, Vishakh Padmakumar,
Jason Phang, Jana Thompson,
Phu Mon Htut, and Samuel R Bowman.
2021.

</span>
<span class="ltx_bibblock">BBQ: A hand-built bias benchmark for question
answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">arXiv preprint arXiv:2110.08193</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ethan Perez, Saffron
Huang, Francis Song, Trevor Cai,
Roman Ring, John Aslanides,
Amelia Glaese, Nat McAleese, and
Geoffrey Irving. 2022.

</span>
<span class="ltx_bibblock">Red teaming language models with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">arXiv preprint arXiv:2202.03286</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perugini and Gonçalves (2002)</span>
<span class="ltx_bibblock">
Saverio Perugini and
Marcos André Gonçalves.
2002.

</span>
<span class="ltx_bibblock">Recommendation and personalization: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Journal of Intelligent Information Systems</em>
(2002).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim
Rocktäschel, Patrick Lewis, Anton
Bakhtin, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. 2019.

</span>
<span class="ltx_bibblock">Language models as knowledge bases?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">arXiv preprint arXiv:1909.01066</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pleiss (2020)</span>
<span class="ltx_bibblock">
et al Pleiss, Geoff.
2020.

</span>
<span class="ltx_bibblock">Identifying mislabeled data using the area under
the margin ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Advances in Neural Information Processing
Systems 33</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pope et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Reiner Pope, Sholto
Douglas, Aakanksha Chowdhery, Jacob
Devlin, James Bradbury, Jonathan Heek,
Kefan Xiao, Shivani Agrawal, and
Jeff Dean. 2023.

</span>
<span class="ltx_bibblock">Efficiently scaling transformer inference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">Proceedings of Machine Learning and Systems</em>
5 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook
Kim, Chris Hallacy, Aditya Ramesh,
Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell,
Pamela Mishkin, Jack Clark,
et al<span class="ltx_text" id="bib.bib104.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural
language supervision. In <em class="ltx_emph ltx_font_italic" id="bib.bib104.4.1">International conference
on machine learning</em>. PMLR, 8748–8763.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit
Sharma, Eric Mitchell, Christopher D
Manning, Stefano Ermon, and Chelsea
Finn. 2024.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model
is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam
Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and
Peter J Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a
unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">Journal of machine learning research</em>
21, 140 (2020),
1–67.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shashank Rajput, Nikhil
Mehta, Anima Singh, Raghunandan
Hulikal Keshavan, Trung Vu, Lukasz
Heldt, Lichan Hong, Yi Tay,
Vinh Tran, Jonah Samost, et al<span class="ltx_text" id="bib.bib107.3.1">.</span>
2024.

</span>
<span class="ltx_bibblock">Recommender systems with generative retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib107.4.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawte et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Vipula Rawte, Amit Sheth,
and Amitava Das. 2023.

</span>
<span class="ltx_bibblock">A survey of hallucination in large foundation
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">arXiv preprint arXiv:2309.05922</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xubin Ren, Wei Wei,
Lianghao Xia, Lixin Su,
Suqi Cheng, Junfeng Wang,
Dawei Yin, and Chao Huang.
2023.

</span>
<span class="ltx_bibblock">Representation learning with large language models
for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">arXiv preprint arXiv:2310.15950</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ricci et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2010)</span>
<span class="ltx_bibblock">
Francesco Ricci, Lior
Rokach, and Bracha Shapira.
2010.

</span>
<span class="ltx_bibblock">Introduction to recommender systems handbook.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">Recommender systems handbook</em>.
Springer, 1–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rombach et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Robin Rombach, Andreas
Blattmann, Dominik Lorenz, Patrick
Esser, and Björn Ommer.
2022.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent
diffusion models. In <em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>.
10684–10695.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sai et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ananya B Sai, Akash Kumar
Mohankumar, and Mitesh M Khapra.
2022.

</span>
<span class="ltx_bibblock">A survey of evaluation metrics used for NLG
systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">ACM Computing Surveys (CSUR)</em>
55, 2 (2022),
1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarkar et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rohan Sarkar, Navaneeth
Bodla, Mariya I Vasileva, Yen-Liang Lin,
Anurag Beniwal, Alan Lu, and
Gerard Medioni. 2023.

</span>
<span class="ltx_bibblock">Outfittransformer: Learning outfit representations
for fashion recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer Vision</em>.
3601–3609.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Timo Schick, Jane
Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Eric Hambro,
Luke Zettlemoyer, Nicola Cancedda, and
Thomas Scialom. 2024.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to
use tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze (2020)</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich
Schütze. 2020.

</span>
<span class="ltx_bibblock">It’s not just size that matters: Small language
models are also few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:2009.07118</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Timo Schick, Sahana
Udupa, and Hinrich Schütze.
2021.

</span>
<span class="ltx_bibblock">Self-diagnosis and self-debiasing: A proposal for
reducing corpus-based bias in nlp.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Transactions of the Association for
Computational Linguistics</em> 9 (2021),
1408–1424.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav
Rastogi, Abhishek Rao, Abu Awal Md
Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro,
Aditya Gupta, Adrià Garriga-Alonso,
et al<span class="ltx_text" id="bib.bib117.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and
extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.4.1">arXiv preprint arXiv:2206.04615</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Weiwei Sun, Lingyong Yan,
Zheng Chen, Shuaiqiang Wang,
Haichao Zhu, Pengjie Ren,
Zhumin Chen, Dawei Yin,
Maarten Rijke, and Zhaochun Ren.
2024.

</span>
<span class="ltx_bibblock">Learning to tokenize for generative retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jiliang Tang, Xia Hu,
and Huan Liu. 2013.

</span>
<span class="ltx_bibblock">Social recommendation: a review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">Social Network Analysis and Mining</em>
3 (2013), 1113–1133.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tarvo et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Alexander Tarvo, Peter F
Sweeney, Nick Mitchell, VT Rajan,
Matthew Arnold, and Ioana Baldini.
2015.

</span>
<span class="ltx_bibblock">CanaryAdvisor: a statistical-based tool for canary
testing. In <em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">Proceedings of the 2015 International
Symposium on Software Testing and Analysis</em>. 418–422.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tonmoy et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
SM Tonmoy, SM Zaman,
Vinija Jain, Anku Rani,
Vipula Rawte, Aman Chadha, and
Amitava Das. 2024.

</span>
<span class="ltx_bibblock">A comprehensive survey of hallucination mitigation
techniques in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">arXiv preprint arXiv:2401.01313</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut
Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al<span class="ltx_text" id="bib.bib122.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.4.1">arXiv preprint arXiv:2302.13971</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Boxin Wang, Weixin Chen,
Hengzhi Pei, Chulin Xie,
Mintong Kang, Chenhui Zhang,
Chejian Xu, Zidi Xiong,
Ritik Dutta, Rylan Schaeffer,
et al<span class="ltx_text" id="bib.bib123.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Decodingtrust: A comprehensive assessment of
trustworthiness in gpt models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.4.1">arXiv preprint arXiv:2306.11698</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma,
Xueyang Feng, Zeyu Zhang,
Hao Yang, Jingsen Zhang,
Zhiyuan Chen, Jiakai Tang,
Xu Chen, Yankai Lin, et al<span class="ltx_text" id="bib.bib124.3.1">.</span>
2023e.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous
agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.4.1">arXiv preprint arXiv:2308.11432</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Lei Wang, Jingsen Zhang,
Xu Chen, Yankai Lin,
Ruihua Song, Wayne Xin Zhao, and
Ji-Rong Wen. 2023f.

</span>
<span class="ltx_bibblock">Recagent: A novel simulation paradigm for
recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">arXiv preprint arXiv:2306.02552</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Shoujin Wang, Xiuzhen
Zhang, Yan Wang, and Francesco Ricci.
2022b.

</span>
<span class="ltx_bibblock">Trustworthy recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.3.1">ACM Transactions on Intelligent Systems and
Technology</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Wenjie Wang, Xinyu Lin,
Fuli Feng, Xiangnan He, and
Tat-Seng Chua. 2023d.

</span>
<span class="ltx_bibblock">Generative recommendation: Towards next-generation
recommender paradigm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">arXiv preprint arXiv:2304.03516</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei,
Dale Schuurmans, Quoc Le,
Ed Chi, Sharan Narang,
Aakanksha Chowdhery, and Denny Zhou.
2022a.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought
reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">arXiv preprint arXiv:2203.11171</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yan Wang, Zhixuan Chu,
Xin Ouyang, Simeng Wang,
Hongyan Hao, Yue Shen,
Jinjie Gu, Siqiao Xue,
James Y Zhang, Qing Cui, et al<span class="ltx_text" id="bib.bib129.3.1">.</span>
2023b.

</span>
<span class="ltx_bibblock">Enhancing recommender systems with large language
model reasoning graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.4.1">arXiv preprint arXiv:2308.10835</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Yancheng Wang, Ziyan
Jiang, Zheng Chen, Fan Yang,
Yingxue Zhou, Eunah Cho,
Xing Fan, Xiaojiang Huang,
Yanbin Lu, and Yingzhen Yang.
2023c.

</span>
<span class="ltx_bibblock">Recmind: Large language model powered agent for
recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">arXiv preprint arXiv:2308.14296</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Alexander Wei, Nika
Haghtalab, and Jacob Steinhardt.
2024a.

</span>
<span class="ltx_bibblock">Jailbroken: How does llm safety training fail?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang,
Dale Schuurmans, Maarten Bosma,
Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib132.3.1">.</span>
2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in
large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.4.1">Advances in neural information processing
systems</em> 35 (2022),
24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Kangning Wei, Jinghua
Huang, and Shaohong Fu.
2007.

</span>
<span class="ltx_bibblock">A survey of e-commerce recommender systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">2007 international conference on service systems
and service management</em>. IEEE, 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Wei Wei, Xubin Ren,
Jiabin Tang, Qinyong Wang,
Lixin Su, Suqi Cheng,
Junfeng Wang, Dawei Yin, and
Chao Huang. 2024b.

</span>
<span class="ltx_bibblock">Llmrec: Large language models with graph
augmentation for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">Proceedings of
the 17th ACM International Conference on Web Search and Data Mining</em>.
806–815.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weisz et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Justin D Weisz, Jessica
He, Michael Muller, Gabriela Hoefer,
Rachel Miles, and Werner Geyer.
2024.

</span>
<span class="ltx_bibblock">Design Principles for Generative AI Applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">arXiv preprint arXiv:2401.14484</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Welbl et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Johannes Welbl, Amelia
Glaese, Jonathan Uesato, Sumanth
Dathathri, John Mellor, Lisa Anne
Hendricks, Kirsty Anderson, Pushmeet
Kohli, Ben Coppin, and Po-Sen Huang.
2021.

</span>
<span class="ltx_bibblock">Challenges in detoxifying language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">arXiv preprint arXiv:2109.07445</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
ChengHo Wu, Chih-Yu Chou,
and Hsin Pang Tsai. 2023a.

</span>
<span class="ltx_bibblock">A Framework for PromptOps in GenAI Application
Development Lifecycle.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jiahao Wu, Qijiong Liu,
Hengchang Hu, Wenqi Fan,
Shengcai Liu, Qing Li,
Xiao-Ming Wu, and Ke Tang.
2023b.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models (LLMs) to Empower
Training-Free Dataset Condensation for Content-Based Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.3.1">arXiv preprint arXiv:2310.09874</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng,
Zhaopeng Qiu, Hao Wang,
Hongchao Gu, Tingjia Shen,
Chuan Qin, Chen Zhu,
Hengshu Zhu, Qi Liu, et al<span class="ltx_text" id="bib.bib139.3.1">.</span>
2023c.

</span>
<span class="ltx_bibblock">A survey on large language models for
recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.4.1">arXiv preprint arXiv:2305.19860</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib140.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu,
Jianghao Lin, Jieming Zhu,
Bo Chen, Ruiming Tang,
Weinan Zhang, Rui Zhang, and
Yong Yu. 2023b.

</span>
<span class="ltx_bibblock">Towards open-world recommendation with knowledge
augmentation from large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.3.1">arXiv preprint arXiv:2306.10933</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhiheng Xi, Wenxiang
Chen, Xin Guo, Wei He,
Yiwen Ding, Boyang Hong,
Ming Zhang, Junzhe Wang,
Senjie Jin, Enyu Zhou, et al<span class="ltx_text" id="bib.bib141.3.1">.</span>
2023a.

</span>
<span class="ltx_bibblock">The rise and potential of large language model
based agents: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.4.1">arXiv preprint arXiv:2309.07864</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Han Xie, Da Zheng,
Jun Ma, Houyu Zhang,
Vassilis N Ioannidis, Xiang Song,
Qing Ping, Sheng Wang,
Carl Yang, Yi Xu, et al<span class="ltx_text" id="bib.bib142.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Graph-aware language model pre-training on a large
graph corpus can help multiple graph applications. In
<em class="ltx_emph ltx_font_italic" id="bib.bib142.4.1">Proceedings of the 29th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining</em>. 5270–5281.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuyuan Xu, Wenyue Hua,
and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Openp5: Benchmarking foundation models for
recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">arXiv preprint arXiv:2306.11134</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu,
Yun Zhu, and Zhen-Hua Ling.
2024.

</span>
<span class="ltx_bibblock">Corrective Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">arXiv preprint arXiv:2401.15884</em>
(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jingfeng Yang, Haoming
Jiang, Qingyu Yin, Danqing Zhang,
Bing Yin, and Diyi Yang.
2022.

</span>
<span class="ltx_bibblock">SEQZERO: Few-shot Compositional Semantic Parsing
with Sequential Prompts and Zero-shot Models. In
<em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">Findings of the Association for Computational
Linguistics: NAACL 2022</em>. 49–60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rui Yang, Lin Song,
Yanwei Li, Sijie Zhao,
Yixiao Ge, Xiu Li, and
Ying Shan. 2024.

</span>
<span class="ltx_bibblock">Gpt4tools: Teaching large language model to use
tools via self-instruction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Linjie
Li, Kevin Lin, Jianfeng Wang,
Chung-Ching Lin, Zicheng Liu, and
Lijuan Wang. 2023.

</span>
<span class="ltx_bibblock">The dawn of lmms: Preliminary explorations with
gpt-4v (ision).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">arXiv preprint arXiv:2309.17421</em>
9, 1 (2023),
1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu,
Jeffrey Zhao, Izhak Shafran,
Tom Griffiths, Yuan Cao, and
Karthik Narasimhan. 2024.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with
large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">Advances in Neural Information Processing
Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao,
Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan
Cao. 2022.

</span>
<span class="ltx_bibblock">React: Synergizing reasoning and acting in language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">arXiv preprint arXiv:2210.03629</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bin Yin, Junjie Xie,
Yu Qin, Zixiang Ding,
Zhichao Feng, Xiang Li, and
Wei Lin. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous knowledge fusion: A novel approach
for personalized recommendation via llm. In
<em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">Proceedings of the 17th ACM Conference on
Recommender Systems</em>. 599–601.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He,
Kaifeng Chen, Pong Eksombatchai,
William L Hamilton, and Jure Leskovec.
2018.

</span>
<span class="ltx_bibblock">Graph convolutional neural networks for web-scale
recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">Proceedings of the 24th
ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>.
974–983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Fajie Yuan,
Yu Song, Youhua Li,
Junchen Fu, Fei Yang,
Yunzhu Pan, and Yongxin Ni.
2023.

</span>
<span class="ltx_bibblock">Where to go next for recommender systems? id-vs.
modality-based recommender models revisited. In
<em class="ltx_emph ltx_font_italic" id="bib.bib152.3.1">Proceedings of the 46th International ACM SIGIR
Conference on Research and Development in Information Retrieval</em>.
2639–2649.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zafrir et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ofir Zafrir, Guy
Boudoukh, Peter Izsak, and Moshe
Wasserblat. 2019.

</span>
<span class="ltx_bibblock">Q8bert: Quantized 8bit bert. In
<em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">2019 Fifth Workshop on Energy Efficient Machine
Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)</em>. IEEE,
36–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
An Zhang, Leheng Sheng,
Yuxin Chen, Hao Li, Yang
Deng, Xiang Wang, and Tat-Seng Chua.
2023d.

</span>
<span class="ltx_bibblock">On generative agents in recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">arXiv preprint arXiv:2310.10108</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jizhi Zhang, Keqin Bao,
Yang Zhang, Wenjie Wang,
Fuli Feng, and Xiangnan He.
2023a.

</span>
<span class="ltx_bibblock">Is chatgpt fair for recommendation? evaluating
fairness in large language model recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib155.3.1">Proceedings of the 17th ACM Conference on
Recommender Systems</em>. 993–999.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Junjie Zhang, Yupeng Hou,
Ruobing Xie, Wenqi Sun,
Julian McAuley, Wayne Xin Zhao,
Leyu Lin, and Ji-Rong Wen.
2023b.

</span>
<span class="ltx_bibblock">Agentcf: Collaborative learning with autonomous
language agents for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">arXiv preprint arXiv:2310.09233</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Shuai Zhang, Lina Yao,
Aixin Sun, and Yi Tay.
2019.

</span>
<span class="ltx_bibblock">Deep learning based recommender system: A survey
and new perspectives.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.3.1">ACM computing surveys (CSUR)</em>
52, 1 (2019),
1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li,
Leyang Cui, Deng Cai,
Lemao Liu, Tingchen Fu,
Xinting Huang, Enbo Zhao,
Yu Zhang, Yulong Chen, et al<span class="ltx_text" id="bib.bib158.3.1">.</span>
2023c.

</span>
<span class="ltx_bibblock">Siren’s song in the AI ocean: a survey on
hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.4.1">arXiv preprint arXiv:2309.01219</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zihao Zhao, Eric Wallace,
Shi Feng, Dan Klein, and
Sameer Singh. 2021.

</span>
<span class="ltx_bibblock">Calibrate before use: Improving few-shot
performance of language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">International
conference on machine learning</em>. PMLR, 12697–12706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kaitlyn Zhou, Su Lin
Blodgett, Adam Trischler, Hal
Daumé III, Kaheer Suleman, and
Alexandra Olteanu. 2022.

</span>
<span class="ltx_bibblock">Deconstructing NLG evaluation: Evaluation
practices, assumptions, and their implications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">arXiv preprint arXiv:2205.06828</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yaochen Zhu, Liang Wu,
Qi Guo, Liangjie Hong, and
Jundong Li. 2023.

</span>
<span class="ltx_bibblock">Collaborative large language model for recommender
systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib161.3.1">arXiv preprint arXiv:2311.01343</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shengyao Zhuang, Bing
Liu, Bevan Koopman, and Guido Zuccon.
2023.

</span>
<span class="ltx_bibblock">Open-source Large Language Models are Strong
Zero-shot Query Likelihood Models for Document Ranking. In
<em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">Findings of the Association for Computational
Linguistics: EMNLP 2023</em>. 8807–8817.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Terry Yue Zhuo, Yujin
Huang, Chunyang Chen, and Zhenchang
Xing. 2023.

</span>
<span class="ltx_bibblock">Red teaming chatgpt via jailbreaking: Bias,
robustness, reliability and toxicity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.3.1">arXiv preprint arXiv:2301.12867</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Andy Zou, Zifan Wang,
J Zico Kolter, and Matt Fredrikson.
2023.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on
aligned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.3.1">arXiv preprint arXiv:2307.15043</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 10 17:16:50 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
