<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.16511] Toloka Visual Question Answering Benchmark</title><meta property="og:description" content="In this paper, we present Toloka Visual Question Answering, a new crowdsourced dataset allowing comparing performance of machine learning systems against human level of expertise in the grounding visual question answer…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Toloka Visual Question Answering Benchmark">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Toloka Visual Question Answering Benchmark">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.16511">

<!--Generated on Wed Feb 28 03:38:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.4" class="ltx_ERROR undefined">\institution</span>
<p id="p1.3" class="ltx_p"><sup id="p1.3.1" class="ltx_sup">¶</sup> JetBrains 
<br class="ltx_break">Belgrade, 11070 Serbia 
<br class="ltx_break"> and <sup id="p1.3.2" class="ltx_sup"><span id="p1.3.2.1" class="ltx_text ltx_font_italic">†</span></sup> Toloka 
<br class="ltx_break">Belgrade, 11000 Serbia 
<br class="ltx_break"> and <sup id="p1.3.3" class="ltx_sup"><span id="p1.3.3.1" class="ltx_text ltx_font_italic">‡</span></sup> Toloka 
<br class="ltx_break">Lucerne, 6005 Switzerland 
<br class="ltx_break">
</p>
</div>
<h1 class="ltx_title ltx_title_document">Toloka Visual Question Answering Benchmark</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id1.1.1" class="ltx_text ltx_font_bold">Dmitry Ustalov<sup id="id1.1.1.1" class="ltx_sup"><span id="id1.1.1.1.1" class="ltx_text ltx_font_medium">¶</span></sup></span> 
<br class="ltx_break"><span id="id6.2.id1" class="ltx_text ltx_font_typewriter">dmitry.ustalov@jetbrains.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">This work was done while the author was with Toloka.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.1" class="ltx_text ltx_font_bold">Nikita Pavlichenko<sup id="id2.1.1.1" class="ltx_sup"><span id="id2.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span> 
<br class="ltx_break"><span id="id7.2.id1" class="ltx_text ltx_font_typewriter">pavlichenko@toloka.ai</span> 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.1" class="ltx_text ltx_font_bold">Sergey Koshelev<sup id="id3.1.1.1" class="ltx_sup"><span id="id3.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span> 
<br class="ltx_break"><span id="id8.2.id1" class="ltx_text ltx_font_typewriter">korzg@toloka.ai</span> 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.1" class="ltx_text ltx_font_bold">Daniil Likhobaba<sup id="id4.1.1.1" class="ltx_sup"><span id="id4.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†</span></sup></span> 
<br class="ltx_break"><span id="id9.2.id1" class="ltx_text ltx_font_typewriter">likhobaba-dp@toloka.ai</span> 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.1" class="ltx_text ltx_font_bold">Alisa Smirnova<sup id="id5.1.1.1" class="ltx_sup"><span id="id5.1.1.1.1" class="ltx_text ltx_font_medium ltx_font_italic">‡</span></sup></span> 
<br class="ltx_break"><span id="id10.2.id1" class="ltx_text ltx_font_typewriter">zero@toloka.ai</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">In this paper, we present <em id="id11.id1.1" class="ltx_emph ltx_font_italic">Toloka Visual Question Answering</em>, a new crowdsourced dataset allowing comparing performance of machine learning systems against human level of expertise in the grounding visual question answering task. In this task, <em id="id11.id1.2" class="ltx_emph ltx_font_italic">given an image and a textual question, one has to draw the bounding box around the object correctly responding to that question</em>. Every image-question pair contains the response, with only one correct response per image. Our dataset contains 45,199 pairs of images and questions in English, provided with ground truth bounding boxes, split into train and two test subsets. Besides describing the dataset and releasing it under a CC BY license, we conducted a series of experiments on open source zero-shot baseline models and organized a multi-phase competition at WSDM Cup that attracted 48 participants worldwide. However, by the time of paper submission, no machine learning model outperformed the non-expert crowdsourcing baseline according to the intersection over union evaluation score.</p>
</div>
<figure id="S0.F1" class="ltx_figure">
<div id="S0.F1.2" class="ltx_block">
<figure id="S0.F1.sf1" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/rectangle-000000090941.jpg" id="S0.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="186" height="139" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S0.F1.sf1.3.2" class="ltx_text" style="font-size:90%;">What do we use to support the immune system and get vitamin C?</span></figcaption>
</figure>
<figure id="S0.F1.sf2" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/rectangle-000000145979.jpg" id="S0.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="186" height="124" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S0.F1.sf2.3.2" class="ltx_text" style="font-size:90%;">What do people use for cutting?</span></figcaption>
</figure>
<figure id="S0.F1.sf3" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/rectangle-000000215107.jpg" id="S0.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="186" height="139" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S0.F1.sf3.3.2" class="ltx_text" style="font-size:90%;">What do you use to hit the ball?</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.4.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Given an image and a textual question, draw a bounding box containing the correct answer to the question.<span id="S0.F1.5.2.1" class="ltx_text ltx_font_medium"> Above is a sample of three image-question pairs from the training subset of our dataset. Every image contains the response, with only one correct response per image. Bounding boxes are drawn for illustrative purposes only; they are not parts of images in our dataset but are available as ground truth for all images. All images are from the MS COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">Lin:14, </a>)</cite> under the same license.</span></span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, prominent multi-modal deep learning models such as CLIP <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">Radford:21, </a>)</cite> and DALL-E <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib21" title="" class="ltx_ref">Ramesh:21, </a>)</cite> have demonstrated remarkable performance in demanding tasks such as text-image similarity measurement and text-to-image generation, respectively. Concurrently, modern machine learning methods have achieved superhuman results on challenging multi-task benchmarks like SuperGLUE <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib22" title="" class="ltx_ref">Wang:19, </a>)</cite> and VLUE <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib26" title="" class="ltx_ref">Zhou:22, </a>)</cite>. However, most of these benchmarks incorporate a combination of well-known tasks with limited modality. In this study, we enhance the level of difficulty for machine learning methods by introducing the <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">Toloka Visual Question Answering</em>, an open-source multi-modal dataset designed to evaluate artificial intelligence systems. We provide a comprehensive description of the benchmark, outline our crowdsourcing pipeline for data collection, and present the performance of current pre-trained and fine-tuned models in tackling the challenging problem of grounding visual question answering.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Our task is formulated as follows. Given an image and an English textual question, the objective is to draw a bounding box around the object that provides the correct response to the question (Figure <a href="#S0.F1" title="Figure 1 ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). For instance, in a photograph of a bathroom, a question like “Where do I wash my hands?” would require selecting the sink as the answer. Successfully solving this task necessitates the non-trivial integration of visual, textual, and commonsense information. We assert that our approach, which employs free-form, open-ended textual questions paired with bounding boxes as answers, presents a fair challenge for contemporary multi-modal models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The remainder of this paper is organized as follows: Section <a href="#S2" title="2 Related Work ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of related work, Section <a href="#S3" title="3 Dataset Description ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> introduces our grounding visual question answering dataset, Section <a href="#S4" title="4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> describes the annotation pipeline employed to create the dataset, Section <a href="#S5" title="5 Metrics and Baselines ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> defines the evaluation metrics and baselines used for assessment, Section <a href="#S6" title="6 Evaluation ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents the evaluation results for publicly-available models and submissions in our competition, Section <a href="#S7" title="7 Error Analysis ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> conducts an error analysis of both human and machine performance on our dataset, and finally, Section <a href="#S8" title="8 Conclusion ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> outlines the limitations of our work and concludes with final remarks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In recent years, the scientific community has made significant progress in the development of diverse datasets containing multi-modal data, enabling numerous applications at the intersection of natural language processing and computer vision. One prominent application in this domain is visual question answering (VQA) <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib1" title="" class="ltx_ref">Antol:15, </a>)</cite>, where models are tasked with providing textual responses based on image-question pairs, often involving commonsense knowledge. Several datasets have been created to facilitate research in VQA, such as GQA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">Hudson:19, </a>)</cite>, CLEVR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib9" title="" class="ltx_ref">CLEVR, </a>)</cite>, and VQA v2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib7" title="" class="ltx_ref">VQAv2, </a>)</cite>, which leverage MS COCO<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://cocodataset.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://cocodataset.org/</a></span></span></span> images <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">Lin:14, </a>)</cite> (which is also the case for our dataset).</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">However, the conventional VQA paradigm assumes that the output should be in textual form. In contrast, visual grounding task requires to find a region of an image referring to a textual description of an image. The RefClef, RefCOCO, RefCOCO+ <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib11" title="" class="ltx_ref">RefCOCO, </a>)</cite>, and <span id="S2.p2.1.1" class="ltx_text ltx_font_smallcaps">GrIT</span> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">Kosmos-2, </a>)</cite> datasets are examples that highlight the challenges of this visual grounding task. Our work lies at the intersection of these tasks, requiring both natural language understanding to process the question and the ability to comprehend the visual scene to detect relevant objects. We frame the problem as a <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">grounding visual question answering task</em>, where the model must output an object identified by a bounding box as the answer to the question. It is important to note that our problem cannot be easily reduced to the standard text-only question answering or detection based solely on textual prompts, as the answer to the question depends on the content of the image. In grounding VQA task <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib28" title="" class="ltx_ref">Zhu:16, </a>; <a href="#bib.bib19" title="" class="ltx_ref">Qiao:21, </a>; <a href="#bib.bib4" title="" class="ltx_ref">Chen_2022_CVPR, </a>)</cite> one predicts the region in the image used to arrive at the answer but not necessarily the answer itself. We guarantee that the answer is always present, yet we firmly believe that our proposed setup presents a formidable challenge for modern multi-modal models.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Description</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our dataset is comprised of the images associated with textual questions (Figure <a href="#S0.F1" title="Figure 1 ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). One entry (instance) in our dataset is a question-image pair labeled with the ground truth coordinates of a bounding box containing the object answering the given question. We guarantee that in most cases each image contains one and only one correct response to the given question. The images were obtained from a subset of the Microsoft Common Objects in Context, MS COCO, dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib17" title="" class="ltx_ref">Lin:14, </a>)</cite> that was licensed under the Creative Commons Attribution (CC BY) license.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.17.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.18.2" class="ltx_text" style="font-size:90%;">Descriptive statistics of our dataset. There are 45,199 instances in total. Image and bounding box dimensions are in pixels. Question lengths are in characters including spaces. All numbers are 95% confidence intervals, except the number of instances.</span></figcaption>
<table id="S3.T1.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.15.16.1" class="ltx_tr">
<th id="S3.T1.15.16.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T1.15.16.1.1.1" class="ltx_text ltx_font_bold">Subset</span></th>
<th id="S3.T1.15.16.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S3.T1.15.16.1.2.1" class="ltx_text ltx_font_bold"># of instances</span></th>
<th id="S3.T1.15.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.15.16.1.3.1" class="ltx_text ltx_font_bold">Image</span></th>
<th id="S3.T1.15.16.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S3.T1.15.16.1.4.1" class="ltx_text ltx_font_bold">Bounding Box</span></th>
<th id="S3.T1.15.16.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T1.15.16.1.5.1" class="ltx_text ltx_font_bold">Question</span></th>
</tr>
<tr id="S3.T1.15.17.2" class="ltx_tr">
<th id="S3.T1.15.17.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.15.17.2.1.1" class="ltx_text ltx_font_bold">Width</span></th>
<th id="S3.T1.15.17.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.15.17.2.2.1" class="ltx_text ltx_font_bold">Height</span></th>
<th id="S3.T1.15.17.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.15.17.2.3.1" class="ltx_text ltx_font_bold">Width</span></th>
<th id="S3.T1.15.17.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.15.17.2.4.1" class="ltx_text ltx_font_bold">Height</span></th>
<th id="S3.T1.15.17.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T1.15.17.2.5.1" class="ltx_text ltx_font_bold">Length</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.5.5" class="ltx_tr">
<th id="S3.T1.5.5.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">train</th>
<th id="S3.T1.5.5.7" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">38,990</th>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.1.1.1.m1.2" class="ltx_Math" alttext="(578,581)" display="inline"><semantics id="S3.T1.1.1.1.m1.2a"><mrow id="S3.T1.1.1.1.m1.2.3.2" xref="S3.T1.1.1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.1.1.1.m1.2.3.2.1" xref="S3.T1.1.1.1.m1.2.3.1.cmml">(</mo><mn id="S3.T1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.m1.1.1.cmml">578</mn><mo id="S3.T1.1.1.1.m1.2.3.2.2" xref="S3.T1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.1.1.1.m1.2.2" xref="S3.T1.1.1.1.m1.2.2.cmml">581</mn><mo stretchy="false" id="S3.T1.1.1.1.m1.2.3.2.3" xref="S3.T1.1.1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.2b"><interval closure="open" id="S3.T1.1.1.1.m1.2.3.1.cmml" xref="S3.T1.1.1.1.m1.2.3.2"><cn type="integer" id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">578</cn><cn type="integer" id="S3.T1.1.1.1.m1.2.2.cmml" xref="S3.T1.1.1.1.m1.2.2">581</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.2c">(578,581)</annotation></semantics></math></td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.2.2.2.m1.2" class="ltx_Math" alttext="(486,489)" display="inline"><semantics id="S3.T1.2.2.2.m1.2a"><mrow id="S3.T1.2.2.2.m1.2.3.2" xref="S3.T1.2.2.2.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.2.2.2.m1.2.3.2.1" xref="S3.T1.2.2.2.m1.2.3.1.cmml">(</mo><mn id="S3.T1.2.2.2.m1.1.1" xref="S3.T1.2.2.2.m1.1.1.cmml">486</mn><mo id="S3.T1.2.2.2.m1.2.3.2.2" xref="S3.T1.2.2.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.2.2.2.m1.2.2" xref="S3.T1.2.2.2.m1.2.2.cmml">489</mn><mo stretchy="false" id="S3.T1.2.2.2.m1.2.3.2.3" xref="S3.T1.2.2.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.2b"><interval closure="open" id="S3.T1.2.2.2.m1.2.3.1.cmml" xref="S3.T1.2.2.2.m1.2.3.2"><cn type="integer" id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">486</cn><cn type="integer" id="S3.T1.2.2.2.m1.2.2.cmml" xref="S3.T1.2.2.2.m1.2.2">489</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.2c">(486,489)</annotation></semantics></math></td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.3.3.3.m1.2" class="ltx_Math" alttext="(103,105)" display="inline"><semantics id="S3.T1.3.3.3.m1.2a"><mrow id="S3.T1.3.3.3.m1.2.3.2" xref="S3.T1.3.3.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.3.3.3.m1.2.3.2.1" xref="S3.T1.3.3.3.m1.2.3.1.cmml">(</mo><mn id="S3.T1.3.3.3.m1.1.1" xref="S3.T1.3.3.3.m1.1.1.cmml">103</mn><mo id="S3.T1.3.3.3.m1.2.3.2.2" xref="S3.T1.3.3.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.3.3.3.m1.2.2" xref="S3.T1.3.3.3.m1.2.2.cmml">105</mn><mo stretchy="false" id="S3.T1.3.3.3.m1.2.3.2.3" xref="S3.T1.3.3.3.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.2b"><interval closure="open" id="S3.T1.3.3.3.m1.2.3.1.cmml" xref="S3.T1.3.3.3.m1.2.3.2"><cn type="integer" id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">103</cn><cn type="integer" id="S3.T1.3.3.3.m1.2.2.cmml" xref="S3.T1.3.3.3.m1.2.2">105</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.2c">(103,105)</annotation></semantics></math></td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.4.4.4.m1.2" class="ltx_Math" alttext="(97,99)" display="inline"><semantics id="S3.T1.4.4.4.m1.2a"><mrow id="S3.T1.4.4.4.m1.2.3.2" xref="S3.T1.4.4.4.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.4.4.4.m1.2.3.2.1" xref="S3.T1.4.4.4.m1.2.3.1.cmml">(</mo><mn id="S3.T1.4.4.4.m1.1.1" xref="S3.T1.4.4.4.m1.1.1.cmml">97</mn><mo id="S3.T1.4.4.4.m1.2.3.2.2" xref="S3.T1.4.4.4.m1.2.3.1.cmml">,</mo><mn id="S3.T1.4.4.4.m1.2.2" xref="S3.T1.4.4.4.m1.2.2.cmml">99</mn><mo stretchy="false" id="S3.T1.4.4.4.m1.2.3.2.3" xref="S3.T1.4.4.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.m1.2b"><interval closure="open" id="S3.T1.4.4.4.m1.2.3.1.cmml" xref="S3.T1.4.4.4.m1.2.3.2"><cn type="integer" id="S3.T1.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.m1.1.1">97</cn><cn type="integer" id="S3.T1.4.4.4.m1.2.2.cmml" xref="S3.T1.4.4.4.m1.2.2">99</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.m1.2c">(97,99)</annotation></semantics></math></td>
<td id="S3.T1.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S3.T1.5.5.5.m1.2" class="ltx_Math" alttext="(36,38)" display="inline"><semantics id="S3.T1.5.5.5.m1.2a"><mrow id="S3.T1.5.5.5.m1.2.3.2" xref="S3.T1.5.5.5.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.5.5.5.m1.2.3.2.1" xref="S3.T1.5.5.5.m1.2.3.1.cmml">(</mo><mn id="S3.T1.5.5.5.m1.1.1" xref="S3.T1.5.5.5.m1.1.1.cmml">36</mn><mo id="S3.T1.5.5.5.m1.2.3.2.2" xref="S3.T1.5.5.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.5.5.5.m1.2.2" xref="S3.T1.5.5.5.m1.2.2.cmml">38</mn><mo stretchy="false" id="S3.T1.5.5.5.m1.2.3.2.3" xref="S3.T1.5.5.5.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.m1.2b"><interval closure="open" id="S3.T1.5.5.5.m1.2.3.1.cmml" xref="S3.T1.5.5.5.m1.2.3.2"><cn type="integer" id="S3.T1.5.5.5.m1.1.1.cmml" xref="S3.T1.5.5.5.m1.1.1">36</cn><cn type="integer" id="S3.T1.5.5.5.m1.2.2.cmml" xref="S3.T1.5.5.5.m1.2.2">38</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.m1.2c">(36,38)</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<th id="S3.T1.10.10.6" class="ltx_td ltx_align_left ltx_th ltx_th_row">public test</th>
<th id="S3.T1.10.10.7" class="ltx_td ltx_align_right ltx_th ltx_th_row">1,705</th>
<td id="S3.T1.6.6.1" class="ltx_td ltx_align_center"><math id="S3.T1.6.6.1.m1.2" class="ltx_Math" alttext="(573,583)" display="inline"><semantics id="S3.T1.6.6.1.m1.2a"><mrow id="S3.T1.6.6.1.m1.2.3.2" xref="S3.T1.6.6.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.6.6.1.m1.2.3.2.1" xref="S3.T1.6.6.1.m1.2.3.1.cmml">(</mo><mn id="S3.T1.6.6.1.m1.1.1" xref="S3.T1.6.6.1.m1.1.1.cmml">573</mn><mo id="S3.T1.6.6.1.m1.2.3.2.2" xref="S3.T1.6.6.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.6.6.1.m1.2.2" xref="S3.T1.6.6.1.m1.2.2.cmml">583</mn><mo stretchy="false" id="S3.T1.6.6.1.m1.2.3.2.3" xref="S3.T1.6.6.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.1.m1.2b"><interval closure="open" id="S3.T1.6.6.1.m1.2.3.1.cmml" xref="S3.T1.6.6.1.m1.2.3.2"><cn type="integer" id="S3.T1.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.1.m1.1.1">573</cn><cn type="integer" id="S3.T1.6.6.1.m1.2.2.cmml" xref="S3.T1.6.6.1.m1.2.2">583</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.1.m1.2c">(573,583)</annotation></semantics></math></td>
<td id="S3.T1.7.7.2" class="ltx_td ltx_align_center"><math id="S3.T1.7.7.2.m1.2" class="ltx_Math" alttext="(483,493)" display="inline"><semantics id="S3.T1.7.7.2.m1.2a"><mrow id="S3.T1.7.7.2.m1.2.3.2" xref="S3.T1.7.7.2.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.7.7.2.m1.2.3.2.1" xref="S3.T1.7.7.2.m1.2.3.1.cmml">(</mo><mn id="S3.T1.7.7.2.m1.1.1" xref="S3.T1.7.7.2.m1.1.1.cmml">483</mn><mo id="S3.T1.7.7.2.m1.2.3.2.2" xref="S3.T1.7.7.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.7.7.2.m1.2.2" xref="S3.T1.7.7.2.m1.2.2.cmml">493</mn><mo stretchy="false" id="S3.T1.7.7.2.m1.2.3.2.3" xref="S3.T1.7.7.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.2.m1.2b"><interval closure="open" id="S3.T1.7.7.2.m1.2.3.1.cmml" xref="S3.T1.7.7.2.m1.2.3.2"><cn type="integer" id="S3.T1.7.7.2.m1.1.1.cmml" xref="S3.T1.7.7.2.m1.1.1">483</cn><cn type="integer" id="S3.T1.7.7.2.m1.2.2.cmml" xref="S3.T1.7.7.2.m1.2.2">493</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.2.m1.2c">(483,493)</annotation></semantics></math></td>
<td id="S3.T1.8.8.3" class="ltx_td ltx_align_center"><math id="S3.T1.8.8.3.m1.2" class="ltx_Math" alttext="(91,100)" display="inline"><semantics id="S3.T1.8.8.3.m1.2a"><mrow id="S3.T1.8.8.3.m1.2.3.2" xref="S3.T1.8.8.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.8.8.3.m1.2.3.2.1" xref="S3.T1.8.8.3.m1.2.3.1.cmml">(</mo><mn id="S3.T1.8.8.3.m1.1.1" xref="S3.T1.8.8.3.m1.1.1.cmml">91</mn><mo id="S3.T1.8.8.3.m1.2.3.2.2" xref="S3.T1.8.8.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.8.8.3.m1.2.2" xref="S3.T1.8.8.3.m1.2.2.cmml">100</mn><mo stretchy="false" id="S3.T1.8.8.3.m1.2.3.2.3" xref="S3.T1.8.8.3.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.3.m1.2b"><interval closure="open" id="S3.T1.8.8.3.m1.2.3.1.cmml" xref="S3.T1.8.8.3.m1.2.3.2"><cn type="integer" id="S3.T1.8.8.3.m1.1.1.cmml" xref="S3.T1.8.8.3.m1.1.1">91</cn><cn type="integer" id="S3.T1.8.8.3.m1.2.2.cmml" xref="S3.T1.8.8.3.m1.2.2">100</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.3.m1.2c">(91,100)</annotation></semantics></math></td>
<td id="S3.T1.9.9.4" class="ltx_td ltx_align_center"><math id="S3.T1.9.9.4.m1.2" class="ltx_Math" alttext="(88,97)" display="inline"><semantics id="S3.T1.9.9.4.m1.2a"><mrow id="S3.T1.9.9.4.m1.2.3.2" xref="S3.T1.9.9.4.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.9.9.4.m1.2.3.2.1" xref="S3.T1.9.9.4.m1.2.3.1.cmml">(</mo><mn id="S3.T1.9.9.4.m1.1.1" xref="S3.T1.9.9.4.m1.1.1.cmml">88</mn><mo id="S3.T1.9.9.4.m1.2.3.2.2" xref="S3.T1.9.9.4.m1.2.3.1.cmml">,</mo><mn id="S3.T1.9.9.4.m1.2.2" xref="S3.T1.9.9.4.m1.2.2.cmml">97</mn><mo stretchy="false" id="S3.T1.9.9.4.m1.2.3.2.3" xref="S3.T1.9.9.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.9.9.4.m1.2b"><interval closure="open" id="S3.T1.9.9.4.m1.2.3.1.cmml" xref="S3.T1.9.9.4.m1.2.3.2"><cn type="integer" id="S3.T1.9.9.4.m1.1.1.cmml" xref="S3.T1.9.9.4.m1.1.1">88</cn><cn type="integer" id="S3.T1.9.9.4.m1.2.2.cmml" xref="S3.T1.9.9.4.m1.2.2">97</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.9.4.m1.2c">(88,97)</annotation></semantics></math></td>
<td id="S3.T1.10.10.5" class="ltx_td ltx_align_center"><math id="S3.T1.10.10.5.m1.2" class="ltx_Math" alttext="(36,39)" display="inline"><semantics id="S3.T1.10.10.5.m1.2a"><mrow id="S3.T1.10.10.5.m1.2.3.2" xref="S3.T1.10.10.5.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.10.10.5.m1.2.3.2.1" xref="S3.T1.10.10.5.m1.2.3.1.cmml">(</mo><mn id="S3.T1.10.10.5.m1.1.1" xref="S3.T1.10.10.5.m1.1.1.cmml">36</mn><mo id="S3.T1.10.10.5.m1.2.3.2.2" xref="S3.T1.10.10.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.10.10.5.m1.2.2" xref="S3.T1.10.10.5.m1.2.2.cmml">39</mn><mo stretchy="false" id="S3.T1.10.10.5.m1.2.3.2.3" xref="S3.T1.10.10.5.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.10.10.5.m1.2b"><interval closure="open" id="S3.T1.10.10.5.m1.2.3.1.cmml" xref="S3.T1.10.10.5.m1.2.3.2"><cn type="integer" id="S3.T1.10.10.5.m1.1.1.cmml" xref="S3.T1.10.10.5.m1.1.1">36</cn><cn type="integer" id="S3.T1.10.10.5.m1.2.2.cmml" xref="S3.T1.10.10.5.m1.2.2">39</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.10.5.m1.2c">(36,39)</annotation></semantics></math></td>
</tr>
<tr id="S3.T1.15.15" class="ltx_tr">
<th id="S3.T1.15.15.6" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">private test</th>
<th id="S3.T1.15.15.7" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">4,504</th>
<td id="S3.T1.11.11.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.11.11.1.m1.2" class="ltx_Math" alttext="(578,584)" display="inline"><semantics id="S3.T1.11.11.1.m1.2a"><mrow id="S3.T1.11.11.1.m1.2.3.2" xref="S3.T1.11.11.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.11.11.1.m1.2.3.2.1" xref="S3.T1.11.11.1.m1.2.3.1.cmml">(</mo><mn id="S3.T1.11.11.1.m1.1.1" xref="S3.T1.11.11.1.m1.1.1.cmml">578</mn><mo id="S3.T1.11.11.1.m1.2.3.2.2" xref="S3.T1.11.11.1.m1.2.3.1.cmml">,</mo><mn id="S3.T1.11.11.1.m1.2.2" xref="S3.T1.11.11.1.m1.2.2.cmml">584</mn><mo stretchy="false" id="S3.T1.11.11.1.m1.2.3.2.3" xref="S3.T1.11.11.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.11.11.1.m1.2b"><interval closure="open" id="S3.T1.11.11.1.m1.2.3.1.cmml" xref="S3.T1.11.11.1.m1.2.3.2"><cn type="integer" id="S3.T1.11.11.1.m1.1.1.cmml" xref="S3.T1.11.11.1.m1.1.1">578</cn><cn type="integer" id="S3.T1.11.11.1.m1.2.2.cmml" xref="S3.T1.11.11.1.m1.2.2">584</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.11.1.m1.2c">(578,584)</annotation></semantics></math></td>
<td id="S3.T1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.12.12.2.m1.2" class="ltx_Math" alttext="(480,487)" display="inline"><semantics id="S3.T1.12.12.2.m1.2a"><mrow id="S3.T1.12.12.2.m1.2.3.2" xref="S3.T1.12.12.2.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.12.12.2.m1.2.3.2.1" xref="S3.T1.12.12.2.m1.2.3.1.cmml">(</mo><mn id="S3.T1.12.12.2.m1.1.1" xref="S3.T1.12.12.2.m1.1.1.cmml">480</mn><mo id="S3.T1.12.12.2.m1.2.3.2.2" xref="S3.T1.12.12.2.m1.2.3.1.cmml">,</mo><mn id="S3.T1.12.12.2.m1.2.2" xref="S3.T1.12.12.2.m1.2.2.cmml">487</mn><mo stretchy="false" id="S3.T1.12.12.2.m1.2.3.2.3" xref="S3.T1.12.12.2.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.12.12.2.m1.2b"><interval closure="open" id="S3.T1.12.12.2.m1.2.3.1.cmml" xref="S3.T1.12.12.2.m1.2.3.2"><cn type="integer" id="S3.T1.12.12.2.m1.1.1.cmml" xref="S3.T1.12.12.2.m1.1.1">480</cn><cn type="integer" id="S3.T1.12.12.2.m1.2.2.cmml" xref="S3.T1.12.12.2.m1.2.2">487</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.12.12.2.m1.2c">(480,487)</annotation></semantics></math></td>
<td id="S3.T1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.13.13.3.m1.2" class="ltx_Math" alttext="(95,101)" display="inline"><semantics id="S3.T1.13.13.3.m1.2a"><mrow id="S3.T1.13.13.3.m1.2.3.2" xref="S3.T1.13.13.3.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.13.13.3.m1.2.3.2.1" xref="S3.T1.13.13.3.m1.2.3.1.cmml">(</mo><mn id="S3.T1.13.13.3.m1.1.1" xref="S3.T1.13.13.3.m1.1.1.cmml">95</mn><mo id="S3.T1.13.13.3.m1.2.3.2.2" xref="S3.T1.13.13.3.m1.2.3.1.cmml">,</mo><mn id="S3.T1.13.13.3.m1.2.2" xref="S3.T1.13.13.3.m1.2.2.cmml">101</mn><mo stretchy="false" id="S3.T1.13.13.3.m1.2.3.2.3" xref="S3.T1.13.13.3.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.13.13.3.m1.2b"><interval closure="open" id="S3.T1.13.13.3.m1.2.3.1.cmml" xref="S3.T1.13.13.3.m1.2.3.2"><cn type="integer" id="S3.T1.13.13.3.m1.1.1.cmml" xref="S3.T1.13.13.3.m1.1.1">95</cn><cn type="integer" id="S3.T1.13.13.3.m1.2.2.cmml" xref="S3.T1.13.13.3.m1.2.2">101</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.13.13.3.m1.2c">(95,101)</annotation></semantics></math></td>
<td id="S3.T1.14.14.4" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.14.14.4.m1.2" class="ltx_Math" alttext="(89,94)" display="inline"><semantics id="S3.T1.14.14.4.m1.2a"><mrow id="S3.T1.14.14.4.m1.2.3.2" xref="S3.T1.14.14.4.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.14.14.4.m1.2.3.2.1" xref="S3.T1.14.14.4.m1.2.3.1.cmml">(</mo><mn id="S3.T1.14.14.4.m1.1.1" xref="S3.T1.14.14.4.m1.1.1.cmml">89</mn><mo id="S3.T1.14.14.4.m1.2.3.2.2" xref="S3.T1.14.14.4.m1.2.3.1.cmml">,</mo><mn id="S3.T1.14.14.4.m1.2.2" xref="S3.T1.14.14.4.m1.2.2.cmml">94</mn><mo stretchy="false" id="S3.T1.14.14.4.m1.2.3.2.3" xref="S3.T1.14.14.4.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.14.14.4.m1.2b"><interval closure="open" id="S3.T1.14.14.4.m1.2.3.1.cmml" xref="S3.T1.14.14.4.m1.2.3.2"><cn type="integer" id="S3.T1.14.14.4.m1.1.1.cmml" xref="S3.T1.14.14.4.m1.1.1">89</cn><cn type="integer" id="S3.T1.14.14.4.m1.2.2.cmml" xref="S3.T1.14.14.4.m1.2.2">94</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.14.14.4.m1.2c">(89,94)</annotation></semantics></math></td>
<td id="S3.T1.15.15.5" class="ltx_td ltx_align_center ltx_border_bb"><math id="S3.T1.15.15.5.m1.2" class="ltx_Math" alttext="(36,38)" display="inline"><semantics id="S3.T1.15.15.5.m1.2a"><mrow id="S3.T1.15.15.5.m1.2.3.2" xref="S3.T1.15.15.5.m1.2.3.1.cmml"><mo stretchy="false" id="S3.T1.15.15.5.m1.2.3.2.1" xref="S3.T1.15.15.5.m1.2.3.1.cmml">(</mo><mn id="S3.T1.15.15.5.m1.1.1" xref="S3.T1.15.15.5.m1.1.1.cmml">36</mn><mo id="S3.T1.15.15.5.m1.2.3.2.2" xref="S3.T1.15.15.5.m1.2.3.1.cmml">,</mo><mn id="S3.T1.15.15.5.m1.2.2" xref="S3.T1.15.15.5.m1.2.2.cmml">38</mn><mo stretchy="false" id="S3.T1.15.15.5.m1.2.3.2.3" xref="S3.T1.15.15.5.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.15.15.5.m1.2b"><interval closure="open" id="S3.T1.15.15.5.m1.2.3.1.cmml" xref="S3.T1.15.15.5.m1.2.3.2"><cn type="integer" id="S3.T1.15.15.5.m1.1.1.cmml" xref="S3.T1.15.15.5.m1.1.1">36</cn><cn type="integer" id="S3.T1.15.15.5.m1.2.2.cmml" xref="S3.T1.15.15.5.m1.2.2">38</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.15.15.5.m1.2c">(36,38)</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure">
<div id="S3.F2.2" class="ltx_block">
<figure id="S3.F2.sf1" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/x1.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="159" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S3.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Heat map of bounding boxes in the private test subset of our dataset. Heat maps of other subsets look similarly.</span></figcaption>
</figure>
<figure id="S3.F2.sf2" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/x2.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="207" height="172" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S3.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">Diagram of the 30 most common types of objects making 26% of the objects in our dataset.</span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text" style="font-size:90%;">Visual analysis of the ground truth bounding boxes in our dataset.</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Our dataset consists of 45,199 instances, which are divided into three subsets as shown in Table <a href="#S3.T1" title="Table 1 ‣ 3 Dataset Description ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: <em id="S3.p2.1.1" class="ltx_emph ltx_font_italic">train</em> (38,990 instances), <em id="S3.p2.1.2" class="ltx_emph ltx_font_italic">public test</em> (1,705 instances), and <em id="S3.p2.1.3" class="ltx_emph ltx_font_italic">private test</em> (4,504 instances). The names of these subsets correspond to the different phases of the competition that we organized (refer to Section <a href="#S6" title="6 Evaluation ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for further information). Since the public release of the entire dataset the <em id="S3.p2.1.4" class="ltx_emph ltx_font_italic">train</em> subset can be used as the training set, the <em id="S3.p2.1.5" class="ltx_emph ltx_font_italic">public test</em> subset as the validation set, and <em id="S3.p2.1.6" class="ltx_emph ltx_font_italic">private test</em> as the test set. The dataset is provided in the form of textual files in comma-separated value (CSV) format, containing the following information: (a) URL of an image on a public content delivery network, (b) question in English, (c) image width and height, and (d) bounding box coordinates (left, top, right, bottom).</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">We made our complete dataset, along with the baselines and ground truth bounding boxes, publicly available on various platforms to encourage research and development of multi-modal question answering models. The dataset can be accessed on Zenodo,<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://doi.org/10.5281/zenodo.7057740" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.7057740</a></span></span></span> Hugging Face Hub,<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://huggingface.co/datasets/toloka/WSDMCup2023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/toloka/WSDMCup2023</a></span></span></span> Kaggle,<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://www.kaggle.com/datasets/dustalov/toloka-wsdm-cup-2023-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/dustalov/toloka-wsdm-cup-2023-vqa</a></span></span></span> and GitHub.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/Toloka/WSDMCup2023" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/Toloka/WSDMCup2023</a></span></span></span> It was released under the same CC BY license as the MS COCO subset we used. To ensure the integrity of the dataset and address potential concerns regarding dataset split-view poisoning <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib3" title="" class="ltx_ref">carlini2023poisoning, </a>)</cite>, we computed and uploaded SHA-256 hashes for the images and electronically signed the repository commits that contain our data files. Additionally, we have uploaded all the images to Zenodo and Kaggle to mitigate any potential unavailability issues with the Azure content delivery network that we utilized to store the images.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Our dataset has an equal proportion of images and questions to allow capturing different parts of different images, making it useful for training both visual and textual aspects of the model. Descriptive statistics of our dataset are presented in Table <a href="#S3.T1" title="Table 1 ‣ 3 Dataset Description ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It is evident that the subsets share a similar structure, with the majority of bounding boxes located near the centers of the images (Figure <a href="#S3.F2.sf1" title="In Figure 2 ‣ 3 Dataset Description ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>). Furthermore, we extracted portions of images from the dataset, confined within the bounding boxes, and captioned them using BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib15" title="" class="ltx_ref">blip2, </a>)</cite>. This process resulted in textual descriptions of the selected objects. By clustering these descriptions in the <em id="S3.p4.1.1" class="ltx_emph ltx_font_italic">test private</em> subset, we found that 72% of them formed 65 distinct clusters, while 28% of objects did not belong to any cluster, demonstrating the diversity and non-triviality of our dataset. The 30 most common types of objects enclosed in bounding boxes are illustrated in Figure <a href="#S3.F2.sf2" title="In Figure 2 ‣ 3 Dataset Description ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">To further analyze the questions in our dataset, we sampled 100 random questions from the private test subset. Then, three authors of the paper manually annotated whether it is possible to answer the given question without seeing an image. We aggregated the annotations by majority vote; the inter-annotator agreement was high as indicated by Krippendorff’s <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="\alpha=0.87" display="inline"><semantics id="S3.p5.1.m1.1a"><mrow id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">α</mi><mo id="S3.p5.1.m1.1.1.1" xref="S3.p5.1.m1.1.1.1.cmml">=</mo><mn id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">0.87</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><eq id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1.1"></eq><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">𝛼</ci><cn type="float" id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">0.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\alpha=0.87</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib14" title="" class="ltx_ref">Krippendorff:18, </a>)</cite>. Our evaluation showed that only 56% of questions were answerable without seeing an image. That is, there was a specific answer to these questions (e.g., “What does the baseball player use to hit the ball?”) while the rest 44% of questions have multiple answers (e.g., “What can be used to eat food with?”) or the question is specifically about the image (e.g., “What is the person riding?”). Thus, our analysis shows that almost a half of the questions cannot be answered without seeing an image.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Annotation Methodology</h2>

<figure id="S4.F3" class="ltx_figure"><img src="/html/2309.16511/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="212" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.6.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.7.2" class="ltx_text" style="font-size:90%;">A diagram of our annotation pipeline. First, we perform an <em id="S4.F3.7.2.1" class="ltx_emph ltx_font_italic">annotator selection</em> (Section <a href="#S4.SS1" title="4.1 Annotator Selection ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). Then, we ask the annotators to draw <em id="S4.F3.7.2.2" class="ltx_emph ltx_font_italic">bounding boxes</em> around interesting objects (Section <a href="#S4.SS2" title="4.2 Bounding Boxes ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) and to <em id="S4.F3.7.2.3" class="ltx_emph ltx_font_italic">compose questions</em> (Section <a href="#S4.SS3" title="4.3 Question Composition ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>). Finally, during <em id="S4.F3.7.2.4" class="ltx_emph ltx_font_italic">post-processing</em>, we ask the annotators to answer the composed image-question pairs (Section <a href="#S4.SS4" title="4.4 Post-Processing ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>).</span></figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We performed all annotations, including the creation of bounding boxes and questions, using the open-call task on the Toloka crowdsourcing platform. The annotations were generated from scratch, utilizing exclusively the CC BY-licensed images from MS COCO. The annotators were asked to select the images containing the objects they found subjectively interesting and then compose questions about these objects. Then, for each question-image pair, we asked the annotators to select the answer on the image using a bounding box, allowing us to exclude unanswerable questions. Although it was possible to facilitate the question composition using such models as DH-GAN <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib10" title="" class="ltx_ref">Kai:21, </a>)</cite>, we decided to stick to the pure crowdsourcing approach. This also allowed us to avoid synthetic data in our task and acquire the more natural formulations made by real humans.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We adopted the well-known methodology called Find-Fix-Verify <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">Bernstein:10, </a>)</cite> for crowdsourcing creative datasets. It separates content production and verification tasks, and enables solving both tasks using crowdsourcing at scale. Our experience shows that the key issue in creative tasks is to ensure that <em id="S4.p2.1.1" class="ltx_emph ltx_font_italic">all the annotators understand the task the same way as we do</em>. Thus, we had to run multiple iterations of task design that finally resulted in seven-stage annotation pipeline in Figure <a href="#S4.F3" title="Figure 3 ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We will describe these stages in four coherent parts in the subsequent subsections. At the verification tasks the annotator who submitted the bounding box or a question and the annotator who verified it were two different people.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Annotator Selection</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As composing good questions and drawing good bounding boxes imply a fair amount of creativity, we had to select the annotators who understand and feel the task the same way as we do. Besides this requirement, we needed the annotators to be able to actually solve it — by being able to formulate grammatically correct questions in English. As a result, we designed a two-step admission procedure for annotator selection that included a language test and a question verification task.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language Test.</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">We had to make sure they have good reading and writing skills in English. Thus, we designed a single-choice test of five questions that was similar to the reading comprehension part of English exams. Each question contained a paragraph of approximately ten complex English sentences and was provided with four possible interpretations. Only one interpretation was correct. Since during prototyping we found that failing this task led to further problems with the question composition, we required the annotators to solve the test without any mistakes.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Question Verification.</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">After the annotators passed the language test, we wanted them to get a good understanding of what we expect them to produce. During prototyping we found that question composition part required additional attention, so the annotators had to solve the same verification task as described in Section <a href="#S4.SS3" title="4.3 Question Composition ‣ 4 Annotation Methodology ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>: given an image, a bounding box, and a question, confirm whether the question is well-formulated according to our strict requirements. We manually annotated the qualification dataset for this task. Those who passed this task were admitted for the real annotation.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Bounding Boxes</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">After sampling images from MS COCO and selecting the right annotators for our task, we performed <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">bounding box annotation</em> in two steps. First, we asked the annotators to pick one large unique object in the given image and draw a tight bounding box around it. Then, for each pair of image and bounding box, we asked the same annotators to check the submitted bounding boxes against the same instruction that we showed in the previous step.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Question Composition</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">After obtaining the bounding boxes, we performed the similar steps to produce questions about the selected objects. We found this part to be the most challenging in our entire annotation task and spent most of our pipeline development time on it. First, given an image and a bounding box, we asked the annotators to compose a simple question in plain English that will allow one to find the object selected in the bounding box. Then, we asked the annotators to check the submitted questions against the same instruction that we showed them before. Since we have only one bounding box per image, we composed only one question per image.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Post-Processing</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We performed three additional steps to ensure a high quality of our dataset to exclude poorly-formulated, leaking, and potentially offensive instances.</p>
</div>
<section id="S4.SS4.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Unanswerable Questions.</h4>

<div id="S4.SS4.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px1.p1.1" class="ltx_p">After receiving the entire dataset, we decided to ask the annotators to perform the same ask as the algorithms should: given an image and a question, draw a bounding box around the answer. We were able to establish the crowdsourcing baseline for further use (see Section <a href="#S5" title="5 Metrics and Baselines ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for more details). Also, we managed to exclude from our dataset the instances for which the ground truth bounding boxes were significantly diverging from the newly-annotated bounding boxes.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Intersection Avoidance.</h4>

<div id="S4.SS4.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px2.p1.1" class="ltx_p">Since we used images from MS COCO, we explicitly checked the overlap between bounding boxes in our dataset and in the original dataset. About 20% of them had non-empty overlap, so we put all such instances into the train dataset. Otherwise the dataset splits were random.</p>
</div>
</section>
<section id="S4.SS4.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Offensive Content.</h4>

<div id="S4.SS4.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS0.Px3.p1.1" class="ltx_p">During the dataset inspection, we found certain unacceptable questions suggesting offensive content. There were two prominent examples. First, there was a question “What can hit the animals?” for an image showing two zebras in a clearly non-hostile environment with a rock on sand.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://toloka-cdn.azureedge.net/wsdmcup2023/000000535978.jpg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://toloka-cdn.azureedge.net/wsdmcup2023/000000535978.jpg</a></span></span></span> We reformulated the question. Second, there was a photo of a zebra shot by a group of hunters with the corresponding question. We fixed this by keyword filtering.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Metrics and Baselines</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">In our task, the answers correspond to bounding box coordinates, with only one bounding box per image. Therefore, we employ the <em id="S5.p1.2.1" class="ltx_emph ltx_font_italic">intersection over union</em> (<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\operatorname{IoU}</annotation></semantics></math>), also known as the <em id="S5.p1.2.2" class="ltx_emph ltx_font_italic">Jaccard index</em>, as our evaluation criterion. For the <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">i</annotation></semantics></math>-th image, we define it as follows:</p>
<table id="S5.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}_{i}=\frac{I_{i}}{U_{i}}\text{,}" display="block"><semantics id="S5.Ex1.m1.1a"><mrow id="S5.Ex1.m1.1.1" xref="S5.Ex1.m1.1.1.cmml"><msub id="S5.Ex1.m1.1.1.2" xref="S5.Ex1.m1.1.1.2.cmml"><mi id="S5.Ex1.m1.1.1.2.2" xref="S5.Ex1.m1.1.1.2.2.cmml">IoU</mi><mi id="S5.Ex1.m1.1.1.2.3" xref="S5.Ex1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S5.Ex1.m1.1.1.1" xref="S5.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S5.Ex1.m1.1.1.3" xref="S5.Ex1.m1.1.1.3.cmml"><mfrac id="S5.Ex1.m1.1.1.3.2" xref="S5.Ex1.m1.1.1.3.2.cmml"><msub id="S5.Ex1.m1.1.1.3.2.2" xref="S5.Ex1.m1.1.1.3.2.2.cmml"><mi id="S5.Ex1.m1.1.1.3.2.2.2" xref="S5.Ex1.m1.1.1.3.2.2.2.cmml">I</mi><mi id="S5.Ex1.m1.1.1.3.2.2.3" xref="S5.Ex1.m1.1.1.3.2.2.3.cmml">i</mi></msub><msub id="S5.Ex1.m1.1.1.3.2.3" xref="S5.Ex1.m1.1.1.3.2.3.cmml"><mi id="S5.Ex1.m1.1.1.3.2.3.2" xref="S5.Ex1.m1.1.1.3.2.3.2.cmml">U</mi><mi id="S5.Ex1.m1.1.1.3.2.3.3" xref="S5.Ex1.m1.1.1.3.2.3.3.cmml">i</mi></msub></mfrac><mo lspace="0em" rspace="0em" id="S5.Ex1.m1.1.1.3.1" xref="S5.Ex1.m1.1.1.3.1.cmml">​</mo><mtext id="S5.Ex1.m1.1.1.3.3" xref="S5.Ex1.m1.1.1.3.3a.cmml">,</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.1b"><apply id="S5.Ex1.m1.1.1.cmml" xref="S5.Ex1.m1.1.1"><eq id="S5.Ex1.m1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1"></eq><apply id="S5.Ex1.m1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.2.1.cmml" xref="S5.Ex1.m1.1.1.2">subscript</csymbol><ci id="S5.Ex1.m1.1.1.2.2.cmml" xref="S5.Ex1.m1.1.1.2.2">IoU</ci><ci id="S5.Ex1.m1.1.1.2.3.cmml" xref="S5.Ex1.m1.1.1.2.3">𝑖</ci></apply><apply id="S5.Ex1.m1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.3"><times id="S5.Ex1.m1.1.1.3.1.cmml" xref="S5.Ex1.m1.1.1.3.1"></times><apply id="S5.Ex1.m1.1.1.3.2.cmml" xref="S5.Ex1.m1.1.1.3.2"><divide id="S5.Ex1.m1.1.1.3.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2"></divide><apply id="S5.Ex1.m1.1.1.3.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.3.2.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.2">subscript</csymbol><ci id="S5.Ex1.m1.1.1.3.2.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2">𝐼</ci><ci id="S5.Ex1.m1.1.1.3.2.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S5.Ex1.m1.1.1.3.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S5.Ex1.m1.1.1.3.2.3.1.cmml" xref="S5.Ex1.m1.1.1.3.2.3">subscript</csymbol><ci id="S5.Ex1.m1.1.1.3.2.3.2.cmml" xref="S5.Ex1.m1.1.1.3.2.3.2">𝑈</ci><ci id="S5.Ex1.m1.1.1.3.2.3.3.cmml" xref="S5.Ex1.m1.1.1.3.2.3.3">𝑖</ci></apply></apply><ci id="S5.Ex1.m1.1.1.3.3a.cmml" xref="S5.Ex1.m1.1.1.3.3"><mtext id="S5.Ex1.m1.1.1.3.3.cmml" xref="S5.Ex1.m1.1.1.3.3">,</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.1c">\operatorname{IoU}_{i}=\frac{I_{i}}{U_{i}}\text{,}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S5.p1.6" class="ltx_p">where <math id="S5.p1.3.m1.1" class="ltx_Math" alttext="I_{i}" display="inline"><semantics id="S5.p1.3.m1.1a"><msub id="S5.p1.3.m1.1.1" xref="S5.p1.3.m1.1.1.cmml"><mi id="S5.p1.3.m1.1.1.2" xref="S5.p1.3.m1.1.1.2.cmml">I</mi><mi id="S5.p1.3.m1.1.1.3" xref="S5.p1.3.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.3.m1.1b"><apply id="S5.p1.3.m1.1.1.cmml" xref="S5.p1.3.m1.1.1"><csymbol cd="ambiguous" id="S5.p1.3.m1.1.1.1.cmml" xref="S5.p1.3.m1.1.1">subscript</csymbol><ci id="S5.p1.3.m1.1.1.2.cmml" xref="S5.p1.3.m1.1.1.2">𝐼</ci><ci id="S5.p1.3.m1.1.1.3.cmml" xref="S5.p1.3.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m1.1c">I_{i}</annotation></semantics></math> represents the intersection area between the ground truth bounding box and the predicted bounding box, and <math id="S5.p1.4.m2.1" class="ltx_Math" alttext="U_{i}" display="inline"><semantics id="S5.p1.4.m2.1a"><msub id="S5.p1.4.m2.1.1" xref="S5.p1.4.m2.1.1.cmml"><mi id="S5.p1.4.m2.1.1.2" xref="S5.p1.4.m2.1.1.2.cmml">U</mi><mi id="S5.p1.4.m2.1.1.3" xref="S5.p1.4.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.p1.4.m2.1b"><apply id="S5.p1.4.m2.1.1.cmml" xref="S5.p1.4.m2.1.1"><csymbol cd="ambiguous" id="S5.p1.4.m2.1.1.1.cmml" xref="S5.p1.4.m2.1.1">subscript</csymbol><ci id="S5.p1.4.m2.1.1.2.cmml" xref="S5.p1.4.m2.1.1.2">𝑈</ci><ci id="S5.p1.4.m2.1.1.3.cmml" xref="S5.p1.4.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m2.1c">U_{i}</annotation></semantics></math> is the union of these boxes. Consequently, for the entire dataset of <math id="S5.p1.5.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.p1.5.m3.1a"><mi id="S5.p1.5.m3.1.1" xref="S5.p1.5.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p1.5.m3.1b"><ci id="S5.p1.5.m3.1.1.cmml" xref="S5.p1.5.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m3.1c">N</annotation></semantics></math> images, the evaluation criterion is the <em id="S5.p1.6.1" class="ltx_emph ltx_font_italic">average intersection over union</em>, denoted as <math id="S5.p1.6.m4.1" class="ltx_Math" alttext="\operatorname{AIoU}" display="inline"><semantics id="S5.p1.6.m4.1a"><mi id="S5.p1.6.m4.1.1" xref="S5.p1.6.m4.1.1.cmml">AIoU</mi><annotation-xml encoding="MathML-Content" id="S5.p1.6.m4.1b"><ci id="S5.p1.6.m4.1.1.cmml" xref="S5.p1.6.m4.1.1">AIoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m4.1c">\operatorname{AIoU}</annotation></semantics></math>:</p>
<table id="S5.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.Ex2.m1.1" class="ltx_Math" alttext="\operatorname{AIoU}=\frac{1}{N}\sum^{N}_{i=1}\operatorname{IoU}_{i}\text{.}" display="block"><semantics id="S5.Ex2.m1.1a"><mrow id="S5.Ex2.m1.1.1" xref="S5.Ex2.m1.1.1.cmml"><mi id="S5.Ex2.m1.1.1.2" xref="S5.Ex2.m1.1.1.2.cmml">AIoU</mi><mo id="S5.Ex2.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.3" xref="S5.Ex2.m1.1.1.3.cmml"><mfrac id="S5.Ex2.m1.1.1.3.2" xref="S5.Ex2.m1.1.1.3.2.cmml"><mn id="S5.Ex2.m1.1.1.3.2.2" xref="S5.Ex2.m1.1.1.3.2.2.cmml">1</mn><mi id="S5.Ex2.m1.1.1.3.2.3" xref="S5.Ex2.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S5.Ex2.m1.1.1.3.1" xref="S5.Ex2.m1.1.1.3.1.cmml">​</mo><mrow id="S5.Ex2.m1.1.1.3.3" xref="S5.Ex2.m1.1.1.3.3.cmml"><munderover id="S5.Ex2.m1.1.1.3.3.1" xref="S5.Ex2.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S5.Ex2.m1.1.1.3.3.1.2.2" xref="S5.Ex2.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S5.Ex2.m1.1.1.3.3.1.3" xref="S5.Ex2.m1.1.1.3.3.1.3.cmml"><mi id="S5.Ex2.m1.1.1.3.3.1.3.2" xref="S5.Ex2.m1.1.1.3.3.1.3.2.cmml">i</mi><mo id="S5.Ex2.m1.1.1.3.3.1.3.1" xref="S5.Ex2.m1.1.1.3.3.1.3.1.cmml">=</mo><mn id="S5.Ex2.m1.1.1.3.3.1.3.3" xref="S5.Ex2.m1.1.1.3.3.1.3.3.cmml">1</mn></mrow><mi id="S5.Ex2.m1.1.1.3.3.1.2.3" xref="S5.Ex2.m1.1.1.3.3.1.2.3.cmml">N</mi></munderover><mrow id="S5.Ex2.m1.1.1.3.3.2" xref="S5.Ex2.m1.1.1.3.3.2.cmml"><msub id="S5.Ex2.m1.1.1.3.3.2.1" xref="S5.Ex2.m1.1.1.3.3.2.1.cmml"><mi id="S5.Ex2.m1.1.1.3.3.2.1.2" xref="S5.Ex2.m1.1.1.3.3.2.1.2.cmml">IoU</mi><mi id="S5.Ex2.m1.1.1.3.3.2.1.3" xref="S5.Ex2.m1.1.1.3.3.2.1.3.cmml">i</mi></msub><mo lspace="0.167em" id="S5.Ex2.m1.1.1.3.3.2a" xref="S5.Ex2.m1.1.1.3.3.2.cmml">⁡</mo><mtext id="S5.Ex2.m1.1.1.3.3.2.2" xref="S5.Ex2.m1.1.1.3.3.2.2a.cmml">.</mtext></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1b"><apply id="S5.Ex2.m1.1.1.cmml" xref="S5.Ex2.m1.1.1"><eq id="S5.Ex2.m1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1"></eq><ci id="S5.Ex2.m1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.2">AIoU</ci><apply id="S5.Ex2.m1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.3"><times id="S5.Ex2.m1.1.1.3.1.cmml" xref="S5.Ex2.m1.1.1.3.1"></times><apply id="S5.Ex2.m1.1.1.3.2.cmml" xref="S5.Ex2.m1.1.1.3.2"><divide id="S5.Ex2.m1.1.1.3.2.1.cmml" xref="S5.Ex2.m1.1.1.3.2"></divide><cn type="integer" id="S5.Ex2.m1.1.1.3.2.2.cmml" xref="S5.Ex2.m1.1.1.3.2.2">1</cn><ci id="S5.Ex2.m1.1.1.3.2.3.cmml" xref="S5.Ex2.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S5.Ex2.m1.1.1.3.3.cmml" xref="S5.Ex2.m1.1.1.3.3"><apply id="S5.Ex2.m1.1.1.3.3.1.cmml" xref="S5.Ex2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.3.3.1.1.cmml" xref="S5.Ex2.m1.1.1.3.3.1">subscript</csymbol><apply id="S5.Ex2.m1.1.1.3.3.1.2.cmml" xref="S5.Ex2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.3.3.1.2.1.cmml" xref="S5.Ex2.m1.1.1.3.3.1">superscript</csymbol><sum id="S5.Ex2.m1.1.1.3.3.1.2.2.cmml" xref="S5.Ex2.m1.1.1.3.3.1.2.2"></sum><ci id="S5.Ex2.m1.1.1.3.3.1.2.3.cmml" xref="S5.Ex2.m1.1.1.3.3.1.2.3">𝑁</ci></apply><apply id="S5.Ex2.m1.1.1.3.3.1.3.cmml" xref="S5.Ex2.m1.1.1.3.3.1.3"><eq id="S5.Ex2.m1.1.1.3.3.1.3.1.cmml" xref="S5.Ex2.m1.1.1.3.3.1.3.1"></eq><ci id="S5.Ex2.m1.1.1.3.3.1.3.2.cmml" xref="S5.Ex2.m1.1.1.3.3.1.3.2">𝑖</ci><cn type="integer" id="S5.Ex2.m1.1.1.3.3.1.3.3.cmml" xref="S5.Ex2.m1.1.1.3.3.1.3.3">1</cn></apply></apply><apply id="S5.Ex2.m1.1.1.3.3.2.cmml" xref="S5.Ex2.m1.1.1.3.3.2"><apply id="S5.Ex2.m1.1.1.3.3.2.1.cmml" xref="S5.Ex2.m1.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S5.Ex2.m1.1.1.3.3.2.1.1.cmml" xref="S5.Ex2.m1.1.1.3.3.2.1">subscript</csymbol><ci id="S5.Ex2.m1.1.1.3.3.2.1.2.cmml" xref="S5.Ex2.m1.1.1.3.3.2.1.2">IoU</ci><ci id="S5.Ex2.m1.1.1.3.3.2.1.3.cmml" xref="S5.Ex2.m1.1.1.3.3.2.1.3">𝑖</ci></apply><ci id="S5.Ex2.m1.1.1.3.3.2.2a.cmml" xref="S5.Ex2.m1.1.1.3.3.2.2"><mtext id="S5.Ex2.m1.1.1.3.3.2.2.cmml" xref="S5.Ex2.m1.1.1.3.3.2.2">.</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1c">\operatorname{AIoU}=\frac{1}{N}\sum^{N}_{i=1}\operatorname{IoU}_{i}\text{.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.7" class="ltx_p">For convenience, we multiply the <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S5.p2.1.m1.1a"><mi id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\operatorname{IoU}</annotation></semantics></math> values by <math id="S5.p2.2.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S5.p2.2.m2.1a"><mn id="S5.p2.2.m2.1.1" xref="S5.p2.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.p2.2.m2.1b"><cn type="integer" id="S5.p2.2.m2.1.1.cmml" xref="S5.p2.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.2.m2.1c">100</annotation></semantics></math>. We used the following baselines to estimate the human and machine performance on our task. We additionally reported the prediction accuracy values that were obtained by choosing the threshold value for <math id="S5.p2.3.m3.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S5.p2.3.m3.1a"><mi id="S5.p2.3.m3.1.1" xref="S5.p2.3.m3.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S5.p2.3.m3.1b"><ci id="S5.p2.3.m3.1.1.cmml" xref="S5.p2.3.m3.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.3.m3.1c">\operatorname{IoU}</annotation></semantics></math> and treating the instances for which the threshold was passed as the correct ones. We used two common thresholds, <math id="S5.p2.4.m4.1" class="ltx_Math" alttext="\operatorname{IoU}=50" display="inline"><semantics id="S5.p2.4.m4.1a"><mrow id="S5.p2.4.m4.1.1" xref="S5.p2.4.m4.1.1.cmml"><mi id="S5.p2.4.m4.1.1.2" xref="S5.p2.4.m4.1.1.2.cmml">IoU</mi><mo id="S5.p2.4.m4.1.1.1" xref="S5.p2.4.m4.1.1.1.cmml">=</mo><mn id="S5.p2.4.m4.1.1.3" xref="S5.p2.4.m4.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.4.m4.1b"><apply id="S5.p2.4.m4.1.1.cmml" xref="S5.p2.4.m4.1.1"><eq id="S5.p2.4.m4.1.1.1.cmml" xref="S5.p2.4.m4.1.1.1"></eq><ci id="S5.p2.4.m4.1.1.2.cmml" xref="S5.p2.4.m4.1.1.2">IoU</ci><cn type="integer" id="S5.p2.4.m4.1.1.3.cmml" xref="S5.p2.4.m4.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.4.m4.1c">\operatorname{IoU}=50</annotation></semantics></math> and <math id="S5.p2.5.m5.1" class="ltx_Math" alttext="\operatorname{IoU}=70" display="inline"><semantics id="S5.p2.5.m5.1a"><mrow id="S5.p2.5.m5.1.1" xref="S5.p2.5.m5.1.1.cmml"><mi id="S5.p2.5.m5.1.1.2" xref="S5.p2.5.m5.1.1.2.cmml">IoU</mi><mo id="S5.p2.5.m5.1.1.1" xref="S5.p2.5.m5.1.1.1.cmml">=</mo><mn id="S5.p2.5.m5.1.1.3" xref="S5.p2.5.m5.1.1.3.cmml">70</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.5.m5.1b"><apply id="S5.p2.5.m5.1.1.cmml" xref="S5.p2.5.m5.1.1"><eq id="S5.p2.5.m5.1.1.1.cmml" xref="S5.p2.5.m5.1.1.1"></eq><ci id="S5.p2.5.m5.1.1.2.cmml" xref="S5.p2.5.m5.1.1.2">IoU</ci><cn type="integer" id="S5.p2.5.m5.1.1.3.cmml" xref="S5.p2.5.m5.1.1.3">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.5.m5.1c">\operatorname{IoU}=70</annotation></semantics></math>, and denote the accuracy values as <math id="S5.p2.6.m6.1" class="ltx_Math" alttext="\operatorname{IoU}&gt;50" display="inline"><semantics id="S5.p2.6.m6.1a"><mrow id="S5.p2.6.m6.1.1" xref="S5.p2.6.m6.1.1.cmml"><mi id="S5.p2.6.m6.1.1.2" xref="S5.p2.6.m6.1.1.2.cmml">IoU</mi><mo id="S5.p2.6.m6.1.1.1" xref="S5.p2.6.m6.1.1.1.cmml">&gt;</mo><mn id="S5.p2.6.m6.1.1.3" xref="S5.p2.6.m6.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.6.m6.1b"><apply id="S5.p2.6.m6.1.1.cmml" xref="S5.p2.6.m6.1.1"><gt id="S5.p2.6.m6.1.1.1.cmml" xref="S5.p2.6.m6.1.1.1"></gt><ci id="S5.p2.6.m6.1.1.2.cmml" xref="S5.p2.6.m6.1.1.2">IoU</ci><cn type="integer" id="S5.p2.6.m6.1.1.3.cmml" xref="S5.p2.6.m6.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.6.m6.1c">\operatorname{IoU}&gt;50</annotation></semantics></math> and <math id="S5.p2.7.m7.1" class="ltx_Math" alttext="\operatorname{IoU}&gt;70" display="inline"><semantics id="S5.p2.7.m7.1a"><mrow id="S5.p2.7.m7.1.1" xref="S5.p2.7.m7.1.1.cmml"><mi id="S5.p2.7.m7.1.1.2" xref="S5.p2.7.m7.1.1.2.cmml">IoU</mi><mo id="S5.p2.7.m7.1.1.1" xref="S5.p2.7.m7.1.1.1.cmml">&gt;</mo><mn id="S5.p2.7.m7.1.1.3" xref="S5.p2.7.m7.1.1.3.cmml">70</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.7.m7.1b"><apply id="S5.p2.7.m7.1.1.cmml" xref="S5.p2.7.m7.1.1"><gt id="S5.p2.7.m7.1.1.1.cmml" xref="S5.p2.7.m7.1.1.1"></gt><ci id="S5.p2.7.m7.1.1.2.cmml" xref="S5.p2.7.m7.1.1.2">IoU</ci><cn type="integer" id="S5.p2.7.m7.1.1.3.cmml" xref="S5.p2.7.m7.1.1.3">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.7.m7.1c">\operatorname{IoU}&gt;70</annotation></semantics></math>, correspondingly.</p>
</div>
<section id="S5.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Crowdsourcing.</h4>

<div id="S5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px1.p1.7" class="ltx_p">We evaluated how well non-expert human annotators can solve our task by running a dedicated round of crowdsourcing annotations on Toloka. We found them to tackle this task successfully without knowing the ground truth. On all three subsets of our data, the average <math id="S5.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">\operatorname{IoU}</annotation></semantics></math> value is <math id="S5.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="87.124\pm 0.746" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">87.124</mn><mo id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">±</mo><mn id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">0.746</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.1">plus-or-minus</csymbol><cn type="float" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.2">87.124</cn><cn type="float" id="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.2.m2.1.1.3">0.746</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.2.m2.1c">87.124\pm 0.746</annotation></semantics></math>, which we consider as a <em id="S5.SS0.SSS0.Px1.p1.7.1" class="ltx_emph ltx_font_italic">strong human baseline</em> for our task. Krippendorff’s <math id="S5.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="S5.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="S5.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.3.m3.1c">\alpha</annotation></semantics></math> coefficients for the public test is <math id="S5.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="0.68" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.4.m4.1a"><mn id="S5.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S5.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">0.68</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.4.m4.1b"><cn type="float" id="S5.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.4.m4.1.1">0.68</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.4.m4.1c">0.68</annotation></semantics></math> and for the private test is <math id="S5.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="0.66" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.5.m5.1a"><mn id="S5.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S5.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">0.66</mn><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.5.m5.1b"><cn type="float" id="S5.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.5.m5.1.1">0.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.5.m5.1c">0.66</annotation></semantics></math>, showing the decent agreement between the responses; we used <math id="S5.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="1-\operatorname{IoU}" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.6.m6.1a"><mrow id="S5.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mn id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">1</mn><mo id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.1" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml">−</mo><mi id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml">IoU</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1"><minus id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.1"></minus><cn type="integer" id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.2">1</cn><ci id="S5.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.6.m6.1.1.3">IoU</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.6.m6.1c">1-\operatorname{IoU}</annotation></semantics></math> as the distance metric when calculating the <math id="S5.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS0.SSS0.Px1.p1.7.m7.1a"><mi id="S5.SS0.SSS0.Px1.p1.7.m7.1.1" xref="S5.SS0.SSS0.Px1.p1.7.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.7.m7.1b"><ci id="S5.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.7.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.7.m7.1c">\alpha</annotation></semantics></math> value.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OFA + SAM.</h4>

<div id="S5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px2.p1.2" class="ltx_p">The first baseline is zero-shot and is primarily based on OFA <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">Wang:22, </a>)</cite>, combined with bounding box correction using SAM. To solve the task, we followed a two-step zero-shot setup. First, we addressed visual question answering, where the model was given a prompt “{question} Name an object in the picture” along with an image. The model provided the name of a clue object to the question. In the second step, an object corresponding to the answer from the previous step was annotated using the prompt “which region does the text "{answer}" describe?”, resulting in <math id="S5.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}=42.462" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">42.462</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.1.m1.1.1.3">42.462</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.1.m1.1c">\operatorname{IoU}=42.462</annotation></semantics></math>. Subsequently, with the obtained bounding boxes, SAM generated the corresponding masks for the annotated object, which were then transformed into bounding boxes. This enabled us to achieve <math id="S5.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\operatorname{IoU}=44.851" display="inline"><semantics id="S5.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">44.851</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.1"></eq><ci id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px2.p1.2.m2.1.1.3">44.851</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px2.p1.2.m2.1c">\operatorname{IoU}=44.851</annotation></semantics></math> with this baseline.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OFA + SAM (VQA without Image).</h4>

<div id="S5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px3.p1.2" class="ltx_p">We added the ablation study that answers questions without images as a new baseline model called OFA + SAM (VQA without Image). In particular, this method performed visual question answering by asking the question to the OFA model with blank white image and then drawing a bounding box corresponding to the obtained textual answer on the original image. This ablation shows that image is important for answering questions. The results were worse than the original OFA + SAM baseline, demonstrating <math id="S5.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}=39.075" display="inline"><semantics id="S5.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">39.075</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p1.1.m1.1.1.3">39.075</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.1.m1.1c">\operatorname{IoU}=39.075</annotation></semantics></math> vs. <math id="S5.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\operatorname{IoU}=44.851" display="inline"><semantics id="S5.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S5.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">44.851</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1"><eq id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.1"></eq><ci id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S5.SS0.SSS0.Px3.p1.2.m2.1.1.3">44.851</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px3.p1.2.m2.1c">\operatorname{IoU}=44.851</annotation></semantics></math> on the private test subset of our dataset.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OVSeg + SAM.</h4>

<div id="S5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px4.p1.1" class="ltx_p">Another zero-shot baseline, called OVSeg <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib16" title="" class="ltx_ref">Liang:23, </a>)</cite>, utilizes SAM <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib13" title="" class="ltx_ref">Kirillov:23, </a>)</cite> as a proposal generator instead of MaskFormer in the original setup. This approach achieved <math id="S5.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}=35.073" display="inline"><semantics id="S5.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">35.073</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px4.p1.1.m1.1.1.3">35.073</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px4.p1.1.m1.1c">\operatorname{IoU}=35.073</annotation></semantics></math> on the private test subset.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Kosmos-2.</h4>

<div id="S5.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px5.p1.1" class="ltx_p">We also evaluated a grounding multi-modal large language model Kosmos-2 <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib18" title="" class="ltx_ref">Kosmos-2, </a>)</cite> in a zero-shot setup. We addressed the visual grounding task with “Find an object which answers the question. Question: "{question}". Answer:”. This baseline demonstrated <math id="S5.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}=22.571" display="inline"><semantics id="S5.SS0.SSS0.Px5.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml">22.571</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px5.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px5.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px5.p1.1.m1.1.1.3">22.571</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px5.p1.1.m1.1c">\operatorname{IoU}=22.571</annotation></semantics></math> on private test.</p>
</div>
</section>
<section id="S5.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">YOLOR + CLIP.</h4>

<div id="S5.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S5.SS0.SSS0.Px6.p1.1" class="ltx_p">Our last baseline used a detection model, YOLOR <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib23" title="" class="ltx_ref">Wang:21, </a>)</cite>, to generate candidate rectangles. Then, we applied CLIP <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib20" title="" class="ltx_ref">Radford:21, </a>)</cite> to measure the similarity between the question and a part of the image bounded by each candidate rectangle. To make a prediction, it used the candidate with the highest similarity. This baseline method achieved <math id="S5.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}=21.292" display="inline"><semantics id="S5.SS0.SSS0.Px6.p1.1.m1.1a"><mrow id="S5.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml">IoU</mi><mo id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.1" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml">21.292</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px6.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1"><eq id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.1"></eq><ci id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.2">IoU</ci><cn type="float" id="S5.SS0.SSS0.Px6.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px6.p1.1.m1.1.1.3">21.292</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px6.p1.1.m1.1c">\operatorname{IoU}=21.292</annotation></semantics></math> on the private test subset.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">To assess our dataset beyond zero-shot baselines and crowd annotators, we conducted a large-scale open-call competition at WSDM Cup, which took place alongside the WSDM ’23 conference.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="http://www.wsdm-conference.org/2023/program/wsdm-cup" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.wsdm-conference.org/2023/program/wsdm-cup</a></span></span></span> To ensure fair participation, we hosted the competition on CodaLab. Participants were given access to the complete <em id="S6.p1.1.1" class="ltx_emph ltx_font_italic">train</em> subset for training their models, as well as a masked <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">public test</em> subset for the leaderboard competition.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/7434" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://codalab.lisn.upsaclay.fr/competitions/7434</a></span></span></span> The final rankings were determined based on performance on a concealed <em id="S6.p1.1.3" class="ltx_emph ltx_font_italic">private test</em> subset, utilizing Docker images provided by the participants and executed on our servers. The inference code was required to complete within one hour on an Azure virtual machine equipped with 16 CPU cores, 200 GB of RAM, and one NVIDIA A100 80 GB GPU. We received a total of 48 participants in our competition, of which 9 submitted their code for the final stage. For the sake of brevity, Table <a href="#S6.T2" title="Table 2 ‣ 6 Evaluation ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports only top-3 performance along with the above-described baselines. We put a more complete table with the competition results to supplementary materials. In the following paragraphs, we provide a brief overview of the methodologies employed by the three winning teams during the reproduction phase on the private test subset of our dataset.</p>
</div>
<figure id="S6.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S6.T2.30.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S6.T2.31.2" class="ltx_text" style="font-size:90%;">Baselines and final top-3 team standings on the <em id="S6.T2.31.2.1" class="ltx_emph ltx_font_italic">private test</em> subset, obtained at the reproduction phase of our competition; for visual convenience, we multiplied the IoU values by 100; out of 48 participants, only 9 submitted their code during the reproduction phase. Baseline methods did not participate in the competition, their places are denoted as “—”.</span></figcaption>
<table id="S6.T2.27" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T2.27.28.1" class="ltx_tr">
<th id="S6.T2.27.28.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S6.T2.27.28.1.1.1" class="ltx_text ltx_font_bold">Place</span></th>
<th id="S6.T2.27.28.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S6.T2.27.28.1.2.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S6.T2.27.28.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S6.T2.27.28.1.3.1" class="ltx_text ltx_font_bold">IoU</span></th>
<th id="S6.T2.27.28.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S6.T2.27.28.1.4.1" class="ltx_text ltx_font_bold">IoU &gt; 50</span></th>
<th id="S6.T2.27.28.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S6.T2.27.28.1.5.1" class="ltx_text ltx_font_bold">IoU &gt; 70</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T2.3.3" class="ltx_tr">
<th id="S6.T2.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S6.T2.3.3.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S6.T2.3.3.5.1" class="ltx_text" style="color:#808080;">Crowdsourcing</span></th>
<td id="S6.T2.1.1.1" class="ltx_td ltx_align_right ltx_border_t"><math id="S6.T2.1.1.1.m1.1" class="ltx_Math" alttext="87.154" display="inline"><semantics id="S6.T2.1.1.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.1.1.1.m1.1.1" xref="S6.T2.1.1.1.m1.1.1.cmml">87.154</mn><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.m1.1b"><cn type="float" id="S6.T2.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.m1.1.1">87.154</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.m1.1c">87.154</annotation></semantics></math></td>
<td id="S6.T2.2.2.2" class="ltx_td ltx_align_right ltx_border_t"><math id="S6.T2.2.2.2.m1.1" class="ltx_Math" alttext="0.954" display="inline"><semantics id="S6.T2.2.2.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.2.2.2.m1.1.1" xref="S6.T2.2.2.2.m1.1.1.cmml">0.954</mn><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.2.m1.1b"><cn type="float" id="S6.T2.2.2.2.m1.1.1.cmml" xref="S6.T2.2.2.2.m1.1.1">0.954</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.2.m1.1c">0.954</annotation></semantics></math></td>
<td id="S6.T2.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><math id="S6.T2.3.3.3.m1.1" class="ltx_Math" alttext="0.914" display="inline"><semantics id="S6.T2.3.3.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.3.3.3.m1.1.1" xref="S6.T2.3.3.3.m1.1.1.cmml">0.914</mn><annotation-xml encoding="MathML-Content" id="S6.T2.3.3.3.m1.1b"><cn type="float" id="S6.T2.3.3.3.m1.1.1.cmml" xref="S6.T2.3.3.3.m1.1.1">0.914</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.3.3.m1.1c">0.914</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.6.6" class="ltx_tr">
<th id="S6.T2.6.6.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">1</th>
<th id="S6.T2.6.6.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.6.6.5.1" class="ltx_text ltx_font_typewriter">wztxy89</span></th>
<td id="S6.T2.4.4.1" class="ltx_td ltx_align_right"><math id="S6.T2.4.4.1.m1.1" class="ltx_Math" alttext="\mathbf{76.347}" display="inline"><semantics id="S6.T2.4.4.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.4.4.1.m1.1.1" xref="S6.T2.4.4.1.m1.1.1.cmml">76.347</mn><annotation-xml encoding="MathML-Content" id="S6.T2.4.4.1.m1.1b"><cn type="float" id="S6.T2.4.4.1.m1.1.1.cmml" xref="S6.T2.4.4.1.m1.1.1">76.347</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.4.4.1.m1.1c">\mathbf{76.347}</annotation></semantics></math></td>
<td id="S6.T2.5.5.2" class="ltx_td ltx_align_right"><math id="S6.T2.5.5.2.m1.1" class="ltx_Math" alttext="\mathbf{0.834}" display="inline"><semantics id="S6.T2.5.5.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.5.5.2.m1.1.1" xref="S6.T2.5.5.2.m1.1.1.cmml">0.834</mn><annotation-xml encoding="MathML-Content" id="S6.T2.5.5.2.m1.1b"><cn type="float" id="S6.T2.5.5.2.m1.1.1.cmml" xref="S6.T2.5.5.2.m1.1.1">0.834</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.5.5.2.m1.1c">\mathbf{0.834}</annotation></semantics></math></td>
<td id="S6.T2.6.6.3" class="ltx_td ltx_align_right"><math id="S6.T2.6.6.3.m1.1" class="ltx_Math" alttext="\mathbf{0.786}" display="inline"><semantics id="S6.T2.6.6.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.6.6.3.m1.1.1" xref="S6.T2.6.6.3.m1.1.1.cmml">0.786</mn><annotation-xml encoding="MathML-Content" id="S6.T2.6.6.3.m1.1b"><cn type="float" id="S6.T2.6.6.3.m1.1.1.cmml" xref="S6.T2.6.6.3.m1.1.1">0.786</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.6.6.3.m1.1c">\mathbf{0.786}</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.9.9" class="ltx_tr">
<th id="S6.T2.9.9.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">2</th>
<th id="S6.T2.9.9.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.9.9.5.1" class="ltx_text ltx_font_typewriter">jinx, Zhouyang_Chi</span></th>
<td id="S6.T2.7.7.1" class="ltx_td ltx_align_right"><math id="S6.T2.7.7.1.m1.1" class="ltx_Math" alttext="\mathbf{76.342}" display="inline"><semantics id="S6.T2.7.7.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.7.7.1.m1.1.1" xref="S6.T2.7.7.1.m1.1.1.cmml">76.342</mn><annotation-xml encoding="MathML-Content" id="S6.T2.7.7.1.m1.1b"><cn type="float" id="S6.T2.7.7.1.m1.1.1.cmml" xref="S6.T2.7.7.1.m1.1.1">76.342</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.7.7.1.m1.1c">\mathbf{76.342}</annotation></semantics></math></td>
<td id="S6.T2.8.8.2" class="ltx_td ltx_align_right"><math id="S6.T2.8.8.2.m1.1" class="ltx_Math" alttext="\mathbf{0.841}" display="inline"><semantics id="S6.T2.8.8.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.8.8.2.m1.1.1" xref="S6.T2.8.8.2.m1.1.1.cmml">0.841</mn><annotation-xml encoding="MathML-Content" id="S6.T2.8.8.2.m1.1b"><cn type="float" id="S6.T2.8.8.2.m1.1.1.cmml" xref="S6.T2.8.8.2.m1.1.1">0.841</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.8.8.2.m1.1c">\mathbf{0.841}</annotation></semantics></math></td>
<td id="S6.T2.9.9.3" class="ltx_td ltx_align_right"><math id="S6.T2.9.9.3.m1.1" class="ltx_Math" alttext="\mathbf{0.779}" display="inline"><semantics id="S6.T2.9.9.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.9.9.3.m1.1.1" xref="S6.T2.9.9.3.m1.1.1.cmml">0.779</mn><annotation-xml encoding="MathML-Content" id="S6.T2.9.9.3.m1.1b"><cn type="float" id="S6.T2.9.9.3.m1.1.1.cmml" xref="S6.T2.9.9.3.m1.1.1">0.779</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.9.9.3.m1.1c">\mathbf{0.779}</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.12.12" class="ltx_tr">
<th id="S6.T2.12.12.4" class="ltx_td ltx_align_center ltx_th ltx_th_row">3</th>
<th id="S6.T2.12.12.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.12.12.5.1" class="ltx_text ltx_font_typewriter">komleva.ep</span></th>
<td id="S6.T2.10.10.1" class="ltx_td ltx_align_right"><math id="S6.T2.10.10.1.m1.1" class="ltx_Math" alttext="\mathbf{75.591}" display="inline"><semantics id="S6.T2.10.10.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.10.10.1.m1.1.1" xref="S6.T2.10.10.1.m1.1.1.cmml">75.591</mn><annotation-xml encoding="MathML-Content" id="S6.T2.10.10.1.m1.1b"><cn type="float" id="S6.T2.10.10.1.m1.1.1.cmml" xref="S6.T2.10.10.1.m1.1.1">75.591</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.10.10.1.m1.1c">\mathbf{75.591}</annotation></semantics></math></td>
<td id="S6.T2.11.11.2" class="ltx_td ltx_align_right"><math id="S6.T2.11.11.2.m1.1" class="ltx_Math" alttext="\mathbf{0.756}" display="inline"><semantics id="S6.T2.11.11.2.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.11.11.2.m1.1.1" xref="S6.T2.11.11.2.m1.1.1.cmml">0.756</mn><annotation-xml encoding="MathML-Content" id="S6.T2.11.11.2.m1.1b"><cn type="float" id="S6.T2.11.11.2.m1.1.1.cmml" xref="S6.T2.11.11.2.m1.1.1">0.756</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.11.11.2.m1.1c">\mathbf{0.756}</annotation></semantics></math></td>
<td id="S6.T2.12.12.3" class="ltx_td ltx_align_right"><math id="S6.T2.12.12.3.m1.1" class="ltx_Math" alttext="\mathbf{0.772}" display="inline"><semantics id="S6.T2.12.12.3.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="S6.T2.12.12.3.m1.1.1" xref="S6.T2.12.12.3.m1.1.1.cmml">0.772</mn><annotation-xml encoding="MathML-Content" id="S6.T2.12.12.3.m1.1b"><cn type="float" id="S6.T2.12.12.3.m1.1.1.cmml" xref="S6.T2.12.12.3.m1.1.1">0.772</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.12.12.3.m1.1c">\mathbf{0.772}</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.15.15" class="ltx_tr">
<th id="S6.T2.15.15.4" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.15.15.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.15.15.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.15.15.5.1" class="ltx_text" style="color:#808080;">OFA + SAM</span></th>
<td id="S6.T2.13.13.1" class="ltx_td ltx_align_right"><math id="S6.T2.13.13.1.m1.1" class="ltx_Math" alttext="44.851" display="inline"><semantics id="S6.T2.13.13.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.13.13.1.m1.1.1" xref="S6.T2.13.13.1.m1.1.1.cmml">44.851</mn><annotation-xml encoding="MathML-Content" id="S6.T2.13.13.1.m1.1b"><cn type="float" id="S6.T2.13.13.1.m1.1.1.cmml" xref="S6.T2.13.13.1.m1.1.1">44.851</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.13.13.1.m1.1c">44.851</annotation></semantics></math></td>
<td id="S6.T2.14.14.2" class="ltx_td ltx_align_right"><math id="S6.T2.14.14.2.m1.1" class="ltx_Math" alttext="0.470" display="inline"><semantics id="S6.T2.14.14.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.14.14.2.m1.1.1" xref="S6.T2.14.14.2.m1.1.1.cmml">0.470</mn><annotation-xml encoding="MathML-Content" id="S6.T2.14.14.2.m1.1b"><cn type="float" id="S6.T2.14.14.2.m1.1.1.cmml" xref="S6.T2.14.14.2.m1.1.1">0.470</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.14.14.2.m1.1c">0.470</annotation></semantics></math></td>
<td id="S6.T2.15.15.3" class="ltx_td ltx_align_right"><math id="S6.T2.15.15.3.m1.1" class="ltx_Math" alttext="0.431" display="inline"><semantics id="S6.T2.15.15.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.15.15.3.m1.1.1" xref="S6.T2.15.15.3.m1.1.1.cmml">0.431</mn><annotation-xml encoding="MathML-Content" id="S6.T2.15.15.3.m1.1b"><cn type="float" id="S6.T2.15.15.3.m1.1.1.cmml" xref="S6.T2.15.15.3.m1.1.1">0.431</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.15.15.3.m1.1c">0.431</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.18.18" class="ltx_tr">
<th id="S6.T2.18.18.4" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.18.18.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.18.18.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.18.18.5.1" class="ltx_text" style="color:#808080;">OFA + SAM (VQA without Image)</span></th>
<td id="S6.T2.16.16.1" class="ltx_td ltx_align_right"><math id="S6.T2.16.16.1.m1.1" class="ltx_Math" alttext="39.075" display="inline"><semantics id="S6.T2.16.16.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.16.16.1.m1.1.1" xref="S6.T2.16.16.1.m1.1.1.cmml">39.075</mn><annotation-xml encoding="MathML-Content" id="S6.T2.16.16.1.m1.1b"><cn type="float" id="S6.T2.16.16.1.m1.1.1.cmml" xref="S6.T2.16.16.1.m1.1.1">39.075</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.16.16.1.m1.1c">39.075</annotation></semantics></math></td>
<td id="S6.T2.17.17.2" class="ltx_td ltx_align_right"><math id="S6.T2.17.17.2.m1.1" class="ltx_Math" alttext="0.407" display="inline"><semantics id="S6.T2.17.17.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.17.17.2.m1.1.1" xref="S6.T2.17.17.2.m1.1.1.cmml">0.407</mn><annotation-xml encoding="MathML-Content" id="S6.T2.17.17.2.m1.1b"><cn type="float" id="S6.T2.17.17.2.m1.1.1.cmml" xref="S6.T2.17.17.2.m1.1.1">0.407</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.17.17.2.m1.1c">0.407</annotation></semantics></math></td>
<td id="S6.T2.18.18.3" class="ltx_td ltx_align_right"><math id="S6.T2.18.18.3.m1.1" class="ltx_Math" alttext="0.370" display="inline"><semantics id="S6.T2.18.18.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.18.18.3.m1.1.1" xref="S6.T2.18.18.3.m1.1.1.cmml">0.370</mn><annotation-xml encoding="MathML-Content" id="S6.T2.18.18.3.m1.1b"><cn type="float" id="S6.T2.18.18.3.m1.1.1.cmml" xref="S6.T2.18.18.3.m1.1.1">0.370</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.18.18.3.m1.1c">0.370</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.21.21" class="ltx_tr">
<th id="S6.T2.21.21.4" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.21.21.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.21.21.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.21.21.5.1" class="ltx_text" style="color:#808080;">OVSeg + SAM</span></th>
<td id="S6.T2.19.19.1" class="ltx_td ltx_align_right"><math id="S6.T2.19.19.1.m1.1" class="ltx_Math" alttext="35.073" display="inline"><semantics id="S6.T2.19.19.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.19.19.1.m1.1.1" xref="S6.T2.19.19.1.m1.1.1.cmml">35.073</mn><annotation-xml encoding="MathML-Content" id="S6.T2.19.19.1.m1.1b"><cn type="float" id="S6.T2.19.19.1.m1.1.1.cmml" xref="S6.T2.19.19.1.m1.1.1">35.073</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.19.19.1.m1.1c">35.073</annotation></semantics></math></td>
<td id="S6.T2.20.20.2" class="ltx_td ltx_align_right"><math id="S6.T2.20.20.2.m1.1" class="ltx_Math" alttext="0.372" display="inline"><semantics id="S6.T2.20.20.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.20.20.2.m1.1.1" xref="S6.T2.20.20.2.m1.1.1.cmml">0.372</mn><annotation-xml encoding="MathML-Content" id="S6.T2.20.20.2.m1.1b"><cn type="float" id="S6.T2.20.20.2.m1.1.1.cmml" xref="S6.T2.20.20.2.m1.1.1">0.372</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.20.20.2.m1.1c">0.372</annotation></semantics></math></td>
<td id="S6.T2.21.21.3" class="ltx_td ltx_align_right"><math id="S6.T2.21.21.3.m1.1" class="ltx_Math" alttext="0.309" display="inline"><semantics id="S6.T2.21.21.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.21.21.3.m1.1.1" xref="S6.T2.21.21.3.m1.1.1.cmml">0.309</mn><annotation-xml encoding="MathML-Content" id="S6.T2.21.21.3.m1.1b"><cn type="float" id="S6.T2.21.21.3.m1.1.1.cmml" xref="S6.T2.21.21.3.m1.1.1">0.309</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.21.21.3.m1.1c">0.309</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.24.24" class="ltx_tr">
<th id="S6.T2.24.24.4" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.24.24.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.24.24.5" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S6.T2.24.24.5.1" class="ltx_text" style="color:#808080;">Kosmos-2</span></th>
<td id="S6.T2.22.22.1" class="ltx_td ltx_align_right"><math id="S6.T2.22.22.1.m1.1" class="ltx_Math" alttext="22.571" display="inline"><semantics id="S6.T2.22.22.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.22.22.1.m1.1.1" xref="S6.T2.22.22.1.m1.1.1.cmml">22.571</mn><annotation-xml encoding="MathML-Content" id="S6.T2.22.22.1.m1.1b"><cn type="float" id="S6.T2.22.22.1.m1.1.1.cmml" xref="S6.T2.22.22.1.m1.1.1">22.571</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.22.22.1.m1.1c">22.571</annotation></semantics></math></td>
<td id="S6.T2.23.23.2" class="ltx_td ltx_align_right"><math id="S6.T2.23.23.2.m1.1" class="ltx_Math" alttext="0.243" display="inline"><semantics id="S6.T2.23.23.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.23.23.2.m1.1.1" xref="S6.T2.23.23.2.m1.1.1.cmml">0.243</mn><annotation-xml encoding="MathML-Content" id="S6.T2.23.23.2.m1.1b"><cn type="float" id="S6.T2.23.23.2.m1.1.1.cmml" xref="S6.T2.23.23.2.m1.1.1">0.243</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.23.23.2.m1.1c">0.243</annotation></semantics></math></td>
<td id="S6.T2.24.24.3" class="ltx_td ltx_align_right"><math id="S6.T2.24.24.3.m1.1" class="ltx_Math" alttext="0.073" display="inline"><semantics id="S6.T2.24.24.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.24.24.3.m1.1.1" xref="S6.T2.24.24.3.m1.1.1.cmml">0.073</mn><annotation-xml encoding="MathML-Content" id="S6.T2.24.24.3.m1.1b"><cn type="float" id="S6.T2.24.24.3.m1.1.1.cmml" xref="S6.T2.24.24.3.m1.1.1">0.073</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.24.24.3.m1.1c">0.073</annotation></semantics></math></td>
</tr>
<tr id="S6.T2.27.27" class="ltx_tr">
<th id="S6.T2.27.27.4" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S6.T2.27.27.4.1" class="ltx_text" style="color:#808080;">—</span></th>
<th id="S6.T2.27.27.5" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S6.T2.27.27.5.1" class="ltx_text" style="color:#808080;">YOLOR + CLIP</span></th>
<td id="S6.T2.25.25.1" class="ltx_td ltx_align_right ltx_border_bb"><math id="S6.T2.25.25.1.m1.1" class="ltx_Math" alttext="21.292" display="inline"><semantics id="S6.T2.25.25.1.m1.1a"><mn mathcolor="#808080" id="S6.T2.25.25.1.m1.1.1" xref="S6.T2.25.25.1.m1.1.1.cmml">21.292</mn><annotation-xml encoding="MathML-Content" id="S6.T2.25.25.1.m1.1b"><cn type="float" id="S6.T2.25.25.1.m1.1.1.cmml" xref="S6.T2.25.25.1.m1.1.1">21.292</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.25.25.1.m1.1c">21.292</annotation></semantics></math></td>
<td id="S6.T2.26.26.2" class="ltx_td ltx_align_right ltx_border_bb"><math id="S6.T2.26.26.2.m1.1" class="ltx_Math" alttext="0.209" display="inline"><semantics id="S6.T2.26.26.2.m1.1a"><mn mathcolor="#808080" id="S6.T2.26.26.2.m1.1.1" xref="S6.T2.26.26.2.m1.1.1.cmml">0.209</mn><annotation-xml encoding="MathML-Content" id="S6.T2.26.26.2.m1.1b"><cn type="float" id="S6.T2.26.26.2.m1.1.1.cmml" xref="S6.T2.26.26.2.m1.1.1">0.209</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.26.26.2.m1.1c">0.209</annotation></semantics></math></td>
<td id="S6.T2.27.27.3" class="ltx_td ltx_align_right ltx_border_bb"><math id="S6.T2.27.27.3.m1.1" class="ltx_Math" alttext="0.201" display="inline"><semantics id="S6.T2.27.27.3.m1.1a"><mn mathcolor="#808080" id="S6.T2.27.27.3.m1.1.1" xref="S6.T2.27.27.3.m1.1.1.cmml">0.201</mn><annotation-xml encoding="MathML-Content" id="S6.T2.27.27.3.m1.1b"><cn type="float" id="S6.T2.27.27.3.m1.1.1.cmml" xref="S6.T2.27.27.3.m1.1.1">0.201</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.27.27.3.m1.1c">0.201</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div id="S6.p2" class="ltx_para">
<dl id="S6.I1" class="ltx_description">
<dt id="S6.I1.ix1" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix1.1.1.1" class="ltx_text ltx_font_bold">3<sup id="S6.I1.ix1.1.1.1.1" class="ltx_sup">rd</sup> Place.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix1.p1" class="ltx_para">
<p id="S6.I1.ix1.p1.1" class="ltx_p">The only single-person winning team, <span id="S6.I1.ix1.p1.1.1" class="ltx_text ltx_font_typewriter">komleva.ep</span>, fine-tuned the pre-trained multi-modal OFA model <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib24" title="" class="ltx_ref">Wang:22, </a>)</cite> on the competition dataset. In order to increase the prediction quality, this team additionally used data from the pre-processed GQA dataset <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib8" title="" class="ltx_ref">Hudson:19, </a>)</cite>.</p>
</div>
</dd>
<dt id="S6.I1.ix2" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix2.1.1.1" class="ltx_text ltx_font_bold">2<sup id="S6.I1.ix2.1.1.1.1" class="ltx_sup">nd</sup> Place.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix2.p1" class="ltx_para">
<p id="S6.I1.ix2.p1.1" class="ltx_p">The team <span id="S6.I1.ix2.p1.1.1" class="ltx_text ltx_font_typewriter">jinx, Zhouyang_Chi</span> devised a three-step pipeline solution. First, at the <em id="S6.I1.ix2.p1.1.2" class="ltx_emph ltx_font_italic">coarse tuning</em> step, they generated textual pseudo answers for the questions and tuned the OFA model to produce textual answers. Then, at the <em id="S6.I1.ix2.p1.1.3" class="ltx_emph ltx_font_italic">fine tuning</em> step, they used prompt engineering of the coarse-tuned OFA model to draw the bounding boxes. Finally, at the <em id="S6.I1.ix2.p1.1.4" class="ltx_emph ltx_font_italic">post-processing</em> step, they ran an ensemble of these coarse- and fine-tuned models to propose and select the best bounding box candidate.</p>
</div>
</dd>
<dt id="S6.I1.ix3" class="ltx_item"><span class="ltx_tag ltx_tag_item"><span id="S6.I1.ix3.1.1.1" class="ltx_text ltx_font_bold">1<sup id="S6.I1.ix3.1.1.1.1" class="ltx_sup">st</sup> Place.</span></span></dt>
<dd class="ltx_item">
<div id="S6.I1.ix3.p1" class="ltx_para">
<p id="S6.I1.ix3.p1.1" class="ltx_p">The <span id="S6.I1.ix3.p1.1.1" class="ltx_text ltx_font_typewriter">wztxy89</span> team created a variant detector using Uni-Perceiver as the multi-modal backbone network <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib27" title="" class="ltx_ref">Zhu:22, </a>)</cite>, with ViT-Adapter for cross-modal localization <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib5" title="" class="ltx_ref">Chen:23, </a>)</cite>, and DINO as the prediction head <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib25" title="" class="ltx_ref">Zhang:23, </a>)</cite>. They also included an auxiliary loss <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib12" title="" class="ltx_ref">Kirillov:19, </a>)</cite> and a test-time augmentation module for improved performance, which helped them win the challenge <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib6" title="" class="ltx_ref">gao2023champion, </a>)</cite>.</p>
</div>
</dd>
</dl>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">Even though the winning systems significantly outperformed our machine learning baselines, no system approached the non-expert level of human performance in our task.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Error Analysis</h2>

<figure id="S7.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S7.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S7.T3.4.2" class="ltx_text" style="font-size:90%;">Distribution of error types of the crowdsourcing baseline and top-3 models from our challenge as described in Section <a href="#S6" title="6 Evaluation ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. The analysis is based on a random sample of 100 <em id="S7.T3.4.2.1" class="ltx_emph ltx_font_italic">most challenging images</em>.</span></figcaption>
<table id="S7.T3.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T3.5.1.1" class="ltx_tr">
<th id="S7.T3.5.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S7.T3.5.1.1.1.1" class="ltx_text ltx_font_bold">Error Class</span></th>
<th id="S7.T3.5.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S7.T3.5.1.1.2.1" class="ltx_text ltx_font_bold">Crowdsourcing</span></th>
<th id="S7.T3.5.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S7.T3.5.1.1.3.1" class="ltx_text ltx_font_bold">1<sup id="S7.T3.5.1.1.3.1.1" class="ltx_sup">st</sup> Place</span></th>
<th id="S7.T3.5.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S7.T3.5.1.1.4.1" class="ltx_text ltx_font_bold">2<sup id="S7.T3.5.1.1.4.1.1" class="ltx_sup">nd</sup> Place</span></th>
<th id="S7.T3.5.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S7.T3.5.1.1.5.1" class="ltx_text ltx_font_bold">3<sup id="S7.T3.5.1.1.5.1.1" class="ltx_sup">rd</sup> Place</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T3.5.2.1" class="ltx_tr">
<td id="S7.T3.5.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Small Object</td>
<td id="S7.T3.5.2.1.2" class="ltx_td ltx_align_right ltx_border_t">26</td>
<td id="S7.T3.5.2.1.3" class="ltx_td ltx_align_right ltx_border_t">13</td>
<td id="S7.T3.5.2.1.4" class="ltx_td ltx_align_right ltx_border_t">13</td>
<td id="S7.T3.5.2.1.5" class="ltx_td ltx_align_right ltx_border_t">11</td>
</tr>
<tr id="S7.T3.5.3.2" class="ltx_tr">
<td id="S7.T3.5.3.2.1" class="ltx_td ltx_align_left">Insignificant Error</td>
<td id="S7.T3.5.3.2.2" class="ltx_td ltx_align_right">19</td>
<td id="S7.T3.5.3.2.3" class="ltx_td ltx_align_right">13</td>
<td id="S7.T3.5.3.2.4" class="ltx_td ltx_align_right">12</td>
<td id="S7.T3.5.3.2.5" class="ltx_td ltx_align_right">16</td>
</tr>
<tr id="S7.T3.5.4.3" class="ltx_tr">
<td id="S7.T3.5.4.3.1" class="ltx_td ltx_align_left">Wrong Object Predicted</td>
<td id="S7.T3.5.4.3.2" class="ltx_td ltx_align_right">5</td>
<td id="S7.T3.5.4.3.3" class="ltx_td ltx_align_right">24</td>
<td id="S7.T3.5.4.3.4" class="ltx_td ltx_align_right">24</td>
<td id="S7.T3.5.4.3.5" class="ltx_td ltx_align_right">20</td>
</tr>
<tr id="S7.T3.5.5.4" class="ltx_tr">
<td id="S7.T3.5.5.4.1" class="ltx_td ltx_align_left">Inaccurate Ground Truth</td>
<td id="S7.T3.5.5.4.2" class="ltx_td ltx_align_right">11</td>
<td id="S7.T3.5.5.4.3" class="ltx_td ltx_align_right">9</td>
<td id="S7.T3.5.5.4.4" class="ltx_td ltx_align_right">9</td>
<td id="S7.T3.5.5.4.5" class="ltx_td ltx_align_right">7</td>
</tr>
<tr id="S7.T3.5.6.5" class="ltx_tr">
<td id="S7.T3.5.6.5.1" class="ltx_td ltx_align_left">Inaccurate Prediction</td>
<td id="S7.T3.5.6.5.2" class="ltx_td ltx_align_right">7</td>
<td id="S7.T3.5.6.5.3" class="ltx_td ltx_align_right">6</td>
<td id="S7.T3.5.6.5.4" class="ltx_td ltx_align_right">7</td>
<td id="S7.T3.5.6.5.5" class="ltx_td ltx_align_right">8</td>
</tr>
<tr id="S7.T3.5.7.6" class="ltx_tr">
<td id="S7.T3.5.7.6.1" class="ltx_td ltx_align_left">Wrong Question, Correct Prediction</td>
<td id="S7.T3.5.7.6.2" class="ltx_td ltx_align_right">20</td>
<td id="S7.T3.5.7.6.3" class="ltx_td ltx_align_right">12</td>
<td id="S7.T3.5.7.6.4" class="ltx_td ltx_align_right">13</td>
<td id="S7.T3.5.7.6.5" class="ltx_td ltx_align_right">14</td>
</tr>
<tr id="S7.T3.5.8.7" class="ltx_tr">
<td id="S7.T3.5.8.7.1" class="ltx_td ltx_align_left">Wrong Question, Incorrect Prediction</td>
<td id="S7.T3.5.8.7.2" class="ltx_td ltx_align_right">0</td>
<td id="S7.T3.5.8.7.3" class="ltx_td ltx_align_right">8</td>
<td id="S7.T3.5.8.7.4" class="ltx_td ltx_align_right">7</td>
<td id="S7.T3.5.8.7.5" class="ltx_td ltx_align_right">8</td>
</tr>
<tr id="S7.T3.5.9.8" class="ltx_tr">
<td id="S7.T3.5.9.8.1" class="ltx_td ltx_align_left ltx_border_bb">Question Ambiguity</td>
<td id="S7.T3.5.9.8.2" class="ltx_td ltx_align_right ltx_border_bb">12</td>
<td id="S7.T3.5.9.8.3" class="ltx_td ltx_align_right ltx_border_bb">15</td>
<td id="S7.T3.5.9.8.4" class="ltx_td ltx_align_right ltx_border_bb">15</td>
<td id="S7.T3.5.9.8.5" class="ltx_td ltx_align_right ltx_border_bb">16</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S7.F4" class="ltx_figure">
<div id="S7.F4.2" class="ltx_block">
<figure id="S7.F4.sf1" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000545058.jpg" id="S7.F4.sf1.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S7.F4.sf1.4.2" class="ltx_text" style="font-size:90%;">What helps to tie hair? <span id="S7.F4.sf1.4.2.1" class="ltx_text ltx_font_bold">Small Object.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf2" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000092045.jpg" id="S7.F4.sf2.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S7.F4.sf2.4.2" class="ltx_text" style="font-size:90%;">What is a device for giving light, one consisting of an electric bulb together with its holder ? <span id="S7.F4.sf2.4.2.1" class="ltx_text ltx_font_bold">Insignificant Error.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf3" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000274633.jpg" id="S7.F4.sf3.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S7.F4.sf3.4.2" class="ltx_text" style="font-size:90%;">Which beverage is made of grapes? <span id="S7.F4.sf3.4.2.1" class="ltx_text ltx_font_bold">Wrong Object Prediction.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf4" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000057647.jpg" id="S7.F4.sf4.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S7.F4.sf4.4.2" class="ltx_text" style="font-size:90%;">What do you play with on a windy day? <span id="S7.F4.sf4.4.2.1" class="ltx_text ltx_font_bold">Inaccurate Ground Truth.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf5" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000555632.jpg" id="S7.F4.sf5.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S7.F4.sf5.4.2" class="ltx_text" style="font-size:90%;">What is used to direct the traffic? <span id="S7.F4.sf5.4.2.1" class="ltx_text ltx_font_bold">Inaccurate Prediction.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf6" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000572381.jpg" id="S7.F4.sf6.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf6.3.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S7.F4.sf6.4.2" class="ltx_text" style="font-size:90%;">What can I open to let fresh air into the room? <span id="S7.F4.sf6.4.2.1" class="ltx_text ltx_font_bold">Wrong Question, Correct Prediction.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf7" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000315065.jpg" id="S7.F4.sf7.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf7.3.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S7.F4.sf7.4.2" class="ltx_text" style="font-size:90%;">Where can I get supplies? <span id="S7.F4.sf7.4.2.1" class="ltx_text ltx_font_bold">Wrong Question, Incorrect Prediction.</span></span></figcaption>
</figure>
<figure id="S7.F4.sf8" class="ltx_figure ltx_align_center"><img src="/html/2309.16511/assets/errors/000000410350.jpg" id="S7.F4.sf8.g1" class="ltx_graphics ltx_img_landscape" width="141" height="94" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.sf8.3.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S7.F4.sf8.4.2" class="ltx_text" style="font-size:90%;">What can we wear? <span id="S7.F4.sf8.4.2.1" class="ltx_text ltx_font_bold">Question Ambiguity.</span></span></figcaption>
</figure>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F4.5.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S7.F4.6.2" class="ltx_text" style="font-size:90%;">Typical examples of error classes we observed during the analysis. Image captions include the associated questions followed by the error class labels and are provided. In the images, <span id="S7.F4.6.2.1" class="ltx_text" style="color:#FF0000;">predictions are red-colored</span> and <span id="S7.F4.6.2.2" class="ltx_text" style="color:#009900;">ground truth is green-colored</span>.</span></figcaption>
</figure>
<div id="S7.p1" class="ltx_para">
<p id="S7.p1.2" class="ltx_p">Before performing error analysis, we evaluated whether the errors made by three top-performing systems were similar or not. We took their predictions on the <em id="S7.p1.2.1" class="ltx_emph ltx_font_italic">private test</em> subset and then computed the Krippendorff’s <math id="S7.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S7.p1.1.m1.1a"><mi id="S7.p1.1.m1.1.1" xref="S7.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S7.p1.1.m1.1b"><ci id="S7.p1.1.m1.1.1.cmml" xref="S7.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.m1.1c">\alpha</annotation></semantics></math> coefficient similarly to Section <a href="#S5" title="5 Metrics and Baselines ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The coefficient value of <math id="S7.p1.2.m2.1" class="ltx_Math" alttext="0.77" display="inline"><semantics id="S7.p1.2.m2.1a"><mn id="S7.p1.2.m2.1.1" xref="S7.p1.2.m2.1.1.cmml">0.77</mn><annotation-xml encoding="MathML-Content" id="S7.p1.2.m2.1b"><cn type="float" id="S7.p1.2.m2.1.1.cmml" xref="S7.p1.2.m2.1.1">0.77</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.2.m2.1c">0.77</annotation></semantics></math> demonstrated a moderate agreement between responses of all three systems. As this indicates that all the three systems tend to produce similar correct and incorrect responses, we further analyzed the errors made by these systems and our crowdsourcing baseline as reported in Section <a href="#S6" title="6 Evaluation ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.2" class="ltx_p">First, we sampled the data instances from the private test set where <math id="S7.p2.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S7.p2.1.m1.1a"><mi id="S7.p2.1.m1.1.1" xref="S7.p2.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S7.p2.1.m1.1b"><ci id="S7.p2.1.m1.1.1.cmml" xref="S7.p2.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.m1.1c">\operatorname{IoU}</annotation></semantics></math> of humans and models was less than <math id="S7.p2.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S7.p2.2.m2.1a"><mn id="S7.p2.2.m2.1.1" xref="S7.p2.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S7.p2.2.m2.1b"><cn type="integer" id="S7.p2.2.m2.1.1.cmml" xref="S7.p2.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.m2.1c">80</annotation></semantics></math>, which resulted in 355 out of 4,504 instances (approximately 8%). <span id="S7.p2.2.1" class="ltx_text ltx_font_bold">This sample is heavy biased towards the most challenging instances</span> as all the models showed poor results on them; it is <em id="S7.p2.2.2" class="ltx_emph ltx_font_italic">not representative of the entire dataset</em>. Out of these 355, we sampled 100 instances and manually evaluated the quality of the obtained bounding boxes. This provided us with 100 judgements per method. We identified the following eight error classes as summarized in Table <a href="#S7.T3" title="Table 3 ‣ 7 Error Analysis ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figure <a href="#S7.F4" title="Figure 4 ‣ 7 Error Analysis ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p"><span id="S7.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Small Object.</span> The target object is very small, thus it was difficult to draw a bounding box precisely because of the low resolution.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.2" class="ltx_p"><span id="S7.I1.i2.p1.2.1" class="ltx_text ltx_font_bold">Insignificant Error.</span> The difference between prediction and ground truth is marginal (roughly, it means that <math id="S7.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S7.I1.i2.p1.1.m1.1a"><mi id="S7.I1.i2.p1.1.m1.1.1" xref="S7.I1.i2.p1.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.1.m1.1b"><ci id="S7.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I1.i2.p1.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.1.m1.1c">\operatorname{IoU}</annotation></semantics></math> was greater than <math id="S7.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="70" display="inline"><semantics id="S7.I1.i2.p1.2.m2.1a"><mn id="S7.I1.i2.p1.2.m2.1.1" xref="S7.I1.i2.p1.2.m2.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S7.I1.i2.p1.2.m2.1b"><cn type="integer" id="S7.I1.i2.p1.2.m2.1.1.cmml" xref="S7.I1.i2.p1.2.m2.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i2.p1.2.m2.1c">70</annotation></semantics></math>).</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p"><span id="S7.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Wrong Object Predicted.</span> The prediction is entirely incorrect, so <math id="S7.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\operatorname{IoU}" display="inline"><semantics id="S7.I1.i3.p1.1.m1.1a"><mi id="S7.I1.i3.p1.1.m1.1.1" xref="S7.I1.i3.p1.1.m1.1.1.cmml">IoU</mi><annotation-xml encoding="MathML-Content" id="S7.I1.i3.p1.1.m1.1b"><ci id="S7.I1.i3.p1.1.m1.1.1.cmml" xref="S7.I1.i3.p1.1.m1.1.1">IoU</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I1.i3.p1.1.m1.1c">\operatorname{IoU}</annotation></semantics></math> was zero.</p>
</div>
</li>
<li id="S7.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i4.p1" class="ltx_para">
<p id="S7.I1.i4.p1.1" class="ltx_p"><span id="S7.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Inaccurate Ground Truth.</span> The prediction is more accurate than ground truth.</p>
</div>
</li>
<li id="S7.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i5.p1" class="ltx_para">
<p id="S7.I1.i5.p1.1" class="ltx_p"><span id="S7.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Inaccurate Prediction.</span> The prediction is significantly less accurate than ground truth.</p>
</div>
</li>
<li id="S7.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i6.p1" class="ltx_para">
<p id="S7.I1.i6.p1.1" class="ltx_p"><span id="S7.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Wrong Question, Correct Prediction.</span> The ground truth does not answer the question but the corresponding prediction does.</p>
</div>
</li>
<li id="S7.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i7.p1" class="ltx_para">
<p id="S7.I1.i7.p1.1" class="ltx_p"><span id="S7.I1.i7.p1.1.1" class="ltx_text ltx_font_bold">Wrong Question, Incorrect Prediction.</span> Neither the ground truth nor the prediction answer the question.</p>
</div>
</li>
<li id="S7.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i8.p1" class="ltx_para">
<p id="S7.I1.i8.p1.1" class="ltx_p"><span id="S7.I1.i8.p1.1.1" class="ltx_text ltx_font_bold">Question Ambiguity.</span> The question might not have a single correct answer (for example, when there are several objects answering the same question). Also, we notice a few cases when the question is unclear or ambiguous (e.g., “What is he doing”).</p>
</div>
</li>
</ul>
</div>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.1" class="ltx_p">Having compared the predictions of the crowd annotators and top-performing machine learning systems, we noticed three facts. First, <em id="S7.p3.1.1" class="ltx_emph ltx_font_italic">it was especially difficult for crowd to draw a bounding box around small objects</em> (the number of such errors is much higher than for models). Second, <em id="S7.p3.1.2" class="ltx_emph ltx_font_italic">the models make a completely wrong prediction more often than the crowd</em>. However, even when the prediction is incorrect, the models still predict some object on the image and not just a random bounding box. Third, <em id="S7.p3.1.3" class="ltx_emph ltx_font_italic">a significant amount of crowd errors is caused by the wrong or ambiguous question</em>. In most of such cases the crowd gives a correct answer. This is inevitable in crowdsourced datasets even with rigorous quality control approach like we used. It is worth noting that this observation is only applicable to the cases where all four approaches fail to make a good prediction (only <math id="S7.p3.1.m1.1" class="ltx_Math" alttext="8\%" display="inline"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mn id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">8</mn><mo id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><csymbol cd="latexml" id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">8\%</annotation></semantics></math> of the data).</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In our grounding visual question answering task, the inputs consisted of an image and a question, with the output being the corresponding bounding box. While the top-performing systems showed remarkable improvement compared to the baselines, none of them surpassed non-expert annotators by a significant margin. We consider this fact important, as it indicates that our benchmark remains relevant until larger multi-modal models become accessible. The entire dataset, except for the images themselves, was generated through crowdsourcing on the Toloka platform, rendering it a valuable resource for the creation of demanding benchmarks. As a future work, we consider increasing the dataset size and adjusting our annotation pipeline to produce questions on different regions in the same image.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">We foresee the following potential <span id="S8.p2.1.1" class="ltx_text ltx_font_bold">downstream applications</span> of our dataset and derivative models beside the evaluation of machine learning models:</p>
<ul id="S8.I1" class="ltx_itemize">
<li id="S8.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i1.p1" class="ltx_para">
<p id="S8.I1.i1.p1.1" class="ltx_p"><span id="S8.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Visual Search.</span> With accurate bounding boxes, grounding VQA enables better understanding and recognition of objects in images, allowing users of e-commerce platforms to query products based on their appearance rather than relying only on text-based search.</p>
</div>
</li>
<li id="S8.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i2.p1" class="ltx_para">
<p id="S8.I1.i2.p1.1" class="ltx_p"><span id="S8.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Augmented Reality (AR).</span> Accurate bounding box annotation can help in integrating virtual objects into real-world scenes during AR applications. Grounding VQA aids in object recognition and aligning virtual content with real-world objects, and can assist in image annotation, making it easier to search images based on specific object queries.</p>
</div>
</li>
<li id="S8.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i3.p1" class="ltx_para">
<p id="S8.I1.i3.p1.1" class="ltx_p"><span id="S8.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Robotics.</span> For robots to interact with objects in their environment effectively, accurate object localization is crucial. grounding VQA can be utilized to identify and track objects, enabling robots to navigate, grasp, and manipulate objects in a more intelligent and precise manner.</p>
</div>
</li>
</ul>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">However, our dataset has the following <span id="S8.p3.1.1" class="ltx_text ltx_font_bold">limitations</span>:</p>
<ul id="S8.I2" class="ltx_itemize">
<li id="S8.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I2.i1.p1" class="ltx_para">
<p id="S8.I2.i1.p1.1" class="ltx_p"><span id="S8.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Dataset Bias.</span> We only consider images from MS COCO, which itself may contain biases related to gender and race. Additionally, the questions and selected objects chosen by annotators may introduce bias or have limited variability, potentially limiting the generalizability of models trained on this dataset.</p>
</div>
</li>
<li id="S8.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I2.i2.p1" class="ltx_para">
<p id="S8.I2.i2.p1.1" class="ltx_p"><span id="S8.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">English-Only Questions.</span> This dataset focuses solely on English questions. However, this narrow focus may restrict the ability of models to handle other languages and cultures.</p>
</div>
</li>
<li id="S8.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I2.i3.p1" class="ltx_para">
<p id="S8.I2.i3.p1.1" class="ltx_p"><span id="S8.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Real-World Applications.</span> Since there are no production-level applications of the proposed grounding VQA setup yet, future real-world applications may involve more complex questions that require deeper understanding, reasoning, and context awareness.</p>
</div>
</li>
</ul>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">We wish to highlight certain <span id="S8.p4.1.1" class="ltx_text ltx_font_bold">potential negative social impacts</span> of models trained on our dataset:</p>
</div>
<div id="S8.p5" class="ltx_para">
<ul id="S8.I3" class="ltx_itemize">
<li id="S8.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I3.i1.p1" class="ltx_para">
<p id="S8.I3.i1.p1.1" class="ltx_p"><span id="S8.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Reinforcing Bias.</span> Due to potential biases present in the dataset, models trained on it may inadvertently perpetuate societal inequalities and discrimination when deployed in real-world applications.</p>
</div>
</li>
<li id="S8.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I3.i2.p1" class="ltx_para">
<p id="S8.I3.i2.p1.1" class="ltx_p"><span id="S8.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Ethical Use of Models.</span> As grounding VQA models become more advanced, they can be exploited for malicious purposes, such as privacy invasion. It is crucial to establish proper safeguards and guidelines to prevent misuse and protect individuals’ rights and well-being.</p>
</div>
</li>
</ul>
</div>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements.</h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.1" class="ltx_p">This work would not have been possible without the collaborative efforts of people from different teams in Toloka. Individuals listed in each contribution role are arranged alphabetically based on their surnames. We thank Ekaterina Fedorenko, Natalia Fedorova, Ujwal Gadiraju, Valentina Mikhno, and Evgeniya Sukhodolskaya for helping in organizing our competition at WSDM Cup. We acknowledge the invaluable efforts of Oleg Pavlov, Mikhail Potalitsyn, and Rosmiyana Shekhovtsova, whose expertise was crucial for the annotation pipeline design. We express gratitude to Anastasia Egupova, Aleksei Gerasimenko, Timur Pevzner, Kuen Pham, Ekaterina Saenko, and Anna Stepanova for their help in raising awareness of the competition among the community, allowing to attract 48 participants from across the world. We are grateful to Egor Babkin, Dmitry Lekomtsev, and Andrei Voitovich for building the Web presence of our competition. We acknowledge the invaluable contribution of Tatiana Ignatova, Daria Kalakina, and Victoriya Vidma in navigating complex legal and financial aspects. Last but not least, we would like to thank the CodaLab and the WSDM Cup teams, especially Hady W. Lauw, and the competition participants, for making it a big success.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015</span>, pages 2425–2433. IEEE Computer Society, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Michael S. Bernstein, Greg Little, Robert C. Miller, Björn Hartmann, Mark S. Ackerman, David R. Karger, David Crowell, and Katrina Panovich.

</span>
<span class="ltx_bibblock">Soylent: A Word Processor with a Crowd Inside.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology</span>, UIST ’10, pages 313–322, New York, NY, USA, 2010. ACM.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Nicholas Carlini, Matthew Jagielski, Christopher A. Choquette-Choo, Daniel Paleka, Will Pearce, Hyrum Anderson, Andreas Terzis, Kurt Thomas, and Florian Tramèr.

</span>
<span class="ltx_bibblock">Poisoning Web-Scale Training Datasets is Practical, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chongyan Chen, Samreen Anjum, and Danna Gurari.

</span>
<span class="ltx_bibblock">Grounding Answers for Visual Questions Asked by Visually Impaired People.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span>, pages 19076–19085. IEEE, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao.

</span>
<span class="ltx_bibblock">Vision Transformer Adapter for Dense Predictions.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Shengyi Gao, Zhe Chen, Guo Chen, Wenhai Wang, and Tong Lu.

</span>
<span class="ltx_bibblock">Champion Solution for the WSDM2023 Toloka VQA Challenge, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</span>, pages 6325–6334. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning.

</span>
<span class="ltx_bibblock">GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</span>, pages 6700–6709. Computer Vision Foundation / IEEE, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick.

</span>
<span class="ltx_bibblock">CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017</span>, pages 1988–1997. IEEE Computer Society, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Shen Kai, Lingfei Wu, Siliang Tang, Yueting Zhuang, Zhen He, Zhuoye Ding, Yun Xiao, and Bo Long.

</span>
<span class="ltx_bibblock">Learning to Generate Visual Questions with Noisy Supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual</span>, pages 11604–11617, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg.

</span>
<span class="ltx_bibblock">ReferItGame: Referring to Objects in Photographs of Natural Scenes.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</span>, pages 787–798, Doha, Qatar, 2014. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Ross B. Girshick, Kaiming He, and Piotr Dollár.

</span>
<span class="ltx_bibblock">Panoptic Feature Pyramid Networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</span>, pages 6399–6408. Computer Vision Foundation / IEEE, 2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint</span>, abs/2304.02643, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Klaus Krippendorff.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Content Analysis: An Introduction to Its Methodology</span>.

</span>
<span class="ltx_bibblock">SAGE Publications, Inc, Thousand Oaks, CA, USA, 4th edition, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">ICML</span>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu.

</span>
<span class="ltx_bibblock">Open-vocabulary semantic segmentation with mask-adapted clip.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 7061–7070, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Computer Vision – ECCV 2014</span>, pages 740–755, Cham, Switzerland, 2014. Springer International Publishing.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.

</span>
<span class="ltx_bibblock">Kosmos-2: Grounding Multimodal Large Language Models to the World.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv</span>, abs/2306, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Yanyuan Qiao, Chaorui Deng, and Qi Wu.

</span>
<span class="ltx_bibblock">Referring Expression Comprehension: A Survey of Methods and Datasets.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Multimedia</span>, 23:4426–4440, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</span>, volume 139 of <span id="bib.bib20.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Zero-Shot Text-to-Image Generation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</span>, volume 139 of <span id="bib.bib21.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 8821–8831. PMLR, 2021.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.

</span>
<span class="ltx_bibblock">SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</span>, pages 3261–3275, 2019.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">You Only Learn One Representation: Unified Network for Multiple Tasks.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">arXiv preprint</span>, abs/2105.04206, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.

</span>
<span class="ltx_bibblock">OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</span>, volume 162 of <span id="bib.bib24.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 23318–23340. PMLR, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum.

</span>
<span class="ltx_bibblock">DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang.

</span>
<span class="ltx_bibblock">VLUE: A Multi-Task Multi-Dimension Benchmark for Evaluating Vision-Language Pre-training.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</span>, volume 162 of <span id="bib.bib26.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 27395–27411. PMLR, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai.

</span>
<span class="ltx_bibblock">Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</span>, pages 16783–16794. IEEE, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yuke Zhu, Oliver Groth, Michael S. Bernstein, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Visual7W: Grounded Question Answering in Images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016</span>, pages 4995–5004. IEEE Computer Society, 2016.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Competition Results at WSDM Cup 2023</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">The competition website was located at <a target="_blank" href="https://toloka.ai/challenges/wsdm2023/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://toloka.ai/challenges/wsdm2023/</a>, while the leaderboard and scoring were performed on the CodaLab platform: <a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/7434" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://codalab.lisn.upsaclay.fr/competitions/7434</a>. Before the start of the competition, all the parts of our dataset were frozen and did not change during the competition.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">Our competition had three key phases: the practice phase, the evaluation phase, and the reproduction phase (Table <a href="#A1.T4" title="Table 4 ‣ Appendix A Competition Results at WSDM Cup 2023 ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In September, we started the <em id="A1.p2.1.1" class="ltx_emph ltx_font_italic">practice</em> phase to let the contestants get used to the task and training data, including the ground truth data. Then, we started the <em id="A1.p2.1.2" class="ltx_emph ltx_font_italic">evaluation</em> phase using the public test dataset without ground truth labels. The contestants had to submit their predictions to the competition platform, which resulted in leaderboard updates. Finally, for the sake of reproducibility, soon after the end of the evaluation phase, we started the <em id="A1.p2.1.3" class="ltx_emph ltx_font_italic">reproduction</em> phase. In this phase, we asked the contestants to provide their solution as a container image. We ran their code to obtain answers for the private test dataset to determine the winners.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">Table <a href="#A1.T5" title="Table 5 ‣ Appendix A Competition Results at WSDM Cup 2023 ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the competition results. Even though the participants managed to improve dramatically upon our baselines, none of the participating systems outperformed our crowdsourcing baseline.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="A1.T4.3.2" class="ltx_text" style="font-size:90%;">Complete timeline of our competition at WSDM Cup 2023</span></figcaption>
<table id="A1.T4.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.4.1.1" class="ltx_tr">
<th id="A1.T4.4.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A1.T4.4.1.1.1.1" class="ltx_text ltx_font_bold">Event</span></th>
<th id="A1.T4.4.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="A1.T4.4.1.1.2.1" class="ltx_text ltx_font_bold">Date</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.4.2.1" class="ltx_tr">
<th id="A1.T4.4.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Practice Starts</th>
<td id="A1.T4.4.2.1.2" class="ltx_td ltx_align_right ltx_border_t">September 16, 2022</td>
</tr>
<tr id="A1.T4.4.3.2" class="ltx_tr">
<th id="A1.T4.4.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Evaluation Starts</th>
<td id="A1.T4.4.3.2.2" class="ltx_td ltx_align_right">September 30, 2022</td>
</tr>
<tr id="A1.T4.4.4.3" class="ltx_tr">
<th id="A1.T4.4.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Evaluation Ends</th>
<td id="A1.T4.4.4.3.2" class="ltx_td ltx_align_right">December 16, 2022</td>
</tr>
<tr id="A1.T4.4.5.4" class="ltx_tr">
<th id="A1.T4.4.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Reproduction Starts</th>
<td id="A1.T4.4.5.4.2" class="ltx_td ltx_align_right">December 19, 2022</td>
</tr>
<tr id="A1.T4.4.6.5" class="ltx_tr">
<th id="A1.T4.4.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Reproduction Ends</th>
<td id="A1.T4.4.6.5.2" class="ltx_td ltx_align_right">January 16, 2023</td>
</tr>
<tr id="A1.T4.4.7.6" class="ltx_tr">
<th id="A1.T4.4.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Post-Competition Starts</th>
<td id="A1.T4.4.7.6.2" class="ltx_td ltx_align_right">January 16, 2023</td>
</tr>
<tr id="A1.T4.4.8.7" class="ltx_tr">
<th id="A1.T4.4.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">WSDM Cup Workshops</th>
<td id="A1.T4.4.8.7.2" class="ltx_td ltx_align_right ltx_border_bb">March 3, 2023</td>
</tr>
</tbody>
</table>
</figure>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.18.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="A1.T5.19.2" class="ltx_text" style="font-size:90%;">Baselines and final team standings on the <em id="A1.T5.19.2.1" class="ltx_emph ltx_font_italic">private test</em> subset, obtained at the reproduction phase of our competition; for visual convenience, we multiplied the IoU values by 100; out of 48 participants, only 9 submitted their code during the reproduction phase. Baseline methods did not participate in the competition, their places are denoted as “—”.</span></figcaption>
<table id="A1.T5.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.15.16.1" class="ltx_tr">
<th id="A1.T5.15.16.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.15.16.1.1.1" class="ltx_text ltx_font_bold">Place</span></th>
<th id="A1.T5.15.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.15.16.1.2.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="A1.T5.15.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A1.T5.15.16.1.3.1" class="ltx_text ltx_font_bold">IoU</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.1.1" class="ltx_tr">
<td id="A1.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T5.1.1.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A1.T5.1.1.3.1" class="ltx_text" style="color:#808080;">Crowdsourcing</span></td>
<td id="A1.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="A1.T5.1.1.1.m1.1" class="ltx_Math" alttext="87.154" display="inline"><semantics id="A1.T5.1.1.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.1.1.1.m1.1.1" xref="A1.T5.1.1.1.m1.1.1.cmml">87.154</mn><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.m1.1b"><cn type="float" id="A1.T5.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.m1.1.1">87.154</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.m1.1c">87.154</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.2.2" class="ltx_tr">
<td id="A1.T5.2.2.2" class="ltx_td ltx_align_center">1</td>
<td id="A1.T5.2.2.3" class="ltx_td ltx_align_center"><span id="A1.T5.2.2.3.1" class="ltx_text ltx_font_typewriter">wztxy89</span></td>
<td id="A1.T5.2.2.1" class="ltx_td ltx_align_center"><math id="A1.T5.2.2.1.m1.1" class="ltx_Math" alttext="\mathbf{76.347}" display="inline"><semantics id="A1.T5.2.2.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A1.T5.2.2.1.m1.1.1" xref="A1.T5.2.2.1.m1.1.1.cmml">76.347</mn><annotation-xml encoding="MathML-Content" id="A1.T5.2.2.1.m1.1b"><cn type="float" id="A1.T5.2.2.1.m1.1.1.cmml" xref="A1.T5.2.2.1.m1.1.1">76.347</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.2.1.m1.1c">\mathbf{76.347}</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.3.3" class="ltx_tr">
<td id="A1.T5.3.3.2" class="ltx_td ltx_align_center">2</td>
<td id="A1.T5.3.3.3" class="ltx_td ltx_align_center"><span id="A1.T5.3.3.3.1" class="ltx_text ltx_font_typewriter">jinx, Zhouyang_Chi</span></td>
<td id="A1.T5.3.3.1" class="ltx_td ltx_align_center"><math id="A1.T5.3.3.1.m1.1" class="ltx_Math" alttext="\mathbf{76.342}" display="inline"><semantics id="A1.T5.3.3.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A1.T5.3.3.1.m1.1.1" xref="A1.T5.3.3.1.m1.1.1.cmml">76.342</mn><annotation-xml encoding="MathML-Content" id="A1.T5.3.3.1.m1.1b"><cn type="float" id="A1.T5.3.3.1.m1.1.1.cmml" xref="A1.T5.3.3.1.m1.1.1">76.342</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.3.3.1.m1.1c">\mathbf{76.342}</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.4.4" class="ltx_tr">
<td id="A1.T5.4.4.2" class="ltx_td ltx_align_center">3</td>
<td id="A1.T5.4.4.3" class="ltx_td ltx_align_center"><span id="A1.T5.4.4.3.1" class="ltx_text ltx_font_typewriter">komleva.ep</span></td>
<td id="A1.T5.4.4.1" class="ltx_td ltx_align_center"><math id="A1.T5.4.4.1.m1.1" class="ltx_Math" alttext="\mathbf{75.591}" display="inline"><semantics id="A1.T5.4.4.1.m1.1a"><mn class="ltx_mathvariant_bold" mathvariant="bold" id="A1.T5.4.4.1.m1.1.1" xref="A1.T5.4.4.1.m1.1.1.cmml">75.591</mn><annotation-xml encoding="MathML-Content" id="A1.T5.4.4.1.m1.1b"><cn type="float" id="A1.T5.4.4.1.m1.1.1.cmml" xref="A1.T5.4.4.1.m1.1.1">75.591</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.4.4.1.m1.1c">\mathbf{75.591}</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.5.5" class="ltx_tr">
<td id="A1.T5.5.5.2" class="ltx_td ltx_align_center">4</td>
<td id="A1.T5.5.5.3" class="ltx_td ltx_align_center"><span id="A1.T5.5.5.3.1" class="ltx_text ltx_font_typewriter">xexanoth</span></td>
<td id="A1.T5.5.5.1" class="ltx_td ltx_align_center"><math id="A1.T5.5.5.1.m1.1" class="ltx_Math" alttext="74.667" display="inline"><semantics id="A1.T5.5.5.1.m1.1a"><mn id="A1.T5.5.5.1.m1.1.1" xref="A1.T5.5.5.1.m1.1.1.cmml">74.667</mn><annotation-xml encoding="MathML-Content" id="A1.T5.5.5.1.m1.1b"><cn type="float" id="A1.T5.5.5.1.m1.1.1.cmml" xref="A1.T5.5.5.1.m1.1.1">74.667</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.5.5.1.m1.1c">74.667</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.6.6" class="ltx_tr">
<td id="A1.T5.6.6.2" class="ltx_td ltx_align_center">5</td>
<td id="A1.T5.6.6.3" class="ltx_td ltx_align_center"><span id="A1.T5.6.6.3.1" class="ltx_text ltx_font_typewriter">Man_of_the_year</span></td>
<td id="A1.T5.6.6.1" class="ltx_td ltx_align_center"><math id="A1.T5.6.6.1.m1.1" class="ltx_Math" alttext="72.768" display="inline"><semantics id="A1.T5.6.6.1.m1.1a"><mn id="A1.T5.6.6.1.m1.1.1" xref="A1.T5.6.6.1.m1.1.1.cmml">72.768</mn><annotation-xml encoding="MathML-Content" id="A1.T5.6.6.1.m1.1b"><cn type="float" id="A1.T5.6.6.1.m1.1.1.cmml" xref="A1.T5.6.6.1.m1.1.1">72.768</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.6.6.1.m1.1c">72.768</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.7.7" class="ltx_tr">
<td id="A1.T5.7.7.2" class="ltx_td ltx_align_center">6</td>
<td id="A1.T5.7.7.3" class="ltx_td ltx_align_center"><span id="A1.T5.7.7.3.1" class="ltx_text ltx_font_typewriter">Haoyu_Zhang, KhylonWong</span></td>
<td id="A1.T5.7.7.1" class="ltx_td ltx_align_center"><math id="A1.T5.7.7.1.m1.1" class="ltx_Math" alttext="71.998" display="inline"><semantics id="A1.T5.7.7.1.m1.1a"><mn id="A1.T5.7.7.1.m1.1.1" xref="A1.T5.7.7.1.m1.1.1.cmml">71.998</mn><annotation-xml encoding="MathML-Content" id="A1.T5.7.7.1.m1.1b"><cn type="float" id="A1.T5.7.7.1.m1.1.1.cmml" xref="A1.T5.7.7.1.m1.1.1">71.998</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.7.7.1.m1.1c">71.998</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.8.8" class="ltx_tr">
<td id="A1.T5.8.8.2" class="ltx_td ltx_align_center">7</td>
<td id="A1.T5.8.8.3" class="ltx_td ltx_align_center"><span id="A1.T5.8.8.3.1" class="ltx_text ltx_font_typewriter">nika-li</span></td>
<td id="A1.T5.8.8.1" class="ltx_td ltx_align_center"><math id="A1.T5.8.8.1.m1.1" class="ltx_Math" alttext="70.525" display="inline"><semantics id="A1.T5.8.8.1.m1.1a"><mn id="A1.T5.8.8.1.m1.1.1" xref="A1.T5.8.8.1.m1.1.1.cmml">70.525</mn><annotation-xml encoding="MathML-Content" id="A1.T5.8.8.1.m1.1b"><cn type="float" id="A1.T5.8.8.1.m1.1.1.cmml" xref="A1.T5.8.8.1.m1.1.1">70.525</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.8.8.1.m1.1c">70.525</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.9.9" class="ltx_tr">
<td id="A1.T5.9.9.2" class="ltx_td ltx_align_center">8</td>
<td id="A1.T5.9.9.3" class="ltx_td ltx_align_center"><span id="A1.T5.9.9.3.1" class="ltx_text ltx_font_typewriter">blinoff</span></td>
<td id="A1.T5.9.9.1" class="ltx_td ltx_align_center"><math id="A1.T5.9.9.1.m1.1" class="ltx_Math" alttext="62.037" display="inline"><semantics id="A1.T5.9.9.1.m1.1a"><mn id="A1.T5.9.9.1.m1.1.1" xref="A1.T5.9.9.1.m1.1.1.cmml">62.037</mn><annotation-xml encoding="MathML-Content" id="A1.T5.9.9.1.m1.1b"><cn type="float" id="A1.T5.9.9.1.m1.1.1.cmml" xref="A1.T5.9.9.1.m1.1.1">62.037</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.9.9.1.m1.1c">62.037</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.10.10" class="ltx_tr">
<td id="A1.T5.10.10.2" class="ltx_td ltx_align_center">9</td>
<td id="A1.T5.10.10.3" class="ltx_td ltx_align_center"><span id="A1.T5.10.10.3.1" class="ltx_text ltx_font_typewriter">Ndhuynh</span></td>
<td id="A1.T5.10.10.1" class="ltx_td ltx_align_center"><math id="A1.T5.10.10.1.m1.1" class="ltx_Math" alttext="61.247" display="inline"><semantics id="A1.T5.10.10.1.m1.1a"><mn id="A1.T5.10.10.1.m1.1.1" xref="A1.T5.10.10.1.m1.1.1.cmml">61.247</mn><annotation-xml encoding="MathML-Content" id="A1.T5.10.10.1.m1.1b"><cn type="float" id="A1.T5.10.10.1.m1.1.1.cmml" xref="A1.T5.10.10.1.m1.1.1">61.247</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.10.10.1.m1.1c">61.247</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.11.11" class="ltx_tr">
<td id="A1.T5.11.11.2" class="ltx_td ltx_align_center"><span id="A1.T5.11.11.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.11.11.3" class="ltx_td ltx_align_center"><span id="A1.T5.11.11.3.1" class="ltx_text" style="color:#808080;">OFA + SAM</span></td>
<td id="A1.T5.11.11.1" class="ltx_td ltx_align_center"><math id="A1.T5.11.11.1.m1.1" class="ltx_Math" alttext="44.851" display="inline"><semantics id="A1.T5.11.11.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.11.11.1.m1.1.1" xref="A1.T5.11.11.1.m1.1.1.cmml">44.851</mn><annotation-xml encoding="MathML-Content" id="A1.T5.11.11.1.m1.1b"><cn type="float" id="A1.T5.11.11.1.m1.1.1.cmml" xref="A1.T5.11.11.1.m1.1.1">44.851</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.11.11.1.m1.1c">44.851</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.12.12" class="ltx_tr">
<td id="A1.T5.12.12.2" class="ltx_td ltx_align_center"><span id="A1.T5.12.12.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.12.12.3" class="ltx_td ltx_align_center"><span id="A1.T5.12.12.3.1" class="ltx_text" style="color:#808080;">OFA + SAM (without Image)</span></td>
<td id="A1.T5.12.12.1" class="ltx_td ltx_align_center"><math id="A1.T5.12.12.1.m1.1" class="ltx_Math" alttext="39.075" display="inline"><semantics id="A1.T5.12.12.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.12.12.1.m1.1.1" xref="A1.T5.12.12.1.m1.1.1.cmml">39.075</mn><annotation-xml encoding="MathML-Content" id="A1.T5.12.12.1.m1.1b"><cn type="float" id="A1.T5.12.12.1.m1.1.1.cmml" xref="A1.T5.12.12.1.m1.1.1">39.075</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.12.12.1.m1.1c">39.075</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.13.13" class="ltx_tr">
<td id="A1.T5.13.13.2" class="ltx_td ltx_align_center"><span id="A1.T5.13.13.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.13.13.3" class="ltx_td ltx_align_center"><span id="A1.T5.13.13.3.1" class="ltx_text" style="color:#808080;">OVSeg + SAM</span></td>
<td id="A1.T5.13.13.1" class="ltx_td ltx_align_center"><math id="A1.T5.13.13.1.m1.1" class="ltx_Math" alttext="35.073" display="inline"><semantics id="A1.T5.13.13.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.13.13.1.m1.1.1" xref="A1.T5.13.13.1.m1.1.1.cmml">35.073</mn><annotation-xml encoding="MathML-Content" id="A1.T5.13.13.1.m1.1b"><cn type="float" id="A1.T5.13.13.1.m1.1.1.cmml" xref="A1.T5.13.13.1.m1.1.1">35.073</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.13.13.1.m1.1c">35.073</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.14.14" class="ltx_tr">
<td id="A1.T5.14.14.2" class="ltx_td ltx_align_center"><span id="A1.T5.14.14.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.14.14.3" class="ltx_td ltx_align_center"><span id="A1.T5.14.14.3.1" class="ltx_text" style="color:#808080;">Kosmos-2</span></td>
<td id="A1.T5.14.14.1" class="ltx_td ltx_align_center"><math id="A1.T5.14.14.1.m1.1" class="ltx_Math" alttext="22.571" display="inline"><semantics id="A1.T5.14.14.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.14.14.1.m1.1.1" xref="A1.T5.14.14.1.m1.1.1.cmml">22.571</mn><annotation-xml encoding="MathML-Content" id="A1.T5.14.14.1.m1.1b"><cn type="float" id="A1.T5.14.14.1.m1.1.1.cmml" xref="A1.T5.14.14.1.m1.1.1">22.571</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.14.14.1.m1.1c">22.571</annotation></semantics></math></td>
</tr>
<tr id="A1.T5.15.15" class="ltx_tr">
<td id="A1.T5.15.15.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T5.15.15.2.1" class="ltx_text" style="color:#808080;">—</span></td>
<td id="A1.T5.15.15.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A1.T5.15.15.3.1" class="ltx_text" style="color:#808080;">YOLOR + CLIP</span></td>
<td id="A1.T5.15.15.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="A1.T5.15.15.1.m1.1" class="ltx_Math" alttext="21.292" display="inline"><semantics id="A1.T5.15.15.1.m1.1a"><mn mathcolor="#808080" id="A1.T5.15.15.1.m1.1.1" xref="A1.T5.15.15.1.m1.1.1.cmml">21.292</mn><annotation-xml encoding="MathML-Content" id="A1.T5.15.15.1.m1.1b"><cn type="float" id="A1.T5.15.15.1.m1.1.1.cmml" xref="A1.T5.15.15.1.m1.1.1">21.292</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.15.15.1.m1.1c">21.292</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Annotator Instructions and Interfaces</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Our annotation pipeline consisted of four phases, each corresponding to an annotation project: “Find an Interesting Object” for initial bounding box selection of objects in images, “Verify Object Selection” to confirm if the selected objects met the requirements, “Ask a Question about the Object” for generating questions about the selected objects, and “Verify Questions About Objects” to validate if the written questions met the requirements. We provide a screenshot of our language test in Figure <a href="#A2.F5" title="Figure 5 ‣ Appendix B Annotator Instructions and Interfaces ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and screenshots of our main annotation tasks in Figure <a href="#A2.F6" title="Figure 6 ‣ Appendix B Annotator Instructions and Interfaces ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, <a href="#A2.F7" title="Figure 7 ‣ Appendix B Annotator Instructions and Interfaces ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, <a href="#A2.F8" title="Figure 8 ‣ Appendix B Annotator Instructions and Interfaces ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, and <a href="#A2.F9" title="Figure 9 ‣ Appendix B Annotator Instructions and Interfaces ‣ Toloka Visual Question Answering Benchmark" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="A2.F5" class="ltx_figure"><img src="/html/2309.16511/assets/interface/language-test.png" id="A2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="A2.F5.3.2" class="ltx_text" style="font-size:90%;">Screenshot of the interface of the language test assignment.</span></figcaption>
</figure>
<figure id="A2.F6" class="ltx_figure"><img src="/html/2309.16511/assets/interface/find-object.png" id="A2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="244" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="A2.F6.3.2" class="ltx_text" style="font-size:90%;">Screenshot of the “Find an Interesting Object” annotation interface.</span></figcaption>
</figure>
<figure id="A2.F7" class="ltx_figure"><img src="/html/2309.16511/assets/interface/verify-object.png" id="A2.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="256" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="A2.F7.3.2" class="ltx_text" style="font-size:90%;">Screenshot of the “Verify Object Selection” annotation interface.</span></figcaption>
</figure>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2309.16511/assets/interface/ask-question.png" id="A2.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="220" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="A2.F8.3.2" class="ltx_text" style="font-size:90%;">Screenshot of the “Ask a Question about the Object” annotation interface.</span></figcaption>
</figure>
<figure id="A2.F9" class="ltx_figure"><img src="/html/2309.16511/assets/interface/verify-question.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="228" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="A2.F9.3.2" class="ltx_text" style="font-size:90%;">Screenshot of the “Verify Questions About Objects” annotation interface.</span></figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.16510" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.16511" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.16511">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.16511" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.16512" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 03:38:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
