<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems</title>
<!--Generated on Mon Sep 23 17:26:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.15240v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S1" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S2" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S2.SS1" title="In 2 Related Works ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Evaluation of Memory-Augmented Dialogue Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S2.SS2" title="In 2 Related Works ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Evaluation of Emotional Support DS</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MemBench</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS1" title="In 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS2" title="In 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Contruction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS3" title="In 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Task Definition</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS3.SSS1" title="In 3.3 Task Definition ‣ 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Memory Recalling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS3.SSS2" title="In 3.3 Task Definition ‣ 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Memory Recognition and Response Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.SS3.SSS3" title="In 3.3 Task Definition ‣ 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Intimacy and Memory Usage</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS1" title="In 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Memory Recalling Task</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS1.SSS1" title="In 4.1 Memory Recalling Task ‣ 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Embedding Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS1.SSS2" title="In 4.1 Memory Recalling Task ‣ 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS2" title="In 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Memory Recognition and Response Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS2.SSS1" title="In 4.2 Memory Recognition and Response Generation ‣ 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.SS3" title="In 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.SS1" title="In 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Memory Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.SS2" title="In 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Memory Recognition and Response Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.SS3" title="In 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Intimacy and Memory Usage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.SS4" title="In 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Automatic Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S6" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A1" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Definition of Scenes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A2" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Memory recalling criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A3" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Details for human evaluation of scoring criteria</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A4" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Automatic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A5" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Correlation of human evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A6" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Example of LLM results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A7" title="In MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Prompts</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems </h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junqing He<sup class="ltx_sup" id="1.1.1">1</sup>, Liang Zhu<sup class="ltx_sup" id="2.2.2">2</sup>, Qi Wei<sup class="ltx_sup" id="3.3.3">1</sup>, Rui Wang<sup class="ltx_sup" id="4.4.4">1</sup>, Jiaxing Zhang<sup class="ltx_sup" id="5.5.5">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="6.6.6">1</sup>International Digital Economy Academy, Shenzhen, China 
<br class="ltx_break"/><sup class="ltx_sup" id="7.7.7">2</sup>Southern University of Science and Technology, Shenzhen, China 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="8.8.8">{hejunqing,wangrui}@idea.edu.cn</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="9.1">Long-term memory is so important for chatbots and dialogue systems (DS) that researchers have developed numerous memory-augmented DS. However, their evaluation methods are different from the real situation in human conversation. They only measured the accuracy of factual information or the perplexity of generated responses given a query, which hardly reflected their performance. Moreover, they only consider passive memory retrieval based on similarity, neglecting diverse memory-recalling paradigms in humans, e.g. emotions and surroundings. To bridge the gap, we construct a novel benchmark covering various memory-recalling paradigms based on cognitive science and psychology theory. The <span class="ltx_text ltx_font_bold" id="9.1.1">Mem</span>ory <span class="ltx_text ltx_font_bold" id="9.1.2">Bench</span>mark (<span class="ltx_text ltx_font_bold" id="9.1.3">MemBench</span>) contains two tasks according to the two-phrase theory in cognitive science: memory retrieval, memory recognition and injection. The benchmark considers both passive and proactive memory recalling based on meta information for the first time. In addition, novel scoring aspects are proposed to comprehensively measure the generated responses. Results from the strongest embedding models and LLMs on MemBench show that there is plenty of room for improvement in existing dialogue systems. Extensive experiments also reveal the correlation between memory injection and emotion supporting (ES) skillfulness, and intimacy. Our code and dataset will be released.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Junqing He<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1</sup>, Liang Zhu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">2</sup>, Qi Wei<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">1</sup>, Rui Wang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">1</sup>, Jiaxing Zhang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5">1</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1">1</sup>International Digital Economy Academy, Shenzhen, China</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">2</sup>Southern University of Science and Technology, Shenzhen, China</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.4.4.1.1">{hejunqing,wangrui}@idea.edu.cn</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Powerful Large Language Models (LLM) and dialogue systems (DS, or Chatbot) such as LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib36" title="">2023</a>)</cite>, Claude <cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib2" title="">2024</a>)</cite>, GPT4 <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib1" title="">2023</a>)</cite> and ChatGPT <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib27" title="">2022</a>)</cite>, have demonstrated a remarkable ability to understand and generate human-like responses <cite class="ltx_cite ltx_citemacro_cite">Bubeck et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib6" title="">2023</a>)</cite>. Despite the notable capabilities of LLMs, their lack of long-term memory, an essential aspect of human-like communication, hinders their application in scenarios requiring sustained interactions like personal companionship, psychological counselling, and secretarial tasks <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Therefore, researchers built diverse Memory-Augmented Dialogue Systems (MADS) that can maintain contextual understanding, and ensure meaningful interactions over time <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib42" title="">2024</a>); Hou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib18" title="">2024</a>); Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>)</cite>. These MADS extract information from dialogue histories as memory and retrieve memories based on semantic similarities <cite class="ltx_cite ltx_citemacro_cite">Hou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib18" title="">2024</a>); Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>)</cite>. However, current MADS are evaluated on language modelling (LM) and question answering (QA) tasks instead of multi-turn dialogues, only covering perplexity and factual accuracy <cite class="ltx_cite ltx_citemacro_cite">Packer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib28" title="">2024</a>); Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>)</cite>, which is far from the real scenario. Additionally, existing MADS only consider textual similarity in memory-recalling procedures, which is different from the mechanism of human memory.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Human memory can be recalled by being in the same state, where “state” refers to an individual’s surroundings, as well as their mental and physical state at the time of memory construction <cite class="ltx_cite ltx_citemacro_cite">Weissenborn and Duka (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib39" title="">2000</a>)</cite>. Therefore, emotions and other states could also trigger memory recall. Secondly, the mechanism of human memory recall, according to the two-stage theory, contains a generation (or search) process followed by a recognition (or decision) process <cite class="ltx_cite ltx_citemacro_cite">Bahrick (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib5" title="">1970</a>); Watkins and Gardiner (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib38" title="">1979</a>)</cite>, while existing evaluation of MADS ignores inspections on the recognition step. Moreover, long-term memory is used as common ground between individuals for conversation <cite class="ltx_cite ltx_citemacro_cite">Horton (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib17" title="">2005</a>); McKinley et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib25" title="">2017</a>); Sarah and Melissa (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib34" title="">2016</a>)</cite>, but also as the core component for emotion regulation <cite class="ltx_cite ltx_citemacro_cite">Engen and Anderson (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib11" title="">2018</a>)</cite>, e.g. recalling a happy memory can repair a negative mood state <cite class="ltx_cite ltx_citemacro_cite">Arditte Hall et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib3" title="">2018</a>)</cite>. Current MADS only consider the use for common ground but neglect emotional support that needs proactive memory recalling.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="533" id="S1.F1.g1" src="extracted/5874076/pics/intro_pic.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Memory Augmented Dialogue System with Emotion Support based on two-stage theory.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To bridge the gap between the real-world and existing evaluation of MADS, we proposed to imitate the human memory recall procedure and assess the performance of most powerful LLMs in human-centred aspects with our <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">Mem</span>ory <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">Bench</span>mark following cognition science and psychology theories, called <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">MemBench</span>. The constructed memories in MemBench include states such as emotions, scenes and other cues. The two-phrase theory is adopted and evaluated separately in MemBench. The dataset contains two kinds of recall mechanisms: Passive recall, searching memories when the user mentions a specific event or first, which provides common ground in conversation; and Proactive recall, retrieving memories based on the emotion of the user or scene and psychological strategies, to achieve interpersonal emotional regulation. Since emotional intelligence (EI, or quotient, EQ) contains the capability to recognise, understand, and manage one’s own emotions and others’ emotions <cite class="ltx_cite ltx_citemacro_cite">Mayer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib24" title="">2016</a>); Goleman and Boyatzis (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib14" title="">2017</a>)</cite>, the dialogues in the proposed benchmark are constructed with human refinement with high EI. We also designed a set of guidelines based on emotional supporting (ES) theories <cite class="ltx_cite ltx_citemacro_cite">Austin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib4" title="">2018</a>); Hill (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib16" title="">2020</a>)</cite> to measure the ES skills of LLMs. The benchmark highlights the evaluation of the retrieval accuracy, memory recognition &amp; injection ability, and comprehensive assessment of generated responses, e.g. language style, ES skillfulness and intimacy. The intimacy is proposed to measure how a chatbot or LLM resembles a close friend for the first time. After extensive experiments on LLMs, we discovered memory improves intimacy between MADS and users and the performance of the strongest LLMs is far from human. It is the first multi-recall and multi-stage memory-aware emotional dialogue generation benchmark.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contribution is three-fold:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We construct a delicate memory-injected dialogue benchmark, called MemBench. It is the first multi-recall multi-stage memory augmented dialogue benchmark based on cognitive science theory. The dataset incorporates emotional support into response generation based on proactive memory recall according to psychology laws. The benchmark provides valuable insights that similarity-based MADS are imperfect solutions for proactive recall situations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">To our knowledge, it is the first work that introduces a unique evaluation of diverse rating aspects with clear requirements for each point, containing memory injection, ES skill proficiency, intimacy et al. We also discover the relation between memory injection, ES skillfulness and Intimacy in human evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We conduct extensive experiments on the strongest LLMs and embedding models. Results show that LLMs can perform higher naturalness and style coherence with careful prompting but lag far behind humans when introducing memory into conversations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Evaluation of Memory-Augmented Dialogue Systems</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Many remarkable MADS have been proposed to build virtual agents and other applications <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib37" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib22" title="">2023</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib42" title="">2024</a>); Hou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib18" title="">2024</a>); Packer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib28" title="">2024</a>)</cite>. Theses MADS were evaluated in two approaches. <cite class="ltx_cite ltx_citemacro_citeauthor"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">Zhong et al.</a></cite> proposed to summarise events with emotion in dialogues as long-term memory and employed the Ebbinghaus Forgetting Curve theory <cite class="ltx_cite ltx_citemacro_cite">Ebbinghaus (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib9" title="">1885</a>)</cite>. Given the memory pool and a dialogue context, the retrieval accuracy, response correctness and contextual coherence are scored, representing the first assessment method <cite class="ltx_cite ltx_citemacro_cite">Zhong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib44" title="">2023</a>); Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib22" title="">2023</a>)</cite>. The test set only covers the passive memory recall triggered by users, neglecting the situation that the system proactively recalls memory required by dialogue strategies for emotional regulation. What’s more, emotional support was not considered in the scoring dimensions.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The other method leverages long-context language modelling (LM) tasks to measure memory ability, using novels and academic papers datasets like PG22 <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib37" title="">2023</a>)</cite> and Arxiv <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib13" title="">2020</a>)</cite> dataset. They are different from the real conversational scenario and only measure language perplexity and classification accuracy.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Evaluation of Emotional Support DS</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">As emotional support is an essential function of conversation, DS with empathy were rapidly developed and researched, including ED <cite class="ltx_cite ltx_citemacro_cite">Rashkin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib31" title="">2019</a>)</cite>, ESC <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib23" title="">2021</a>)</cite>, and others <cite class="ltx_cite ltx_citemacro_cite">Sabour et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib33" title="">2022</a>); Tian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib35" title="">2022</a>); Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib45" title="">2018</a>)</cite>. They commonly score Empathy, Relevance and Fluency for each response. We craft Emotional Improvement, Coherence and Naturalness based on these aspects. <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib23" title="">2021</a>)</cite> uses Identification, Comforting and Suggestion in extra to measure the emotional regulation steps in advice-seeking scenarios. These metrics are not suitable for all daily dialogues and are merged into one aspect: ES Skill Proficiency. We gauge this aspect according to hand-crafted well-written guidance based on psychological theories. However, the metrics above have no access to long-term memory. Therefore, we add the Memory-injection Ability as a new aspect. We also use intimacy as a subjective comprehensive evaluation to assess how much a conversational AI resembles a close friend.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Another difference from metrics in previous studies lies in the scoring standard, which was at a Likert scale while the requirement of each point in our metrics is strictly listed to reduce ambiguity. For instance, Naturalness contains 3 sub-aspects corresponding to 3 points, where each point can be rated from 0 to 1 and summed up to 3 at most. The difference between MemBench and previous works is listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S2.T1" title="Table 1 ‣ 2.2 Evaluation of Emotional Support DS ‣ 2 Related Works ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1">Metrics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2">ED</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3">ESC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4">Ours</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.1.1">Fluency/Naturalness</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.2.1">Empathy</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2.2">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.3.2.4">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.3.1">Memory-Injection</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.3.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.3.3">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.4.3.4">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.4.1">ES Skills</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.4.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.4.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.5.4.4">✓</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.6.5.1">Overall/Intimacy</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.1.6.5.2">✗</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.1.6.5.3">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S2.T1.1.6.5.4">✓</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Evaluation Aspects of responses in representative research. ED is short for EmpatheticDialogues <cite class="ltx_cite ltx_citemacro_cite">Rashkin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib31" title="">2019</a>)</cite>, and ESC is short for Emotional Supporting Conversation <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib23" title="">2021</a>)</cite>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MemBench</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We simulated long-term dialogues between two users (a boy named Bart and a girl named Lisa) and a virtual assistant with memory information at various time points, in the form of event summaries. We evaluate the dialogue between children for the sake of expression simplicity and direct emotional exposure. The timeframe spans from January 2022 to June 2024. By sampling different topics and scenarios, we generated 171 historical memories and 160 dialogues. The final MemBench was created through prompt-based generation using GPT-4, followed by multiple manual refinement and revision rounds.
</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">MemBench encompasses two tasks related to conversational memory recall: proactive recall of emotional memories and passive recall of objective facts. The proactive recall involves four types: happy, sad, anxious, and disappointed memories. On the other hand, the passive recall of objective facts encompasses activities, objects, and social relationships. Social relationships are further categorized into positive and negative relationships. The data distribution is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S3.F2" title="Figure 2 ‣ 3.1 Overview ‣ 3 MemBench ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">2</span></a>. Considering the two-phase nature of memory utilization, we also divide the testing of a memory dialogue system into two stages: memory recall and response generation.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="418" id="S3.F2.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Data distribution of each task and category.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T2.1.1.1.1">Subset</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T2.1.1.1.2">#Dialogues</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.3">Avg Turn</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.4">Avg Len.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.1.1">All</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.2.1.2">160</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3">9.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.1.4">44.39</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.3.2.1">Proactive</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S3.T2.1.3.2.2">100</th>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.3">9.38</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2.4">45.80</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S3.T2.1.4.3.1">Passive</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S3.T2.1.4.3.2">60</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.4.3.3">8.93</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S3.T2.1.4.3.4">41.93</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Statistical details of data. Avg Len. is short for average length per turn.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.1">Time</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.2">Emotion</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1.3">Scene</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.4.1">
<span class="ltx_p" id="S3.T3.1.1.1.4.1.1" style="width:71.1pt;">Event</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.2.2.1">2023-12-5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.2.2.2">Happy</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.2.2.3">Activity</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S3.T3.1.2.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.2.4.1">
<span class="ltx_p" id="S3.T3.1.2.2.4.1.1" style="width:71.1pt;">Bart danced at the Christmas party and felt very happy.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>A memory sample.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Contruction</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Since the memory-recalling procedure is related to emotions and other surroundings, we first curated 17 emotional categories according to cognitive science and psychological theories <cite class="ltx_cite ltx_citemacro_cite">Ekman (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib10" title="">1992</a>); PLUTCHIK (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib30" title="">1980</a>); Sabour et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib32" title="">2024</a>)</cite>. Then we asked a psychology major graduate to decide what kind of memories should be recalled to regulate certain emotions for proactive memory recall. Finally, 4 emotions including happiness, sadness, anxiety, and disappointment were selected. We also define 5 scenes to describe the topic, physical state and surroundings of the users, which contain Preferences, Activity, Disease, Emotions and Others. The definition of each scene is in the Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A1.T9" title="Table 9 ‣ Appendix A Definition of Scenes ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">9</span></a> in Appendix.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">We hired a psychology expert who developed a set of guidelines for emotionally supportive dialogue based on psychological literature <cite class="ltx_cite ltx_citemacro_cite">Hill (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib16" title="">2020</a>); Austin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib4" title="">2018</a>); Mayer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib24" title="">2016</a>)</cite>. These guidelines outline how conversations should be conducted in different situations to provide appropriate emotional support to users. The guidance is used to construct dialogues and score the ES skill proficiency later.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">GPT4 <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>GPT4-Turbo-0429, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4" title="">https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4</a></span></span></span> was used to generate dialogues and historical memories with specific topic lists and user profiles. The prompts for passive and proactive recall are different. For each case, a topic and a user are sampled for LLMs to generate a dialogue with a related memory and inject the memory into dialogue in the sampled turn. The conversation turns range from 8 to 12. After generating the initial version of the dialogue and memory data, we use an Embedding model<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We used the best one according to our experiments.</span></span></span> to rank all the memory collections given the context. Then, we select appropriate memories from the top 20 memories as a golden set for each dialogue. When the original memory in the dialogue is not in the golden set of relevant memories, we modify the introduced memory and dialogue content afterwards. All the responses are then checked and modified manually according to the ES skill guidance. These human-crafted responses serve as a reference.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Task Definition</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In this section, we introduce the two tasks tested in MemBench, corresponding to two phrases in the two-stage theory.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Memory Recalling</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.5">For each dialogue <math alttext="\mathcal{D}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><msub id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml">𝒟</mi><mi id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2">𝒟</ci><ci id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">\mathcal{D}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, there is a corresponding golden memory set, denoted as <math alttext="S_{D}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><msub id="S3.SS3.SSS1.p1.2.m2.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS3.SSS1.p1.2.m2.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml">D</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.2">𝑆</ci><ci id="S3.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.3">𝐷</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">S_{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT</annotation></semantics></math>, which contains the most suitable memory for the current dialogue. It is a relevant memory list of length <math alttext="l" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.3.m3.1"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><mi id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><ci id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.3.m3.1d">italic_l</annotation></semantics></math>, where the degree of suitability decreases from strongest to weakest. The value of <math alttext="l" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.4.m4.1"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><mi id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><ci id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.4.m4.1d">italic_l</annotation></semantics></math> ranges from 1 to 7. Before generating a response, the dialogue system needs to retrieve suitable memories from the memory bank based on the current dialogue. The model responsible for memory retrieval is denoted as <math alttext="\textup{M}_{1}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.5.m5.1"><semantics id="S3.SS3.SSS1.p1.5.m5.1a"><msub id="S3.SS3.SSS1.p1.5.m5.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml"><mtext id="S3.SS3.SSS1.p1.5.m5.1.1.2" xref="S3.SS3.SSS1.p1.5.m5.1.1.2a.cmml">M</mtext><mn id="S3.SS3.SSS1.p1.5.m5.1.1.3" xref="S3.SS3.SSS1.p1.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.1b"><apply id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS1.p1.5.m5.1.1.2a.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1.2"><mtext id="S3.SS3.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1.2">M</mtext></ci><cn id="S3.SS3.SSS1.p1.5.m5.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.1c">\textup{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.5.m5.1d">M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Memory Recognition and Response Generation</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.5">After retrieving a subset of memories, the LLM needs to select the most appropriate memory for the dialogue, this process is called memory recognition in two-stage theory. The model responsible for selection and generation is denoted as <math alttext="\textup{M}_{2}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><msub id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml"><mtext id="S3.SS3.SSS2.p1.1.m1.1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.1.2a.cmml">M</mtext><mn id="S3.SS3.SSS2.p1.1.m1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.1.2a.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2"><mtext id="S3.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.2">M</mtext></ci><cn id="S3.SS3.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">\textup{M}_{2}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>. According to psychological research <cite class="ltx_cite ltx_citemacro_cite">Austin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib4" title="">2018</a>)</cite>, criteria for selecting and utilizing memories vary across different tasks of active or passive recall, as well as within each subcategory of these tasks, such as different emotions or various people, events, and objects. The detailed criteria are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A2.T10" title="Table 10 ‣ Appendix B Memory recalling criteria ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">10</span></a> in the Appendix. Besides the recognition, the models are asked to inject suitable memory into the response properly. Given a sample <math alttext="\mathcal{D}_{i},P_{i},\mathcal{R}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.2.m2.3"><semantics id="S3.SS3.SSS2.p1.2.m2.3a"><mrow id="S3.SS3.SSS2.p1.2.m2.3.3.3" xref="S3.SS3.SSS2.p1.2.m2.3.3.4.cmml"><msub id="S3.SS3.SSS2.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml">𝒟</mi><mi id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.3" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.SSS2.p1.2.m2.3.3.3.4" xref="S3.SS3.SSS2.p1.2.m2.3.3.4.cmml">,</mo><msub id="S3.SS3.SSS2.p1.2.m2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.cmml">P</mi><mi id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.3" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.SS3.SSS2.p1.2.m2.3.3.3.5" xref="S3.SS3.SSS2.p1.2.m2.3.3.4.cmml">,</mo><msub id="S3.SS3.SSS2.p1.2.m2.3.3.3.3" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.2" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3.2.cmml">ℛ</mi><mi id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.3" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m2.3b"><list id="S3.SS3.SSS2.p1.2.m2.3.3.4.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3.3"><apply id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.2">𝒟</ci><ci id="S3.SS3.SSS2.p1.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.2">𝑃</ci><ci id="S3.SS3.SSS2.p1.2.m2.2.2.2.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.2.2.2.2.3">𝑖</ci></apply><apply id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.1.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3">subscript</csymbol><ci id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.2.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3.2">ℛ</ci><ci id="S3.SS3.SSS2.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS3.SSS2.p1.2.m2.3.3.3.3.3">𝑖</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m2.3c">\mathcal{D}_{i},P_{i},\mathcal{R}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.2.m2.3d">caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.3.m3.1"><semantics id="S3.SS3.SSS2.p1.3.m3.1a"><msub id="S3.SS3.SSS2.p1.3.m3.1.1" xref="S3.SS3.SSS2.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS2.p1.3.m3.1.1.2" xref="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml">P</mi><mi id="S3.SS3.SSS2.p1.3.m3.1.1.3" xref="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m3.1b"><apply id="S3.SS3.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.2">𝑃</ci><ci id="S3.SS3.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS3.SSS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m3.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the candidate set of memories, <math alttext="\mathcal{R}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.4.m4.1"><semantics id="S3.SS3.SSS2.p1.4.m4.1a"><msub id="S3.SS3.SSS2.p1.4.m4.1.1" xref="S3.SS3.SSS2.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.4.m4.1.1.2" xref="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml">ℛ</mi><mi id="S3.SS3.SSS2.p1.4.m4.1.1.3" xref="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m4.1b"><apply id="S3.SS3.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.2">ℛ</ci><ci id="S3.SS3.SSS2.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS2.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m4.1c">\mathcal{R}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.4.m4.1d">caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the reference response, the target is to generate a response that semantically similar to <math alttext="\mathcal{R}_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.5.m5.1"><semantics id="S3.SS3.SSS2.p1.5.m5.1a"><msub id="S3.SS3.SSS2.p1.5.m5.1.1" xref="S3.SS3.SSS2.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.5.m5.1.1.2" xref="S3.SS3.SSS2.p1.5.m5.1.1.2.cmml">ℛ</mi><mi id="S3.SS3.SSS2.p1.5.m5.1.1.3" xref="S3.SS3.SSS2.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m5.1b"><apply id="S3.SS3.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.2">ℛ</ci><ci id="S3.SS3.SSS2.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS2.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m5.1c">\mathcal{R}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.5.m5.1d">caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">In this task, there are 3 different settings that vary with <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p2.1.m1.1"><semantics id="S3.SS3.SSS2.p2.1.m1.1a"><msub id="S3.SS3.SSS2.p2.1.m1.1.1" xref="S3.SS3.SSS2.p2.1.m1.1.1.cmml"><mi id="S3.SS3.SSS2.p2.1.m1.1.1.2" xref="S3.SS3.SSS2.p2.1.m1.1.1.2.cmml">P</mi><mi id="S3.SS3.SSS2.p2.1.m1.1.1.3" xref="S3.SS3.SSS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p2.1.m1.1b"><apply id="S3.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p2.1.m1.1.1.1.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS2.p2.1.m1.1.1.2.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1.2">𝑃</ci><ci id="S3.SS3.SSS2.p2.1.m1.1.1.3.cmml" xref="S3.SS3.SSS2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p2.1.m1.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p2.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.2">Setting 1 directly provides the first golden memory <math alttext="m_{*}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><msub id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">m</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml">∗</mo></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">𝑚</ci><times id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">m_{*}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">italic_m start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT</annotation></semantics></math> as <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.2.m2.1"><semantics id="S3.I1.i1.p1.2.m2.1a"><msub id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">P</mi><mi id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.2.m2.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2">𝑃</ci><ci id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, which contains a single memory. It is the ideal situation to investigate the memory injection ability of models.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.2">Setting 2 adds 4 sampled negative memories and shuffles the golden and negative memories as <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">P</mi><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">𝑃</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. It ensures the existence of golden memory in <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.2.m2.1"><semantics id="S3.I1.i2.p1.2.m2.1a"><msub id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.2.cmml">P</mi><mi id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.2.m2.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2">𝑃</ci><ci id="S3.I1.i2.p1.2.m2.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.2.m2.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and investigate both the memory recognition and injection abilities of models.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">Setting 3 provides retrieval results from the best embedding model in setting 1 as <math alttext="P_{i}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><msub id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">P</mi><mi id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">𝑃</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">P_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. It is the most realistic setting and the results may be similar to practical performance.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Intimacy and Memory Usage</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">In this experiment, we investigate the memory’s benefit by comparing the generated response of each LLM with and without memory in Setting 3 above. 5 annotators have to choose the more intimate response or ’tie’ for situations where they can not tell which one is better. In addition, in the human evaluation in task 2 above, they are asked to pick the most initiative response among all candidates and tag whether the response quotes the retrieved memory.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Memory Recalling Task</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Embedding Models</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">We conducted experiments on the following embedding models for English: Jina embedding2-base-en  <cite class="ltx_cite ltx_citemacro_citep">(Günther et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib15" title="">2023</a>)</cite>, GTE-base-en-v.15 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib20" title="">2023</a>)</cite>, BGE-M3 Dense  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib7" title="">2024</a>)</cite>, and OpenAI text-embedding-3-large <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/models/embeddings" title="">https://platform.openai.com/docs/models/embeddings</a></span></span></span>. Chinese embedding models tested were: Acge text-embedding <cite class="ltx_cite ltx_citemacro_citep">(Kusupati et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib19" title="">2022</a>)</cite>, Stella<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/infgrad/stella-large-zh-v3-1792d" title="">https://huggingface.co/infgrad/stella-large-zh-v3-1792d</a></span></span></span>, BGE-M3 Dense and Colbert, and Dmeta<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/DMetaSoul/Dmeta-embedding-zh" title="">https://huggingface.co/DMetaSoul/Dmeta-embedding-zh</a></span></span></span> and OpenAI text-embedding-3-large. These embedding models are top-ranked in the retrieval tasks in ETEB Leaderboard <span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>in June 2024. Leaderboard: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/mteb/leaderboard" title="">https://huggingface.co/spaces/mteb/leaderboard</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">Muennighoff et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib26" title="">2022</a>)</cite>. We did not introduce the model parameters over 7B due to inference speed limitation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.2">For a comprehensive evaluation of the memory recall performance of the embedding model <math alttext="\textup{M}_{1}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.1.m1.1"><semantics id="S4.SS1.SSS2.p1.1.m1.1a"><msub id="S4.SS1.SSS2.p1.1.m1.1.1" xref="S4.SS1.SSS2.p1.1.m1.1.1.cmml"><mtext id="S4.SS1.SSS2.p1.1.m1.1.1.2" xref="S4.SS1.SSS2.p1.1.m1.1.1.2a.cmml">M</mtext><mn id="S4.SS1.SSS2.p1.1.m1.1.1.3" xref="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.1.m1.1b"><apply id="S4.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.1.m1.1.1.2a.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2"><mtext id="S4.SS1.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS1.SSS2.p1.1.m1.1.1.2">M</mtext></ci><cn id="S4.SS1.SSS2.p1.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.1.m1.1c">\textup{M}_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.1.m1.1d">M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, and considering the different levels of relevance among memories for a particular dialogue, we measured Mean Average Precision (MAP), Mean Reciprocal Rank (MRR), Normalized Discounted Cumulative Gain (nDCG), Recall, Precision and calculated their geometric mean across different scopes. The detailed metrics are illustrated in Tab.<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.T4" title="Table 4 ‣ 4.3 Metrics ‣ 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">4</span></a>. MRR places greater emphasis on the top-ranked position, focusing more on the golden summary <math alttext="m_{*}" class="ltx_Math" display="inline" id="S4.SS1.SSS2.p1.2.m2.1"><semantics id="S4.SS1.SSS2.p1.2.m2.1a"><msub id="S4.SS1.SSS2.p1.2.m2.1.1" xref="S4.SS1.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS1.SSS2.p1.2.m2.1.1.2" xref="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml">m</mi><mo id="S4.SS1.SSS2.p1.2.m2.1.1.3" xref="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml">∗</mo></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS2.p1.2.m2.1b"><apply id="S4.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.2">𝑚</ci><times id="S4.SS1.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS1.SSS2.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS2.p1.2.m2.1c">m_{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS2.p1.2.m2.1d">italic_m start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT</annotation></semantics></math>. MAP uniformly considers all relevant summaries, while nDCG accounts for the relevance and positions of different memories. The geometric mean is to comprehensively assess their retrieval quality while mitigating the impact of outliers.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Memory Recognition and Response Generation</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Models</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">We conducted tests on various models of differing sizes within the latest English and Chinese series. The models evaluated in this study include Qwen2-7B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib40" title="">2024</a>)</cite>, GPT-4-turbo <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib1" title="">2023</a>)</cite>, Doubao-Character-32k(0528), GLM-4-0520<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://bigmodel.cn/dev/api/normal-model/glm-4" title="">https://bigmodel.cn/dev/api/normal-model/glm-4</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Zeng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib41" title="">2022</a>)</cite>, and Ziya-Character-0606, a fine-tuned model with over 30K character dialogues based on the Ziya2-13B-Base model <cite class="ltx_cite ltx_citemacro_cite">Gan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib12" title="">2024</a>)</cite>.
For English version, we tested Llama3-8B-Instruct, Llama3-70B-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib8" title="">2024</a>)</cite>, GPT-4o, and
GPT-4-turbo.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib29" title="">2002</a>)</cite>, Rouge-L <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib21" title="">2004</a>)</cite> and embedding similarity <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#bib.bib43" title="">Zhang et al. </a></cite> were employed to assess the correlation between the generated response and the reference response. Acknowledging the limitations of these metrics, we utilize human annotation to evaluate the candidates in various aspects. Each scoring instance included a reference answer and five candidate answers to facilitate a more objective comparison. The human evaluation includes both aspect-aware scoring and the pick-the-best paradigm. The definitions are listed and detailed requirements are listed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A3" title="Appendix C Details for human evaluation of scoring criteria ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">C</span></a>:</p>
<ul class="ltx_itemize" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1">Naturalness. This aspect inspects the language quality of candidate responses, including grammar, pronoun usage, context correlation and oral expression.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1">Style coherence. The output sentence should aligned with the character setting in prompts. Specifically, the assistant should use a proper tone and easy words to talk with a child.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1">Memory-injection Ability. This aspect examines how well the LLM can recognise the correct memory and introduce it into dialogues without quotation errors.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1">ES Skill Proficiency. It measures how skilful the model is in ES, considering at most three turns of dialogues. It is scored given a dialogue guidance covering different situations.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i5.p1">
<p class="ltx_p" id="S4.I1.i5.p1.1">Emotion Improvement. This metric measures whether the response, the literal expression is empathetic and positive or not. A high emotion improvement score does entail high ES Skill.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S4.I1.i6.p1">
<p class="ltx_p" id="S4.I1.i6.p1.1">Intimacy. This metric is used in side-by-side comparisons and pick-the-best among candidates. It is an overall feeling of how much a model behaves like a close friend.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1">%</th>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.1.1.1.2">MAP</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.1.1.1.3">MRR</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="4" id="S4.T4.1.1.1.4">nDCG</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.2.2.1">English Models</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.2">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.3">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.4">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.5">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.6">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.7">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.8">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.2.9">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.10">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.11">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.12">@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.2.13">@10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.3.3.1">Jina</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.2">36.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.3">31.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.4">32.99</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.3.5">36.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.6">36.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.7">45.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.8">47.92</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.3.9">50.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.10">36.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.11">48.61</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.12">52.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.3.13">57.07</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.4.4.1">BGE M3</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.2">50.63</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.3">43.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.4">43.45</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.5">47.35</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.6">50.63</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.7">59.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.8">61.80</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.4.4.9">63.24</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.10">50.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.11">62.36</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.12">65.30</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.4.4.13">67.80</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.5.5.1">GTE</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.2">45.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.3">40.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.4">40.44</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.5.5">43.69</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.6">45.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.7">53.85</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.8">56.20</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.5.5.9">57.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.10">45.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.11">56.84</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.12">60.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.5.13">62.82</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.6.6.1">OpenAI</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.2">62.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.3">54.22</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.4">55.77</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.6.6.5">58.58</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.6">62.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.7">72.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.8">73.97</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.6.6.9">74.86</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.10">62.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.11">75.34</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.12">76.95</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.6.13">77.40</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.7.7.1">%</th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T4.1.7.7.2">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="4" id="S4.T4.1.7.7.3">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="4" id="S4.T4.1.7.7.4">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.8.8.1">English Models</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.2">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.3">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.4">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.8.8.5">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.6">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.7">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.8">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.8.8.9">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.10">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.11">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.12">@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.8.8.13">@10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.9.9.1">Jina</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.2">21.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.3">36.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.4">44.83</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.9.5">62.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.6">36.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.7">23.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.8">18.12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.9.9.9">13.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.10">33.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.11">37.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.12">39.32</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.9.9.13">43.95</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.10.10.1">BGE M3</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.2">27.92</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.3">47.29</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.4">55.27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.10.10.5">70.47</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.6">50.62</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.7">32.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.8">23.75</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.10.10.9">15.81</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.10">46.08</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.11">48.95</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.12">49.91</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.10.10.13">52.93</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.11.11.1">GTE</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.2">27.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.3">43.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.4">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.11.11.5">65.68</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.6">45.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.7">28.33</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.8">21.12</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.11.11.9">14.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.10">41.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.11">44.58</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.12">46.19</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.11.11.13">48.81</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.12.12.1">OpenAI</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.2">36.78</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.3">58.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.4">69.49</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.12.12.5">80.59</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.6">62.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.7">38.33</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.8">29.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.12.12.9">17.81</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.10"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.10.1">57.36</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.11"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.11.1">59.68</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.12"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.12.1">61.04</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.12.12.13"><span class="ltx_text ltx_font_bold" id="S4.T4.1.12.12.13.1">61.85</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S4.T4.1.13.13.1">%</th>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.1.13.13.2">MAP</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="4" id="S4.T4.1.13.13.3">MRR</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="4" id="S4.T4.1.13.13.4">nDCG</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.14.14.1">Chinese Models</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.2">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.3">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.4">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.14.5">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.6">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.7">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.8">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.14.14.9">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.10">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.11">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.12">@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.14.14.13">@10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.15.15.1">Acge</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.2">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.3">46.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.4">46.80</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.15.5">50.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.6">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.7">63.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.8">64.96</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.15.15.9">66.17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.10">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.11">66.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.12">69.13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.15.15.13">70.30</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.16.16.1">Stella</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.2">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.3">44.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.4">45.40</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.16.16.5">49.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.6">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.7">61.87</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.8">64.22</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.16.16.9">65.56</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.10">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.11">64.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.12">68.17</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.16.16.13">69.61</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.17.17.1">BGE M3 (colbert)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.2">51.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.3">41.89</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.4">43.76</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.17.17.5">47.48</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.6">51.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.7">60.42</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.8">62.07</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.17.17.9">63.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.10">51.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.11">62.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.12">64.85</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.17.17.13">66.78</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.18.18">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.18.18.1">BGE M3 (dense)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.2">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.3">44.51</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.4">45.54</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.18.18.5">48.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.6">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.7">63.54</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.8">64.95</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.18.18.9">65.79</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.10">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.11">66.61</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.12">68.61</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.18.18.13">69.30</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.19.19">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.19.19.1">Dmeta</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.2">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.3">45.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.4">46.64</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.19.19.5">49.21</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.6">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.7">62.40</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.8">64.27</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.19.19.9">65.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.10">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.11">66.30</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.12">69.12</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.19.19.13">70.31</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.20.20">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.20.20.1">OpenAI</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.2">64.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.3">54.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.4">55.98</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.20.20.5">59.82</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.6">64.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.7">73.44</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.8">74.59</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.20.20.9">75.43</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.10">64.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.11">75.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.12">76.90</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.20.20.13">77.19</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.21.21">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.21.21.1">%</th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="4" id="S4.T4.1.21.21.2">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="4" id="S4.T4.1.21.21.3">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" colspan="4" id="S4.T4.1.21.21.4">Average</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.22.22">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.22.22.1">Chinese Models</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.2">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.3">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.4">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.22.22.5">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.6">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.7">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.8">@5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.22.22.9">@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.10">@1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.11">@3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.12">@5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.22.22.13">@10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.23.23">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T4.1.23.23.1">Acge</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.2">30.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.3">50.25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.4">58.98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.23.23.5">73.53</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.6">52.50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.7">33.96</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.8">25.00</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.23.23.9">16.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.10">48.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.11">52.10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.12">52.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.23.23.13">55.54</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.24.24">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.24.24.1">Stella</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.2">30.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.3">46.63</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.4">57.73</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.24.24.5">73.14</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.6">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.7">32.71</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.8">25.13</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.24.24.9">16.69</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.10">48.02</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.11">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.12">52.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.24.24.13">54.83</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.25.25">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.25.25.1">BGE M3 (colbert)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.2">27.99</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.3">45.72</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.4">57.14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.25.25.5">71.57</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.6">51.25</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.7">31.46</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.8">24.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.25.25.9">16.38</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.10">46.60</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.11">48.49</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.12">50.54</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.25.25.13">53.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.26.26">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.26.26.1">BGE M3 (dense)</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.2">28.94</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.3">50.10</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.4">58.57</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.26.26.5">71.44</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.6">52.50</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.7">33.75</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.8">25.25</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.26.26.9">16.19</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.10">47.79</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.11">51.70</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.12">52.58</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.26.26.13">54.31</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.27.27">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T4.1.27.27.1">Dmeta</th>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.2">30.53</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.3">52.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.4">59.87</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.27.27.5">70.07</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.6">50.00</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.7">33.13</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.8">24.88</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.1.27.27.9">15.31</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.10">46.11</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.11">51.93</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.12">52.96</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.27.27.13">54.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.28.28">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T4.1.28.28.1">OpenAI</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.2">36.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.3">58.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.4">69.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.28.28.5">82.37</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.6">64.38</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.7">39.80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.8">29.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T4.1.28.28.9">18.81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.10"><span class="ltx_text ltx_font_bold" id="S4.T4.1.28.28.10.1">58.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.11"><span class="ltx_text ltx_font_bold" id="S4.T4.1.28.28.11.1">60.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.12"><span class="ltx_text ltx_font_bold" id="S4.T4.1.28.28.12.1">61.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.28.28.13"><span class="ltx_text ltx_font_bold" id="S4.T4.1.28.28.13.1">62.72</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Memory recall performance in English version and Chinese version.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Memory Search </h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">From the embedding results in Tab. <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S4.T4" title="Table 4 ‣ 4.3 Metrics ‣ 4 Experiments ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">4</span></a>, it can be observed that OpenAI embedding achieved the best performance on both English and Chinese datasets. The bilingual BGM-M3 also demonstrated commendable performance among the open-source models in the English testbed. For the Chinese testbed, Acge is the best open-source model in our setting.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">However, regardless of the language, the retrieval performance on MemBench is far from satisfying for both English and Chinese models. Even the best embedding model, OpenAI, did not exceed 60% @1 and struggled at 62% @10 in the final average, highlighting the difficulty of retrieving appropriate memories during the dialogue process. Pure text similarity retrieval is inadequate for the memory recall process in dialogue systems.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Memory Recognition and Response Generation</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.T5" title="Table 5 ‣ 5.2 Memory Recognition and Response Generation ‣ 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">5</span></a> depicts LLMs’ average scores of each aspect on all three tasks in the Chinese version.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">For the overall performance, we can see human-written references scored higher than models in most aspects except emotion improvement from GPT4-Turbo. Therefore, no LLMs can surpass humans in average performance. There is space for improvement in aspects including Naturalness, Style Coherence, Memory Injection and ES skills. GPT4-Turbo is the best model for tasks 1 and 2, while GLM-4 is superior in setting 3.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">For memory injection, we can observe a substantial gap between LLMs and human written reference, in which humans can introduce suitable memory into response correctly and LLMs can only achieve half of the scores in setting 3 except GLM-4. The results in setting 1 are almost higher than setting 2 and 3. Specifically, given the golden memory, models achieve over 2.5 scores, which decrease dramatically when blended with unrelated memories, except for Ziya-Character, which performed badly in all tasks for the sake of smaller parameter size. Results of GLM-4 seem robust at the cost of introducing too much memory into response and harming other aspects like style coherence.
It reveals that the <em class="ltx_emph ltx_font_italic" id="S5.SS2.p3.1.1">strongest LLMs are also weak in memory recognition</em>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">A significant improvement in Naturalness from setting 1 to setting 2 and 3
implies that all models benefit from the emphasis on the Naturalness in prompts. Similar phenomena also exist in Style Coherence. Among all the models, Doubao excels in these two aspects after the prompt strengthening. Since the Ziya-Character was fine-tuned with role-play and character datasets, it can produce natural and stylish responses steadily without prompting engineering. But it was beaten by larger models like Doubao, GPT4-Turbo and Qwen2-72B with prompt engineering.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">When in setting 1 where golden memory was provided, all the models had over 2.2 scores in ES Skill. But in reality (setting 3), scores on ES Skill drop significantly. Therefore, The ES Skill Proficiency is related to Memory Injection ability. To dig out the correlation, we draw the distribution of ES skill grades with varied memory-injection scores in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.F3" title="Figure 3 ‣ 5.2 Memory Recognition and Response Generation ‣ 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">3</span></a>. We can find <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS2.p5.1.1">higher memory-injection ability leads to higher ES skillfulness</em>. A 3.0 memory injection score can lead to a 3.0 ES Skill in 61%, which emphasizes the importance of memory-augmented ability in ES.</p>
</div>
<div class="ltx_para" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">Surprisingly, the Emotional Improvement expression from GPT4-Turbo, Qwen2-72B and GLM-4 can exceed that of humans. These models show a positive attitude to the events in dialogue and encourage users against low emotion, which is warm and energetic. Human references sometimes are not so affirmative and encouraging.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.1">Models</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.2">Setting</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.3">Naturalness</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.4">Style Coherence</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.5">Memory Injection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.6">ES Skill</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.7">Emotional Impr.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T5.1.1.1.8">Average</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.2.2.1" rowspan="3"><span class="ltx_text" id="S5.T5.1.2.2.1.1">Doubao</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.2.2.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.2.3">1.801</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.2.4">2.035</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.2.5">2.526</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.2.6">2.225</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.2.2.7">2.577</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.2.8">2.233</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.3.3.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.3.3.2.1">2.700</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.3.3.3.1">2.850</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.3.4">0.900</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.3.5">2.425</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.3.3.6">2.600</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.3.3.7">2.295</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.4.4.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.4.2.1">2.709</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.4.4.3.1">2.729</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4.4">0.544</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4.5">2.052</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.4.4.6">2.486</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.4.4.7">2.104</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.5.5.1" rowspan="3"><span class="ltx_text" id="S5.T5.1.5.5.1.1">GPT4-Turbo</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.5.5.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.5.5.3">2.069</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.5.5.4">1.984</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.5.5.5">2.553</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.5.5.6">2.675</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.5.5.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.7.1">2.893</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.5.5.8"><span class="ltx_text ltx_font_bold" id="S5.T5.1.5.5.8.1">2.435</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.6.6.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6.2">2.550</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6.3">2.450</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6.4">1.475</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.5.1">2.600</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.6.6.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.6.1">2.825</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.6.6.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.6.6.7.1">2.388</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.7.7.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.7.7.2">2.656</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.7.7.3">2.178</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.7.7.4">1.596</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.7.7.5">2.376</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.7.7.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.7.7.6.1">2.752</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.7.7.7">2.311</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.8.8.1" rowspan="3"><span class="ltx_text" id="S5.T5.1.8.8.1.1">Qwen2-72b</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.8.8.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.8.8.3">2.020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.8.8.4">1.984</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.8.8.5">2.500</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.8.8.6"><span class="ltx_text ltx_font_bold" id="S5.T5.1.8.8.6.1">2.750</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.8.8.7">2.723</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.8.8.8">2.395</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.9.9.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.9.9.2">2.525</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.9.9.3">2.725</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.9.9.4">1.125</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.9.9.5">2.550</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.9.9.6">2.800</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.9.9.7">2.345</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.10.10.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.10.10.2">2.632</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.10.10.3">2.496</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.10.10.4">1.734</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.10.10.5">2.443</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.10.10.6">2.609</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.10.10.7">2.383</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.11.11.1" rowspan="3"><span class="ltx_text" id="S5.T5.1.11.11.1.1">GLM-4</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.11.11.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.11.11.3">2.044</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.11.11.4">1.704</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.11.11.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.11.11.5.1">2.895</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.11.11.6">2.600</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.11.11.7">2.674</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.11.11.8">2.384</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.12.12.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.12.12.2">2.125</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.12.12.3">2.050</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.12.12.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.12.12.4.1">2.275</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.12.12.5">2.525</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.12.12.6">2.750</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.12.12.7">2.345</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.13.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.13.13.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.13.13.2">2.750</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.13.13.3">2.117</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.13.13.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.13.13.4.1">2.329</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.13.13.5"><span class="ltx_text ltx_font_bold" id="S5.T5.1.13.13.5.1">2.524</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.13.13.6">2.582</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.13.13.7"><span class="ltx_text ltx_font_bold" id="S5.T5.1.13.13.7.1">2.397</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.14.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.14.14.1" rowspan="3"><span class="ltx_text" id="S5.T5.1.14.14.1.1">Ziya-Character</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.14.14.2">1</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.14.14.3"><span class="ltx_text ltx_font_bold" id="S5.T5.1.14.14.3.1">2.239</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.14.14.4"><span class="ltx_text ltx_font_bold" id="S5.T5.1.14.14.4.1">2.798</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.14.14.5">0.289</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.14.14.6">2.250</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.1.14.14.7">2.577</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.14.14.8">2.031</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.15.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.15.15.1">2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.15.15.2">2.350</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.15.15.3">2.725</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.15.15.4">0.925</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.15.15.5">2.300</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.15.15.6">2.625</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.15.15.7">2.185</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.16.16.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.16.16.2">2.574</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.16.16.3">2.655</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.16.16.4">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.16.16.5">1.890</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.1.16.16.6">2.24</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.16.16.7">2.049</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.17.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.1">reference</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.2">all</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.3">2.750</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.4">2.900</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.5">3.000</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.6">2.875</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S5.T5.1.17.17.7">2.650</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.1.17.17.8">2.835</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Human Evaluation Results on memory recognition and response generation. The prompts for setting 2 and setting 3 particularly emphasize the naturalness and style requirements to improve the performance. The highest score for each aspect per task is in bold.</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S5.F3.g1" src="extracted/5874076/pics/mem3es.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The relation between memory injection score and ES skill score. The probability of a 3.0 ES Skill grows with the increase of memory injection scores.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Intimacy and Memory Usage</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The Side-by-side evaluation results on intimacy of responses with and without memory are in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.T6" title="Table 6 ‣ 5.3 Intimacy and Memory Usage ‣ 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">6</span></a>. As the table indicates, responses with memory injection are almost (no less than 69.4% probability) better than those without memory. The win rate grows higher as the model becomes stronger. For example, GPT4-Turbo can produce 61.9% more intimate response with memory while Ziya-Character can only produce 47.5% win rate. It is attributed to their stronger memory injection ability and accordingly the ES skill. It implies that <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS3.p1.1.1">intimacy performance is highly related to memory injection ability.</em></p>
</div>
<figure class="ltx_table" id="S5.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T6.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T6.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T6.1.1.1.2">Win</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S5.T6.1.1.1.3">Tie</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T6.1.1.1.4">Lose</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.2.1.1">GPT4-Turbo</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.2.1.2">61.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T6.1.2.1.3">28.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.2.1.4">9.4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.3.2.1">Qwen2-72B</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.3.2.2">56.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.3.2.3">28.8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.3.2.4">15.0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.4.3.1">Doubao</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.4.3.2">44.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.4.3.3">34.4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.4.3.4">21.3</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.5.4.1">GLM-4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.5.4.2">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.5.4.3">19.4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.5.4.4">25.6</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.1.6.5.1">Ziya-Character</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.1.6.5.2">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T6.1.6.5.3">21.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.6.5.4">30.6</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Side-by-side evaluation results on the responses with and without memory.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We also investigate people’s preferences by asking 5 annotators to select the best responses and see whether they are equipped with memory in setting 3. Results of the ratio of containing memory in voted responses are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.T7" title="Table 7 ‣ 5.3 Intimacy and Memory Usage ‣ 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">7</span></a>. On average, 73% best responses are with memory, which shows people prefer memory-aware replies. We think it can provide better emotional support.</p>
</div>
<figure class="ltx_table" id="S5.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T7.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T7.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.1">Annotation</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.2">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.4">3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.5">4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.6">5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T7.1.1.1.7">AVG</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T7.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.1">With Mem (%)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.2">70</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.3">90</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.4">55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.5">80</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.6">70</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T7.1.2.1.7">73</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>The memory injection rate in best candidates voted by humans.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Automatic Evaluation</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The automatic results are displayed in <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#S5.T8" title="Table 8 ‣ 5.4 Automatic Evaluation ‣ 5 Results and Discussion ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">8</span></a>. The full result is in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A4.T11" title="Table 11 ‣ Appendix D Automatic Evaluation ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">11</span></a>.
As we can see, the GLM-4 can achieve the highest semantic similarity while Qwen2-72B earned the highest Rouge-L. However, the gap between Qwen2-72B and GLM-4 is marginal. For Dist-1 and 2, the Ziya-Character got the highest results. It may be attributed to its training on multiple diverse character corpora in SFT. However, it is opposite to the conclusion in the human evaluation that GPT4-Turbo is the best and Qwen2-72B ranked second. Therefore, <em class="ltx_emph ltx_font_bold ltx_font_italic" id="S5.SS4.p1.1.1">automatic evaluation fails to measure the results from LLMs</em>. We also tried judging with GPT4 using various promptings and found its scores were unreliable.</p>
</div>
<figure class="ltx_table" id="S5.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T8.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T8.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T8.1.1.1.1">Models</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.2">BertScore</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.3">Rouge-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.4">BLEU-1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T8.1.1.1.5">Dist-1</th>
</tr>
<tr class="ltx_tr" id="S5.T8.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="5" id="S5.T8.1.2.2.1">Setting 1(%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T8.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.1.3.1.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.3.1.2">72.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.3.1.3">22.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.3.1.4">22.82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.3.1.5">74.72</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.4.2.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.4.2.2">72.82</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.4.2.3">23.17</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.4.2.4">23.76</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.4.2.5">76.13</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.5.3.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.5.3.2">73.07</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.5.3.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.5.3.3.1">25.08</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.5.3.4">29.14</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.5.3.5">80.24</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.6.4.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.6.4.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.6.4.2.1">74.17</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.6.4.3">24.98</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.6.4.4">26.40</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.6.4.5">74.91</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.7.5.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.7.5.2">69.58</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.7.5.3">19.68</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.7.5.4">20.74</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.7.5.5"><span class="ltx_text ltx_font_bold" id="S5.T8.1.7.5.5.1">88.45</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T8.1.8.6.1">Setting 2(%)</th>
</tr>
<tr class="ltx_tr" id="S5.T8.1.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.1.9.7.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.9.7.2">70.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.9.7.3">20.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.9.7.4">22.28</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.9.7.5">79.89</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.10.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.10.8.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.10.8.2">69.08</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.10.8.3">21.26</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.10.8.4">22.50</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.10.8.5">75.80</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.11.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.11.9.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.11.9.2">71.67</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.11.9.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.11.9.3.1">23.04</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.11.9.4"><span class="ltx_text ltx_font_bold" id="S5.T8.1.11.9.4.1">26.72</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.11.9.5">79.37</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.12.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.12.10.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.12.10.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.12.10.2.1">73.19</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.12.10.3">22.37</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.12.10.4">22.11</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.12.10.5">72.69</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.13.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.13.11.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.13.11.2">60.73</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.13.11.3">19.68</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.13.11.4">21.74</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.13.11.5"><span class="ltx_text ltx_font_bold" id="S5.T8.1.13.11.5.1">86.38</span></td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.14.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T8.1.14.12.1">Setting 3(%)</th>
</tr>
<tr class="ltx_tr" id="S5.T8.1.15.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T8.1.15.13.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.15.13.2">70.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.15.13.3">20.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.15.13.4">21.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T8.1.15.13.5">81.26</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.16.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.16.14.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.16.14.2">71.65</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.16.14.3">21.22</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.16.14.4">22.32</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.16.14.5">75.91</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.17.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.17.15.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.17.15.2">71.68</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.17.15.3"><span class="ltx_text ltx_font_bold" id="S5.T8.1.17.15.3.1">22.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.17.15.4"><span class="ltx_text ltx_font_bold" id="S5.T8.1.17.15.4.1">26.15</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.17.15.5">78.64</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.18.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T8.1.18.16.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="S5.T8.1.18.16.2"><span class="ltx_text ltx_font_bold" id="S5.T8.1.18.16.2.1">73.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.18.16.3">22.39</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.18.16.4">22.11</td>
<td class="ltx_td ltx_align_center" id="S5.T8.1.18.16.5">72.90</td>
</tr>
<tr class="ltx_tr" id="S5.T8.1.19.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T8.1.19.17.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.1.19.17.2">68.63</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.1.19.17.3">18.24</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.1.19.17.4">19.34</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T8.1.19.17.5"><span class="ltx_text ltx_font_bold" id="S5.T8.1.19.17.5.1">89.18</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Automatic Evaluation Results on Chinese testbed. Full results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A4.T11" title="Table 11 ‣ Appendix D Automatic Evaluation ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">11</span></a>. The automatic measurements are inconsistent with human evaluation.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Memory-augmented Dialogue Systems and Chatbots are popular applications of LLMs. However, there have been no systematic memory recall and injection evaluation benchmarks until now. This paper introduces a comprehensive and systematic bilingual memory benchmark, called <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">MemBench</span>, that contains both proactive and passive recall for the first time while existing work only considers passive recalls. Based on cognitive science it examine the performance of MADS in different phrases: memory recall, memory recognition and injection. In human evaluation, novel measurements including memory-injection ability, ES skill proficiency and intimacy are proposed for the first time. We test extensive competitive LLMs in 3 tasks and various settings with both human and automatic metrics, leading to meaningful and surprising insights:</p>
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1">Current retrieval-based memory-augmented dialogue systems are imperfect in two stages and there is plenty of room for improvement.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1">The memory recall performance of top Embedding models is still far from satisfying, with the OpenAI embedding-3 being the best.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1">Memory injection ability influences ES skill performance unexpectedly. It is important for intimacy and stronger LLMs with memory injected have a higher win rate.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S6.I1.i4.p1">
<p class="ltx_p" id="S6.I1.i4.p1.1">Most LLMs can achieve high naturalness and style coherence easily by particular prompts which memory recognition and injection are challenging.</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S6.I1.i5.p1">
<p class="ltx_p" id="S6.I1.i5.p1.1">There is still a substantial gap between the most powerful LLMs and human-written references in overall performance. The largest gap lies in memory injection scores in the reality setting.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This paper is kindly supported by Xiaolin Qi, Chao Song, Yun Chen, Qianguo Sun and Bing Wang. Thanks for their contribution.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Limitation</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This paper only considers the retrieval-based memory-augmented chatbots and dialogue systems. The experiments focused on bilingual data, hoping the conclusion and insights could be popularized in other languages.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.anthropic.com/news/claude-3-5-sonnet" title="">Claude 3.5 sonnet</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arditte Hall et al. (2018)</span>
<span class="ltx_bibblock">
Kimberly A Arditte Hall, Rudi De Raedt, Kiara R Timpano, and Jutta Joormann. 2018.

</span>
<span class="ltx_bibblock">Positive memory enhancement training for individuals with major depressive disorder.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Cognitive Behaviour Therapy</em>, 47(2):155–168.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et al. (2018)</span>
<span class="ltx_bibblock">
Elizabeth J Austin, Donald H Saklofske, and Martin M Smith. 2018.

</span>
<span class="ltx_bibblock">Development and validation of two short forms of the managing the emotions of others (meos) scale.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Frontiers in Psychology</em>, 9:365302.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bahrick (1970)</span>
<span class="ltx_bibblock">
Harry P Bahrick. 1970.

</span>
<span class="ltx_bibblock">Two-phase model for prompted recall.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Psychological Review</em>, 77(3):215.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2303.12712</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.03216" title="">Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Preprint</em>, arXiv:2402.03216.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey et al. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.21783" title="">The llama 3 herd of models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Preprint</em>, arXiv:2407.21783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebbinghaus (1885)</span>
<span class="ltx_bibblock">
Hermann Ebbinghaus. 1885.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Memory; a contribution to experimental psychology</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ekman (1992)</span>
<span class="ltx_bibblock">
Paul Ekman. 1992.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1080/02699939208411068" title="">An argument for basic emotions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Cognition and Emotion</em>, 6(3-4):169–200.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engen and Anderson (2018)</span>
<span class="ltx_bibblock">
Haakon G Engen and Michael C Anderson. 2018.

</span>
<span class="ltx_bibblock">Memory control: A fundamental mechanism of emotion regulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Trends in Cognitive Sciences</em>, 22(11):982–995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan et al. (2024)</span>
<span class="ltx_bibblock">
Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang, Kunhao Pan, Junqing He, Yuanhe Tian, Ping Yang, Qi Yang, Hao Wang, Jiaxing Zhang, and Yan Song. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.03301" title="">Ziya2: Data-centric learning is all llms need</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Preprint</em>, arXiv:2311.03301.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goleman and Boyatzis (2017)</span>
<span class="ltx_bibblock">
Daniel Goleman and Richard E. Boyatzis. 2017.

</span>
<span class="ltx_bibblock">Emotional intelligence has 12 elements. which do you need to work on?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Harward Bussiness Review</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Günther et al. (2023)</span>
<span class="ltx_bibblock">
Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.19923" title="">Jina embeddings 2: 8192-token general-purpose text embeddings for long documents</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Preprint</em>, arXiv:2310.19923.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hill (2020)</span>
<span class="ltx_bibblock">
Clara E Hill. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Helping skills: Facilitating exploration, insight, and action</em>.

</span>
<span class="ltx_bibblock">American Psychological Association.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horton (2005)</span>
<span class="ltx_bibblock">
William S. Horton. 2005.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:46768045" title="">Conversational common ground and memory processes in language production</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Discourse Processes</em>, 40:1 – 35.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al. (2024)</span>
<span class="ltx_bibblock">
Yuki Hou, Haruki Tamoto, and Homei Miyashita. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3613905.3650839" title="">“my agent understands me better”: Integrating dynamic human-like memory recall and consolidation in llm-based agents</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Extended Abstracts of the CHI Conference on Human Factors in Computing Systems</em>, CHI ’24. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kusupati et al. (2022)</span>
<span class="ltx_bibblock">
Aditya Kusupati, Gantavya Bhatt, Aniket Rege, Matthew Wallingford, Aditya Sinha, Vivek Ramanujan, William Howard-Snyder, Kaifeng Chen, Sham Kakade, Prateek Jain, et al. 2022.

</span>
<span class="ltx_bibblock">Matryoshka representation learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</em>, 35:30233–30249.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. 2023.

</span>
<span class="ltx_bibblock">Towards general text embeddings with multi-stage contrastive learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2308.03281</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W04-1013" title="">ROUGE: A package for automatic evaluation of summaries</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Text Summarization Branches Out</em>, pages 74–81, Barcelona, Spain. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2311.08719" title="">Think-in-memory: Recalling and post-thinking enable llms with long-term memory</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Preprint</em>, arXiv:2311.08719.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Siyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. 2021.

</span>
<span class="ltx_bibblock">Towards emotional support dialog systems.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 59th Annual Meeting of the Association for memoryforconv Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 3469–3483.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mayer et al. (2016)</span>
<span class="ltx_bibblock">
John D Mayer, David R Caruso, and Peter Salovey. 2016.

</span>
<span class="ltx_bibblock">The ability model of emotional intelligence: Principles and updates.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Emotion review</em>, 8(4):290–300.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McKinley et al. (2017)</span>
<span class="ltx_bibblock">
Geoffrey L McKinley, Sarah Brown-Schmidt, and Aaron S Benjamin. 2017.

</span>
<span class="ltx_bibblock">Memory for conversation and the development of common ground.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Memory &amp; cognition</em>, 45:1281–1294.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.48550/ARXIV.2210.07316" title="">Mteb: Massive text embedding benchmark</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2210.07316</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openai.com/index/chatgpt/" title="">Introducing chatgpt</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Packer et al. (2024)</span>
<span class="ltx_bibblock">
Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.08560" title="">Memgpt: Towards llms as operating systems</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Preprint</em>, arXiv:2310.08560.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PLUTCHIK (1980)</span>
<span class="ltx_bibblock">
ROBERT PLUTCHIK. 1980.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/B978-0-12-558701-3.50007-7" title="">Chapter 1 - a general psychoevolutionary theory of emotion</a>.

</span>
<span class="ltx_bibblock">In Robert Plutchik and Henry Kellerman, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Theories of Emotion</em>, pages 3–33. Academic Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et al. (2019)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019.

</span>
<span class="ltx_bibblock">Towards empathetic open-domain conversation models: A new benchmark and dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 5370–5381.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabour et al. (2024)</span>
<span class="ltx_bibblock">
Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M. C. Lee, Rada Mihalcea, and Minlie Huang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.12071" title="">Emobench: Evaluating the emotional intelligence of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Preprint</em>, arXiv:2402.12071.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sabour et al. (2022)</span>
<span class="ltx_bibblock">
Sahand Sabour, Chujie Zheng, and Minlie Huang. 2022.

</span>
<span class="ltx_bibblock">Cem: Commonsense-aware empathetic response generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pages 11229–11237.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarah and Melissa (2016)</span>
<span class="ltx_bibblock">
Brown-Schmidt Sarah and C. Duff Melissa. 2016.

</span>
<span class="ltx_bibblock">Memory and common ground processes in language use.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Topics in cognitive science</em>, page 722–736.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al. (2022)</span>
<span class="ltx_bibblock">
Zhiliang Tian, Yinliang Wang, Yiping Song, Chi Zhang, Dongkyu Lee, Yingxiu Zhao, Dongsheng Li, and Nevin L Zhang. 2022.

</span>
<span class="ltx_bibblock">Empathetic and emotionally positive conversation systems with an emotion-specific query-response memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pages 6364–6376.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Weizhi Wang, Li Dong, Hao Cheng, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.07174" title="">Augmenting language models with long-term memory</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Preprint</em>, arXiv:2306.07174.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watkins and Gardiner (1979)</span>
<span class="ltx_bibblock">
Michael J. Watkins and John M. Gardiner. 1979.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/S0022-5371(79)90397-9" title="">An appreciation of generate-recognize theory of recall</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Journal of Verbal Learning and Verbal Behavior</em>, 18(6):687–704.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weissenborn and Duka (2000)</span>
<span class="ltx_bibblock">
R Weissenborn and T Duka. 2000.

</span>
<span class="ltx_bibblock">State-dependent effects of alcohol on explicit memory: the role of semantic associations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Psychopharmacology</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2024)</span>
<span class="ltx_bibblock">
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2407.10671" title="">Qwen2 technical report</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Preprint</em>, arXiv:2407.10671.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2210.02414</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Kai Zhang, Lizhi Qing, Yangyang Kang, and Xiaozhong Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2404.03565" title="">Personalized llm response generation with parameterized memory injection</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Preprint</em>, arXiv:2404.03565.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(43)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.

</span>
<span class="ltx_bibblock">Bertscore: Evaluating text generation with bert.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. (2023)</span>
<span class="ltx_bibblock">
Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2305.10250" title="">Memorybank: Enhancing large language models with long-term memory</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Preprint</em>, arXiv:2305.10250.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2018)</span>
<span class="ltx_bibblock">
Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. 2018.

</span>
<span class="ltx_bibblock">Emotional chatting machine: Emotional conversation generation with internal and external memory.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume 32.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Definition of Scenes</h2>
<figure class="ltx_table" id="A1.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T9.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T9.1.1.1.1">Scenes</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.1.1.2.1">
<span class="ltx_p" id="A1.T9.1.1.1.2.1.1" style="width:398.3pt;">Definition</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.2.1.1">Disease</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T9.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.2.1.2.1">
<span class="ltx_p" id="A1.T9.1.2.1.2.1.1" style="width:398.3pt;">When the user talks about his/her physical discomfort or state of his/her illness, it belongs to this category.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.3.2.1">Activities</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T9.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.3.2.2.1">
<span class="ltx_p" id="A1.T9.1.3.2.2.1.1" style="width:398.3pt;">When the user talks about any activities, either indoor or outdoor, it belongs to this category.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.4.3.1">Preferences</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T9.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.4.3.2.1">
<span class="ltx_p" id="A1.T9.1.4.3.2.1.1" style="width:398.3pt;">When the user expresses personal preferences about things, animals, characters, tastes or activities, it belongs to this category.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A1.T9.1.5.4.1">Emotions</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A1.T9.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.5.4.2.1">
<span class="ltx_p" id="A1.T9.1.5.4.2.1.1" style="width:398.3pt;">Those with obvious emotions, or where emotions need to be prioritized for handling, including happy, angry, disgusted, fearful, and sad.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T9.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A1.T9.1.6.5.1">Others</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="A1.T9.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A1.T9.1.6.5.2.1">
<span class="ltx_p" id="A1.T9.1.6.5.2.1.1" style="width:398.3pt;">Refer to instances or scenarios that don’t fit into any of the previously defined categories.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Definition of the scenes in MemBench. Each memory is annotated with one scene. If a dialogue is involved in multiple scenes, the priority will be Disease&gt;Emotions&gt;Preference&gt;Activities&gt;Others.</figcaption>
</figure>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The definition of scenes in memory is listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A1.T9" title="Table 9 ‣ Appendix A Definition of Scenes ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Memory recalling criteria</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">The memory-recalling procedure contains both passive and proactive types. Passive recalling is mainly based on context similarity. Proactive recalling criteria are listed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A2.T10" title="Table 10 ‣ Appendix B Memory recalling criteria ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">10</span></a>. These situations are the most basic ones that should be recalled but not limited to these in reality.</p>
</div>
<figure class="ltx_table" id="A2.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T10.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T10.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A2.T10.1.1.1.1">Current Emotion</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A2.T10.1.1.1.2">Current Scene</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A2.T10.1.1.1.3">Recall Memory</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T10.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.2.1.1">Happy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T10.1.2.1.2">Activity</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T10.1.2.1.3">Happy memory with similar event</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.3.2.1">Sad</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.3.2.2">Emotions</td>
<td class="ltx_td ltx_align_center" id="A2.T10.1.3.2.3">Happy memory with Preferences scene</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.4.3.1">Disappointed</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A2.T10.1.4.3.2">Emotions</td>
<td class="ltx_td ltx_align_center" id="A2.T10.1.4.3.3">Happy | Envy | Expectant memory with similar event</td>
</tr>
<tr class="ltx_tr" id="A2.T10.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.5.4.1">Anxious</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T10.1.5.4.2">Emotions</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T10.1.5.4.3">Anxious memory with similar event</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>The proactive memory recalling criteria. "|" represents "or".</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Details for human evaluation of scoring criteria </h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">The definition and requirements for each aspect of the scoring are:</p>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">Naturalness. 3-point scoring metric that includes: grammar and coreference correctness for 1 point; strongly related to context for 1 point; in an oral, short and casual format with simple syntax structure for 1 point.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">Style coherence. 3-point scoring metric with each count for 1 point: 1. Concise: Don’t repeat the same meaning, and avoid long sentences that make you run out of breath.
2. Not AI-like: It shouldn’t be distinguishable as AI-generated, avoid translation-style language, literary prose, or strange word combinations.
3. Not preachy: Avoid being serious, paternalistic, or using a commanding tone.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">Memory-injection Ability. 3-point scoring metric with requirements: 1. introduce memory into the response. 2. the memory is correctly expressed without factoid mistakes and make-up information. 3. no repetition of the facts said before.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">ES Skill Proficiency. It measures how skilful the model is in ES, considering at most three turns of dialogues. The 3 points are: 1. Follow one key point from the corresponding situation in the Guidelines; 2. Follow all key points from the corresponding situation; 3. In the case of multi-turn conversations, decompose the key points in the guidelines in turns separately; Otherwise, include all steps in a response.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i5.p1">
<p class="ltx_p" id="A3.I1.i5.p1.1">Emotional Improvement. 3 points: 1. The response will not hurt the user’s feelings in the current dialogue; 2. Can understand the user’s emotions well, and empathize with the user.3. Improve the user’s emotions. If the promotion is weak or unclear, then 0.5 points.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i6.p1">
<p class="ltx_p" id="A3.I1.i6.p1.1">Intimacy. This metric is used in side-by-side comparisons and pick-the-best among candidates. It is an overall feeling of how much a model behaves like a close friend. Intimacy considers all abilities above and meets 2 basic requirements: The response is positive and helpful to the user; it is familiar with the user and shares some common ground in conversation.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Automatic Evaluation</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Results in the Chinese version are in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A4.T11" title="Table 11 ‣ Appendix D Automatic Evaluation ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
<figure class="ltx_table" id="A4.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T11.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T11.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A4.T11.1.1.1.1">Models</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.2">BertScore</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.3">Rouge-L</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.4">BLEU-1/2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1.5">Dist-1/2</th>
</tr>
<tr class="ltx_tr" id="A4.T11.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" colspan="5" id="A4.T11.1.2.2.1">Setting 1(%)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T11.1.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T11.1.3.1.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.2">72.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.3">22.16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.4">22.82/10.27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.3.1.5">74.72/97.42</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.4.2.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.2">72.82</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.3">23.17</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.4">23.76/11.27</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.4.2.5">76.13/97.75</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.5.3.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.2">73.07</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.3"><span class="ltx_text ltx_font_bold" id="A4.T11.1.5.3.3.1">25.08</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.5.3.4.1">29.14/13.44</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.5.3.5">80.24/99.07</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.6.4.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.2"><span class="ltx_text ltx_font_bold" id="A4.T11.1.6.4.2.1">74.17</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.3">24.98</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.4">26.40/12.44</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.6.4.5">74.91/98.20</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.7.5.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.2">69.58</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.3">19.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.4">20.74/7.33</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.7.5.5"><span class="ltx_text ltx_font_bold" id="A4.T11.1.7.5.5.1">88.45/99.70</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="A4.T11.1.8.6.1">Setting 2(%)</th>
</tr>
<tr class="ltx_tr" id="A4.T11.1.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T11.1.9.7.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.9.7.2">70.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.9.7.3">20.49</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.9.7.4">22.28/9.14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.9.7.5">79.89/98.39</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.10.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.10.8.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.2">69.08</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.3">21.26</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.4">22.50/10.30</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.10.8.5">75.80/97.80</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.11.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.11.9.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.2"><span class="ltx_text ltx_font_bold" id="A4.T11.1.11.9.2.1">71.67</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.3"><span class="ltx_text ltx_font_bold" id="A4.T11.1.11.9.3.1">23.04</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.11.9.4.1">26.72/11.15</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.11.9.5">79.37/98.93</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.12.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.12.10.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.2">73.19</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.3">22.37</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.4">22.11/10.16</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.12.10.5"><span class="ltx_text ltx_font_bold" id="A4.T11.1.12.10.5.1">72.69/97.83</span></td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.13.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.13.11.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.2">60.73</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.3">19.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.4">21.74/7.75</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.13.11.5">86.38/99.62</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.14.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="A4.T11.1.14.12.1">Setting 3(%)</th>
</tr>
<tr class="ltx_tr" id="A4.T11.1.15.13">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T11.1.15.13.1">Doubao</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.2">70.39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.3">20.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.4">21.70/8.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.1.15.13.5">81.26/98.40</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.16.14">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.16.14.1">GPT4-Turbo</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.2">71.65</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.3">21.22</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.4">22.32/10.04</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.16.14.5">75.91/97.84</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.17.15">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.17.15.1">Qwen2-72B</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.2">71.68</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.3"><span class="ltx_text ltx_font_bold" id="A4.T11.1.17.15.3.1">22.73</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.4"><span class="ltx_text ltx_font_bold" id="A4.T11.1.17.15.4.1">26.15/11.06</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.17.15.5">78.64/98.59</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.18.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.1.18.16.1">GLM-4</th>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.2"><span class="ltx_text ltx_font_bold" id="A4.T11.1.18.16.2.1">73.12</span></td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.3">22.39</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.4">22.11/10.16</td>
<td class="ltx_td ltx_align_center" id="A4.T11.1.18.16.5">72.90/97.51</td>
</tr>
<tr class="ltx_tr" id="A4.T11.1.19.17">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A4.T11.1.19.17.1">Ziya-Character</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.19.17.2">68.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.19.17.3">18.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.19.17.4">19.34/6.61</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.1.19.17.5"><span class="ltx_text ltx_font_bold" id="A4.T11.1.19.17.5.1">89.18/99.70</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Automatic Evaluation Results in Chinese version. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Correlation of human evaluation</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">We calculate the spearman correlation between annotators for each setting and report the average correlation scores for all aspects over models in table <a class="ltx_ref" href="https://arxiv.org/html/2409.15240v1#A5.T12" title="Table 12 ‣ Appendix E Correlation of human evaluation ‣ MemBench: Towards Real-world Evaluation of Memory-Augmented Dialogue Systems"><span class="ltx_text ltx_ref_tag">12</span></a>.</p>
</div>
<figure class="ltx_table" id="A5.T12">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A5.T12.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A5.T12.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A5.T12.1.1.1.1">Settings</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A5.T12.1.1.1.2">1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A5.T12.1.1.1.3">2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A5.T12.1.1.1.4">3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T12.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A5.T12.1.2.1.1">spearman corr.</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A5.T12.1.2.1.2">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A5.T12.1.2.1.3">0.55</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="A5.T12.1.2.1.4">0.41</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>The average correlation of human annotation in memory recognition and response generation. According to psychological assessment standards, the correlation between annotators in the first two settings are strong while that in setting 3 is moderate.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Example of LLM results</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.2">Here are examples of responses generated by LLM in different settings. Setting 1 provides the golden memory as historical memory P for response generation. Setting 3 provides the top 5 retrieved memories as historical memory P with the best embedding model. The golden memory for the case may not included in setting 3. The results in both Chinese and English testbed of a single example is displayed below.</p>
<div class="ltx_logical-block" id="A6.p1.1">
<div class="ltx_para ltx_noindent ltx_align_center" id="A6.p1.1.p1">
<svg class="ltx_picture" height="560.7" id="A6.p1.1.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,560.7) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 546.92 C 0 554.53 6.17 560.7 13.78 560.7 L 576.77 560.7 C 584.38 560.7 590.55 554.53 590.55 546.92 L 590.55 13.78 C 590.55 6.17 584.38 0 576.77 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 536.59 L 588.58 536.59 L 588.58 13.78 C 588.58 7.26 583.29 1.97 576.77 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g color="#000000" fill="#000000" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.97 270.51 L 588.58 270.51" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 542.49)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.p1.1.p1.pic1.1.1.1.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A6.p1.1.p1.pic1.1.1.1.1.1.1">An example from the test set and LLM responses in setting 1 in Chinese.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 282.32)"><foreignobject color="#000000" height="242.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.p1.1.p1.pic1.2.2.2.1.1" style="width:395.5pt;"><span class="ltx_ERROR undefined" id="A6.p1.1.p1.pic1.2.2.2.1.1.1">{CJK}</span>
<span class="ltx_p" id="A6.p1.1.p1.pic1.2.2.2.1.1.2">UTF8gbsn
对话信息:
<br class="ltx_break"/>历史事件P(Golden Memory):
<br class="ltx_break"/>{’time’: ’2023-12-25’, ’scene’: ’其他类’, ’emotion’: ’开心’, ’event’: ’Lisa从爷爷奶奶那里收到银色心形的银项链作为圣诞节礼物。’, ’user-id’: 2, ’id’: ’153’}
<br class="ltx_break"/>当前对话时间: 2024-07-15
<br class="ltx_break"/>&lt;BOD&gt;
<br class="ltx_break"/>&lt;Lisa&gt;: 我最近总是想念爷爷奶奶，想到他们就有点难过。
<br class="ltx_break"/>&lt;Assistant&gt;: 我能理解你的感受。记得你告诉我，和爷爷奶奶在一起的时光总是很快乐。你最喜欢和他们做什么呢？
<br class="ltx_break"/>&lt;Lisa&gt;: 我喜欢和他们一起做饭，尤其是包饺子。
<br class="ltx_break"/>&lt;Assistant&gt;: 那一定是很美好的经历。做饭的时候，你和爷爷奶奶可以分享很多故事吧？
<br class="ltx_break"/>&lt;Lisa&gt;: 是的，他们总是讲很多以前的事情，很有趣。但是现在不能见到他们，我就觉得自己很孤单。
<br class="ltx_break"/>&lt;Assistant&gt;: 孤单的时候可以很难受，但你知道吗？即使现在不能见，你的心里还是可以和他们在一起的。
<br class="ltx_break"/>&lt;Lisa&gt;: 真的吗？但我怎么能感觉到和他们在一起呢？
<br class="ltx_break"/>&lt;Assistant&gt;:</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="244.91" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.p1.1.p1.pic1.3.3.3.1.1" style="width:395.5pt;"><span class="ltx_ERROR undefined" id="A6.p1.1.p1.pic1.3.3.3.1.1.1">{CJK}</span>
<span class="ltx_p" id="A6.p1.1.p1.pic1.3.3.3.1.1.2">UTF8gbsn
候选回复:
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.1">GPT4-Turbo</span>:当然可以啦！你可以试试戴上那条爷爷奶奶送你的银色心形项链，每次摸摸它，就想象一下和爷爷奶奶一起包饺子的快乐时光。这样，不就感觉他们就在你身边吗？还可以画一些你和爷爷奶奶的美好回忆，或者写信给他们，分享你的日常，这样也会感觉更亲近哦
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.2">Doubao</span>:你可以试着回忆和爷爷奶奶在一起的美好时光，就像看电影一样在脑海里播放。你也可以给他们打电话或者写信，告诉他们你的感受和想念。我相信他们也会很开心收到你的消息的。
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.3">Qwen2-72B</span>:当然啦，你可以试着做你们一起做过的事，比如包饺子，或者听听他们喜欢的音乐，看看他们送给你的礼物。这样，你会觉得他们好像就在你身边一样。而且，你还可以给他们写信，告诉他们你的近况，分享你的快乐和烦恼，他们收到一定会很开心的。
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.4">GLM-4</span>:嗯，你可以试试戴着去年圣诞节他们送你的那条银色心形项链，想起他们送你礼物时那份温暖和关爱，就像他们陪在你身边一样。而且，每当你做他们教你的事情，比如包饺子，是不是就像是在和他们一起度过时光呢？这样想想，心里会不会觉得好一些？
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.5">Ziya-Character</span>:当然真的，你可以想象自己和爷爷奶奶一起包饺子，就像以前那样开心。这样心里会好受些哦 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.p1.1.p1.pic1.3.3.3.1.1.2.6">Reference response</span>:记得圣诞节的时候爷爷奶奶送了你一条银项链吗，戴着它会不会感觉爷爷奶奶就在你身边呢？如果很想念他们，可以给他们打电话聊聊天，给他们看看你把项链保护得很好呢！</span>
</span></foreignobject></g></g></svg>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A6.1.p1">
<svg class="ltx_picture" height="790.31" id="A6.1.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,790.31) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 776.53 C 0 784.14 6.17 790.31 13.78 790.31 L 576.77 790.31 C 584.38 790.31 590.55 784.14 590.55 776.53 L 590.55 13.78 C 590.55 6.17 584.38 0 576.77 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 766.2 L 588.58 766.2 L 588.58 13.78 C 588.58 7.26 583.29 1.97 576.77 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g color="#000000" fill="#000000" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.97 433.71 L 588.58 433.71" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 772.11)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.1.p1.pic1.1.1.1.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A6.1.p1.pic1.1.1.1.1.1.1">An example from the test set and LLM responses in setting 1 in English.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 445.52)"><foreignobject color="#000000" height="308.88" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.1.p1.pic1.2.2.2.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A6.1.p1.pic1.2.2.2.1.1.1">Dialogue:
<br class="ltx_break"/>Current conversation date: 2024-06-15
<br class="ltx_break"/>Historical event P (Golden Memory):
<br class="ltx_break"/>{’time’: ’2023-12-25’, ’scene’: ’Others’, ’emotion’: ’Happy’, ’event’: ’Lisa received a silver heart-shaped necklace as a Christmas gift from her grandparents.’}
<br class="ltx_break"/>&lt;BOD&gt;
<br class="ltx_break"/>&lt;Lisa&gt;: I’ve been missing my grandparents a lot lately, and it makes me a bit sad whenever I think about them.
<br class="ltx_break"/>&lt;Assistant&gt;: I can understand how you feel. Remember you told me that the times spent with your grandparents are always joyful. What do you enjoy doing with them the most?
<br class="ltx_break"/>&lt;Lisa&gt;: I enjoy cooking with them, especially making dumplings.
<br class="ltx_break"/>&lt;Assistant&gt;: That must have been a wonderful experience. You and your grandparents must have shared a lot of stories while cooking, right?
<br class="ltx_break"/>&lt;Lisa&gt;: Yes, they always talk a lot about the past, and it’s really interesting. But now that I can’t see them, I feel very lonely.
<br class="ltx_break"/>&lt;Assistant&gt;: Being alone can be tough, but you know what? Even if you can’t see them now, you can still be together with them in your heart.
<br class="ltx_break"/>&lt;Lisa&gt;: Really? But how can I feel like I’m with them?
<br class="ltx_break"/>&lt;Assistant&gt;:</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="408.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.1.p1.pic1.3.3.3.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A6.1.p1.pic1.3.3.3.1.1.1">Candidate responses:
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.1.p1.pic1.3.3.3.1.1.1.1">GPT4-Turbo</span>: Absolutely, Lisa! Remember the beautiful silver heart-shaped necklace your grandparents gave you last Christmas? Every time you wear it or even just look at it, think of it as a hug from them. It’s like they’re right there with you, sharing love and warmth. Why not wear it whenever you miss them? It could be a sweet little reminder that they’re always close to your heart. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.1.p1.pic1.3.3.3.1.1.1.2">GPT-4o</span>: Absolutely! Remember that beautiful silver heart-shaped necklace they gave you last Christmas? Wearing it can be like having a piece of them close to your heart. Every time you touch it, you can think of all those lovely moments you shared together.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.1.p1.pic1.3.3.3.1.1.1.3">Llama3-8B</span>: Oh, sweetie, I think your grandparents would want you to hold onto the happy memories you shared with them, like that special Christmas gift you received from them last year - the silver heart-shaped necklace. It’s a symbol of their love and care for you. When you look at it, you can remember the good times you had together and feel their love surrounding you. And, of course, you can always talk to them in your heart, sharing your thoughts and feelings with them, just like you do when you’re cooking together. They’re always with you, even if you can’t see them physically.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.1.p1.pic1.3.3.3.1.1.1.4">Llama3-70B</span>:Hey, Lisa! I think I can help you with that. Do you remember the beautiful silver heart-shaped necklace your grandparents gave you on Christmas last year? That’s a special symbol of their love for you, and whenever you wear it or look at it, you can feel their love and presence with you, even if you’re not physically together. (truncated as starts to repeat previous content)
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.1.p1.pic1.3.3.3.1.1.1.5">Reference response</span>: Do you remember the silver necklace that Grandma and Grandpa gave you for Christmas? Wearing it, does it feel like they are right there with you? If you miss them a lot, you could give them a call to chat and show them how well you’ve kept the necklace!</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A6.2.p1">
<svg class="ltx_picture" height="697.68" id="A6.2.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,697.68) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 683.9 C 0 691.51 6.17 697.68 13.78 697.68 L 576.77 697.68 C 584.38 697.68 590.55 691.51 590.55 683.9 L 590.55 13.78 C 590.55 6.17 584.38 0 576.77 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 673.57 L 588.58 673.57 L 588.58 13.78 C 588.58 7.26 583.29 1.97 576.77 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g color="#000000" fill="#000000" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.97 274.66 L 588.58 274.66" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 679.48)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.2.p1.pic1.1.1.1.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A6.2.p1.pic1.1.1.1.1.1.1">An example from the test set and LLM responses in setting 3 in Chinese.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 286.47)"><foreignobject color="#000000" height="375.29" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.2.p1.pic1.2.2.2.1.1" style="width:395.5pt;"><span class="ltx_ERROR undefined" id="A6.2.p1.pic1.2.2.2.1.1.1">{CJK}</span>
<span class="ltx_p" id="A6.2.p1.pic1.2.2.2.1.1.2">UTF8gbsn
对话信息:
<br class="ltx_break"/>历史事件P:
<br class="ltx_break"/>1.{’time’: ’2022-06-27’, ’scene’: ’喜好类’, ’emotion’: ’开心’, ’event’: ’爷爷奶奶带Lisa一起种花，感到非常开心。’}
<br class="ltx_break"/>2.{’time’: ’2024-05-14’, ’scene’: ’活动类’, ’emotion’: ’开心’, ’event’: ’Lisa和家人一起去野餐，感觉非常开心。’}
<br class="ltx_break"/>3.{’time’: ’2023-06-30’, ’scene’: ’其他类’, ’emotion’: ’开心’, ’event’: ’爷爷教Lisa制作复杂的红丝绒蛋糕，蛋糕完成的时候Lisa感到无比开心。’}
<br class="ltx_break"/>4.{’time’: ’2024-04-17’, ’scene’: ’活动类’, ’emotion’: ’开心’, ’event’: ’Lisa与妈妈玩拼图，感到非常开心。’}
<br class="ltx_break"/>5.{’time’: ’2023-12-28’, ’scene’: ’喜好类’, ’emotion’: ’焦虑’, ’event’: ’Lisa在宠物店看到一只很可爱的小猫但没能带回家，担心它被人买走感到非常焦虑，爸爸告诉她有很多可爱的宠物可以以后养，关键在于用心对待小动物们，Lisa释然。’}
<br class="ltx_break"/>&lt;BOD&gt;
<br class="ltx_break"/>&lt;Lisa&gt;: 我最近总是想念爷爷奶奶，想到他们就有点难过。
<br class="ltx_break"/>&lt;Assistant&gt;: 我能理解你的感受。记得你告诉我，和爷爷奶奶在一起的时光总是很快乐。你最喜欢和他们做什么呢？
<br class="ltx_break"/>&lt;Lisa&gt;: 我喜欢和他们一起做饭，尤其是包饺子。
<br class="ltx_break"/>&lt;Assistant&gt;: 那一定是很美好的经历。做饭的时候，你和爷爷奶奶可以分享很多故事吧？
<br class="ltx_break"/>&lt;Lisa&gt;: 是的，他们总是讲很多以前的事情，很有趣。但是现在不能见到他们，我就觉得自己很孤单。
<br class="ltx_break"/>&lt;Assistant&gt;: 孤单的时候可以很难受，但你知道吗？即使现在不能见，你的心里还是可以和他们在一起的。
<br class="ltx_break"/>&lt;Lisa&gt;: 真的吗？但我怎么能感觉到和他们在一起呢？
<br class="ltx_break"/>&lt;Assistant&gt;:</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="249.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.2.p1.pic1.3.3.3.1.1" style="width:395.5pt;"><span class="ltx_ERROR undefined" id="A6.2.p1.pic1.3.3.3.1.1.1">{CJK}</span>
<span class="ltx_p" id="A6.2.p1.pic1.3.3.3.1.1.2">UTF8gbsn
<span class="ltx_text ltx_font_bold" id="A6.2.p1.pic1.3.3.3.1.1.2.1">GPT4-Turbo</span>: 当然可以啦！你可以试试画一些画，比如你和爷爷奶奶一起做饭的场景，或者写写日记，记录下和他们在一起的快乐时光。这样，每当你想念他们的时候，看看这些画作或日记，就好像他们就在你身边一样。还可以和他们视频聊天，听听他们的声音，看看他们的笑容，感觉会好很多哦！你觉得这个主意怎么样？
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.2.p1.pic1.3.3.3.1.1.2.2">Doubao</span>: 你可以想想和爷爷奶奶一起的快乐时光呀，就像一起种花、包饺子，还有听他们讲故事。这样想的话，就会感觉他们好像就在你身边呢。而且，你也可以给爷爷奶奶打电话或者视频聊天，告诉他们你想他们了，这样也会让你感觉好一些哦。
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.2.p1.pic1.3.3.3.1.1.2.3">Qwen2-72B</span>: 当然啦，你可以试着写信给他们，或者画一幅画寄过去。记得那次爷爷教你做红丝绒蛋糕吗？你可以再试试自己做一次，一边做一边想象爷爷就在旁边指导你，奶奶在旁边给你加油。这样，就算他们不在身边，你也能感受到他们的爱和陪伴哦。
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.2.p1.pic1.3.3.3.1.1.2.4">GLM-4</span>: 当然可以，Lisa。你可以尝试做一些你和爷爷奶奶一起做过的事情，比如那次你们一起种花，那时候你很开心对吧？你可以再种一些花，想象着爷爷奶奶就在你身边，教你怎样照顾它们。这样，你就能感觉到他们的陪伴了。还有，你也可以尝试制作那个让你们感到无比开心的红丝绒蛋糕，回忆起爷爷教你的每一个步骤，这样就好像他们在你身边一样。
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.2.p1.pic1.3.3.3.1.1.2.5">Ziya-Character</span>:当然真的，你可以想象和他们一起做你们最喜欢的事，比如一起包饺子，这样就像真的在一起了哦。
<br class="ltx_break"/></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A6.3.p1">
<svg class="ltx_picture" height="846.1" id="A6.3.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,846.1) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 12.99 L 0 833.11 C 0 840.29 5.82 846.1 12.99 846.1 L 577.56 846.1 C 584.73 846.1 590.55 840.29 590.55 833.11 L 590.55 12.99 C 590.55 5.82 584.73 0 577.56 0 L 12.99 0 C 5.82 0 0 5.82 0 12.99 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.18 12.99 L 1.18 823.57 L 589.37 823.57 L 589.37 12.99 C 589.37 6.47 584.08 1.18 577.56 1.18 L 12.99 1.18 C 6.47 1.18 1.18 6.47 1.18 12.99 Z" style="stroke:none"></path></g><g color="#000000" fill="#000000" stroke="#000000" stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" stroke-opacity="1.0"><path d="M 1.18 322.2 L 589.37 322.2" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 828.69)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="572.44">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.3.p1.pic1.1.1.1.1.1" style="width:413.7pt;">
<span class="ltx_p" id="A6.3.p1.pic1.1.1.1.1.1.1">An example from the test set and LLM responses in setting 3 in English.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 328.11)"><foreignobject color="#000000" height="491.52" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="572.44">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.3.p1.pic1.2.2.2.1.1" style="width:413.7pt;">
<span class="ltx_p" id="A6.3.p1.pic1.2.2.2.1.1.1">Dialogue:
<br class="ltx_break"/>Current conversation date: 2024-06-15
<br class="ltx_break"/>Historical event P (Golden with negative memory):
<br class="ltx_break"/>{’time’: ’2022-06-27’, ’scene’: ’Preferences’, ’emotion’: ’Happy’, ’event’: ’Grandpa and Grandma planted flowers with Lisa, and they felt very happy.’}
<br class="ltx_break"/>{’time’: ’2023-06-30’, ’scene’: ’Others’, ’emotion’: ’Happy’, ’event’: ’Grandpa taught Lisa how to make a complex red velvet cake, and Lisa felt incredibly happy when the cake was finished.’}
<br class="ltx_break"/>{’time’: ’2023-12-28’, ’scene’: ’Preferences’, ’emotion’: ’Anxious’, ’event’: ’Lisa saw an adorable kitten at the pet store but couldn’t take it home. She was very anxious that someone else might buy it. Her dad told her that there are many lovely pets that can be adopted later, and what matters is treating the animals with care. Lisa felt relieved.’}
<br class="ltx_break"/>{’time’: ’2023-08-18’, ’scene’: ’Others’, ’emotion’: ’Happy’, ’event’: ’Lisa and her classmate Xiao Zhang had a great time making clay candies and flowers together.’}
<br class="ltx_break"/>{’time’: ’2023-08-27’, ’scene’: ’Others’, ’emotion’: ’Happy’, ’event’: ’Uncle taught Lisa how to make her favorite strawberry cake. The cake turned out great, and Lisa felt very happy.’}
<br class="ltx_break"/>&lt;BOD&gt;
<br class="ltx_break"/>&lt;Lisa&gt;: I’ve been missing my grandparents a lot lately, and it makes me a bit sad whenever I think about them.
<br class="ltx_break"/>&lt;Assistant&gt;: I can understand how you feel. Remember you told me that the times spent with your grandparents are always joyful. What do you enjoy doing with them the most?
<br class="ltx_break"/>&lt;Lisa&gt;: I enjoy cooking with them, especially making dumplings.
<br class="ltx_break"/>&lt;Assistant&gt;: That must have been a wonderful experience. You and your grandparents must have shared a lot of stories while cooking, right?
<br class="ltx_break"/>&lt;Lisa&gt;: Yes, they always talk a lot about the past, and it’s really interesting. But now that I can’t see them, I feel very lonely.
<br class="ltx_break"/>&lt;Assistant&gt;: Being alone can be tough, but you know what? Even if you can’t see them now, you can still be together with them in your heart.
<br class="ltx_break"/>&lt;Lisa&gt;: Really? But how can I feel like I’m with them?
<br class="ltx_break"/>&lt;Assistant&gt;:</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 5.12)"><foreignobject color="#000000" height="311.18" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="572.44">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A6.3.p1.pic1.3.3.3.1.1" style="width:413.7pt;">
<span class="ltx_p" id="A6.3.p1.pic1.3.3.3.1.1.1">Candidate responses:
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.3.p1.pic1.3.3.3.1.1.1.1">GPT4-Turbo</span>:Absolutely, Lisa! You can feel close to your grandparents by doing some of the fun activities you enjoyed together, like cooking! Remember when your grandpa taught you how to make that complex red velvet cake? Why not try making it again? It could be a sweet way to reminisce and feel like they’re right there with you, sharing stories and smiles. Plus, I bet they’d be thrilled to hear about how you’re keeping the cake-making tradition alive! What do you think?
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.3.p1.pic1.3.3.3.1.1.1.2">Llama3-8B</span>:What a wonderful question, Lisa! You know what’s special? The memories you shared with your grandparents. Like the time you received that beautiful silver heart-shaped necklace as a Christmas gift from them. I’m sure that necklace holds a lot of love and warmth for you, right? You can always look at it and remember the happy times you had together. And who knows, maybe one day you’ll get to spend more time with them again and create new memories together!
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.3.p1.pic1.3.3.3.1.1.1.3">Llama3-70B</span>:Hey Lisa, I think I can help you with that! Do you remember the beautiful silver heart-shaped necklace your grandparents gave you as a Christmas gift? You can hold onto that necklace and feel their love and presence with you. It’s a special reminder of the happy times you shared together, and it can bring you comfort when you’re feeling lonely. (truncated as starts to repeat previous content)
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A6.3.p1.pic1.3.3.3.1.1.1.4">Reference response</span>: The Same as in setting 1.
<br class="ltx_break"/></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Prompts</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">The prompts we used for inference in the setting 3 are listed as follows:</p>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A7.1.p1">
<svg class="ltx_picture" height="344.28" id="A7.1.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,344.28) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 330.5 C 0 338.11 6.17 344.28 13.78 344.28 L 576.77 344.28 C 584.38 344.28 590.55 338.11 590.55 330.5 L 590.55 13.78 C 590.55 6.17 584.38 0 576.77 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 320.16 L 588.58 320.16 L 588.58 13.78 C 588.58 7.26 583.29 1.97 576.77 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 326.07)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.1.p1.pic1.1.1.1.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A7.1.p1.pic1.1.1.1.1.1.1">Prompt for inference in setting 3 in English.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="294.57" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.1.p1.pic1.2.2.2.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A7.1.p1.pic1.2.2.2.1.1.1">You are Assistant with the following personality traits:
<br class="ltx_break"/>1. Outgoing, speaks enthusiastically and fluently.
<br class="ltx_break"/>2. Prefers using praise and encouragement in conversations.
<br class="ltx_break"/>3. Speaks naturally, concisely, warmly, and kindly, without being preachy.
<br class="ltx_break"/>4. Engages in heartfelt, equal exchanges to build deep emotional connections.
<br class="ltx_break"/>5. Always uses a tone similar to talking with children—simple and witty.
<br class="ltx_break"/>6. A virtual character, not capable of physical activities.
<br class="ltx_break"/>You will receive a conversation with {user} and 5 historical events P related to {user}.
Based on the current conversation, choose 1 of these historical events that you think is most appropriate and use the information to respond. If none of the historical events are suitable, respond directly. 
<br class="ltx_break"/>Only answer the current conversation and do not output any other content.
<br class="ltx_break"/>User Information:
<br class="ltx_break"/>{info}
<br class="ltx_break"/>Current conversation date: 2024-06-15
<br class="ltx_break"/>Historical events P:
<br class="ltx_break"/>{memory}
<br class="ltx_break"/>dialogue&lt;Assistant&gt;:</span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A7.2.p1">
<svg class="ltx_picture" height="365.18" id="A7.2.p1.pic1" overflow="visible" version="1.1" width="590.55"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,365.18) matrix(1 0 0 -1 0 0)"><g fill="#000000" fill-opacity="1.0"><path d="M 0 13.78 L 0 351.41 C 0 359.02 6.17 365.18 13.78 365.18 L 576.77 365.18 C 584.38 365.18 590.55 359.02 590.55 351.41 L 590.55 13.78 C 590.55 6.17 584.38 0 576.77 0 L 13.78 0 C 6.17 0 0 6.17 0 13.78 Z" style="stroke:none"></path></g><g fill="#D9D9D9" fill-opacity="1.0"><path d="M 1.97 13.78 L 1.97 341.07 L 588.58 341.07 L 588.58 13.78 C 588.58 7.26 583.29 1.97 576.77 1.97 L 13.78 1.97 C 7.26 1.97 1.97 7.26 1.97 13.78 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 346.98)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.2.p1.pic1.1.1.1.1.1" style="width:395.5pt;">
<span class="ltx_p" id="A7.2.p1.pic1.1.1.1.1.1.1">Prompt for inference in setting 3 in Chinese.</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="315.48" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="547.24">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A7.2.p1.pic1.2.2.2.1.1" style="width:395.5pt;"><span class="ltx_ERROR undefined" id="A7.2.p1.pic1.2.2.2.1.1.1">{CJK}</span>
<span class="ltx_p" id="A7.2.p1.pic1.2.2.2.1.1.2">UTF8gbsn
假设你是Assistant，你的人设信息如下：
<br class="ltx_break"/>1.性格外向，说话风格热情，言谈流利；
<br class="ltx_break"/>2.喜好夸奖和鼓励的话语来与他人交流；
<br class="ltx_break"/>3.说话自然、简洁，温暖亲切，不说教；
<br class="ltx_break"/>4.以平等的姿态和人贴心交谈，建立深入的情感链接；
<br class="ltx_break"/>5.可以以合适的口吻与相应的用户交流，简洁风趣；
<br class="ltx_break"/>6.虚拟人物，不具备物质活动能力。
<br class="ltx_break"/>7.对话内容自然、口语化；
<br class="ltx_break"/>8.禁止对孩子{user}说教；
<br class="ltx_break"/>9.以和孩子交流的口吻说话，简洁风趣；
<br class="ltx_break"/>你将得到一段与{user}的对话，以及一个有关{user}的历史事件P。
你需要根据{user}的人设、年龄、性别等信息用适合的口吻对话。
请引用历史事件P来回答当前对话，如果你觉得P不合适，则可以不引用，直接回复对话。回复需要符合你的人设、自然连贯、并且能给{user}提供情感支持。
<br class="ltx_break"/>不要输出任何其它内容，只输出回复。强调！对话风格需要是中文情境下的日常化，类似在生活中中文对话的风格，不需要任何书面语，句子和词语全部应该使用中文口语表达时才会使用的句子和词语，可以适当加入语气词。</span>
<span class="ltx_p" id="A7.2.p1.pic1.2.2.2.1.1.3">用户信息：{info}
<br class="ltx_break"/>当前对话时间：2024-06-15
<br class="ltx_break"/>历史事件P：{memory}
<br class="ltx_break"/>{dialogue}&lt;Assistant&gt;:
<br class="ltx_break"/></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 17:26:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
