<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2308.10511] Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis</title><meta property="og:description" content="Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2308.10511">

<!--Generated on Wed Feb 28 12:01:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Instant Segmentation,  Mask-RCNN,  DLA
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shrestha Datta
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Computer Science and Engineering</span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Shahjalal University of Science and Technology
<br class="ltx_break"></span>Sylhet, Bangladesh 
<br class="ltx_break">shresthadatta910@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md Adith Mollah
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Computer Science and Engineering</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">Shahjalal University of Science and Technology
<br class="ltx_break"></span>Sylhet, Bangladesh 
<br class="ltx_break">adibhasan35@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">                 Raisa Fairooz
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">                 <span id="id5.1.id1" class="ltx_text ltx_font_italic">Computer Science and Engineering</span>
<br class="ltx_break">                 <span id="id6.2.id2" class="ltx_text ltx_font_italic">Shahjalal University of Science and Technology
<br class="ltx_break"></span>                 Sylhet, Bangladesh 
<br class="ltx_break">                 raisafairoozshafa@gmail.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">                 Tariful Islam Fahim
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">                 <span id="id7.1.id1" class="ltx_text ltx_font_italic">Computer Science and Engineering</span>
<br class="ltx_break">                 <span id="id8.2.id2" class="ltx_text ltx_font_italic">Shahjalal University of Science and Technology
<br class="ltx_break"></span>                 Sylhet, Bangladesh 
<br class="ltx_break">                 tarifulislamfahim12@gmail.com
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Understanding digital documents is like solving a puzzle, especially historical ones. Document Layout Analysis (DLA) helps with this puzzle by dividing documents into sections like paragraphs, images, and tables. This is crucial for machines to read and understand these documents. In the DL Sprint 2.0 competition, we worked on understanding Bangla documents. We used a dataset called BaDLAD with lots of examples. We trained a special model called Mask R-CNN to help with this understanding. We made this model better by step-by-step hyperparameter tuning, and we achieved a good dice score of 0.889. However, not everything went perfectly. We tried using a model trained for English documents, but it didn’t fit well with Bangla. This showed us that each language has its own challenges. Our solution for the DL Sprint 2.0 is publicly available at https://www.kaggle.com/competitions/dlsprint2/discussion/432201 along with notebooks, weights, and inference notebook.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Instant Segmentation, Mask-RCNN, DLA

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deciphering the structure of intricate digital documents is a fundamental stride in transforming them into intelligible machine-readable formats, pivotal for practical applications. With the burgeoning advances in machine learning and the realm of deep neural networks, the task of deciphering and transcribing documents, especially those of historical significance, remains an intricate conundrum. This is where Document Layout Analysis (DLA) emerges as a beacon of understanding. DLA is a preprocessing phase of document transcription. It segments a document into semantic units such as paragraphs, text-boxes, images, and tables. This is essential for OCR, as it allows the OCR engine to correctly identify and extract text from documents.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The DL Sprint 2.0 competition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> was a challenge to develop a DLA system for Bangla documents. The competition dataset, BaDLAD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, contains 33,695 human-annotated document samples from six domains.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We approached this challenge by training a Mask R-CNN model<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> for instance segmentation. Mask R-CNN is a state-of-the-art object detection model that can be used to segment objects in images. We fine-tuned the Mask R-CNN model on the BaDLAD dataset, and we also used hyperparameter optimization to improve the performance of our model.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our final submission achieved a dice score of 0.88900. This is a competitive result, and it shows that our approach is a promising step toward developing a robust DLA system for Bangla documents.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In addition to our work on the Mask R-CNN model, we also experimented with using pre-trained weights from an English document layout analysis model. However, this did not yield a significant improvement in performance. We believe that this is because the English document layout analysis model was not trained on a dataset that is representative of the challenges of Bangla document layout analysis.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Dataset Overview</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In order to improve the performance of Bengali document layout analysis, a multi-domain large Bengali Document Layout Analysis Dataset: BaDLAD has been used to train on the respective model. The dataset contains 3,695 human-annotated document samples from six domains - i) books and magazines ii) public domain govt. documents iii) liberation war documents iv) new newspapers v) historical newspapers and vi) property deeds; with 710K polygon annotations for four unit types: text-box, paragraph, image, and table.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Model Overview</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">For instance, segmenting objects within images Mask R-CNN R 50 FPN 3x model architecture has been used
following COCO (Common Objects in Context) format. For efficiency and effectiveness in image analysis tasks,
ResNet-50 backbone architecture is used along with Feature Pyramid Network, assisting the model for better understanding objects at different scales within an image. Moreover, the model is trained for three times the standard number of epochs that are typically used during the training phase. Typically Mask R-CNN (M R-CNN) models are trained for 10000 to 50000 iterations.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Tuned Hyperparameters Synopsis</span>
</h3>

<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS1.4.1.1" class="ltx_text">II-C</span>1 </span>Base Learning Rate</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">The base learning rate, determines the step size of gradient descent when the model updates its parameters during model training.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS2.4.1.1" class="ltx_text">II-C</span>2 </span>Pretrained Weights</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The initial weights for the model to start training from. The model architecture loads the pre-trained model weights if mentioned, instead of untrained initialized weights.</p>
</div>
</section>
<section id="S2.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS3.4.1.1" class="ltx_text">II-C</span>3 </span>Maximum Iteration</h4>

<div id="S2.SS3.SSS3.p1" class="ltx_para">
<p id="S2.SS3.SSS3.p1.1" class="ltx_p">The maximum number of training iterations. The training process will stop after executing the specified maximum number of iterations.</p>
</div>
</section>
<section id="S2.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS4.4.1.1" class="ltx_text">II-C</span>4 </span>Gamma</h4>

<div id="S2.SS3.SSS4.p1" class="ltx_para">
<p id="S2.SS3.SSS4.p1.1" class="ltx_p">A factor by which the learning rate is multiplied after each step. It Controls the rate of learning rate decay after specified steps.</p>
</div>
</section>
<section id="S2.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS3.SSS5.4.1.1" class="ltx_text">II-C</span>5 </span>Warmup Iterations</h4>

<div id="S2.SS3.SSS5.p1" class="ltx_para">
<p id="S2.SS3.SSS5.p1.1" class="ltx_p">The number of warm-up iterations at the beginning of training where the learning rate gradually increases from a lower value to the base learning rate helps to avoid instability while training.</p>
</div>
</section>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Other Hyperparameters</span>
</h3>

<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS1.4.1.1" class="ltx_text">II-D</span>1 </span>Batch Size</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">In the mini-batch, the gradient descent is applied based on each batch. We have used the mini-batch method with a batch size of 8 images per batch.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS4.SSS2.4.1.1" class="ltx_text">II-D</span>2 </span>Workers</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">We have utilized 2 workers to run two batches in parallel.</p>
</div>
</section>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS5.4.1.1" class="ltx_text">II-E</span> </span><span id="S2.SS5.5.2" class="ltx_text ltx_font_italic">Performance Metric</span>
</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">In this section, we discuss the performance metric used to evaluate the effectiveness of our method. The Dice score is a commonly used performance metric for measuring the similarity between two sets. It is often used in medical image segmentation tasks to quantify the agreement between the predicted and ground truth segmentations.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para">
<p id="S2.SS5.p2.1" class="ltx_p">The Dice score (also known as the F1 score or the Sørensen-Dice coefficient) is defined as follows:</p>
</div>
<div id="S2.SS5.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\text{Dice Score}=\frac{2\times\text{Intersection}}{\text{Total Predicted}+\text{Total Ground Truth}}" display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mtext id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2a.cmml">Dice Score</mtext><mo id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.cmml">=</mo><mfrac id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><mrow id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mn id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.1.1.3.2.1" xref="S2.E1.m1.1.1.3.2.1.cmml">×</mo><mtext id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3a.cmml">Intersection</mtext></mrow><mrow id="S2.E1.m1.1.1.3.3" xref="S2.E1.m1.1.1.3.3.cmml"><mtext id="S2.E1.m1.1.1.3.3.2" xref="S2.E1.m1.1.1.3.3.2a.cmml">Total Predicted</mtext><mo id="S2.E1.m1.1.1.3.3.1" xref="S2.E1.m1.1.1.3.3.1.cmml">+</mo><mtext id="S2.E1.m1.1.1.3.3.3" xref="S2.E1.m1.1.1.3.3.3a.cmml">Total Ground Truth</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1"><eq id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"></eq><ci id="S2.E1.m1.1.1.2a.cmml" xref="S2.E1.m1.1.1.2"><mtext id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2">Dice Score</mtext></ci><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><divide id="S2.E1.m1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.3"></divide><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><times id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2.1"></times><cn type="integer" id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">2</cn><ci id="S2.E1.m1.1.1.3.2.3a.cmml" xref="S2.E1.m1.1.1.3.2.3"><mtext id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">Intersection</mtext></ci></apply><apply id="S2.E1.m1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.3.3"><plus id="S2.E1.m1.1.1.3.3.1.cmml" xref="S2.E1.m1.1.1.3.3.1"></plus><ci id="S2.E1.m1.1.1.3.3.2a.cmml" xref="S2.E1.m1.1.1.3.3.2"><mtext id="S2.E1.m1.1.1.3.3.2.cmml" xref="S2.E1.m1.1.1.3.3.2">Total Predicted</mtext></ci><ci id="S2.E1.m1.1.1.3.3.3a.cmml" xref="S2.E1.m1.1.1.3.3.3"><mtext id="S2.E1.m1.1.1.3.3.3.cmml" xref="S2.E1.m1.1.1.3.3.3">Total Ground Truth</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\text{Dice Score}=\frac{2\times\text{Intersection}}{\text{Total Predicted}+\text{Total Ground Truth}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p4" class="ltx_para">
<p id="S2.SS5.p4.1" class="ltx_p">Where:</p>
<table id="S4.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S2.Ex1.2.1.1.1" class="ltx_text ltx_markedasmath">Intersection</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex1.m2.1" class="ltx_Math" alttext="\displaystyle=\text{Number of overlapping pixels in}" display="inline"><semantics id="S2.Ex1.m2.1a"><mrow id="S2.Ex1.m2.1.1" xref="S2.Ex1.m2.1.1.cmml"><mi id="S2.Ex1.m2.1.1.2" xref="S2.Ex1.m2.1.1.2.cmml"></mi><mo id="S2.Ex1.m2.1.1.1" xref="S2.Ex1.m2.1.1.1.cmml">=</mo><mtext id="S2.Ex1.m2.1.1.3" xref="S2.Ex1.m2.1.1.3a.cmml">Number of overlapping pixels in</mtext></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m2.1b"><apply id="S2.Ex1.m2.1.1.cmml" xref="S2.Ex1.m2.1.1"><eq id="S2.Ex1.m2.1.1.1.cmml" xref="S2.Ex1.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex1.m2.1.1.2.cmml" xref="S2.Ex1.m2.1.1.2">absent</csymbol><ci id="S2.Ex1.m2.1.1.3a.cmml" xref="S2.Ex1.m2.1.1.3"><mtext id="S2.Ex1.m2.1.1.3.cmml" xref="S2.Ex1.m2.1.1.3">Number of overlapping pixels in</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle=\text{Number of overlapping pixels in}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S2.Ex2.4.3.2.1" class="ltx_text ltx_markedasmath">predicted and ground truth</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S2.Ex3.4.3.2.1" class="ltx_text ltx_markedasmath">segmentations</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S2.Ex4.2.1.1.1" class="ltx_text ltx_markedasmath">Total Predicted</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex4.m2.1" class="ltx_Math" alttext="\displaystyle=\text{Total number of pixels}" display="inline"><semantics id="S2.Ex4.m2.1a"><mrow id="S2.Ex4.m2.1.1" xref="S2.Ex4.m2.1.1.cmml"><mi id="S2.Ex4.m2.1.1.2" xref="S2.Ex4.m2.1.1.2.cmml"></mi><mo id="S2.Ex4.m2.1.1.1" xref="S2.Ex4.m2.1.1.1.cmml">=</mo><mtext id="S2.Ex4.m2.1.1.3" xref="S2.Ex4.m2.1.1.3a.cmml">Total number of pixels</mtext></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex4.m2.1b"><apply id="S2.Ex4.m2.1.1.cmml" xref="S2.Ex4.m2.1.1"><eq id="S2.Ex4.m2.1.1.1.cmml" xref="S2.Ex4.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex4.m2.1.1.2.cmml" xref="S2.Ex4.m2.1.1.2">absent</csymbol><ci id="S2.Ex4.m2.1.1.3a.cmml" xref="S2.Ex4.m2.1.1.3"><mtext id="S2.Ex4.m2.1.1.3.cmml" xref="S2.Ex4.m2.1.1.3">Total number of pixels</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex4.m2.1c">\displaystyle=\text{Total number of pixels}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S2.Ex5.4.3.2.1" class="ltx_text ltx_markedasmath">in predicted segmentation</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span id="S2.Ex6.2.1.1.1" class="ltx_text ltx_markedasmath">Total Ground Truth</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S2.Ex6.m2.1" class="ltx_Math" alttext="\displaystyle=\text{Total number of pixels}" display="inline"><semantics id="S2.Ex6.m2.1a"><mrow id="S2.Ex6.m2.1.1" xref="S2.Ex6.m2.1.1.cmml"><mi id="S2.Ex6.m2.1.1.2" xref="S2.Ex6.m2.1.1.2.cmml"></mi><mo id="S2.Ex6.m2.1.1.1" xref="S2.Ex6.m2.1.1.1.cmml">=</mo><mtext id="S2.Ex6.m2.1.1.3" xref="S2.Ex6.m2.1.1.3a.cmml">Total number of pixels</mtext></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex6.m2.1b"><apply id="S2.Ex6.m2.1.1.cmml" xref="S2.Ex6.m2.1.1"><eq id="S2.Ex6.m2.1.1.1.cmml" xref="S2.Ex6.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex6.m2.1.1.2.cmml" xref="S2.Ex6.m2.1.1.2">absent</csymbol><ci id="S2.Ex6.m2.1.1.3a.cmml" xref="S2.Ex6.m2.1.1.3"><mtext id="S2.Ex6.m2.1.1.3.cmml" xref="S2.Ex6.m2.1.1.3">Total number of pixels</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex6.m2.1c">\displaystyle=\text{Total number of pixels}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><span id="S2.Ex7.4.3.2.1" class="ltx_text ltx_markedasmath">in ground truth segmentation</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS5.p5" class="ltx_para">
<p id="S2.SS5.p5.1" class="ltx_p">The Dice score ranges from 0 to 1, with 1 indicating perfect overlap between the predicted and ground truth segmentations, and 0 indicating no overlap.

<br class="ltx_break">
<br class="ltx_break">Dice score provides a meaningful measure of segmentation accuracy, allowing us to assess the quality of our method’s predictions. Although dice score is a good measure for instance segmentation tasks of computer vision but it still has limitations in certain situations.</p>
</div>
</section>
<section id="S2.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS6.4.1.1" class="ltx_text">II-F</span> </span><span id="S2.SS6.5.2" class="ltx_text ltx_font_italic">Final Sequential Submission Approach</span>
</h3>

<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Sub.</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Pretrained Weight</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Tr. Split</span></th>
<th id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">B. LR</span></th>
<th id="S2.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Warmup Iter.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<td id="S2.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">1</td>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75%</td>
<td id="S2.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.007</td>
<td id="S2.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">100</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<td id="S2.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">2</td>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Output from sub. 1</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">75%</td>
<td id="S2.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.001</td>
<td id="S2.T1.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">100</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<td id="S2.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">3</td>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Output from sub. 2</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">75%</td>
<td id="S2.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.0005</td>
<td id="S2.T1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">0</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<td id="S2.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">4</td>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">Output from sub. 3</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">75%</td>
<td id="S2.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.00001</td>
<td id="S2.T1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">0</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<td id="S2.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">5</td>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">Output from sub. 4</td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">75%</td>
<td id="S2.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.000001</td>
<td id="S2.T1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">0</td>
</tr>
<tr id="S2.T1.1.7.6" class="ltx_tr">
<td id="S2.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">6</td>
<td id="S2.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Output from sub. 5</td>
<td id="S2.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">99%</td>
<td id="S2.T1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.000001</td>
<td id="S2.T1.1.7.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Training Configuration Part 1: Showing training hyperparameters pre-trained weights, train dataset percentage in train-test split(Tr. Split), Base learning Rate(B. LR), and initial warmup iterations(Warmup Iter.) for each submission Sequence(sub.)</figcaption>
</figure>
<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Sub.</span></th>
<th id="S2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">No. of Iter.</span></th>
<th id="S2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.1.1.1.3.1" class="ltx_text ltx_font_bold">Gamma</span></th>
<th id="S2.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S2.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">Dice Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.2.1" class="ltx_tr">
<th id="S2.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<td id="S2.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">22k</td>
<td id="S2.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.0001</td>
<td id="S2.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.88223</td>
</tr>
<tr id="S2.T2.1.3.2" class="ltx_tr">
<th id="S2.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">2</th>
<td id="S2.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">22k</td>
<td id="S2.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">0.0001</td>
<td id="S2.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">0.88783</td>
</tr>
<tr id="S2.T2.1.4.3" class="ltx_tr">
<th id="S2.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">3</th>
<td id="S2.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">22k</td>
<td id="S2.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">0.0001</td>
<td id="S2.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">0.88842</td>
</tr>
<tr id="S2.T2.1.5.4" class="ltx_tr">
<th id="S2.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">4</th>
<td id="S2.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">22k</td>
<td id="S2.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">0.00001</td>
<td id="S2.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">0.8887</td>
</tr>
<tr id="S2.T2.1.6.5" class="ltx_tr">
<th id="S2.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r">5</th>
<td id="S2.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">22k</td>
<td id="S2.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">0.00001</td>
<td id="S2.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">0.88894</td>
</tr>
<tr id="S2.T2.1.7.6" class="ltx_tr">
<th id="S2.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">6</th>
<td id="S2.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">5k</td>
<td id="S2.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.00001</td>
<td id="S2.T2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0.88900</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Training Configuration Part 2: Showing training hyperparameters number of iterations(No. of Iter.) and gamma for each submission Sequence(sub.) and corresponding dice score in percentage(Accuracy)</figcaption>
</figure>
<div id="S2.SS6.p1" class="ltx_para">
<p id="S2.SS6.p1.1" class="ltx_p">Our final submission sequence is decomposed into 6 steps, each of which has been tinkered with different hyperparameter values in order to improve the performance of the respective model on the provided dataset.

<br class="ltx_break"></p>
</div>
<div id="S2.SS6.p2" class="ltx_para">
<p id="S2.SS6.p2.1" class="ltx_p">In terms of the first submission of the final submission sequence, no pre-trained base model is injected for pre-trained weights. Rather, training commenced from scratch by considering a training split of 75%, a base learning rate of 0.007, a warmup iteration value of 100, a gamma value of .0001, and a maximum iteration of 22000 eventually providing dice score of 0.88223. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS6.p3" class="ltx_para">
<p id="S2.SS6.p3.1" class="ltx_p">Starting from the second submission sequence till the final one, a cumulative approach has been taken in terms of using pre-trained weights. In the case of the second submission, the first submission’s pre-trained model weight is injected along with one particular change for the base learning rate having the value of 0.001 and keeping every parameter the same as the previous one providing a dice score of 0.88783, huge improvements over the former one.

<br class="ltx_break"></p>
</div>
<div id="S2.SS6.p4" class="ltx_para">
<p id="S2.SS6.p4.1" class="ltx_p">The warmup iterations value has been considered from 100 to 0 from the 3rd submission sequence.
For the 3rd, 4th and 5th sequence of submissions, a similar cumulative approach has been applied for adding pre-trained weights of previous submissions to continue its training in the current submissions with updated hyperparameters. Only the learning rate has been different in these submissions eventually providing a dice score of 0.88894.

<br class="ltx_break"></p>
</div>
<div id="S2.SS6.p5" class="ltx_para">
<p id="S2.SS6.p5.1" class="ltx_p">Finally, in the last submission, a total of 110,000 iteration version of the model weights has been considered to continue training with a maximum iteration value of 5000 more iterations, train split of 99% and learning rate of 0.000001. So the final version of the submission is the result of the model being iterated over 115,000 times eventually providing the best dice score for our latest submission, which is 0.88900.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Results and Analysis</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will discuss and analyze our results with respect to the hyperparameters tuned at each step of the training. We tested our model on the test data provided by DL sprint 2.0 based on the dice score from the equation (<a href="#S2.E1" title="In II-E Performance Metric ‣ II Methodology ‣ Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). The results for each submission are given in the table <a href="#S2.T2" title="TABLE II ‣ II-F Final Sequential Submission Approach ‣ II Methodology ‣ Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.

<br class="ltx_break"></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">At the first step of our training, we tried training a model from scratch on the DL Sprint 2.0 dataset and training a Mask-RCNN model pre-trained on the PubLayNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Although using the pre-trained model gave a better score for the first 10,000 iterations it did not yield any better results on further training even a total of 22, 000 iterations compared to the trained model from scratch. The trained model from scratch for 22k iterations obtains a score of 0.88223 which we selected as our model for further training.

<br class="ltx_break"></p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">For the second step, as the model showed a high variation of performance at each iteration as we continued to increase our training iterations, so we decreased the initial learning rate to 0.001. Thus training for a further 22,000 iterations yielded a score of 0.88783.

<br class="ltx_break"></p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Continuing to our third step, as we continued training the model further on small learning rates the model continued to show slight improvement of score compared to larger similar learning rates of previous steps. So, we tried to keep the learning rate smaller at 0.0005 in this step. As larger learning rates showed variation in scores, we changed the number of warmup steps to 0, so that the learning rate does not increase with iterations initially. In this way, after training for further 22,000 iterations, we got a score of 0.88842.

<br class="ltx_break"></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">For the fourth and fifth steps, we continued to observe the same behavior of the learning rate causing us to decrease it further to 0.00001 for the fourth step and 0.000001 for the fifth step. As the learning rate is becoming too low and the model is showing stable improvements on further training, to keep the learning rate stable we decreased the gamma to 10 times making it 0.00001 for both steps. We continued training our model for 22,000 iterations on the fourth step and further for 22,000 iterations on the fifth step resulting in a score of 0.8887 and 0.88894 respectively.

<br class="ltx_break"></p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">In the sixth step of our continuation of training, we noticed a stable improvement in model performance just by increasing the training data making the training data to validation data ratio of 99:1 compared to the previous 75% of our training data. With this hyperparameter change and continuing our training for 5,000 iterations lead our model to improve to a score of 0.889.

<br class="ltx_break"></p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">In total, we trained our model for 115k iterations with varying hyperparameters at each step reaching a model performance dice score of 0.889. We can see that the model training shows a gradual increase in performance with increasing iterations and selecting proper hyperparameters after specific iterations as depicted in the table <a href="#S2.T2" title="TABLE II ‣ II-F Final Sequential Submission Approach ‣ II Methodology ‣ Performance Enhancement Leveraging Mask-RCNN on Bengali Document Layout Analysis" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. We can also see that increasing training data after some iteration increases the model performance.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we presented our approach to the DL Sprint 2.0 competition. We trained a Mask R-CNN model for instance segmentation on the BaDLAD dataset, and we also used hyperparameter optimization to improve the performance of our model. Our final submission achieved a dice score of 0.889.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">We believe that our approach is a promising step towards developing a robust DLA system for Bangla documents. However, there is still more work to be done. In the future, we plan to improve our approach by enhancing the dataset and by incorporating more advanced techniques.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We hope that our work has the potential to add value to the field of document layout analysis. DLA is a key technology for many applications, such as OCR, machine translation, and search. By developing better DLA systems for Bangla documents, we can make these applications more accessible to the millions of people who speak Bangla.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">We have observed that using proper hyperparameters can improve the model’s performance with further training. Also increasing the dataset also increases the performance of the model.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
CaseDrive.

</span>
<span class="ltx_bibblock">publaynet-models.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/CaseDrive/publaynet-models</span>, 2022.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Md Asif Haider, Salman Khondker, Sameen53, Sushmit, and Tahsin.

</span>
<span class="ltx_bibblock">Dl sprint 2.0 - buet cse fest 2023, 2023.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://kaggle.com/competitions/dlsprint2</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Computer Vision
(ICCV)</span>, pages 2980–2988, 2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Md. Istiak Hossain Shihab, Md. Rakibul Hasan, Mahfuzur Rahman Emon,
Syed Mobassir Hossen, Md. Nazmuddoha Ansary, Intesur Ahmed, Fazle Rabbi
Rakib, Shahriar Elahi Dhruvo, Souhardya Saha Dip, Akib Hasan Pavel,
Marsia Haque Meghla, Md. Rezwanul Haque, Sayma Sultana Chowdhury, Farig
Sadeque, Tahsin Reasat, Ahmed Imtiaz Humayun, and Asif Shahriyar Sushmit.

</span>
<span class="ltx_bibblock">Badlad: A large multi-domain bengali document layout analysis
dataset, 2023.

</span>
</li>
</ul>
</section>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"></p>
</div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2308.10510" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2308.10511" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2308.10511">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2308.10511" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2308.10512" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 12:01:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
