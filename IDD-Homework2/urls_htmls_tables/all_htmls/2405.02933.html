<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Relay Decoding: Concatenating Large Language Models for Machine Translation</title>
<!--Generated on Wed May 15 19:11:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.02933v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S1" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2.SS1" title="In 2 Approach â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Task Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2.SS2" title="In 2 Approach â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Concatenate Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2.SS3" title="In 2 Approach â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Training Strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS1" title="In 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS1.SSS0.Px1" title="In 3.1 Experimental Details â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS1.SSS0.Px2" title="In 3.1 Experimental Details â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS1.SSS0.Px3" title="In 3.1 Experimental Details â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS2" title="In 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS3" title="In 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS3.SSS0.Px1" title="In 3.3 Analysis â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Is it necessary to finetune the LLMs during training?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.SS3.SSS0.Px2" title="In 3.3 Analysis â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">How much data is required to complete the training of the mapping layer?</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S4" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S4.SS0.SSS0.Px1" title="In 4 Related Work â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">LLMs for Machine Translation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S4.SS0.SSS0.Px2" title="In 4 Related Work â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title">Concatenation of LLMs.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S5" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#A1" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Mapping Layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#A2" title="In Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Experiments System Settings and Evaluation Metric</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Relay Decoding: Concatenating Large Language Models for Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chengpeng Fu<sup class="ltx_sup" id="id1.1.id1">1,2</sup>, Xiaocheng Feng<sup class="ltx_sup" id="id2.2.id2">1,2</sup>, Yichong Huang<sup class="ltx_sup" id="id3.3.id3">1</sup>, Wenshuai Huo<sup class="ltx_sup" id="id4.4.id4">1,2</sup>,
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.id5">Baohang Li<sup class="ltx_sup" id="id5.5.id5.1">1</sup>, Hui Wang<sup class="ltx_sup" id="id5.5.id5.2">2</sup>, Bin Qin<sup class="ltx_sup" id="id5.5.id5.3">1,2</sup>, Ting Liu<sup class="ltx_sup" id="id5.5.id5.4">1,2</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">1</sup>Harbin Institution of Technology, Harbin, China 
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">2</sup>Pengcheng Laboratory, Shenzhen, China
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id8.8.id8">{cpfu,xcfeng,ychuang,baohangli,qinb,tliu}@ir.hit.edu.cn</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id9.9.id9">{huowsh,wangh06}@pcl.ac.cn
<br class="ltx_break"/></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id10.id1">Leveraging large language models for machine translation has demonstrated promising results. However, it does require the large language models to possess the capability of handling both the source and target languages in machine translation. When it is challenging to find large models that support the desired languages, resorting to continuous learning methods becomes a costly endeavor. To mitigate these expenses, we propose an innovative approach called <span class="ltx_text ltx_font_bold" id="id10.id1.1">RD</span> (<span class="ltx_text ltx_font_bold" id="id10.id1.2">R</span>elay <span class="ltx_text ltx_font_bold" id="id10.id1.3">D</span>ecoding), which entails concatenating two distinct large models that individually support the source and target languages. By incorporating a simple mapping layer to facilitate the connection between these two models and utilizing a limited amount of parallel data for training, we successfully achieve superior results in the machine translation task. Experimental results conducted on the Multi30k and WikiMatrix datasets validate the effectiveness of our proposed method.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The dataset and associated codes will be publicly available.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Relay Decoding: Concatenating Large Language Models for Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Chengpeng Fu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1,2</sup>, Xiaocheng Feng<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">1,2</sup>, Yichong Huang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">1</sup>, Wenshuai Huo<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">1,2</sup>,</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Baohang Li<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1">1</sup>, Hui Wang<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.2">2</sup>, Bin Qin<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.3">1,2</sup>, Ting Liu<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.4">1,2</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">1</sup>Harbin Institution of Technology, Harbin, China</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">2</sup>Pengcheng Laboratory, Shenzhen, China</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.1">{cpfu,xcfeng,ychuang,baohangli,qinb,tliu}@ir.hit.edu.cn</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.6.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.6.6.1.1">{huowsh,wangh06}@pcl.ac.cn</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S1.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>LLaMAâ€™s supported languages include English and French while Aquila mainly support English and Chinese. Both Aquila2 and LLaMA are not proficient in handling the Chinese to French translation task individually. In such cases, we can concatenate the two models to accomplish the translation task.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The remarkable capabilities of large language models (LLMs) with billions of parameters have been demonstrated across various tasks. Several studies have leveraged these LLMs to accomplish and enhance machine translation tasks <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib24" title="">2023b</a>; Li, <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib15" title="">2023</a>; Garcia etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib9" title="">2023</a>; Jiao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib12" title="">2023</a>; Lyu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib16" title="">2023</a>; Huang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib11" title="">2024</a>)</cite>. Through techniques such as In-Context Learning(ICL), Chain-of-Thought(COT) and Instructions Finetuning, these LLMs have been able to achieve translation abilities comparable to state-of-the-art machine translation systems.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.2">However, the use of LLMs for translation is still limited by the languages supported by these models. Frequently, it is challenging to find LLMs that can effectively support both the source language <math alttext="L_{a}" class="ltx_Math" display="inline" id="S1.p2.1.m1.1"><semantics id="S1.p2.1.m1.1a"><msub id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mi id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">L</mi><mi id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">subscript</csymbol><ci id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">ğ¿</ci><ci id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">L_{a}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and target languages <math alttext="L_{b}" class="ltx_Math" display="inline" id="S1.p2.2.m2.1"><semantics id="S1.p2.2.m2.1a"><msub id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml"><mi id="S1.p2.2.m2.1.1.2" xref="S1.p2.2.m2.1.1.2.cmml">L</mi><mi id="S1.p2.2.m2.1.1.3" xref="S1.p2.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><apply id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S1.p2.2.m2.1.1.1.cmml" xref="S1.p2.2.m2.1.1">subscript</csymbol><ci id="S1.p2.2.m2.1.1.2.cmml" xref="S1.p2.2.m2.1.1.2">ğ¿</ci><ci id="S1.p2.2.m2.1.1.3.cmml" xref="S1.p2.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">L_{b}</annotation><annotation encoding="application/x-llamapun" id="S1.p2.2.m2.1d">italic_L start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> simultaneously, which poses a significant limitation. In such a scenario, one direct approach is to further train the existing LLM, which primarily supports one language, to incorporate the capabilities of another language<cite class="ltx_cite ltx_citemacro_citep">(Cui etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib6" title="">2023</a>)</cite>. However, this requires an enormous amount of pretrained data and poses significant challenges due to the large framework of the model. Additionally, it is crucial to ensure that catastrophic forgetting does not occur, preserving the proficiency of the model in its original language.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Are there any strategies to mitigate the costly nature of continuous learning? We have observed that, in practice, it is relatively straightforward to acquire LLMs that excel exclusively in either the source or target languages. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, by concatenating these specialized language models, it becomes conceivable to achieve translation without incurring the exorbitant expenses associated with continuous learning. In exceptional scenarios, when confronted with languages that lack pre-existing LLMs, a viable approach involves training a monolingual large model from scratch for the specific language. Subsequently, employing a concatenation technique enables us to accomplish machine translation, while also circumventing the issue of catastrophic forgetting in continuous learning.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Drawing on these insights, we propose a simple yet effective method <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">RD</span> (<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">R</span>elay <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">D</span>ecoding) for large model concatenation to achieve machine translation, where each large model specifically supports the source and target languages of the translation task. RD utilizes a simple mapping layer to connect two LLMs, leveraging a small portion of parallel corpora to train this mapping layer. In our experiments conducted on datasets such as Multi30k and WikiMatrix, utilizing the LLaMA and Aquila2 models, we find that our approach surpasses the method of fine-tuning with a single large model. Furthermore, we observed significant improvements of over 3 BLEU points in certain language pairs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approach</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Task Description</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.10">For a translation task from Language <math alttext="L_{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">L_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> to Language <math alttext="L_{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><msub id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">L_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_L start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, when it is not possible to find a single large model that performs well for both languages simultaneously, we focus on finding a separate large model for each language that excels in that specific language. Let <math alttext="M_{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><msub id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">M</mi><mi id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">ğ‘€</ci><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">M_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> denote a large language model that excels in language <math alttext="L_{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">L_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">M</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">ğ‘€</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> denote another large language model that excels in language <math alttext="L_{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">L_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">italic_L start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>. RD aims to concatenate <math alttext="M_{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><msub id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">M</mi><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">ğ‘€</ci><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">M_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.1"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mi id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml">M</mi><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2">ğ‘€</ci><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> to achieve the translation task from <math alttext="L_{a}" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9.1"><semantics id="S2.SS1.p1.9.m9.1a"><msub id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">L_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">italic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> to <math alttext="L_{b}" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m10.1"><semantics id="S2.SS1.p1.10.m10.1a"><msub id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">L</mi><mi id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">ğ¿</ci><ci id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">L_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m10.1d">italic_L start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Concatenate Method</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.12">As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2.F2" title="Figure 2 â€£ 2.2 Concatenate Method â€£ 2 Approach â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, for a given sentence <math alttext="X=\{x^{1},x^{2},...,x^{K}\}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.4"><semantics id="S2.SS2.p1.1.m1.4a"><mrow id="S2.SS2.p1.1.m1.4.4" xref="S2.SS2.p1.1.m1.4.4.cmml"><mi id="S2.SS2.p1.1.m1.4.4.5" xref="S2.SS2.p1.1.m1.4.4.5.cmml">X</mi><mo id="S2.SS2.p1.1.m1.4.4.4" xref="S2.SS2.p1.1.m1.4.4.4.cmml">=</mo><mrow id="S2.SS2.p1.1.m1.4.4.3.3" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml"><mo id="S2.SS2.p1.1.m1.4.4.3.3.4" stretchy="false" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml">{</mo><msup id="S2.SS2.p1.1.m1.2.2.1.1.1" xref="S2.SS2.p1.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.2.2.1.1.1.2" xref="S2.SS2.p1.1.m1.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS2.p1.1.m1.2.2.1.1.1.3" xref="S2.SS2.p1.1.m1.2.2.1.1.1.3.cmml">1</mn></msup><mo id="S2.SS2.p1.1.m1.4.4.3.3.5" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msup id="S2.SS2.p1.1.m1.3.3.2.2.2" xref="S2.SS2.p1.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS2.p1.1.m1.3.3.2.2.2.2.cmml">x</mi><mn id="S2.SS2.p1.1.m1.3.3.2.2.2.3" xref="S2.SS2.p1.1.m1.3.3.2.2.2.3.cmml">2</mn></msup><mo id="S2.SS2.p1.1.m1.4.4.3.3.6" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><mi id="S2.SS2.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS2.p1.1.m1.1.1.cmml">â€¦</mi><mo id="S2.SS2.p1.1.m1.4.4.3.3.7" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml">,</mo><msup id="S2.SS2.p1.1.m1.4.4.3.3.3" xref="S2.SS2.p1.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS2.p1.1.m1.4.4.3.3.3.2" xref="S2.SS2.p1.1.m1.4.4.3.3.3.2.cmml">x</mi><mi id="S2.SS2.p1.1.m1.4.4.3.3.3.3" xref="S2.SS2.p1.1.m1.4.4.3.3.3.3.cmml">K</mi></msup><mo id="S2.SS2.p1.1.m1.4.4.3.3.8" stretchy="false" xref="S2.SS2.p1.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.4b"><apply id="S2.SS2.p1.1.m1.4.4.cmml" xref="S2.SS2.p1.1.m1.4.4"><eq id="S2.SS2.p1.1.m1.4.4.4.cmml" xref="S2.SS2.p1.1.m1.4.4.4"></eq><ci id="S2.SS2.p1.1.m1.4.4.5.cmml" xref="S2.SS2.p1.1.m1.4.4.5">ğ‘‹</ci><set id="S2.SS2.p1.1.m1.4.4.3.4.cmml" xref="S2.SS2.p1.1.m1.4.4.3.3"><apply id="S2.SS2.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1">superscript</csymbol><ci id="S2.SS2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.2.2.1.1.1.2">ğ‘¥</ci><cn id="S2.SS2.p1.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS2.p1.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS2.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2">superscript</csymbol><ci id="S2.SS2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS2.p1.1.m1.3.3.2.2.2.2">ğ‘¥</ci><cn id="S2.SS2.p1.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S2.SS2.p1.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">â€¦</ci><apply id="S2.SS2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS2.p1.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS2.p1.1.m1.4.4.3.3.3">superscript</csymbol><ci id="S2.SS2.p1.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS2.p1.1.m1.4.4.3.3.3.2">ğ‘¥</ci><ci id="S2.SS2.p1.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS2.p1.1.m1.4.4.3.3.3.3">ğ¾</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.4c">X=\{x^{1},x^{2},...,x^{K}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.4d">italic_X = { italic_x start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , â€¦ , italic_x start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT }</annotation></semantics></math> in language <math alttext="L_{a}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><msub id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml">L</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2">ğ¿</ci><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">L_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_L start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> containing <math alttext="K" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">K</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_K</annotation></semantics></math> tokens, we utilize <math alttext="M_{a}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">M</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">ğ‘€</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">M_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math> to decode and generate its corresponding representation, denoted as <math alttext="H\in\mathcal{R}^{K*D_{h}}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><mrow id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">H</mi><mo id="S2.SS2.p1.5.m5.1.1.1" xref="S2.SS2.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.5.m5.1.1.3.2" xref="S2.SS2.p1.5.m5.1.1.3.2.cmml">â„›</mi><mrow id="S2.SS2.p1.5.m5.1.1.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.cmml"><mi id="S2.SS2.p1.5.m5.1.1.3.3.2" xref="S2.SS2.p1.5.m5.1.1.3.3.2.cmml">K</mi><mo id="S2.SS2.p1.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.p1.5.m5.1.1.3.3.1.cmml">âˆ—</mo><msub id="S2.SS2.p1.5.m5.1.1.3.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.3.cmml"><mi id="S2.SS2.p1.5.m5.1.1.3.3.3.2" xref="S2.SS2.p1.5.m5.1.1.3.3.3.2.cmml">D</mi><mi id="S2.SS2.p1.5.m5.1.1.3.3.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.3.3.cmml">h</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><in id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1.1"></in><ci id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">ğ»</ci><apply id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2">â„›</ci><apply id="S2.SS2.p1.5.m5.1.1.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3"><times id="S2.SS2.p1.5.m5.1.1.3.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.1"></times><ci id="S2.SS2.p1.5.m5.1.1.3.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.2">ğ¾</ci><apply id="S2.SS2.p1.5.m5.1.1.3.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.3.3.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.3">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.3.3.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.3.2">ğ·</ci><ci id="S2.SS2.p1.5.m5.1.1.3.3.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3.3.3">â„</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">H\in\mathcal{R}^{K*D_{h}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">italic_H âˆˆ caligraphic_R start_POSTSUPERSCRIPT italic_K âˆ— italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>. <math alttext="D_{h}" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><msub id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2" xref="S2.SS2.p1.6.m6.1.1.2.cmml">D</mi><mi id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3.cmml">h</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2">ğ·</ci><ci id="S2.SS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">D_{h}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT</annotation></semantics></math> is the hidden states size of <math alttext="M_{a}" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.1"><semantics id="S2.SS2.p1.7.m7.1a"><msub id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.p1.7.m7.1.1.2" xref="S2.SS2.p1.7.m7.1.1.2.cmml">M</mi><mi id="S2.SS2.p1.7.m7.1.1.3" xref="S2.SS2.p1.7.m7.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.p1.7.m7.1.1.2">ğ‘€</ci><ci id="S2.SS2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">M_{a}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.1d">italic_M start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT</annotation></semantics></math>. Subsequently, we utilize a mapping function <math alttext="W_{p}\in\mathcal{R}^{D_{h}*D_{e}}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m8.1"><semantics id="S2.SS2.p1.8.m8.1a"><mrow id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml"><msub id="S2.SS2.p1.8.m8.1.1.2" xref="S2.SS2.p1.8.m8.1.1.2.cmml"><mi id="S2.SS2.p1.8.m8.1.1.2.2" xref="S2.SS2.p1.8.m8.1.1.2.2.cmml">W</mi><mi id="S2.SS2.p1.8.m8.1.1.2.3" xref="S2.SS2.p1.8.m8.1.1.2.3.cmml">p</mi></msub><mo id="S2.SS2.p1.8.m8.1.1.1" xref="S2.SS2.p1.8.m8.1.1.1.cmml">âˆˆ</mo><msup id="S2.SS2.p1.8.m8.1.1.3" xref="S2.SS2.p1.8.m8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.8.m8.1.1.3.2" xref="S2.SS2.p1.8.m8.1.1.3.2.cmml">â„›</mi><mrow id="S2.SS2.p1.8.m8.1.1.3.3" xref="S2.SS2.p1.8.m8.1.1.3.3.cmml"><msub id="S2.SS2.p1.8.m8.1.1.3.3.2" xref="S2.SS2.p1.8.m8.1.1.3.3.2.cmml"><mi id="S2.SS2.p1.8.m8.1.1.3.3.2.2" xref="S2.SS2.p1.8.m8.1.1.3.3.2.2.cmml">D</mi><mi id="S2.SS2.p1.8.m8.1.1.3.3.2.3" xref="S2.SS2.p1.8.m8.1.1.3.3.2.3.cmml">h</mi></msub><mo id="S2.SS2.p1.8.m8.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS2.p1.8.m8.1.1.3.3.1.cmml">âˆ—</mo><msub id="S2.SS2.p1.8.m8.1.1.3.3.3" xref="S2.SS2.p1.8.m8.1.1.3.3.3.cmml"><mi id="S2.SS2.p1.8.m8.1.1.3.3.3.2" xref="S2.SS2.p1.8.m8.1.1.3.3.3.2.cmml">D</mi><mi id="S2.SS2.p1.8.m8.1.1.3.3.3.3" xref="S2.SS2.p1.8.m8.1.1.3.3.3.3.cmml">e</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><apply id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1"><in id="S2.SS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1.1"></in><apply id="S2.SS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.2.1.cmml" xref="S2.SS2.p1.8.m8.1.1.2">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.2.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2.2">ğ‘Š</ci><ci id="S2.SS2.p1.8.m8.1.1.2.3.cmml" xref="S2.SS2.p1.8.m8.1.1.2.3">ğ‘</ci></apply><apply id="S2.SS2.p1.8.m8.1.1.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.3.1.cmml" xref="S2.SS2.p1.8.m8.1.1.3">superscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.3.2.cmml" xref="S2.SS2.p1.8.m8.1.1.3.2">â„›</ci><apply id="S2.SS2.p1.8.m8.1.1.3.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3"><times id="S2.SS2.p1.8.m8.1.1.3.3.1.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.1"></times><apply id="S2.SS2.p1.8.m8.1.1.3.3.2.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.2"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.3.3.2.1.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.2">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.3.3.2.2.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.2.2">ğ·</ci><ci id="S2.SS2.p1.8.m8.1.1.3.3.2.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.2.3">â„</ci></apply><apply id="S2.SS2.p1.8.m8.1.1.3.3.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.3.3.3.1.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.3">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.3.3.3.2.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.3.2">ğ·</ci><ci id="S2.SS2.p1.8.m8.1.1.3.3.3.3.cmml" xref="S2.SS2.p1.8.m8.1.1.3.3.3.3">ğ‘’</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">W_{p}\in\mathcal{R}^{D_{h}*D_{e}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m8.1d">italic_W start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT âˆˆ caligraphic_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT âˆ— italic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> to project the obtained hidden representation <math alttext="H" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m9.1"><semantics id="S2.SS2.p1.9.m9.1a"><mi id="S2.SS2.p1.9.m9.1.1" xref="S2.SS2.p1.9.m9.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.1b"><ci id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.9.m9.1d">italic_H</annotation></semantics></math> into the input space of <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS2.p1.10.m10.1"><semantics id="S2.SS2.p1.10.m10.1a"><msub id="S2.SS2.p1.10.m10.1.1" xref="S2.SS2.p1.10.m10.1.1.cmml"><mi id="S2.SS2.p1.10.m10.1.1.2" xref="S2.SS2.p1.10.m10.1.1.2.cmml">M</mi><mi id="S2.SS2.p1.10.m10.1.1.3" xref="S2.SS2.p1.10.m10.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.10.m10.1b"><apply id="S2.SS2.p1.10.m10.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.10.m10.1.1.1.cmml" xref="S2.SS2.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS2.p1.10.m10.1.1.2.cmml" xref="S2.SS2.p1.10.m10.1.1.2">ğ‘€</ci><ci id="S2.SS2.p1.10.m10.1.1.3.cmml" xref="S2.SS2.p1.10.m10.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.10.m10.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.10.m10.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>. <math alttext="D_{e}" class="ltx_Math" display="inline" id="S2.SS2.p1.11.m11.1"><semantics id="S2.SS2.p1.11.m11.1a"><msub id="S2.SS2.p1.11.m11.1.1" xref="S2.SS2.p1.11.m11.1.1.cmml"><mi id="S2.SS2.p1.11.m11.1.1.2" xref="S2.SS2.p1.11.m11.1.1.2.cmml">D</mi><mi id="S2.SS2.p1.11.m11.1.1.3" xref="S2.SS2.p1.11.m11.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.11.m11.1b"><apply id="S2.SS2.p1.11.m11.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.11.m11.1.1.1.cmml" xref="S2.SS2.p1.11.m11.1.1">subscript</csymbol><ci id="S2.SS2.p1.11.m11.1.1.2.cmml" xref="S2.SS2.p1.11.m11.1.1.2">ğ·</ci><ci id="S2.SS2.p1.11.m11.1.1.3.cmml" xref="S2.SS2.p1.11.m11.1.1.3">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.11.m11.1c">D_{e}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.11.m11.1d">italic_D start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> is the embedding layer size of <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS2.p1.12.m12.1"><semantics id="S2.SS2.p1.12.m12.1a"><msub id="S2.SS2.p1.12.m12.1.1" xref="S2.SS2.p1.12.m12.1.1.cmml"><mi id="S2.SS2.p1.12.m12.1.1.2" xref="S2.SS2.p1.12.m12.1.1.2.cmml">M</mi><mi id="S2.SS2.p1.12.m12.1.1.3" xref="S2.SS2.p1.12.m12.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.12.m12.1b"><apply id="S2.SS2.p1.12.m12.1.1.cmml" xref="S2.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.12.m12.1.1.1.cmml" xref="S2.SS2.p1.12.m12.1.1">subscript</csymbol><ci id="S2.SS2.p1.12.m12.1.1.2.cmml" xref="S2.SS2.p1.12.m12.1.1.2">ğ‘€</ci><ci id="S2.SS2.p1.12.m12.1.1.3.cmml" xref="S2.SS2.p1.12.m12.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.12.m12.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.12.m12.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>. For the sake of simplicity and efficiency, we employ a linear layer as the mapping layer<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We also attempt alternative methods of connecting the structures, which are documented in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#A1" title="Appendix A Mapping Layers â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span>, similar to the connection methods used in many multi-modal large models <cite class="ltx_cite ltx_citemacro_citep">(Koh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib13" title="">2023</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib25" title="">2023c</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib23" title="">a</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S2.F2.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Using Chinese-French translation as a case in point for the process of Relay Decoding.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Next, we introduce a prompt to facilitate better generation by <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">M</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ‘€</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>. When the source language is Chinese and the target is English, the prompt would be as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\#\#\#[Chinese]:X\quad\#\#\#[English]:" class="ltx_Math" display="block" id="S2.Ex1.m1.3"><semantics id="S2.Ex1.m1.3a"><mrow id="S2.Ex1.m1.3.3" xref="S2.Ex1.m1.3.3.cmml"><mrow id="S2.Ex1.m1.2.2.1" xref="S2.Ex1.m1.2.2.1.cmml"><mi id="S2.Ex1.m1.2.2.1.3" mathvariant="normal" xref="S2.Ex1.m1.2.2.1.3.cmml">#</mi><mo id="S2.Ex1.m1.2.2.1.2" xref="S2.Ex1.m1.2.2.1.2.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.4" mathvariant="normal" xref="S2.Ex1.m1.2.2.1.4.cmml">#</mi><mo id="S2.Ex1.m1.2.2.1.2a" xref="S2.Ex1.m1.2.2.1.2.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.5" mathvariant="normal" xref="S2.Ex1.m1.2.2.1.5.cmml">#</mi><mo id="S2.Ex1.m1.2.2.1.2b" xref="S2.Ex1.m1.2.2.1.2.cmml">â¢</mo><mrow id="S2.Ex1.m1.2.2.1.1.1" xref="S2.Ex1.m1.2.2.1.1.2.cmml"><mo id="S2.Ex1.m1.2.2.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.2.2.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.2.2.1.1.1.1" xref="S2.Ex1.m1.2.2.1.1.1.1.cmml"><mi id="S2.Ex1.m1.2.2.1.1.1.1.2" xref="S2.Ex1.m1.2.2.1.1.1.1.2.cmml">C</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.3" xref="S2.Ex1.m1.2.2.1.1.1.1.3.cmml">h</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1a" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.4" xref="S2.Ex1.m1.2.2.1.1.1.1.4.cmml">i</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1b" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.5" xref="S2.Ex1.m1.2.2.1.1.1.1.5.cmml">n</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1c" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.6" xref="S2.Ex1.m1.2.2.1.1.1.1.6.cmml">e</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1d" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.7" xref="S2.Ex1.m1.2.2.1.1.1.1.7.cmml">s</mi><mo id="S2.Ex1.m1.2.2.1.1.1.1.1e" xref="S2.Ex1.m1.2.2.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.2.2.1.1.1.1.8" xref="S2.Ex1.m1.2.2.1.1.1.1.8.cmml">e</mi></mrow><mo id="S2.Ex1.m1.2.2.1.1.1.3" rspace="0.278em" stretchy="false" xref="S2.Ex1.m1.2.2.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.Ex1.m1.3.3.4" rspace="0.278em" xref="S2.Ex1.m1.3.3.4.cmml">:</mo><mrow id="S2.Ex1.m1.3.3.2.1" xref="S2.Ex1.m1.3.3.2.2.cmml"><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">X</mi><mspace id="S2.Ex1.m1.3.3.2.1.2" width="1em" xref="S2.Ex1.m1.3.3.2.2.cmml"></mspace><mrow id="S2.Ex1.m1.3.3.2.1.1" xref="S2.Ex1.m1.3.3.2.1.1.cmml"><mi id="S2.Ex1.m1.3.3.2.1.1.3" mathvariant="normal" xref="S2.Ex1.m1.3.3.2.1.1.3.cmml">#</mi><mo id="S2.Ex1.m1.3.3.2.1.1.2" xref="S2.Ex1.m1.3.3.2.1.1.2.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.4" mathvariant="normal" xref="S2.Ex1.m1.3.3.2.1.1.4.cmml">#</mi><mo id="S2.Ex1.m1.3.3.2.1.1.2a" xref="S2.Ex1.m1.3.3.2.1.1.2.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.5" mathvariant="normal" xref="S2.Ex1.m1.3.3.2.1.1.5.cmml">#</mi><mo id="S2.Ex1.m1.3.3.2.1.1.2b" xref="S2.Ex1.m1.3.3.2.1.1.2.cmml">â¢</mo><mrow id="S2.Ex1.m1.3.3.2.1.1.1.1" xref="S2.Ex1.m1.3.3.2.1.1.1.2.cmml"><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.2" stretchy="false" xref="S2.Ex1.m1.3.3.2.1.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.3.3.2.1.1.1.1.1" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.2.cmml">E</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.3.cmml">n</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1a" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.4" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.4.cmml">g</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1b" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.5" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.5.cmml">l</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1c" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.6" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.6.cmml">i</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1d" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.7" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.7.cmml">s</mi><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1e" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml">â¢</mo><mi id="S2.Ex1.m1.3.3.2.1.1.1.1.1.8" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.8.cmml">h</mi></mrow><mo id="S2.Ex1.m1.3.3.2.1.1.1.1.3" rspace="0.278em" stretchy="false" xref="S2.Ex1.m1.3.3.2.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S2.Ex1.m1.3.3.5" rspace="0.278em" xref="S2.Ex1.m1.3.3.5.cmml">:</mo><mi id="S2.Ex1.m1.3.3.6" xref="S2.Ex1.m1.3.3.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.3b"><apply id="S2.Ex1.m1.3.3.cmml" xref="S2.Ex1.m1.3.3"><and id="S2.Ex1.m1.3.3a.cmml" xref="S2.Ex1.m1.3.3"></and><apply id="S2.Ex1.m1.3.3b.cmml" xref="S2.Ex1.m1.3.3"><ci id="S2.Ex1.m1.3.3.4.cmml" xref="S2.Ex1.m1.3.3.4">:</ci><apply id="S2.Ex1.m1.2.2.1.cmml" xref="S2.Ex1.m1.2.2.1"><times id="S2.Ex1.m1.2.2.1.2.cmml" xref="S2.Ex1.m1.2.2.1.2"></times><ci id="S2.Ex1.m1.2.2.1.3.cmml" xref="S2.Ex1.m1.2.2.1.3">#</ci><ci id="S2.Ex1.m1.2.2.1.4.cmml" xref="S2.Ex1.m1.2.2.1.4">#</ci><ci id="S2.Ex1.m1.2.2.1.5.cmml" xref="S2.Ex1.m1.2.2.1.5">#</ci><apply id="S2.Ex1.m1.2.2.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.2.2.1.1.2.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.2.2.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1"><times id="S2.Ex1.m1.2.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.1"></times><ci id="S2.Ex1.m1.2.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.2">ğ¶</ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.3">â„</ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.4.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.4">ğ‘–</ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.5.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.5">ğ‘›</ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.6.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.6">ğ‘’</ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.7.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.7">ğ‘ </ci><ci id="S2.Ex1.m1.2.2.1.1.1.1.8.cmml" xref="S2.Ex1.m1.2.2.1.1.1.1.8">ğ‘’</ci></apply></apply></apply><list id="S2.Ex1.m1.3.3.2.2.cmml" xref="S2.Ex1.m1.3.3.2.1"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">ğ‘‹</ci><apply id="S2.Ex1.m1.3.3.2.1.1.cmml" xref="S2.Ex1.m1.3.3.2.1.1"><times id="S2.Ex1.m1.3.3.2.1.1.2.cmml" xref="S2.Ex1.m1.3.3.2.1.1.2"></times><ci id="S2.Ex1.m1.3.3.2.1.1.3.cmml" xref="S2.Ex1.m1.3.3.2.1.1.3">#</ci><ci id="S2.Ex1.m1.3.3.2.1.1.4.cmml" xref="S2.Ex1.m1.3.3.2.1.1.4">#</ci><ci id="S2.Ex1.m1.3.3.2.1.1.5.cmml" xref="S2.Ex1.m1.3.3.2.1.1.5">#</ci><apply id="S2.Ex1.m1.3.3.2.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.3.3.2.1.1.1.2.1.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.3.3.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1"><times id="S2.Ex1.m1.3.3.2.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.1"></times><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.2">ğ¸</ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.3">ğ‘›</ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.4.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.4">ğ‘”</ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.5.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.5">ğ‘™</ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.6.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.6">ğ‘–</ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.7.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.7">ğ‘ </ci><ci id="S2.Ex1.m1.3.3.2.1.1.1.1.1.8.cmml" xref="S2.Ex1.m1.3.3.2.1.1.1.1.1.8">â„</ci></apply></apply></apply></list></apply><apply id="S2.Ex1.m1.3.3c.cmml" xref="S2.Ex1.m1.3.3"><ci id="S2.Ex1.m1.3.3.5.cmml" xref="S2.Ex1.m1.3.3.5">:</ci><share href="https://arxiv.org/html/2405.02933v1#S2.Ex1.m1.3.3.2.cmml" id="S2.Ex1.m1.3.3d.cmml" xref="S2.Ex1.m1.3.3"></share><csymbol cd="latexml" id="S2.Ex1.m1.3.3.6.cmml" xref="S2.Ex1.m1.3.3.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.3c">\#\#\#[Chinese]:X\quad\#\#\#[English]:</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.3d"># # # [ italic_C italic_h italic_i italic_n italic_e italic_s italic_e ] : italic_X # # # [ italic_E italic_n italic_g italic_l italic_i italic_s italic_h ] :</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The pattern <math alttext="\#\#\#[\star]" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.2" xref="S2.SS2.p3.1.m1.1.2.cmml"><mi id="S2.SS2.p3.1.m1.1.2.2" mathvariant="normal" xref="S2.SS2.p3.1.m1.1.2.2.cmml">#</mi><mo id="S2.SS2.p3.1.m1.1.2.1" xref="S2.SS2.p3.1.m1.1.2.1.cmml">â¢</mo><mi id="S2.SS2.p3.1.m1.1.2.3" mathvariant="normal" xref="S2.SS2.p3.1.m1.1.2.3.cmml">#</mi><mo id="S2.SS2.p3.1.m1.1.2.1a" xref="S2.SS2.p3.1.m1.1.2.1.cmml">â¢</mo><mi id="S2.SS2.p3.1.m1.1.2.4" mathvariant="normal" xref="S2.SS2.p3.1.m1.1.2.4.cmml">#</mi><mo id="S2.SS2.p3.1.m1.1.2.1b" xref="S2.SS2.p3.1.m1.1.2.1.cmml">â¢</mo><mrow id="S2.SS2.p3.1.m1.1.2.5.2" xref="S2.SS2.p3.1.m1.1.2.5.1.cmml"><mo id="S2.SS2.p3.1.m1.1.2.5.2.1" stretchy="false" xref="S2.SS2.p3.1.m1.1.2.5.1.1.cmml">[</mo><mo id="S2.SS2.p3.1.m1.1.1" lspace="0em" rspace="0em" xref="S2.SS2.p3.1.m1.1.1.cmml">â‹†</mo><mo id="S2.SS2.p3.1.m1.1.2.5.2.2" stretchy="false" xref="S2.SS2.p3.1.m1.1.2.5.1.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.2"><times id="S2.SS2.p3.1.m1.1.2.1.cmml" xref="S2.SS2.p3.1.m1.1.2.1"></times><ci id="S2.SS2.p3.1.m1.1.2.2.cmml" xref="S2.SS2.p3.1.m1.1.2.2">#</ci><ci id="S2.SS2.p3.1.m1.1.2.3.cmml" xref="S2.SS2.p3.1.m1.1.2.3">#</ci><ci id="S2.SS2.p3.1.m1.1.2.4.cmml" xref="S2.SS2.p3.1.m1.1.2.4">#</ci><apply id="S2.SS2.p3.1.m1.1.2.5.1.cmml" xref="S2.SS2.p3.1.m1.1.2.5.2"><csymbol cd="latexml" id="S2.SS2.p3.1.m1.1.2.5.1.1.cmml" xref="S2.SS2.p3.1.m1.1.2.5.2.1">delimited-[]</csymbol><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">â‹†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\#\#\#[\star]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d"># # # [ â‹† ]</annotation></semantics></math> is employed to denote the name of the specific language. In our case, we use the target language to replace these patterns.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.5">After tokenizing the prompt and passing it through the embedding layer of <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS2.p4.1.m1.1"><semantics id="S2.SS2.p4.1.m1.1a"><msub id="S2.SS2.p4.1.m1.1.1" xref="S2.SS2.p4.1.m1.1.1.cmml"><mi id="S2.SS2.p4.1.m1.1.1.2" xref="S2.SS2.p4.1.m1.1.1.2.cmml">M</mi><mi id="S2.SS2.p4.1.m1.1.1.3" xref="S2.SS2.p4.1.m1.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.m1.1b"><apply id="S2.SS2.p4.1.m1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.1.m1.1.1.1.cmml" xref="S2.SS2.p4.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p4.1.m1.1.1.2.cmml" xref="S2.SS2.p4.1.m1.1.1.2">ğ‘€</ci><ci id="S2.SS2.p4.1.m1.1.1.3.cmml" xref="S2.SS2.p4.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.m1.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.m1.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, we obtain the prompt input representation <math alttext="E_{p}" class="ltx_Math" display="inline" id="S2.SS2.p4.2.m2.1"><semantics id="S2.SS2.p4.2.m2.1a"><msub id="S2.SS2.p4.2.m2.1.1" xref="S2.SS2.p4.2.m2.1.1.cmml"><mi id="S2.SS2.p4.2.m2.1.1.2" xref="S2.SS2.p4.2.m2.1.1.2.cmml">E</mi><mi id="S2.SS2.p4.2.m2.1.1.3" xref="S2.SS2.p4.2.m2.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.2.m2.1b"><apply id="S2.SS2.p4.2.m2.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.2.m2.1.1.1.cmml" xref="S2.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p4.2.m2.1.1.2.cmml" xref="S2.SS2.p4.2.m2.1.1.2">ğ¸</ci><ci id="S2.SS2.p4.2.m2.1.1.3.cmml" xref="S2.SS2.p4.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.2.m2.1c">E_{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.2.m2.1d">italic_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. Finally, we concatenate the mapped representations <math alttext="H" class="ltx_Math" display="inline" id="S2.SS2.p4.3.m3.1"><semantics id="S2.SS2.p4.3.m3.1a"><mi id="S2.SS2.p4.3.m3.1.1" xref="S2.SS2.p4.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.3.m3.1b"><ci id="S2.SS2.p4.3.m3.1.1.cmml" xref="S2.SS2.p4.3.m3.1.1">ğ»</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.3.m3.1c">H</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.3.m3.1d">italic_H</annotation></semantics></math> with the prompt representations <math alttext="E_{p}" class="ltx_Math" display="inline" id="S2.SS2.p4.4.m4.1"><semantics id="S2.SS2.p4.4.m4.1a"><msub id="S2.SS2.p4.4.m4.1.1" xref="S2.SS2.p4.4.m4.1.1.cmml"><mi id="S2.SS2.p4.4.m4.1.1.2" xref="S2.SS2.p4.4.m4.1.1.2.cmml">E</mi><mi id="S2.SS2.p4.4.m4.1.1.3" xref="S2.SS2.p4.4.m4.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.4.m4.1b"><apply id="S2.SS2.p4.4.m4.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.4.m4.1.1.1.cmml" xref="S2.SS2.p4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p4.4.m4.1.1.2.cmml" xref="S2.SS2.p4.4.m4.1.1.2">ğ¸</ci><ci id="S2.SS2.p4.4.m4.1.1.3.cmml" xref="S2.SS2.p4.4.m4.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.4.m4.1c">E_{p}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.4.m4.1d">italic_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> and feed them into <math alttext="M_{b}" class="ltx_Math" display="inline" id="S2.SS2.p4.5.m5.1"><semantics id="S2.SS2.p4.5.m5.1a"><msub id="S2.SS2.p4.5.m5.1.1" xref="S2.SS2.p4.5.m5.1.1.cmml"><mi id="S2.SS2.p4.5.m5.1.1.2" xref="S2.SS2.p4.5.m5.1.1.2.cmml">M</mi><mi id="S2.SS2.p4.5.m5.1.1.3" xref="S2.SS2.p4.5.m5.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.5.m5.1b"><apply id="S2.SS2.p4.5.m5.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p4.5.m5.1.1.1.cmml" xref="S2.SS2.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p4.5.m5.1.1.2.cmml" xref="S2.SS2.p4.5.m5.1.1.2">ğ‘€</ci><ci id="S2.SS2.p4.5.m5.1.1.3.cmml" xref="S2.SS2.p4.5.m5.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.5.m5.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.5.m5.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> for further decoding and generation.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S2.T1.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Method</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1">Zh-Fr</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="2" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1">Zh-De</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S2.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1">Zh-Cs</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.1.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.2.1">chrF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.3.1">BLEU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.4.1">chrF</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.5.1">BLEU</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.1.2.2.6"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.6.1">chrF</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.3.1.1">Bilingual</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.2">20.70</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.3">47.5</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.4">10.82</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.5">38.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.6">7.52</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S2.T1.1.3.3.7">27.7</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.4.4.1.1">Aquila2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.2">19.65</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.3">49.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.4">10.32</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.5">43.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.4.4.6">8.75</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.4.4.7">36.1</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.5.5.1.1">LLaMA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.2">25.76</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.3">53.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.4">15.00</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.5">49.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.5.5.6">10.08</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.5.5.7">38.6</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.1.1">RD (Aquila2+LLaMA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.2.1">27.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.3.1">55.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.4.1">17.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.5"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.5.1">49.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S2.T1.1.6.6.6"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.6.1">13.44</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S2.T1.1.6.6.7"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.7.1">39.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The result of RD Method for Zh-Fr, Zh-De, Zh-Cs translation tasks on Multi30k dataset. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Training Strategy</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">We formulate translation task as generating target text tokens conditioned on a source text tokens and prompt prefix. The log likelihood of target sentence Y (tokenized as <math alttext="\{y_{1},y_{2},..,y_{T}\}" class="ltx_math_unparsed" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1b"><mo id="S2.SS3.p1.1.m1.1.1" stretchy="false">{</mo><msub id="S2.SS3.p1.1.m1.1.2"><mi id="S2.SS3.p1.1.m1.1.2.2">y</mi><mn id="S2.SS3.p1.1.m1.1.2.3">1</mn></msub><mo id="S2.SS3.p1.1.m1.1.3">,</mo><msub id="S2.SS3.p1.1.m1.1.4"><mi id="S2.SS3.p1.1.m1.1.4.2">y</mi><mn id="S2.SS3.p1.1.m1.1.4.3">2</mn></msub><mo id="S2.SS3.p1.1.m1.1.5">,</mo><mo id="S2.SS3.p1.1.m1.1.6" lspace="0em" rspace="0.0835em">.</mo><mo id="S2.SS3.p1.1.m1.1.7" lspace="0.0835em" rspace="0.167em">.</mo><mo id="S2.SS3.p1.1.m1.1.8">,</mo><msub id="S2.SS3.p1.1.m1.1.9"><mi id="S2.SS3.p1.1.m1.1.9.2">y</mi><mi id="S2.SS3.p1.1.m1.1.9.3">T</mi></msub><mo id="S2.SS3.p1.1.m1.1.10" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\{y_{1},y_{2},..,y_{T}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">{ italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , . . , italic_y start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math>) conditioned on its source sentence X is:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="l(X,Y)=\sum_{t=1}^{T}logP_{\theta}(y_{t}|H,E_{p},y_{1},y_{2},...,y_{t-1})" class="ltx_Math" display="block" id="S2.Ex2.m1.5"><semantics id="S2.Ex2.m1.5a"><mrow id="S2.Ex2.m1.5.5" xref="S2.Ex2.m1.5.5.cmml"><mrow id="S2.Ex2.m1.5.5.3" xref="S2.Ex2.m1.5.5.3.cmml"><mi id="S2.Ex2.m1.5.5.3.2" xref="S2.Ex2.m1.5.5.3.2.cmml">l</mi><mo id="S2.Ex2.m1.5.5.3.1" xref="S2.Ex2.m1.5.5.3.1.cmml">â¢</mo><mrow id="S2.Ex2.m1.5.5.3.3.2" xref="S2.Ex2.m1.5.5.3.3.1.cmml"><mo id="S2.Ex2.m1.5.5.3.3.2.1" stretchy="false" xref="S2.Ex2.m1.5.5.3.3.1.cmml">(</mo><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">X</mi><mo id="S2.Ex2.m1.5.5.3.3.2.2" xref="S2.Ex2.m1.5.5.3.3.1.cmml">,</mo><mi id="S2.Ex2.m1.2.2" xref="S2.Ex2.m1.2.2.cmml">Y</mi><mo id="S2.Ex2.m1.5.5.3.3.2.3" stretchy="false" xref="S2.Ex2.m1.5.5.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex2.m1.5.5.2" rspace="0.111em" xref="S2.Ex2.m1.5.5.2.cmml">=</mo><mrow id="S2.Ex2.m1.5.5.1" xref="S2.Ex2.m1.5.5.1.cmml"><munderover id="S2.Ex2.m1.5.5.1.2" xref="S2.Ex2.m1.5.5.1.2.cmml"><mo id="S2.Ex2.m1.5.5.1.2.2.2" movablelimits="false" xref="S2.Ex2.m1.5.5.1.2.2.2.cmml">âˆ‘</mo><mrow id="S2.Ex2.m1.5.5.1.2.2.3" xref="S2.Ex2.m1.5.5.1.2.2.3.cmml"><mi id="S2.Ex2.m1.5.5.1.2.2.3.2" xref="S2.Ex2.m1.5.5.1.2.2.3.2.cmml">t</mi><mo id="S2.Ex2.m1.5.5.1.2.2.3.1" xref="S2.Ex2.m1.5.5.1.2.2.3.1.cmml">=</mo><mn id="S2.Ex2.m1.5.5.1.2.2.3.3" xref="S2.Ex2.m1.5.5.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex2.m1.5.5.1.2.3" xref="S2.Ex2.m1.5.5.1.2.3.cmml">T</mi></munderover><mrow id="S2.Ex2.m1.5.5.1.1" xref="S2.Ex2.m1.5.5.1.1.cmml"><mi id="S2.Ex2.m1.5.5.1.1.3" xref="S2.Ex2.m1.5.5.1.1.3.cmml">l</mi><mo id="S2.Ex2.m1.5.5.1.1.2" xref="S2.Ex2.m1.5.5.1.1.2.cmml">â¢</mo><mi id="S2.Ex2.m1.5.5.1.1.4" xref="S2.Ex2.m1.5.5.1.1.4.cmml">o</mi><mo id="S2.Ex2.m1.5.5.1.1.2a" xref="S2.Ex2.m1.5.5.1.1.2.cmml">â¢</mo><mi id="S2.Ex2.m1.5.5.1.1.5" xref="S2.Ex2.m1.5.5.1.1.5.cmml">g</mi><mo id="S2.Ex2.m1.5.5.1.1.2b" xref="S2.Ex2.m1.5.5.1.1.2.cmml">â¢</mo><msub id="S2.Ex2.m1.5.5.1.1.6" xref="S2.Ex2.m1.5.5.1.1.6.cmml"><mi id="S2.Ex2.m1.5.5.1.1.6.2" xref="S2.Ex2.m1.5.5.1.1.6.2.cmml">P</mi><mi id="S2.Ex2.m1.5.5.1.1.6.3" xref="S2.Ex2.m1.5.5.1.1.6.3.cmml">Î¸</mi></msub><mo id="S2.Ex2.m1.5.5.1.1.2c" xref="S2.Ex2.m1.5.5.1.1.2.cmml">â¢</mo><mrow id="S2.Ex2.m1.5.5.1.1.1.1" xref="S2.Ex2.m1.5.5.1.1.1.1.1.cmml"><mo id="S2.Ex2.m1.5.5.1.1.1.1.2" stretchy="false" xref="S2.Ex2.m1.5.5.1.1.1.1.1.cmml">(</mo><mrow id="S2.Ex2.m1.5.5.1.1.1.1.1" xref="S2.Ex2.m1.5.5.1.1.1.1.1.cmml"><msub id="S2.Ex2.m1.5.5.1.1.1.1.1.6" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.6.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6.2.cmml">y</mi><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.6.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6.3.cmml">t</mi></msub><mo fence="false" id="S2.Ex2.m1.5.5.1.1.1.1.1.5" xref="S2.Ex2.m1.5.5.1.1.1.1.1.5.cmml">|</mo><mrow id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml"><mi id="S2.Ex2.m1.3.3" xref="S2.Ex2.m1.3.3.cmml">H</mi><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.5" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml">,</mo><msub id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml">E</mi><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml">p</mi></msub><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.6" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml">,</mo><msub id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.2.cmml">y</mi><mn id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.3.cmml">1</mn></msub><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.7" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml">,</mo><msub id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.2.cmml">y</mi><mn id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.3.cmml">2</mn></msub><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.8" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml">,</mo><mi id="S2.Ex2.m1.4.4" mathvariant="normal" xref="S2.Ex2.m1.4.4.cmml">â€¦</mi><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.9" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml">,</mo><msub id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.2.cmml">y</mi><mrow id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.cmml"><mi id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.2" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.2.cmml">t</mi><mo id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.1" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.1.cmml">âˆ’</mo><mn id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.3" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S2.Ex2.m1.5.5.1.1.1.1.3" stretchy="false" xref="S2.Ex2.m1.5.5.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.5b"><apply id="S2.Ex2.m1.5.5.cmml" xref="S2.Ex2.m1.5.5"><eq id="S2.Ex2.m1.5.5.2.cmml" xref="S2.Ex2.m1.5.5.2"></eq><apply id="S2.Ex2.m1.5.5.3.cmml" xref="S2.Ex2.m1.5.5.3"><times id="S2.Ex2.m1.5.5.3.1.cmml" xref="S2.Ex2.m1.5.5.3.1"></times><ci id="S2.Ex2.m1.5.5.3.2.cmml" xref="S2.Ex2.m1.5.5.3.2">ğ‘™</ci><interval closure="open" id="S2.Ex2.m1.5.5.3.3.1.cmml" xref="S2.Ex2.m1.5.5.3.3.2"><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">ğ‘‹</ci><ci id="S2.Ex2.m1.2.2.cmml" xref="S2.Ex2.m1.2.2">ğ‘Œ</ci></interval></apply><apply id="S2.Ex2.m1.5.5.1.cmml" xref="S2.Ex2.m1.5.5.1"><apply id="S2.Ex2.m1.5.5.1.2.cmml" xref="S2.Ex2.m1.5.5.1.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.2.1.cmml" xref="S2.Ex2.m1.5.5.1.2">superscript</csymbol><apply id="S2.Ex2.m1.5.5.1.2.2.cmml" xref="S2.Ex2.m1.5.5.1.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.2.2.1.cmml" xref="S2.Ex2.m1.5.5.1.2">subscript</csymbol><sum id="S2.Ex2.m1.5.5.1.2.2.2.cmml" xref="S2.Ex2.m1.5.5.1.2.2.2"></sum><apply id="S2.Ex2.m1.5.5.1.2.2.3.cmml" xref="S2.Ex2.m1.5.5.1.2.2.3"><eq id="S2.Ex2.m1.5.5.1.2.2.3.1.cmml" xref="S2.Ex2.m1.5.5.1.2.2.3.1"></eq><ci id="S2.Ex2.m1.5.5.1.2.2.3.2.cmml" xref="S2.Ex2.m1.5.5.1.2.2.3.2">ğ‘¡</ci><cn id="S2.Ex2.m1.5.5.1.2.2.3.3.cmml" type="integer" xref="S2.Ex2.m1.5.5.1.2.2.3.3">1</cn></apply></apply><ci id="S2.Ex2.m1.5.5.1.2.3.cmml" xref="S2.Ex2.m1.5.5.1.2.3">ğ‘‡</ci></apply><apply id="S2.Ex2.m1.5.5.1.1.cmml" xref="S2.Ex2.m1.5.5.1.1"><times id="S2.Ex2.m1.5.5.1.1.2.cmml" xref="S2.Ex2.m1.5.5.1.1.2"></times><ci id="S2.Ex2.m1.5.5.1.1.3.cmml" xref="S2.Ex2.m1.5.5.1.1.3">ğ‘™</ci><ci id="S2.Ex2.m1.5.5.1.1.4.cmml" xref="S2.Ex2.m1.5.5.1.1.4">ğ‘œ</ci><ci id="S2.Ex2.m1.5.5.1.1.5.cmml" xref="S2.Ex2.m1.5.5.1.1.5">ğ‘”</ci><apply id="S2.Ex2.m1.5.5.1.1.6.cmml" xref="S2.Ex2.m1.5.5.1.1.6"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.6.1.cmml" xref="S2.Ex2.m1.5.5.1.1.6">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.6.2.cmml" xref="S2.Ex2.m1.5.5.1.1.6.2">ğ‘ƒ</ci><ci id="S2.Ex2.m1.5.5.1.1.6.3.cmml" xref="S2.Ex2.m1.5.5.1.1.6.3">ğœƒ</ci></apply><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S2.Ex2.m1.5.5.1.1.1.1.1.5.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.5">conditional</csymbol><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.6.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.1.1.1.6.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.6.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6.2">ğ‘¦</ci><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.6.3.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.6.3">ğ‘¡</ci></apply><list id="S2.Ex2.m1.5.5.1.1.1.1.1.4.5.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4"><ci id="S2.Ex2.m1.3.3.cmml" xref="S2.Ex2.m1.3.3">ğ»</ci><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.2">ğ¸</ci><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.1.1.1.3">ğ‘</ci></apply><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.2">ğ‘¦</ci><cn id="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.3.cmml" type="integer" xref="S2.Ex2.m1.5.5.1.1.1.1.1.2.2.2.3">1</cn></apply><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.2">ğ‘¦</ci><cn id="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.3.cmml" type="integer" xref="S2.Ex2.m1.5.5.1.1.1.1.1.3.3.3.3">2</cn></apply><ci id="S2.Ex2.m1.4.4.cmml" xref="S2.Ex2.m1.4.4">â€¦</ci><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4"><csymbol cd="ambiguous" id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4">subscript</csymbol><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.2">ğ‘¦</ci><apply id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3"><minus id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.1.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.1"></minus><ci id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.2.cmml" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.2">ğ‘¡</ci><cn id="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.3.cmml" type="integer" xref="S2.Ex2.m1.5.5.1.1.1.1.1.4.4.4.3.3">1</cn></apply></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.5c">l(X,Y)=\sum_{t=1}^{T}logP_{\theta}(y_{t}|H,E_{p},y_{1},y_{2},...,y_{t-1})</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.5d">italic_l ( italic_X , italic_Y ) = âˆ‘ start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_l italic_o italic_g italic_P start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | italic_H , italic_E start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_y start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">The loss <math alttext="L" class="ltx_Math" display="inline" id="S2.SS3.p2.1.m1.1"><semantics id="S2.SS3.p2.1.m1.1a"><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p2.1.m1.1d">italic_L</annotation></semantics></math> is then the negative log likelihood of all samples in a batch of N bilingual parallel pairs:</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S2.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L=-\frac{1}{N}\sum_{i=1}^{N}l(X_{i},Y_{i})" class="ltx_Math" display="block" id="S2.Ex3.m1.2"><semantics id="S2.Ex3.m1.2a"><mrow id="S2.Ex3.m1.2.2" xref="S2.Ex3.m1.2.2.cmml"><mi id="S2.Ex3.m1.2.2.4" xref="S2.Ex3.m1.2.2.4.cmml">L</mi><mo id="S2.Ex3.m1.2.2.3" xref="S2.Ex3.m1.2.2.3.cmml">=</mo><mrow id="S2.Ex3.m1.2.2.2" xref="S2.Ex3.m1.2.2.2.cmml"><mo id="S2.Ex3.m1.2.2.2a" xref="S2.Ex3.m1.2.2.2.cmml">âˆ’</mo><mrow id="S2.Ex3.m1.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.cmml"><mfrac id="S2.Ex3.m1.2.2.2.2.4" xref="S2.Ex3.m1.2.2.2.2.4.cmml"><mn id="S2.Ex3.m1.2.2.2.2.4.2" xref="S2.Ex3.m1.2.2.2.2.4.2.cmml">1</mn><mi id="S2.Ex3.m1.2.2.2.2.4.3" xref="S2.Ex3.m1.2.2.2.2.4.3.cmml">N</mi></mfrac><mo id="S2.Ex3.m1.2.2.2.2.3" xref="S2.Ex3.m1.2.2.2.2.3.cmml">â¢</mo><mrow id="S2.Ex3.m1.2.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.2.cmml"><munderover id="S2.Ex3.m1.2.2.2.2.2.3" xref="S2.Ex3.m1.2.2.2.2.2.3.cmml"><mo id="S2.Ex3.m1.2.2.2.2.2.3.2.2" movablelimits="false" xref="S2.Ex3.m1.2.2.2.2.2.3.2.2.cmml">âˆ‘</mo><mrow id="S2.Ex3.m1.2.2.2.2.2.3.2.3" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.cmml"><mi id="S2.Ex3.m1.2.2.2.2.2.3.2.3.2" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.2.cmml">i</mi><mo id="S2.Ex3.m1.2.2.2.2.2.3.2.3.1" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.1.cmml">=</mo><mn id="S2.Ex3.m1.2.2.2.2.2.3.2.3.3" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S2.Ex3.m1.2.2.2.2.2.3.3" xref="S2.Ex3.m1.2.2.2.2.2.3.3.cmml">N</mi></munderover><mrow id="S2.Ex3.m1.2.2.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.2.2.cmml"><mi id="S2.Ex3.m1.2.2.2.2.2.2.4" xref="S2.Ex3.m1.2.2.2.2.2.2.4.cmml">l</mi><mo id="S2.Ex3.m1.2.2.2.2.2.2.3" xref="S2.Ex3.m1.2.2.2.2.2.2.3.cmml">â¢</mo><mrow id="S2.Ex3.m1.2.2.2.2.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.2.2.2.3.cmml"><mo id="S2.Ex3.m1.2.2.2.2.2.2.2.2.3" stretchy="false" xref="S2.Ex3.m1.2.2.2.2.2.2.2.3.cmml">(</mo><msub id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.2.cmml">X</mi><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.Ex3.m1.2.2.2.2.2.2.2.2.4" xref="S2.Ex3.m1.2.2.2.2.2.2.2.3.cmml">,</mo><msub id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.cmml"><mi id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.2" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.2.cmml">Y</mi><mi id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.3" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.Ex3.m1.2.2.2.2.2.2.2.2.5" stretchy="false" xref="S2.Ex3.m1.2.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.2b"><apply id="S2.Ex3.m1.2.2.cmml" xref="S2.Ex3.m1.2.2"><eq id="S2.Ex3.m1.2.2.3.cmml" xref="S2.Ex3.m1.2.2.3"></eq><ci id="S2.Ex3.m1.2.2.4.cmml" xref="S2.Ex3.m1.2.2.4">ğ¿</ci><apply id="S2.Ex3.m1.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2"><minus id="S2.Ex3.m1.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2"></minus><apply id="S2.Ex3.m1.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2"><times id="S2.Ex3.m1.2.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.3"></times><apply id="S2.Ex3.m1.2.2.2.2.4.cmml" xref="S2.Ex3.m1.2.2.2.2.4"><divide id="S2.Ex3.m1.2.2.2.2.4.1.cmml" xref="S2.Ex3.m1.2.2.2.2.4"></divide><cn id="S2.Ex3.m1.2.2.2.2.4.2.cmml" type="integer" xref="S2.Ex3.m1.2.2.2.2.4.2">1</cn><ci id="S2.Ex3.m1.2.2.2.2.4.3.cmml" xref="S2.Ex3.m1.2.2.2.2.4.3">ğ‘</ci></apply><apply id="S2.Ex3.m1.2.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2"><apply id="S2.Ex3.m1.2.2.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.2.2.2.2.2.3.1.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3">superscript</csymbol><apply id="S2.Ex3.m1.2.2.2.2.2.3.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.2.2.2.2.2.3.2.1.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3">subscript</csymbol><sum id="S2.Ex3.m1.2.2.2.2.2.3.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3.2.2"></sum><apply id="S2.Ex3.m1.2.2.2.2.2.3.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3"><eq id="S2.Ex3.m1.2.2.2.2.2.3.2.3.1.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.1"></eq><ci id="S2.Ex3.m1.2.2.2.2.2.3.2.3.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.2">ğ‘–</ci><cn id="S2.Ex3.m1.2.2.2.2.2.3.2.3.3.cmml" type="integer" xref="S2.Ex3.m1.2.2.2.2.2.3.2.3.3">1</cn></apply></apply><ci id="S2.Ex3.m1.2.2.2.2.2.3.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.3.3">ğ‘</ci></apply><apply id="S2.Ex3.m1.2.2.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2"><times id="S2.Ex3.m1.2.2.2.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.3"></times><ci id="S2.Ex3.m1.2.2.2.2.2.2.4.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.4">ğ‘™</ci><interval closure="open" id="S2.Ex3.m1.2.2.2.2.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2"><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.2">ğ‘‹</ci><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.2">ğ‘Œ</ci><ci id="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S2.Ex3.m1.2.2.2.2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.2c">L=-\frac{1}{N}\sum_{i=1}^{N}l(X_{i},Y_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m1.2d">italic_L = - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_l ( italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">When utilizing only one LLM for translation, finetuning has been shown to yield optimal results. Therefore, in our approach, we also experiment with incorporating finetuning, which involves simultaneously adjusting the parameters of the large model during the training process. To prevent the occurrence of catastrophic forgetting, we introduced LORA<cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib10" title="">2021</a>)</cite> as a mechanism to mitigate this challenge.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we provide a description of the datasets, experimental setup employed in our study and an in-depth analysis of the results obtained.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Details</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Large Language Models</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The LLMs utilized in our experiments include the Aquila2-7B model <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/FlagAI-Open/Aquila2.</span></span></span> and the LLaMA-7B model <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib20" title="">2023</a>)</cite>. In our experiments, we primarily focus on translation from Chinese to French, German, and Czech. The Aquila2 model primarily focuses on English and Chinese proficiency and performs remarkably well in tasks involving these languages. On the other hand, the LLaMA model has been pretrained on datasets encompassing twenty languages, such as English, French, German, and Czech, but its Chinese proficiency is relatively lower.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Datasets</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The datasets used in our experiments are Multi30k dataset <cite class="ltx_cite ltx_citemacro_citep">(Elliott etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib8" title="">2016</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib7" title="">2017</a>; Barrault etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib3" title="">2018</a>)</cite> and WikiMatrix dataset <cite class="ltx_cite ltx_citemacro_citep">(Schwenk etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib19" title="">2019</a>)</cite>. Multi30k dataset contains images and their captions in four languages: English(En), French(Fr),
Germany(De), and Czech(Cs). For Chinese translation task, we have annotated a Chinese version of the Multi30k dataset<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>The Chinese version of multi30k will be available.</span></span></span>. Initially, we employ ChatGPT<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://chat.openai.com/.</span></span></span> to translate the English content of the dataset into Chinese. Subsequently, we conduct manual revisions to address any inaccuracies or lack of fluency in the translation. As for test set, we use Flickr2017 for Zh-Fr and Zh-De and Flickr2018 for Zh-Cs. Regarding WikiMatrix, we specifically choose the Simplified Chinese portion of the dataset. From this subset, we select the top 1000 highest-scoring pairs as our test set, while the remaining pairs are used for training.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">We compared our method with the following approaches: (1) Transformer-based bilingual translation model. (2) Results of instruction fine-tuning large models, including LLaMA and Aquila2. Thatâ€™s an important point to note that while LLaMA may have lower proficiency in Chinese, it still has some capability in handling and generating Chinese due to the presence of a portion of Chinese data in its training set. Similarly, Aquila2 modelâ€™s training corpus may also include a small amount of French, German, and Czech data. As a result, fine-tuning directly on these models can still achieve some level of performance in translation tasks for the respective languages.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Main Results</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The experimental results on Multi30k dataset for Zh-Fr, Zh-De, and Zh-Cs are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S2.T1" title="Table 1 â€£ 2.2 Concatenate Method â€£ 2 Approach â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>. From the table, we observe that our RD method achieves the best results. When comparing the last three rows with the first row, which represents the bilingual transformer approach, we find that utilizing large models with the same parallel corpus outperforms training from scratch. This indicates that the language alignment capability of the large models is indeed utilized during training, even though they were pretrained only on monolingual data. The results of fine-tuning large models specialized in one language (rows 2 and 3) show that these models still have some limitations in completing translation tasks. Additionally, we have also discovered that LLMs specialized in the target language tend to exhibit superior performance in translation tasks. Our concatenation method also surpasses the performance of fine-tuning with a single large model, demonstrating the need for pretraining large models on both the source and target languages to achieve better translation performance and this further validates the effectiveness of our proposed concatenation method.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Analysis</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Is it necessary to finetune the LLMs during training?</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">Our approach involves training a mapping layer to connect two large models, but during training, we also need to consider whether to adjust the parameters of the large models. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.T2" title="Table 2 â€£ How much data is required to complete the training of the mapping layer? â€£ 3.3 Analysis â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, we test the translation performance of Zh-Fr under different finetuning conditions on Multi30K datasets and find that simultaneously finetuning the parameters of the second large model (i.e., the one specialized in the target language) yield better results. On the other hand, fine-tuning the parameters of the first large model has a less significant impact. For finetuning, we utilized the efficient finetuning method known as LORA due to its higher efficiency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">How much data is required to complete the training of the mapping layer?</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#S3.T3" title="Table 3 â€£ How much data is required to complete the training of the mapping layer? â€£ 3.3 Analysis â€£ 3 Experiment â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we conducted Zh-Fr translation experiments using training sets of different sizes on WikiMatrix datasets. The findings reveal that on the WikiMatrix dataset, training with approximately 60,000 data points is adequate for training the mapping layer. This requirement is considerably smaller compared to the dataset size typically needed by traditional bilingual methods. Moreover, our method surpasses these methods in performance.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1">Aquila2 (ZH)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T2.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1">LLaMA (FR)</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1">Zh-Fr</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">Not Finetune</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2">Not Finetune</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T2.1.2.1.3">25.94</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.3.2.1">Not Finetune</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.3.2.2">Finetune</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.3.2.3.1">27.36</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.4.3.1">Finetune</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.1.4.3.2">Not Finetune</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.1.4.3.3">23.37</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.1.5.4.1">Finetune</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.1.5.4.2">Finetune</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T2.1.5.4.3">26.88</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The BLEU scores of different finetune settings on Multi30k dataset.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">#Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1">2W</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.3.1">3W</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.4.1">4W</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.2.1.1.1">RD</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.1.2">11.79</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.1.3">12.98</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.2.1.4">13.64</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T3.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.3.2.1.1">#Dataset</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.3.2.2.1">5W</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.3.2.3.1">6W</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.3.2.4.1">7W</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T3.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.4.3.1.1">RD</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.4.3.2">14.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.4.3.3">15.44</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S3.T3.1.4.3.4">15.52</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The BLEU scores associated with varying WikiMatrix dataset sizes.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">LLMs for Machine Translation.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">With the remarkable advancements of LLMs, researchers have extensively evaluated their translation capabilities using various methodologies. <cite class="ltx_cite ltx_citemacro_citet">Vilar etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib21" title="">2023</a>); Zhang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib24" title="">2023b</a>); Bawden and Yvon (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib4" title="">2023</a>)</cite> devise different prompts to facilitate translation and also examine the translation performance in various few-shot scenarios. <cite class="ltx_cite ltx_citemacro_citet">Peng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib18" title="">2023</a>); Huang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib11" title="">2024</a>)</cite> utilize Chain-of-thought or difficulty analysis techniques to address translation challenges. In order to achieve better performance, <cite class="ltx_cite ltx_citemacro_citet">Li etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib14" title="">2023</a>); Jiao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib12" title="">2023</a>); Chen etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib5" title="">2023</a>); Alves etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib1" title="">2023</a>); Xu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib22" title="">2023</a>)</cite> have explored the approach of finetuning LLMs using parallel data. All of the aforementioned methods require full support from the LLMs for the languages involved in translation. Our approach, on the other hand, is primarily designed for situations where a single large language model cannot handle all of these languages simultaneously.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Concatenation of LLMs.</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Bansal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib2" title="">2024</a>)</cite> leverages the concatenation of a smaller model and a larger model to augment the capabilities of the larger one, such as enhancing low-resource language comprehension and mathematical computation abilities. In comparison, our concatenation method is specifically tailored for machine translation tasks. Furthermore, unlike our method, they do indeed require both models to be capable of handling vocabulary from both languages involved in the translation task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we propose an approach that involves concatenating two LLMs, each specialized in the source and target languages, to achieve machine translation. This method circumvents the higher costs associated with continuous learning approaches. In the future, we plan to delve deeper into this concatenation method and investigate how to accomplish the connection solely with monolingual data as the finetuning approach for LLMs does not necessitate the use of bilingual data.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">In our concatenation approach, we require a certain amount of parallel data to train the parameters of the concatenation module. Acquiring parallel data can be costly, so in the future, we plan to explore methods that rely on monolingual data and back-translation to train the parameters of the concatenation module.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves etÂ al. (2023)</span>
<span class="ltx_bibblock">
Duarte Alves, Nuno Guerreiro, JoÃ£o Alves, JosÃ© Pombal, Ricardo Rei, JosÃ© deÂ Souza, Pierre Colombo, and Andre Martins. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.744" title="">Steering large language models for machine translation with finetuning and in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 11127â€“11148, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal etÂ al. (2024)</span>
<span class="ltx_bibblock">
Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, and Partha Talukdar. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=jjA4O1vJRz" title="">LLM augmented LLMs: Expanding capabilities through composition</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barrault etÂ al. (2018)</span>
<span class="ltx_bibblock">
LoÃ¯c Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018.

</span>
<span class="ltx_bibblock">Findings of the third shared task on multimodal machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Third Conference on Machine Translation: Shared Task Papers</em>, pages 304â€“323.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon (2023)</span>
<span class="ltx_bibblock">
Rachel Bawden and FranÃ§ois Yvon. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.16" title="">Investigating the translation performance of a large multilingual language model: the case of BLOOM</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 157â€“170, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock">Improving translation faithfulness of large language models via augmenting instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2308.12674</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yiming Cui, Ziqing Yang, and Xin Yao. 2023.

</span>
<span class="ltx_bibblock">Efficient and effective text encoding for chinese llama and alpaca.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2304.08177</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott etÂ al. (2017)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, LoÃ¯c Barrault, Fethi Bougares, and Lucia Specia. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4718" title="">Findings of the second shared task on multimodal machine translation and multilingual image description</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 215â€“233, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elliott etÂ al. (2016)</span>
<span class="ltx_bibblock">
Desmond Elliott, Stella Frank, Khalil Simaâ€™an, and Lucia Specia. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W16-3210" title="">Multi30K: Multilingual English-German image descriptions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 5th Workshop on Vision and Language</em>, pages 70â€“74, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garcia etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xavier Garcia, Yamini Bansal, Colin Cherry, George Foster, Maxim Krikun, Melvin Johnson, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock">The unreasonable effectiveness of few-shot learning for machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">International Conference on Machine Learning</em>, pages 10867â€“10878. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2021)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2106.09685</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yichong Huang, Xiaocheng Feng, Baohang Li, Chengpeng Fu, Wenshuai Huo, Ting Liu, and Bing Qin. 2024.

</span>
<span class="ltx_bibblock">Aligning translation-specific understanding to general understanding in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2401.05072</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, and Zhaopeng Tu. 2023.

</span>
<span class="ltx_bibblock">Parrot: Translating during chat using large language models tuned with human translation and feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 15009â€“15020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koh etÂ al. (2023)</span>
<span class="ltx_bibblock">
JingÂ Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. 2023.

</span>
<span class="ltx_bibblock">Grounding language models to images for multimodal inputs and outputs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Machine Learning</em>, pages 17283â€“17300. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Chen, and Jiajun Chen. 2023.

</span>
<span class="ltx_bibblock">Eliciting the translation ability of large language models via multilingual finetuning with translation instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2305.15083</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2023)</span>
<span class="ltx_bibblock">
Yinheng Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.ranlp-1.69" title="">A practical survey on zero-shot prompt design for in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing</em>, pages 641â€“647, Varna, Bulgaria. INCOMA Ltd., Shoumen, Bulgaria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chenyang Lyu, Jitao Xu, and Longyue Wang. 2023.

</span>
<span class="ltx_bibblock">New trends in machine translation using large language models: Case examples with chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2305.01181</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311â€“318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Keqin Peng, Liang Ding, Qihuang Zhong, LiÂ Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.373" title="">Towards making the most of ChatGPT for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 5622â€“5633, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk etÂ al. (2019)</span>
<span class="ltx_bibblock">
Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco GuzmÃ¡n. 2019.

</span>
<span class="ltx_bibblock">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:1907.05791</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, etÂ al. 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vilar etÂ al. (2023)</span>
<span class="ltx_bibblock">
David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.859" title="">Prompting PaLM for translation: Assessing strategies and performance</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 15406â€“15427, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Haoran Xu, YoungÂ Jin Kim, Amr Sharaf, and HanyÂ Hassan Awadalla. 2023.

</span>
<span class="ltx_bibblock">A paradigm shift in machine translation: Boosting translation performance of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2309.11674</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023a)</span>
<span class="ltx_bibblock">
AoÂ Zhang, Hao Fei, Yuan Yao, Wei Ji, LiÂ Li, Zhiyuan Liu, and Tat-Seng Chua. 2023a.

</span>
<span class="ltx_bibblock">Transfer visual prompt generator across llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2305.01278</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Biao Zhang, Barry Haddow, and Alexandra Birch. 2023b.

</span>
<span class="ltx_bibblock">Prompting large language model for machine translation: A case study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2301.07069</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023c)</span>
<span class="ltx_bibblock">
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. 2023c.

</span>
<span class="ltx_bibblock">Enhanced visual instruction tuning for text-rich image understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following</em>.

</span>
</li>
</ul>
</section>
<figure class="ltx_figure" id="A0.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="A0.F3.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Different mapping layers.</figcaption>
</figure>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Mapping Layers</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We explored three different approaches for achieving the mapping as shown in Figure<a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#A0.F3" title="Figure 3 â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>:</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">(1) Linear: Directly employing a linear layer, as previously mentioned, denoted by <math alttext="FC" class="ltx_Math" display="inline" id="A1.p2.1.m1.1"><semantics id="A1.p2.1.m1.1a"><mrow id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mi id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml">F</mi><mo id="A1.p2.1.m1.1.1.1" xref="A1.p2.1.m1.1.1.1.cmml">â¢</mo><mi id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><times id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1.1"></times><ci id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2">ğ¹</ci><ci id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">FC</annotation><annotation encoding="application/x-llamapun" id="A1.p2.1.m1.1d">italic_F italic_C</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.3">(2) Cross-attention: Employing a cross-attention structure with the source language input <math alttext="X" class="ltx_Math" display="inline" id="A1.p3.1.m1.1"><semantics id="A1.p3.1.m1.1a"><mi id="A1.p3.1.m1.1.1" xref="A1.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="A1.p3.1.m1.1b"><ci id="A1.p3.1.m1.1.1.cmml" xref="A1.p3.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="A1.p3.1.m1.1d">italic_X</annotation></semantics></math>, passed through the embedding layer of <math alttext="M_{b}" class="ltx_Math" display="inline" id="A1.p3.2.m2.1"><semantics id="A1.p3.2.m2.1a"><msub id="A1.p3.2.m2.1.1" xref="A1.p3.2.m2.1.1.cmml"><mi id="A1.p3.2.m2.1.1.2" xref="A1.p3.2.m2.1.1.2.cmml">M</mi><mi id="A1.p3.2.m2.1.1.3" xref="A1.p3.2.m2.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p3.2.m2.1b"><apply id="A1.p3.2.m2.1.1.cmml" xref="A1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A1.p3.2.m2.1.1.1.cmml" xref="A1.p3.2.m2.1.1">subscript</csymbol><ci id="A1.p3.2.m2.1.1.2.cmml" xref="A1.p3.2.m2.1.1.2">ğ‘€</ci><ci id="A1.p3.2.m2.1.1.3.cmml" xref="A1.p3.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.2.m2.1c">M_{b}</annotation><annotation encoding="application/x-llamapun" id="A1.p3.2.m2.1d">italic_M start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math>, serving as the query, denoted by <math alttext="CA" class="ltx_Math" display="inline" id="A1.p3.3.m3.1"><semantics id="A1.p3.3.m3.1a"><mrow id="A1.p3.3.m3.1.1" xref="A1.p3.3.m3.1.1.cmml"><mi id="A1.p3.3.m3.1.1.2" xref="A1.p3.3.m3.1.1.2.cmml">C</mi><mo id="A1.p3.3.m3.1.1.1" xref="A1.p3.3.m3.1.1.1.cmml">â¢</mo><mi id="A1.p3.3.m3.1.1.3" xref="A1.p3.3.m3.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p3.3.m3.1b"><apply id="A1.p3.3.m3.1.1.cmml" xref="A1.p3.3.m3.1.1"><times id="A1.p3.3.m3.1.1.1.cmml" xref="A1.p3.3.m3.1.1.1"></times><ci id="A1.p3.3.m3.1.1.2.cmml" xref="A1.p3.3.m3.1.1.2">ğ¶</ci><ci id="A1.p3.3.m3.1.1.3.cmml" xref="A1.p3.3.m3.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p3.3.m3.1c">CA</annotation><annotation encoding="application/x-llamapun" id="A1.p3.3.m3.1d">italic_C italic_A</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">(3) Cross-attention with query embedding: Utilizing a cross-attention structure with randomly initialized query embeddings, denoted by <math alttext="CA-Q" class="ltx_Math" display="inline" id="A1.p4.1.m1.1"><semantics id="A1.p4.1.m1.1a"><mrow id="A1.p4.1.m1.1.1" xref="A1.p4.1.m1.1.1.cmml"><mrow id="A1.p4.1.m1.1.1.2" xref="A1.p4.1.m1.1.1.2.cmml"><mi id="A1.p4.1.m1.1.1.2.2" xref="A1.p4.1.m1.1.1.2.2.cmml">C</mi><mo id="A1.p4.1.m1.1.1.2.1" xref="A1.p4.1.m1.1.1.2.1.cmml">â¢</mo><mi id="A1.p4.1.m1.1.1.2.3" xref="A1.p4.1.m1.1.1.2.3.cmml">A</mi></mrow><mo id="A1.p4.1.m1.1.1.1" xref="A1.p4.1.m1.1.1.1.cmml">âˆ’</mo><mi id="A1.p4.1.m1.1.1.3" xref="A1.p4.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p4.1.m1.1b"><apply id="A1.p4.1.m1.1.1.cmml" xref="A1.p4.1.m1.1.1"><minus id="A1.p4.1.m1.1.1.1.cmml" xref="A1.p4.1.m1.1.1.1"></minus><apply id="A1.p4.1.m1.1.1.2.cmml" xref="A1.p4.1.m1.1.1.2"><times id="A1.p4.1.m1.1.1.2.1.cmml" xref="A1.p4.1.m1.1.1.2.1"></times><ci id="A1.p4.1.m1.1.1.2.2.cmml" xref="A1.p4.1.m1.1.1.2.2">ğ¶</ci><ci id="A1.p4.1.m1.1.1.2.3.cmml" xref="A1.p4.1.m1.1.1.2.3">ğ´</ci></apply><ci id="A1.p4.1.m1.1.1.3.cmml" xref="A1.p4.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p4.1.m1.1c">CA-Q</annotation><annotation encoding="application/x-llamapun" id="A1.p4.1.m1.1d">italic_C italic_A - italic_Q</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1">We also conducted Zh-Fr translation experiments on the Multi30k dataset, and the experimental results are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#A1.T4" title="Table 4 â€£ Appendix A Mapping Layers â€£ Relay Decoding: Concatenating Large Language Models for Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A1.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Mapping Method</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">FC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T4.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1">CA</span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T4.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.4.1">CA-Q</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="A1.T4.1.2.2.1"><span class="ltx_text ltx_font_bold" id="A1.T4.1.2.2.1.1">Zh-Fr</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.2.2.2">27.36</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.2.2.3">11.80</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="A1.T4.1.2.2.4">17.92</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The BLEU score of different mapping method on Multi30k dataset.</figcaption>
</figure>
<div class="ltx_para" id="A1.p6">
<p class="ltx_p" id="A1.p6.1">We observe that the Linear mapping method achieved the best results on the Multi30k dataset, while the cross-attention series method yield lower results, even lower than the baseline methods. This could be attributed to the larger number of parameters introduced by these methods, which may not be effectively learned due to the relatively small scale of the Multi30k dataset.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experiments System Settings and Evaluation Metric</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We use Adam optimizer and 2000 warm-up updates. The learning rate is 1e-5. For evaluation, we use 4-gram BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2405.02933v1#bib.bib17" title="">2002</a>)</cite> and chrF scores by multi-bleu.pl in Moses<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://github.com/moses-smt/mosesdecoder</span></span></span>. We train all models on NVIDIA 80GB A100 GPUs.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May 15 19:11:22 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
