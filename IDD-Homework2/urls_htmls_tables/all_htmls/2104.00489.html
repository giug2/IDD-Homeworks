<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2104.00489] PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN</title><meta property="og:description" content="We introduce PyVertical, a framework supporting vertical federated learning using split neural networks. The proposed framework allows a data scientist to train neural networks on data features vertically partitioned a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2104.00489">

<!--Generated on Sat Mar  2 06:28:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniele Romanini 
<br class="ltx_break">OpenMined
<br class="ltx_break"><span id="id4.4.id1" class="ltx_text ltx_font_typewriter">daler.romanini@gmail.com</span>
&amp;Adam James Hall <sup id="id5.5.id2" class="ltx_sup"><span id="id5.5.id2.1" class="ltx_text ltx_font_italic">∗</span></sup>
<br class="ltx_break">Edinburgh Napier University / OpenMined
<br class="ltx_break"><span id="id6.6.id3" class="ltx_text ltx_font_typewriter">adam@openmined.org</span>
<span id="id7.7.id4" class="ltx_ERROR undefined">\AND</span>Pavlos Papadopoulos <sup id="id8.8.id5" class="ltx_sup"><span id="id8.8.id5.1" class="ltx_text ltx_font_italic">∗</span></sup>
<br class="ltx_break">Edinburgh Napier University / Apheris
<br class="ltx_break"><span id="id9.9.id6" class="ltx_text ltx_font_typewriter">pavlos.papadopoulos@napier.ac.uk</span>
&amp;Tom Titcombe <sup id="id10.10.id7" class="ltx_sup"><span id="id10.10.id7.1" class="ltx_text ltx_font_italic">∗</span></sup> 
<br class="ltx_break">Tessella / OpenMined
<br class="ltx_break"><span id="id11.11.id8" class="ltx_text ltx_font_typewriter">tom.titcombe@tessella.com   </span>
<span id="id12.12.id9" class="ltx_ERROR undefined">\AND</span>Abbas Ismail
<br class="ltx_break">Birla Institute of Technology, Mesra
<br class="ltx_break"><span id="id13.13.id10" class="ltx_text ltx_font_typewriter">be10285.17@bitmesra.ac.in</span>
&amp;Tudor Cebere
<br class="ltx_break">OpenMined
<br class="ltx_break"><span id="id14.14.id11" class="ltx_text ltx_font_typewriter">tudor@openmined.org         </span>
<span id="id15.15.id12" class="ltx_ERROR undefined">\AND</span>Robert Sandmann
<br class="ltx_break">Apheris
<br class="ltx_break"><span id="id16.16.id13" class="ltx_text ltx_font_typewriter">r.sandmann@apheris.com</span>
&amp;Robin Roehm
<br class="ltx_break">Apheris
<br class="ltx_break"><span id="id17.17.id14" class="ltx_text ltx_font_typewriter">r.roehm@apheris.com         </span>
<span id="id18.18.id15" class="ltx_ERROR undefined">\AND</span>Michael A. Hoeh
<br class="ltx_break">Apheris 
<br class="ltx_break"><span id="id19.19.id16" class="ltx_text ltx_font_typewriter">m.hoeh@apheris.com</span>
</span><span class="ltx_author_notes"> Authors have equal contribution in this work</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">We introduce PyVertical, a framework supporting vertical federated learning using split neural networks. The proposed framework allows a data scientist to train neural networks on data features vertically partitioned across multiple owners while keeping raw data on an owner’s device. To link entities shared across different datasets’ partitions, we use Private Set Intersection on IDs associated with data points. To demonstrate the validity of the proposed framework, we present the training of a simple dual-headed split neural network for a MNIST classification task, with data samples vertically distributed across two data owners and a data scientist.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">With ubiquitous data collection, individuals are constantly generating diverse swathes of data, including location, health, financial information. These data streams are often collected by separate entities and are sufficient for high utility use-cases. A common challenge faced by data scientists is utilising data isolated in silos to train machine learning (ML) algorithms. When this data is commercially sensitive, personal or otherwise under strict legal protection, it cannot be simply merged with data controlled by another party. To ensure data privacy is not compromised during the training or inference process, several privacy-preserving ML techniques, such as Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al., <a href="#bib.bib30" title="" class="ltx_ref">2016</a>; Konečnỳ et al., <a href="#bib.bib26" title="" class="ltx_ref">2016</a>; McMahan &amp; Ramage, <a href="#bib.bib29" title="" class="ltx_ref">2017</a>; Bonawitz et al., <a href="#bib.bib4" title="" class="ltx_ref">2019</a>; Ryffel et al., <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>, focus on training ML models on distributed datasets by keeping data in the custody of its corresponding holder. FL typically splits data horizontally. This is where datasets are distributed across multiple owners that have the same features and represent different data subjects <cite class="ltx_cite ltx_citemacro_citep">(Kantarcioglu &amp; Clifton, <a href="#bib.bib24" title="" class="ltx_ref">2004</a>)</cite>. However, it is common in real-world scenarios to find datasets which are vertically distributed <cite class="ltx_cite ltx_citemacro_citep">(McConnell &amp; Skillicorn, <a href="#bib.bib28" title="" class="ltx_ref">2004</a>)</cite>, i.e. different features of the same data subject are distributed across multiple data owners. For example, specialists or general hospitals may hold different parts of a patient’s medical data.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">To address the issue of learning from vertically distributed data, we use Split Neural Networks (SplitNN) to first map data into an abstract, shareable representation. This allows information from multiple sources to be combined for learning without exposing raw data. We combine this with Private Set Intersection (PSI) to identify and link data points belonging to the same data samples shared among parties. This process facilitates Vertical Federated Learning (VFL) for non-linear functions.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contributions</h3>

<div id="S1.SS1.p1" class="ltx_para ltx_noindent">
<p id="S1.SS1.p1.1" class="ltx_p">In this work, we extend the proposal of <cite class="ltx_cite ltx_citemacro_citep">(Angelou et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>, regarding the use of (SplitNNs) and PSI in Vertical Federated Learning. We use the PySyft library for privacy-preserving machine learning <cite class="ltx_cite ltx_citemacro_citep">(Ryffel et al., <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> to train a Vertically Federated ML algorithm on data distributed across the premises of one or multiple data owners. This work is released as an open-source framework, PyVertical. To the best of our knowledge, this is the first open-source framework to perform machine learning on vertically distributed datasets using Split Neural Networks<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code is available at PyVertical:<a target="_blank" href="https://github.com/OpenMined/PyVertical" title="" class="ltx_ref ltx_href"> https://github.com/OpenMined/PyVertical</a></span></span></span>.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para ltx_noindent">
<p id="S1.SS1.p2.1" class="ltx_p">We verify our method on a two-party, vertically-partitioned MNIST dataset. Our work presents a dual-headed scenario, where data from two separate data owners (who holds different parts of the data samples) and a data scientist (who, in our case, holds data labels) are securely aligned and combined for model training. However, this work could be extended to multiple data owners using the same principle we describe here.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background Knowledge and Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Private Set Intersection</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">Private Set Intersection (PSI) <cite class="ltx_cite ltx_citemacro_citep">(Freedman et al., <a href="#bib.bib16" title="" class="ltx_ref">2004</a>; Huang et al., <a href="#bib.bib22" title="" class="ltx_ref">2012</a>; De Cristofaro &amp; Tsudik, <a href="#bib.bib9" title="" class="ltx_ref">2010</a>; Dachman-Soled et al., <a href="#bib.bib8" title="" class="ltx_ref">2009</a>)</cite> is a multi-party computation cryptographic technique which allows two parties, where each hold a set of elements, to compute the intersection of these elements, without revealing anything to the other party except for the elements in the intersection. Different PSI protocols have been proposed <cite class="ltx_cite ltx_citemacro_citep">(Buddhavarapu et al., <a href="#bib.bib5" title="" class="ltx_ref">2020</a>; Ion et al., <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; Chase &amp; Miao, <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Pinkas et al., <a href="#bib.bib33" title="" class="ltx_ref">2018</a>)</cite> and employed for scenarios such as private contact discovery <cite class="ltx_cite ltx_citemacro_citep">(Demmler et al., <a href="#bib.bib10" title="" class="ltx_ref">2018</a>)</cite> and also privacy-preserving contact tracing <cite class="ltx_cite ltx_citemacro_citep">(Angelou et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p">In this work, we use a PSI implementation based on a Diffie-Hellman key exchange that uses Bloom filters compression to reduce the communication complexity <cite class="ltx_cite ltx_citemacro_citep">(Angelou et al., <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>. This protocol works with two parties computing the intersection between their sets. However, the chosen PSI framework can be replaced with an alternative implementation, for instance, to compute directly the intersection of datasets coming from more than two parties <cite class="ltx_cite ltx_citemacro_citep">(Hazay &amp; Venkitasubramaniam, <a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Split Neural Networks</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Split learning is a concept of training a model that is split into segments held by different parties or on different devices. A neural network model trained this way is called a Split Neural Network, or SplitNN. In SplitNN, each model segment transforms its input data into an intermediate data representation (as the output of a hidden layer of a classic neural network). This intermediate data is transmitted to the next segment until the training or the inference process is completed. During backpropagation, the gradient is also propagated across different segments. Compared to data-centric FL, split learning can also be useful to reduce the computational burden on data owners, who in many real-world scenarios may have limited computational resources <cite class="ltx_cite ltx_citemacro_citep">(Gupta &amp; Raskar, <a href="#bib.bib18" title="" class="ltx_ref">2018</a>; Vepakomma et al., <a href="#bib.bib39" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Vertical Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">Vertical federated learning (VFL) is the concept of collaboratively training a model on a dataset where data features are split amongst multiple parties <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite>. For example, different healthcare organizations may have different data for the same patient. Considering the sensitivity of the data, these two organizations cannot simply merge their information without violating that person’s privacy. For this reason, a machine learning model should be trained collaboratively, and data should be kept on the corresponding premises.
Machine learning algorithms for vertical partitioned data is not a new concept, and many studies for new models and algorithms have been proposed in this area <cite class="ltx_cite ltx_citemacro_citep">(Feng &amp; Yu, <a href="#bib.bib15" title="" class="ltx_ref">2020</a>; Liu et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Du &amp; Atallah, <a href="#bib.bib11" title="" class="ltx_ref">2001</a>; Du et al., <a href="#bib.bib12" title="" class="ltx_ref">2004</a>; Vaidya &amp; Clifton, <a href="#bib.bib38" title="" class="ltx_ref">2002</a>; Karr et al., <a href="#bib.bib25" title="" class="ltx_ref">2009</a>; Sanil et al., <a href="#bib.bib35" title="" class="ltx_ref">2004</a>; Wan et al., <a href="#bib.bib40" title="" class="ltx_ref">2007</a>; Gascón et al., <a href="#bib.bib17" title="" class="ltx_ref">2017</a>; Thapa et al., <a href="#bib.bib36" title="" class="ltx_ref">2020</a>; Hardy et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Nock et al., <a href="#bib.bib31" title="" class="ltx_ref">2018</a>)</cite>. Existing open-source VFL frameworks include FedML <cite class="ltx_cite ltx_citemacro_citep">(He et al., <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, which implements multi-party linear models <cite class="ltx_cite ltx_citemacro_citep">(Hardy et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p">Similarly to our work, the use of split networks for vertical federated learning has been proposed <cite class="ltx_cite ltx_citemacro_citep">(Ceballos et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>. However, differently from our work, the authors investigate multiple methods for combining information sent to the data scientist from disjoint datasets. Moreover, they do not consider the entity resolution problem for aligning data across parties, whereas we illustrate how PSI can be successfully exploited prior to the training process to account for this.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Framework Description</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">We introduce PyVertical, a framework written in Python for vertical federated learning using SplitNNs and PSI. PyVertical is built upon the privacy-preserving deep learning library PySyft <cite class="ltx_cite ltx_citemacro_citep">(Ryffel et al., <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> to provide security features and mechanisms for model training, such as pointers to data, without exposing private information.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">A set of data features are distributed across one or more data owners. We refer to a full dataset split vertically across the features as a <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">vertical dataset</span>. Each of the data owners takes part in the model training, alongside a data scientist who orchestrates the process. The data scientist could also be a data owner itself, holding features or data labels. The data features in the vertical datasets may intersect. Each data point is associated with a unique ID based on the data point’s subject, the format of which is agreed by the data owners (e.g. legal names, email addresses, ID card numbers). The data owners use PSI to agree on a shared set of data IDs (process described in Section <a href="#S3.SS1" title="3.1 Data Resolution Protocol ‣ 3 Framework Description ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>); each data owner discards non-shared data from their datasets and sorts their datasets by ID, such that element <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">n</annotation></semantics></math> of each vertical dataset corresponds to the same data subject.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.2" class="ltx_p">In our framework, the data scientist is able to define a split neural network model and send the corresponding model segments to the data owners.
Each data owner’s model segment maps their data samples to an abstract representation with <math id="S3.p3.1.m1.1" class="ltx_Math" alttext="k_{i}" display="inline"><semantics id="S3.p3.1.m1.1a"><msub id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.2" xref="S3.p3.1.m1.1.1.2.cmml">k</mi><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.2">𝑘</ci><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">k_{i}</annotation></semantics></math> neurons. The output from each model segment (which would correspond to a hidden layer of a complete classic neural network) is sent to the data scientist and concatenated to form a <math id="S3.p3.2.m2.1" class="ltx_Math" alttext="\sum_{i}k_{i}" display="inline"><semantics id="S3.p3.2.m2.1a"><mrow id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><msub id="S3.p3.2.m2.1.1.1" xref="S3.p3.2.m2.1.1.1.cmml"><mo id="S3.p3.2.m2.1.1.1.2" xref="S3.p3.2.m2.1.1.1.2.cmml">∑</mo><mi id="S3.p3.2.m2.1.1.1.3" xref="S3.p3.2.m2.1.1.1.3.cmml">i</mi></msub><msub id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml"><mi id="S3.p3.2.m2.1.1.2.2" xref="S3.p3.2.m2.1.1.2.2.cmml">k</mi><mi id="S3.p3.2.m2.1.1.2.3" xref="S3.p3.2.m2.1.1.2.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><apply id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.1.cmml" xref="S3.p3.2.m2.1.1.1">subscript</csymbol><sum id="S3.p3.2.m2.1.1.1.2.cmml" xref="S3.p3.2.m2.1.1.1.2"></sum><ci id="S3.p3.2.m2.1.1.1.3.cmml" xref="S3.p3.2.m2.1.1.1.3">𝑖</ci></apply><apply id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.2.1.cmml" xref="S3.p3.2.m2.1.1.2">subscript</csymbol><ci id="S3.p3.2.m2.1.1.2.2.cmml" xref="S3.p3.2.m2.1.1.2.2">𝑘</ci><ci id="S3.p3.2.m2.1.1.2.3.cmml" xref="S3.p3.2.m2.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">\sum_{i}k_{i}</annotation></semantics></math> length vector. The data scientist also defines a model segment corresponding to the final part of the split neural network. This segment remains on the data scientist’s premises and maps the concatenated, intermediate data (i.e. the output from data owners’ model segments) into a shape relevant to the task. During model training, the data scientist calculates batch loss and updates their model segment’s weights. The data scientist then sends the final gradient to the data owners, each of whom updates their own model segment by completing backpropagation independently. We assume all parties are honest-but-curious actors. Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Framework Description ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates model inference under this framework for the experiment outlined in Section <a href="#S4" title="4 Experiment ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2104.00489/assets/figures/PyVertical_architecture.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="389" height="405" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Parties and datasets in the conducted experiment. Data Scientist holds a part of the SplitNN and the labels dataset. Data Owners hold their images datasets and parts of the SplitNN</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Resolution Protocol</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">We use a PSI Python library <cite class="ltx_cite ltx_citemacro_citep">(Angelou et al., <a href="#bib.bib2" title="" class="ltx_ref">2020</a>)</cite> to identify intersections between data points in two datasets based on unique IDs. In this work, we consider a setting where the data scientist has access to ground truth labels. For all three parties (two data owners + one data scientist) to agree on data points shared among all datasets, the protocol works as follow: firstly, the data scientist runs the PSI protocol independently with each data owner. The intersection of IDs between the data scientist and each data owner is revealed to the data scientist. The data scientist computes the global intersection from the two previous independently computed intersections and communicates the global intersection to the data owners. In this setting, the data owners do not communicate and are not aware of each other’s identity in any regard. In practice, this is an ideal feature of the protocol as having knowledge of a group’s or individual’s participation in a training process can reveal sensitive commercial and personal information in and of itself. Moreover, as the single IDs’ intersection lists are only revealed to the data scientist, there is no risk for the data owners to learn which information the other data owners owns. Each of the data owners learns only the information necessary for training or inference.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">To verify the validity of our framework, we train a dual-headed SplitNN on a vertically-partitioned version of the MNIST dataset. We generate the data by splitting the images in MNIST into left and right halves, providing a dataset of each half to different data owners. The data scientist defines and sends an identical, multi-layered neural network to each of the data owners that takes 392-length vectors as input (flattened representation of 28x14 pixel images). The data scientist also defines and keeps on its premises the second part of the neural network, which outputs a softmax layer for classification. The data scientist can access the ground truth labels and calculate the loss for each data batch. The data scientist controls the training process and hyperparameters. Appendix <a href="#A2" title="Appendix B Experimental Setup ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> provides more details on the specific values used in model training.
The objective of this experiment is to demonstrate that the proposed framework allows vertically-partitioned learning. This specific experiment should be considered a proof-of-concept, thus not highly optimised for the specific task. Nevertheless, we report the results of the experiment in Figure <a href="#A2.F4" title="Figure 4 ‣ Appendix B Experimental Setup ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (in Appendix <a href="#A2" title="Appendix B Experimental Setup ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation and Conclusion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We have developed and distributed our work as an open-source project. We hope that PyVertical can serve as a useful tool for researching neural-networks-based VFL. We find PSI an appropriate and useful method for resolving data subjects across datasets; many datasets and domains already collect unique IDs, such as usernames or national identifiers for medical data, making our method widely applicable. Finally, we successfully train a dual-headed model on a vertically-partitioned MNIST dataset, demonstrating that the proposed framework and method work in principle.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Limitations and Future Work</h3>

<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">The experiment performed in this work assumes that all the parties involved (data owners and data scientist) act honestly. To develop a truly scalable, robust VFL system, additional precautions should be taken into account: identity management, validation of adherence to PSI protocol, and a method agreeing on data ID schema, to name a few.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">This work investigates a symmetric SplitNN model: we assume that each data owner holds an identical model segment and that data points are split equally between data owners. Future work should investigate the impact of imbalanced vertical datasets <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib27" title="" class="ltx_ref">2020</a>)</cite> and the resulting difficulties from the asymmetric model segment convergence due to the use of different sized models and learning rates.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">Finally, we illustrate an example of a training process with two data owners and a data scientist holding labels. While the proposed framework can support more parties in principle, we aim to investigate how to apply the process to massively multi-headed Vertical Federated Learning tasks. Additionally, we plan to research and integrate other privacy-preserving ML techniques into our workflow, such as decentralised identities <cite class="ltx_cite ltx_citemacro_citep">(Papadopoulos et al., <a href="#bib.bib32" title="" class="ltx_ref">2021</a>; Abramson et al., <a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> and differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Dwork et al., <a href="#bib.bib14" title="" class="ltx_ref">2006</a>; Dwork, <a href="#bib.bib13" title="" class="ltx_ref">2008</a>; Titcombe et al., <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, to further enhance privacy guarantees.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramson et al. (2020)</span>
<span class="ltx_bibblock">
Will Abramson, Adam James Hall, Pavlos Papadopoulos, Nikolaos Pitropakis, and
William J. Buchanan.

</span>
<span class="ltx_bibblock">A distributed trust framework for privacy-preserving machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pp.  205–220, 2020.

</span>
<span class="ltx_bibblock">ISSN 1611-3349.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-030-58986-8˙14</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="http://dx.doi.org/10.1007/978-3-030-58986-8_14" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1007/978-3-030-58986-8_14</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angelou et al. (2020)</span>
<span class="ltx_bibblock">
Nick Angelou, Ayoud Benaissa, Bogdan Cebere, Will Clark, Phillipp Schoppmann,
Rutuja Surve, Daniel Liu, and Ben Szymbow.

</span>
<span class="ltx_bibblock">PSI Source Code, 2020.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/OpenMined/PSI" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/OpenMined/PSI</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Angelou et al. (2020)</span>
<span class="ltx_bibblock">
Nick Angelou, Ayoub Benaissa, Bogdan Cebere, William Clark, Adam James Hall,
Michael A Hoeh, Daniel Liu, Pavlos Papadopoulos, Robin Roehm, Robert
Sandmann, Phillipp Schoppmann, and Tom Titcombe.

</span>
<span class="ltx_bibblock">Asymmetric private set intersection with applications to contact
tracing and private vertical federated machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.09350</em>, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al. (2019)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konecny, Stefano Mazzocchi,
H Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1902.01046</em>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Buddhavarapu et al. (2020)</span>
<span class="ltx_bibblock">
Prasad Buddhavarapu, Andrew Knox, Payman Mohassel, Shubho Sengupta, Erik
Taubeneck, and Vlad Vlaskin.

</span>
<span class="ltx_bibblock">Private matching for compute.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IACR Cryptol. ePrint Arch.</em>, 2020:599, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ceballos et al. (2020)</span>
<span class="ltx_bibblock">
Iker Ceballos, Vivek Sharma, Eduardo Mugica, Abhishek Singh, Alberto Roman,
Praneeth Vepakomma, and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Splitnn-driven vertical partitioning, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chase &amp; Miao (2020)</span>
<span class="ltx_bibblock">
Melissa Chase and Peihan Miao.

</span>
<span class="ltx_bibblock">Private set intersection in the internet setting from lightweight
oblivious prf.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Annual International Cryptology Conference</em>, pp.  34–63.
Springer, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dachman-Soled et al. (2009)</span>
<span class="ltx_bibblock">
Dana Dachman-Soled, Tal Malkin, Mariana Raykova, and Moti Yung.

</span>
<span class="ltx_bibblock">Efficient robust private set intersection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Applied Cryptography and Network
Security</em>, pp.  125–142. Springer, 2009.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Cristofaro &amp; Tsudik (2010)</span>
<span class="ltx_bibblock">
Emiliano De Cristofaro and Gene Tsudik.

</span>
<span class="ltx_bibblock">Practical private set intersection protocols with linear complexity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">International Conference on Financial Cryptography and Data
Security</em>, pp.  143–159. Springer, 2010.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demmler et al. (2018)</span>
<span class="ltx_bibblock">
Daniel Demmler, Peter Rindal, Mike Rosulek, and Ni Trieu.

</span>
<span class="ltx_bibblock">Pir-psi: Scaling private contact discovery.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IACR Cryptol. ePrint Arch.</em>, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du &amp; Atallah (2001)</span>
<span class="ltx_bibblock">
Wenliang Du and Mikhail J Atallah.

</span>
<span class="ltx_bibblock">Privacy-preserving cooperative statistical analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Seventeenth Annual Computer Security Applications
Conference</em>, pp.  102–110. IEEE, 2001.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2004)</span>
<span class="ltx_bibblock">
Wenliang Du, Yunghsiang S Han, and Shigang Chen.

</span>
<span class="ltx_bibblock">Privacy-preserving multivariate statistical analysis: Linear
regression and classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2004 SIAM international conference on
data mining</em>, pp.  222–233. SIAM, 2004.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2008)</span>
<span class="ltx_bibblock">
Cynthia Dwork.

</span>
<span class="ltx_bibblock">Differential privacy: A survey of results.

</span>
<span class="ltx_bibblock">In Manindra Agrawal, Dingzhu Du, Zhenhua Duan, and Angsheng Li
(eds.), <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Theory and Applications of Models of Computation</em>, pp.  1–19,
Berlin, Heidelberg, 2008. Springer Berlin Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-79228-4.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork et al. (2006)</span>
<span class="ltx_bibblock">
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.

</span>
<span class="ltx_bibblock">Calibrating noise to sensitivity in private data analysis.

</span>
<span class="ltx_bibblock">In Shai Halevi and Tal Rabin (eds.), <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Theory of Cryptography</em>,
pp.  265–284, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-32732-5.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng &amp; Yu (2020)</span>
<span class="ltx_bibblock">
Siwei Feng and Han Yu.

</span>
<span class="ltx_bibblock">Multi-participant multi-class vertical federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.11154</em>, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freedman et al. (2004)</span>
<span class="ltx_bibblock">
Michael J Freedman, Kobbi Nissim, and Benny Pinkas.

</span>
<span class="ltx_bibblock">Efficient private matching and set intersection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International conference on the theory and applications of
cryptographic techniques</em>, pp.  1–19. Springer, 2004.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gascón et al. (2017)</span>
<span class="ltx_bibblock">
Adrià Gascón, Phillipp Schoppmann, Borja Balle, Mariana Raykova, Jack
Doerner, Samee Zahur, and David Evans.

</span>
<span class="ltx_bibblock">Privacy-preserving distributed linear regression on high-dimensional
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings on Privacy Enhancing Technologies</em>, 2017(4):345–364, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta &amp; Raskar (2018)</span>
<span class="ltx_bibblock">
Otkrist Gupta and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Distributed learning of deep neural network over multiple agents.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of Network and Computer Applications</em>, 116:1–8, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hardy et al. (2017)</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.10677</em>, 2017.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hazay &amp; Venkitasubramaniam (2017)</span>
<span class="ltx_bibblock">
Carmit Hazay and Muthuramakrishnan Venkitasubramaniam.

</span>
<span class="ltx_bibblock">Scalable multi-party private set-intersection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IACR International Workshop on Public Key Cryptography</em>,
pp.  175–203. Springer, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, et al.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2012)</span>
<span class="ltx_bibblock">
Yan Huang, David Evans, and Jonathan Katz.

</span>
<span class="ltx_bibblock">Private set intersection: Are garbled circuits better than custom
protocols?

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">NDSS</em>, 2012.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ion et al. (2020)</span>
<span class="ltx_bibblock">
Mihaela Ion, Ben Kreuter, Ahmet Erhan Nergiz, Sarvar Patel, Shobhit Saxena,
Karn Seth, Mariana Raykova, David Shanahan, and Moti Yung.

</span>
<span class="ltx_bibblock">On deploying secure computing: Private
intersection-sum-with-cardinality.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2020 IEEE European Symposium on Security and Privacy
(EuroS&amp;P)</em>, pp.  370–389. IEEE, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kantarcioglu &amp; Clifton (2004)</span>
<span class="ltx_bibblock">
Murat Kantarcioglu and Chris Clifton.

</span>
<span class="ltx_bibblock">Privacy-preserving distributed mining of association rules on
horizontally partitioned data.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on knowledge and data engineering</em>,
16(9):1026–1037, 2004.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karr et al. (2009)</span>
<span class="ltx_bibblock">
Alan F Karr, Xiaodong Lin, Ashish P Sanil, and Jerome P Reiter.

</span>
<span class="ltx_bibblock">Privacy-preserving analysis of vertically partitioned data using
secure matrix products.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Journal of Official Statistics</em>, 25(1):125,
2009.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al. (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>, 2016.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Yang Liu, Xiong Zhang, and Libin Wang.

</span>
<span class="ltx_bibblock">Asymmetrically vertical federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.07427</em>, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McConnell &amp; Skillicorn (2004)</span>
<span class="ltx_bibblock">
Sabine McConnell and David B Skillicorn.

</span>
<span class="ltx_bibblock">Building predictors from vertically distributed data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2004 conference of the Centre for
Advanced Studies on Collaborative research</em>, pp.  150–162, 2004.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan &amp; Ramage (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan and Daniel Ramage.

</span>
<span class="ltx_bibblock">Federated learning: Collaborative machine learning without
centralized training data.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Google Research Blog</em>, 3, 2017.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2017/04/federated-learning-collaborative.html</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al. (2016)</span>
<span class="ltx_bibblock">
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1602.05629</em>, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nock et al. (2018)</span>
<span class="ltx_bibblock">
Richard Nock, Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Entity resolution and federated learning get a federated resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.04035</em>, 2018.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papadopoulos et al. (2021)</span>
<span class="ltx_bibblock">
Pavlos Papadopoulos, Will Abramson, Adam J. Hall, Nikolaos Pitropakis, and
William J. Buchanan.

</span>
<span class="ltx_bibblock">Privacy and trust redefined in federated machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Machine Learning and Knowledge Extraction</em>, 3(2):333–356, 2021.

</span>
<span class="ltx_bibblock">ISSN 2504-4990.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/make3020017</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.mdpi.com/2504-4990/3/2/17" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2504-4990/3/2/17</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinkas et al. (2018)</span>
<span class="ltx_bibblock">
Benny Pinkas, Thomas Schneider, and Michael Zohner.

</span>
<span class="ltx_bibblock">Scalable private set intersection based on ot extension.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Privacy and Security (TOPS)</em>, 21(2):1–35, 2018.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryffel et al. (2018)</span>
<span class="ltx_bibblock">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach.

</span>
<span class="ltx_bibblock">A generic framework for privacy preserving deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1811.04017</em>, 2018.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanil et al. (2004)</span>
<span class="ltx_bibblock">
Ashish P Sanil, Alan F Karr, Xiaodong Lin, and Jerome P Reiter.

</span>
<span class="ltx_bibblock">Privacy preserving regression modelling via distributed computation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the tenth ACM SIGKDD international conference
on Knowledge discovery and data mining</em>, pp.  677–682, 2004.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thapa et al. (2020)</span>
<span class="ltx_bibblock">
Chandra Thapa, M. A. P. Chamikara, and Seyit Camtepe.

</span>
<span class="ltx_bibblock">Splitfed: When federated learning meets split learning, 2020.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Titcombe et al. (2021)</span>
<span class="ltx_bibblock">
Tom Titcombe, Adam J. Hall, Pavlos Papadopoulos, and Daniele Romanini.

</span>
<span class="ltx_bibblock">Practical defences against model inversion attacks for split neural
networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.05743</em>, 2021.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaidya &amp; Clifton (2002)</span>
<span class="ltx_bibblock">
Jaideep Vaidya and Chris Clifton.

</span>
<span class="ltx_bibblock">Privacy preserving association rule mining in vertically partitioned
data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the eighth ACM SIGKDD international
conference on Knowledge discovery and data mining</em>, pp.  639–644, 2002.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vepakomma et al. (2018)</span>
<span class="ltx_bibblock">
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar.

</span>
<span class="ltx_bibblock">Split learning for health: Distributed deep learning without sharing
raw patient data.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.00564</em>, 2018.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al. (2007)</span>
<span class="ltx_bibblock">
Li Wan, Wee Keong Ng, Shuguo Han, and Vincent CS Lee.

</span>
<span class="ltx_bibblock">Privacy-preservation for gradient descent methods.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining</em>, pp.  775–783, 2007.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</em>,
10(2):1–19, 2019.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>PyVertical Protocol</h2>

<div id="A1.p1" class="ltx_para ltx_noindent">
<p id="A1.p1.1" class="ltx_p">Figure <a href="#A1.F2" title="Figure 2 ‣ Appendix A PyVertical Protocol ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> describes the PyVertical protocol applied to the MNIST dataset for a single data owner. The dual-headed PSI data linkage process is presented in Figure <a href="#A1.F3" title="Figure 3 ‣ Appendix A PyVertical Protocol ‣ PyVertical: A Vertical Federated Learning Framework for Multi-headed SplitNN" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Note that, in this illustration, there is only one data scientist; the duplicated icon is just to illustrate in more details how the data scientist runs a single PSI with each data owner separately, and that this could be done in parallel.</p>
</div>
<figure id="A1.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.00489/assets/figures/full_dataset.png" id="A1.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="220" height="230" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Full dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.00489/assets/figures/two_datasets.png" id="A1.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="279" height="231" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Split images and labels datasets</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.00489/assets/figures/psi_linkage.png" id="A1.F2.sf3.g1" class="ltx_graphics ltx_img_square" width="236" height="192" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>PSI linkage and ordering</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2104.00489/assets/figures/splitnn.png" id="A1.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="275" height="191" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(d) </span>SplitNN training</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Vertical federated learning proof-of-concept implementation of <cite class="ltx_cite ltx_citemacro_cite">Angelou et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite></figcaption>
</figure>
<figure id="A1.F3" class="ltx_figure"><img src="/html/2104.00489/assets/figures/Dual-Headed_PSI_architecture.png" id="A1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="308" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>i) Data Scientist computes the intersection with Data Owner 1. ii) Data Scientist computes the intersection with Data Owner 2. iii) Data Scientist computes the global intersection.</figcaption>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Experimental Setup</h2>

<div id="A2.p1" class="ltx_para ltx_noindent">
<p id="A2.p1.1" class="ltx_p">The data owner model segment maps 392-length input into a 64-length intermediate vector with a ReLU activation, which is an abstract encoding of the data. The data scientist controls a separate neural network that takes as input a 128-length vector (concatenated data owner outputs) and transforms it into a softmax-activated, 10-class vector representing the possible digits in the dataset. The data scientist’s model has a 500-length hidden layer with a ReLU activation. All layers are fully-connected. A learning rate of 0.01 is used for the data owner models and 0.1 for the data scientist model. Data is grouped into batches of size 128. Only the first 20,000 training images of MNIST are used, and the model is trained for 30 epochs.</p>
</div>
<figure id="A2.F4" class="ltx_figure"><img src="/html/2104.00489/assets/figures/training_results.png" id="A2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="359" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Train and validation accuracy for an unoptimised dual-headed SplitNN on vertically-partitioned MNIST.</figcaption>
</figure>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2104.00488" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2104.00489" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2104.00489">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2104.00489" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2104.00490" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 06:28:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
