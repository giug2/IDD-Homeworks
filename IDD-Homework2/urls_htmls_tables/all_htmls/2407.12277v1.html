<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.12277] Multimodal Reranking for Knowledge-Intensive Visual Question Answering</title><meta property="og:description" content="Knowledge-intensive visual question answering requires models to effectively use external knowledge to help answer visual questions. A typical pipeline includes a knowledge retriever and an answer generator.
However, a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Reranking for Knowledge-Intensive Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Reranking for Knowledge-Intensive Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.12277">

<!--Generated on Mon Aug  5 15:13:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Multimodal Reranking for Knowledge-Intensive 
<br class="ltx_break">Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haoyang Wen<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break">Carnegie Mellon University 
<br class="ltx_break"><span id="id2.2.id1" class="ltx_text ltx_font_typewriter">hwen3@cs.cmu.edu</span> 
<br class="ltx_break">&amp;Honglei Zhuang 
<br class="ltx_break">Google 
<br class="ltx_break"><span id="id3.3.id2" class="ltx_text ltx_font_typewriter">hlz@google.com</span> 
<br class="ltx_break">&amp;Hamed Zamani<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span> 
<br class="ltx_break">University of Massachusetts Amherst
<br class="ltx_break"><span id="id4.4.id3" class="ltx_text ltx_font_typewriter">zamani@cs.umass.edu
<br class="ltx_break"><span id="id4.4.id3.1" class="ltx_ERROR undefined">\AND</span></span>Alexander Hauptmann 
<br class="ltx_break">Carnegie Mellon University 
<br class="ltx_break"><span id="id5.5.id4" class="ltx_text ltx_font_typewriter">alex@cs.cmu.edu</span> 
<br class="ltx_break">&amp;Michael Bendersky 
<br class="ltx_break">Google 
<br class="ltx_break"><span id="id6.6.id5" class="ltx_text ltx_font_typewriter">bemike@google.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes"><sup id="id7.7.id1" class="ltx_sup">∗</sup>Work performed while at Google.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Knowledge-intensive visual question answering requires models to effectively use external knowledge to help answer visual questions. A typical pipeline includes a knowledge retriever and an answer generator.
However, a retriever that utilizes local information, such as an image patch, may not provide reliable question-candidate relevance scores.
Besides, the two-tower architecture also limits the relevance score modeling of a retriever to select top candidates for answer generator reasoning.
In this paper, we introduce an additional module, a multi-modal reranker, to improve the ranking quality of knowledge candidates for answer generation. Our reranking module takes multi-modal information from both candidates and questions and performs cross-item interaction for better relevance score modeling. Experiments on OK-VQA and A-OKVQA show that multi-modal reranker from distant supervision provides consistent improvements. We also find a training-testing discrepancy with reranking in answer generation, where performance improves if training knowledge candidates are similar to or noisier than those used in testing.</p>
</div>
<div id="p1" class="ltx_para ltx_noindent">
<div id="p1.1" class="ltx_block ltx_align_bottom">
<p id="p1.1.2" class="ltx_p"><span id="p1.1.2.1" class="ltx_text ltx_font_bold">Multimodal Reranking for Knowledge-Intensive 
<br class="ltx_break">Visual Question Answering</span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.1" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.1.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">

<span id="p1.1.1.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.2.1.1.1" class="ltx_tr">
<span id="p1.1.1.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Haoyang Wen<span id="footnotex3" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex3.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span></span></span></span>
<span id="p1.1.1.2.1.2.2" class="ltx_tr">
<span id="p1.1.1.2.1.2.2.1" class="ltx_td ltx_align_center">Carnegie Mellon University</span></span>
<span id="p1.1.1.2.1.3.3" class="ltx_tr">
<span id="p1.1.1.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.1.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">hwen3@cs.cmu.edu</span></span></span>
</span>
</span></span>                      <span id="p1.1.1.3" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.3.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.3.1.1.1" class="ltx_tr">
<span id="p1.1.1.3.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Honglei Zhuang</span></span></span>
<span id="p1.1.1.3.1.2.2" class="ltx_tr">
<span id="p1.1.1.3.1.2.2.1" class="ltx_td ltx_align_center">Google</span></span>
<span id="p1.1.1.3.1.3.3" class="ltx_tr">
<span id="p1.1.1.3.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.1.3.1.3.3.1.1" class="ltx_text ltx_font_typewriter">hlz@google.com</span></span></span>
</span>
</span></span>                      <span id="p1.1.1.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.1.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.1.1.1.1" class="ltx_tr">
<span id="p1.1.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Hamed Zamani<span id="footnotex4" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span id="footnotex4.1.1.1" class="ltx_text ltx_font_medium">1</span></span></span></span></span><span id="p1.1.1.1.1.1.1.1.1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><sup id="p1.1.1.1.1.1.1.1.1.1" class="ltx_sup"><span id="p1.1.1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_medium">∗</span></sup>Work performed while at Google.</span></span></span></span></span></span>
<span id="p1.1.1.1.1.2.1" class="ltx_tr">
<span id="p1.1.1.1.1.2.1.1" class="ltx_td ltx_align_center">University of Massachusetts Amherst</span></span>
<span id="p1.1.1.1.1.3.2" class="ltx_tr">
<span id="p1.1.1.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="p1.1.1.1.1.3.2.1.1" class="ltx_text ltx_font_typewriter">zamani@cs.umass.edu</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
<p id="p1.1.3" class="ltx_p ltx_align_center" style="width:433.6pt;"><span id="p1.1.3.1" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.1.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.1.1.1.1" class="ltx_tr">
<span id="p1.1.3.1.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Alexander Hauptmann</span></span></span>
<span id="p1.1.3.1.1.2.2" class="ltx_tr">
<span id="p1.1.3.1.1.2.2.1" class="ltx_td ltx_align_center">Carnegie Mellon University</span></span>
<span id="p1.1.3.1.1.3.3" class="ltx_tr">
<span id="p1.1.3.1.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.3.1.1.3.3.1.1" class="ltx_text ltx_font_typewriter">alex@cs.cmu.edu</span></span></span>
</span>
</span></span>                      <span id="p1.1.3.2" class="ltx_text ltx_inline-block" style="width:0.0pt;">
<span id="p1.1.3.2.1" class="ltx_tabular ltx_align_top">
<span class="ltx_tbody">
<span id="p1.1.3.2.1.1.1" class="ltx_tr">
<span id="p1.1.3.2.1.1.1.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Michael Bendersky</span></span></span>
<span id="p1.1.3.2.1.2.2" class="ltx_tr">
<span id="p1.1.3.2.1.2.2.1" class="ltx_td ltx_align_center">Google</span></span>
<span id="p1.1.3.2.1.3.3" class="ltx_tr">
<span id="p1.1.3.2.1.3.3.1" class="ltx_td ltx_align_center"><span id="p1.1.3.2.1.3.3.1.1" class="ltx_text ltx_font_typewriter">bemike@google.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering">
</div>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Knowledge-intensive visual question answering (KI-VQA), compared to conventional visual question answering, provides questions that cannot be directly answered with images. It requires models to use external knowledge for answer reasoning and synthesis, as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2407.12277/assets/image/COCO_train2014_000000470055.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="246" height="164" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S1.F1.1" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.1.1.1" class="ltx_tr">
<td id="S1.F1.1.1.1.1" class="ltx_td ltx_align_left">
<span id="S1.F1.1.1.1.1.1" class="ltx_text ltx_font_bold">Q</span>: What US city is associated with this type of pizza?</td>
</tr>
<tr id="S1.F1.1.2.2" class="ltx_tr">
<td id="S1.F1.1.2.2.1" class="ltx_td ltx_align_left">
<span id="S1.F1.1.2.2.1.1" class="ltx_text ltx_font_bold">A</span>: Chicago</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An example from OK-VQA, which requires knowledge to associate deep-dish pizza and Chicago.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">A typical KI-VQA system contains a retrieval model to find relevant external knowledge, and an answer generator that performs reasoning over retrieved knowledge to produce the answer. One line of research investigates methods for an effective retrieval pipeline, which includes the choices of knowledge bases <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>; Gardères et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Luo et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>, and methods on retrieval with visual descriptions <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite> or image-text retrieval <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>; Lin et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Answer generation models usually use retrieval relevance scores to select top candidates <cite class="ltx_cite ltx_citemacro_cite">Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. Although achieving great success, it may sometimes provide unreliable scores, especially for retrieval using images. Because we usually split an image into a series of image patches and perform retrieval with individual patches, a high relevance score of one patch may not necessarily translate to a high overall question-candidate relevance. Besides, the two-tower architecture of a retriever model also lacks cross-item modeling for predicting precise relevance scores.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we propose to include multi-modal reranking to improve the relevance score modeling, as reranking have already shown its importance in various knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_citep">(Liu, <a href="#bib.bib22" title="" class="ltx_ref">2009</a>; Lee et al., <a href="#bib.bib17" title="" class="ltx_ref">2018</a>; Wang et al., <a href="#bib.bib37" title="" class="ltx_ref">2018</a>; Mao et al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Glass et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>; Hofstätter et al., <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite>. The multi-modal reranking uses the multi-modal question and multi-modal knowledge items to obtain the relevance score. Specifically, we finetune a pretrained multi-modal language model <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2023b</a>)</cite> to perform a multi-modal cross-item interaction between the question and knowledge items. We train our reranker on the same dataset as answer generator training, distantly supervised by checking if answer candidates appear in the knowledge text. The benefits of this reranking component are two-folded. On one side, as other typical reranking components, it can provide more reliable relevance scores by modeling the cross-item interaction. On the other side, because most of the existing retrieval models performs uni-modal retrieval <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, reranking with multi-modal interaction can improve the quality of retrieval by multi-modal information from question and knowledge candidates.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We perform experiments on OK-VQA <cite class="ltx_cite ltx_citemacro_citep">(Marino et al., <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> and A-OKQVA <cite class="ltx_cite ltx_citemacro_citep">(Schwenk et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>, based on image-text retrieval <cite class="ltx_cite ltx_citemacro_citep">(Jia et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. The results show that the distantly-supervised reranker provides consistent improvement compared to the pipeline without a reranker.
We also observe a training-testing discrepancy with reranking for answer generation, finding that performance improves when training knowledge candidates are similar to or noisier than testing candidates. We also find that an oracle reranker can provide a promising performance upperbound, which sheds light on future research directions in this area.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2407.12277/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="162" height="65" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A basic KI-VQA framework, which first retrieves relevant top knowledge candidates with using visual question and then combine the question and retrieved knowledge candidates to generate the answer. The dashed box is our reranking module in Section <a href="#S3" title="3 Multi-Modal Reranking ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>A Knowledge-Intensive Visual Question Answering Framework</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we will introduce a basic framework for KI-VQA, including image-text retrieval and answer generation, as illustrated in Figure <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Wikipedia-Based Image Text Dataset</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In this work, we use a multi-modal knowledge base, Wikipedia-Based Image Text Dataset (WIT) <cite class="ltx_cite ltx_citemacro_citep">(Srinivasan et al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>. In addition to previous work that uses text from encyclopedia, WIT contains images from Wikipedia and the surrounding text at different levels, including their captions and surrounding sections. Therefore, we consider WIT as a combination of image and text knowledge.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image-Text Retrieval</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.4" class="ltx_p">Previous work has explored the use of different retrieval model choices <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>; Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>; Lin et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. We follow one line of research that adopts image-text retrieval <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> using pretrained image-text language model with dual-encoder architecture <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>); Jia et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. Following <cite class="ltx_cite ltx_citemacro_citet">Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, we use sliding window with a stride to generate multiple image regions from question image. Each image region is considered as a query and will be encoded by image encoder model <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\phi_{i}(\cdot)" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><mrow id="S2.SS2.p1.1.m1.1.2" xref="S2.SS2.p1.1.m1.1.2.cmml"><msub id="S2.SS2.p1.1.m1.1.2.2" xref="S2.SS2.p1.1.m1.1.2.2.cmml"><mi id="S2.SS2.p1.1.m1.1.2.2.2" xref="S2.SS2.p1.1.m1.1.2.2.2.cmml">ϕ</mi><mi id="S2.SS2.p1.1.m1.1.2.2.3" xref="S2.SS2.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.2.1" xref="S2.SS2.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S2.SS2.p1.1.m1.1.2.3.2" xref="S2.SS2.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.1.m1.1.2.3.2.1" xref="S2.SS2.p1.1.m1.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS2.p1.1.m1.1.2.3.2.2" xref="S2.SS2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.2"><times id="S2.SS2.p1.1.m1.1.2.1.cmml" xref="S2.SS2.p1.1.m1.1.2.1"></times><apply id="S2.SS2.p1.1.m1.1.2.2.cmml" xref="S2.SS2.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.2.2.1.cmml" xref="S2.SS2.p1.1.m1.1.2.2">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.2.2.2.cmml" xref="S2.SS2.p1.1.m1.1.2.2.2">italic-ϕ</ci><ci id="S2.SS2.p1.1.m1.1.2.2.3.cmml" xref="S2.SS2.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\phi_{i}(\cdot)</annotation></semantics></math>. We encode captions in WIT dataset as the representation for candidates with text encoder model <math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\phi_{t}(\cdot)" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><mrow id="S2.SS2.p1.2.m2.1.2" xref="S2.SS2.p1.2.m2.1.2.cmml"><msub id="S2.SS2.p1.2.m2.1.2.2" xref="S2.SS2.p1.2.m2.1.2.2.cmml"><mi id="S2.SS2.p1.2.m2.1.2.2.2" xref="S2.SS2.p1.2.m2.1.2.2.2.cmml">ϕ</mi><mi id="S2.SS2.p1.2.m2.1.2.2.3" xref="S2.SS2.p1.2.m2.1.2.2.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.2.1" xref="S2.SS2.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S2.SS2.p1.2.m2.1.2.3.2" xref="S2.SS2.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S2.SS2.p1.2.m2.1.2.3.2.1" xref="S2.SS2.p1.2.m2.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">⋅</mo><mo stretchy="false" id="S2.SS2.p1.2.m2.1.2.3.2.2" xref="S2.SS2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.2.cmml" xref="S2.SS2.p1.2.m2.1.2"><times id="S2.SS2.p1.2.m2.1.2.1.cmml" xref="S2.SS2.p1.2.m2.1.2.1"></times><apply id="S2.SS2.p1.2.m2.1.2.2.cmml" xref="S2.SS2.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.2.2.1.cmml" xref="S2.SS2.p1.2.m2.1.2.2">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.2.2.2.cmml" xref="S2.SS2.p1.2.m2.1.2.2.2">italic-ϕ</ci><ci id="S2.SS2.p1.2.m2.1.2.2.3.cmml" xref="S2.SS2.p1.2.m2.1.2.2.3">𝑡</ci></apply><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\phi_{t}(\cdot)</annotation></semantics></math>, as captions in Wikipedia are generally informative. Relevance score between an image region <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msub id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2" xref="S2.SS2.p1.3.m3.1.1.2.cmml">v</mi><mi id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2">𝑣</ci><ci id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">v_{i}</annotation></semantics></math> and a WIT candidate <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><mi id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">c</annotation></semantics></math> is obtained with the inner product of their representations</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.3" class="ltx_Math" alttext="r_{t}(v_{i},c)=\phi_{i}(v_{i})^{T}\phi_{t}(c)." display="block"><semantics id="S2.Ex1.m1.3a"><mrow id="S2.Ex1.m1.3.3.1" xref="S2.Ex1.m1.3.3.1.1.cmml"><mrow id="S2.Ex1.m1.3.3.1.1" xref="S2.Ex1.m1.3.3.1.1.cmml"><mrow id="S2.Ex1.m1.3.3.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.cmml"><msub id="S2.Ex1.m1.3.3.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.3.cmml"><mi id="S2.Ex1.m1.3.3.1.1.1.3.2" xref="S2.Ex1.m1.3.3.1.1.1.3.2.cmml">r</mi><mi id="S2.Ex1.m1.3.3.1.1.1.3.3" xref="S2.Ex1.m1.3.3.1.1.1.3.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.3.3.1.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.2.cmml">(</mo><msub id="S2.Ex1.m1.3.3.1.1.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.3.3.1.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.Ex1.m1.3.3.1.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.Ex1.m1.3.3.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.2.cmml">,</mo><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">c</mi><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.1.1.4" xref="S2.Ex1.m1.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.3.3.1.1.3" xref="S2.Ex1.m1.3.3.1.1.3.cmml">=</mo><mrow id="S2.Ex1.m1.3.3.1.1.2" xref="S2.Ex1.m1.3.3.1.1.2.cmml"><msub id="S2.Ex1.m1.3.3.1.1.2.3" xref="S2.Ex1.m1.3.3.1.1.2.3.cmml"><mi id="S2.Ex1.m1.3.3.1.1.2.3.2" xref="S2.Ex1.m1.3.3.1.1.2.3.2.cmml">ϕ</mi><mi id="S2.Ex1.m1.3.3.1.1.2.3.3" xref="S2.Ex1.m1.3.3.1.1.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.1.2.2" xref="S2.Ex1.m1.3.3.1.1.2.2.cmml">​</mo><msup id="S2.Ex1.m1.3.3.1.1.2.1" xref="S2.Ex1.m1.3.3.1.1.2.1.cmml"><mrow id="S2.Ex1.m1.3.3.1.1.2.1.1.1" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.2.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.cmml">(</mo><msub id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.cmml"><mi id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.2.cmml">v</mi><mi id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.2.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.cmml">)</mo></mrow><mi id="S2.Ex1.m1.3.3.1.1.2.1.3" xref="S2.Ex1.m1.3.3.1.1.2.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.1.2.2a" xref="S2.Ex1.m1.3.3.1.1.2.2.cmml">​</mo><msub id="S2.Ex1.m1.3.3.1.1.2.4" xref="S2.Ex1.m1.3.3.1.1.2.4.cmml"><mi id="S2.Ex1.m1.3.3.1.1.2.4.2" xref="S2.Ex1.m1.3.3.1.1.2.4.2.cmml">ϕ</mi><mi id="S2.Ex1.m1.3.3.1.1.2.4.3" xref="S2.Ex1.m1.3.3.1.1.2.4.3.cmml">t</mi></msub><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.1.2.2b" xref="S2.Ex1.m1.3.3.1.1.2.2.cmml">​</mo><mrow id="S2.Ex1.m1.3.3.1.1.2.5.2" xref="S2.Ex1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.2.5.2.1" xref="S2.Ex1.m1.3.3.1.1.2.cmml">(</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">c</mi><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.2.5.2.2" xref="S2.Ex1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.Ex1.m1.3.3.1.2" xref="S2.Ex1.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.3b"><apply id="S2.Ex1.m1.3.3.1.1.cmml" xref="S2.Ex1.m1.3.3.1"><eq id="S2.Ex1.m1.3.3.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.3"></eq><apply id="S2.Ex1.m1.3.3.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1"><times id="S2.Ex1.m1.3.3.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.2"></times><apply id="S2.Ex1.m1.3.3.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.1.3.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.3">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.1.3.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.3.2">𝑟</ci><ci id="S2.Ex1.m1.3.3.1.1.1.3.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.3.3">𝑡</ci></apply><interval closure="open" id="S2.Ex1.m1.3.3.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1"><apply id="S2.Ex1.m1.3.3.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.2">𝑣</ci><ci id="S2.Ex1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑐</ci></interval></apply><apply id="S2.Ex1.m1.3.3.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2"><times id="S2.Ex1.m1.3.3.1.1.2.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2.2"></times><apply id="S2.Ex1.m1.3.3.1.1.2.3.cmml" xref="S2.Ex1.m1.3.3.1.1.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.2.3.1.cmml" xref="S2.Ex1.m1.3.3.1.1.2.3">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.2.3.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2.3.2">italic-ϕ</ci><ci id="S2.Ex1.m1.3.3.1.1.2.3.3.cmml" xref="S2.Ex1.m1.3.3.1.1.2.3.3">𝑖</ci></apply><apply id="S2.Ex1.m1.3.3.1.1.2.1.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.2.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1">superscript</csymbol><apply id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.2">𝑣</ci><ci id="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1.1.1.1.3">𝑖</ci></apply><ci id="S2.Ex1.m1.3.3.1.1.2.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.2.1.3">𝑇</ci></apply><apply id="S2.Ex1.m1.3.3.1.1.2.4.cmml" xref="S2.Ex1.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.2.4.1.cmml" xref="S2.Ex1.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.2.4.2.cmml" xref="S2.Ex1.m1.3.3.1.1.2.4.2">italic-ϕ</ci><ci id="S2.Ex1.m1.3.3.1.1.2.4.3.cmml" xref="S2.Ex1.m1.3.3.1.1.2.4.3">𝑡</ci></apply><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.3c">r_{t}(v_{i},c)=\phi_{i}(v_{i})^{T}\phi_{t}(c).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Answer Generation</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We follow previous work <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>; Lin et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> that performs reasoning over top candidates within an encoder-decoder architecture. We also incorporate the multi-modal information <cite class="ltx_cite ltx_citemacro_cite">Salemi et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite>, compared to previous work that mostly uses text-based information.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Our answer generation module is finetuned on vision language models that takes the combination of image and text as input (<span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span>, <cite class="ltx_cite ltx_citemacro_citep">Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2023b</a>; Li et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a></cite>).
We first encode each top candidate separately. The input of each candidate consists of question image, candidate image and text following a template<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text ltx_font_typewriter">question: &lt;question text&gt; candidate: &lt;caption&gt;</span></span></span></span> to compose question and candidate. We encode the image with a Vision Transformer <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib7" title="" class="ltx_ref">2021</a>)</cite>, which takes a series of image patches <math id="S2.SS3.p2.1.m1.3" class="ltx_Math" alttext="\boldsymbol{x}^{v}=\left[x^{v}_{1},\ldots,x^{v}_{n}\right]" display="inline"><semantics id="S2.SS3.p2.1.m1.3a"><mrow id="S2.SS3.p2.1.m1.3.3" xref="S2.SS3.p2.1.m1.3.3.cmml"><msup id="S2.SS3.p2.1.m1.3.3.4" xref="S2.SS3.p2.1.m1.3.3.4.cmml"><mi id="S2.SS3.p2.1.m1.3.3.4.2" xref="S2.SS3.p2.1.m1.3.3.4.2.cmml">𝒙</mi><mi id="S2.SS3.p2.1.m1.3.3.4.3" xref="S2.SS3.p2.1.m1.3.3.4.3.cmml">v</mi></msup><mo id="S2.SS3.p2.1.m1.3.3.3" xref="S2.SS3.p2.1.m1.3.3.3.cmml">=</mo><mrow id="S2.SS3.p2.1.m1.3.3.2.2" xref="S2.SS3.p2.1.m1.3.3.2.3.cmml"><mo id="S2.SS3.p2.1.m1.3.3.2.2.3" xref="S2.SS3.p2.1.m1.3.3.2.3.cmml">[</mo><msubsup id="S2.SS3.p2.1.m1.2.2.1.1.1" xref="S2.SS3.p2.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS3.p2.1.m1.2.2.1.1.1.2.2" xref="S2.SS3.p2.1.m1.2.2.1.1.1.2.2.cmml">x</mi><mn id="S2.SS3.p2.1.m1.2.2.1.1.1.3" xref="S2.SS3.p2.1.m1.2.2.1.1.1.3.cmml">1</mn><mi id="S2.SS3.p2.1.m1.2.2.1.1.1.2.3" xref="S2.SS3.p2.1.m1.2.2.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S2.SS3.p2.1.m1.3.3.2.2.4" xref="S2.SS3.p2.1.m1.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">…</mi><mo id="S2.SS3.p2.1.m1.3.3.2.2.5" xref="S2.SS3.p2.1.m1.3.3.2.3.cmml">,</mo><msubsup id="S2.SS3.p2.1.m1.3.3.2.2.2" xref="S2.SS3.p2.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS3.p2.1.m1.3.3.2.2.2.2.2" xref="S2.SS3.p2.1.m1.3.3.2.2.2.2.2.cmml">x</mi><mi id="S2.SS3.p2.1.m1.3.3.2.2.2.3" xref="S2.SS3.p2.1.m1.3.3.2.2.2.3.cmml">n</mi><mi id="S2.SS3.p2.1.m1.3.3.2.2.2.2.3" xref="S2.SS3.p2.1.m1.3.3.2.2.2.2.3.cmml">v</mi></msubsup><mo id="S2.SS3.p2.1.m1.3.3.2.2.6" xref="S2.SS3.p2.1.m1.3.3.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.3b"><apply id="S2.SS3.p2.1.m1.3.3.cmml" xref="S2.SS3.p2.1.m1.3.3"><eq id="S2.SS3.p2.1.m1.3.3.3.cmml" xref="S2.SS3.p2.1.m1.3.3.3"></eq><apply id="S2.SS3.p2.1.m1.3.3.4.cmml" xref="S2.SS3.p2.1.m1.3.3.4"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.3.3.4.1.cmml" xref="S2.SS3.p2.1.m1.3.3.4">superscript</csymbol><ci id="S2.SS3.p2.1.m1.3.3.4.2.cmml" xref="S2.SS3.p2.1.m1.3.3.4.2">𝒙</ci><ci id="S2.SS3.p2.1.m1.3.3.4.3.cmml" xref="S2.SS3.p2.1.m1.3.3.4.3">𝑣</ci></apply><list id="S2.SS3.p2.1.m1.3.3.2.3.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2"><apply id="S2.SS3.p2.1.m1.2.2.1.1.1.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1">subscript</csymbol><apply id="S2.SS3.p2.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.2.2.1.1.1.2.1.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1">superscript</csymbol><ci id="S2.SS3.p2.1.m1.2.2.1.1.1.2.2.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1.2.2">𝑥</ci><ci id="S2.SS3.p2.1.m1.2.2.1.1.1.2.3.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1.2.3">𝑣</ci></apply><cn type="integer" id="S2.SS3.p2.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS3.p2.1.m1.2.2.1.1.1.3">1</cn></apply><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">…</ci><apply id="S2.SS3.p2.1.m1.3.3.2.2.2.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2">subscript</csymbol><apply id="S2.SS3.p2.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS3.p2.1.m1.3.3.2.2.2.2.1.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2">superscript</csymbol><ci id="S2.SS3.p2.1.m1.3.3.2.2.2.2.2.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2.2.2">𝑥</ci><ci id="S2.SS3.p2.1.m1.3.3.2.2.2.2.3.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2.2.3">𝑣</ci></apply><ci id="S2.SS3.p2.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS3.p2.1.m1.3.3.2.2.2.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.3c">\boldsymbol{x}^{v}=\left[x^{v}_{1},\ldots,x^{v}_{n}\right]</annotation></semantics></math>, <span id="S2.SS3.p2.1.2" class="ltx_text ltx_font_italic">i.e.</span>, image tokens, to produce image representations</p>
<table id="S2.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.2" class="ltx_Math" alttext="\boldsymbol{E}^{v}=\left[\boldsymbol{e}^{v}_{1},\ldots,\boldsymbol{e}^{v}_{n}\right]=\operatorname{\textit{Enc}}_{v}(\boldsymbol{x}^{v})." display="block"><semantics id="S2.Ex2.m1.2a"><mrow id="S2.Ex2.m1.2.2.1" xref="S2.Ex2.m1.2.2.1.1.cmml"><mrow id="S2.Ex2.m1.2.2.1.1" xref="S2.Ex2.m1.2.2.1.1.cmml"><msup id="S2.Ex2.m1.2.2.1.1.6" xref="S2.Ex2.m1.2.2.1.1.6.cmml"><mi id="S2.Ex2.m1.2.2.1.1.6.2" xref="S2.Ex2.m1.2.2.1.1.6.2.cmml">𝑬</mi><mi id="S2.Ex2.m1.2.2.1.1.6.3" xref="S2.Ex2.m1.2.2.1.1.6.3.cmml">v</mi></msup><mo id="S2.Ex2.m1.2.2.1.1.7" xref="S2.Ex2.m1.2.2.1.1.7.cmml">=</mo><mrow id="S2.Ex2.m1.2.2.1.1.2.2" xref="S2.Ex2.m1.2.2.1.1.2.3.cmml"><mo id="S2.Ex2.m1.2.2.1.1.2.2.3" xref="S2.Ex2.m1.2.2.1.1.2.3.cmml">[</mo><msubsup id="S2.Ex2.m1.2.2.1.1.1.1.1" xref="S2.Ex2.m1.2.2.1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.2.2.1.1.1.1.1.2.2" xref="S2.Ex2.m1.2.2.1.1.1.1.1.2.2.cmml">𝒆</mi><mn id="S2.Ex2.m1.2.2.1.1.1.1.1.3" xref="S2.Ex2.m1.2.2.1.1.1.1.1.3.cmml">1</mn><mi id="S2.Ex2.m1.2.2.1.1.1.1.1.2.3" xref="S2.Ex2.m1.2.2.1.1.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S2.Ex2.m1.2.2.1.1.2.2.4" xref="S2.Ex2.m1.2.2.1.1.2.3.cmml">,</mo><mi mathvariant="normal" id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">…</mi><mo id="S2.Ex2.m1.2.2.1.1.2.2.5" xref="S2.Ex2.m1.2.2.1.1.2.3.cmml">,</mo><msubsup id="S2.Ex2.m1.2.2.1.1.2.2.2" xref="S2.Ex2.m1.2.2.1.1.2.2.2.cmml"><mi id="S2.Ex2.m1.2.2.1.1.2.2.2.2.2" xref="S2.Ex2.m1.2.2.1.1.2.2.2.2.2.cmml">𝒆</mi><mi id="S2.Ex2.m1.2.2.1.1.2.2.2.3" xref="S2.Ex2.m1.2.2.1.1.2.2.2.3.cmml">n</mi><mi id="S2.Ex2.m1.2.2.1.1.2.2.2.2.3" xref="S2.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml">v</mi></msubsup><mo id="S2.Ex2.m1.2.2.1.1.2.2.6" xref="S2.Ex2.m1.2.2.1.1.2.3.cmml">]</mo></mrow><mo id="S2.Ex2.m1.2.2.1.1.8" xref="S2.Ex2.m1.2.2.1.1.8.cmml">=</mo><mrow id="S2.Ex2.m1.2.2.1.1.4.2" xref="S2.Ex2.m1.2.2.1.1.4.3.cmml"><msub id="S2.Ex2.m1.2.2.1.1.3.1.1" xref="S2.Ex2.m1.2.2.1.1.3.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S2.Ex2.m1.2.2.1.1.3.1.1.2" xref="S2.Ex2.m1.2.2.1.1.3.1.1.2a.cmml">Enc</mtext><mi id="S2.Ex2.m1.2.2.1.1.3.1.1.3" xref="S2.Ex2.m1.2.2.1.1.3.1.1.3.cmml">v</mi></msub><mo id="S2.Ex2.m1.2.2.1.1.4.2a" xref="S2.Ex2.m1.2.2.1.1.4.3.cmml">⁡</mo><mrow id="S2.Ex2.m1.2.2.1.1.4.2.2" xref="S2.Ex2.m1.2.2.1.1.4.3.cmml"><mo stretchy="false" id="S2.Ex2.m1.2.2.1.1.4.2.2.2" xref="S2.Ex2.m1.2.2.1.1.4.3.cmml">(</mo><msup id="S2.Ex2.m1.2.2.1.1.4.2.2.1" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1.cmml"><mi id="S2.Ex2.m1.2.2.1.1.4.2.2.1.2" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1.2.cmml">𝒙</mi><mi id="S2.Ex2.m1.2.2.1.1.4.2.2.1.3" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1.3.cmml">v</mi></msup><mo stretchy="false" id="S2.Ex2.m1.2.2.1.1.4.2.2.3" xref="S2.Ex2.m1.2.2.1.1.4.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S2.Ex2.m1.2.2.1.2" xref="S2.Ex2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.2b"><apply id="S2.Ex2.m1.2.2.1.1.cmml" xref="S2.Ex2.m1.2.2.1"><and id="S2.Ex2.m1.2.2.1.1a.cmml" xref="S2.Ex2.m1.2.2.1"></and><apply id="S2.Ex2.m1.2.2.1.1b.cmml" xref="S2.Ex2.m1.2.2.1"><eq id="S2.Ex2.m1.2.2.1.1.7.cmml" xref="S2.Ex2.m1.2.2.1.1.7"></eq><apply id="S2.Ex2.m1.2.2.1.1.6.cmml" xref="S2.Ex2.m1.2.2.1.1.6"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.6.1.cmml" xref="S2.Ex2.m1.2.2.1.1.6">superscript</csymbol><ci id="S2.Ex2.m1.2.2.1.1.6.2.cmml" xref="S2.Ex2.m1.2.2.1.1.6.2">𝑬</ci><ci id="S2.Ex2.m1.2.2.1.1.6.3.cmml" xref="S2.Ex2.m1.2.2.1.1.6.3">𝑣</ci></apply><list id="S2.Ex2.m1.2.2.1.1.2.3.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2"><apply id="S2.Ex2.m1.2.2.1.1.1.1.1.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex2.m1.2.2.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1">superscript</csymbol><ci id="S2.Ex2.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1.2.2">𝒆</ci><ci id="S2.Ex2.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1.2.3">𝑣</ci></apply><cn type="integer" id="S2.Ex2.m1.2.2.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.2.2.1.1.1.1.1.3">1</cn></apply><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">…</ci><apply id="S2.Ex2.m1.2.2.1.1.2.2.2.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.2.2.2.1.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2">subscript</csymbol><apply id="S2.Ex2.m1.2.2.1.1.2.2.2.2.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2">superscript</csymbol><ci id="S2.Ex2.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2.2.2">𝒆</ci><ci id="S2.Ex2.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2.2.3">𝑣</ci></apply><ci id="S2.Ex2.m1.2.2.1.1.2.2.2.3.cmml" xref="S2.Ex2.m1.2.2.1.1.2.2.2.3">𝑛</ci></apply></list></apply><apply id="S2.Ex2.m1.2.2.1.1c.cmml" xref="S2.Ex2.m1.2.2.1"><eq id="S2.Ex2.m1.2.2.1.1.8.cmml" xref="S2.Ex2.m1.2.2.1.1.8"></eq><share href="#S2.Ex2.m1.2.2.1.1.2.cmml" id="S2.Ex2.m1.2.2.1.1d.cmml" xref="S2.Ex2.m1.2.2.1"></share><apply id="S2.Ex2.m1.2.2.1.1.4.3.cmml" xref="S2.Ex2.m1.2.2.1.1.4.2"><apply id="S2.Ex2.m1.2.2.1.1.3.1.1.cmml" xref="S2.Ex2.m1.2.2.1.1.3.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.3.1.1.1.cmml" xref="S2.Ex2.m1.2.2.1.1.3.1.1">subscript</csymbol><ci id="S2.Ex2.m1.2.2.1.1.3.1.1.2a.cmml" xref="S2.Ex2.m1.2.2.1.1.3.1.1.2"><mtext class="ltx_mathvariant_italic" id="S2.Ex2.m1.2.2.1.1.3.1.1.2.cmml" xref="S2.Ex2.m1.2.2.1.1.3.1.1.2">Enc</mtext></ci><ci id="S2.Ex2.m1.2.2.1.1.3.1.1.3.cmml" xref="S2.Ex2.m1.2.2.1.1.3.1.1.3">𝑣</ci></apply><apply id="S2.Ex2.m1.2.2.1.1.4.2.2.1.cmml" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.2.2.1.1.4.2.2.1.1.cmml" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1">superscript</csymbol><ci id="S2.Ex2.m1.2.2.1.1.4.2.2.1.2.cmml" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1.2">𝒙</ci><ci id="S2.Ex2.m1.2.2.1.1.4.2.2.1.3.cmml" xref="S2.Ex2.m1.2.2.1.1.4.2.2.1.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.2c">\boldsymbol{E}^{v}=\left[\boldsymbol{e}^{v}_{1},\ldots,\boldsymbol{e}^{v}_{n}\right]=\operatorname{\textit{Enc}}_{v}(\boldsymbol{x}^{v}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">We combine image representations and text token embeddings <math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\boldsymbol{E}^{t}" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><msup id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml"><mi id="S2.SS3.p3.1.m1.1.1.2" xref="S2.SS3.p3.1.m1.1.1.2.cmml">𝑬</mi><mi id="S2.SS3.p3.1.m1.1.1.3" xref="S2.SS3.p3.1.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><apply id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.1.m1.1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS3.p3.1.m1.1.1.2.cmml" xref="S2.SS3.p3.1.m1.1.1.2">𝑬</ci><ci id="S2.SS3.p3.1.m1.1.1.3.cmml" xref="S2.SS3.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\boldsymbol{E}^{t}</annotation></semantics></math> to produce fused representations with a Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a href="#bib.bib36" title="" class="ltx_ref">2017</a>)</cite></p>
<table id="S2.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex3.m1.1" class="ltx_Math" alttext="\boldsymbol{H}=\left[\boldsymbol{H}^{v}_{q};\boldsymbol{H}^{v}_{c};\boldsymbol{H}^{t}\right]=\operatorname{\textit{Enc}}_{t}\left(\left[\boldsymbol{E}^{v}_{q};\boldsymbol{E}^{v}_{c};\boldsymbol{E}^{t}\right]\right)," display="block"><semantics id="S2.Ex3.m1.1a"><mrow id="S2.Ex3.m1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.cmml"><mrow id="S2.Ex3.m1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.7" xref="S2.Ex3.m1.1.1.1.1.7.cmml">𝑯</mi><mo id="S2.Ex3.m1.1.1.1.1.8" xref="S2.Ex3.m1.1.1.1.1.8.cmml">=</mo><mrow id="S2.Ex3.m1.1.1.1.1.3.3" xref="S2.Ex3.m1.1.1.1.1.3.4.cmml"><mo id="S2.Ex3.m1.1.1.1.1.3.3.4" xref="S2.Ex3.m1.1.1.1.1.3.4.cmml">[</mo><msubsup id="S2.Ex3.m1.1.1.1.1.1.1.1" xref="S2.Ex3.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.2.2" xref="S2.Ex3.m1.1.1.1.1.1.1.1.2.2.cmml">𝑯</mi><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.3.cmml">q</mi><mi id="S2.Ex3.m1.1.1.1.1.1.1.1.2.3" xref="S2.Ex3.m1.1.1.1.1.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S2.Ex3.m1.1.1.1.1.3.3.5" xref="S2.Ex3.m1.1.1.1.1.3.4.cmml">;</mo><msubsup id="S2.Ex3.m1.1.1.1.1.2.2.2" xref="S2.Ex3.m1.1.1.1.1.2.2.2.cmml"><mi id="S2.Ex3.m1.1.1.1.1.2.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.2.2.2.2.2.cmml">𝑯</mi><mi id="S2.Ex3.m1.1.1.1.1.2.2.2.3" xref="S2.Ex3.m1.1.1.1.1.2.2.2.3.cmml">c</mi><mi id="S2.Ex3.m1.1.1.1.1.2.2.2.2.3" xref="S2.Ex3.m1.1.1.1.1.2.2.2.2.3.cmml">v</mi></msubsup><mo id="S2.Ex3.m1.1.1.1.1.3.3.6" xref="S2.Ex3.m1.1.1.1.1.3.4.cmml">;</mo><msup id="S2.Ex3.m1.1.1.1.1.3.3.3" xref="S2.Ex3.m1.1.1.1.1.3.3.3.cmml"><mi id="S2.Ex3.m1.1.1.1.1.3.3.3.2" xref="S2.Ex3.m1.1.1.1.1.3.3.3.2.cmml">𝑯</mi><mi id="S2.Ex3.m1.1.1.1.1.3.3.3.3" xref="S2.Ex3.m1.1.1.1.1.3.3.3.3.cmml">t</mi></msup><mo id="S2.Ex3.m1.1.1.1.1.3.3.7" xref="S2.Ex3.m1.1.1.1.1.3.4.cmml">]</mo></mrow><mo id="S2.Ex3.m1.1.1.1.1.9" xref="S2.Ex3.m1.1.1.1.1.9.cmml">=</mo><mrow id="S2.Ex3.m1.1.1.1.1.5.2" xref="S2.Ex3.m1.1.1.1.1.5.3.cmml"><msub id="S2.Ex3.m1.1.1.1.1.4.1.1" xref="S2.Ex3.m1.1.1.1.1.4.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S2.Ex3.m1.1.1.1.1.4.1.1.2" xref="S2.Ex3.m1.1.1.1.1.4.1.1.2a.cmml">Enc</mtext><mi id="S2.Ex3.m1.1.1.1.1.4.1.1.3" xref="S2.Ex3.m1.1.1.1.1.4.1.1.3.cmml">t</mi></msub><mo id="S2.Ex3.m1.1.1.1.1.5.2a" xref="S2.Ex3.m1.1.1.1.1.5.3.cmml">⁡</mo><mrow id="S2.Ex3.m1.1.1.1.1.5.2.2" xref="S2.Ex3.m1.1.1.1.1.5.3.cmml"><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.2" xref="S2.Ex3.m1.1.1.1.1.5.3.cmml">(</mo><mrow id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml"><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.4" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml">[</mo><msubsup id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.cmml"><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.2" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.2.cmml">𝑬</mi><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.3.cmml">q</mi><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.5" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml">;</mo><msubsup id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.cmml"><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.2" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.2.cmml">𝑬</mi><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.3.cmml">c</mi><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.3.cmml">v</mi></msubsup><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.6" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml">;</mo><msup id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.cmml"><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.2" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.2.cmml">𝑬</mi><mi id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.3" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.3.cmml">t</mi></msup><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.7" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml">]</mo></mrow><mo id="S2.Ex3.m1.1.1.1.1.5.2.2.3" xref="S2.Ex3.m1.1.1.1.1.5.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.Ex3.m1.1.1.1.2" xref="S2.Ex3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.1b"><apply id="S2.Ex3.m1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1"><and id="S2.Ex3.m1.1.1.1.1a.cmml" xref="S2.Ex3.m1.1.1.1"></and><apply id="S2.Ex3.m1.1.1.1.1b.cmml" xref="S2.Ex3.m1.1.1.1"><eq id="S2.Ex3.m1.1.1.1.1.8.cmml" xref="S2.Ex3.m1.1.1.1.1.8"></eq><ci id="S2.Ex3.m1.1.1.1.1.7.cmml" xref="S2.Ex3.m1.1.1.1.1.7">𝑯</ci><list id="S2.Ex3.m1.1.1.1.1.3.4.cmml" xref="S2.Ex3.m1.1.1.1.1.3.3"><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.2.2">𝑯</ci><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.2.3">𝑣</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.1.1.1.3">𝑞</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2.2.2">𝑯</ci><ci id="S2.Ex3.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2.2.3">𝑣</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.2.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.2.2.2.3">𝑐</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.3.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.3.3.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.3.3.3">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.3.3.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.3.3.3.2">𝑯</ci><ci id="S2.Ex3.m1.1.1.1.1.3.3.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.3.3.3.3">𝑡</ci></apply></list></apply><apply id="S2.Ex3.m1.1.1.1.1c.cmml" xref="S2.Ex3.m1.1.1.1"><eq id="S2.Ex3.m1.1.1.1.1.9.cmml" xref="S2.Ex3.m1.1.1.1.1.9"></eq><share href="#S2.Ex3.m1.1.1.1.1.3.cmml" id="S2.Ex3.m1.1.1.1.1d.cmml" xref="S2.Ex3.m1.1.1.1"></share><apply id="S2.Ex3.m1.1.1.1.1.5.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2"><apply id="S2.Ex3.m1.1.1.1.1.4.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.4.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.4.1.1">subscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.4.1.1.2a.cmml" xref="S2.Ex3.m1.1.1.1.1.4.1.1.2"><mtext class="ltx_mathvariant_italic" id="S2.Ex3.m1.1.1.1.1.4.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.4.1.1.2">Enc</mtext></ci><ci id="S2.Ex3.m1.1.1.1.1.4.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.4.1.1.3">𝑡</ci></apply><list id="S2.Ex3.m1.1.1.1.1.5.2.2.1.4.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3"><apply id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.2">𝑬</ci><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.2.3">𝑣</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.1.1.3">𝑞</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2">subscript</csymbol><apply id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.2">𝑬</ci><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.2.3">𝑣</ci></apply><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.2.2.3">𝑐</ci></apply><apply id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3"><csymbol cd="ambiguous" id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.1.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3">superscript</csymbol><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.2.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.2">𝑬</ci><ci id="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.3.cmml" xref="S2.Ex3.m1.1.1.1.1.5.2.2.1.3.3.3">𝑡</ci></apply></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.1c">\boldsymbol{H}=\left[\boldsymbol{H}^{v}_{q};\boldsymbol{H}^{v}_{c};\boldsymbol{H}^{t}\right]=\operatorname{\textit{Enc}}_{t}\left(\left[\boldsymbol{E}^{v}_{q};\boldsymbol{E}^{v}_{c};\boldsymbol{E}^{t}\right]\right),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS3.p3.3" class="ltx_p">where <math id="S2.SS3.p3.2.m1.1" class="ltx_Math" alttext="\boldsymbol{E}^{v}_{q}" display="inline"><semantics id="S2.SS3.p3.2.m1.1a"><msubsup id="S2.SS3.p3.2.m1.1.1" xref="S2.SS3.p3.2.m1.1.1.cmml"><mi id="S2.SS3.p3.2.m1.1.1.2.2" xref="S2.SS3.p3.2.m1.1.1.2.2.cmml">𝑬</mi><mi id="S2.SS3.p3.2.m1.1.1.3" xref="S2.SS3.p3.2.m1.1.1.3.cmml">q</mi><mi id="S2.SS3.p3.2.m1.1.1.2.3" xref="S2.SS3.p3.2.m1.1.1.2.3.cmml">v</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m1.1b"><apply id="S2.SS3.p3.2.m1.1.1.cmml" xref="S2.SS3.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.2.m1.1.1.1.cmml" xref="S2.SS3.p3.2.m1.1.1">subscript</csymbol><apply id="S2.SS3.p3.2.m1.1.1.2.cmml" xref="S2.SS3.p3.2.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.2.m1.1.1.2.1.cmml" xref="S2.SS3.p3.2.m1.1.1">superscript</csymbol><ci id="S2.SS3.p3.2.m1.1.1.2.2.cmml" xref="S2.SS3.p3.2.m1.1.1.2.2">𝑬</ci><ci id="S2.SS3.p3.2.m1.1.1.2.3.cmml" xref="S2.SS3.p3.2.m1.1.1.2.3">𝑣</ci></apply><ci id="S2.SS3.p3.2.m1.1.1.3.cmml" xref="S2.SS3.p3.2.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m1.1c">\boldsymbol{E}^{v}_{q}</annotation></semantics></math>, <math id="S2.SS3.p3.3.m2.1" class="ltx_Math" alttext="\boldsymbol{H}^{v}_{c}" display="inline"><semantics id="S2.SS3.p3.3.m2.1a"><msubsup id="S2.SS3.p3.3.m2.1.1" xref="S2.SS3.p3.3.m2.1.1.cmml"><mi id="S2.SS3.p3.3.m2.1.1.2.2" xref="S2.SS3.p3.3.m2.1.1.2.2.cmml">𝑯</mi><mi id="S2.SS3.p3.3.m2.1.1.3" xref="S2.SS3.p3.3.m2.1.1.3.cmml">c</mi><mi id="S2.SS3.p3.3.m2.1.1.2.3" xref="S2.SS3.p3.3.m2.1.1.2.3.cmml">v</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m2.1b"><apply id="S2.SS3.p3.3.m2.1.1.cmml" xref="S2.SS3.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.3.m2.1.1.1.cmml" xref="S2.SS3.p3.3.m2.1.1">subscript</csymbol><apply id="S2.SS3.p3.3.m2.1.1.2.cmml" xref="S2.SS3.p3.3.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.3.m2.1.1.2.1.cmml" xref="S2.SS3.p3.3.m2.1.1">superscript</csymbol><ci id="S2.SS3.p3.3.m2.1.1.2.2.cmml" xref="S2.SS3.p3.3.m2.1.1.2.2">𝑯</ci><ci id="S2.SS3.p3.3.m2.1.1.2.3.cmml" xref="S2.SS3.p3.3.m2.1.1.2.3">𝑣</ci></apply><ci id="S2.SS3.p3.3.m2.1.1.3.cmml" xref="S2.SS3.p3.3.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m2.1c">\boldsymbol{H}^{v}_{c}</annotation></semantics></math> represent the image token representations for question and candidate image, respectively. We also include an empty candidate that consists only of the question image and text.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">While decoding, to reduce the total number of representations, we only keep the question image and text representations from the empty candidate, and the token representations that corresponds to each knowledge caption text. We concatenate these token representations to form a global representation for decoder to perform cross-attention and generate each answer token autoregressively <cite class="ltx_cite ltx_citemacro_cite">Izacard and Grave (<a href="#bib.bib14" title="" class="ltx_ref">2021b</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Multi-Modal Reranking</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Vanilla retrieval-generation frameworks directly use relevance score from individual image patches. However, a high relevance from a region does not necessarily imply the overall relevance. In this section, we propose multi-modal reranking, as illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Multi-Modal Reranking ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which takes multi-modal question and knowledge as input and produces relevance scores with cross-item interaction.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.12277/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="221" height="95" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Framework of multimodal reranking.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Modeling</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Our ranking model is also finetuned on the multi-modal pretrained language model. For each question-candidate pair, we first encode the question and candidate image separately, and obtain two series of token representations <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="\boldsymbol{E}^{v}_{q},\boldsymbol{E}^{v}_{c}" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.2.2" xref="S3.SS1.p1.1.m1.2.2.3.cmml"><msubsup id="S3.SS1.p1.1.m1.1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.1.1.2.2" xref="S3.SS1.p1.1.m1.1.1.1.1.2.2.cmml">𝑬</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.1.1.3.cmml">q</mi><mi id="S3.SS1.p1.1.m1.1.1.1.1.2.3" xref="S3.SS1.p1.1.m1.1.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S3.SS1.p1.1.m1.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.3.cmml">,</mo><msubsup id="S3.SS1.p1.1.m1.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.2" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2.cmml">𝑬</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.3.cmml">c</mi><mi id="S3.SS1.p1.1.m1.2.2.2.2.2.3" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml">v</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><list id="S3.SS1.p1.1.m1.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2"><apply id="S3.SS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">subscript</csymbol><apply id="S3.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2.2">𝑬</ci><ci id="S3.SS1.p1.1.m1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.2.3">𝑣</ci></apply><ci id="S3.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.1.1.3">𝑞</ci></apply><apply id="S3.SS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">subscript</csymbol><apply id="S3.SS1.p1.1.m1.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2">superscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.2">𝑬</ci><ci id="S3.SS1.p1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.2.3">𝑣</ci></apply><ci id="S3.SS1.p1.1.m1.2.2.2.2.3.cmml" xref="S3.SS1.p1.1.m1.2.2.2.2.3">𝑐</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">\boldsymbol{E}^{v}_{q},\boldsymbol{E}^{v}_{c}</annotation></semantics></math>. Then we concatenate the two series of image token representations, with text token embeddings <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{E}^{t}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">𝑬</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑬</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\boldsymbol{E}^{t}</annotation></semantics></math> following same template in Section <a href="#S2.SS3" title="2.3 Answer Generation ‣ 2 A Knowledge-Intensive Visual Question Answering Framework ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a> for a Transformer to produce fused token representations</p>
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.1" class="ltx_Math" alttext="\boldsymbol{H}^{r}=\operatorname{\textit{Enc}}_{r}\left(\left[\boldsymbol{E}^{v}_{q};\boldsymbol{E}^{v}_{c};\boldsymbol{E}^{t}_{q};\boldsymbol{E}^{t}_{t}\right]\right)." display="block"><semantics id="S3.Ex4.m1.1a"><mrow id="S3.Ex4.m1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml"><mrow id="S3.Ex4.m1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.cmml"><msup id="S3.Ex4.m1.1.1.1.1.4" xref="S3.Ex4.m1.1.1.1.1.4.cmml"><mi id="S3.Ex4.m1.1.1.1.1.4.2" xref="S3.Ex4.m1.1.1.1.1.4.2.cmml">𝑯</mi><mi id="S3.Ex4.m1.1.1.1.1.4.3" xref="S3.Ex4.m1.1.1.1.1.4.3.cmml">r</mi></msup><mo id="S3.Ex4.m1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.Ex4.m1.1.1.1.1.2.2" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml"><msub id="S3.Ex4.m1.1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.1.1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.1.1.2a.cmml">Enc</mtext><mi id="S3.Ex4.m1.1.1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3.cmml">r</mi></msub><mo id="S3.Ex4.m1.1.1.1.1.2.2a" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml">⁡</mo><mrow id="S3.Ex4.m1.1.1.1.1.2.2.2" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml"><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.2" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml">(</mo><mrow id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml"><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.5" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml">[</mo><msubsup id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.cmml"><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.2.cmml">𝑬</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.3.cmml">q</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.3.cmml">v</mi></msubsup><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.6" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml">;</mo><msubsup id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.2.cmml">𝑬</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.3.cmml">c</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.3.cmml">v</mi></msubsup><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.7" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml">;</mo><msubsup id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.2.cmml">𝑬</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.3.cmml">q</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.3.cmml">t</mi></msubsup><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.8" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml">;</mo><msubsup id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.cmml"><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.2" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.2.cmml">𝑬</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.3.cmml">t</mi><mi id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.3" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.3.cmml">t</mi></msubsup><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.9" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml">]</mo></mrow><mo id="S3.Ex4.m1.1.1.1.1.2.2.2.3" xref="S3.Ex4.m1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex4.m1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.1b"><apply id="S3.Ex4.m1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1"><eq id="S3.Ex4.m1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.3"></eq><apply id="S3.Ex4.m1.1.1.1.1.4.cmml" xref="S3.Ex4.m1.1.1.1.1.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.4.1.cmml" xref="S3.Ex4.m1.1.1.1.1.4">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.4.2.cmml" xref="S3.Ex4.m1.1.1.1.1.4.2">𝑯</ci><ci id="S3.Ex4.m1.1.1.1.1.4.3.cmml" xref="S3.Ex4.m1.1.1.1.1.4.3">𝑟</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2"><apply id="S3.Ex4.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.2a.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S3.Ex4.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.2">Enc</mtext></ci><ci id="S3.Ex4.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1.1.3">𝑟</ci></apply><list id="S3.Ex4.m1.1.1.1.1.2.2.2.1.5.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4"><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1">subscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.2">𝑬</ci><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.2.3">𝑣</ci></apply><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.1.1.3">𝑞</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2">subscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.2">𝑬</ci><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.2.3">𝑣</ci></apply><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.2.2.3">𝑐</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3">subscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.2">𝑬</ci><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.2.3">𝑡</ci></apply><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.3.3.3">𝑞</ci></apply><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4">subscript</csymbol><apply id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.2">𝑬</ci><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.2.3">𝑡</ci></apply><ci id="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.3.cmml" xref="S3.Ex4.m1.1.1.1.1.2.2.2.1.4.4.3">𝑡</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.1c">\boldsymbol{H}^{r}=\operatorname{\textit{Enc}}_{r}\left(\left[\boldsymbol{E}^{v}_{q};\boldsymbol{E}^{v}_{c};\boldsymbol{E}^{t}_{q};\boldsymbol{E}^{t}_{t}\right]\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.3" class="ltx_p">We follow <cite class="ltx_cite ltx_citemacro_citet">Zhuang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> and use 1-step decoding to obtain the score from the unnormalized loglikelihood of a special token “<span id="S3.SS1.p1.3.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">&lt;extra_id_10&gt;</span>”</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.2" class="ltx_Math" alttext="\hat{r}_{qc}=\textit{Dense}\left(\textit{Dec}\left(\boldsymbol{H}^{r}\right)\right)_{\left(\texttt{&lt;extra\_id\_10&gt;}\right)}." display="block"><semantics id="S3.Ex5.m1.2a"><mrow id="S3.Ex5.m1.2.2.1" xref="S3.Ex5.m1.2.2.1.1.cmml"><mrow id="S3.Ex5.m1.2.2.1.1" xref="S3.Ex5.m1.2.2.1.1.cmml"><msub id="S3.Ex5.m1.2.2.1.1.3" xref="S3.Ex5.m1.2.2.1.1.3.cmml"><mover accent="true" id="S3.Ex5.m1.2.2.1.1.3.2" xref="S3.Ex5.m1.2.2.1.1.3.2.cmml"><mi id="S3.Ex5.m1.2.2.1.1.3.2.2" xref="S3.Ex5.m1.2.2.1.1.3.2.2.cmml">r</mi><mo id="S3.Ex5.m1.2.2.1.1.3.2.1" xref="S3.Ex5.m1.2.2.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.Ex5.m1.2.2.1.1.3.3" xref="S3.Ex5.m1.2.2.1.1.3.3.cmml"><mi id="S3.Ex5.m1.2.2.1.1.3.3.2" xref="S3.Ex5.m1.2.2.1.1.3.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.1.1.3.3.1" xref="S3.Ex5.m1.2.2.1.1.3.3.1.cmml">​</mo><mi id="S3.Ex5.m1.2.2.1.1.3.3.3" xref="S3.Ex5.m1.2.2.1.1.3.3.3.cmml">c</mi></mrow></msub><mo id="S3.Ex5.m1.2.2.1.1.2" xref="S3.Ex5.m1.2.2.1.1.2.cmml">=</mo><mrow id="S3.Ex5.m1.2.2.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.Ex5.m1.2.2.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.3a.cmml">Dense</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.2.cmml">​</mo><msub id="S3.Ex5.m1.2.2.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.cmml"><mrow id="S3.Ex5.m1.2.2.1.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex5.m1.2.2.1.1.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3a.cmml">Dec</mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">𝑯</mi><mi id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">r</mi></msup><mo id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex5.m1.2.2.1.1.1.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.Ex5.m1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1a.cmml"><mo id="S3.Ex5.m1.1.1.1.3.1" xref="S3.Ex5.m1.1.1.1.1a.cmml">(</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex5.m1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.cmml">&lt;extra_id_10&gt;</mtext><mo id="S3.Ex5.m1.1.1.1.3.2" xref="S3.Ex5.m1.1.1.1.1a.cmml">)</mo></mrow></msub></mrow></mrow><mo lspace="0em" id="S3.Ex5.m1.2.2.1.2" xref="S3.Ex5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.2b"><apply id="S3.Ex5.m1.2.2.1.1.cmml" xref="S3.Ex5.m1.2.2.1"><eq id="S3.Ex5.m1.2.2.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.2"></eq><apply id="S3.Ex5.m1.2.2.1.1.3.cmml" xref="S3.Ex5.m1.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m1.2.2.1.1.3.1.cmml" xref="S3.Ex5.m1.2.2.1.1.3">subscript</csymbol><apply id="S3.Ex5.m1.2.2.1.1.3.2.cmml" xref="S3.Ex5.m1.2.2.1.1.3.2"><ci id="S3.Ex5.m1.2.2.1.1.3.2.1.cmml" xref="S3.Ex5.m1.2.2.1.1.3.2.1">^</ci><ci id="S3.Ex5.m1.2.2.1.1.3.2.2.cmml" xref="S3.Ex5.m1.2.2.1.1.3.2.2">𝑟</ci></apply><apply id="S3.Ex5.m1.2.2.1.1.3.3.cmml" xref="S3.Ex5.m1.2.2.1.1.3.3"><times id="S3.Ex5.m1.2.2.1.1.3.3.1.cmml" xref="S3.Ex5.m1.2.2.1.1.3.3.1"></times><ci id="S3.Ex5.m1.2.2.1.1.3.3.2.cmml" xref="S3.Ex5.m1.2.2.1.1.3.3.2">𝑞</ci><ci id="S3.Ex5.m1.2.2.1.1.3.3.3.cmml" xref="S3.Ex5.m1.2.2.1.1.3.3.3">𝑐</ci></apply></apply><apply id="S3.Ex5.m1.2.2.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1"><times id="S3.Ex5.m1.2.2.1.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.2"></times><ci id="S3.Ex5.m1.2.2.1.1.1.3a.cmml" xref="S3.Ex5.m1.2.2.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.Ex5.m1.2.2.1.1.1.3.cmml" xref="S3.Ex5.m1.2.2.1.1.1.3">Dense</mtext></ci><apply id="S3.Ex5.m1.2.2.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex5.m1.2.2.1.1.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1">subscript</csymbol><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1"><times id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3a.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_italic" id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.3">Dec</mtext></ci><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">𝑯</ci><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">𝑟</ci></apply></apply><ci id="S3.Ex5.m1.1.1.1.1a.cmml" xref="S3.Ex5.m1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" mathsize="70%" id="S3.Ex5.m1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1">&lt;extra_id_10&gt;</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.2c">\hat{r}_{qc}=\textit{Dense}\left(\textit{Dec}\left(\boldsymbol{H}^{r}\right)\right)_{\left(\texttt{&lt;extra\_id\_10&gt;}\right)}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Ranker Training</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Because we do not have ground-truth relevance scores, we adopt distant supervision labels for reranking training. In a typical VQA setting, each answer consists of 10 answer candidate annotations. We count the number of answer candidates that occur in the knowledge candidate text as <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="o" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">o</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">o</annotation></semantics></math>. The distantly supervised relevance score is obtained similar to VQA accuracy <cite class="ltx_cite ltx_citemacro_cite">Antol et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite></p>
<table id="S3.Ex6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex6.m1.3" class="ltx_Math" alttext="r_{qc}=\min\left\{o/3,1\right\}." display="block"><semantics id="S3.Ex6.m1.3a"><mrow id="S3.Ex6.m1.3.3.1" xref="S3.Ex6.m1.3.3.1.1.cmml"><mrow id="S3.Ex6.m1.3.3.1.1" xref="S3.Ex6.m1.3.3.1.1.cmml"><msub id="S3.Ex6.m1.3.3.1.1.3" xref="S3.Ex6.m1.3.3.1.1.3.cmml"><mi id="S3.Ex6.m1.3.3.1.1.3.2" xref="S3.Ex6.m1.3.3.1.1.3.2.cmml">r</mi><mrow id="S3.Ex6.m1.3.3.1.1.3.3" xref="S3.Ex6.m1.3.3.1.1.3.3.cmml"><mi id="S3.Ex6.m1.3.3.1.1.3.3.2" xref="S3.Ex6.m1.3.3.1.1.3.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex6.m1.3.3.1.1.3.3.1" xref="S3.Ex6.m1.3.3.1.1.3.3.1.cmml">​</mo><mi id="S3.Ex6.m1.3.3.1.1.3.3.3" xref="S3.Ex6.m1.3.3.1.1.3.3.3.cmml">c</mi></mrow></msub><mo id="S3.Ex6.m1.3.3.1.1.2" xref="S3.Ex6.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex6.m1.3.3.1.1.1.1" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml"><mi id="S3.Ex6.m1.1.1" xref="S3.Ex6.m1.1.1.cmml">min</mi><mo id="S3.Ex6.m1.3.3.1.1.1.1a" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml">⁡</mo><mrow id="S3.Ex6.m1.3.3.1.1.1.1.1" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml"><mo id="S3.Ex6.m1.3.3.1.1.1.1.1.2" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml">{</mo><mrow id="S3.Ex6.m1.3.3.1.1.1.1.1.1" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S3.Ex6.m1.3.3.1.1.1.1.1.1.2" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.2.cmml">o</mi><mo id="S3.Ex6.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.1.cmml">/</mo><mn id="S3.Ex6.m1.3.3.1.1.1.1.1.1.3" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.3.cmml">3</mn></mrow><mo id="S3.Ex6.m1.3.3.1.1.1.1.1.3" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml">,</mo><mn id="S3.Ex6.m1.2.2" xref="S3.Ex6.m1.2.2.cmml">1</mn><mo id="S3.Ex6.m1.3.3.1.1.1.1.1.4" xref="S3.Ex6.m1.3.3.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex6.m1.3.3.1.2" xref="S3.Ex6.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex6.m1.3b"><apply id="S3.Ex6.m1.3.3.1.1.cmml" xref="S3.Ex6.m1.3.3.1"><eq id="S3.Ex6.m1.3.3.1.1.2.cmml" xref="S3.Ex6.m1.3.3.1.1.2"></eq><apply id="S3.Ex6.m1.3.3.1.1.3.cmml" xref="S3.Ex6.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.Ex6.m1.3.3.1.1.3.1.cmml" xref="S3.Ex6.m1.3.3.1.1.3">subscript</csymbol><ci id="S3.Ex6.m1.3.3.1.1.3.2.cmml" xref="S3.Ex6.m1.3.3.1.1.3.2">𝑟</ci><apply id="S3.Ex6.m1.3.3.1.1.3.3.cmml" xref="S3.Ex6.m1.3.3.1.1.3.3"><times id="S3.Ex6.m1.3.3.1.1.3.3.1.cmml" xref="S3.Ex6.m1.3.3.1.1.3.3.1"></times><ci id="S3.Ex6.m1.3.3.1.1.3.3.2.cmml" xref="S3.Ex6.m1.3.3.1.1.3.3.2">𝑞</ci><ci id="S3.Ex6.m1.3.3.1.1.3.3.3.cmml" xref="S3.Ex6.m1.3.3.1.1.3.3.3">𝑐</ci></apply></apply><apply id="S3.Ex6.m1.3.3.1.1.1.2.cmml" xref="S3.Ex6.m1.3.3.1.1.1.1"><min id="S3.Ex6.m1.1.1.cmml" xref="S3.Ex6.m1.1.1"></min><apply id="S3.Ex6.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1"><divide id="S3.Ex6.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.1"></divide><ci id="S3.Ex6.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.2">𝑜</ci><cn type="integer" id="S3.Ex6.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S3.Ex6.m1.3.3.1.1.1.1.1.1.3">3</cn></apply><cn type="integer" id="S3.Ex6.m1.2.2.cmml" xref="S3.Ex6.m1.2.2">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m1.3c">r_{qc}=\min\left\{o/3,1\right\}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.2" class="ltx_p">On OK-VQA, we split the training dataset of original dataset into sub-training and -development sets. At each training step, for a question <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">q</annotation></semantics></math>, we uniformly sample a candidate set <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{C}" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\mathcal{C}</annotation></semantics></math> from the retrieval results, and apply pairwise logistics ranking loss <cite class="ltx_cite ltx_citemacro_cite">Burges et al. (<a href="#bib.bib4" title="" class="ltx_ref">2005</a>)</cite>, which compares the ranking between all pairs of candidates in the set</p>
<table id="S3.Ex7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex7.m1.3" class="ltx_Math" alttext="\ell(q)=\sum_{c\in\mathcal{C}}\sum_{c^{\prime}\in\mathcal{C}}\mathbb{I}_{r_{qc}&gt;r_{qc^{\prime}}}\log\left(1+e^{\hat{r}_{qc^{\prime}}-r_{qc}}\right)." display="block"><semantics id="S3.Ex7.m1.3a"><mrow id="S3.Ex7.m1.3.3.1" xref="S3.Ex7.m1.3.3.1.1.cmml"><mrow id="S3.Ex7.m1.3.3.1.1" xref="S3.Ex7.m1.3.3.1.1.cmml"><mrow id="S3.Ex7.m1.3.3.1.1.3" xref="S3.Ex7.m1.3.3.1.1.3.cmml"><mi mathvariant="normal" id="S3.Ex7.m1.3.3.1.1.3.2" xref="S3.Ex7.m1.3.3.1.1.3.2.cmml">ℓ</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.3.1" xref="S3.Ex7.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.Ex7.m1.3.3.1.1.3.3.2" xref="S3.Ex7.m1.3.3.1.1.3.cmml"><mo stretchy="false" id="S3.Ex7.m1.3.3.1.1.3.3.2.1" xref="S3.Ex7.m1.3.3.1.1.3.cmml">(</mo><mi id="S3.Ex7.m1.1.1" xref="S3.Ex7.m1.1.1.cmml">q</mi><mo stretchy="false" id="S3.Ex7.m1.3.3.1.1.3.3.2.2" xref="S3.Ex7.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.Ex7.m1.3.3.1.1.2" xref="S3.Ex7.m1.3.3.1.1.2.cmml">=</mo><mrow id="S3.Ex7.m1.3.3.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.cmml"><munder id="S3.Ex7.m1.3.3.1.1.1.2" xref="S3.Ex7.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.2.2" xref="S3.Ex7.m1.3.3.1.1.1.2.2.cmml">∑</mo><mrow id="S3.Ex7.m1.3.3.1.1.1.2.3" xref="S3.Ex7.m1.3.3.1.1.1.2.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.2.3.2" xref="S3.Ex7.m1.3.3.1.1.1.2.3.2.cmml">c</mi><mo id="S3.Ex7.m1.3.3.1.1.1.2.3.1" xref="S3.Ex7.m1.3.3.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.3.3.1.1.1.2.3.3" xref="S3.Ex7.m1.3.3.1.1.1.2.3.3.cmml">𝒞</mi></mrow></munder><mrow id="S3.Ex7.m1.3.3.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.cmml"><munder id="S3.Ex7.m1.3.3.1.1.1.1.2" xref="S3.Ex7.m1.3.3.1.1.1.1.2.cmml"><mo movablelimits="false" id="S3.Ex7.m1.3.3.1.1.1.1.2.2" xref="S3.Ex7.m1.3.3.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.Ex7.m1.3.3.1.1.1.1.2.3" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.cmml"><msup id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.2" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.2.cmml">c</mi><mo id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.3" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.3.cmml">′</mo></msup><mo id="S3.Ex7.m1.3.3.1.1.1.1.2.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex7.m1.3.3.1.1.1.1.2.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.3.cmml">𝒞</mi></mrow></munder><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.Ex7.m1.3.3.1.1.1.1.1.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.2.cmml">𝕀</mi><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.cmml"><msub id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.2.cmml">r</mi><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.1.cmml">​</mo><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.3.cmml">c</mi></mrow></msub><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.1.cmml">&gt;</mo><msub id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.2.cmml">r</mi><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.1.cmml">​</mo><msup id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.2.cmml">c</mi><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.3.cmml">′</mo></msup></mrow></msub></mrow></msub><mo lspace="0.167em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.1.1.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex7.m1.2.2" xref="S3.Ex7.m1.2.2.cmml">log</mi><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1a" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml"><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml">(</mo><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml">+</mo><msup id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">e</mi><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml"><msub id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.cmml"><mover accent="true" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.2.cmml">r</mi><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.1.cmml">^</mo></mover><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.1.cmml">​</mo><msup id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.2.cmml">c</mi><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.3.cmml">′</mo></msup></mrow></msub><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.1.cmml">−</mo><msub id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml">r</mi><mrow id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">q</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.1" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">c</mi></mrow></msub></mrow></msup></mrow><mo id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex7.m1.3.3.1.2" xref="S3.Ex7.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex7.m1.3b"><apply id="S3.Ex7.m1.3.3.1.1.cmml" xref="S3.Ex7.m1.3.3.1"><eq id="S3.Ex7.m1.3.3.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.2"></eq><apply id="S3.Ex7.m1.3.3.1.1.3.cmml" xref="S3.Ex7.m1.3.3.1.1.3"><times id="S3.Ex7.m1.3.3.1.1.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.3.1"></times><ci id="S3.Ex7.m1.3.3.1.1.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.3.2">ℓ</ci><ci id="S3.Ex7.m1.1.1.cmml" xref="S3.Ex7.m1.1.1">𝑞</ci></apply><apply id="S3.Ex7.m1.3.3.1.1.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1"><apply id="S3.Ex7.m1.3.3.1.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S3.Ex7.m1.3.3.1.1.1.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2.2"></sum><apply id="S3.Ex7.m1.3.3.1.1.1.2.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2.3"><in id="S3.Ex7.m1.3.3.1.1.1.2.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2.3.1"></in><ci id="S3.Ex7.m1.3.3.1.1.1.2.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2.3.2">𝑐</ci><ci id="S3.Ex7.m1.3.3.1.1.1.2.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.2.3.3">𝒞</ci></apply></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1"><apply id="S3.Ex7.m1.3.3.1.1.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2">subscript</csymbol><sum id="S3.Ex7.m1.3.3.1.1.1.1.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.2"></sum><apply id="S3.Ex7.m1.3.3.1.1.1.1.2.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3"><in id="S3.Ex7.m1.3.3.1.1.1.1.2.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.1"></in><apply id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2">superscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.2">𝑐</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.2.3">′</ci></apply><ci id="S3.Ex7.m1.3.3.1.1.1.1.2.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.2.3.3">𝒞</ci></apply></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1"><times id="S3.Ex7.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.2"></times><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.2">𝕀</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3"><gt id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.1"></gt><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.2">𝑟</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3"><times id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.1"></times><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.2">𝑞</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.2.3.3">𝑐</ci></apply></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.2">𝑟</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3"><times id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.1"></times><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.2">𝑞</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3">superscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.2">𝑐</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.3.3.3.3.3.3">′</ci></apply></apply></apply></apply></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1"><log id="S3.Ex7.m1.2.2.cmml" xref="S3.Ex7.m1.2.2"></log><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1"><plus id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.1"></plus><cn type="integer" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.2">1</cn><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝑒</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3"><minus id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.1"></minus><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2">subscript</csymbol><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2"><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.1">^</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.2.2">𝑟</ci></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3"><times id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.1"></times><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.2">𝑞</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3">superscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.2">𝑐</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.2.3.3.3">′</ci></apply></apply></apply><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.2">𝑟</ci><apply id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3"><times id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.1"></times><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.2">𝑞</ci><ci id="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.Ex7.m1.3.3.1.1.1.1.1.1.1.1.1.3.3.3.3.3">𝑐</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex7.m1.3c">\ell(q)=\sum_{c\in\mathcal{C}}\sum_{c^{\prime}\in\mathcal{C}}\mathbb{I}_{r_{qc}&gt;r_{qc^{\prime}}}\log\left(1+e^{\hat{r}_{qc^{\prime}}-r_{qc}}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Discrepancy on Applying Reranking for Answer Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">During answer generation training, it is straightforward to apply the ranking model and use the reranked top candidates as input. However, directly applying reranking on both training and testing will instead hurt the model performance.
This is because applying the ranker on the training set, from which the ranker is trained, performs much better than when applied to the unseen test set.
As we will illustrate in Section <a href="#S4.SS3" title="4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>, learning answer generation with higher quality ranking results while testing on lower quality ranking results will in general have a negative impact to answer generation performance. Therefore, we will keep the initial retrieval results for answer generation training, while using the reranked results for model testing.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We conduct experiments on OK-VQA <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> and A-OKVQA <cite class="ltx_cite ltx_citemacro_cite">Schwenk et al. (<a href="#bib.bib31" title="" class="ltx_ref">2022</a>)</cite>. OK-VQA introduces visual questions that requires external knowledge. A-OKVQA further emphasizes commonsense reasoning over world knowledge. For both datasets, we evaluate the performance on the validation set. Following the standard setting, we use VQA accuracy as the metric. We use ALIGN <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>)</cite> for image-text retrieval, and use PaLI <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2023b</a>)</cite> to initialize (vision and text) Transformers in answer generation and reranking independently. Besides retrieved knowledge candidates, we also follow REVIVE <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> and use candidates generated from GPT-3. For our model with REVIVE GPT-3, we replace the last 5 candidates of the aggregated candidates with top-5 GPT-3 generated candidates from <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>. Appendix <a href="#A1" title="Appendix A Experiment Setup ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> includes a detailed experimental setup.</p>
</div>
<figure id="S4.SS1.tab1" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<table id="S4.SS1.tab1.1" class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.SS1.tab1.1.1.1" class="ltx_tr">
<th id="S4.SS1.tab1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.SS1.tab1.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<td id="S4.SS1.tab1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.SS1.tab1.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></td>
</tr>
<tr id="S4.SS1.tab1.1.2.2" class="ltx_tr">
<th id="S4.SS1.tab1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BAN+KG <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">26.7</td>
</tr>
<tr id="S4.SS1.tab1.1.3.3" class="ltx_tr">
<th id="S4.SS1.tab1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mucko <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib40" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.3.3.2" class="ltx_td ltx_align_center">29.2</td>
</tr>
<tr id="S4.SS1.tab1.1.4.4" class="ltx_tr">
<th id="S4.SS1.tab1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ConceptBERT <cite class="ltx_cite ltx_citemacro_citep">(Gardères et al., <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.4.4.2" class="ltx_td ltx_align_center">33.7</td>
</tr>
<tr id="S4.SS1.tab1.1.5.5" class="ltx_tr">
<th id="S4.SS1.tab1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KRISP <cite class="ltx_cite ltx_citemacro_citep">(Marino et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.5.5.2" class="ltx_td ltx_align_center">38.9</td>
</tr>
<tr id="S4.SS1.tab1.1.6.6" class="ltx_tr">
<th id="S4.SS1.tab1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Vis-DPR <cite class="ltx_cite ltx_citemacro_citep">(Luo et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.6.6.2" class="ltx_td ltx_align_center">39.2</td>
</tr>
<tr id="S4.SS1.tab1.1.7.7" class="ltx_tr">
<th id="S4.SS1.tab1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MAVEx <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.7.7.2" class="ltx_td ltx_align_center">40.3</td>
</tr>
<tr id="S4.SS1.tab1.1.8.8" class="ltx_tr">
<th id="S4.SS1.tab1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KAT <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.8.8.2" class="ltx_td ltx_align_center">44.3</td>
</tr>
<tr id="S4.SS1.tab1.1.9.9" class="ltx_tr">
<th id="S4.SS1.tab1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">TRiG <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.9.9.2" class="ltx_td ltx_align_center">49.4</td>
</tr>
<tr id="S4.SS1.tab1.1.10.10" class="ltx_tr">
<th id="S4.SS1.tab1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Our model</th>
<td id="S4.SS1.tab1.1.10.10.2" class="ltx_td ltx_align_center"><span id="S4.SS1.tab1.1.10.10.2.1" class="ltx_text ltx_font_bold">52.6</span></td>
</tr>
<tr id="S4.SS1.tab1.1.11.11" class="ltx_tr">
<th id="S4.SS1.tab1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2">
<span id="S4.SS1.tab1.1.11.11.1.1" class="ltx_ERROR undefined">\hdashline</span>     <span id="S4.SS1.tab1.1.11.11.1.2" class="ltx_text ltx_font_italic">models with GPT-3 generated candidates</span>
</th>
</tr>
<tr id="S4.SS1.tab1.1.12.12" class="ltx_tr">
<th id="S4.SS1.tab1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PICa <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib39" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.12.12.2" class="ltx_td ltx_align_center">48.0</td>
</tr>
<tr id="S4.SS1.tab1.1.13.13" class="ltx_tr">
<th id="S4.SS1.tab1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KAT <cite class="ltx_cite ltx_citemacro_citep">(Gui et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.13.13.2" class="ltx_td ltx_align_center">53.1</td>
</tr>
<tr id="S4.SS1.tab1.1.14.14" class="ltx_tr">
<th id="S4.SS1.tab1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">REVIVE <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.SS1.tab1.1.14.14.2" class="ltx_td ltx_align_center">56.6</td>
</tr>
<tr id="S4.SS1.tab1.1.15.15" class="ltx_tr">
<th id="S4.SS1.tab1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Our model + REVIVE GPT-3</th>
<td id="S4.SS1.tab1.1.15.15.2" class="ltx_td ltx_align_center"><span id="S4.SS1.tab1.1.15.15.2.1" class="ltx_text ltx_font_bold">57.2</span></td>
</tr>
<tr id="S4.SS1.tab1.1.16.16" class="ltx_tr">
<th id="S4.SS1.tab1.1.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">
<span id="S4.SS1.tab1.1.16.16.1.1" class="ltx_ERROR undefined">\hdashline</span>Our model w/ oracle ranking</th>
<td id="S4.SS1.tab1.1.16.16.2" class="ltx_td ltx_align_center ltx_border_bb">64.4</td>
</tr>
</tbody>
</table>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results comparison on OK-VQA dataset.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.T2" class="ltx_table ltx_figure_panel">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ViLBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">30.6</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a href="#bib.bib35" title="" class="ltx_ref">2019</a>)</cite>
</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center">30.7</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KRISP <cite class="ltx_cite ltx_citemacro_citep">(Marino et al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>
</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center">33.7</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">GPV-2 <cite class="ltx_cite ltx_citemacro_citep">(Kamath et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center">48.6</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Our model</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.6.5.2.1" class="ltx_text ltx_font_bold">51.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results comparision on A-OKVQA dataset.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<section id="S4.SS2" class="ltx_subsection ltx_figure_panel">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Our results on Table <a href="#S4.SS1" title="4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and Table <a href="#S4.T2" title="Table 2 ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrate the performance compared to some existing work. Results on Table <a href="#S4.SS1" title="4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> show that we provide competitive performance compared to these systems. We also include a comparison for models with GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite> generated candidates. We find that our framework can further improve the answer generation quality with GPT-3 generated candidates from <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite> and outperform these baselines.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We also show that an oracle ranking from distant supervision can provide a promising upper bound, indicating that there is still a large room for future work on improving ranking in this challenge.</p>
</div>
<section id="S4.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effects of Ranking Methods.</h4>

<div id="S4.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS0.Px1.p1.1" class="ltx_p">We further conduct experiments with different ranking methods to illustrate the performance of multi-modal ranking. The results are shown in Table <a href="#S4.T3" title="Table 3 ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We compared variants of our model, including the model that generates answer directly without external knowledge, and the model with initial image retrieval without further reranking. We can find the steady improvement brought by multi-modal reranking on both datasets. We provide additional comparison to other reranking strategies in Appendix <a href="#A2" title="Appendix B Additional Comparison with Other Ranking Strategies ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a> and zero-shot multi-modal large models in Appendix <a href="#A3" title="Appendix C Comparison With Zero-Shot Multi-Modal Large Models ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a> that are not instruction tuned on OK-VQA.</p>
</div>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Training and Testing Discrepancy</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">As we discussed in Section <a href="#S3.SS3" title="3.3 Discrepancy on Applying Reranking for Answer Generation ‣ 3 Multi-Modal Reranking ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>, directly applying a trained ranking model on both training and testing will hurt the performance. We further illustrate it empirically in Table <a href="#S4.T4" title="Table 4 ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We can find that if the model is trained on higher quality candidates while applied on lower quality candidates, we will observe a drastic performance drop. On the contrary, when the quality in test is better than in training, we can still find steady improvement. This phenomenon indicates that an answer generator trained with higher-quality data can not effectively conduct knowledge reasoning on noisier data, and we should therefore train the model with noisier data.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA Accuracy</span></th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.1.2.2.1.1" class="ltx_text ltx_font_smallcaps">Ok-vqa</span></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T3.1.2.2.2.1" class="ltx_text ltx_font_smallcaps">A-okvqa</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<th id="S4.T3.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">No retrieval</th>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t">50.6</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">50.4</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<th id="S4.T3.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">   + Image Retrieval</th>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center">52.1</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center">50.3</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<th id="S4.T3.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">    + Multimodal Reranking</th>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.5.3.2.1" class="ltx_text ltx_font_bold">52.6</span></td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.1.5.3.3.1" class="ltx_text ltx_font_bold">51.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Effects of multimodal reranking, compared to model without retrieval and model without reranking.</figcaption>
</figure>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.7.8.1" class="ltx_tr">
<th id="S4.T4.7.8.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S4.T4.7.8.1.1.1" class="ltx_text ltx_font_bold">Source of Candidates</span></th>
<th id="S4.T4.7.8.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.7.8.1.2.1" class="ltx_text ltx_font_bold">VQA</span></th>
</tr>
<tr id="S4.T4.7.9.2" class="ltx_tr">
<th id="S4.T4.7.9.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Train</th>
<th id="S4.T4.7.9.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Test</th>
<th id="S4.T4.7.9.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Discrepancy</th>
<th id="S4.T4.7.9.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T4.7.9.2.4.1" class="ltx_text ltx_font_bold">Accuracy</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1" class="ltx_tr">
<td id="S4.T4.1.1.2" class="ltx_td ltx_align_left ltx_border_t">Retrieval</td>
<td id="S4.T4.1.1.3" class="ltx_td ltx_align_left ltx_border_t">Retrieval</td>
<td id="S4.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T4.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.1.1.1.m1.1a"><mo stretchy="false" id="S4.T4.1.1.1.m1.1.1" xref="S4.T4.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></td>
<td id="S4.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_t">52.1</td>
</tr>
<tr id="S4.T4.2.2" class="ltx_tr">
<td id="S4.T4.2.2.2" class="ltx_td ltx_align_left">Reranking</td>
<td id="S4.T4.2.2.3" class="ltx_td ltx_align_left">Reranking</td>
<td id="S4.T4.2.2.1" class="ltx_td ltx_align_center"><math id="S4.T4.2.2.1.m1.1" class="ltx_Math" alttext="\searrow" display="inline"><semantics id="S4.T4.2.2.1.m1.1a"><mo id="S4.T4.2.2.1.m1.1.1" xref="S4.T4.2.2.1.m1.1.1.cmml">↘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.1.m1.1b"><ci id="S4.T4.2.2.1.m1.1.1.cmml" xref="S4.T4.2.2.1.m1.1.1">↘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.1.m1.1c">\searrow</annotation></semantics></math></td>
<td id="S4.T4.2.2.4" class="ltx_td ltx_align_center">50.7</td>
</tr>
<tr id="S4.T4.3.3" class="ltx_tr">
<td id="S4.T4.3.3.2" class="ltx_td ltx_align_left">Retrieval</td>
<td id="S4.T4.3.3.3" class="ltx_td ltx_align_left">Reranking</td>
<td id="S4.T4.3.3.1" class="ltx_td ltx_align_center"><math id="S4.T4.3.3.1.m1.1" class="ltx_Math" alttext="\nearrow" display="inline"><semantics id="S4.T4.3.3.1.m1.1a"><mo id="S4.T4.3.3.1.m1.1.1" xref="S4.T4.3.3.1.m1.1.1.cmml">↗</mo><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.1.m1.1b"><ci id="S4.T4.3.3.1.m1.1.1.cmml" xref="S4.T4.3.3.1.m1.1.1">↗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.1.m1.1c">\nearrow</annotation></semantics></math></td>
<td id="S4.T4.3.3.4" class="ltx_td ltx_align_center">52.6</td>
</tr>
<tr id="S4.T4.4.4" class="ltx_tr">
<td id="S4.T4.4.4.2" class="ltx_td ltx_align_left">
<span id="S4.T4.4.4.2.1" class="ltx_ERROR undefined">\hdashline</span>Oracle</td>
<td id="S4.T4.4.4.3" class="ltx_td ltx_align_left">Oracle</td>
<td id="S4.T4.4.4.1" class="ltx_td ltx_align_center"><math id="S4.T4.4.4.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.4.4.1.m1.1a"><mo stretchy="false" id="S4.T4.4.4.1.m1.1.1" xref="S4.T4.4.4.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.4.1.m1.1b"><ci id="S4.T4.4.4.1.m1.1.1.cmml" xref="S4.T4.4.4.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.4.1.m1.1c">\rightarrow</annotation></semantics></math></td>
<td id="S4.T4.4.4.4" class="ltx_td ltx_align_center">64.4</td>
</tr>
<tr id="S4.T4.5.5" class="ltx_tr">
<td id="S4.T4.5.5.2" class="ltx_td ltx_align_left">Oracle</td>
<td id="S4.T4.5.5.3" class="ltx_td ltx_align_left">Retrieval</td>
<td id="S4.T4.5.5.1" class="ltx_td ltx_align_center"><math id="S4.T4.5.5.1.m1.1" class="ltx_Math" alttext="\searrow" display="inline"><semantics id="S4.T4.5.5.1.m1.1a"><mo id="S4.T4.5.5.1.m1.1.1" xref="S4.T4.5.5.1.m1.1.1.cmml">↘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.5.5.1.m1.1b"><ci id="S4.T4.5.5.1.m1.1.1.cmml" xref="S4.T4.5.5.1.m1.1.1">↘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.5.5.1.m1.1c">\searrow</annotation></semantics></math></td>
<td id="S4.T4.5.5.4" class="ltx_td ltx_align_center">47.2</td>
</tr>
<tr id="S4.T4.6.6" class="ltx_tr">
<td id="S4.T4.6.6.2" class="ltx_td ltx_align_left">Retrieval</td>
<td id="S4.T4.6.6.3" class="ltx_td ltx_align_left">Oracle</td>
<td id="S4.T4.6.6.1" class="ltx_td ltx_align_center"><math id="S4.T4.6.6.1.m1.1" class="ltx_Math" alttext="\nearrow" display="inline"><semantics id="S4.T4.6.6.1.m1.1a"><mo id="S4.T4.6.6.1.m1.1.1" xref="S4.T4.6.6.1.m1.1.1.cmml">↗</mo><annotation-xml encoding="MathML-Content" id="S4.T4.6.6.1.m1.1b"><ci id="S4.T4.6.6.1.m1.1.1.cmml" xref="S4.T4.6.6.1.m1.1.1">↗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.6.6.1.m1.1c">\nearrow</annotation></semantics></math></td>
<td id="S4.T4.6.6.4" class="ltx_td ltx_align_center">59.7</td>
</tr>
<tr id="S4.T4.7.7" class="ltx_tr">
<td id="S4.T4.7.7.2" class="ltx_td ltx_align_left ltx_border_bb">Retrieval</td>
<td id="S4.T4.7.7.3" class="ltx_td ltx_align_left ltx_border_bb">Retrieval</td>
<td id="S4.T4.7.7.1" class="ltx_td ltx_align_center ltx_border_bb"><math id="S4.T4.7.7.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.7.7.1.m1.1a"><mo stretchy="false" id="S4.T4.7.7.1.m1.1.1" xref="S4.T4.7.7.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.7.7.1.m1.1b"><ci id="S4.T4.7.7.1.m1.1.1.cmml" xref="S4.T4.7.7.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.7.7.1.m1.1c">\rightarrow</annotation></semantics></math></td>
<td id="S4.T4.7.7.4" class="ltx_td ltx_align_center ltx_border_bb">52.1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Effects of discrepancy between knowledge candidates for training and testing. <math id="S4.T4.11.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T4.11.m1.1b"><mo stretchy="false" id="S4.T4.11.m1.1.1" xref="S4.T4.11.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T4.11.m1.1c"><ci id="S4.T4.11.m1.1.1.cmml" xref="S4.T4.11.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.11.m1.1d">\rightarrow</annotation></semantics></math> means the qualities of knowledge candidates in training and test are similar. <math id="S4.T4.12.m2.1" class="ltx_Math" alttext="\searrow" display="inline"><semantics id="S4.T4.12.m2.1b"><mo id="S4.T4.12.m2.1.1" xref="S4.T4.12.m2.1.1.cmml">↘</mo><annotation-xml encoding="MathML-Content" id="S4.T4.12.m2.1c"><ci id="S4.T4.12.m2.1.1.cmml" xref="S4.T4.12.m2.1.1">↘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.12.m2.1d">\searrow</annotation></semantics></math> means the quality in training is better than test. <math id="S4.T4.13.m3.1" class="ltx_Math" alttext="\nearrow" display="inline"><semantics id="S4.T4.13.m3.1b"><mo id="S4.T4.13.m3.1.1" xref="S4.T4.13.m3.1.1.cmml">↗</mo><annotation-xml encoding="MathML-Content" id="S4.T4.13.m3.1c"><ci id="S4.T4.13.m3.1.1.cmml" xref="S4.T4.13.m3.1.1">↗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.13.m3.1d">\nearrow</annotation></semantics></math> means the quality in test is better than training.</figcaption>
</figure>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">A typical knowledge-intensive visual question answering model involves a knowledge retrieval to find relevant information, and answer generator to produce the answer <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Gardères et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>); Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Yang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>); Salemi et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>); Shao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>. Previous work on knowledge-intensive visual question answering explores knowledge bases in different modalities, such as text items <cite class="ltx_cite ltx_citemacro_cite">Luo et al. (<a href="#bib.bib24" title="" class="ltx_ref">2021</a>); Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, graph items <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib18" title="" class="ltx_ref">2020</a>); Gardères et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, and the composition of image items and text items <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>. Our work differs from previous work by involving multi-modal knowledge items as the knowledge base, where each item contains both image and text information.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">There is also a line of research investigating answer reranking, where they first produce a list of answer candidates, and then rerank those candidates to obtain the most reliable answer <cite class="ltx_cite ltx_citemacro_cite">Marino et al. (<a href="#bib.bib26" title="" class="ltx_ref">2021</a>); Si et al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>); Wu et al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>. Instead, the focus of our work is to first retrieve a set of knowledge candidates that can help answer generation, and then improve the quality of knowledge candidate set through multimodal knowledge candidate reranking. Those selected candidates will still serves as additional knowledge input for answer generation reasoning.</p>
</div>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we introduce reranking, a critical stage for knowledge-intensive tasks, into KI-VQA. Our multi-modal reranking component takes multi-modal questions and knowledge candidates as input and perform cross-item interaction. Experiments show that our proposed multi-modal reranking can provide better knowledge candidates and improve the answer generation accuracy. Experiments on the training-testing discrepancy indicate that incorporating noisier knowledge candidates during training enhances model robustness, while training with higher quality candidates than those used in testing negatively impacts performance.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In this paper, we focus on applying multi-modal reranking to KI-VQA. However, because of the nature of visual data, directly adding visual information may significantly increase input size, and we will require more total memory to train the model. In this paper, to reduce the total memory use, we have a much smaller number of knowledge candidates for reasoning in answer generation module compared to previous work which only uses text-based knowledge candidates. Nevertheless, it is still important to further investigate more efficient ways to incorporate visual information.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Although multi-modal reranking achieves promising performance on knowledge-intensive visual question answering, it is still an open question that whether multi-modal reranking can be used help other vision-language tasks. Besides, it is also important to develop a benchmark to evaluate multi-modal reranking models systematically, which is not covered by this work.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Similarly, in this work, we only use ALIGN and PaLI as the pretrained model for retrieval, reranking and answer generation. Although it is natural to extend the framework in this work to other pretrained models, it is still interesting to see how it contributes to different (large and small) models. We provide some preliminary results comparing our reranking pipeline with zero-shot multi-modal large models <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>); Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite> in Appendix <a href="#A3" title="Appendix C Comparison With Zero-Shot Multi-Modal Large Models ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>, but we also notice that some work <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>); Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023a</a>)</cite> uses OK-VQA as instruction tuning data, making it hard to compare/be adopted directly.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">We also notice that there is another line of research investigating how to effectively use large language models for knowledge-intensive visual question answering <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a href="#bib.bib39" title="" class="ltx_ref">2022</a>); Gui et al. (<a href="#bib.bib11" title="" class="ltx_ref">2022</a>); Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>); Salemi et al. (<a href="#bib.bib30" title="" class="ltx_ref">2023</a>); Shao et al. (<a href="#bib.bib32" title="" class="ltx_ref">2023</a>)</cite>. Although our preliminary results show that our framework can still provide additional improvements over same the large language model queries as in <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2022</a>)</cite>, it is still an open question to effectively use and combine the retrieval pipeline and large language model queries.</p>
</div>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">This work was supported in part by the Google Visiting Scholar program and the Center for Intelligent Information Retrieval. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.</p>
</div>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html" title="" class="ltx_ref ltx_href">Flamingo: a visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICCV.2015.279" title="" class="ltx_ref ltx_href">VQA: visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015</em>, pages 2425–2433. IEEE Computer Society.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burges et al. (2005)</span>
<span class="ltx_bibblock">
Christopher J. C. Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Gregory N. Hullender. 2005.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/1102351.1102363" title="" class="ltx_ref ltx_href">Learning to rank using gradient descent</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Machine Learning, Proceedings of the Twenty-Second International Conference (ICML 2005), Bonn, Germany, August 7-11, 2005</em>, volume 119 of <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">ACM International Conference Proceeding Series</em>, pages 89–96. ACM.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023a)</span>
<span class="ltx_bibblock">
Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. 2023a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.09478" title="" class="ltx_ref ltx_href">Minigpt-v2: large language model as a unified interface for vision-language multi-task learning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.09478.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2023b)</span>
<span class="ltx_bibblock">
Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish V. Thapliyal, James Bradbury, and Weicheng Kuo. 2023b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/pdf?id=mWVoBz4W0u" title="" class="ltx_ref ltx_href">PaLI: A jointly-scaled multilingual language-image model</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth 16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Feng Gao, Qing Ping, Govind Thattai, Aishwarya N. Reganti, Ying Nian Wu, and Prem Natarajan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR52688.2022.00501" title="" class="ltx_ref ltx_href">Transform-retrieve-generate: Natural language-centric outside-knowledge visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022</em>, pages 5057–5067. IEEE.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gardères et al. (2020)</span>
<span class="ltx_bibblock">
François Gardères, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.findings-emnlp.44" title="" class="ltx_ref ltx_href">ConceptBert: Concept-aware representation for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, pages 489–498, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glass et al. (2022)</span>
<span class="ltx_bibblock">
Michael Glass, Gaetano Rossiello, Md Faisal Mahbub Chowdhury, Ankita Naik, Pengshan Cai, and Alfio Gliozzo. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.194" title="" class="ltx_ref ltx_href">Re2G: Retrieve, rerank, generate</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 2701–2715, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gui et al. (2022)</span>
<span class="ltx_bibblock">
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.naacl-main.70" title="" class="ltx_ref ltx_href">KAT: A knowledge augmented transformer for vision-and-language</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 956–968, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hofstätter et al. (2023)</span>
<span class="ltx_bibblock">
Sebastian Hofstätter, Jiecao Chen, Karthik Raman, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3591687" title="" class="ltx_ref ltx_href">Fid-light: Efficient and effective retrieval-augmented text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023</em>, pages 1437–1447. ACM.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021a)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=NTEz-6wysdb" title="" class="ltx_ref ltx_href">Distilling knowledge from reader to retriever for question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave (2021b)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.eacl-main.74" title="" class="ltx_ref ltx_href">Leveraging passage retrieval with generative models for open domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pages 874–880, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2021)</span>
<span class="ltx_bibblock">
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/jia21b.html" title="" class="ltx_ref ltx_href">Scaling up visual and vision-language representation learning with noisy text supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 4904–4916. PMLR.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamath et al. (2022)</span>
<span class="ltx_bibblock">
Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and Aniruddha Kembhavi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-20059-5_38" title="" class="ltx_ref ltx_href">Webly supervised concept expansion for general purpose vision models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI</em>, volume 13696 of <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 662–681. Springer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2018)</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Seongjun Yun, Hyunjae Kim, Miyoung Ko, and Jaewoo Kang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D18-1053" title="" class="ltx_ref ltx_href">Ranking paragraphs for improving answer recall in open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 565–569, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Guohao Li, Xin Wang, and Wenwu Zhu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3394171.3413943" title="" class="ltx_ref ltx_href">Boosting visual question answering with context-aware knowledge aggregation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">MM ’20: The 28th ACM International Conference on Multimedia, Virtual Event / Seattle, WA, USA, October 12-16, 2020</em>, pages 1227–1235. ACM.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v202/li23q.html" title="" class="ltx_ref ltx_href">BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, volume 202 of <em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 19730–19742. PMLR.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, and Lu Yuan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://papers.nips.cc/paper_files/paper/2022/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html" title="" class="ltx_ref ltx_href">REVIVE: regional visual representation matters in knowledge-based visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.48550/ARXIV.2310.03744" title="" class="ltx_ref ltx_href">Improved baselines with visual instruction tuning</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2310.03744.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu (2009)</span>
<span class="ltx_bibblock">
Tie-Yan Liu. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1561/1500000016" title="" class="ltx_ref ltx_href">Learning to rank for information retrieval</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Found. Trends Inf. Retr.</em>, 3(3):225–331.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html" title="" class="ltx_ref ltx_href">ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada</em>, pages 13–23.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. (2021)</span>
<span class="ltx_bibblock">
Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.517" title="" class="ltx_ref ltx_href">Weakly-supervised visual-retriever-reader for knowledge-based question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 6417–6431, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2021)</span>
<span class="ltx_bibblock">
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.findings-acl.29" title="" class="ltx_ref ltx_href">Reader-guided passage reranking for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 344–350, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2021)</span>
<span class="ltx_bibblock">
Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR46437.2021.01389" title="" class="ltx_ref ltx_href">KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021</em>, pages 14111–14121. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino et al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2019.00331" title="" class="ltx_ref ltx_href">OK-VQA: A visual question answering benchmark requiring external knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019</em>, pages 3195–3204. Computer Vision Foundation / IEEE.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, volume 139 of <em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts et al. (2022)</span>
<span class="ltx_bibblock">
Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2203.17189" title="" class="ltx_ref ltx_href">Scaling up models and data with <span id="bib.bib29.3.3.1" class="ltx_text ltx_markedasmath ltx_font_typewriter">t5x</span> and <span id="bib.bib29.4.4.2" class="ltx_text ltx_markedasmath ltx_font_typewriter">seqio</span></a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.5.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.17189</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi et al. (2023)</span>
<span class="ltx_bibblock">
Alireza Salemi, Juan Altmayer Pizzorno, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3591629" title="" class="ltx_ref ltx_href">A symmetric dual encoding dense retrieval framework for knowledge-intensive visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023</em>, pages 110–120. ACM.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk et al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1007/978-3-031-20074-8_9" title="" class="ltx_ref ltx_href">A-OKVQA: A benchmark for visual question answering using world knowledge</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VIII</em>, volume 13668 of <em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in Computer Science</em>, pages 146–162. Springer.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. (2023)</span>
<span class="ltx_bibblock">
Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR52729.2023.01438" title="" class="ltx_ref ltx_href">Prompting large language models with answer heuristics for knowledge-based visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023</em>, pages 14974–14983. IEEE.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. (2021)</span>
<span class="ltx_bibblock">
Qingyi Si, Zheng Lin, Ming yu Zheng, Peng Fu, and Weiping Wang. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.acl-long.317" title="" class="ltx_ref ltx_href">Check it again:progressive visual question answering via visual entailment</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 4101–4110, Online. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivasan et al. (2021)</span>
<span class="ltx_bibblock">
Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3404835.3463257" title="" class="ltx_ref ltx_href">Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR ’21, page 2443–2449, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/D19-1514" title="" class="ltx_ref ltx_href">LXMERT: Learning cross-modality encoder representations from transformers</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 5100–5111, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html" title="" class="ltx_ref ltx_href">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, pages 5998–6008.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v32i1.12053" title="" class="ltx_ref ltx_href">R<math id="bib.bib37.1.1.m1.1" class="ltx_Math" alttext="{}^{\mbox{3}}" display="inline"><semantics id="bib.bib37.1.1.m1.1a"><msup id="bib.bib37.1.1.m1.1.1" xref="bib.bib37.1.1.m1.1.1.cmml"><mi id="bib.bib37.1.1.m1.1.1a" xref="bib.bib37.1.1.m1.1.1.cmml"></mi><mtext id="bib.bib37.1.1.m1.1.1.1" xref="bib.bib37.1.1.m1.1.1.1a.cmml">3</mtext></msup><annotation-xml encoding="MathML-Content" id="bib.bib37.1.1.m1.1b"><apply id="bib.bib37.1.1.m1.1.1.cmml" xref="bib.bib37.1.1.m1.1.1"><ci id="bib.bib37.1.1.m1.1.1.1a.cmml" xref="bib.bib37.1.1.m1.1.1.1"><mtext mathsize="70%" id="bib.bib37.1.1.m1.1.1.1.cmml" xref="bib.bib37.1.1.m1.1.1.1">3</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib37.1.1.m1.1c">{}^{\mbox{3}}</annotation></semantics></math>: Reinforced ranker-reader for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.2.1" class="ltx_emph ltx_font_italic">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018</em>, pages 5981–5988. AAAI Press.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2022)</span>
<span class="ltx_bibblock">
Jialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v36i3.20174" title="" class="ltx_ref ltx_href">Multi-modal answer validation for knowledge-based VQA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022</em>, pages 2712–2721. AAAI Press.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1609/aaai.v36i3.20215" title="" class="ltx_ref ltx_href">An empirical study of GPT-3 for few-shot knowledge-based VQA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022</em>, pages 3081–3089. AAAI Press.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2020)</span>
<span class="ltx_bibblock">
Zihao Zhu, Jing Yu, Yujing Wang, Yajing Sun, Yue Hu, and Qi Wu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.24963/ijcai.2020/153" title="" class="ltx_ref ltx_href">Mucko: Multi-layer cross-modal knowledge reasoning for fact-based visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</em>, pages 1097–1103. ijcai.org.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. (2023)</span>
<span class="ltx_bibblock">
Honglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Michael Bendersky. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3539618.3592047" title="" class="ltx_ref ltx_href">RankT5: Fine-tuning T5 for text ranking with ranking losses</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023</em>, pages 2308–2313. ACM.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experiment Setup</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">We initialize image-text retrieval module with pretrained ALIGN checkpoint, and we initialize both answer generation and multi-modal reranking module with pretrained PaLI-3b checkpoint.</p>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.1" class="ltx_p">In the retrieval module, we crop a question image into a series of patches with kernel size 224 with stride 64. We use each image patch to retrieve top-20 candidates and then aggregate candidates from one question image. If there are candidates that are retrieved by multiple image patches in the same image, we will keep the one with highest relevance score. We use aggregated top-20 candidates as candidates set for answer generation training and testing.</p>
</div>
<div id="A1.p3" class="ltx_para">
<p id="A1.p3.1" class="ltx_p">For OK-VQA, the multi-modal reranker takes 8500 of examples from training set for training, and the rest of them for model development. For each question, the reranker takes aggregated candidates from top-20 image patch retrieval as the candidate set. At each training step, we will sample 20 candidates for each question and perform pairwise logistics training. We select the reranker checkpoint based on Hits@k. The reranker is then applied to the aggregated image retrieval results to obtain the reranked relevance scores.</p>
</div>
<div id="A1.p4" class="ltx_para">
<p id="A1.p4.1" class="ltx_p">Answer generation is trained with batch size as 32 for 10K. Reranker is trained with batch size as 8 for 20K steps. The learning rate is 1e-4. We implement the models based on T5X <cite class="ltx_cite ltx_citemacro_cite">Roberts et al. (<a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Comparison with Other Ranking Strategies</h2>

<figure id="A2.T5" class="ltx_table">
<table id="A2.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T5.1.1.1" class="ltx_tr">
<th id="A2.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Ranking Methods</span></th>
<th id="A2.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T5.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T5.1.2.1" class="ltx_tr">
<th id="A2.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Distillation <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a href="#bib.bib13" title="" class="ltx_ref">2021a</a>)</cite>
</th>
<td id="A2.T5.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">51.5</td>
</tr>
<tr id="A2.T5.1.3.2" class="ltx_tr">
<th id="A2.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">RankT5 <cite class="ltx_cite ltx_citemacro_cite">Zhuang et al. (<a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="A2.T5.1.3.2.2" class="ltx_td ltx_align_center">52.3</td>
</tr>
<tr id="A2.T5.1.4.3" class="ltx_tr">
<th id="A2.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Reranking</th>
<td id="A2.T5.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T5.1.4.3.2.1" class="ltx_text ltx_font_bold">52.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Effects of multimodal ranking. We can find that learning reranker using distillation from answer generator can instead hurt the performance. Our multimodal reranker trained with small data provides competitive performance even compare to RankT5 which is pretrained on large amount of data.</figcaption>
</figure>
<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We also compare our model to the same multi-modal reranking model architecture trained with knowledge distillation from answer generation <cite class="ltx_cite ltx_citemacro_citep">(Izacard and Grave, <a href="#bib.bib13" title="" class="ltx_ref">2021a</a>)</cite> and RankT5 <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al., <a href="#bib.bib41" title="" class="ltx_ref">2023</a>)</cite> in Table <a href="#A2.T5" title="Table 5 ‣ Appendix B Additional Comparison with Other Ranking Strategies ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We can find that supervision from knowledge distillation can not provide reliable labels to train a reasonable reranking module. While both text-based reranking and multi-modal reranking can contribute to the performance, and multi-modal reranking can provide better performance. Especially, compared to RankT5 which is pretrained with over 500K items, our reranker only trained with around 8000 items. But it can still achieve competitive performance.</p>
</div>
<figure id="A2.T6" class="ltx_table">
<table id="A2.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.1.1.1" class="ltx_tr">
<th id="A2.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="A2.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Methods</span></th>
<th id="A2.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T6.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA Acc.</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.1.2.1" class="ltx_tr">
<th id="A2.T6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">BLIP-2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>
</th>
<td id="A2.T6.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">45.9</td>
</tr>
<tr id="A2.T6.1.3.2" class="ltx_tr">
<th id="A2.T6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Flamingo-80b <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>
</th>
<td id="A2.T6.1.3.2.2" class="ltx_td ltx_align_center">50.6</td>
</tr>
<tr id="A2.T6.1.4.3" class="ltx_tr">
<th id="A2.T6.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Our model</th>
<td id="A2.T6.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T6.1.4.3.2.1" class="ltx_text ltx_font_bold">52.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Comparison between multi-modal large models on OK-VQA datasets. We can find that our model provides promising performance compared to the zero-shot performance of those multi-modal large models.</figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Comparison With Zero-Shot Multi-Modal Large Models</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We also provide additional comparison in Table <a href="#A2.T6" title="Table 6 ‣ Appendix B Additional Comparison with Other Ranking Strategies ‣ Acknowledgment ‣ Limitations ‣ 6 Conclusion ‣ 5 Related Work ‣ 4.3 Training and Testing Discrepancy ‣ Effects of Ranking Methods. ‣ 4.2 Results ‣ 4.1 Setup ‣ 4 Experiments ‣ Multimodal Reranking for Knowledge-Intensive Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> between some multi-modal large models on OK-VQA, including Flamingo-80b <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite> and BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>. We report their zero-shot performance compared to our model. The results show that smaller model can still achieve competitive performance when comparing to the zero-shot capability of those large models. We also note that there are some other multi-modal large models such as LLAVA 1.5 <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib21" title="" class="ltx_ref">2023</a>)</cite>, MiniGPT4-V2 <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib5" title="" class="ltx_ref">2023a</a>)</cite>, which are instruction tuned with OK-VQA and therefore cannot be directly compared. But in general, our proposed framework can be extended to other multi-modal language models that take the combination of image and text input.</p>
</div>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</section>
</div>
</div>
</figure>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.12276" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.12277" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.12277">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.12277" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.12278" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 15:13:26 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
