<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Jianguo Zhang
    <sup class="ltx_sup" id="id2.2.id1">
     <span class="ltx_text ltx_font_italic" id="id2.2.id1.1">
      ∗
     </span>
    </sup>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id3.1.id1">
     Tian Lan
    </span>
   </span>
   <span class="ltx_author_notes">
    Equal contributions.
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id4.1.id1">
     Rithesh Murthy
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id5.1.id1">
     Weiran Yao
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id6.1.id1">
     Zhiwei Liu
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id7.1.id1">
     Juntao Tan
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id8.1.id1">
     Shelby Heinecke
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id9.1.id1">
     Huan Wang
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id10.1.id1">
     Juan Carlos Niebles
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id11.1.id1">
     Silvio Savarese
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    <span class="ltx_text" id="id12.1.id1">
     Caiming Xiong
    </span>
    <br class="ltx_break"/>
    Salesforce Research, USA
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id13.2.id2">
     {jianguozhang, tian.lan}@salesforce.com
    </span>
   </span>
   <span class="ltx_author_notes">
    <span class="ltx_contact ltx_role_affiliation">
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id14.id1">
   The massive successes of large language models (LLMs) encourage the emerging exploration of LLM-augmented Autonomous Agents (LAAs).
An LAA is able to generate actions with its core LLM and interact with environments, which facilitates the ability to resolve complex tasks by conditioning on past interactions such as observations and actions.
Since the investigation of LAA is still very recent, limited explorations are available.
Therefore, we provide a comprehensive comparison of LAA in terms of both agent architectures and LLM backbones.
Additionally, we propose a new strategy to orchestrate multiple LAAs such that each labor LAA focuses on one type of action,
   <span class="ltx_text ltx_font_italic" id="id14.id1.1">
    i.e.
   </span>
   BOLAA, where a controller manages the communication among multiple agents.
We conduct simulations on both decision-making and multi-step reasoning environments, which comprehensively justify the capacity of LAAs.
Our performance results provide quantitative suggestions for designing LAA architectures and the optimal choice of LLMs, as well as the compatibility of both.
We release our implementation code of LAAs to the public at
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/JimSalesforce/BOLAA" target="_blank" title="">
    https://github.com/JimSalesforce/BOLAA
   </a>
   .
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para ltx_noindent" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent booming successes of large language models (LLMs)
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      openai2023gpt4
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      touvron2023llama
     </span>
     )
    </cite>
    motivate emerging exploration of employing LLM to tackle various complex tasks
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      zhang2023dialogstudio
     </span>
     )
    </cite>
    , amongst which
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">
     L
    </span>
    LM-augmented
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.2">
     A
    </span>
    utonomous
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.3">
     A
    </span>
    gents (LAAs)
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      shinn2023reflexion
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      madaan2023self
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      huang2022language
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      kim2023language
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      paul2023refiner
     </span>
     ;
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      yao2023react
     </span>
     )
    </cite>
    stand with most spotlights.
LAA extends the intelligence of LLM to sequential action executions, exhibiting superiority in interacting with environments and resolving complex tasks via collecting observations.
To name a few, BabyAGI
    <span class="ltx_note ltx_role_footnote" id="footnote1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoheinakajima/babyagi" target="_blank" title="">
        https://github.com/yoheinakajima/babyagi
       </a>
      </span>
     </span>
    </span>
    proposes an AI-powered task management system, which leverages OpenAI LLM
    <span class="ltx_note ltx_role_footnote" id="footnote2">
     <sup class="ltx_note_mark">
      2
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        2
       </sup>
       <span class="ltx_tag ltx_tag_note">
        2
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://platform.openai.com/docs/api-reference" target="_blank" title="">
        https://platform.openai.com/docs/api-reference
       </a>
      </span>
     </span>
    </span>
    to create, prioritize, and execute tasks.
AutoGPT
    <span class="ltx_note ltx_role_footnote" id="footnote3">
     <sup class="ltx_note_mark">
      3
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        3
       </sup>
       <span class="ltx_tag ltx_tag_note">
        3
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank" title="">
        https://github.com/Significant-Gravitas/Auto-GPT
       </a>
      </span>
     </span>
    </span>
    is another popular open-source LAA framework that enables the API calling capability of LLMs.
ReAct
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      yao2023react
     </span>
     )
    </cite>
    is a recently proposed LAA method to interact with environments then consecutively generate the next action.
Langchain
    <span class="ltx_note ltx_role_footnote" id="footnote4">
     <sup class="ltx_note_mark">
      4
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        4
       </sup>
       <span class="ltx_tag ltx_tag_note">
        4
       </span>
       <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/langchain-ai/langchain" target="_blank" title="">
        https://github.com/langchain-ai/langchain
       </a>
      </span>
     </span>
    </span>
    is a recently released open-source framework for developing LAA.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Due to the initial investigation, LAA is rather under-explored.
Firstly, the optimal agent architecture is undetermined.
ReAct
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      yao2023react
     </span>
     )
    </cite>
    prompts the agents with pre-defined examples such that the LLM learns to generate the next action via in-context learning.
Moreover, ReAct argues that an agent should have intermediate reasoning steps before action executions.
ReWOO
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      xu2023rewoo
     </span>
     )
    </cite>
    introduces additional planning steps for LAA.
Langchain generalizes the ReAct agent with zero-shot tool usage ability.
Intrinsically, the optimal architecture of agents should be aligned with both tasks and the associated LLM backbone, which is less explored in the existing works.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Secondly, understanding the efficacy of the existing LLMs in LAA is far from comprehensive. The existing preliminary works only compare the performances of a few LLM backbones.
ReAct adopts the PaLM
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      chowdhery2022palm
     </span>
     )
    </cite>
    as the backbone LLM.
ReWOO employs OpenAI text-davinci-003 model for instruction-tuning Alpaca model
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      alpaca
     </span>
     )
    </cite>
    for agent planning.
MIND2Web
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      deng2023mind2web
     </span>
     )
    </cite>
    compares Flan-T5 and OpenAI GPT3.5/4 for generalist web agent.
Nevertheless, few current works comprehensively compare the performance of LAA with regard to various pre-trained LLMs.
A very recent work
    <cite class="ltx_cite ltx_citemacro_citep">
     (
     <span class="ltx_ref ltx_missing_citation ltx_ref_self">
      liu2023agentbench
     </span>
     )
    </cite>
    releases a benchmark for evaluating LLMs as Agents. Nevertheless, they fail to jointly consider the agent architectures along with their LLM backbones.
Selecting the optimal LLMs from both efficacy and efficiency perspectives advances the current exploration of LAA.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Thirdly, the increasing complexity of tasks may require the orchestration of multiple agents.
ReWOO recently identifies that decoupling reasoning from observation improves the efficiency for LAA.
In this paper, we argue that as the task complexity increases, especially in open-domain environments,
it is better to coordinate multiple agents to complete one task.
For example, regarding the web navigation task, we could employ one
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">
     click agent
    </span>
    to interact with clickable buttons and request another
    <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">
     search agent
    </span>
    to retrieve additional resources.
Nonetheless, there are few works discussing how to orchestrate multiple agents and investigating the impacts of orchestration.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    To address these research gaps, this paper proposes to comprehensively compare the performances of LAAs.
We dive deep into the agent architecture of LAAs and the LLM backbones.
Specifically, we construct agent benchmarks from the existing environments to evaluate the performances of various agent architectures built upon various LLM backbones.
The tasks in our agent benchmarks are associated with different task complexity levels, which enables the agent performance analyses w.r.t. task complexity.
Those agent architectures are designed to extensively verify the existing design choices.
Regarding the orchestration of multiple LAAs, we propose a novel LAA architecture BOLAA
    <span class="ltx_note ltx_role_footnote" id="footnote5">
     <sup class="ltx_note_mark">
      5
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        5
       </sup>
       <span class="ltx_tag ltx_tag_note">
        5
       </span>
       For easy memorizing, we intentionally name it the same as paper title.
      </span>
     </span>
    </span>
    , which has a controller module on top of multiple collaborated agents, for enabling the selection and communication between multiple labor LAA.
   </p>
  </div>
  <div class="ltx_para ltx_noindent" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    The contributions of this paper are as follows:
   </p>
   <ul class="ltx_itemize" id="S1.I1">
    <li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i1.p1">
      <p class="ltx_p" id="S1.I1.i1.p1.1">
       We develop 6 different LAA agent architecture. We combine them with various backbone LLMs to justify the designing intuition of LAA from prompting, self-thinking, and planning.
We also develop BOLAA for orchestrating multi-agent strategy, which enhances the action interaction ability of solo agents.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para" id="S1.I1.i2.p1">
      <p class="ltx_p" id="S1.I1.i2.p1.1">
       We conduct extensive experiments on both decision-making web navigation environment and knowledge reasoning task environment.
We report the performance in terms of final sparse rewards and intermediate recalls, which provides qualitative indications for the optimal choice of LAAs as well as their compatible LLMs.
      </p>
     </div>
    </li>
    <li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
     <span class="ltx_tag ltx_tag_item">
      •
     </span>
     <div class="ltx_para ltx_noindent" id="S1.I1.i3.p1">
      <p class="ltx_p" id="S1.I1.i3.p1.1">
       BOLAA on the WebShop environment consistently yields the best performance compared with other LAA architectures.
Our results demonstrate that the importance of designing specialist agents to collaborate on resolving complex task, which should be as equally important as training a large LLM with high generalization ability.
      </p>
     </div>
    </li>
   </ul>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Augmented Language Agent Architecture
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     The completion of a complex task typically entails multiple stages. An agent must possess an understanding of these stages and plan accordingly. Chain-of-Thoughts, also known as CoT
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       wei2022chain
      </span>
      )
     </cite>
     , is a groundbreaking work that prompts the agent to deconstruct challenging reasoning tasks into smaller, more manageable steps. On the other hand, ReAct
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       yao2023react
      </span>
      )
     </cite>
     proposes leveraging this aptitude for reasoning and action within Language and Learning Models (LLMs) to foster interactive engagement with the environment, such as utilizing the Wikipedia search API, by mapping observations to the generation of reasoning and action traces or API calls in natural language. This agent architecture has given rise to various applications, including HuggingGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       shen2023hugginggpt
      </span>
      )
     </cite>
     , Generative Agents
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       park2023generative
      </span>
      )
     </cite>
     , WebGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       nakano2021webgpt
      </span>
      )
     </cite>
     , AutoGPT
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       autogpt23
      </span>
      )
     </cite>
     , BabyAGI
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       babyagi23
      </span>
      )
     </cite>
     , and Langchain
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       langchain23
      </span>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     However, these approaches neglect to incorporate valuable feedback, such as environment rewards, to enhance the agent’s behaviors, resulting in performances that rely solely on the quality of the pre-trained Language and Learning Model (LLM). Self-refine
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       madaan2023learning
      </span>
      )
     </cite>
     tackles this limitation by employing a single LLM as a generator, refiner, and provider of feedback, enabling iterative refinement of outputs. However, it is not specifically tailored for real-world task-based interaction with the environment. On the other hand, REX
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       murthy2023rex
      </span>
      )
     </cite>
     and RAP
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       hao2023reasoning
      </span>
      )
     </cite>
     repurpose the LLM to function as both a comprehensive world model and a reasoning agent. They incorporate Monte Carlo Tree Search for strategic exploration within the vast realm of reasoning with environment rewards. This approach facilitates effective navigation and decision-making in intricate domains.
     <cite class="ltx_cite ltx_citemacro_citet">
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       shinn2023reflexion
      </span>
     </cite>
     presents Reflexion, a framework that equips agents with dynamic memory and self-reflection capabilities, enhancing their reasoning skills. Self-reflection plays a pivotal role, allowing autonomous agents to iteratively refine past actions, make improvements, and prevent repetitive errors. Recently,
     <cite class="ltx_cite ltx_citemacro_citet">
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       yao2023retroformer
      </span>
     </cite>
     proposes a framework, namely Retroformer, which leverages policy gradient optimization to align the agent’s behaviors with environment-specific rewards by learning a plug-in retrospective language model.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Web Agent
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     Web navigation is the foundation for humans to collect information and communicate.
Before the boom of LLM, previous endeavours
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       liu2018reinforcement
      </span>
      ;
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       shi2017world
      </span>
      )
     </cite>
     already explored how to train web agent in a web simulation environment.
Very recently, a series of works have been devoted to developing LAA to tackle complex web navigation tasks.
Though action space of web navigation is almost infinite due to numerous available elements online, these action can be divided into a few operation types, such as
     <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">
      click
     </span>
     ,
     <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.2">
      type
     </span>
     and
     <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.3">
      select
     </span>
     .
MIND2Web
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       deng2023mind2web
      </span>
      )
     </cite>
     collects a web browser data to fine-tune LLM to generate executable actions, which functions as a Web LAA.
WebAgent
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       gur2023real
      </span>
      )
     </cite>
     is able to decompose task instruction into sub-tasks, which directly generates executable python program for web navigation.
WebArena
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       zhou2023webarena
      </span>
      )
     </cite>
     supports realistic tasks simulation for designing Web LAA.
Langchain and ChatGPT both provide convenient web plugin such that the LLM behaves as Web LAA.
We believe that the web navigation is the next fundamental task for LAA to shine its superiority.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    Tool Agent
   </h3>
   <div class="ltx_para ltx_noindent" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     The evolution of LLM and their interactions with various tools has been a focal point of recent research. The concept of a “Tool Agent” encapsulates the idea of LLMs leveraging external tools to enhance their capabilities and solve complex tasks. One of the pioneering works in this domain is the introduction of “Gorilla”
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       patil2023gorilla
      </span>
      )
     </cite>
     .
This model is adept at writing API calls and exhibits the ability to adapt test-time document changes.
Another noteworthy work is the “ToolLLM” framework
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       qin2023toolllm
      </span>
      )
     </cite>
     .
This open-source framework incorporates LLMs to efficiently engage with a myriad of tools, particularly APIs, to execute intricate tasks. The framework encompasses ToolBench, an instruction-tuning dataset tailored for tool utilization
More recently, a paradigm shift in teaching LLMs to use new tools has been discussed in
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       hsieh2023tool
      </span>
      )
     </cite>
     , which champions the use of tool documentation.
The authors present empirical evidence suggesting that tool documentation offers detailed descriptions of tool usage, which is a more effective and scalable approach. Notably, their research indicates that zero-shot prompts, which are exclusively based on tool documentation, can rival the performance of few-shot prompts.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Agent Architectures
  </h2>
  <div class="ltx_para ltx_noindent" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we compare various LAA architectures.
We first present how to design different solo LAA based on the intuition of existing work. We then present the our orchestration designing of multiple LAAs,
    <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">
     i.e.
    </span>
    BOLAA.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Solo Agents
   </h3>
   <figure class="ltx_figure" id="S3.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S3.F1.g1" src="/html/2402.15506/assets/figure/LAA-woPlan.pdf"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     The LAA architectures for Zeroshot-LAA (ZS-LAA), ZeroshotThink LAA (ZST-LAA) and ReAct LAA. ZS-LAA generates actions from LLM with zeroshot prompt. ZST-LAA extends ZS-LAA with self-think. ReAct LAA advances ZST-LAA with fewshot prompt. They all resolve a given task by interacting with environment via actions to collect observations. Better view in colors.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Hereafter, we present 5 different LAAs.
Each type of LAA is able to interact with the environment with its own interaction strategy.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Zeroshot LAA
     </span>
     (ZS-LAA)
directly extends the LLM to be action executor.
Specifically, the prompt for LLMs to function as the action executor consists of detailed descriptions for those actions.
For example, if we prompt LAA to understand the
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">
      click
     </span>
     action with “
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">
      click: using this action to click observed [button], the clickable buttons are in [].
     </span>
     ”, it may behave as a web navigation agent.
We present the architecture of ZS-LAA in Figure
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     (a).
The working flow is as follows:
    </p>
    <ul class="ltx_itemize" id="S3.I1">
     <li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S3.I1.i1.p1">
       <p class="ltx_p" id="S3.I1.i1.p1.1">
        <span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">
         Initial step
        </span>
        : firstly, the ZS-LAA receives the task instruction and constructs the zeroshot prompt. Then, the LLM layer generates a possible response, which is parsed to output a feasible action. After that, the observation from environment is appended into the agent memory.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
       <p class="ltx_p" id="S3.I1.i2.p1.1">
        <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">
         Working teps
        </span>
        : the agent checks whether the task is finished. If not, ZS-LAA retrieves the previous actions and observations from memory, and constructs the prompts for LLM to generate the next executable actions. ZS-LAA continues the working stage until reaching the maximum steps or completing the task.
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S3.SS1.p2.2">
     ZS-LAA is a minimum LAA architecture. It enables the action generation ability of LLM via zeroshot prompt layer, which is easy to generalize to new environments and requires no examples.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">
      ZeroshotThink LAA
     </span>
     (ZST-LAA) is an extended version of ZS-LAA. Different from ZS-LAA, ZST-LAA has an additional self-think flow.
The architecture of ZST-LAA is presented in Figure
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     (b), where we denote the self-think flow as in pink arrow lines.
Self-think is running in intermediate steps of action generations flow, which enables the Chain-of-Thought (CoT) reasoning ability.
    </p>
    <ul class="ltx_itemize" id="S3.I2">
     <li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para ltx_noindent" id="S3.I2.i1.p1">
       <p class="ltx_p" id="S3.I2.i1.p1.1">
        <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.1">
         Self-think Step
        </span>
        : before generating the next action, ZST-LAA collect observations and previous actions to construct the
        <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.2">
         think
        </span>
        prompt. Then, the
        <span class="ltx_text ltx_font_italic" id="S3.I2.i1.p1.1.3">
         thought
        </span>
        is stored into memory.
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S3.SS1.p3.2">
     Self-think step is generally useful when given reasoning tasks.
Note that the think prompt is also in a zero-shot format, such as
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p3.2.1">
      “think: using this action to plan your actions and reasoning”
     </span>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p4">
    <p class="ltx_p" id="S3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">
      ReAct LAA
     </span>
     additionally advances ZST-LAA in the prompt layer, where fewshot examples are provided.
The architecture of ReAct LAA is illustrated in Figure
     <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     (c).
ReAct LAA is able to leverage successful running examples to improve the action generation ability of LLM and enhance the environment interaction of LAA, because those fewshot examples endows the in-context learning ability of LLM.
However, the drawback for ReAct LAA is that, due to the limited context length, fewer token spaces are available after the occupancy of fewshot examples in the prompt.
    </p>
   </div>
   <figure class="ltx_figure" id="S3.F2">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S3.F2.g1" src="/html/2402.15506/assets/figure/planact.pdf"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 2:
     </span>
     The LAA architectures for PlanAct LAA and PlanReAct LAA.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p5">
    <p class="ltx_p" id="S3.SS1.p5.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">
      PlanAct LAA
     </span>
     is designed to facilitate the planning ability of LAA. PlanAct LAA differs from ZS-LAA in two parts: 1) the planning flow and 2) the fewshot prompt.
The architecture is depicted in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
The planning flow is executed before the initial action generation step, which has additional plan prompt to construct the input for the core LLM.
    </p>
    <ul class="ltx_itemize" id="S3.I3">
     <li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para ltx_noindent" id="S3.I3.i1.p1">
       <p class="ltx_p" id="S3.I3.i1.p1.1">
        <span class="ltx_text ltx_font_italic" id="S3.I3.i1.p1.1.1">
         Planning Step
        </span>
        : PlanAct LAA generates a plan for a given task before interacting with environments.
The plan is memorized and will be retrieved to construct prompts.
       </p>
      </div>
     </li>
    </ul>
    <p class="ltx_p" id="S3.SS1.p5.2">
     It is worth noting that the plan prompt in this paper is in fewshot way, which allows LAA to generate plans based on previous successful plans.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p6">
    <p class="ltx_p" id="S3.SS1.p6.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">
      PlanReAct LAA
     </span>
     extends PlanAct LAA with additional self-think flow, which also enables the CoT ability.
The architecture of PlanReAct LAA is presented in Figure
     <a class="ltx_ref" href="#S3.F2" title="Figure 2 ‣ 3.1 Solo Agents ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
Intuitively, since the Planning flow is executed before the LAA observes the environment, self-think flow alleviates the hallucination incurred from incorrect plans.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S3.SS1.p7">
    <p class="ltx_p" id="S3.SS1.p7.1">
     Next, we introduce our multi-agent orchestrating architecture,
     <span class="ltx_text ltx_font_italic" id="S3.SS1.p7.1.1">
      i.e.
     </span>
     BOLAA.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    BOLAA: Orchestrating Multiple Agents.
   </h3>
   <figure class="ltx_figure" id="S3.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering" id="S3.F3.g1" src="/html/2402.15506/assets/figure/Orchestrator.pdf"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     The BOLAA architecture, which employs a controller to orchestrate multiple LAAs.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Though the success of the existing LLMs in completing various language understanding tasks, plenty of issues are still under-explored, such as the context length constraints, in-context learning and generalization ability, and etc.
Hence, it is challenging to employ a solo LAA to complete all tasks, especially when tasks are of high complexity.
Therefore, we propose a new agent architecture for orchestrating multiple LAAs, which is illustrated in Figure
     <a class="ltx_ref" href="#S3.F3" title="Figure 3 ‣ 3.2 BOLAA: Orchestrating Multiple Agents. ‣ 3 Agent Architectures ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
BOLAA has two main modules, the labor agents pool and the controller.
The labor agents pool manages multiple LAAs.
Each LAA may only focus on generating one type of actions.
For example, in the web navigation environment, we could establish
     <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">
      click
     </span>
     LAA and
     <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">
      search
     </span>
     LAA.
In this way, the former only generates the next button to click, while the later only outputs search query, which divides a complex task into feasible tasks.
The controller is devised to selectively call LAAs from agents pool.
Controller has the agents selection layer for choosing the most relevant LAA to call.
Then, the controller constructs the message for the selected LAA and builds the communication.
After obtaining the response from the labor LAA, the controller parses it to an executable action and then interacts with the environment.
Note that we can also design those labor LAAs to be think/plan agent.
In this way, the self-think and plan work flows are also retained.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiment
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Environment Benchmark
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     We construct the evaluation benchmarks from two environments,
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">
      i.e.,
     </span>
     the WebShop
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       yao2022webshop
      </span>
      )
     </cite>
     and HotPotQA
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       yang2018hotpotqa
      </span>
      )
     </cite>
     with Wikipedia API usage
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       yao2023react
      </span>
      )
     </cite>
     .
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p2">
    <p class="ltx_p" id="S4.SS1.p2.2">
     WebShop is a recently proposed online shopping website environment with 1.18M real-world products and human instructions.
Each instruction is associated with one ground-truth product, and contains attribute requirements,
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.2.1">
      e.g. I’m looking for a travel monopod camera tripod with quick release and easy to carry, and price lower than 130.00 dollars.
     </span>
     This instruction includes 3 attribute requirements
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.2.2">
      i.e.
     </span>
     “quick release”, “camera tripod” and “easy carry” attributes.
We define the complexity of an instruction using the number of attribute requirements.
Thus, this instruction example above is of complexity
     <math alttext="3" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1">
      <semantics id="S4.SS1.p2.1.m1.1a">
       <mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">
        3
       </mn>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b">
        <cn id="S4.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p2.1.m1.1.1">
         3
        </cn>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">
        3
       </annotation>
      </semantics>
     </math>
     .
We equally sample 150 instructions regarding each complexity level.
Since we have fewer than 150 instructions for complexity larger than 6, we only include instructions from complexity in
     <math alttext="\{1,2,\dots,6\}" class="ltx_Math" display="inline" id="S4.SS1.p2.2.m2.4">
      <semantics id="S4.SS1.p2.2.m2.4a">
       <mrow id="S4.SS1.p2.2.m2.4.5.2" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
        <mo id="S4.SS1.p2.2.m2.4.5.2.1" stretchy="false" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
         {
        </mo>
        <mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">
         1
        </mn>
        <mo id="S4.SS1.p2.2.m2.4.5.2.2" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
         ,
        </mo>
        <mn id="S4.SS1.p2.2.m2.2.2" xref="S4.SS1.p2.2.m2.2.2.cmml">
         2
        </mn>
        <mo id="S4.SS1.p2.2.m2.4.5.2.3" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
         ,
        </mo>
        <mi id="S4.SS1.p2.2.m2.3.3" mathvariant="normal" xref="S4.SS1.p2.2.m2.3.3.cmml">
         …
        </mi>
        <mo id="S4.SS1.p2.2.m2.4.5.2.4" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
         ,
        </mo>
        <mn id="S4.SS1.p2.2.m2.4.4" xref="S4.SS1.p2.2.m2.4.4.cmml">
         6
        </mn>
        <mo id="S4.SS1.p2.2.m2.4.5.2.5" stretchy="false" xref="S4.SS1.p2.2.m2.4.5.1.cmml">
         }
        </mo>
       </mrow>
       <annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.4b">
        <set id="S4.SS1.p2.2.m2.4.5.1.cmml" xref="S4.SS1.p2.2.m2.4.5.2">
         <cn id="S4.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S4.SS1.p2.2.m2.1.1">
          1
         </cn>
         <cn id="S4.SS1.p2.2.m2.2.2.cmml" type="integer" xref="S4.SS1.p2.2.m2.2.2">
          2
         </cn>
         <ci id="S4.SS1.p2.2.m2.3.3.cmml" xref="S4.SS1.p2.2.m2.3.3">
          …
         </ci>
         <cn id="S4.SS1.p2.2.m2.4.4.cmml" type="integer" xref="S4.SS1.p2.2.m2.4.4">
          6
         </cn>
        </set>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.4c">
        \{1,2,\dots,6\}
       </annotation>
      </semantics>
     </math>
     , which sums up to 900 tasks for benchmark evaluation in the WebShop environment.
In the WebShop environment, an agent operates either
     <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p2.2.3">
      search[query]
     </span>
     or
     <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p2.2.4">
      click[element]
     </span>
     actions to interact the environment, for evaluating the interactive decision making ability of LAA.
The observation from WebShop is simplified web browser, which includes the clickable buttons and associated page content.
LAA interacts with the WebShop environment as a web navigation agent.
    </p>
   </div>
   <div class="ltx_para ltx_noindent" id="S4.SS1.p3">
    <p class="ltx_p" id="S4.SS1.p3.1">
     HotPotQA with Wikipedia API is another environment considered in this paper,
which contains multi-hop questions answering tasks that requires reasoning
over two or more Wikipedia passages.
This simulation environment serves as a powerful tool for evaluating the multi-step planning and comprehension capabilities and information retrieval skills of AI models, ensuring they are proficient in sourcing reliable information from vast online resources. With its unique blend of real-world internet browsing scenarios and text analysis, HotpotQA is an invaluable asset for the advancement of augmented large language agent systems.
In HotPotQA environment, an agent has three types of actions,
     <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">
      i.e.
     </span>
     ,
     <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.2">
      search[entity]
     </span>
     ,
     <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.3">
      lookup[string]
     </span>
     and
     <span class="ltx_text ltx_font_smallcaps" id="S4.SS1.p3.1.4">
      finish[answer]
     </span>
     to interact with HotPotQA environment.
HotPotQA environment aims at evaluate the knowledge reasoning ability of LAA.
We randomly sample 100 questions from easy, medium and hard levels, which constitutes the final 300 benchmark questions for evaluating LAAs.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Evaluation Metrics
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     We mainly use the
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">
      reward
     </span>
     score in each environment to evaluate the performances of LAAs.
In the WebShop environment, the reward is defined as the attribute overlapping ratio between the bought item and ground truth item.
In HotPotQA environment, the reward is defined as the F1 score grading between agent answer and ground-truth answer. Additionally, we develop the
     <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.2">
      Recall
     </span>
     performance for WebShop environment, which is defined as 1 if the ground truth item is retrieved and 0 if not during one task session.
The Recall is reported as the average recall scores across all tasks in WebShop environment.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    LLM Utilization
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     The core component of LAA is the LLM backbone. We compare different LLMs with various choices of model size and context length.
We reported the results w.r.t. open LLM models such as fastchat-3b, vicuna-3b/13b/33b
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       zheng2023judging
      </span>
      )
     </cite>
     , Llama-2-7b/13b/70b
     <span class="ltx_note ltx_role_footnote" id="footnote6">
      <sup class="ltx_note_mark">
       6
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         6
        </sup>
        <span class="ltx_tag ltx_tag_note">
         6
        </span>
        All Llama-2 models are -chat-hf version.
       </span>
      </span>
     </span>
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       touvron2023llama
      </span>
      )
     </cite>
     , MPT-7b/30b
     <cite class="ltx_cite ltx_citemacro_citep">
      (
      <span class="ltx_ref ltx_missing_citation ltx_ref_self">
       MosaicML2023Introducing
      </span>
      )
     </cite>
     , xgen-8k-7b, longchat-16k-7b/13b and OpenAI API LLMs, including text-davinci-003, gpt-3.5-turbo and
gpt-3.5-turbo-16k.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Decision-making Simulation
   </h3>
   <figure class="ltx_table" id="S4.T1">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Average reward in the WebShop environment. Len denotes the maximum context length.
     <span class="ltx_text ltx_font_bold" id="S4.T1.5.1">
      Bold
     </span>
     results denote the best results in one row,
     <span class="ltx_text ltx_font_italic" id="S4.T1.6.2">
      i.e.
     </span>
     best LAA architecture w.r.t. one LLM.
     <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.7.3">
      Underline
     </span>
     results denote the best performance in one column,
     <span class="ltx_text ltx_font_italic" id="S4.T1.8.4">
      i.e.
     </span>
     best LLM regarding one LAA architecture.
    </figcaption>
    <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.9">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T1.9.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.9.1.1.1" rowspan="2">
        <span class="ltx_text" id="S4.T1.9.1.1.1.1">
         LLM
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.9.1.1.2" rowspan="2">
        <span class="ltx_text" id="S4.T1.9.1.1.2.1">
         Len.
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T1.9.1.1.3">
        LAA Architecture
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.2.2">
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.1">
        ZS
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.2">
        ZST
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.3">
        ReAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.4">
        PlanAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.5">
        PlanReAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.2.2.6">
        BOLAA
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.3.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.9.3.3.1">
        fastchat-t5-3b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.9.3.3.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.3">
        0.3971
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.4">
        0.2832
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.5">
        0.3098
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.6">
        0.3837
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.7">
        0.1507
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.3.3.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.3.3.8.1">
         0.5169
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.4.4.1">
        vicuna-7b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.4.4.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.3">
        0.0012
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.4">
        0.0002
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.5">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.4.4.5.1">
         0.1033
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.6">
        0.0555
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.7">
        0.0674
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.4.4.8">
        0.0604
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.5.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.5.5.1">
        vicuna-13b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.5.5.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.3">
        0.0340
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.4">
        0.0451
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.5">
        0.1509
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.6">
        0.3120
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.7">
        0.4127
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.5.5.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.5.5.8.1">
         0.5350
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.6.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.6.6.1">
        vicuna-33b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.6.6.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.3">
        0.1356
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.4">
        0.2049
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.5">
        0.1887
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.6">
        0.3692
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.7">
        0.3125
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.6.6.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.6.6.8.1">
         0.5612
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.7.7">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.7.7.1">
        llama-2-7b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.7.7.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.3">
        0.0042
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.4">
        0.0068
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.5">
        0.1248
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.6">
        0.3156
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.7">
        0.2761
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.7.7.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.7.7.8.1">
         0.4648
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.8.8">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.8.8.1">
        llama-2-13b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.8.8.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.3">
        0.0662
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.4">
        0.0420
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.5">
        0.2568
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.6">
        <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T1.9.8.8.6.1">
         0.4892
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.7">
        0.4091
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.8.8.8">
        0.3716
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.9.9">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.9.9.1">
        llama-2-70b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.9.9.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.3">
        0.0122
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.4">
        0.0080
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.5">
        0.4426
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.6">
        0.2979
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.7">
        0.3770
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.9.9.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.9.9.8.1">
         0.5040
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.10.10">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.10.10.1">
        mpt-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.10.10.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.3">
        0.0001
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.4">
        0.0001
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.5">
        0.0573
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.6">
        0.0656
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.7">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.10.10.7.1">
         0.1574
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.10.10.8">
        0.0632
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.11.11">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.11.11.1">
        mpt-30b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.11.11.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.3">
        0.1664
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.4">
        0.1255
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.5">
        0.3119
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.6">
        0.3060
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.7">
        0.3198
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.11.11.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.11.11.8.1">
         0.4381
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.12.12">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.12.12.1">
        xgen-8k-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.12.12.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.3">
        0.0001
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.4">
        0.0015
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.5">
        0.0685
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.6">
        0.1574
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.7">
        0.1004
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.12.12.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.12.12.8.1">
         0.3697
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.13.13">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.13.13.1">
        longchat-7b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.13.13.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.3">
        0.0165
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.4">
        0.0171
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.5">
        0.069
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.6">
        0.0917
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.7">
        0.1322
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.13.13.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.13.13.8.1">
         0.1964
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.14.14">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.14.14.1">
        longchat-13b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.14.14.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.3">
        0.0007
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.4">
        0.0007
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.5">
        0.2373
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.6">
        0.3978
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.7">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.14.14.7.1">
         0.4019
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.14.14.8">
        0.3205
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.15.15">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.9.15.15.1">
        text-davinci-003
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.9.15.15.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.3">
        0.5292
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.4">
        0.5395
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.5">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.9.15.15.5.1">
         0.5474
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.6">
        0.4751
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.7">
        0.4912
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.9.15.15.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.15.15.8.1">
         0.6341
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.16.16">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.16.16.1">
        gpt-3.5-turbo
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T1.9.16.16.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.3">
        0.5061
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.4">
        0.5057
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.5">
        0.5383
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.6">
        0.4667
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.7">
        0.5483
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T1.9.16.16.8">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.9.16.16.8.1">
         <span class="ltx_text ltx_font_bold" id="S4.T1.9.16.16.8.1.1">
          0.6567
         </span>
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T1.9.17.17">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.9.17.17.1">
        gpt-3.5-turbo-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.9.17.17.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.3">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.9.17.17.3.1">
         0.5657
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.4">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.9.17.17.4.1">
         0.5642
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.5">
        0.4898
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.6">
        0.4565
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.7">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T1.9.17.17.7.1">
         0.5607
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.9.17.17.8">
        <span class="ltx_text ltx_font_bold" id="S4.T1.9.17.17.8.1">
         0.6541
        </span>
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     In this section, we present and compare the decision-making performances of LAAs in the WebShop environment.
The performance regarding the average reward is reported in Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
The agent prompts are constructed based on the maximum context length of different LLM models.
Regarding BOLAA, we devise one search LAA and one click LAA to generate search query and click elements, respectively.
We have the following observation:
    </p>
    <ul class="ltx_itemize" id="S4.I1">
     <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i1.p1">
       <p class="ltx_p" id="S4.I1.i1.p1.1">
        BOLAA performs the best compared with the other LAA architectures, especially when built on the high performing LLMs. BOLAA is able to actively select the appropriate LAA and yield qualitative communication, which stabilizes the action generation.
We observe that BOLAA, when paired with a 3b fastchat-t5 LLM, performs comparably to other LAA architectures with more powerful LLMs.
The superiority of BOLAA indicates that orchestrating multiple smaller-sized LAAs is a better choice if the computing resources are limited.
This further exemplifies the potential for fine-tuning multiple smaller-sized specialised LAAs rather than fine-tuning one large generalized LAA.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i2.p1">
       <p class="ltx_p" id="S4.I1.i2.p1.1">
        Pairing the LLM with the optimal LAA architecture is crucial. For example, Llama-2-13b performs best under PlanAct LAA arch while Llama-2-70b performs best under the BOLAA arch. Also, Longchat-13b-16K performs best when using PlanAct and PlanReAct, which may indicate the extraordinary planning ability of longchat-13b-16k models.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i3.p1">
       <p class="ltx_p" id="S4.I1.i3.p1.1">
        Increasing the context length alone may not necessarily improve the LAA performances. For example, when comparing longchat-13b-16k with llama-2-13b models, the latter yields better performances though with less context length. By checking the running log of those LAAs, we observe more occurrence of hallucinated generation when the LAA runs for more steps, which in the end degrades the benefits of longer context.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i4.p1">
       <p class="ltx_p" id="S4.I1.i4.p1.1">
        A powerful LLM is able to generalize under the zeroshot LAA arch. The best performance of OpenAI API-based models are actually under ZS and ZST arch. This indicates the great potential of developing a generic LAA with powerful LLM.
Actually, this is currently what open-source projects are working towards, directly calling OpenAI API and tuning the zeroshot agent prompt instead.
Our benchmark results quantitatively justify that using only a ZS LAA can already achieve comparable or even better performances than LAA arch with additional Plan or Self-think flow. However, for other less powerful LLMs, fewshot prompts are necessary for LAAs.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para ltx_noindent" id="S4.I1.i5.p1">
       <p class="ltx_p" id="S4.I1.i5.p1.1">
        Plan flow generally improves the performances when the agent is built on open-source LLMs.
By comparing the performances of ReAct, PlanAct and PlanReAct, we observe a performance gain on most LLM cases when using plan flow. However, planning and thinking require the LLM to be able to reason in steps, which may be challenging for small size LLMs.
For example, fastchat-t5-3b performs above average on ZS LAA arch. But the performance degrades by a large margin under PlanReAct arch.
       </p>
      </div>
     </li>
    </ul>
   </div>
   <figure class="ltx_table" id="S4.T2">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Average recall in the WebShop environment. Len denotes the maximum context length.
     <span class="ltx_text ltx_font_bold" id="S4.T2.5.1">
      Bold
     </span>
     results denote the best results in one row,
     <span class="ltx_text ltx_font_italic" id="S4.T2.6.2">
      i.e.
     </span>
     best LAA architecture w.r.t. one LLM.
     <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.7.3">
      Underline
     </span>
     results denote the best performance in one column,
     <span class="ltx_text ltx_font_italic" id="S4.T2.8.4">
      i.e.
     </span>
     best LLM regarding one LAA architecture.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.9">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T2.9.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.9.1.1.1" rowspan="2">
        <span class="ltx_text" id="S4.T2.9.1.1.1.1">
         LLM
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T2.9.1.1.2" rowspan="2">
        <span class="ltx_text" id="S4.T2.9.1.1.2.1">
         Len.
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S4.T2.9.1.1.3">
        LAA Architecture
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.2.2">
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.1">
        ZS
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.2">
        ZST
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.3">
        ReAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.4">
        PlanAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.5">
        PlanReAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.2.2.6">
        BOLAA
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.3.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.9.3.3.1">
        fastchat-t5-3b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.9.3.3.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.3">
        0.3533
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.4">
        0.3122
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.5">
        0.3800
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.6">
        0.3700
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.7">
        0.3722
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.3.3.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.3.3.8.1">
         0.3867
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.4.4.1">
        vicuna-7b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.4.4.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.3">
        0.0833
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.4">
        0.0500
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.5">
        0.3600
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.6">
        0.3233
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.7">
        0.3278
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.4.4.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.4.4.8.1">
         0.3522
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.5.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.5.5.1">
        vicuna-13b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.5.5.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.3">
        0.0867
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.4">
        0.0644
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.5">
        0.3622
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.6">
        0.3444
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.7">
        0.2367
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.5.5.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.5.5.8.1">
         0.3700
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.6.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.6.6.1">
        vicuna-33b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.6.6.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.3">
        0.3600
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.4">
        0.3411
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.5">
        0.3822
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.6">
        0.3733
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.7">
        0.3567
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.6.6.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.6.6.8.1">
         0.3956
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.7.7">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.7.7.1">
        llama-2-7b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.7.7.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.3">
        0.0678
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.4">
        0.0311
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.5">
        0.3744
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.6">
        0.3400
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.7">
        0.3578
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.7.7.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.7.7.8.1">
         0.3856
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.8.8">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.8.8.1">
        llama-2-13b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.8.8.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.3">
        0.2856
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.4">
        0.2211
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.5">
        0.3844
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.6">
        0.3278
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.7">
        0.3500
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.8.8.8">
        <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T2.9.8.8.8.1">
         0.4078
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.9.9">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.9.9.1">
        llama-2-70b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.9.9.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.3">
        0.3344
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.4">
        0.3244
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.5">
        0.3789
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.6">
        0.3400
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.7">
        0.3600
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.9.9.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.9.9.8.1">
         0.4011
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.10.10">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.10.10.1">
        mpt-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.10.10.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.3">
        0.0144
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.4">
        0.0322
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.5">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.10.10.5.1">
         0.3644
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.6">
        0.3200
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.7">
        0.3400
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.10.10.8">
        0.3600
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.11.11">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.11.11.1">
        mpt-30b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.11.11.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.3">
        0.2973
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.4">
        0.3372
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.5">
        0.3333
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.6">
        0.3575
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.7">
        0.3412
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.11.11.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.11.11.8.1">
         0.3900
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.12.12">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.12.12.1">
        xgen-8k-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.12.12.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.3">
        0.0667
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.4">
        0.1400
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.5">
        0.3711
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.6">
        0.3400
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.7">
        0.3278
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.12.12.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.12.12.8.1">
         0.3800
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.13.13">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.13.13.1">
        longchat-7b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.13.13.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.3">
        0.1344
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.4">
        0.1856
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.5">
        0.3644
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.6">
        0.3622
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.7">
        0.3622
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.13.13.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.13.13.8.1">
         0.3811
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.14.14">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.14.14.1">
        longchat-13b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.14.14.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.3">
        0.0756
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.4">
        0.0867
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.5">
        0.3678
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.6">
        0.3467
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.7">
        0.3471
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.14.14.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.14.14.8.1">
         0.3789
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.15.15">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.9.15.15.1">
        text-davinci-003
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.9.15.15.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.3">
        0.3800
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.4">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.9.15.15.4.1">
         0.3856
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.5">
        0.3767
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.6">
        0.3711
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.7">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.9.15.15.7.1">
         0.3889
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.9.15.15.8">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.15.15.8.1">
         0.3956
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.16.16">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.16.16.1">
        gpt-3.5-turbo
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T2.9.16.16.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.3">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.9.16.16.3.1">
         0.3889
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.4">
        0.3756
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.5">
        <span class="ltx_text ltx_font_bold" id="S4.T2.9.16.16.5.1">
         0.3933
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.6">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T2.9.16.16.6.1">
         0.3789
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.7">
        0.3867
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T2.9.16.16.8">
        0.3929
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T2.9.17.17">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.9.17.17.1">
        gpt-3.5-turbo-16k-0613
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T2.9.17.17.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.3">
        0.3856
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.4">
        0.3833
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.5">
        <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T2.9.17.17.5.1">
         0.4011
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.6">
        0.3756
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.7">
        0.3811
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.9.17.17.8">
        0.3933
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p2">
    <p class="ltx_p" id="S4.SS4.p2.1">
     We also report the intermediate Recall performances for all LAAs, which are illustrated in Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
Recall is mainly related to the search action.
High recall performances indicate that the LAA is capable of generating a precise search query.
High recalls usually lead to better rewards. But they are not tightly related.
For example, Llama-2-70b has a recall performance of nearly 0.3344 on ZS LAA, which is comparable to the best LAA.
However, the reward performance in Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     of ZS LAA Llama-2-70b is only 0.0122.
The reason is that generating the search query requires a different LLM ability from generating the correct click action, where the latter is more challenging.
Another observation is that our proposed BOLAA generally performs the best on all LLMs, which indicates that separating the search agent from the click agent improves the accuracy of the search action, leading to a higher recall value.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS4.p3">
    <p class="ltx_p" id="S4.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">
      LAA performance w.r.t. Complexity
     </span>
     .
After the overall performances of those LAAs and LLMs are compared, we conduct more details investigation of the performance w.r.t. the task complexity.
Due to the space limitation, we only report the performance of text-davinci-003 and llama-2-70b. The reward performance is illustrated in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     .
The BOLAA model consistently performs better on all complexity levels.
We also observe the degraded performances when the task complexity is increased, which follows the intuition.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F4">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf1">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F4.sf1.g1" src="/html/2402.15506/assets/figure/Reward_text-davinci-003.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        text-davinci-003
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F4.sf2">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F4.sf2.g1" src="/html/2402.15506/assets/figure/Reward_llama-2-70b.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        Llama-2-70b
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     The reward w.r.t. task complexity in WebShop. Each bar represents one LAA.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="S4.F5">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf1">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F5.sf1.g1" src="/html/2402.15506/assets/figure/Recall_text-davinci-003.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        text-davinci-003
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F5.sf2">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F5.sf2.g1" src="/html/2402.15506/assets/figure/Recall_llama-2-70b.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        Llama-2-70b
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     The recall w.r.t. task complexity in WebShop. Each bar represents one LAA.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS4.p4">
    <p class="ltx_p" id="S4.SS4.p4.1">
     Surprisingly, we find out that further increasing the complexity of tasks greater than 4 will not further degrade the performances. The reason is that the recall performance increases when the task is of higher complexity, which we demonstrated in Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.4 Decision-making Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . This is due to the fact that high-complexity task instruction provides more additional context information for the LAA. As such, the
     <span class="ltx_text ltx_font_italic" id="S4.SS4.p4.1.1">
      search
     </span>
     action can be more specific and accurate under high complexity levels.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.5
    </span>
    Knowledge Reasoning Simulation
   </h3>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p1">
    <p class="ltx_p" id="S4.SS5.p1.1">
     We benchmark on the HotPotQA environment to evaluate the multi-step reasoning ability of LAAs.
Since the available search, lookup and finish operations are all related to knowledge reasoning in this environment and hard to separate, we therefore leave the BOLAA arch for future work and only compare the performance on other agent arch.
The results are in Table
     <a class="ltx_ref" href="#S4.T3" title="Table 3 ‣ 4.5 Knowledge Reasoning Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
In general, ReAct agent arch achieves the best performances, which can be interpreted in twofold.
Firstly, fewshot prompt is necessary to enable the action generation and reasoning ability for LAA, especially when experimenting with those small-size language models.
Secondly, comparing ReAct, PlanAct, and PlanReAct, we would conclude that planning flow of LAA hinders performance the in knowledge reasoning environment and tasks.
The reason is that knowledge reasoning tasks require contextualized information to conduct reasoning, whereas planning flow is executed ahead of interactions.
Thus, those generated plans tend to lead to more hallucination of LAA.
Thirdly, regarding this knowledge reasoning task, model size is much more important than the context length.
Large-sized model has better abilities in reasoning, thus performing better.
Additionally, the superior reasoning ability of OpenAI gpt-3.5 models is again verified.
We also observe the best performance of Llama-2-70b on all open-source LLMs, which suggests that potential future fine-tuning can be applied on Llama-2 models.
    </p>
   </div>
   <figure class="ltx_table" id="S4.T3">
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Average reward in the HotPotQA environment. Len denotes the maximum context length.
     <span class="ltx_text ltx_font_bold" id="S4.T3.5.1">
      Bold
     </span>
     results denote the best results in one row,
     <span class="ltx_text ltx_font_italic" id="S4.T3.6.2">
      i.e.
     </span>
     best LAA architecture w.r.t. one LLM.
     <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.7.3">
      Underline
     </span>
     results denote the best performance in one column,
     <span class="ltx_text ltx_font_italic" id="S4.T3.8.4">
      i.e.
     </span>
     best LLM regarding one LAA architecture.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.9">
     <tbody class="ltx_tbody">
      <tr class="ltx_tr" id="S4.T3.9.1.1">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.9.1.1.1" rowspan="2">
        <span class="ltx_text" id="S4.T3.9.1.1.1.1">
         LLM
        </span>
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T3.9.1.1.2" rowspan="2">
        <span class="ltx_text" id="S4.T3.9.1.1.2.1">
         Len.
        </span>
       </th>
       <td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S4.T3.9.1.1.3">
        LAA Architecture
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.2.2">
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.2.2.1">
        ZS
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.2.2.2">
        ZST
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.2.2.3">
        ReAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.2.2.4">
        PlanAct
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.2.2.5">
        PlanReAct
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.3.3">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.9.3.3.1">
        fastchat-t5-3b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.9.3.3.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.3.3.3">
        0.0252
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.3.3.4">
        0.0067
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.3.3.5">
        0.0692
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.3.3.6">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.3.3.6.1">
         0.1155
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.3.3.7">
        0.0834
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.4.4">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.4.4.1">
        vicuna-7b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.4.4.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.4.4.3">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.4.4.3.1">
         0.1339
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.4.4.4">
        0.0797
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.4.4.5">
        0.0318
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.4.4.6">
        0.0868
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.4.4.7">
        0.0956
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.5.5">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.5.5.1">
        vicuna-13b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.5.5.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.5.5.3">
        0.1541
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.5.5.4">
        0.0910
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.5.5.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.5.5.5.1">
         0.2637
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.5.5.6">
        0.1754
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.5.5.7">
        0.2075
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.6.6">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.6.6.1">
        vicuna-33b
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.6.6.2">
        2k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.6.6.3">
        0.2180
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.6.6.4">
        0.2223
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.6.6.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.6.6.5.1">
         0.2602
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.6.6.6">
        0.1333
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.6.6.7">
        0.2016
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.7.7">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.7.7.1">
        llama-2-7b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.7.7.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.7.7.3">
        0.0395
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.7.7.4">
        0.0207
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.7.7.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.7.7.5.1">
         0.2624
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.7.7.6">
        0.1780
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.7.7.7">
        0.1417
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.8.8">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.8.8.1">
        llama-2-13b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.8.8.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.8.8.3">
        0.1731
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.8.8.4">
        0.2313
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.8.8.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.8.8.5.1">
         0.2521
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.8.8.6">
        0.2192
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.8.8.7">
        0.2177
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.9.9">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.9.9.1">
        llama-2-70b-chat
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.9.9.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.3">
        0.2809
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.4">
        0.3207
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.9.9.5.1">
         0.3558
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.6">
        0.1424
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.9.9.7">
        0.1797
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.10.10">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.10.10.1">
        mpt-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.10.10.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.10.10.3">
        0.0982
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.10.10.4">
        0.0483
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.10.10.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.10.10.5.1">
         0.1707
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.10.10.6">
        0.1147
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.10.10.7">
        0.1195
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.11.11">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.11.11.1">
        mpt-30b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.11.11.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.11.11.3">
        0.1562
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.11.11.4">
        0.2141
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.11.11.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.11.11.5.1">
         0.3261
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.11.11.6">
        0.2224
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.11.11.7">
        0.2315
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.12.12">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.12.12.1">
        xgen-8k-7b-instruct
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.12.12.2">
        8k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.12.12.3">
        0.1502
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.12.12.4">
        0.1244
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.12.12.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.12.12.5.1">
         0.1937
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.12.12.6">
        0.1116
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.12.12.7">
        0.1096
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.13.13">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.13.13.1">
        vicuna-7b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.13.13.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.13.13.3">
        0.0773
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.13.13.4">
        0.1053
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.13.13.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.13.13.5.1">
         0.2554
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.13.13.6">
        0.1759
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.13.13.7">
        0.1642
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.14.14">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.14.14.1">
        longchat-7b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.14.14.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.14.14.3">
        0.0791
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.14.14.4">
        0.0672
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.14.14.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.14.14.5.1">
         0.2161
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.14.14.6">
        0.1296
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.14.14.7">
        0.0971
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.15.15">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.15.15.1">
        longchat-13b-16k
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.15.15.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.15.15.3">
        0.1083
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.15.15.4">
        0.0562
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.15.15.5">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.15.15.5.1">
         0.2387
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.15.15.6">
        0.1623
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.15.15.7">
        0.1349
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.16.16">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.9.16.16.1">
        text-davinci-003
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T3.9.16.16.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.16.16.3">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.9.16.16.3.1">
         0.3430
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.16.16.4">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.9.16.16.4.1">
         0.3304
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.16.16.5">
        <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.T3.9.16.16.5.1">
         0.4503
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.16.16.6">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.9.16.16.6.1">
         0.3577
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.9.16.16.7">
        <span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.9.16.16.7.1">
         0.4101
        </span>
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.17.17">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.17.17.1">
        gpt-3.5-turbo
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S4.T3.9.17.17.2">
        4k
       </th>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.17.17.3">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.17.17.3.1">
         0.3340
        </span>
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.17.17.4">
        0.3254
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.17.17.5">
        0.3226
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.17.17.6">
        0.2762
       </td>
       <td class="ltx_td ltx_align_center" id="S4.T3.9.17.17.7">
        0.3192
       </td>
      </tr>
      <tr class="ltx_tr" id="S4.T3.9.18.18">
       <th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.9.18.18.1">
        gpt-3.5-turbo-16k-0613
       </th>
       <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T3.9.18.18.2">
        16k
       </th>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.9.18.18.3">
        <span class="ltx_text ltx_font_bold" id="S4.T3.9.18.18.3.1">
         0.3027
        </span>
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.9.18.18.4">
        0.2264
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.9.18.18.5">
        0.1859
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.9.18.18.6">
        0.2113
       </td>
       <td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.9.18.18.7">
        0.2251
       </td>
      </tr>
     </tbody>
    </table>
   </figure>
   <figure class="ltx_figure" id="S4.F6">
    <div class="ltx_flex_figure">
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf1">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F6.sf1.g1" src="/html/2402.15506/assets/figure/Reward_hotpot_text_davinci.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (a)
        </span>
        text-davinci-003
       </figcaption>
      </figure>
     </div>
     <div class="ltx_flex_cell ltx_flex_size_2">
      <figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S4.F6.sf2">
       <img alt="Refer to caption" class="ltx_graphics" id="S4.F6.sf2.g1" src="/html/2402.15506/assets/figure/Reward_hotpot_llama2.pdf"/>
       <figcaption class="ltx_caption">
        <span class="ltx_tag ltx_tag_figure">
         (b)
        </span>
        Llama-2-70b
       </figcaption>
      </figure>
     </div>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     The reward w.r.t. complexity level in HotPotQA. Each bar represents one LAA.
    </figcaption>
   </figure>
   <div class="ltx_para ltx_noindent" id="S4.SS5.p2">
    <p class="ltx_p" id="S4.SS5.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">
      LAA performance w.r.t. Complexity
     </span>
     .
Since we have easy, medium, and high level tasks, we compare the performance of Llama-2-70b and regarding different levels of complexity, as illustrated in Figure
     <a class="ltx_ref" href="#S4.F6" title="Figure 6 ‣ 4.5 Knowledge Reasoning Simulation ‣ 4 Experiment ‣ The Agent Collection: Designing Unified Data and Training Pipeline for Effective Agent Learning">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     .
We observe degrading performance if increasing the complexity of tasks.
In HotPotQA tasks, the hardness is defined as the question answer hops.
Therefore, hard question requires more context understanding and reasoning ability of LAA.
Though OpenAI text-davinci-003 model consistently outperforms Llama-2-70b on all levels of complexity, their difference is of smaller margin in hard questions.
Since hard questions requires more resoning efforts, we can conclude that Llama-2-70b posses comparable reasoning ability with text-davinci-003.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Conclusion and Future Work
  </h2>
  <div class="ltx_para ltx_noindent" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this paper, we systematically investigate the performances of various LAA architecture paired with different LLM backbones.
We also provide one novel orchestrating method for multiple agents,
    <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">
     i.e.
    </span>
    BOLAA.
The benchmarking results provide experimental justification for the LAA investigation and verify the potential benefits of BOLAA architecture.
During the investigation, we also identify the challenge of designing BOLAA architecture for environments with compounding actions.
In the future, we will explore whether we can harness LLMs in the controller such that selection and communication with labor agents is also fully autonomous.
We will continue developing more LAA architectures and include more LLMs and environments for evaluations.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
