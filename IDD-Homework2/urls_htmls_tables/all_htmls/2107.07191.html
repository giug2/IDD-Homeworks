<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2107.07191] Deep Learning based Food Instance Segmentation using Synthetic Data</title><meta property="og:description" content="In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to sol…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Deep Learning based Food Instance Segmentation using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Deep Learning based Food Instance Segmentation using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2107.07191">

<!--Generated on Sun Mar  3 23:00:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Deep Learning based Food Instance Segmentation using Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Deokhwan Park<sup id="id6.6.id1" class="ltx_sup"><span id="id6.6.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Joosoon Lee<sup id="id7.7.id2" class="ltx_sup"><span id="id7.7.id2.1" class="ltx_text ltx_font_italic">1</span></sup>, Junseok Lee<sup id="id8.8.id3" class="ltx_sup"><span id="id8.8.id3.1" class="ltx_text ltx_font_italic">1</span></sup> and Kyoobin Lee<sup id="id9.9.id4" class="ltx_sup"><span id="id9.9.id4.1" class="ltx_text ltx_font_italic">1</span></sup>
</span><span class="ltx_author_notes"><sup id="id10.10.id1" class="ltx_sup"><span id="id10.10.id1.1" class="ltx_text ltx_font_italic">1</span></sup>School of Integrated Technology (SIT), Gwangju Institute of Science and Technology (GIST), Republic of Korea. <span id="id11.11.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">joosoon1111@gist.ac.kr</span>© 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id12.id1" class="ltx_p">In the process of intelligently segmenting foods in images using deep neural networks for diet management, data collection and labeling for network training are very important but labor-intensive tasks. In order to solve the difficulties of data collection and annotations, this paper proposes a food segmentation method applicable to real-world through synthetic data. To perform food segmentation on healthcare robot systems, such as meal assistance robot arm, we generate synthetic data using the open-source 3D graphics software Blender placing multiple objects on meal plate and train Mask R-CNN for instance segmentation. Also, we build a data collection system and verify our segmentation model on real-world food data. As a result, on our real-world dataset, the model trained only synthetic data is available to segment food instances that are not trained with 52.2% mask AP@all, and improve performance by +6.4%p after fine-tuning comparing to the model trained from scratch. In addition, we also confirm the possibility and performance improvement on the public dataset for fair analysis. Our code and pre-trained weights are avaliable online at: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/gist-ailab/Food-Instance-Segmentation</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Some experts predict that 38 percent of adults in the world will be overweight and 20 percent obese by 2030 if the trend continues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Due to the increasing obesity rate, the importance of diet management and balanced nutrition intake has recently increased. In particular, services are gradually being developed to automatically calculate and record kinds of food and calories through photos of food to be consumed. The most important technology in this service is food recognition and can be widely used in a variety of service robots, including meal assistance robots, serving robots, and cooking robots.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Because of increasing importance of food-aware tasks, many researchers are working hard on the production of food-aware datasets and the development of food recognition. There are three methods to recognize food: food classification, food detection, and food segmentation. Food classification is a task that matches the type of food in an image through a single image, and many public datasets are also released because it is relatively easier than other tasks during the data collection and labeling phase. However, in order to determine a more accurate food intake in the diet management service, it is necessary to pinpoint the real food portion within the image. Therefore, food segmentation are more useful in this service than food classification and food detection which provides information of the food types and the position by expressing in a bounding box. Nevertheless, there are three difficulties in food segmentation. First, as shown in Table <a href="#S1.T1" title="TABLE I ‣ I INTRODUCTION ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, released public datasets available for food segmentation are very scarce compared to food classification public datasets, and most datasets are not publicly available even if released. Second, when producing a dataset personally, there are a tremendous variety of food types and it takes a huge labor cost to labeling. Third, food segmentation is still a challenging task because the variations in shape, volume, texture, color, and composition of food are too large.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To address the presented difficulties, we employed two methods. First, We introduced deep neural network for instance food segmentation. In the early works of food segmentation, multiple food items were recognized mainly through image processing techniques: Normalized Cuts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, Deformable Part Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, RANSAC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, JSEG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, Grab Cut <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, and Random Forest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In those cases, the sophistication of technique is more important than the acquisition of datasets. Lately, with the introduction of deep learning, deep neural network has eliminated the hassle of image processing by finding food features in the image on its own. There is a study that simultaneously localizes and recognizes foods in images using Fast R-CNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Moreover, there is CNN-based food segmentation using pixel-wise annotation-free data through saliency map estimation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, most relevant studies do not distinguish the same type of food in different locations as semantic segmentation, and it is most important to provide sufficient data to allow itself to learn. In that sense, secondly, we generated synthetic data and train these to apply food segmentation in real-world environments, called Sim-to-Real technique. The Sim-to-Real is an efficient technique that is already being studied in robot-related tasks, such as robot simulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and robot control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, etc. Also, it has the advantage of overcoming environmental simulations or data that are difficult to implement in real-world environments. Using this application, segmentation masks were easily obtained by randomly placing plates and multiple objects in a virtual environment to create a synthetic data describing the situation in which food was contained on the plate in a real world. Using this synthetic data and the Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> model, which is most commonly used in segmentation tasks. We conduct a class-agnostic food instance segmentation that recognizes that food types are not classified (only classify background and food) but are different. Furthermore, we found the following effects:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Unseen food instance segmentation of first-time tableware and first-time food is possible in real-world environments through random object creation of synthetic data and deep learning</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Food segmentation is sufficiently possible in real-world environments when learning using synthetic data only</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">After learning with synthetic data, fine-tuning with real-world data improves performance</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">By distinguishing the same food in different locations, it can be used efficiently in robot fields, such as food picking, which can be utilized later</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper is divided into 4 sections including this introduction section. In section 2, the data production process of synthetic and real-world data, models and parameters used in learning, and evaluation metrics used in performance comparisons are described. In section 3, performance comparison results were described according to the combination of learning data: synthetic data, our real-world data we collected, and public data called UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Finally, in section 4, the conclusions are given.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>List of food datasets</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
Name</td>
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Task</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">Reference</td>
</tr>
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span>
Food50</td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">PFID</td>
<td id="S1.T1.1.3.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite></td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">TADA</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite></td>
</tr>
<tr id="S1.T1.1.5" class="ltx_tr">
<td id="S1.T1.1.5.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food85</td>
<td id="S1.T1.1.5.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.5.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></td>
</tr>
<tr id="S1.T1.1.6" class="ltx_tr">
<td id="S1.T1.1.6.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food50Chen</td>
<td id="S1.T1.1.6.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite></td>
</tr>
<tr id="S1.T1.1.7" class="ltx_tr">
<td id="S1.T1.1.7.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">UEC FOOD-100</td>
<td id="S1.T1.1.7.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Detection</td>
<td id="S1.T1.1.7.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite></td>
</tr>
<tr id="S1.T1.1.8" class="ltx_tr">
<td id="S1.T1.1.8.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food-101</td>
<td id="S1.T1.1.8.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.8.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite></td>
</tr>
<tr id="S1.T1.1.9" class="ltx_tr">
<td id="S1.T1.1.9.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">UEC FOOD-256</td>
<td id="S1.T1.1.9.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Detection</td>
<td id="S1.T1.1.9.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite></td>
</tr>
<tr id="S1.T1.1.10" class="ltx_tr">
<td id="S1.T1.1.10.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">UNICT-FD1200</td>
<td id="S1.T1.1.10.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.10.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite></td>
</tr>
<tr id="S1.T1.1.11" class="ltx_tr">
<td id="S1.T1.1.11.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">VIREO</td>
<td id="S1.T1.1.11.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.11.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite></td>
</tr>
<tr id="S1.T1.1.12" class="ltx_tr">
<td id="S1.T1.1.12.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food524DB</td>
<td id="S1.T1.1.12.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.12.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite></td>
</tr>
<tr id="S1.T1.1.13" class="ltx_tr">
<td id="S1.T1.1.13.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food475DB</td>
<td id="S1.T1.1.13.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.13.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite></td>
</tr>
<tr id="S1.T1.1.14" class="ltx_tr">
<td id="S1.T1.1.14.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">MAFood-121</td>
<td id="S1.T1.1.14.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.14.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite></td>
</tr>
<tr id="S1.T1.1.15" class="ltx_tr">
<td id="S1.T1.1.15.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">ISIA Food-200</td>
<td id="S1.T1.1.15.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.15.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite></td>
</tr>
<tr id="S1.T1.1.16" class="ltx_tr">
<td id="S1.T1.1.16.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">FoodX-251</td>
<td id="S1.T1.1.16.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.16.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></td>
</tr>
<tr id="S1.T1.1.17" class="ltx_tr">
<td id="S1.T1.1.17.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">ChineseFoodNet</td>
<td id="S1.T1.1.17.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification</td>
<td id="S1.T1.1.17.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></td>
</tr>
<tr id="S1.T1.1.18" class="ltx_tr">
<td id="S1.T1.1.18.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">UNIMIB2015</td>
<td id="S1.T1.1.18.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification and Leftover</td>
<td id="S1.T1.1.18.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite></td>
</tr>
<tr id="S1.T1.1.19" class="ltx_tr">
<td id="S1.T1.1.19.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Foood201-Segmented</td>
<td id="S1.T1.1.19.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Segmentation</td>
<td id="S1.T1.1.19.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></td>
</tr>
<tr id="S1.T1.1.20" class="ltx_tr">
<td id="S1.T1.1.20.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">UNIMIB2016</td>
<td id="S1.T1.1.20.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Classification and Segmentation</td>
<td id="S1.T1.1.20.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite></td>
</tr>
<tr id="S1.T1.1.21" class="ltx_tr">
<td id="S1.T1.1.21.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">SUECFood</td>
<td id="S1.T1.1.21.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Segmentation</td>
<td id="S1.T1.1.21.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></td>
</tr>
<tr id="S1.T1.1.22" class="ltx_tr">
<td id="S1.T1.1.22.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Food50Seg</td>
<td id="S1.T1.1.22.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Segmentation</td>
<td id="S1.T1.1.22.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></td>
</tr>
<tr id="S1.T1.1.23" class="ltx_tr">
<td id="S1.T1.1.23.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_rule" style="width:100%;height:1.2pt;background:black;display:inline-block;"> </span></td>
<td id="S1.T1.1.23.2" class="ltx_td" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="S1.T1.1.23.3" class="ltx_td" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
</table>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">METHODS</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We propose a unseen food segmentation method that enables segmentation of untrained foods from real-world images. We used a deep neural network and constructed a synthetic dataset and a real-world dataset for unseen food segmentation using deep learning. For training deep neural network, data is the most important factor. In reality, however, it is quite challenging to build an appropriate dataset for every task. Therefore, we used Sim-to-Real, which learns deep neural networks using synthetic data and applies them to real-world. If we get real food data, a lot of time and expense is needed for data collection and annotation. So, we generated synthetic data using Blander simulator, a computer graphics software, to conserve resource. Also, we collected real-world data by building our food image acquisition system for verification of unseen real-world food segmentation.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Dataset</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p"><span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_bold">Synthetic Dataset</span> The use of synthetic data for training and testing deep neural networks has gained in popularity in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. The Blender, a 3D computer graphics production software capable of realistic rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, is often used to create synthetic data. Realistic high-quality synthetic data is required for deep neural networks to show high performance for real situations. Therefore, we generated the synthetic data using Blender.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In general, food is usually served in bowls and plates. Especially, meal tray is usually used in hospital and school. We actively introduce domain randomization to ensure that the distribution of synthetic data includes real-world data. So we created a random texture on the meal tray to recognize a variety of plates robustly, and created a variety of background textures and distractors around meal tray to be robust against environmental changes. In addition lighting conditions, such as the number, position, and intensity of light points in the virtual environment, also changed during data generation phase. To express food in synthetic data, various kinds of primitives were grouped together and placed on a plate, that resemble food with various colors and textures on the meal tray so that the network can recognize various foods robustly. Therefore, we placed meal tray modeled using the blender and generated objects of various sizes and shapes on the meal tray in a virtual simulation space as shown in Fig <a href="#S2.F1" title="Figure 1 ‣ II-A Dataset ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We then generated synthetic data by capturing it at various angles and locations of camera. We created 28,839 synthetic data, including RGB images and mask images, for unseen food segmentation. As shown in Fig <a href="#S2.F2" title="Figure 2 ‣ II-A Dataset ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as the examples of dataset, textures on the plate and background are in a complex form of mixed colors and patterns. Food-like objects located in the food tray and distractors outsider of meal tray composed of clustered primitives also have diverse colors. However, in the mask images, only the objects expressing food are projected as instance for segmentation, while the distrcators are expressed as background.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2107.07191/assets/images/data_generation.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="498" height="391" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of synthetic dataset</figcaption>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2107.07191/assets/images/example_data_syn_2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="506" height="414" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of synthetic dataset</figcaption>
</figure>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">Real-world Dataset</span>
We built a real-world food dataset for 50 kinds of Korean food. We selected 50 kinds of Korean food (rice, kimchi, fried egg,etc) through consultation and investigation by experts of hospital and institution, and collected dataset. Each meal tray was assembled with five food items shown in Fig <a href="#S2.F3" title="Figure 3 ‣ II-A Dataset ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We built a real-world food dataset using the food acquisition system that captures images from various angles, heights, and lights as shown in fig <a href="#S2.F3" title="Figure 3 ‣ II-A Dataset ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We generated data from various backgrounds to verify the robustness of the network even in environmental changes. We make a real-world food dataset by annotating 229 images acquired through the food image acquisition system. The examples of dataset is shown in Fig <a href="#S2.F4" title="Figure 4 ‣ II-A Dataset ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2107.07191/assets/images/data_aqusition_system.jpg" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="417" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(Left) Examples of Korean food (Right) Data acquisition system</figcaption>
</figure>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2107.07191/assets/images/example_data_real.jpg" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="390" height="315" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of real-world dataset.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Deep Neural Network</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We used Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> that widely used in the instance segmentation.
Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is an extension of the Fast RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>, an algorithm used for object detection.
The overall network architecture are shown in Fig . As shown in the Figure <a href="#S2.F5" title="Figure 5 ‣ II-B Deep Neural Network ‣ II METHODS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> consists of Backbone network, region proposal network(RPN), feature pyramid network(FPN), RoIAlign, and classifier. Mask-RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is built on a backbone convolutional neural network architecture for feature extraction. Backbone network used a feature pyramid network based on a ResNet-50. In feature pyramid network, the features of various layers are considered together in a pyramid-shaped manner, it gives rich semantic information compared to single networks that use only the last feature. Region proposal network is a network that scans images by sliding window and finds areas containing objects. We refer to the area that RPN searches as anchors and use RPN predictions to select higher anchors that are likely to contain objects and refine their location and size. On the last stage, Region proposal network uses the proposed ROI to perform class preference, bounding-box regression, and mask preference. We give data and ground truth of food image as input to the network and we get output the instances of segmentation.</p>
</div>
<figure id="S2.F5" class="ltx_figure"><img src="/html/2107.07191/assets/images/mask_rcnn_final.png" id="S2.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="990" height="321" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The architecture of Mask R-CNN using for food instance segmentation.</figcaption>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Training Details</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We trained MASK R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> model implemented in PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> with stochastic gradient descent(SGD) optimizer configured with learning rate of 0.0001, weight decay of 0.00005 and batch size of 8 on Titan RTX (24GB) GPU. We trained model on three types, first training with only synthetic dataset, second training only real-world dataset, the last fine-tuning with real-world dataset after pre-triaing on synthetic dataset. During fine-tuning the model, the model trained with only synthetic data first, and then only real dataset is used to fine-tune the pre-trained model.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Evaluation Metrics</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">For performance evaluation for unseen food segmentation, we utilize the same metric of COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, one of the most popular criteria of instance segmentation. The Intersection over Union (IoU), also known as the Jacquard Index, is a simple and highly effective rating metric that calculates the overlapping area between the predicted and ground truth divisions: <span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">IoU=area of overlap/area of union</span>. The proposed outputs of segmentation are post-processed with non-max suppression by the threshold of 0.5 for IoU.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">The mean Average Precision(mAP) is used evaluation metric of the performance of the instance segmentation. Precision and recall are required to calculate the mAP. Precision means the true positive ratio of predicted results which can be calculated by adding true positive and false positive: <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">Precition=true positive/(true positive+false positive)</span>. Recall means the true positive ratio of all ground truths which can be calculated by adding true positive and false negative: <span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_italic">Recall=true positive/(true positive+false negative)</span>. Therefore, a high Recall value means that deep neural network recorded a high proportion of the predicted results among ground truths.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">This results in mean Average Precision(mAP) being obtained through the Recall and Precision values. The main metric for evaluation is mean Average Precision(mAP), which is calculated by averaging the precisions under Intersection over Union(IoU) thresholds from 0.50 to 0.95 at the step of 0.05.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENT AND RESULTS</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We experimented that training the MASK R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> model on synthetic dataset and evaluation on our real-world dataset to verify the performance of unseen food segmentation. Furthermore, we conducted an experiment using a public dataset to verify the generalized performance of the algorithm. In all the experiments, our trained model segments food instances, which are category-agnostic and only certain to be food as a single category.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Result on our dataset</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We categorized our real-world dataset into three types: easy, medium, and hard, based on background diversity within the image. Easy samples have a completely black background, medium samples have a black background with light reflection and hard samples have a wide variety of backgrounds. We have 73 easy samples, 61 medium samples, and 95 hard samples. Easy samples were used for training and medium and hard samples were used for testing.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The experimental results can be found in Table <a href="#S3.T2" title="TABLE II ‣ III-A Result on our dataset ‣ III EXPERIMENT AND RESULTS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. The two columns of Table <a href="#S3.T2" title="TABLE II ‣ III-A Result on our dataset ‣ III EXPERIMENT AND RESULTS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> (headed as <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">Synthetic Only</span> and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">Real Only</span>) demonstrate the performance of models that trained only synthetic data and real data from-scratch, respectively. The column of <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_italic">Syn+Real</span> shows the performance of the model fine-tuned on real-world data after pre-training on synthetic data. The real-world data utilized on each training phase, are our dataset and public dataset UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, headed on each rows. Sim-to-Real can show good performance by training network using similar synthetic data to real-world reported on the column of <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_italic">Synthetic Only</span>. Our network trains with only synthetic data and shows 52.2% in terms of mAP as a result of evaluating with real-world data. The result suggested that the network learned by using only synthetic data via Sim-to-Real to become unseen food segmentation for real-world data. Furthermore, we confirm that the performance increased by about 8.8% when the model was fine-tuned with real data compared to learning with real-world data from scratch. As shown in Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Result on public dataset ‣ III EXPERIMENT AND RESULTS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the model trained with synthetic data only tends not to recognize watery foods such as soup. This seems unresponsive due to the lack of liquid modeling in training synthetic data, but it is simply overcome by fine-tuning with real data. Also the fine-tuned model shows the advantage of robustness not mistaking in the background compared to the model trained with real data only.</p>
</div>
<figure id="S3.T2" class="ltx_table">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Segmentation evaluation results of mask AP and box AP for each dataset.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T2.3" class="ltx_tabular ltx_figure_panel ltx_align_middle">
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Test Sets</td>
<td id="S3.T2.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1pt 0.0pt;">Metric</td>
<td id="S3.T2.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Synthetic+Real<sup id="S3.T2.1.1.1.1" class="ltx_sup"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_italic">1</span></sup>
</td>
<td id="S3.T2.1.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Synthetic Only</td>
<td id="S3.T2.1.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Real Only</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Our test set</td>
<td id="S3.T2.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1pt 0.0pt;">BBOX<sup id="S3.T2.2.2.1.1" class="ltx_sup"><span id="S3.T2.2.2.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>
</td>
<td id="S3.T2.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">-</td>
<td id="S3.T2.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">80.0</td>
<td id="S3.T2.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">-</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">(Synthetic)</td>
<td id="S3.T2.3.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">SEG<sup id="S3.T2.3.3.1.1" class="ltx_sup"><span id="S3.T2.3.3.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>
</td>
<td id="S3.T2.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">-</td>
<td id="S3.T2.3.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">87.9</td>
<td id="S3.T2.3.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">-</td>
</tr>
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">Our test set</td>
<td id="S3.T2.3.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1pt 0.0pt;">BBOX</td>
<td id="S3.T2.3.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;"><span id="S3.T2.3.4.3.1" class="ltx_text ltx_font_bold">76.1</span></td>
<td id="S3.T2.3.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">51.4</td>
<td id="S3.T2.3.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">65.6</td>
</tr>
<tr id="S3.T2.3.5" class="ltx_tr">
<td id="S3.T2.3.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">(Real)</td>
<td id="S3.T2.3.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" style="padding:1pt 0.0pt;">SEG</td>
<td id="S3.T2.3.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;"><span id="S3.T2.3.5.3.1" class="ltx_text ltx_font_bold">79.0</span></td>
<td id="S3.T2.3.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">52.2</td>
<td id="S3.T2.3.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding:1pt 0.0pt;">72.6</td>
</tr>
<tr id="S3.T2.3.6" class="ltx_tr">
<td id="S3.T2.3.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">UNIMIB</td>
<td id="S3.T2.3.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" style="padding:1pt 0.0pt;">BBOX</td>
<td id="S3.T2.3.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;"><span id="S3.T2.3.6.3.1" class="ltx_text ltx_font_bold">80.6</span></td>
<td id="S3.T2.3.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">35.7</td>
<td id="S3.T2.3.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding:1pt 0.0pt;">79.3</td>
</tr>
<tr id="S3.T2.3.7" class="ltx_tr">
<td id="S3.T2.3.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">2016</td>
<td id="S3.T2.3.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_b" style="padding:1pt 0.0pt;">SEG</td>
<td id="S3.T2.3.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;"><span id="S3.T2.3.7.3.1" class="ltx_text ltx_font_bold">82.7</span></td>
<td id="S3.T2.3.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">32.9</td>
<td id="S3.T2.3.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_b" style="padding:1pt 0.0pt;">81.7</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S3.I1" class="ltx_itemize ltx_figure_panel">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><sup id="S3.I1.i1.p1.1.1" class="ltx_sup"><span id="S3.I1.i1.p1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">1</span></sup><span id="S3.I1.i1.p1.1.2" class="ltx_text" style="font-size:80%;">Synthetic+Real means pre-training with synthetic data and then fine-tuning with real-world data.</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><sup id="S3.I1.i2.p1.1.1" class="ltx_sup"><span id="S3.I1.i2.p1.1.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">2</span></sup><span id="S3.I1.i2.p1.1.2" class="ltx_text" style="font-size:80%;">BBOX means box AP@all and SEG means mask AP@all as defined in COCO dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S3.I1.i2.p1.1.3.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S3.I1.i2.p1.1.4.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S3.I1.i2.p1.1.5" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
</li>
</ul>
</div>
</div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Result on public dataset</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> has been collected in a real canteen environment. The images contain food on their plates and are also placed outside their plates. In some cases, there are several foods on a plate. The UNIMIB2016 is a dataset for food instance segmentation that captures food from the top view. The UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is composed of 1,010 tray images with multiple foods and containing 73 food categories. The 1,010 tray images are split into a training set and a test set to contain about 70% and 30% of each food instance, resulted in 650 tray image training sets and 360 image test sets. Although the UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> contains the categories of each food, we utilize all data as single category, food, for comparison with our unseen food segmentation performance.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We conducted experiments using synthetic data, UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> as real-world data, fine-tuning with real-world data after pre-training on synthetic data, and the results can be seen through Table <a href="#S3.T2" title="TABLE II ‣ III-A Result on our dataset ‣ III EXPERIMENT AND RESULTS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>. When the network was trained with only the synthetic data, mAP was 32.9. Because some data of UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> dataset is several food closely attached on a same plate, Although the network did not train with foods in the UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, network can implement food instance segmentation as shown in Figure <a href="#S3.F6" title="Figure 6 ‣ III-B Result on public dataset ‣ III EXPERIMENT AND RESULTS ‣ Deep Learning based Food Instance Segmentation using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Unlike synthetic data, because some data in the UNIMIB2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> dataset multiple foods are clustered together on the same plate, the model trained on synthetic data tends to recognize foods on a single plate as one instance. Despite, using real-world data shows better results than using the synthetic data, in the case of training with fine-tuning with real-world data after pre-training on synthetic dataset, the highest result was obtained with 82.7% in terms of mAP. As a result, training on synthetic dataset is applicable to real-world data via Sim-to-Real and also takes a roll of general feature extraction that is more appropriate for fine-tuning as task-specific adaption.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2107.07191/assets/images/inference_seg_v2.jpg" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="405" height="453" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Inference examples of segmentation results</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSIONS</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we demonstrate the possibility of food instance segmentation that have never been seen in real-world environment through synthetic data generation and training of Mask R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> model. On our real-world dataset, food instances can be segmented sufficiently with a performance of 52.2% as using a network learned from only synthetic data. Also, when fine-tuning a model learned from only synthetic data with real-world data, +6.4%p performance is improved better than the model trained from scratch. Experiments on public dataset(UNIMIB 2016<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>) show that it is sufficient to segment food, even if it is not the same meal tray. Since this work can distinguish between different food instances but cannot recognize the type of food, it is also remaining challenge to expand intelligence for recognition of food categories. We suggest a study as our future work, transferring knowledge from classification intelligence that can be implemented with relatively easy to collect data to recognize the category of mask instance in our food instance segmentation models.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Kelly, W. Yang, C.-S. Chen, K. Reynolds, and J. He, “Global burden of
obesity in 2005 and projections to 2030,” <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">International journal of
obesity</span>, vol. 32, no. 9, pp. 1431–1437, 2008.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
F. Zhu, M. Bosch, N. Khanna, C. J. Boushey, and E. J. Delp, “Multiple
hypotheses image segmentation and classification with application to dietary
assessment,” <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health informatics</span>,
vol. 19, no. 1, pp. 377–388, 2014.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Y. Matsuda, H. Hoashi, and K. Yanai, “Recognition of multiple-food images by
detecting candidate regions,” in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2012 IEEE International Conference on
Multimedia and Expo</span>, pp. 25–30, IEEE, 2012.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Anthimopoulos, J. Dehais, P. Diem, and S. Mougiakakou, “Segmentation and
recognition of multi-food meal images for carbohydrate counting,” in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">13th IEEE International Conference on BioInformatics and BioEngineering</span>,
pp. 1–4, IEEE, 2013.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Food recognition: a new dataset,
experiments, and results,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health
informatics</span>, vol. 21, no. 3, pp. 588–598, 2016.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Fang, C. Liu, K. Tahboub, F. Zhu, E. J. Delp, and C. J. Boushey, “ctada:
The design of a crowdsourcing tool for online food image identification and
segmentation,” in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">2018 IEEE Southwest Symposium on image analysis and
interpretation (SSIAI)</span>, pp. 25–28, IEEE, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Inunganbi, A. Seal, and P. Khanna, “Classification of food images through
interactive image segmentation,” in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Asian Conference on Intelligent
Information and Database Systems</span>, pp. 519–528, Springer, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. Shimoda and K. Yanai, “Cnn-based food image segmentation without pixel-wise
annotation,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">International Conference on Image Analysis and
Processing</span>, pp. 449–457, Springer, 2015.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Bolanos and P. Radeva, “Simultaneous food localization and recognition,”
in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">2016 23rd International Conference on Pattern Recognition (ICPR)</span>,
pp. 3140–3145, IEEE, 2016.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
F. Golemo, A. A. Taiga, A. Courville, and P.-Y. Oudeyer, “Sim-to-real transfer
with neural-augmented robot simulation,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Conference on Robot
Learning</span>, pp. 817–828, PMLR, 2018.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel, “Sim-to-real transfer
of robotic control with dynamics randomization,” in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">2018 IEEE
international conference on robotics and automation (ICRA)</span>, pp. 3803–3810,
IEEE, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in
<span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">2017 IEEE International Conference on Computer Vision (ICCV)</span>,
pp. 2980–2988, 2017.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Food recognition: a new dataset,
experiments, and results,” <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE journal of biomedical and health
informatics</span>, vol. 21, no. 3, pp. 588–598, 2016.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Joutou and K. Yanai, “A food image recognition system with multiple kernel
learning,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of the 16th IEEE International Conference on
Image Processing</span>, ICIP’09, p. 285–288, IEEE Press, 2009.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Chen, K. Dhingra, W. Wu, L. Yang, R. Sukthankar, and J. Yang, “Pfid:
Pittsburgh fast-food image dataset,” in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">2009 16th IEEE International
Conference on Image Processing (ICIP)</span>, pp. 289–292, IEEE, 2009.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Mariappan, M. Bosch, F. Zhu, C. J. Boushey, D. A. Kerr, D. S. Ebert, and
E. J. Delp, “Personal dietary assessment using mobile devices,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Computational Imaging VII</span>, vol. 7246, p. 72460Z, International Society for
Optics and Photonics, 2009.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
H. Hoashi, T. Joutou, and K. Yanai, “Image recognition of 85 food categories
by feature fusion,” in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">2010 IEEE International Symposium on
Multimedia</span>, pp. 296–301, IEEE, 2010.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M.-Y. Chen, Y.-H. Yang, C.-J. Ho, S.-H. Wang, S.-M. Liu, E. Chang, C.-H. Yeh,
and M. Ouhyoung, “Automatic chinese food identification and quantity
estimation,” in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">SIGGRAPH Asia 2012 Technical Briefs</span>, pp. 1–4, 2012.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Matsuda, H. Hoashi, and K. Yanai, “Recognition of multiple-food images by
detecting candidate regions,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2012 IEEE International Conference on
Multimedia and Expo</span>, pp. 25–30, IEEE, 2012.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. Bossard, M. Guillaumin, and L. Van Gool, “Food-101–mining discriminative
components with random forests,” in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">European conference on computer
vision</span>, pp. 446–461, Springer, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y. Kawano and K. Yanai, “Automatic expansion of a food image dataset
leveraging existing categories with domain adaptation,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">European
Conference on Computer Vision</span>, pp. 3–17, Springer, 2014.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
G. M. Farinella, D. Allegra, M. Moltisanti, F. Stanco, and S. Battiato,
“Retrieval and classification of food images,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Computers in biology
and medicine</span>, vol. 77, pp. 23–39, 2016.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Chen and C.-W. Ngo, “Deep-based ingredient recognition for cooking recipe
retrieval,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the 24th ACM international conference on
Multimedia</span>, pp. 32–41, 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Learning cnn-based features for
retrieval of food images,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">International Conference on Image
Analysis and Processing</span>, pp. 426–434, Springer, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Cnn-based features for retrieval
and classification of food images,” <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Computer Vision and Image
Understanding</span>, vol. 176, pp. 70–77, 2018.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
E. Aguilar, M. Bolaños, and P. Radeva, “Regularized uncertainty-based
multi-task learning model for food analysis,” <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Journal of Visual
Communication and Image Representation</span>, vol. 60, pp. 360–370, 2019.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
W. Min, L. Liu, Z. Luo, and S. Jiang, “Ingredient-guided cascaded
multi-attention network for food recognition,” in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of the
27th ACM International Conference on Multimedia</span>, pp. 1331–1339, 2019.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P. Kaur, K. Sikka, W. Wang, S. Belongie, and A. Divakaran, “Foodx-251: a
dataset for fine-grained food classification,” <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1907.06167</span>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. Chen, Y. Zhu, H. Zhou, L. Diao, and D. Wang, “Chinesefoodnet: A large-scale
image dataset for chinese food recognition,” <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv preprint
arXiv:1705.02743</span>, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
G. Ciocca, P. Napoletano, and R. Schettini, “Food recognition and leftover
estimation for daily diet monitoring,” in <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">International Conference on
Image Analysis and Processing</span>, pp. 334–341, Springer, 2015.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
A. Meyers, N. Johnston, V. Rathod, A. Korattikara, A. Gorban, N. Silberman,
S. Guadarrama, G. Papandreou, J. Huang, and K. P. Murphy, “Im2calories:
towards an automated mobile vision food diary,” in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of the
IEEE International Conference on Computer Vision</span>, pp. 1233–1241, 2015.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Gao, W. Tan, L. Ma, Y. Wang, and W. Tang, “Musefood: Multi-sensor-based
food volume estimation on smartphones,” in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">2019 IEEE SmartWorld,
Ubiquitous Intelligence &amp; Computing, Advanced &amp; Trusted Computing, Scalable
Computing &amp; Communications, Cloud &amp; Big Data Computing, Internet of People
and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)</span>,
pp. 899–906, IEEE, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
S. Aslan, G. Ciocca, D. Mazzini, and R. Schettini, “Benchmarking algorithms
for food localization and semantic segmentation,” <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">International Journal
of Machine Learning and Cybernetics</span>, vol. 11, no. 12, pp. 2827–2847, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
S. Back, J. Kim, R. Kang, S. Choi, and K. Lee, “Segmenting unseen industrial
components in a heavy clutter using rgb-d fusion and synthetic data,” in
<span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Conference on Image Processing (ICIP)</span>,
pp. 828–832, IEEE, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Danielczuk, M. Matl, S. Gupta, A. Li, A. Lee, J. Mahler, and K. Goldberg,
“Segmenting unknown 3d objects from real depth images using mask r-cnn
trained on synthetic data,” in <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">2019 International Conference on
Robotics and Automation (ICRA)</span>, pp. 7283–7290, IEEE, 2019.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Blender Online Community, <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Blender - a 3D modelling and rendering
package</span>.

</span>
<span class="ltx_bibblock">Blender Foundation, Blender Institute, Amsterdam, Mon 08/06/2018.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international
conference on computer vision</span>, pp. 1440–1448, 2015.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,
A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in
pytorch,” in <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">NIPS-W</span>, 2017.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pp. 740–755,
Springer, 2014.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2107.07190" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2107.07191" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2107.07191">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2107.07191" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2107.07192" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar  3 23:00:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
