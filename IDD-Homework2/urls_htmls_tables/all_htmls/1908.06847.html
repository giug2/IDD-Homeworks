<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1908.06847] Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges</title><meta property="og:description" content="There is a growing interest in the wireless communications community to complement the traditional model-driven design approaches with data-driven machine learning (ML)-based solutions. While conventional ML approaches…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1908.06847">

<!--Generated on Sat Mar  2 16:24:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Solmaz Niknam, Harpreet S. Dhillon, and Jeffery H. Reed
</span><span class="ltx_author_notes">Authors are with Wireless@VT, Department of ECE, Virginia Tech, Blacksburg, VA (email: {slmzniknam, hdhillon, reedjh}@vt.edu). The support of the U.S. NSF (Grants CNS-1564148, CNS-1814477 and CNS-1642873) is gratefully acknowledged.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">There is a growing interest in the wireless communications community to complement the traditional model-driven design approaches with data-driven machine learning (ML)-based solutions. While conventional ML approaches rely on the assumption of having the data and processing heads in a central entity, this is not always feasible in wireless communications applications because of the inaccessibility of private data and large communication overhead required to transmit raw data to central ML processors. As a result, decentralized ML approaches that keep the data where it is generated are much more appealing. Owing to its privacy-preserving nature, federated learning is particularly relevant for many wireless applications, especially in the context of fifth generation (5G) networks. In this article, we provide an accessible introduction to the general idea of federated learning, discuss several possible applications in 5G networks, and describe key technical challenges and open problems for future research on federated learning in the context of wireless communications.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Availability of unprecedented amount of data and advancements in computing and parallel processing have led to a renewed interest in machine learning (ML) across many research fields including wireless communications.
For wireless communication, the adoption of ML for system design and analysis is particularly appealing because the traditional model-driven approaches are not rich enough to capture the growing complexity and heterogeneity of the modern wireless networks. An alternate to solely utilizing mathematical analyses, such as the ones used in model-driven communication system design, is to <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">learn</span> these models using massive amounts of data, which is often available to the network. This is expected to result in a complete paradigm-shift in the wireless system design.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Leveraging ML and massive amount of data has also been identified and explored as a viable solution to the pressing challenges facing the communication technology industry by leading standard development organizations in the <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="\text{3}^{\rm{rd}}" display="inline"><semantics id="S1.p2.1.m1.1a"><msup id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mtext id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2a.cmml">3</mtext><mi id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">rd</mi></msup><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">superscript</csymbol><ci id="S1.p2.1.m1.1.1.2a.cmml" xref="S1.p2.1.m1.1.1.2"><mtext id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">3</mtext></ci><ci id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3">rd</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">\text{3}^{\rm{rd}}</annotation></semantics></math> generation partnership project (3GPP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. For Release 16, 3GPP has started to improve the <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">data exposure capability</em> by specifying how to collect and feed the data back to the network functions for their use to support data-driven ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. In fact, by exposing more data effectively, ML can provide better data pattern differentiation.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, managing the large-scale data to maintain the efficiency and scalability of the ML algorithms has obviously been a challenge. In addition, in wireless networks the data is produced by and distributed over billions of devices<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>As per Cisco, the number of device connections is forecasted to grow to 12.3 billion by 2020 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</span></span></span>. This necessitates the need for exploring learning solutions that can efficiently handle distributed datasets. Traditional centralized ML schemes are not quite suitable for such cases because they require the data to be transferred and processed in a central entity, which may not be possible to implement in practice due to the inaccessibility of private data. Therefore, it naturally triggers the idea of the decentralized learning solutions, in which all the private data is kept where it is generated and only locally trained models are transferred to the central entity.
Moreover, decentralized ML can significantly reduce the network bandwidth and energy consumption by sending only the features of interest rather than the stream of the raw data. Another motive behind keeping the data where it is generated and performing on-device learning is to facilitate ML to respond to real time events in latency sensitive applications. The availability of small on-device computation units, such as TrueNorth and Snapdragon neural processors, paves the way for decentralized learning solutions by providing the required hardware platform.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Federated machine learning</em> is an emerging decentralized approach that is particularly cognizant of the aforementioned challenges, including privacy and resource constraints. It utilizes the on-device processing power and untapped private data by performing the model training in a decentralized manner and keeping the data where it is generated. In this article, we provide easily accessible introduction to the general concept of federated ML as an extension of the original <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">federated approach</em> proposed by Google recently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We then describe the salient features of federated ML, which differentiate it from the other decentralized learning approaches. Building on this, we discuss several key applications of the federated learning framework in fifth generation (5G) networks spanning from the content popularity prediction in edge computing architecture to the use case of federated learning in 5G core network. In order to provide a concrete example, simulations have been performed on a standard dataset to demonstrate how federated learning can be utilized to predict the content popularity in a cache-enabled network for augmented reality (AR) applications. Finally, the article concludes with an extensive discussion about challenges and future research directions. These challenges are mainly related to the security, privacy and the performance of the current federated algorithm, as well as its important considerations in wireless settings.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"><img src="/html/1908.06847/assets/x1.png" id="S1.F1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="203" height="131" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An illustration of the concept of federated learning. While individually training of each local learner over its <em id="S1.F1.3.1" class="ltx_emph ltx_font_italic">limited</em> dataset leads to partial models, by collaborative training, a comprehensive model can be achieved.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Preliminaries and Overview</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recently introduced by Google, federated learning is a decentralized learning approach where training is performed over a <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">federation</em> of distributed learners. It is essential to distinguish the decentralized inference approaches with centralized training from the concept of federated ML where <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">decentralized training</em> is performed for decentralized inference. The objective of this approach is to keep the training dataset where it is generated and perform the model training locally at each individual learner in the federation. After training a local model, each individual learner transfers its local model parameters, instead of raw training dataset, to an aggregating unit. The aggregator utilizes the local model parameters to update<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The aggregation of the local model parameters can be accomplished either synchronously or asynchronously. Readers can refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> for more details.</span></span></span> a global model which is eventually fed back to the individual local learners for their use. As a result, each local learner benefits from the datasets of the other learners only through the global model, shared by the aggregator, without explicitly accessing their privacy-sensitive data. While this scheme is inherently more privacy-preserving than sharing raw data, some models may still reveal information about the underlying data because of which local learners add an additional layer of protection by transferring encrypted versions of their models to the aggregator. A secure aggregation algorithm as a class of secure multi-party computation is used to aggregate the encrypted local models without the need for decrypting the models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. An illustration of the federated learning concept is provided in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Several key aspects of federated learning differentiate it from the existing distributed learning schemes. One of the common assumptions of such learning schemes is that the data samples of learners are realizations of independent and identically distributed (iid) random variables. However, in the federated ML setting, different learners may be observing separate parts of the process (with possible overlaps between them), thus generating datasets that may not be representative of the distribution of the entire data. Therefore, federated learning deals with <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">non-iid</span> datasets of the locally-trained learners.
As an example, one can consider the task of building a high definition (HD) map for autonomous driving, where the autonomous vehicles only collect the location and sensing information related to the routes they traverse; or in the task of hand-written digits recognition where local learners have samples of different digits. Second, the datasets are <em id="S2.p2.1.2" class="ltx_emph ltx_font_italic">unbalanced</em> in size. For instance, in the HD map example, the dataset collected at different autonomous vehicles may vary in size due to different environment they pass through. Last, the datasets are <em id="S2.p2.1.3" class="ltx_emph ltx_font_italic">massively distributed</em> among the local learners, where the number of data samples per local learner is smaller than the total number of learners participating in the training.
These salient features of the dataset, i.e. non-iid, distributed and unbalanced training data, differentiates the federated ML framework from the other related approaches, which are discussed below.</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><em id="S2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Distributed learning</em> schemes are the ones in which the aggregator organizes the locally collected data (usually in the form of locally trained models due to the stringent communication limitations) to provide a holistic and more accurate estimation of the parameters under study. In this form of learning, the local learners act solely as local data collectors and do not require the global model through any feedback from the aggregator. Distributed learning in wireless sensor network (WSN) for monitoring belongs to this category of learning. For instance, in temperature monitoring WSN, each sensor in the network communicates the local model trained by its dataset to the fusion center. The fusion center aggregates the local information to construct a global estimate of the temperature of the field.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><em id="S2.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Parallel learning<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text ltx_font_upright">3</span></span><span id="footnote3.4" class="ltx_text ltx_font_upright">In the ML community, it is often called distributed ML. However, we decided to use the term </span><em id="footnote3.5" class="ltx_emph">parallel learning</em><span id="footnote3.6" class="ltx_text ltx_font_upright">, owing to its objective which is </span><em id="footnote3.7" class="ltx_emph">parallelizing</em><span id="footnote3.8" class="ltx_text ltx_font_upright"> the gradient computation and aggregation across multiple worker nodes, to distinguish this type of learning from the distributed learning that we previously discussed in the context of WSN networks.</span></span></span></span></em> refers to the learning schemes whose main objective is to scale up the algorithm or accelerate the learning process or both. In this type of learning, the available training set at a central parameter server is divided into subsets of data and assigned to a group of worker machines. Therefore, the datasets assigned to each worker machine have the same underlying distribution. Subsequently, the training process is performed in parallel and the parameters are fed back to the parameter server. In this setting, model parallelism<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>In model parallelism, the entire dataset is assigned to all worker machines. However, each machine is responsible for estimating certain model parameters.</span></span></span> is another way of distributing the workload compared to the data parallelism. This type of learning is performed in datacenters where the worker machines obtain data from a shared storage and hence, unlike federated learning, they will end up having samples from the same distribution. In addition, the average number of data samples per worker is way larger than the number of worker machines participating in the training process which is different from the federated setting where the data is massively distributed.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><em id="S2.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Distributed ensemble learning</em>, also known as committee-based learning, is a learning approach in which multiple learners (such as classifiers and regressors) are combined to improve the overall performance. In this scheme, portions of the dataset are assigned to train different models. These models are then aggregated to reduce the likelihood of choosing an insufficient one. In general, the goal of such learning methods is to learn from a mixture of experts (models) rather than improving a global model using a naturally distributed dataset through a federation of local learners with communication constraints.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span><span id="S2.T1.2.1" class="ltx_text ltx_font_smallcaps">Features, design goals and applications of Federated ML and other distributed approaches.</span></figcaption>
<table id="S2.T1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.3.1.1" class="ltx_tr">
<th id="S2.T1.3.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.3.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.1.1.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S2.T1.3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Scheme</span></span>
</span>
</th>
<th id="S2.T1.3.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.1.1.2.1.1" class="ltx_p" style="width:227.6pt;">   <span id="S2.T1.3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Salient features and design goals</span></span>
</span>
</th>
<th id="S2.T1.3.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt">
<span id="S2.T1.3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.1.1.3.1.1" class="ltx_p" style="width:130.9pt;">    <span id="S2.T1.3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.3.2.1" class="ltx_tr">
<td id="S2.T1.3.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.2.1.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S2.T1.3.2.1.1.1.1.1" class="ltx_text ltx_font_bold">Distributed learning</span></span>
</span>
</td>
<td id="S2.T1.3.2.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.2.1.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="S2.I2" class="ltx_itemize">
<span id="S2.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I2.i1.p1" class="ltx_para">
<span id="S2.I2.i1.p1.1" class="ltx_p">The goal is to provide a holistic estimation of the parameters under study</span>
</span></span>
<span id="S2.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I2.i2.p1" class="ltx_para">
<span id="S2.I2.i2.p1.1" class="ltx_p">The global model is not fed back to the local learners</span>
</span></span>
</span></span>
</span>
</td>
<td id="S2.T1.3.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.2.1.3.1.1" class="ltx_p" style="width:130.9pt;">
<span id="S2.I3" class="ltx_itemize">
<span id="S2.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I3.i1.p1" class="ltx_para">
<span id="S2.I3.i1.p1.1" class="ltx_p">Distributed learning in WSN</span>
</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.3.3.2" class="ltx_tr">
<td id="S2.T1.3.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.3.2.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S2.T1.3.3.2.1.1.1.1" class="ltx_text ltx_font_bold">Parallel learning</span></span>
</span>
</td>
<td id="S2.T1.3.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.3.2.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="S2.I4" class="ltx_itemize">
<span id="S2.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I4.i1.p1" class="ltx_para">
<span id="S2.I4.i1.p1.1" class="ltx_p">The goal is to accelerate the learning process and scale up the algorithm</span>
</span></span>
<span id="S2.I4.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I4.i2.p1" class="ltx_para">
<span id="S2.I4.i2.p1.1" class="ltx_p">Data is distributed in a iid fashion</span>
</span></span>
<span id="S2.I4.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I4.i3.p1" class="ltx_para">
<span id="S2.I4.i3.p1.1" class="ltx_p">Data is not massively distributed among learners</span>
</span></span>
<span id="S2.I4.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I4.i4.p1" class="ltx_para">
<span id="S2.I4.i4.p1.1" class="ltx_p">There is no communication constraint consideration</span>
</span></span>
</span></span>
</span>
</td>
<td id="S2.T1.3.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.3.2.3.1.1" class="ltx_p" style="width:130.9pt;">
<span id="S2.I5" class="ltx_itemize">
<span id="S2.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I5.i1.p1" class="ltx_para">
<span id="S2.I5.i1.p1.1" class="ltx_p">Distributed learning in datacenters environment</span>
</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.3.4.3" class="ltx_tr">
<td id="S2.T1.3.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.4.3.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S2.T1.3.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Ensemble learning</span></span>
</span>
</td>
<td id="S2.T1.3.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.4.3.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="S2.I6" class="ltx_itemize">
<span id="S2.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I6.i1.p1" class="ltx_para">
<span id="S2.I6.i1.p1.1" class="ltx_p">The goal is to produce an optimal model by learning from a mixture of several types of the models</span>
</span></span>
<span id="S2.I6.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I6.i2.p1" class="ltx_para">
<span id="S2.I6.i2.p1.1" class="ltx_p">Data is distributed in a iid fashion</span>
</span></span>
<span id="S2.I6.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I6.i3.p1" class="ltx_para">
<span id="S2.I6.i3.p1.1" class="ltx_p">There is no communication constraint consideration</span>
</span></span>
</span></span>
</span>
</td>
<td id="S2.T1.3.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.3.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.4.3.3.1.1" class="ltx_p" style="width:130.9pt;">
<span id="S2.I7" class="ltx_itemize">
<span id="S2.I7.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I7.i1.p1" class="ltx_para">
<span id="S2.I7.i1.p1.1" class="ltx_p">Bagging, boosting and stacking algorithms that can be used in remote sensing, face recognition and so on.</span>
</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.3.5.4" class="ltx_tr">
<td id="S2.T1.3.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.3.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.5.4.1.1.1" class="ltx_p" style="width:71.1pt;"><span id="S2.T1.3.5.4.1.1.1.1" class="ltx_text ltx_font_bold">Federated learning</span></span>
</span>
</td>
<td id="S2.T1.3.5.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.3.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.5.4.2.1.1" class="ltx_p" style="width:227.6pt;">
<span id="S2.I8" class="ltx_itemize">
<span id="S2.I8.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I8.i1.p1" class="ltx_para">
<span id="S2.I8.i1.p1.1" class="ltx_p">The goal is to perform the model training using the naturally distributed datasets over several learners</span>
</span></span>
<span id="S2.I8.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I8.i2.p1" class="ltx_para">
<span id="S2.I8.i2.p1.1" class="ltx_p">The global model is fed back to the local learners for their use</span>
</span></span>
<span id="S2.I8.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I8.i3.p1" class="ltx_para">
<span id="S2.I8.i3.p1.1" class="ltx_p">Data is distributed in non-iid fashion</span>
</span></span>
<span id="S2.I8.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I8.i4.p1" class="ltx_para">
<span id="S2.I8.i4.p1.1" class="ltx_p">Data is massively distributed over local learners</span>
</span></span>
<span id="S2.I8.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I8.i5.p1" class="ltx_para">
<span id="S2.I8.i5.p1.1" class="ltx_p">There are communication constraints such as privacy, security, power and bandwidth limitations in accessing the data</span>
</span></span>
</span></span>
</span>
</td>
<td id="S2.T1.3.5.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.3.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.5.4.3.1.1" class="ltx_p" style="width:130.9pt;">
<span id="S2.I9" class="ltx_itemize">
<span id="S2.I9.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I9.i1.p1" class="ltx_para">
<span id="S2.I9.i1.p1.1" class="ltx_p">Edge computing and caching</span>
</span></span>
<span id="S2.I9.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I9.i2.p1" class="ltx_para">
<span id="S2.I9.i2.p1.1" class="ltx_p">Autonomous driving</span>
</span></span>
<span id="S2.I9.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I9.i3.p1" class="ltx_para">
<span id="S2.I9.i3.p1.1" class="ltx_p">Federated ML for spectrum management</span>
</span></span>
<span id="S2.I9.i4" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I9.i4.p1" class="ltx_para">
<span id="S2.I9.i4.p1.1" class="ltx_p">Coexistence  of  heterogeneous  systems (For example, DSRC and c-V2X)</span>
</span></span>
<span id="S2.I9.i5" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S2.I9.i5.p1" class="ltx_para">
<span id="S2.I9.i5.p1.1" class="ltx_p">Federated ML in 5G core network</span>
</span></span>
</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Applications of federated learning for Wireless Communications</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">After introducing federated learning and describing some of its salient features, we will now elaborate on a few of its use cases in the area of wireless communications. These applications are primarily inspired from the expected applications of 5G networks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Edge Computing and Caching</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Content caching and data computing at the edge of the wireless network is a promising approach to reduce backhaul traffic load.
The general idea is to bring the popular content closer to the edge terminals, namely small base stations (SBSs) and access points (infrastructure caching) or even user devices (infrastructure-less caching), such that it can be conveniently accessed locally. Such a paradigm has the potential of enabling applications with stringent delay and bandwidth requirements. The success of this architecture relies on precisely determining which contents should be placed in each cache, which is an active area of research.
The approach that is usually taken in the literature is to utilize static or dynamic statistical models for content popularity identification. Unlike static models that do not capture the time varying nature of the real-time content popularity, dynamic models reflect the instantaneous popularity by considering the statistical properties of the content. However, model-driven content popularity identification is not capable of considering multitude of factors that influence content popularity. Moreover, directly accessing the privacy-sensitive user data for content differentiation may not be possible in practice. Federated learning with the premise of utilizing the locally trained models rather than directly accessing the user data seems to be a match made in heaven for content popularity prediction in proactive caching in wireless networks (see Fig. <a href="#S3.F2" title="Figure 2 ‣ III-A Edge Computing and Caching ‣ III Applications of federated learning for Wireless Communications ‣ Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). For instance, in AR, federated learning can be used to learn certain popular elements of the augmentations from the other users without obtaining their privacy-sensitive data directly. This <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">popular</span> information is then pre-fetched and stored locally to reduce the latency. In addition, in self-driving cars, information related to traffic can be learned through other vehicles using federated learning and pre-cached in road side units.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<p id="S3.F2.1" class="ltx_p ltx_align_center"><span id="S3.F2.1.1" class="ltx_text"><img src="/html/1908.06847/assets/x2.png" id="S3.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="201" height="125" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Illustration of the application of federated learning for edge computing and caching. Here, the local learners can be edge users (self-driving cars in an autonomous vehicle network or users' augmented/virtual reality headsets) and the aggregator can be an edge computing platform (such as radio base stations or unmanned aerial vehicles) in the edge network.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In order to concretely demonstrate the applicability of federated learning, we have carried out simulations to predict content popularity in a cache-enabled network for AR applications. We consider a scenario where AR-enabled users hold up their device camera on a target place (such as museum, restaurant, amusement park and so on) to get more information about it.
To reduce the latency in the AR-based demonstration and improve users experience, the popular content related to a specific place is predicted and cached proactively. However, the selection of popular content is based on search history of the users and their interaction with the content. Unfortunately, such information is private in nature and cannot be shared with the network most of the time, even though it could have significantly improved the content popularity prediction. In order to preserve the user privacy and improve the service quality at the same time, we invoke federated learning to predict content popularity based on the user-content interaction.
We utilize AutoEncoders (AE) to predict the top contents (or rating/interaction score for the contents) that would be more appealing to the user, using the publicly available dataset MovieLens 1M. The parameters of an AE with 1 hidden layer of 128 neurons are learned by each user/learner to minimize the reconstruction error (in terms of root mean square error, or RMSE) in the federated setting. Fig. <a href="#S3.F3" title="Figure 3 ‣ III-A Edge Computing and Caching ‣ III Applications of federated learning for Wireless Communications ‣ Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrates the RMSE versus different number of users that participate in the training during each round. In addition, we considered a baseline scenario (centralized) where an AE is trained on the raw training samples obtained directly from the users rather than aggregating the individually trained models. Although implementing this in practice may not be possible because of the privacy concerns of sharing user-content interaction with the network, we consider this as a baseline case for the sake of comparison as well as demonstrating the effectiveness of federated learning. From the figure, we clearly observe that federated learning performs almost as well as the centralized scheme.
Therefore, in this case, transmitting locally trained models to the aggregator is almost as efficient as transmitting the raw data.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text"><img src="/html/1908.06847/assets/x3.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="155" height="105" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Comparison of the error performance of the federated learning and the baseline centralized schemes.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Spectrum Management</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The physics of propagation at millimeter wave (mm-wave) frequencies provides an opportunity to rethink the rules of spectrum access. In future 5G networks, a hybrid spectrum landscape of low and high frequencies (i.e. microwave and mm-wave bands) with different types of licensing is necessary to enable key 5G verticals. The hybrid spectrum access needs collaborative and more autonomous spectrum sharing strategies that are adapted to the environment and applications in 5G networks. However, accessing the spectrum dynamically and in a distributed manner is complicated. The high-resolution spectrum utilization data of all radios may be required, which may not be easy to share because of privacy concerns. In fact, all radios need to share their sensory data such as spectrum occupancy data, device non-linearity information and detection of abnormal signals, such as interference. However, these data are privacy sensitive, and radios may not be willing to send out information related to their frequencies of operation.
In addition, centralized strategies, where spectrum usage information is gathered in a spectrum access database, may not always be appropriate. Not to mention that making inference on such huge amounts of data requires enormous processing power and large scale optimization that would be computationally prohibitive. Therefore, the future of spectrum autonomy likely depends on crowd-sourced and decentralized intelligent radio networks where spectrum sharing is performed collaboratively. Federated ML, where each radio transfers its local spectrum utilization model, can be leveraged to address these issues. The aggregator utilizes the local spectrum utilization model parameters to update a global model which is eventually fed back to the individual radios for spectrum access decision. It is worth noting that the same strategy can also be used to facilitate coexistence of two wireless systems. A specific setting of current interest that can benefit from such a solution is the coexistence of dedicated short-range communication (DSRC) and cellular-connected vehicle-to-everything (c-V2X) in the same intelligent transport systems (ITS) band.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">5G Core Network</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Network data analytic function (NWDAF) is a new network function defined by 3GPP to provide more data exposure capability for ML-enabled functionalities even in the core network. It provides the ability to make use of intelligent techniques in the network management system. This enables the operators to automate the network management and configuration tasks which in turn lowers the operational expenditure by reducing the human-machine interaction. In general, NWDAF is capable of connecting to any network function (NF) and utilizing any data in the core network (see Fig. <a href="#S3.F4" title="Figure 4 ‣ III-C 5G Core Network ‣ III Applications of federated learning for Wireless Communications ‣ Federated Learning for Wireless Communications: Motivation, Opportunities and Challenges" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>). In addition, any NF can request network analytic information.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Now back to the federated setting, we have thus far considered that while the datasets may be based on observing different parts of the process (different sample spaces), they all contain the same feature space.
<em id="S3.SS3.p2.1.1" class="ltx_emph ltx_font_italic">Horizontal fragmentation</em> is the technical term for this type of data distribution. However, there could be situations in which distributed datasets may share the same sample space but differ in feature space, namely <em id="S3.SS3.p2.1.2" class="ltx_emph ltx_font_italic">vertically fragmented</em>. Inspired by the notion of data fragmentation, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> has introduced vertical federated ML for vertically fragmented datasets over the federation of local learners.
It is worth mentioning that in vertical federated ML, features such as having non-iid, unbalance and massively distributed datasets are considered over feature space.
In order to understand the idea of vertically fragmented datasets completely, lets consider two datasets that cover all the subscribers of the network (and hence have the same <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_italic">sample space</span>). However, they could easily differ in terms of the features. For instance, the first dataset could contain the registration and authentication information while the second could contain information related to the network slice selection for each user.
Given such description, vertical federated learning best fits the core network structure, where each entity handles certain features of dataset related to the overall users in the network. For instance, access mobility management function<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In 5G core architecture, entities are now referred to as functions to emphasize on them being virtual rather than physical entities.</span></span></span> (AMF) and session management function (SMF) manage mobility and session establishment (IP address allocation, traffic routing and so on), respectively. For more details on the rest of the functions, interested readers are advised to refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Here, NWDAF can act as the global node that handles the aggregation of the user data. The datasets of the users are vertically fragmented over different entities in the core network, where each entity keeps record of a specific data feature related to all the users. Using vertical federated learning, each entity in the core network transfers its local encrypted model trained by locally collected data features rather than sending the raw data to the NWDAF entity. This can significantly alleviate the massive cybersecurity vulnerability within the network topology introduced by network function virtualization (NFV).</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/1908.06847/assets/x4.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="197" height="97" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>3GPP 5G system architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Challenges and Future Direction</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Research on federated learning is still in its early stages. Despite the apparent opportunities it offers from the edge to the core networks, there exist several critical challenges in applying federated learning to wireless networks. Some of the challenges and future research directions are discussed next.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Security and Privacy Challenges and Considerations</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Protecting privacy of the local datasets is the fundamental premise of the federated ML.
To prevent models from revealing their data, a <em id="S4.SS1.p1.1.1" class="ltx_emph ltx_font_italic">secure aggregation</em> algorithm has been proposed to aggregate the encrypted local models without the need for decrypting them in the aggregator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. However, the participation of a specific local learner can still be disclosed through analyzing the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Differentially private federated algorithms have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> to provide privacy at local learner-level rather than protecting a single data sample.
However, these algorithms sacrifice the model performance or require extra computation and specific number of local learners to participate in the model training. Therefore, efficient federated algorithms that deliver models with high performance as well as privacy protection without adding computational burden are highly desirable.
In addition, some neural network models might unintentionally memorize unique aspects of the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This is in fact an important issue in case of federated learning where models are trained over sensitive user data. Given the fact that the premise of federated learning is to utilize the user data without revealing the private information, data memorization should be efficiently handled to reduce the likelihood of data disclosure in case of an attack.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Similar to the other ML approaches, in federated learning, local models are often re-trained by the newly collected data to reflect the changes on the trained model. Therefore, an adversary can surreptitiously influence the local training datasets to manipulate the result of the model by embedding carefully designed sample to <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">data-poison</em> the federated learning process. It can even threaten the model by sending gradient updates to perform <em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">model-poisoning attack</em>. Federated learning has been analyzed through an adversarial lens to examine the vulnerability of the learning process to the model-poisoning adversaries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Poisoning resilience defense mechanisms are urgently required, as federated learning in its primary form is susceptible to such adversarial attacks.
In addition, a curious aggregator or even a local learner can perform membership inference attacks against other local learners. In an inference attack, the attacker's objective is to infer if a particular data point belongs to the training dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The repeated update of the model parameters is a key factor in boosting the accuracy of membership attacks. There are various types of inference attacks such as, parameter inference, input inference and attribute inference attacks, that can jeopardize the privacy of the local learners. Therefore, vulnerability of federated learning to these attacks and the corresponding defense mechanisms should be investigated, as well.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Challenges and Considerations Related to the Algorithm</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Similar to almost every decentralized algorithm, one of the essential considerations of federated learning is the convergence of the algorithm under limited communication and computation resources. Theoretical analysis on the convergence bounds of the gradient descent based federated learning for convex loss functions has been carried out in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Analytical evaluations on the circumstances under which the algorithm converges for non-convex loss functions are beneficial as well, as in some models including deep neural networks, the natural objective of the model is to learn a non-convex function.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Furthermore, considerations such as optimum number of local learners to participate in the global update, grouping of the local learners, and frequency of local updates and global aggregation, that induce trade-off between model performance and resource preservation, are application-dependent and worth investigation.
In addition, for some models such as federated deep neural networks, even the updates might still be large in size for low-powered devices such as IoT nodes. Therefore, approaches that sparsify and compress the model parameters are computationally efficient and reduce the resource consumption.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Challenges and Considerations in Wireless Settings</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Owing to the limited capacity of wireless channels, information needs to be quantized before it is sent over the channel. Since local learners and the aggregator need to exchange model parameters over the wireless channel, this would give rise to the paradigm of federated learning with parameter quantization. One important consideration in such a paradigm would be the robustness of models in the presence of quantization error. Besides communication bandwidth, noise and interference are other factors that exacerbate the channel bottleneck. Robustness to these channel effects should be considered, as well.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Another important consideration is the convergence time. The convergence time federated learning includes not only the computation time on the local learners and the aggregator but also the communication time between them which depends on the wireless channel quality. Therefore, wireless channel quality should be considered when optimizing the frequency of the local updates and global aggregation.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p">Moreover, when learning a deep model, there are model compression techniques and sparse training approaches to reduce the complexity of the model and scale down the model parameters. These approaches are useful for devices with limited processing power to learn deep models. Therefore, there is a tradeoff between reducing the complexity and maintaining the accuracy of the model. In federated deep learning for wireless applications, communication cost and quality of the wireless channel should also be considered in the model optimization. In addition, given the time varying nature of the wireless channel, model compression can be done in an adaptive manner depending on the quality of the wireless channel.</p>
</div>
<div id="S4.SS3.p4" class="ltx_para">
<p id="S4.SS3.p4.1" class="ltx_p">Besides devices availability and their willingness to participate in the learning process, quality of the wireless channel between the global aggregator and a specific local learner will also impact its selection for training and should be considered jointly with other factors. There might be cases where a particular device is willing to contribute but its corresponding wireless channel is not strong enough to transfer the model parameters with predetermined quality, which may degrade the accuracy of the global model.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Concluding Remarks</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This article discussed the role of federated ML in addressing some of the challenges in wireless communications mainly related to the 5G paradigm. Federated ML is an emerging decentralized learning solution that tries to address the energy, bandwidth, delay and data privacy concerns in wireless communications by performing decentralized model training.
We started by providing an accessible introduction to the concept of federated learning and its salient features. We then introduced several use cases of federated learning in 5G networks, spanning from edge to the core network. Simulations have been carried out to demonstrate the applicability of federated learning to content popularity prediction in a cache-enabled network for AR applications. Our results indicate that federated learning could approach the performance of the centralized scheme in which the training is performed centrally by transferring all the data from the users to the central node (which is often not possible in practice due to privacy concerns). Numerous issues and open challenges are also discussed that require further research effort in this direction.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
ETSI, “Improved operator experience through experiential networked
intelligence (ENI),” Sophia Antipolis, France, Tech. Rep. No. 22, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
ATIS, “Evolution to an artificial intelligence-enabled network,”
Washington, DC, USA, Tech. Rep. ATIS-I-0000068, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
3GPP TR 23.791, “Study of enablers for network automation for 5G
(release 16),” Tech. Rep., 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cisco, “Cisco visual networking index: Global mobile data traffic forecast
update 2017-2022,” San Jose, CA, USA, Tech. Rep. C11-738429-01, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Bonawitz <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Towards federated learning at scale: System
design,” in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">SysML</em>, 2019, to appear. [Online]. Available:
https://arxiv.org/abs/1902.01046

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
H. B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proc. Int. Conf. on AI and Statist.</em>, Apr., 2017, pp.
1273–1282.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
K. Bonawitz <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Practical secure aggregation for federated
learning on user-held data,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">NIPS workshop on Private Multi-Party
Mach. Learn.</em>, Dec. 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Q. Yang, Y. Liu, T. Chen, and Y. Tong, “Federated machine learning: Concept
and applications,” <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACM Trans. Intell. Syst. Technol.</em>, vol. 10, no. 2,
pp. 1–19, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
3GPP TS 23.501 , “System architecture for the 5G system (release
16),” Tech. Rep., 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Z. Wang <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Beyond inferring class representatives: User-level
privacy leakage from federated learning,” in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">IEEE Conf. on Comput.
Commun.</em>, 2019, pp. 2512–2520.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
learning: A client level perspective,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ICLR conf.</em>, 2019, to
appear. [Online]. Available: https://arxiv.org/abs/1712.07557

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The secret
sharer: Evaluating and testing unintended memorization in neural networks,”
in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">USENIX Security Symp.</em>, 2019, pp. 267–284.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Bhagoji <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Analyzing federated learning through an adversarial
lens,” in <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">Int. Conf. on Mach. Learn.</em>, vol. 97, June 2019, pp.
634–643.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov, “Exploiting unintended
feature leakage in collaborative learning,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Symp. on Security
and Privacy</em>, 2019, pp. 691–706.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
S. Wang <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Adaptive federated learning in resource constrained
edge computing systems,” <em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>, vol. 37, no. 6,
pp. 1205–1221, June 2019.

</span>
</li>
</ul>
</section>
<figure id="tab1" class="ltx_float biography">
<table id="tab1.1" class="ltx_tabular">
<tr id="tab1.1.1" class="ltx_tr">
<td id="tab1.1.1.1" class="ltx_td"></td>
<td id="tab1.1.1.2" class="ltx_td">
<span id="tab1.1.1.2.1" class="ltx_inline-block">
<span id="tab1.1.1.2.1.1" class="ltx_p"><span id="tab1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Solmaz Niknam</span> 
received her Ph.D. degree from Kansas State University, KS, USA in 2018. During her Ph.D., she was a recipient of the Kansas Ph.D. students Fellowship. She is currently a postdoctoral associate at Virginia Tech. Her research interests include wireless communication with emphasis on 5G mm-wave networks and ML/AI in communication.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab2" class="ltx_float biography">
<table id="tab2.1" class="ltx_tabular">
<tr id="tab2.1.1" class="ltx_tr">
<td id="tab2.1.1.1" class="ltx_td"></td>
<td id="tab2.1.1.2" class="ltx_td">
<span id="tab2.1.1.2.1" class="ltx_inline-block">
<span id="tab2.1.1.2.1.1" class="ltx_p"><span id="tab2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Harpreet S. Dhillon</span>  (S’11-M’13-SM’19) is an Associate Professor of Electrical and Computer Engineering and the Elizabeth and James E. Turner Jr. ’56 Faculty Fellow at Virginia Tech. He received his B.Tech. degree from IIT Guwahati in 2008, his M.S. degree from Virginia Tech in 2010, and his Ph.D. degree from the University of Texas at Austin in 2013, all in Electrical Engineering. His research interests include communication theory, wireless networks, stochastic geometry, and machine learning. He is a Clarivate Analytics Highly Cited Researcher and a recipient of five best paper awards. He serves as an Editor for three IEEE journals.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="tab3" class="ltx_float biography">
<table id="tab3.1" class="ltx_tabular">
<tr id="tab3.1.1" class="ltx_tr">
<td id="tab3.1.1.1" class="ltx_td"></td>
<td id="tab3.1.1.2" class="ltx_td">
<span id="tab3.1.1.2.1" class="ltx_inline-block">
<span id="tab3.1.1.2.1.1" class="ltx_p"><span id="tab3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jeffrey H. Reed</span>  (F’05) is currently the Willis G. Worcester Professor with the Bradley Department of Electrical and Computer Engineering, Virginia Tech. He received his B.S., M.S., and Ph.D. degrees from the University of California, Davis, CA, USA, in 1979, 1980 and 1987, respectively. He is the Founding Faculty Member of the Ted and Karyn Hume Center for National Security and Technology. In 2012, he served on the President’s Council for the Advisors of Science and Technology Working Group and is currently interim director of the Commonwealth Cyber Initiative. He is the author of three books and over 200 journal and conference papers.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1908.06845" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1908.06847" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1908.06847">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1908.06847" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1908.06848" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 16:24:41 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
