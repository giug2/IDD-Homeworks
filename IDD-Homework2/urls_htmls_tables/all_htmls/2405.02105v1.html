<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph</title>
<!--Generated on Fri May  3 14:00:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models Open Research Knowledge Graph Structured Summarization." lang="en" name="keywords"/>
<base href="/html/2405.02105v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S1" title="In Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S2" title="In Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3" title="In Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Materials and Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS1" title="In 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Material: Our Evaluation Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS2" title="In 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Method: Three Large Language Models Applied for Research Dimensions Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS2.SSS1" title="In 3.2 Method: Three Large Language Models Applied for Research Dimensions Generation ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>The three LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS2.SSS2" title="In 3.2 Method: Three Large Language Models Applied for Research Dimensions Generation ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Research dimensions generation prompt for the LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS3" title="In 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS3.SSS1" title="In 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Semantic alignment and deviation evaluations using GPT-3.5</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS3.SSS2" title="In 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>ORKG property and LLM-generated research dimension mappings by GPT-3.5</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS3.SSS3" title="In 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Scientific embeddings-based semantic distance evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S3.SS3.SSS4" title="In 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>Human assessment survey comparing ORKG properties with LLM-generated research dimensions</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4" title="In Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.SS1" title="In 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Semantic alignment and deviation evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.SS2" title="In 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Property and research dimension mappings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.SS3" title="In 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Embeddings-based evaluations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.SS4" title="In 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Human assessment survey</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S5" title="In Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>L3S Research Center, Leibniz Univesity Hannover, Hannover, Germany
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>vladyslav.nechakhin@l3s.de</span></span></span> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Leibniz Information Centre for Science and Technology, Hannover, Germany
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id2.1"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>jennifer.dsouza@tib.eu</span></span></span> </span></span></span><span class="ltx_note ltx_role_institutetext" id="id3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>University of Mannheim, Mannheim, Germany
<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id3.1"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">email: </span>steffen.eger@uni-mannheim.de</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Evaluating Large Language Models for Structured Science Summarization in the 
<br class="ltx_break"/>Open Research Knowledge Graph</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vladyslav Nechakhin 
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-0146-1207" title="ORCID identifier">0000-0003-0146-1207</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jennifer D’Souza
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6616-9509" title="ORCID identifier">0000-0002-6616-9509</a></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Steffen Eger
</span><span class="ltx_author_notes">33
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4663-8336" title="ORCID identifier">0000-0003-4663-8336</a></span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers’ contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it’s essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG’s manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Large Language Models Open Research Knowledge Graph Structured Summarization.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The exponential growth of scholarly publications poses a significant challenge for researchers seeking to efficiently explore and navigate the vast landscape of scientific literature <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib5" title="">5</a>]</cite>. This proliferation of publications necessitates the development of strategies that go beyond traditional keyword-based search methods to facilitate effective and strategic reading practices. In response to this challenge, structured representation of scientific papers has emerged as a valuable approach for enhancing FAIR research discovery and comprehension. By describing research contributions in a structured, machine-actionable format w.r.t. the salient properties of research, also seen as research dimensions, similar such structured papers can be easily compared offering researchers a systematic and quick snapshot of research progress within specific domains, thus enabling them efficient ways to stay updated with research progress.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">One notable initiative aimed at publishing structured representations of scientific papers is the Open Research Knowledge Graph (<a class="ltx_ref ltx_href" href="https://orkg.org/" title="">ORKG</a>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib6" title="">6</a>]</cite>. The ORKG endeavors to describe papers in terms of various research dimensions or properties. For instance, the properties "model family", "pretraining architecture", "number of parameters", "hardware used" that can effectively be applied to offer structured, machine-actionable summaries of research contributions on the research problem "transformer model" in the domain of Computer Science (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>). Furthermore, another distinguishing characteristic of the properties is aside from offering a structured summary of a transformer model, they are also generically applicable across various contributions on the same problem thus making the structured paper descriptions comparable. Thus these properties can be explicitly stated as research comparison properties. As another example, papers with the research problem of "DNA sequencing techniques" in the domain of Biology can be described as structured summaries based on the following properties: "sequencing platform", "read length in base pairs", "reagents cost", "runtime in days" (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>). This type of paper description provides a structured framework for understanding and contextualizing research findings. Notably, however, the predominant method in the ORKG for creating structured paper descriptions or research comparisons is manually performed by domain experts. This means that the domain experts based on their prior knowledge and experience on a research problem select and describe the research comparison properties. While this ensures high-quality of the resulting structured papers in the ORKG, the manual annotation cycles cannot effectively scale the ORKG in practice. Specifically, the manual extraction of these salient properties of research or research comparison properties presents two significant challenges: 1) manual annotation a time-consuming process; and 2) it introduces inconsistencies among human annotators, potentially leading to variations in interpretation and annotation.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="393" id="S1.F1.g1" src="extracted/5575392/img/fig1.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>ORKG Comparison - A Catalog of Transformer Models (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://orkg.org/comparison/R656113/" title="">https://orkg.org/comparison/R656113/</a>)</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="400" id="S1.F2.g1" src="extracted/5575392/img/fig2.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>ORKG Comparison - Survey of sequencing techniques (<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://orkg.org/comparison/R44668/" title="">https://orkg.org/comparison/R44668/</a>)</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address the challenges associated with the manual annotation of research comparison properties, this study tests the feasibility of using pretrained Large Language Models (LLMs) to automatically suggest or recommend research dimensions as candidate properties as a viable alternative solution. Specifically, three different LLM variants, viz. GPT-3.5-turbo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib3" title="">3</a>]</cite>, Llama 2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib28" title="">28</a>]</cite>, and Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib14" title="">14</a>]</cite>, are tested and empirically compared for their advanced natural language processing (NLP) capabilities when applied to the task of recommending research dimensions as candidate properites. Our choice to apply LLMs is based on the following experimental consideration. The multidisciplinary nature of scientific research poses unique challenges to the identification and extraction of salient properties across domains. LLMs, with their ability to contextualize and understand natural language at scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib16" title="">16</a>]</cite>, are particularly well-suited to navigate the complexities of interdisciplinary research and recommend relevant dimensions that capture the essence of diverse scholarly works. By automating the extraction process, LLMs aim to alleviate the time constraints associated with manual annotation and ensure a higher level of consistency in the specification of research dimensions by using the same system prompt or fine-tuning on gold-standard ORKG data to better align them to the task. The role of LLMs in this context is to assist domain-expert human annotators rather than replace them entirely. By leveraging the capabilities of LLMs, researchers can streamline the process of dimension extraction and enhance the efficiency and reliability of comparative analysis across diverse research fields.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this context, the central research question (RQ) of this study is to examine the performance of state-of-the-art Large Language Models (LLMs) in recommending research dimensions. To address this RQ, we compiled a multidisciplinary, gold-standard dataset of human-annotated scientific papers from the Open Research Knowledge Graph (ORKG), detailed in the Materials and Methods section (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.SS1" title="3.1 Material: Our Evaluation Dataset ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">subsection 3.1</span></a>). This dataset includes structured summary property annotations by domain experts. We conducted a detailed comparative evaluation of the domain-expert annotated properties from the ORKG against the dimensions generated by LLMs for the same papers. Our evaluations are based on four unique perspectives: 1) semantic alignment and deviation assessment by GPT-3.5 between ORKG properties and LLM-generated dimensions, 2) fine-grained property mapping accuracy by GPT-3.5, 3) SciNCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib21" title="">21</a>]</cite> embeddings-based cosine similarity between ORKG properties and LLM-generated dimensions, and 4) a survey with human experts comparing their annotations of ORKG properties with the LLM-generated dimensions.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">As such, the contribution of this work is a comprehensive set of insights into the readiness of LLMs to support human annotators in the task of structuring their research contributions. Our findings reveal a moderate alignment between LLM-generated dimensions and manually annotated ORKG properties, indicating the potential for LLMs to learn from human-annotated data. However, there is a noticeable gap in the mapping of dimensions generated by LLMs and those annotated by domain experts, highlighting the need for fine-tuning LLMs on domain-specific datasets to reduce this disparity. Despite this gap, LLMs demonstrate the ability to capture the semantic relationships between LLM-generated dimensions and ORKG properties, as evidenced by strong correlation results of embeddings similarity. In the survey, the human experts noted that while they were not ready to change their existing property annotations based on the LLM generated dimensions, they highlighted the utility of the auto-LLM recommendation service at the time of creating the structured summary descriptions. This directly informs a future research direction in making LLMs fit for structured science summarization.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The utilization of Large Language Models (LLMs) for various NLP tasks has has seen widespread adoption in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib9" title="">9</a>]</cite>. Within the realm of scientific literature analysis, researchers have explored the potential of LLMs for tasks such as generating summaries and abstracts of research papers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib15" title="">15</a>]</cite>, extracting insights and identifying patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib20" title="">20</a>]</cite>, aiding in literature reviews <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib4" title="">4</a>]</cite>, enhancing knowledge integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib18" title="">18</a>]</cite>, etc. However, the specific application of LLMs for recommending research dimensions to obtain structured representations of research contributions is a relatively new area of investigation that we explore in this work. Furthermore, to offer insights into the readiness of LLMs over our novel task, we perform a comprehensive set of evaluations comparing the LLM-generated research dimensions and the human expert annotated properties. As a straightforward preliminary evaluation, we measure the semantic similarity between the LLM and human annotated properties. To do this, we employ a specialized language model tuned for the scientific domain to create embeddings for the respective properties.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The development of domain-specific language models has been a significant advancement in NLP. In the scientific domain, a series of specialized models have emerged. SciBERT, introduced by Beltagy et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib8" title="">8</a>]</cite>, was the first language model tailored for scientific texts. This was followed by SPECTER, developed by Cohan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib11" title="">11</a>]</cite>. More recently, Ostendorff et al. introduced SciNCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib21" title="">21</a>]</cite>, a language model designed to capture semantic similarity between scientific concepts by leveraging pre-trained BERT embeddings. SciNCL has demonstrated its effectiveness in evaluating the nuances of scientific concepts, making it an ideal choice for assessing LLM-generated dimensions in scientific literature analysis. In this study, we utilize SciNCL, the most recent and advanced variant, to generate embeddings for ORKG properties and LLM-generated dimensions.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">In the context of evaluating LLM-generated dimensions against manually curated properties, several studies have employed similarity measures to quantify the relatedness between the two sets of textual data. One widely used metric is cosine similarity, which measures the cosine of the angle between two vectors representing the dimensions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib25" title="">25</a>]</cite>. This measure has been employed in various studies, such as Yasunaga et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib31" title="">31</a>]</cite>, who used cosine similarity to assess the similarity between automatically generated summaries by LLMs and human-written annotations. Similarly, Banerjee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib7" title="">7</a>]</cite> employed cosine similarity as a metric to benchmark the accuracy of LLM-generated answers of autonomous conversational agents. In contrast to cosine similarity, other studies have explored alternative similarity measures for evaluating LLM-generated content. For instance, Jaccard similarity measures the intersection over the union of two sets, providing a measure of overlap between them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib29" title="">29</a>]</cite>. This measure has been employed in tasks such as document clustering and topic modeling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib22" title="">22</a>]</cite>. Jaccard similarity offers a distinct perspective on the overlap between manually curated and LLM-generated properties, as it focuses on the shared elements between the two sets rather than their overall similarity. We considered both cosine and Jaccard similarity in our evaluation, however, based on our embedding representation, we ultimately chose to use cosine similarity as our distance measure.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Finally, aside from the straightforward similarity computations between the two sets of properties, we also leverage the capabilities of LLMs as evaluators. The utilization of LLMs as evaluators in various NLP tasks has been proven to be a successful approach in a number of recent publications. For instance, Kocmi and Federmann <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib17" title="">17</a>]</cite> demonstrated the effectiveness of GPT-based metrics for assessing translation quality, achieving state-of-the-art accuracy in both reference-based and reference-free modes. Similarly, the Eval4NLP 2023 shared task, organized by Leiter et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib19" title="">19</a>]</cite>, explored the use of LLMs as explainable metrics for machine translation and summary evaluation, showcasing the potential of prompting and score extraction techniques to achieve results on par with or even surpassing recent reference-free metrics. In our study, we employ the GPT-3.5 model as an evaluator, leveraging its capabilities to assess the quality of LLM-generated research dimensions.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">In summary, previous research has laid the groundwork for evaluating LLMs’ performance in scientific literature analysis, and our study builds upon these efforts by exploring the application of LLMs for recommending research dimensions and evaluating their quality using specialized language models and similarity measures.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Materials and Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section is organized into three subsections. In the first subsection, the creation of the gold-standard evaluation dataset from the ORKG with domain-expert, human-annotated research comparison properties used for assessing the similarity with the LLM generated properties is described. The second subsection provides an overview of the three LLMs, viz. GPT-3.5, Llama 2, and Mistral, applied to automatically generate the research comparison properties, highlighting their respective technical characteristics. Lastly, the third subsection discusses the various evaluation methods used in this study offering differing perspectives on the similarity comparison of the ORKG properties for the instances in our gold-standard dataset versus those generated by the LLMs.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Material: Our Evaluation Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As alluded to in the Introduction, a central <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">RQ</span> of this work is to compare the research dimensions generated by three different LLMs with the human-annotated research comparison properties in the ORKG. For this, we created an evaluation dataset of annotated research dimensions based on the ORKG. As a starting point, we curated a selection of <a class="ltx_ref ltx_href" href="https://orkg.org/comparisons" title="">ORKG Comparisons</a> by selecting comparisons that were created by experienced ORKG users. These users had varied research backgrounds. The selection criteria of comparisons from these users were as follows: the comparisons had to have at least 3 properties and contain at least 5 contributions, since we wanted to ensure that the properties were not too sparse representation of a research problem, but were those that generically reflected a research comparison over several works. On application of this criteria, the resulting dataset comprised 103 ORKG Comparisons. These selected gold-standard comparisons contained 1,317 papers from 35 different research fields addressing over 150 distinct research problems. The gold-standard dataset can be downloaded from the <a class="ltx_ref ltx_href" href="https://doi.org/10.25835/6oyn9d1n" title="">Leibniz University Data Repository</a>. The selection of comparisons ensured the diversity of research fields’ distribution, containing Earth Sciences, Natural Language Processing, Medicinal Chemistry and Pharmaceutics, Operations Research, Systems Engineering, Cultural History, Semantic Web and others. See <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.F3" title="Figure 3 ‣ 3.1 Material: Our Evaluation Dataset ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> for the full distribution of research fields in our dataset.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S3.F3.g1" src="extracted/5575392/img/fig3.png" width="412"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Research field distribution of the selected papers in our evaluation dataset containing domain-expert human-annotated properties that were applied to represent the paper’s structured contribution descriptions in the Open Research Knowledge Graph (ORKG). </figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Once we had the comparisons, we then looked at the individual structured papers within each comparison and extracted their human annotated properties. Thus our resulting dataset, is highly multidisciplinary, comprising structured paper instances from the ORKG with their corresponding domain expert property annotations across different fields of research. For instance, the properties listed below were extracted form the comparison "A Catalog of Transformer Models" (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>):</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p3.1">
["has model", "model family", "date created", "organization",
"innovation", "pretraining architecture", "pretraining task",
"fine-tuning task", "training corpus", "optimizer",
"tokenization", "number of parameters", "maximum number of
parameters (in million)", "hardware used", "hardware
information", "extension", "has source code", "blog post",
"license", "research problem"]
</pre>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Another example of structured paper’s properties in the comparison "Survey of sequencing techniques" (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>) is as follows:</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS1.p5.1">
["cost of machine in $", "cost of resequencing a human genome
with 30X coverage in $", "cost per Gigabyte data output in $",
"data output rate in Gigabyte per day", "has research problem",
"read length in base pairs (paired-end*) ", "reads per run in
Million", "reagents cost in $", "runtime in days", "sequencing
platform", "total data output yield in Gigabyte"]
</pre>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">The aforementioned dataset is now the gold-standard that we use in the evaluations for the LLM-generated research dimensions. In this section, we provide a clear distinction between the terminology of ORKG <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.1">properties</span> and LLM-generated <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">research dimensions</span>. According to our hypothesis, ORKG properties are not necessarily identical to research dimensions. Contribution properties within ORKG relate to specific attributes or characteristics associated with individual research papers in a comparison, outlining aspects such as authorship, publication year, methodology, and findings. Conversely, research dimensions encapsulate the multifaceted aspects of a given research problem, constituting the nuanced themes or axes along which scholarly investigations are conducted. ORKG contribution properties offer insights into the attributes of individual papers, research dimensions operate at a broader level, revealing the finer-grained thematic fundamentals of research endeavors. While ORKG contribution properties focus on the specifics of research findings, research dimensions offer a more comprehensive context for analyzing a research question that can be used for finding similar papers that share the same dimensions. In order to test the alignment of LLM-generated research dimensions to ORKG properties, several LLMs were selected to be compared, as described in the next section.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Method: Three Large Language Models Applied for Research Dimensions Generation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we discuss the LLMs applied for automated research dimensions generation as well as the task-specific prompt that was designed as input to the LLM.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>The three LLMs</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">To test the automated generation of research dimensions, we tested and compared the output from three different state-of-the-art LLMs with comparable parameter counts, namely GPT-3.5, Llama 2 and Mistral.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">GPT-3.5, developed by OpenAI, is one of the most influential LLMs to date, its number of parameters is not publicly disclosed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib1" title="">1</a>]</cite>. In comparison, it’s predecessor GPT-3 models come in different sizes and contain from 125 million parameters for the smallest model to 175 billion for the largest <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib9" title="">9</a>]</cite>. GPT-3.5 has demonstrated exceptional performance on a range of NLP tasks including translation, question answering and text completion. Notably, the capabilities of this model are accessed through the OpenAI API, since the model is closed-source, which limits direct access to its architecture for further exploration or customization.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">Llama 2 by Meta AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib28" title="">28</a>]</cite>, the second iteration of the Llama LLM, represents a significant advancement in the field. Featuring twice the context length of its predecessor Llama 1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib27" title="">27</a>]</cite>, Llama 2 offers researchers and practitioners a versatile tool for working with NLP. Importantly, Llama 2 is available free of charge for both research and commercial purposes, with multiple parameter configurations available, including a 13 billion parameter option. In addition, the model supports fine-tuning and self-hosting, which enhances its adaptability to a variety of use cases.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1">Mistral, developed by Mistral AI, is a significant competitor in the landscape of LLMs. With a parameter count of 7.3 billion, Mistral demonstrates competitive performance in various benchmarks, often outperforming Llama 2 despite its smaller size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib14" title="">14</a>]</cite>. In addition, Mistral is distinguished by its open source code released under the Apache 2.0 license, making it easily accessible for research and commercial applications. Notably, Mistral has an 8k context window, compared to 4k context window of Llama 2, which allows for a more complete understanding of context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p5">
<p class="ltx_p" id="S3.SS2.SSS1.p5.1">Overall, GPT-3.5, despite its closed source nature, remains influential in NLP research, with a vast number of parameters that facilitate its generic task performance in the context of a wide variety of applications. Conversely, Llama 2 and Mistral, with their open source nature, provide flexibility and accessibility for researchers and developers while displaying similar performance characteristics to GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib2" title="">2</a>]</cite>. Released shortly after Llama 2, Mistral, in particular, shows notable performance advantages over Llama 2, highlighting the rapid pace of innovation and improvement in the development of LLMs. These differences between the models lay the groundwork for assessing their alignment with manually curated properties from ORKG and determining their potential for automated research metadata creation and retrieval of related work.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Prompt variations utilizing different prompt engineering techniques to instruct LLMs for the research dimensions generation task.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1" style="width:51.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1">Prompting technique</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:199.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1.1.1">System prompt</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.3.1">
<span class="ltx_p" id="S3.T1.1.1.1.3.1.1" style="width:88.2pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.3.1.1.1">Output example</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1.1" style="width:51.2pt;">Zero-Shot</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.2.1">
<span class="ltx_p" id="S3.T1.1.2.1.2.1.1" style="width:199.2pt;">You will be provided with a research problem, your task is to list dimensions that are relevant to find similar papers for the research problem. Respond only in the format of a python list.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.3.1">
<span class="ltx_p" id="S3.T1.1.2.1.3.1.1" style="width:88.2pt;">["Natural Language Processing", "Text Analysis", "Machine Learning", "Deep Learning", "Information Retrieval", "Artificial Intelligence", "Language Models", "Document Summarization"]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.1.1">
<span class="ltx_p" id="S3.T1.1.3.2.1.1.1" style="width:51.2pt;">Few-Shot</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1" style="width:199.2pt;">You will be provided with a research problem, your task is to list dimensions that are relevant to find similar papers for the research problem. Respond only in the format of a python list. 
<br class="ltx_break"/>The following are two successfully completed task examples. 
<br class="ltx_break"/>Research problem: "Transformer models" 
<br class="ltx_break"/>Research dimensions: [’model’, ’date created’, ’pretraining architecture’, ’pretraining task’, ’training corpus’, ’optimizer’, ’tokenization’, ’number of parameters’, ’license’] 
<br class="ltx_break"/>Research problem: "Liposomes as drug carriers" 
<br class="ltx_break"/>Research dimensions: [’Type of nanocarrier’, ’Nanoparticle preparation method’, ’Lipid composition’, ’Drug type’, ’Particle size’]</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T1.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.3.1">
<span class="ltx_p" id="S3.T1.1.3.2.3.1.1" style="width:88.2pt;">[’Summarization approach’, ’Document type’, ’Language’, ’Evaluation metric’, ’Model type’, ’Training dataset’, ’Compression ratio’, ’Summary length’]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.1.1">
<span class="ltx_p" id="S3.T1.1.4.3.1.1.1" style="width:51.2pt;">Chain-of-Thought</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1" style="width:199.2pt;">You will be provided with a research problem, your task is to list dimensions that are relevant to find similar papers for the research problem. Provide justification why each dimension is relevant for finding similar papers. Think step-by-step. At the end combine all the relevant dimensions in the format of a python list.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.4.3.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.3.1">
<span class="ltx_p" id="S3.T1.1.4.3.3.1.1" style="width:88.2pt;">["Task/Methodology", "Domain/Genre", "Evaluation Metrics", "Language", "Input/Output Format", "Deep Learning/Traditional Methods", "Applications"]</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Research dimensions generation prompt for the LLMs</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">LLM’s performance on a particular task is highly dependent on the quality of the prompt. To find the optimal prompt methodology, our study explores various established prompting techniques, including zero-shot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib24" title="">24</a>]</cite>, few-shot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib9" title="">9</a>]</cite>, and chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#bib.bib30" title="">30</a>]</cite>. The simplest type of prompt is a zero-shot approach, wherein pretrained LLMs owing to the large-scale coverage of human tasks within their pretraining datasets demonstrate competence in task execution without prior exposure to specific examples. While zero-shot prompting provides satisfactory results for certain tasks, some of the more complex tasks require few-shot prompting. By providing the model with several examples, few-shot prompting enables in-context learning, potentially enhancing model performance. Another popular technique is chain-of-thought prompting, which instructs the model to think step-by-step. Guiding the LLM through sequential steps helps to break down complex tasks into manageable components that are easier for the LLM to complete.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">In this study, the task involves providing the LLM with a research problem, and based on this input, the model should suggest research dimensions that it finds relevant to structure contributions from similar papers that address the research problem. Our system prompts for each technique along with the examples of output for a research problem "Automatic text summarization" from GPT-3.5 are shown in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.T1" title="Table 1 ‣ 3.2.1 The three LLMs ‣ 3.2 Method: Three Large Language Models Applied for Research Dimensions Generation ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Our analysis shows that the utilization of more advanced prompting techniques did not necessarily result in superior outcomes, which leads us to believe that our original zero-shot prompt is sufficient for our task completion. The absence of discernible performance improvements with the adoption of more complex prompting techniques highlights the effectiveness of the initial zero-shot prompt in aligning with the objectives of research dimension extraction. Consequently, we will be apply the zero-shot prompt methodology.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1">To test the alignment of LLM-generated research dimensions to ORKG properties, each of the LLMs was given the same prompt to create a list of dimensions that are relevant to find similar papers based on the provided research problem. <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.T2" title="Table 2 ‣ 3.2.2 Research dimensions generation prompt for the LLMs ‣ 3.2 Method: Three Large Language Models Applied for Research Dimensions Generation ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Table 2</span></a> shows the comparison between some of the manually created properties form ORKG and the research dimensions, provided by GPT-3.5, Llama 2 and Mistral.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of manually created ORKG properties with LLM-generated research dimensions for the same papers.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1" style="width:42.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.1.1">Research problem</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.2.1.1.1">ORKG properties</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.3.1">
<span class="ltx_p" id="S3.T2.1.1.1.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.3.1.1.1">GPT-3.5 dimensions</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.4.1">
<span class="ltx_p" id="S3.T2.1.1.1.4.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.4.1.1.1">Llama 2 dimensions</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.5.1">
<span class="ltx_p" id="S3.T2.1.1.1.5.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.5.1.1.1">Mistral dimensions</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.2.1.1.1.1" style="width:42.7pt;"><a class="ltx_ref ltx_href" href="https://orkg.org/comparison/R141123/" title="">Limestone mineral identification using Hyperion imagery</a></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.2.1.2.1.1" style="width:71.1pt;">’Data used’,
’Research problem’,
’Study Area’,
’Software’,
’Analysis’,
’Processing’,
’Field instrument’,
’Reference’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.3.1">
<span class="ltx_p" id="S3.T2.1.2.1.3.1.1" style="width:56.9pt;">’Mineralogy’,
’Limestone’,
’Hyperion imagery’,
’Identification’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.4.1">
<span class="ltx_p" id="S3.T2.1.2.1.4.1.1" style="width:85.4pt;">’Remote Sensing’,
’Hyperspectral Imaging’,
’Mineral Identification’,
’Limestone’,
’Hyperion Imagery’,
’Geological Mapping’,
’Image Classification’,
’Spectral Analysis’,
’Earth Observation’,
’Geoscience’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.2.1.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.2.1.5.1">
<span class="ltx_p" id="S3.T2.1.2.1.5.1.1" style="width:85.4pt;">’Limestone’,
’Mineral identification’,
’Remote sensing’,
’Hyperion imagery’,
’Spectral signatures’,
’Band ratios’,
’Textural features’,
’Shape descriptors’,
’Machine learning algorithms’,
’Support Vector Machines’,
’Decision trees’,
’Random forests’</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.3.2.1.1.1" style="width:42.7pt;"><a class="ltx_ref ltx_href" href="https://orkg.org/comparison/R204005/" title="">Do human noroviruses infect animals</a></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.3.2.2.1.1" style="width:71.1pt;">’Material’,
’Method’,
’Symptoms and signs’,
’Research problem’,
’Species’,
’Location’,
’Antigen used’,
’Immunoglobulin class’,
’Primers’,
’Noroviruses found’,
’Norovirus genotype’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.3.2.3">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.3.1">
<span class="ltx_p" id="S3.T2.1.3.2.3.1.1" style="width:56.9pt;">’viral infectivity’,
’host range’,
’cross-species transmission’,
’zoonotic potential’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.3.2.4">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.4.1">
<span class="ltx_p" id="S3.T2.1.3.2.4.1.1" style="width:85.4pt;">’Host species’,
’Viral genotype’,
’Transmission route’,
’Infection prevalence’,
’Pathogenesis’,
’Diagnostic methods’,
’Epidemiology’,
’Evolutionary history’,
’Interaction with other pathogens’,
’Public health implications’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.3.2.5">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.3.2.5.1">
<span class="ltx_p" id="S3.T2.1.3.2.5.1.1" style="width:85.4pt;">’Norovirus’,
’Animals’,
’Transmission’,
’Species’,
’Symptoms’,
’Epidemiology’,
’Serotypes’,
’Immunity’,
’Detection methods’,
’Genome analysis’</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">This section outlines the methodology used to evaluate our dataset, namely the automated evaluation of semantic alignment and deviation as well as mapping between ORKG properties and LLM-generated research dimensions performed by GPT-3.5. Additionally, we present our approach to calculating embedding similarity between properties and research dimensions.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Semantic alignment and deviation evaluations using GPT-3.5</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">To measure the semantic similarity between between ORKG properties and LLM-generated research dimensions, we conducted semantic alignment and deviation assessments using an LLM-based evaluator. In this context, semantic alignment refers to the degree to which two sets of concepts share similar meanings, whereas semantic deviation assesses how far apart they are in terms of meaning. As the LLM evaluator, we leveraged GPT-3.5. As input it was provided with both the lists of properties from ORKG and the dimensions extracted by the LLMs, in a string format, per research problem. Semantic alignment was rated on a scale from 1 to 5, using the following system prompt to perform this task:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS1.p2.1">
You will be provided with two lists of strings, your task is to
rate the semantic alignment between the lists on the scale form
1 to 5.Your response must only include an integer representing
your assessment of the semantic alignment, include no other
text.
</pre>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1">Additionally, the prompt included the detailed description of the scoring system shown in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.T3" title="Table 3 ‣ 3.3.1 Semantic alignment and deviation evaluations using GPT-3.5 ‣ 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Description of the semantic alignment scores provided in the GPT-3.5 system prompt.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1">Score</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.1.1.2.1">
<span class="ltx_p" id="S3.T3.1.1.1.2.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.2.1.1">1 - Strongly Disaligned</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T3.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.2.1.2.1">
<span class="ltx_p" id="S3.T3.1.2.1.2.1.1" style="width:256.1pt;">The strings in the lists have minimal or no semantic similarity.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.3.2.1">2 - Disaligned</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.3.2.2.1">
<span class="ltx_p" id="S3.T3.1.3.2.2.1.1" style="width:256.1pt;">The strings in the lists have limited semantic alignment.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.4.3.1">3 - Neutral</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.4.3.2.1">
<span class="ltx_p" id="S3.T3.1.4.3.2.1.1" style="width:256.1pt;">The semantic alignment between the lists is moderate or average.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T3.1.5.4.1">4 - Aligned</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T3.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.5.4.2.1">
<span class="ltx_p" id="S3.T3.1.5.4.2.1.1" style="width:256.1pt;">The strings in the lists show substantial semantic alignment.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S3.T3.1.6.5.1">5 - Strongly Aligned</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S3.T3.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T3.1.6.5.2.1">
<span class="ltx_p" id="S3.T3.1.6.5.2.1.1" style="width:256.1pt;">The strings in the lists exhibit high semantic coherence and alignment.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.1">To further validate the accuracy of our alignment scores, we leveraged GPT-3.5 as an evaluator again but this time to generate the semantic deviation scores. By using this contrastive alignment versus deviation evaluation method, we can cross-reference where the LLM evaluator displays a strong agreement in its evaluation and assess the evaluations for reliability. Specifically, we evaluate the same set of manually curated properties and LLM-generated research dimensions using both agents, with the expectation that the ratings will exhibit an inverse relationship. That is, high alignment scores should correspond to low deviation scores, and vice versa. The convergence of these opposing measures would provide strong evidence for the validity of our evaluation results. Similar to the task of alignment rating, the system prompt below was used to instruct GPT-3.5 to measure semantic deviation where the ratings described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.T4" title="Table 4 ‣ 3.3.1 Semantic alignment and deviation evaluations using GPT-3.5 ‣ 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Table 4</span></a> was also part of the prompt.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p5">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS1.p5.1">
You will be provided with two lists of strings, your task is to
rate the semantic deviation between the lists on the scale form
1 to 5. Your response must only include an integer representing
your assessment of the semantic deviation, include no other
text.
</pre>
</div>
<figure class="ltx_table" id="S3.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Description of the semantic deviation scores provided in the GPT-3.5 system prompt.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1.1">Score</span></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T4.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.1.1.2.1">
<span class="ltx_p" id="S3.T4.1.1.1.2.1.1" style="width:227.6pt;"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T4.1.2.1.1">1 - Minimal Deviation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.2.1.2.1">
<span class="ltx_p" id="S3.T4.1.2.1.2.1.1" style="width:227.6pt;">The strings in the lists show little or no semantic difference.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T4.1.3.2.1">2 - Low Deviation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.3.2.2.1">
<span class="ltx_p" id="S3.T4.1.3.2.2.1.1" style="width:227.6pt;">The semantic variance between the lists is limited.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T4.1.4.3.1">3 - Moderate Deviation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.4.3.2.1">
<span class="ltx_p" id="S3.T4.1.4.3.2.1.1" style="width:227.6pt;">There is a moderate level of semantic difference between the strings in the lists.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="S3.T4.1.5.4.1">4 - Substantial Deviation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T4.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.5.4.2.1">
<span class="ltx_p" id="S3.T4.1.5.4.2.1.1" style="width:227.6pt;">The lists exhibit a considerable semantic gap or difference.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="S3.T4.1.6.5.1">5 - Significant Deviation</th>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S3.T4.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T4.1.6.5.2.1">
<span class="ltx_p" id="S3.T4.1.6.5.2.1.1" style="width:227.6pt;">The semantic disparity between the lists is pronounced and substantial.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS3.SSS1.p6">
<p class="ltx_p" id="S3.SS3.SSS1.p6.1">By combining these two evaluations, we can gain a more nuanced understanding of the relationship between the ORKG properties and LLM-generated research dimensions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>ORKG property and LLM-generated research dimension mappings by GPT-3.5</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">To further analyse the relationships between ORKG properties and LLM-generated research dimensions, we used GPT-3.5 to find the mappings between individual properties and dimensions. This approach diverges from the previous semantic alignment and deviation evaluations, which considered the lists as a whole. Instead, we instructed GPT-3.5 to identify the number of properties that exhibit similarity with individual research dimensions. This was achieved by providing the model with the two lists of properties and dimensions and prompting it to count the number of similar values between the lists.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">The system prompt used for this task was as follows:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.SSS2.p3.1">
You will be provided with two lists of strings, your task is to
count how many values from list1 are similar to values of
list2. Respond only with an integer, include no other text.
</pre>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1">By leveraging GPT-3.5’s capabilities in this manner, we were able to count the number of LLM-generated research dimensions that are related to individual ORKG properties. The mapping count provides a more fine-grained insight into the relationships between the properties and dimensions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Scientific embeddings-based semantic distance evaluations</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">To further examine the semantic relationships between ORKG properties and LLM-generated research dimensions, we employed embeddings-based approach. Specifically, we utilized SciNCL to generate vector embeddings for both the ORKG properties and the LLM-generated research dimensions. These embeddings were then compared using cosine similarity as a measure of semantic similarity. We evaluated the similarity of ORKG properties to the research dimensions generated by GPT-3.5, Llama 2, and Mistral. Additionally, we compared the LLM-generated dimensions to each other, providing insights into the consistency and variability of the research dimensions generated by different LLMs. By leveraging embeddings-based evaluations, we were able to quantify the semantic similarity between the ORKG properties and the LLM-generated research dimensions, as well as among the dimensions themselves.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Human assessment survey comparing ORKG properties with LLM-generated research dimensions</h4>
<div class="ltx_para" id="S3.SS3.SSS4.p1">
<p class="ltx_p" id="S3.SS3.SSS4.p1.1">We conducted a survey to evaluate the utility of LLM generated dimensions in the context of domain-expert annotated ORKG properties. The survey was designed to solicit the impressions of domain experts when shown their original annotated properties versus the research dimensions generated by GPT-3.5. We selected participants who are experienced at creating structured paper descriptions in the ORKG. These participants included <a class="ltx_ref ltx_href" href="https://orkg.org/about/28/Curation_Grants" title="">ORKG curation grant</a> participants, ORKG employees, and authors whose comparisons were displayed on the ORKG <a class="ltx_ref ltx_href" href="https://orkg.org/featured-comparisons" title="">featured comparisons</a> page. Each participant was given a maximum of 5 surveys, for five different papers they structured respectively, each evaluating properties versus research dimensions. They had the choice to respond to one, some, or all of them. At the end, we received 23 total responses to our survey corresponding to 23 different papers.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS4.p2">
<p class="ltx_p" id="S3.SS3.SSS4.p2.1">The survey itself consisted of five questions, most of which were designed on a Likert scale, to guage the domain expert assessment of the effectiveness of the LLM-generated research dimensions. Per survey, as evaluation data, participants were presented with two tables: one including their annotated ORKG property names and values (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.F4" title="Figure 4 ‣ 3.3.4 Human assessment survey comparing ORKG properties with LLM-generated research dimensions ‣ 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>), and the second one consisting of research dimension name, its description, and value generated by GPT-3.5 from a title and abstract of the same paper (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S3.F5" title="Figure 5 ‣ 3.3.4 Human assessment survey comparing ORKG properties with LLM-generated research dimensions ‣ 3.3 Method: Three Types of Similarity Evaluations between ORKG Properties vs. LLM-generated Research Dimensions ‣ 3 Materials and Methods ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>). Following this data was the survey questionnaire. The questions asked participants to rate the relevance of LLM-generated research dimensions, consider making edits to the original ORKG structured contribution, and evaluate the usefulness of LLM-generated content as suggestions before creating their structured contributions. Additionally, participants were asked to describe how LLM-generated content would have been helpful and rate the alignment of LLM-generated research dimensions with the original ORKG structured contribution. The survey questionnaire is shown below:</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS4.p3">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">How many of the properties generated by ChatGPT are relevant to your ORKG structured contribution? (Your answer should be a number)</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Considering the ChatGPT-generated content, would you consider making edits to your original ORKG structured contribution?</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">If the ChatGPT-generated content were available to you as suggestions before you created your structured contribution, would it have been helpful?</p>
<ol class="ltx_enumerate" id="S3.I1.i3.I1">
<li class="ltx_item" id="S3.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span>
<div class="ltx_para" id="S3.I1.i3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i3.I1.i1.p1.1">If you answered "Yes" to the question above, could you describe how it would have been helpful?</p>
</div>
</li>
</ol>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">On a scale of 1 to 5, please rate how well the ChatGPT-generated response aligns with your ORKG structured contribution.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">We plan to release an AI-powered feature to support users in creating their ORKG contributions with automated suggestions. In this context, please share any additional comments or thoughts you have regarding the given ChatGPT-generated structured contribution and its relevance to your ORKG contribution.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="116" id="S3.F4.g1" src="extracted/5575392/img/fig4.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Example of ORKG properties shown to survey respondents</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="113" id="S3.F5.g1" src="extracted/5575392/img/fig5.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example of GPT dimensions shown to survey respondents</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS4.p4">
<p class="ltx_p" id="S3.SS3.SSS4.p4.1">The subsequent section will present the results obtained from these methodologies, providing insights into the similarity between ORKG properties and LLM-generated research dimensions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section presents the results of our evaluation, which aimed to assess the LLMs’ performance on the task of recommending research dimensions by calculating the similarity between ORKG properties and LLM-generated research dimensions. We employed three types of similarity evaluations: semantic alignment and deviation assessments, property and research dimension mappings using GPT-3.5, and embeddings-based evaluations using SciNCL.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Semantic alignment and deviation evaluations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The average alignment between paper properties and research field dimensions was found to be 2.9 out of 5, indicating a moderate level of alignment. In contrast, the average deviation was 4.1 out of 5, suggesting a higher degree of deviation. When normalized, the proportional values translate to 41.2% alignment and 58.8% deviation (<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.F6" title="Figure 6 ‣ 4.1 Semantic alignment and deviation evaluations ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">6</span></a>). These results imply that while there is some alignment between paper properties and research field dimensions, there is also a significant amount of deviation, highlighting the difference between the concepts of structured paper’s properties and research dimensions. This outcome supports our hypothesis that LLM-based research dimensions generated solely from a research problem, relying on LLM-encoded knowledge, may not fully capture the nuanced inclinations of domain experts when they annotate ORKG properties to structure their contributions, where the domain experts have the full paper at hand. We posit that an LLM, not explicitly tuned on the scientific domain, despite its vast parameter space, is not able to emulate human expert subjectivity to structure contributions.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="263" id="S4.F6.g1" src="extracted/5575392/img/fig6.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Proportional values of semantic alignment and deviation between ORKG properties and GPT-generated dimensions</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Property and research dimension mappings</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We examine our earlier posited claim in a more fine-grained manner by comparing the property versus research dimension mappings. The average number of mappings was found to be 0.33, indicating a low number of mappings between paper properties and research field dimensions. The average ORKG property count was 4.73, while the average GPT dimension count was 8. These results suggest that LLMs can generate a more diverse set of research dimensions than the ORKG properties, but with a lower degree of similarity. Notably, the nature of ORKG properties and research dimensions differs in their scope and focus. Common ORKG properties like "research problem", "method", "data source" and others provide valuable information about a specific paper, but they can not be used to comprehensively describe a research field as a whole. In contrast, research dimensions refer to shared properties of a research question, rather than focus on an individual paper. This difference contributes to the low mapping between paper properties and research field dimensions, which further consolidates our conjecture that an LLM based on only its own knowledge applied on a given research problem might not be able to completely emulate a human expert’s subjectivity in defining ORKG properties. These results therefore are not a direct reflection of the inability of the LLMs tested to recommend suitable properties to structure contributions on the theme. This then opens the avenue for future work to explore how fine-tuned LLMs on the scientific domain perform on the task as a direct extension of our work.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Embeddings-based evaluations</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The embeddings-based evaluations provide a more nuanced perspective on the semantic relationships between the ORKG properties and the LLM-generated research dimensions. By leveraging the SciNCL embeddings, we were able to quantify the cosine similarity between these two concepts, offering insights into their alignment. The results indicate a high degree of semantic similarity, with the cosine similarity between ORKG properties and the LLM-generated dimensions reaching 0.84 for GPT-3.5, 0.79 for Mistral, and 0.76 for Llama 2. These values suggest that the LLM-generated dimensions exhibit a strong correlation with the manually curated ORKG properties, signifying a substantial semantic overlap between the two.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Furthermore, the correlation heatmap (<a class="ltx_ref" href="https://arxiv.org/html/2405.02105v1#S4.F7" title="Figure 7 ‣ 4.3 Embeddings-based evaluations ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">7</span></a>) provides a visual representation of these similarities, highlighting the strongest correlations between ORKG properties and LLM-generated dimensions. The embeddings-based evaluations indicate that GPT-3.5 demonstrates the highest similarity to the ORKG properties, outperforming both Llama 2 and Mistral. When comparing LLM-generated dimensions between each other, a strong similarity observed between the Llama 2 and Mistral dimensions highlights the remarkable consistency in the research dimensions generated by these two models.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="412" id="S4.F7.g1" src="extracted/5575392/img/fig7.png" width="412"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Correlation heatmap of cosine similarity between ORKG properties and LLM-generated dimensions</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Overall, the embeddings-based evaluations provide a quantitative representation of the semantic relationships between the ORKG properties and the LLM-generated research dimensions. These results suggest that while there are notable differences between the two, the LLMs exhibit a strong capacity to generate dimensions that are semantically aligned with the manually curated ORKG properties, particularly in the case of GPT-3.5. This finding highlights the potential of LLMs to serve as valuable tools for automated research metadata creation and the retrieval of related work.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Human assessment survey</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To further evaluate the utility of the LLM-generated research dimensions, we conducted a survey to solicit feedback from domain experts who annotated the properties to create structured science summary representations or structured contribution descriptions in the ORKG. The survey was designed to assess the participants’ impressions when presented with their original ORKG properties alongside the research dimensions generated by GPT-3.5.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">For the first question in the Questionnaire assessing the relevance of LLM-generated dimensions to create a structured paper summary or to structure the contribution of a paper, on average, 36.3% of the research dimensions generated by LLMs were considered highly relevant (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S4.F8" title="Figure 8 ‣ 4.4 Human assessment survey ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>). This suggests that LLM-generated dimensions can provide useful suggestions for creating structured contributions on ORKG. For the second question, majority of participants (60.9%) did not think it was necessary to make changes to their existing ORKG structure paper property annotations based on the LLM-generated dimensions, indicating that while the suggestions were relevant, they may not have been sufficient to warrant significant changes (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S4.F9" title="Figure 9 ‣ 4.4 Human assessment survey ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>). However, based on the third question, the survey also revealed that the majority of authors (65.2%) believed that having LLM-generated content as suggestions before creating the structured science summaries or structured contributions would have been helpful (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S4.F10" title="Figure 10 ‣ 4.4 Human assessment survey ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>). The respondents noted that such LLM-based dimension suggestions could serve as a useful starting point, provide a basis for improvement, and aid in including additional properties. Based on the fourth question, the alignment between LLM-generated research dimensions and the original ORKG structured contribution properties was rated as moderate, with an average rating of 2.65 out of 5 (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2405.02105v1#S4.F11" title="Figure 11 ‣ 4.4 Human assessment survey ‣ 4 Results and Discussion ‣ Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>). This indicates that while there is some similarity between the two, there is room for alignment. As such participants raised concerns about the specificity of generated dimensions potentially diverging from the actual goal of the paper. For the final question on the release of such an LLM-based feature, the respondents emphasized the importance of aligning LLMs based on the domain expert property names while allowing descriptions to be generated, ensuring relevance across different research domains and capturing specific details like measurement values and units.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="266" id="S4.F8.g1" src="extracted/5575392/img/s1q1.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Count of the number of relevant LLM-generated research dimensions to structure a paper’s contribution or create a structured science summary.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="290" id="S4.F9.g1" src="extracted/5575392/img/s1q2.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Willingness of the participants to make changes to their existing annotated ORKG properties when shown the LLM-generated research dimensions. </figcaption>
</figure>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="293" id="S4.F10.g1" src="extracted/5575392/img/s1q3.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Utility of LLM-suggested dimensions as suggestions</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="262" id="S4.F11.g1" src="extracted/5575392/img/s2q5.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Alignment of LLM-generated dimensions to ORKG structured contributions</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Overall, the findings of the survey indicate that LLM-generated dimensions exhibited a moderate alignment with manually extracted properties. Although the generated properties did not perfectly align with the original contributions, they still provided valuable suggestions that authors found potentially helpful in various aspects of creating structured contributions for ORKG. For instance, the suggestions were deemed useful in facilitating the creation of comparisons, identifying relevant properties, and providing a starting point for further refinement. However, concerns regarding the specificity and alignment of the generated properties with research goals were noted, suggesting areas for further refinement. These concerns highlight the need for LLMs to better capture the nuances of research goals and objectives in order to generate more targeted and relevant suggestions. Nonetheless, the overall positive feedback from participants suggests that AI tools, such as LLMs, hold promise in assisting users in creating structured research contributions and comparisons within the ORKG platform.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this study, we investigated the performance of state-of-the-art Large Language Models (LLMs) in recommending research dimensions, aiming to address the central research question: How effectively do LLMs perform on the task of recommending research dimensions? Through a series of evaluations, including semantic alignment and deviation assessments, property and research dimension mappings, embeddings-based evaluations, and a human assessment survey, we sought to provide insights into the capabilities and limitations of LLMs in this domain.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The findings of our study elucidated several key aspects of LLM performance in recommending research dimensions. First, our semantic alignment and deviation assessments revealed a moderate level of alignment between manually curated ORKG properties and LLM-generated research dimensions, accompanied by a higher degree of deviation. While LLMs demonstrate some capacity to capture semantic similarities, there are notable differences between the concepts of structured paper properties and research dimensions. This suggests that LLMs may not fully emulate the nuanced inclinations of domain experts when structuring contributions.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Second, our property and research dimension mappings analysis indicated a low number of mappings between paper properties and research dimensions. While LLMs can generate a more diverse set of research dimensions than the ORKG properties, the degree of similarity is lower, highlighting the challenges in aligning LLM-generated dimensions with human expert curated properties.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Third, our embeddings-based evaluations showed that GPT-3.5 achieved the highest semantic similarity between ORKG properties and LLM-generated research dimensions, outperforming Mistral and Llama 2 in that order.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">Fourth and finally, our human assessment survey provided valuable feedback from domain experts, indicating a moderate alignment between LLM-generated dimensions and manually annotated properties. While the suggestions provided by LLMs were deemed potentially helpful in various aspects of creating structured contributions, concerns regarding specificity and alignment with research goals were noted, suggesting areas for improvement.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">In conclusion, our study contributes to a deeper understanding of LLM performance in recommending research dimensions to create structured science summary representations in the ORKG. While LLMs show promise as tools for automated research metadata creation and the retrieval of related work, further development is necessary to enhance their accuracy and relevance in this domain. Future research may explore the fine-tuning of LLMs on scientific domains to improve their performance in recommending research dimensions.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">CRediT Author Contributions</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Author contributions are as follows: Conceptualization, Jennifer D’Souza and Steffen Eger; methodology, Vladyslav Nechakhin; validation, Vladyslav Nechakhin; investigation, Vladyslav Nechakhin and Jennifer D’Souza; resources, Vladyslav Nechakhin and Jennifer D’Souza; data curation, Vladyslav Nechakhin; writing—original draft preparation, Vladyslav Nechakhin and Jennifer D’Souza.; writing—review and editing, Jennifer D’Souza and Steffen Eger; visualization, Vladyslav Nechakhin; supervision, Jennifer D’Souza and Steffen Eger; project administration, Jennifer D’Souza; funding acquisition, Jennifer D’Souza. All authors have read and agreed to the published version of the manuscript.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Funding</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported by the German BMBF project SCINEXT (ID 01lS22070), the European Research Council for ScienceGRAPH (GA ID: 819536), and German DFG for NFDI4DataScience (no. 460234259).</p>
</div>
</section>
<section class="ltx_section" id="Sx3">
<h2 class="ltx_title ltx_title_section">Data Availability</h2>
<div class="ltx_para" id="Sx3.p1">
<p class="ltx_p" id="Sx3.p1.1">The evaluation dataset created in this paper is publicly accessible at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.25835/6oyn9d1n" title="">https://doi.org/10.25835/6oyn9d1n</a>.</p>
</div>
</section>
<section class="ltx_section" id="Sx4">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx4.p1">
<p class="ltx_p" id="Sx4.p1.1">We thank all members of the ORKG Team for their dedication in creating and maintaining the ORKG platform. Furthermore, we thank all the participants of our survey for providing their insightful feedback and responses.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Introducing chatgpt. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>, accessed: 2024-04-23

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Open llm leaderboard. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" title="">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>, accessed: 2024-04-23

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Antu, S.A., Chen, H., Richards, C.K.: Using llm (large language model) to improve efficiency in literature review for undergraduate research (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Arab Oghli, O., D’Souza, J., Auer, S.: Clustering semantic predicates in the open research knowledge graph. In: International Conference on Asian Digital Libraries. pp. 477–484. Springer (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Auer, S., Oelen, A., Haris, M., Stocker, M., D’Souza, J., Farfar, K.E., Vogt, L., Prinz, M., Wiens, V., Jaradeh, M.Y.: Improving access to scientific literature with knowledge graphs. Bibliothek Forschung und Praxis <span class="ltx_text ltx_font_bold" id="bib.bib6.1.1">44</span>(3), 516–529 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Banerjee, D., Singh, P., Avadhanam, A., Srivastava, S.: Benchmarking llm powered chatbots: methods and metrics. arXiv preprint arXiv:2308.04624 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Beltagy, I., Lo, K., Cohan, A.: Scibert: A pretrained language model for scientific text. arXiv preprint arXiv:1903.10676 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib9.1.1">33</span>, 1877–1901 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Cai, H., Cai, X., Chang, J., Li, S., Yao, L., Wang, C., Gao, Z., Li, Y., Lin, M., Yang, S., et al.: Sciassess: Benchmarking llm proficiency in scientific literature analysis. arXiv preprint arXiv:2403.01976 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Cohan, A., Feldman, S., Beltagy, I., Downey, D., Weld, D.S.: Specter: Document-level representation learning using citation-informed transformers. arXiv preprint arXiv:2004.07180 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Ferdous, R., et al.: An efficient k-means algorithm integrated with jaccard distance measure for document clustering. In: 2009 first asian himalayas international conference on internet. pp. 1–6. IEEE (2009)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Harnad, S.: Language writ large: Llms, chatgpt, grounding, meaning and understanding. arXiv preprint arXiv:2402.02243 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas, D.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv preprint arXiv:2310.06825 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Jin, H., Zhang, Y., Meng, D., Wang, J., Tan, J.: A comprehensive survey on process-oriented automatic text summarization with exploration of llm-based methods. arXiv preprint arXiv:2403.02901 (2024)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Karanikolas, N., Manga, E., Samaridi, N., Tousidou, E., Vassilakopoulos, M.: Large language models versus natural language understanding and generation. In: Proceedings of the 27th Pan-Hellenic Conference on Progress in Computing and Informatics. pp. 278–290 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Kocmi, T., Federmann, C.: Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Latif, E., Fang, L., Ma, P., Zhai, X.: Knowledge distillation of llm for education. arXiv preprint arXiv:2312.15842 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Leiter, C., Opitz, J., Deutsch, D., Gao, Y., Dror, R., Eger, S.: The eval4nlp 2023 shared task on prompting large language models as explainable metrics. arXiv preprint arXiv:2310.19792 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Liang, W., Zhang, Y., Cao, H., Wang, B., Ding, D., Yang, X., Vodrahalli, K., He, S., Smith, D., Yin, Y., et al.: Can large language models provide useful feedback on research papers? a large-scale empirical analysis. arXiv preprint arXiv:2310.01783 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Ostendorff, M., Rethmeier, N., Augenstein, I., Gipp, B., Rehm, G.: Neighborhood contrastive learning for scientific document representations with citation embeddings. arXiv preprint arXiv:2202.06671 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
O’callaghan, D., Greene, D., Carthy, J., Cunningham, P.: An analysis of the coherence of descriptors in topic modeling. Expert Systems with Applications <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">42</span>(13), 5645–5657 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving language understanding by generative pre-training (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsupervised multitask learners. OpenAI blog <span class="ltx_text ltx_font_bold" id="bib.bib24.1.1">1</span>(8),  9 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Singhal, A., et al.: Modern information retrieval: A brief overview. IEEE Data Eng. Bull. <span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">24</span>(4), 35–43 (2001)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Thakkar, H., Manimaran, A.: Comprehensive examination of instruction-based language models: A comparative analysis of mistral-7b and llama-2-7b. In: 2023 International Conference on Emerging Research in Computational Science (ICERCS). pp. 1–6. IEEE (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Verma, V., Aggarwal, R.K.: A comparative analysis of similarity measures akin to the jaccard index in collaborative recommendations: empirical and theoretical perspective. Social Network Analysis and Mining <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">10</span>(1),  43 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib30.1.1">35</span>, 24824–24837 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yasunaga, M., Kasai, J., Zhang, R., Fabbri, A.R., Li, I., Friedman, D., Radev, D.R.: Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. In: Proceedings of the AAAI conference on artificial intelligence. vol. 33, pp. 7386–7393 (2019)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May  3 14:00:55 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
