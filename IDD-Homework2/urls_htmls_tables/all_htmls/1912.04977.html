<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1912.04977] Advances and Open Problems in Federated Learning</title><meta property="og:description" content="Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), w…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Advances and Open Problems in Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Advances and Open Problems in Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1912.04977">

<!--Generated on Wed Mar 13 11:28:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Advances and Open Problems in Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Peter Kairouz<sup id="id85.2.id1" class="ltx_sup">7</sup><span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Peter Kairouz and H. Brendan McMahan conceived, coordinated, and edited this work. Correspondence to <a href="kairouz@google.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">kairouz@google.com</a> and <a href="mcmahan@google.com" title="" class="ltx_ref ltx_url ltx_font_typewriter">mcmahan@google.com</a>.</span></span></span>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">H. Brendan McMahan<sup id="id86.2.id1" class="ltx_sup"><span id="id86.2.id1.1" class="ltx_text ltx_font_italic">7∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Brendan Avent<sup id="id87.2.id1" class="ltx_sup"><span id="id87.2.id1.1" class="ltx_text ltx_font_italic">21</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aurélien Bellet<sup id="id88.2.id1" class="ltx_sup">9</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mehdi Bennis<sup id="id89.2.id1" class="ltx_sup"><span id="id89.2.id1.1" class="ltx_text ltx_font_italic">19</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arjun Nitin Bhagoji<sup id="id90.2.id1" class="ltx_sup"><span id="id90.2.id1.1" class="ltx_text ltx_font_italic">13</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kallista Bonawitz<sup id="id91.2.id1" class="ltx_sup"><span id="id91.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zachary Charles<sup id="id92.2.id1" class="ltx_sup"><span id="id92.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Graham Cormode<sup id="id93.2.id1" class="ltx_sup"><span id="id93.2.id1.1" class="ltx_text ltx_font_italic">23</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rachel Cummings<sup id="id94.2.id1" class="ltx_sup">6</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rafael G.L. D’Oliveira<sup id="id95.2.id1" class="ltx_sup"><span id="id95.2.id1.1" class="ltx_text ltx_font_italic">14</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hubert Eichner<sup id="id96.2.id1" class="ltx_sup"><span id="id96.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Salim El Rouayheb<sup id="id97.2.id1" class="ltx_sup"><span id="id97.2.id1.1" class="ltx_text ltx_font_italic">14</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Evans<sup id="id98.2.id1" class="ltx_sup"><span id="id98.2.id1.1" class="ltx_text ltx_font_italic">22</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Josh Gardner<sup id="id99.2.id1" class="ltx_sup"><span id="id99.2.id1.1" class="ltx_text ltx_font_italic">24</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zachary Garrett<sup id="id100.2.id1" class="ltx_sup">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adrià Gascón<sup id="id101.2.id1" class="ltx_sup">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Badih Ghazi<sup id="id102.2.id1" class="ltx_sup">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Phillip B. Gibbons<sup id="id103.2.id1" class="ltx_sup"><span id="id103.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marco Gruteser<sup id="id104.2.id1" class="ltx_sup"><span id="id104.2.id1.1" class="ltx_text ltx_font_italic">7,14</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zaid Harchaoui<sup id="id105.2.id1" class="ltx_sup"><span id="id105.2.id1.1" class="ltx_text ltx_font_italic">24</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chaoyang He<sup id="id106.2.id1" class="ltx_sup"><span id="id106.2.id1.1" class="ltx_text ltx_font_italic">21</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lie He <sup id="id107.2.id1" class="ltx_sup"><span id="id107.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhouyuan Huo <sup id="id108.2.id1" class="ltx_sup"><span id="id108.2.id1.1" class="ltx_text ltx_font_italic">20</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ben Hutchinson<sup id="id109.2.id1" class="ltx_sup"><span id="id109.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Justin Hsu<sup id="id110.2.id1" class="ltx_sup"><span id="id110.2.id1.1" class="ltx_text ltx_font_italic">25</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martin Jaggi<sup id="id111.2.id1" class="ltx_sup"><span id="id111.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tara Javidi<sup id="id112.2.id1" class="ltx_sup"><span id="id112.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gauri Joshi<sup id="id113.2.id1" class="ltx_sup"><span id="id113.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mikhail Khodak<sup id="id114.2.id1" class="ltx_sup"><span id="id114.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jakub Konečný<sup id="id115.2.id1" class="ltx_sup"><span id="id115.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aleksandra Korolova<sup id="id116.2.id1" class="ltx_sup"><span id="id116.2.id1.1" class="ltx_text ltx_font_italic">21</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Farinaz Koushanfar<sup id="id117.2.id1" class="ltx_sup"><span id="id117.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sanmi Koyejo<sup id="id118.2.id1" class="ltx_sup"><span id="id118.2.id1.1" class="ltx_text ltx_font_italic">7,18</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tancrède Lepoint<sup id="id119.2.id1" class="ltx_sup">7</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yang Liu<sup id="id120.2.id1" class="ltx_sup"><span id="id120.2.id1.1" class="ltx_text ltx_font_italic">12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Prateek Mittal<sup id="id121.2.id1" class="ltx_sup"><span id="id121.2.id1.1" class="ltx_text ltx_font_italic">13</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mehryar Mohri<sup id="id122.2.id1" class="ltx_sup"><span id="id122.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Richard Nock<sup id="id123.2.id1" class="ltx_sup"><span id="id123.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ayfer Özgür<sup id="id124.2.id1" class="ltx_sup"><span id="id124.2.id1.1" class="ltx_text ltx_font_italic">15</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rasmus Pagh<sup id="id125.2.id1" class="ltx_sup"><span id="id125.2.id1.1" class="ltx_text ltx_font_italic">7,10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hang Qi<sup id="id126.2.id1" class="ltx_sup"><span id="id126.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daniel Ramage<sup id="id127.2.id1" class="ltx_sup"><span id="id127.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ramesh Raskar<sup id="id128.2.id1" class="ltx_sup"><span id="id128.2.id1.1" class="ltx_text ltx_font_italic">11</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mariana Raykova<sup id="id129.2.id1" class="ltx_sup"><span id="id129.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dawn Song<sup id="id130.2.id1" class="ltx_sup"><span id="id130.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weikang Song<sup id="id131.2.id1" class="ltx_sup"><span id="id131.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sebastian U. Stich<sup id="id132.2.id1" class="ltx_sup"><span id="id132.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziteng Sun<sup id="id133.2.id1" class="ltx_sup"><span id="id133.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ananda Theertha Suresh<sup id="id134.2.id1" class="ltx_sup"><span id="id134.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Florian Tramèr<sup id="id135.2.id1" class="ltx_sup"><span id="id135.2.id1.1" class="ltx_text ltx_font_italic">15</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Praneeth Vepakomma<sup id="id136.2.id1" class="ltx_sup"><span id="id136.2.id1.1" class="ltx_text ltx_font_italic">11</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jianyu Wang<sup id="id137.2.id1" class="ltx_sup"><span id="id137.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Li Xiong<sup id="id138.2.id1" class="ltx_sup"><span id="id138.2.id1.1" class="ltx_text ltx_font_italic">5</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zheng Xu<sup id="id139.2.id1" class="ltx_sup"><span id="id139.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiang Yang<sup id="id140.2.id1" class="ltx_sup"><span id="id140.2.id1.1" class="ltx_text ltx_font_italic">8</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Felix X. Yu<sup id="id141.2.id1" class="ltx_sup"><span id="id141.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Han Yu<sup id="id142.2.id1" class="ltx_sup"><span id="id142.2.id1.1" class="ltx_text ltx_font_italic">12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sen Zhao<sup id="id143.2.id1" class="ltx_sup"><span id="id143.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"><sup id="id144.26.id1" class="ltx_sup"><span id="id144.26.id1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">1</span></sup><span id="id84.25.24" class="ltx_text" style="font-size:90%;">Australian National University, <sup id="id84.25.24.1" class="ltx_sup"><span id="id84.25.24.1.1" class="ltx_text ltx_font_italic">2</span></sup>Carnegie Mellon University, <sup id="id84.25.24.2" class="ltx_sup"><span id="id84.25.24.2.1" class="ltx_text ltx_font_italic">3</span></sup>Cornell University,
<br class="ltx_break"><sup id="id84.25.24.3" class="ltx_sup"><span id="id84.25.24.3.1" class="ltx_text ltx_font_italic">4</span></sup>École Polytechnique Fédérale de Lausanne, <sup id="id84.25.24.4" class="ltx_sup"><span id="id84.25.24.4.1" class="ltx_text ltx_font_italic">5</span></sup>Emory University, <sup id="id84.25.24.5" class="ltx_sup"><span id="id84.25.24.5.1" class="ltx_text ltx_font_italic">6</span></sup>Georgia Institute of Technology,
<br class="ltx_break"><sup id="id84.25.24.6" class="ltx_sup"><span id="id84.25.24.6.1" class="ltx_text ltx_font_italic">7</span></sup>Google Research, <sup id="id84.25.24.7" class="ltx_sup"><span id="id84.25.24.7.1" class="ltx_text ltx_font_italic">8</span></sup>Hong Kong University of Science and Technology, <sup id="id84.25.24.8" class="ltx_sup"><span id="id84.25.24.8.1" class="ltx_text ltx_font_italic">9</span></sup>INRIA, <sup id="id84.25.24.9" class="ltx_sup"><span id="id84.25.24.9.1" class="ltx_text ltx_font_italic">10</span></sup>IT University of Copenhagen,
<br class="ltx_break"><sup id="id84.25.24.10" class="ltx_sup"><span id="id84.25.24.10.1" class="ltx_text ltx_font_italic">11</span></sup>Massachusetts Institute of Technology, <sup id="id84.25.24.11" class="ltx_sup"><span id="id84.25.24.11.1" class="ltx_text ltx_font_italic">12</span></sup>Nanyang Technological University, <sup id="id84.25.24.12" class="ltx_sup"><span id="id84.25.24.12.1" class="ltx_text ltx_font_italic">13</span></sup>Princeton University,
<br class="ltx_break"><sup id="id84.25.24.13" class="ltx_sup"><span id="id84.25.24.13.1" class="ltx_text ltx_font_italic">14</span></sup>Rutgers University, <sup id="id84.25.24.14" class="ltx_sup"><span id="id84.25.24.14.1" class="ltx_text ltx_font_italic">15</span></sup>Stanford University, <sup id="id84.25.24.15" class="ltx_sup"><span id="id84.25.24.15.1" class="ltx_text ltx_font_italic">16</span></sup>University of California Berkeley,
<br class="ltx_break"><sup id="id84.25.24.16" class="ltx_sup"><span id="id84.25.24.16.1" class="ltx_text ltx_font_italic">17</span></sup> University of California San Diego, <sup id="id84.25.24.17" class="ltx_sup"><span id="id84.25.24.17.1" class="ltx_text ltx_font_italic">18</span></sup>University of Illinois Urbana-Champaign, <sup id="id84.25.24.18" class="ltx_sup"><span id="id84.25.24.18.1" class="ltx_text ltx_font_italic">19</span></sup>University of Oulu, 
<br class="ltx_break"><sup id="id84.25.24.19" class="ltx_sup"><span id="id84.25.24.19.1" class="ltx_text ltx_font_italic">20</span></sup>University of Pittsburgh, <sup id="id84.25.24.20" class="ltx_sup"><span id="id84.25.24.20.1" class="ltx_text ltx_font_italic">21</span></sup>University of Southern California, <sup id="id84.25.24.21" class="ltx_sup"><span id="id84.25.24.21.1" class="ltx_text ltx_font_italic">22</span></sup>University of Virginia,
<br class="ltx_break"><sup id="id84.25.24.22" class="ltx_sup"><span id="id84.25.24.22.1" class="ltx_text ltx_font_italic">23</span></sup>University of Warwick, <sup id="id84.25.24.23" class="ltx_sup"><span id="id84.25.24.23.1" class="ltx_text ltx_font_italic">24</span></sup>University of Washington, <sup id="id84.25.24.24" class="ltx_sup"><span id="id84.25.24.24.1" class="ltx_text ltx_font_italic">25</span></sup>University of Wisconsin–Madison
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id145.id1" class="ltx_p">Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<nav class="ltx_TOC ltx_list_toc ltx_toc_toc"><h6 class="ltx_title ltx_title_contents">Contents</h6>
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S1" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S1.SS1" title="In 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>The Cross-Device Federated Learning Setting</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S1.SS1.SSS1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1.1 </span>The Lifecycle of a Model in Federated Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S1.SS1.SSS2" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1.2 </span>A Typical Federated Training Process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS2" title="In 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Federated Learning Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S1.SS3" title="In 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Organization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S2" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S2.SS1" title="In 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Fully Decentralized / Peer-to-Peer Distributed Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS1.SSS1" title="In 2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Algorithmic Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S2.SS1.SSS2" title="In 2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Practical Challenges</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS2" title="In 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Cross-Silo Federated Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS3" title="In 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Split Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S2.SS4" title="In 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Executive summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S3" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Improving Efficiency and Effectiveness</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S3.SS1" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Non-IID Data in Federated Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS1.SSS1" title="In 3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Strategies for Dealing with Non-IID Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S3.SS2" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Optimization Algorithms for Federated Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS2.SSS1" title="In 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Optimization Algorithms and Convergence Rates for IID Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS2.SSS2" title="In 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Optimization Algorithms and Convergence Rates for Non-IID Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S3.SS3" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Multi-Task Learning, Personalization, and Meta-Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS3.SSS1" title="In 3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Personalization via Featurization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS3.SSS2" title="In 3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Multi-Task Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS3.SSS3" title="In 3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Local Fine Tuning and Meta-Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS3.SSS4" title="In 3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.4 </span>When is a Global FL-trained Model Better?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S3.SS4" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Adapting ML Workflows for Federated Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS1" title="In 3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Hyperparameter Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS2" title="In 3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Neural Architecture Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S3.SS4.SSS3" title="In 3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Debugging and Interpretability for FL</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS5" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Communication and Compression</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS6" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Application To More Types of Machine Learning Problems and Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S3.SS7" title="In 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Executive summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S4" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Preserving the Privacy of User Data</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS1" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Actors, Threat Models, and Privacy in Depth</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S4.SS2" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Tools and Technologies</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS2.SSS1" title="In 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Secure Computations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS2.SSS2" title="In 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Privacy-Preserving Disclosures</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS2.SSS3" title="In 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.3 </span>Verifiability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S4.SS3" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Protections Against External Malicious Actors</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS3.SSS1" title="In 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Auditing the Iterates and Final Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS3.SSS2" title="In 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Training with Central Differential Privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS3.SSS3" title="In 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Concealing the Iterates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS3.SSS4" title="In 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.4 </span>Repeated Analyses over Evolving Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS3.SSS5" title="In 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.5 </span>Preventing Model Theft and Misuse</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S4.SS4" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Protections Against an Adversarial Server</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS4.SSS1" title="In 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Challenges: Communication Channels, Sybil Attacks, and Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS4.SSS2" title="In 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Limitations of Existing Solutions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS4.SSS3" title="In 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Training with Distributed Differential Privacy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS4.SSS4" title="In 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.4 </span>Preserving Privacy While Training Sub-Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S4.SS5" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>User Perception</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS5.SSS1" title="In 4.5 User Perception ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Understanding Privacy Needs for Particular Analysis Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S4.SS5.SSS2" title="In 4.5 User Perception ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Behavioral Research to Elicit Privacy Preferences</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S4.SS6" title="In 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Executive Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S5" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Defending Against Attacks and Failures</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S5.SS1" title="In 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Adversarial Attacks on Model Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S5.SS1.SSS1" title="In 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Goals and Capabilities of an Adversary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S5.SS1.SSS2" title="In 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Model Update Poisoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S5.SS1.SSS3" title="In 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Data Poisoning Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S5.SS1.SSS4" title="In 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.4 </span>Inference-Time Evasion Attacks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S5.SS1.SSS5" title="In 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.5 </span>Defensive Capabilities from Privacy Guarantees</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS2" title="In 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Non-Malicious Failure Modes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS3" title="In 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Exploring the Tension between Privacy and Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S5.SS4" title="In 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Executive Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S6" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ensuring Fairness and Addressing Sources of Bias</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS1" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Bias in Training Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS2" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Fairness Without Access to Sensitive Attributes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS3" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Fairness, Privacy, and Robustness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS4" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Leveraging Federation to Improve Model Diversity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS5" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.5 </span>Federated Fairness: New Opportunities and Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S6.SS6" title="In 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.6 </span>Executive Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a href="#S7" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Addressing System Challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS1" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Platform Development and Deployment Challenges</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a href="#S7.SS2" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>System Induced Bias</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S7.SS2.SSS1" title="In 7.2 System Induced Bias ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Device Availability Profiles</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S7.SS2.SSS2" title="In 7.2 System Induced Bias ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Examples of System Induced Bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a href="#S7.SS2.SSS3" title="In 7.2 System Induced Bias ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Open Challenges in Quantifying and Mitigating System Induced Bias</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS3" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>System Parameter Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS4" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>On-Device Runtime</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS5" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.5 </span>The Cross-Silo Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a href="#S7.SS6" title="In 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.6 </span>Executive Summary</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a href="#S8" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Concluding Remarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a href="#A1" title="In Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Software and Datasets for Federated Learning</span></a></li>
</ol></nav>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. It embodies the principles of focused collection and data minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning. This area has received significant interest recently, both from research and applied perspectives. This paper describes the defining characteristics and challenges of the federated learning setting, highlights important practical constraints and considerations, and then enumerates a range of valuable research directions. The goals of this work are to highlight research problems that are of significant theoretical and practical interest, and to encourage research on problems that could have significant real-world impact.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The term <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">federated learning</em> was introduced in 2016 by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">McMahan et al.</span> [<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite>: “We term our approach Federated Learning, since the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server.” An unbalanced and non-IID (identically and independently distributed) data partitioning across a massive number of unreliable devices with limited communication bandwidth was introduced as the defining set of challenges.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Significant related work predates the introduction of the term federated learning. A longstanding goal pursued by many research communities (including cryptography, databases, and machine learning) is to analyze and learn from data distributed among many owners without exposing that data. Cryptographic methods for computing on encrypted data were developed starting in the early 1980s <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib396" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">396</span></a>, <a href="#bib.bib492" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">492</span></a>]</cite>, and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Agrawal and Srikant</span> [<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Vaidya et al.</span> [<a href="#bib.bib457" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">457</span></a>]</cite> are early examples of work that sought to learn from local data using a centralized server while preserving privacy.
Conversely, even since the introduction of the term federated learning, we are aware of no single work that directly addresses the full set of FL challenges. Thus, the term federated learning provides a convenient shorthand for a set of characteristics, constraints, and challenges that often co-occur in applied ML problems on decentralized data where privacy is paramount.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper originated at the Workshop on Federated Learning and Analytics held June 17–18th, 2019, hosted at Google’s Seattle office. During the course of this two-day event, the need for a broad paper surveying the many open challenges in the area of federated learning became clear.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>During the preparation of this work, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib301" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">301</span></a>]</cite> independently released an excellent but less comprehensive survey.</span></span></span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A key property of many of the problems discussed is that they are inherently interdisciplinary — solving them likely requires not just machine learning, but techniques from distributed optimization, cryptography, security, differential privacy, fairness, compressed sensing, systems, information theory, statistics, and more. Many of the hardest problems are at the intersections of these areas, and so we believe collaboration will be essential to ongoing progress. One of the goals of this work is to highlight the ways in which techniques from these fields can potentially be combined, raising both interesting possibilities as well as new challenges.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Since the term federated learning was initially introduced with an emphasis on mobile and edge device applications <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>, <a href="#bib.bib334" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">334</span></a>]</cite>, interest in applying FL to other applications has greatly increased, including some which might involve only a small number of relatively reliable clients, for example multiple organizations collaborating to train a model. We term these two federated learning settings “cross-device” and “cross-silo” respectively. Given these variations, we propose a somewhat broader definition of federated learning:</p>
<blockquote id="S1.p6.2" class="ltx_quote">
<p id="S1.p6.2.1" class="ltx_p"><em id="S1.p6.2.1.1" class="ltx_emph ltx_font_italic"><span id="S1.p6.2.1.1.1" class="ltx_text ltx_font_bold">Federated learning</span> is a machine learning setting where multiple entities (clients) collaborate in solving a machine learning problem, under the
coordination of a central server or service provider. Each client’s raw data is stored locally and not exchanged or transferred; instead, focused updates intended for immediate aggregation are used to achieve the learning objective.</em></p>
</blockquote>
<p id="S1.p6.3" class="ltx_p">Focused updates are updates narrowly scoped to contain the minimum information necessary for the specific learning task at hand; aggregation is performed as early as possible in the service of data minimization. We note that this definition distinguishes federated learning from fully decentralized (peer-to-peer) learning techniques as discussed in <a href="#S2.SS1" title="2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Although privacy-preserving data analysis has been studied for more than 50 years, only in the past decade have solutions been widely deployed at scale (e.g. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib177" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">177</span></a>, <a href="#bib.bib154" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">154</span></a>]</cite>). Cross-device federated learning and federated data analysis are now being applied in consumer digital products. Google makes extensive use of federated learning in the Gboard mobile keyboard <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib376" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">376</span></a>, <a href="#bib.bib222" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">222</span></a>, <a href="#bib.bib491" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">491</span></a>, <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">112</span></a>, <a href="#bib.bib383" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">383</span></a>]</cite>, as well as in features on Pixel phones <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> and in Android Messages <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib439" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">439</span></a>]</cite>. While Google has pioneered cross-device FL, interest in this setting is now much broader, for example: Apple is using cross-device FL in iOS 13 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>, for applications like the QuickType keyboard and the vocal classifier for “Hey Siri” <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>; doc.ai is developing cross-device FL solutions for medical research <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib149" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">149</span></a>]</cite>, and Snips has explored cross-device FL for hotword detection <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib298" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">298</span></a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Cross-silo applications have also been proposed or described in myriad domains including finance risk prediction for reinsurance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib476" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">476</span></a>]</cite>, pharmaceuticals discovery <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib179" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">179</span></a>]</cite>, electronic health records mining <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib184" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">184</span></a>]</cite>, medical data segmentation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib139" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite>, and smart manufacturing <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib354" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">354</span></a>]</cite>.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">The growing demand for federated learning technology has resulted in a number of tools and frameworks becoming available. These include TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, Federated AI Technology Enabler <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, PySyft <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib399" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">399</span></a>]</cite>, Leaf <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>, PaddleFL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> and Clara Training Framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">125</span></a>]</cite>;
more details in Appendix <a href="#A1" title="Appendix A Software and Datasets for Federated Learning ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>. Commercial data platforms incorporating federated learning are in development from established technology companies as well as smaller start-ups.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> contrasts both cross-device and cross-silo federated learning with traditional single-datacenter distributed learning across a range of axes. These characteristics establish many of the constraints that practical federated learning systems must typically satisfy, and hence serve to both motivate and inform the open challenges in federated learning. They will be discussed at length in the sections that follow.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">These two FL variants are called out as representative and important examples, but different FL settings may have different combinations of these characteristics. For the remainder of this paper, we consider the cross-device FL setting unless otherwise noted, though many of the problems apply to other FL settings as well. Section <a href="#S2" title="2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> specifically addresses some of the many other variations and applications.</p>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">Next, we consider cross-device federated learning in more detail, focusing on practical aspects common to a typical large-scale deployment of the technology; <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bonawitz et al.</span> [<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> provides even more detail for a particular production system, including a discussion of specific architectural choices and considerations.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>The Cross-Device Federated Learning Setting</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">This section takes an applied perspective, and unlike the previous section, does not attempt to be definitional. Rather, the goal is to describe some of the practical issues in cross-device FL and how they might fit into a broader machine learning development and deployment ecosystem. The hope is to provide useful context and motivation for the open problems that follow, as well as to aid researchers in estimating how straightforward it would be to deploy a particular new approach in a real-world system. We begin by sketching the lifecycle of a model before considering a FL training process.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.1" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.1.2" class="ltx_tr">
<td id="S1.T1.1.2.1" class="ltx_td ltx_align_top ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;"></td>
<td id="S1.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.2.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Datacenter <span id="S1.T1.1.2.2.1.1.1.1" class="ltx_text">distributed learning</span></span></span>
</span>
</td>
<td id="S1.T1.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.3.1.1" class="ltx_p" style="width:130.1pt;"><span id="S1.T1.1.2.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Cross-silo <span id="S1.T1.1.2.3.1.1.1.1" class="ltx_text">federated learning</span></span></span>
</span>
</td>
<td id="S1.T1.1.2.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.2.4.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.2.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Cross-device <span id="S1.T1.1.2.4.1.1.1.1" class="ltx_text">federated learning</span></span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.3" class="ltx_tr">
<td id="S1.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.3.1.1.1.1" class="ltx_text" style="font-size:90%;">Setting</span></span>
</span>
</td>
<td id="S1.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.3.2.1.1.1" class="ltx_text" style="font-size:90%;">Training a model on a large but “flat” dataset. Clients are compute nodes in a single cluster or datacenter.</span></span>
</span>
</td>
<td id="S1.T1.1.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.3.1.1" class="ltx_p" style="width:130.1pt;"><span id="S1.T1.1.3.3.1.1.1" class="ltx_text" style="font-size:90%;">Training a model on siloed data. Clients are different organizations (e.g. medical or financial) or geo-distributed datacenters.</span></span>
</span>
</td>
<td id="S1.T1.1.3.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.3.4.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.3.4.1.1.1" class="ltx_text" style="font-size:90%;">The clients are a very large number of mobile or IoT devices.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.4" class="ltx_tr">
<td id="S1.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.4.1.1.1.1" class="ltx_text" style="font-size:90%;">Data </span><span id="S1.T1.1.4.1.1.1.2" class="ltx_text" style="font-size:90%;">distribution</span></span>
</span>
</td>
<td id="S1.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.4.2.1.1.1" class="ltx_text" style="font-size:90%;">Data is centrally stored and can be shuffled and balanced across clients. Any client can read any part of the dataset.</span></span>
</span>
</td>
<td id="S1.T1.1.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.4.3.1.1" class="ltx_p" style="width:296.3pt;"><span id="S1.T1.1.4.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Data is generated locally and remains decentralized.</span><span id="S1.T1.1.4.3.1.1.2" class="ltx_text" style="font-size:90%;"> Each client stores its own data and cannot read the data of other clients. Data is not independently or identically distributed.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.5" class="ltx_tr">
<td id="S1.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Orchestration</span></span>
</span>
</td>
<td id="S1.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.5.2.1.1.1" class="ltx_text" style="font-size:90%;">Centrally orchestrated.</span></span>
</span>
</td>
<td id="S1.T1.1.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.5.3.1.1" class="ltx_p" style="width:296.3pt;"><span id="S1.T1.1.5.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">A central orchestration server/service organizes the training</span><span id="S1.T1.1.5.3.1.1.2" class="ltx_text" style="font-size:90%;">, but never sees raw data.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.6" class="ltx_tr">
<td id="S1.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.6.1.1.1.1" class="ltx_text" style="font-size:90%;">Wide-area </span><span id="S1.T1.1.6.1.1.1.2" class="ltx_text" style="font-size:90%;">communication</span></span>
</span>
</td>
<td id="S1.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.6.2.1.1.1" class="ltx_text" style="font-size:90%;">None (fully connected clients in one datacenter/cluster).</span></span>
</span>
</td>
<td id="S1.T1.1.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.6.3.1.1" class="ltx_p" style="width:296.3pt;"><span id="S1.T1.1.6.3.1.1.1" class="ltx_text" style="font-size:90%;">Typically a hub-and-spoke topology, with the hub representing a coordinating service provider (typically without data) and the spokes connecting to clients.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.7" class="ltx_tr">
<td id="S1.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.7.1.1.1.1" class="ltx_text" style="font-size:90%;">Data </span><span id="S1.T1.1.7.1.1.1.2" class="ltx_text" style="font-size:90%;">availability</span></span>
</span>
</td>
<td id="S1.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.2.1.1" class="ltx_p" style="width:252.9pt;"><span class="ltx_rule" style="width:43.4pt;height:0.4pt;position:relative; bottom:2.3pt;background:black;display:inline-block;"> </span><span id="S1.T1.1.7.2.1.1.1" class="ltx_text" style="font-size:90%;"> All clients are almost always available. </span><span class="ltx_rule ltx_align_center" style="width:43.4pt;height:0.4pt;position:relative; bottom:2.3pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S1.T1.1.7.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.7.3.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.7.3.1.1.1" class="ltx_text" style="font-size:90%;">Only a fraction of clients are available at any one time, often with diurnal or other variations.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.2.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Distribution scale</span></span>
</span>
</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.3.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Typically 1 - 1000 clients.</span></span>
</span>
</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.4.1.1" class="ltx_p" style="width:130.1pt;"><span id="S1.T1.1.1.4.1.1.1" class="ltx_text" style="font-size:90%;">Typically 2 - 100 clients.</span></span>
</span>
</td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.1.1.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Massively parallel, up to </span><math id="S1.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="10^{10}" display="inline"><semantics id="S1.T1.1.1.1.1.1.m1.1a"><msup id="S1.T1.1.1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.1.1.m1.1.1.cmml"><mn mathsize="90%" id="S1.T1.1.1.1.1.1.m1.1.1.2" xref="S1.T1.1.1.1.1.1.m1.1.1.2.cmml">10</mn><mn mathsize="90%" id="S1.T1.1.1.1.1.1.m1.1.1.3" xref="S1.T1.1.1.1.1.1.m1.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.1.1.m1.1b"><apply id="S1.T1.1.1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1.2">10</cn><cn type="integer" id="S1.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S1.T1.1.1.1.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.1.1.m1.1c">10^{10}</annotation></semantics></math><span id="S1.T1.1.1.1.1.1.2" class="ltx_text" style="font-size:90%;"> clients.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.8" class="ltx_tr">
<td id="S1.T1.1.8.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.8.1.1.1.1" class="ltx_text" style="font-size:90%;">Primary </span><span id="S1.T1.1.8.1.1.1.2" class="ltx_text" style="font-size:90%;">bottleneck</span></span>
</span>
</td>
<td id="S1.T1.1.8.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.8.2.1.1.1" class="ltx_text" style="font-size:90%;">Computation is more often the bottleneck in the datacenter, where very fast networks can be assumed.</span></span>
</span>
</td>
<td id="S1.T1.1.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.3.1.1" class="ltx_p" style="width:130.1pt;"><span id="S1.T1.1.8.3.1.1.1" class="ltx_text" style="font-size:90%;">Might be computation or communication.</span></span>
</span>
</td>
<td id="S1.T1.1.8.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.8.4.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.8.4.1.1.1" class="ltx_text" style="font-size:90%;">Communication is often the primary bottleneck, though it depends on the task. Generally, cross-device federated computations use wi-fi or slower connections.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.9" class="ltx_tr">
<td id="S1.T1.1.9.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.9.1.1.1.1" class="ltx_text" style="font-size:90%;">Addressability</span></span>
</span>
</td>
<td id="S1.T1.1.9.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.2.1.1" class="ltx_p" style="width:252.9pt;"><span id="S1.T1.1.9.2.1.1.1" class="ltx_text" style="font-size:90%;">Each client has an identity or name that allows the system to access it specifically.</span></span>
</span>
</td>
<td id="S1.T1.1.9.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.9.3.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.9.3.1.1.1" class="ltx_text" style="font-size:90%;">Clients cannot be indexed directly (i.e., no use of client identifiers).</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.10" class="ltx_tr">
<td id="S1.T1.1.10.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.10.1.1.1.1" class="ltx_text" style="font-size:90%;">Client </span><span id="S1.T1.1.10.1.1.1.2" class="ltx_text" style="font-size:90%;">statefulness</span></span>
</span>
</td>
<td id="S1.T1.1.10.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.2.1.1" class="ltx_p" style="width:252.9pt;"><span id="S1.T1.1.10.2.1.1.1" class="ltx_text" style="font-size:90%;">Stateful — each client may participate in each round of the computation, carrying state from round to round.</span></span>
</span>
</td>
<td id="S1.T1.1.10.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.10.3.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.10.3.1.1.1" class="ltx_text" style="font-size:90%;">Stateless — each client will likely participate only once in a task, so generally a fresh sample of never-before-seen clients in each round of computation is assumed.</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.11" class="ltx_tr">
<td id="S1.T1.1.11.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.11.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.11.1.1.1.1" class="ltx_text" style="font-size:90%;">Client </span><span id="S1.T1.1.11.1.1.1.2" class="ltx_text" style="font-size:90%;">reliability</span></span>
</span>
</td>
<td id="S1.T1.1.11.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;" colspan="2">
<span id="S1.T1.1.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.11.2.1.1" class="ltx_p" style="width:252.9pt;"><span class="ltx_rule" style="width:72.3pt;height:0.4pt;position:relative; bottom:2.3pt;background:black;display:inline-block;"> </span><span id="S1.T1.1.11.2.1.1.1" class="ltx_text" style="font-size:90%;"> Relatively few failures. </span><span class="ltx_rule ltx_align_center" style="width:72.3pt;height:0.4pt;position:relative; bottom:2.3pt;background:black;display:inline-block;"> </span></span>
</span>
</td>
<td id="S1.T1.1.11.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.11.3.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.11.3.1.1.1" class="ltx_text" style="font-size:90%;">Highly unreliable — 5% or more of the clients participating in a round of computation are expected to fail or drop out (e.g. because the device becomes ineligible when battery, network, or idleness requirements are violated).</span></span>
</span>
</td>
</tr>
<tr id="S1.T1.1.12" class="ltx_tr">
<td id="S1.T1.1.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.1.1.1" class="ltx_p" style="width:57.8pt;"><span id="S1.T1.1.12.1.1.1.1" class="ltx_text" style="font-size:90%;">Data partition axis</span></span>
</span>
</td>
<td id="S1.T1.1.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.2.1.1" class="ltx_p" style="width:115.6pt;"><span id="S1.T1.1.12.2.1.1.1" class="ltx_text" style="font-size:90%;">Data can be partitioned / re-partitioned arbitrarily across clients.</span></span>
</span>
</td>
<td id="S1.T1.1.12.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.3.1.1" class="ltx_p" style="width:130.1pt;"><span id="S1.T1.1.12.3.1.1.1" class="ltx_text" style="font-size:90%;">Partition is fixed. Could be example-partitioned (horizontal) or feature-partitioned (vertical).</span></span>
</span>
</td>
<td id="S1.T1.1.12.4" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span id="S1.T1.1.12.4.1" class="ltx_inline-block ltx_align_top">
<span id="S1.T1.1.12.4.1.1" class="ltx_p" style="width:159.0pt;"><span id="S1.T1.1.12.4.1.1.1" class="ltx_text" style="font-size:90%;">Fixed partitioning by example (horizontal).</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Typical characteristics of federated learning settings vs. distributed learning in the datacenter (e.g. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib150" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">150</span></a>]</cite>). Cross-device and cross-silo federated learning are two examples of FL domains, but are not intended to be exhaustive. The primary defining characteristics of FL are highlighted in bold, but the other characteristics are also critical in determining which techniques are applicable.</figcaption>
</figure>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p"><span id="S1.F1.1.1" class="ltx_text">
<span id="S1.F1.1.1.1" class="ltx_inline-block ltx_align_bottom"><img src="/html/1912.04977/assets/x1.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="222" alt="Refer to caption">
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The lifecycle of an FL-trained model and the various actors in a federated learning system. This figure is revisited in <a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a> from a threat models perspective.
</figcaption>
</figure>
<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.1 </span>The Lifecycle of a Model in Federated Learning</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p">The FL process is typically driven by a model engineer developing a model for a particular application. For example, a domain expert in natural language processing may develop a next word prediction model for use in a virtual keyboard. <a href="#S1.F1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows the primary components and actors. At a high level, a typical workflow is:</p>
</div>
<div id="S1.SS1.SSS1.p2" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Problem identification:</span> The model engineer identifies a problem to be solved with FL.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Client instrumentation:</span> If needed, the clients (e.g. an app running on mobile phones) are instrumented to store locally (with limits on time and quantity) the necessary training data. In many cases, the app already will have stored this data (e.g. a text messaging app must store text messages, a photo management app already stores photos). However, in some cases additional data or metadata might need to be maintained, e.g. user interaction data to provide labels for a supervised learning task.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Simulation prototyping (optional):</span> The model engineer may prototype model architectures and test learning hyperparameters in an FL simulation using a proxy dataset.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Federated model training: </span> Multiple federated training tasks are started to train different variations of the model, or use different optimization hyperparameters.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">(Federated) model evaluation:</span> After the tasks have trained sufficiently (typically a few days, see below), the models are analyzed and good candidates selected. Analysis may include metrics computed on standard datasets in the datacenter, or federated evaluation wherein the models are pushed to held-out clients for evaluation on local client data.</p>
</div>
</li>
<li id="S1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S1.I1.i6.p1" class="ltx_para">
<p id="S1.I1.i6.p1.1" class="ltx_p"><span id="S1.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Deployment:</span> Finally, once a good model is selected, it goes through a standard model launch process, including manual quality assurance, live A/B testing (usually by using the new model on some devices and the previous generation model on other devices to compare their in-vivo performance), and a staged rollout (so that poor behavior can be discovered and rolled back before affecting too many users). The specific launch process for a model is set by the owner of the application and is usually independent of how the model is trained. In other words, this step would apply equally to a model trained with federated learning or with a traditional datacenter approach.</p>
</div>
</li>
</ol>
</div>
<div id="S1.SS1.SSS1.p3" class="ltx_para">
<p id="S1.SS1.SSS1.p3.1" class="ltx_p">One of the primary practical challenges an FL system faces is making the above workflow as straightforward as possible, ideally approaching the ease-of-use achieved by ML systems for centralized training. While much of this paper concerns federated training specifically, there are many other components including federated analytics tasks like model evaluation and debugging. Improving these is the focus of <a href="#S3.SS4" title="3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>. For now, we consider in more detail the training of a single FL model (Step <a href="#S1.I1.i4" title="Item 4 ‣ 1.1.1 The Lifecycle of a Model in Federated Learning ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> above).</p>
</div>
<figure id="S1.T2" class="ltx_table">
<table id="S1.T2.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T2.2.2" class="ltx_tr">
<td id="S1.T2.2.2.3" class="ltx_td ltx_align_right ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">Total population size</td>
<td id="S1.T2.2.2.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S1.T2.1.1.1.m1.1" class="ltx_Math" alttext="10^{6}" display="inline"><semantics id="S1.T2.1.1.1.m1.1a"><msup id="S1.T2.1.1.1.m1.1.1" xref="S1.T2.1.1.1.m1.1.1.cmml"><mn id="S1.T2.1.1.1.m1.1.1.2" xref="S1.T2.1.1.1.m1.1.1.2.cmml">10</mn><mn id="S1.T2.1.1.1.m1.1.1.3" xref="S1.T2.1.1.1.m1.1.1.3.cmml">6</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.1.1.1.m1.1b"><apply id="S1.T2.1.1.1.m1.1.1.cmml" xref="S1.T2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T2.1.1.1.m1.1.1.1.cmml" xref="S1.T2.1.1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.T2.1.1.1.m1.1.1.2.cmml" xref="S1.T2.1.1.1.m1.1.1.2">10</cn><cn type="integer" id="S1.T2.1.1.1.m1.1.1.3.cmml" xref="S1.T2.1.1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.1.1.1.m1.1c">10^{6}</annotation></semantics></math>–<math id="S1.T2.2.2.2.m2.1" class="ltx_Math" alttext="10^{10}" display="inline"><semantics id="S1.T2.2.2.2.m2.1a"><msup id="S1.T2.2.2.2.m2.1.1" xref="S1.T2.2.2.2.m2.1.1.cmml"><mn id="S1.T2.2.2.2.m2.1.1.2" xref="S1.T2.2.2.2.m2.1.1.2.cmml">10</mn><mn id="S1.T2.2.2.2.m2.1.1.3" xref="S1.T2.2.2.2.m2.1.1.3.cmml">10</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.2.2.2.m2.1b"><apply id="S1.T2.2.2.2.m2.1.1.cmml" xref="S1.T2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S1.T2.2.2.2.m2.1.1.1.cmml" xref="S1.T2.2.2.2.m2.1.1">superscript</csymbol><cn type="integer" id="S1.T2.2.2.2.m2.1.1.2.cmml" xref="S1.T2.2.2.2.m2.1.1.2">10</cn><cn type="integer" id="S1.T2.2.2.2.m2.1.1.3.cmml" xref="S1.T2.2.2.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.2.2.2.m2.1c">10^{10}</annotation></semantics></math> devices</td>
</tr>
<tr id="S1.T2.4.5" class="ltx_tr">
<td id="S1.T2.4.5.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;">Devices selected for one round of training</td>
<td id="S1.T2.4.5.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">50 – 5000</td>
</tr>
<tr id="S1.T2.4.4" class="ltx_tr">
<td id="S1.T2.4.4.3" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;">Total devices that participate in training one model</td>
<td id="S1.T2.4.4.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">
<math id="S1.T2.3.3.1.m1.1" class="ltx_Math" alttext="10^{5}" display="inline"><semantics id="S1.T2.3.3.1.m1.1a"><msup id="S1.T2.3.3.1.m1.1.1" xref="S1.T2.3.3.1.m1.1.1.cmml"><mn id="S1.T2.3.3.1.m1.1.1.2" xref="S1.T2.3.3.1.m1.1.1.2.cmml">10</mn><mn id="S1.T2.3.3.1.m1.1.1.3" xref="S1.T2.3.3.1.m1.1.1.3.cmml">5</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.3.3.1.m1.1b"><apply id="S1.T2.3.3.1.m1.1.1.cmml" xref="S1.T2.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.T2.3.3.1.m1.1.1.1.cmml" xref="S1.T2.3.3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.T2.3.3.1.m1.1.1.2.cmml" xref="S1.T2.3.3.1.m1.1.1.2">10</cn><cn type="integer" id="S1.T2.3.3.1.m1.1.1.3.cmml" xref="S1.T2.3.3.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.3.3.1.m1.1c">10^{5}</annotation></semantics></math>–<math id="S1.T2.4.4.2.m2.1" class="ltx_Math" alttext="10^{7}" display="inline"><semantics id="S1.T2.4.4.2.m2.1a"><msup id="S1.T2.4.4.2.m2.1.1" xref="S1.T2.4.4.2.m2.1.1.cmml"><mn id="S1.T2.4.4.2.m2.1.1.2" xref="S1.T2.4.4.2.m2.1.1.2.cmml">10</mn><mn id="S1.T2.4.4.2.m2.1.1.3" xref="S1.T2.4.4.2.m2.1.1.3.cmml">7</mn></msup><annotation-xml encoding="MathML-Content" id="S1.T2.4.4.2.m2.1b"><apply id="S1.T2.4.4.2.m2.1.1.cmml" xref="S1.T2.4.4.2.m2.1.1"><csymbol cd="ambiguous" id="S1.T2.4.4.2.m2.1.1.1.cmml" xref="S1.T2.4.4.2.m2.1.1">superscript</csymbol><cn type="integer" id="S1.T2.4.4.2.m2.1.1.2.cmml" xref="S1.T2.4.4.2.m2.1.1.2">10</cn><cn type="integer" id="S1.T2.4.4.2.m2.1.1.3.cmml" xref="S1.T2.4.4.2.m2.1.1.3">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T2.4.4.2.m2.1c">10^{7}</annotation></semantics></math>
</td>
</tr>
<tr id="S1.T2.4.6" class="ltx_tr">
<td id="S1.T2.4.6.1" class="ltx_td ltx_align_right" style="padding-top:1pt;padding-bottom:1pt;">Number of rounds for model convergence</td>
<td id="S1.T2.4.6.2" class="ltx_td ltx_align_left" style="padding-top:1pt;padding-bottom:1pt;">500 – 10000</td>
</tr>
<tr id="S1.T2.4.7" class="ltx_tr">
<td id="S1.T2.4.7.1" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">Wall-clock training time</td>
<td id="S1.T2.4.7.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">1 – 10 days</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Order-of-magnitude sizes for typical cross-device federated learning applications.</figcaption>
</figure>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">1.1.2 </span>A Typical Federated Training Process</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p">We now consider a template for FL training that encompasses the Federated Averaging algorithm of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">McMahan et al.</span> [<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite> and many others; again, variations are possible, but this gives a common starting point.</p>
</div>
<div id="S1.SS1.SSS2.p2" class="ltx_para">
<p id="S1.SS1.SSS2.p2.1" class="ltx_p">A server (service provider) orchestrates the training process, by repeating the following steps until training is stopped (at the discretion of the model engineer who is monitoring the training process):</p>
<ol id="S1.I2" class="ltx_enumerate">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p"><span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Client selection:</span> The server samples from a set of clients meeting eligibility requirements. For example, mobile phones might only check in to the server if they are plugged in, on an unmetered wi-fi connection, and idle, in order to avoid impacting the user of the device.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p"><span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Broadcast:</span> The selected clients download the current model weights and a training program (e.g. a TensorFlow graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>) from the server.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p"><span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Client computation:</span> Each selected device locally computes an update to the model by executing the training program, which might for example run SGD on the local data (as in Federated Averaging).</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p"><span id="S1.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Aggregation:</span> The server collects an aggregate of the device updates. For efficiency, stragglers might be dropped at this point once a sufficient number of devices have reported results. This stage is also the integration point for many other techniques which will be discussed later, possibly including: secure aggregation for added privacy, lossy compression of aggregates for communication efficiency, and noise addition and update clipping for differential privacy.</p>
</div>
</li>
<li id="S1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S1.I2.i5.p1" class="ltx_para">
<p id="S1.I2.i5.p1.1" class="ltx_p"><span id="S1.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">Model update:</span> The server locally updates the shared model based on the aggregated update computed from the clients that participated in the current round.</p>
</div>
</li>
</ol>
</div>
<div id="S1.SS1.SSS2.p3" class="ltx_para">
<p id="S1.SS1.SSS2.p3.1" class="ltx_p">Table <a href="#S1.T2" title="Table 2 ‣ 1.1.1 The Lifecycle of a Model in Federated Learning ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> gives typical order-of-magnitude sizes for the quantities involved in a typical federated learning application on mobile devices.</p>
</div>
<div id="S1.SS1.SSS2.p4" class="ltx_para">
<p id="S1.SS1.SSS2.p4.1" class="ltx_p">The separation of the client computation, aggregation, and model update phases is not a strict requirement of federated learning, and it indeed excludes certain classes of algorithms, for example asynchronous SGD where each client’s update is immediately applied to the model, before any aggregation with updates from other clients. Such asynchronous approaches may simplify some aspects of system design, and also be beneficial from an optimization perspective (though this point can be debated). However, the approach presented above has a substantial advantage in affording a separation of concerns between different lines of research: advances in compression, differential privacy, and secure multi-party computation can be developed for standard primitives like computing sums or means over decentralized updates, and then composed with arbitrary optimization or analytics algorithms, so long as those algorithms are expressed in terms of aggregation primitives.</p>
</div>
<div id="S1.SS1.SSS2.p5" class="ltx_para">
<p id="S1.SS1.SSS2.p5.1" class="ltx_p">It is also worth emphasizing that in two respects, the FL training process should not impact the user experience. First, as outlined above, even though model parameters are typically sent to some devices during the broadcast phase of each round of federated training, these models are an ephemeral part of the training process, and not used to make “live” predictions shown to the user. This is crucial, because training ML models is challenging, and a misconfiguration of hyperparameters can produce a model that makes bad predictions. Instead, user-visible use of the model is deferred to a rollout process as detailed above in Step <a href="#S1.I1.i6" title="Item 6 ‣ 1.1.1 The Lifecycle of a Model in Federated Learning ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> of the model lifecycle. Second, the training itself is intended to be invisible to the user — as described under client selection, training does not slow the device or drain the battery because it only executes when the device is idle and connected to power. However, the limited availability these constraints introduce leads directly to open research challenges which will be discussed subsequently, such as semi-cyclic data availability and the potential for bias in client selection.</p>
</div>
</section>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Federated Learning Research</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">The remainder of this paper surveys many open problems that are motivated by the constraints and challenges of real-world federated learning settings, from training models on medical data from a hospital system to training using hundreds of millions of mobile devices. Needless to say, most researchers working on federated learning problems will likely not be deploying production FL systems, nor have access to fleets of millions of real-world devices. This leads to a key distinction between the practical settings that motivate the work and experiments conducted in simulation which provide evidence of the suitability of a given approach to the motivating problem.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">This makes FL research somewhat different than other ML fields from an experimental perspective, leading to additional considerations in conducting FL research. In particular, when highlighting open problems, we have attempted, when possible, to also indicate relevant performance metrics which can be measured in simulation, the characteristics of datasets which will make them more representative of real-world performance, etc. The need for simulation also has ramifications for the presentation of FL research. While not intended to be authoritative or absolute, we make the following modest suggestions for presenting FL research that addresses the open problems we describe:</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<ul id="S1.I3" class="ltx_itemize">
<li id="S1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i1.p1" class="ltx_para">
<p id="S1.I3.i1.p1.1" class="ltx_p">As shown in <a href="#S1.T1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, the FL setting can encompass a wide range of problems. Compared to fields where the setting and goals are well-established, it is important to precisely describe the details of the particular FL setting of interest, particularly when the proposed approach makes assumptions that may not be appropriate in all settings (e.g. stateful clients that participate in all rounds).</p>
</div>
</li>
<li id="S1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i2.p1" class="ltx_para">
<p id="S1.I3.i2.p1.1" class="ltx_p">Of course, details of any simulations should be presented in order to make the research reproducible. But it is also important to explain which aspects of the real-world setting the simulation is designed to capture (and which it is not), in order to effectively make the case that success on the simulated problem implies useful progress on the real-world objective. We hope that the guidance in this paper will help with this.</p>
</div>
</li>
<li id="S1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I3.i3.p1" class="ltx_para">
<p id="S1.I3.i3.p1.1" class="ltx_p">Privacy and communication efficiency are always first-order concerns in FL, even if the experiments are simulations running on a single machine using public data. More so than with other types of ML, for any proposed approach it is important to be unambiguous about <em id="S1.I3.i3.p1.1.1" class="ltx_emph ltx_font_italic">where computation happens</em> as well as <em id="S1.I3.i3.p1.1.2" class="ltx_emph ltx_font_italic">what is communicated</em>.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p">Software libraries for federated learning simulation as well as standard datasets can help ease the challenges of conducting effective FL research; <a href="#A1" title="Appendix A Software and Datasets for Federated Learning ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a> summarizes some of the currently available options. Developing standard evaluation metrics and establishing standard benchmark datasets for different federated learning settings (cross-device and cross-silo) remain highly important directions for ongoing work.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Organization</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p"><a href="#S2" title="2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2</span></a> builds on the ideas in <a href="#S1.T1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, exploring other FL settings and problems beyond the original focus on cross-device settings. <a href="#S3" title="3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3</span></a> then turns to core questions around improving the efficiency and effectiveness of federated learning.
<a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a> undertakes a careful consideration of threat models and considers a range of technologies toward the goal of achieving rigorous privacy protections. As with all machine learning systems, in federated learning applications there may be incentives to manipulate the models being trained, and failures of various kinds are inevitable; these challenges are discussed in <a href="#S5" title="5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5</span></a>. Finally, we address the important challenges of providing fair and unbiased models in <a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we will discuss areas of research related to the topics discussed in the previous section. Even though not being the main focus of the remainder of the paper, progress in these areas could motivate design of the next generation of production systems.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Fully Decentralized / Peer-to-Peer Distributed Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In federated learning, a central server orchestrates the training process and receives the contributions of all clients. The server is thus a central player which also potentially represents a single point of failure. While large companies or organizations can play this role in some application scenarios, a reliable and powerful central server may not always be available or desirable in more collaborative learning scenarios <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib459" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">459</span></a>]</cite>. Furthermore, the server may even become a bottleneck when the number of clients is very large, as demonstrated by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Lian et al.</span> [<a href="#bib.bib305" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">305</span></a>]</cite> (though this can be mitigated by careful system design, e.g. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The key idea of fully decentralized learning is to replace communication with the server by peer-to-peer communication between individual clients. The communication topology is represented as a connected graph in which nodes are the clients and an edge indicates a communication channel between two clients. The network graph is typically chosen to be sparse with small maximum degree so that each node only needs to send/receive messages to/from a small number of peers; this is in contrast to the star graph of the server-client architecture. In fully decentralized algorithms, a round corresponds to each client performing a local update and exchanging information with their neighbors in the graph<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note, however, that the notion of a round does not need to even make sense in this setting. See for instance the discussion on clock models in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib85" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">85</span></a>]</cite>.</span></span></span>. In the context of machine learning, the local update is typically a local (stochastic) gradient step and the communication consists in averaging one’s local model parameters with the neighbors. Note that there is no longer a global state of the model as in standard federated learning, but the process can be designed such that all local models converge to the desired global solution, i.e., the individual models gradually reach consensus. While multi-agent optimization has a long history in the control community, fully decentralized variants of SGD and other optimization algorithms have recently been considered in machine learning both for improved scalability in datacenters <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite> as well as for decentralized networks of devices <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib127" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">127</span></a>, <a href="#bib.bib459" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">459</span></a>, <a href="#bib.bib443" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">443</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib278" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">278</span></a>, <a href="#bib.bib291" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">291</span></a>, <a href="#bib.bib173" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">173</span></a>]</cite>. They consider undirected network graphs, although the case of directed networks (encoding unidirectional channels which may arise in real-world scenarios such as social networks or data markets) has also been studied in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib226" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">226</span></a>]</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">It is worth noting that even in the decentralized setting outlined above, a central authority may still be in charge of setting up the learning task. Consider for instance the following questions: Who decides what is the model to be trained in the decentralized setting? What algorithm to use? What hyperparameters? Who is responsible for debugging when something does not work as expected? A certain degree of trust of the participating clients in a central authority would still be needed to answer these questions. Alternatively, the decisions could be taken by the client who proposes the learning task, or collaboratively through a consensus scheme (see <a href="#S2.SS1.SSS2" title="2.1.2 Practical Challenges ‣ 2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1.2</span></a>).</p>
</div>
<figure id="S2.T3" class="ltx_table">
<table id="S2.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T3.1.1" class="ltx_tr">
<td id="S2.T3.1.1.1" class="ltx_td ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S2.T3.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.2.1.1" class="ltx_p"><span id="S2.T3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Federated learning</span></span>
</span>
</td>
<td id="S2.T3.1.1.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.1.3.1.1" class="ltx_p"><span id="S2.T3.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Fully decentralized</span><span id="S2.T3.1.1.3.1.1.2" class="ltx_text ltx_font_bold"> <span id="S2.T3.1.1.3.1.1.2.1" class="ltx_text">(peer-to-peer) learning</span></span></span>
</span>
</td>
</tr>
<tr id="S2.T3.1.2" class="ltx_tr">
<td id="S2.T3.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Orchestration</td>
<td id="S2.T3.1.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.2.2.1.1" class="ltx_p">A central orchestration server or service organizes the training, but never sees raw data.</span>
</span>
</td>
<td id="S2.T3.1.2.3" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.2.3.1.1" class="ltx_p">No centralized orchestration.</span>
</span>
</td>
</tr>
<tr id="S2.T3.1.3" class="ltx_tr">
<td id="S2.T3.1.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">Wide-area communication</td>
<td id="S2.T3.1.3.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.3.2.1.1" class="ltx_p">Typically a hub-and-spoke topology, with the hub representing a coordinating service provider (typically without data) and the spokes connecting to clients.</span>
</span>
</td>
<td id="S2.T3.1.3.3" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span id="S2.T3.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.1.3.3.1.1" class="ltx_p">Peer-to-peer topology, with a possibly dynamic connectivity graph.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>A comparison of the key distinctions between federated learning and fully decentralized learning. Note that as with FL, decentralized learning can be further divided into different use-cases, with distinctions similar to those made in <a href="#S1.T1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> comparing cross-silo and cross-device FL.</figcaption>
</figure>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.1" class="ltx_p"><a href="#S2.T3" title="In 2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> provides a comparison between federated and peer-to-peer learning. While the architectural assumptions of decentralized learning are distinct from those of federated learning, it can often be applied to similar problem domains, many of the same challenges arise, and there is significant overlap in the research communities. Thus, we consider decentralized learning in this paper as well; in this section challenges specific to the decentralized approach are explicitly considered, but many of the open problems in other sections also arise in the decentralized case.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Algorithmic Challenges</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">A large number of important algorithmic questions remain open on the topic of real-world usability of decentralized schemes for machine learning. Some questions are analogous to the special case of federated learning with a central server, and other challenges come as an additional side-effect of being fully decentralized or trust-less. We outline some particular areas in the following.</p>
</div>
<section id="S2.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Effect of network topology and asynchrony on decentralized SGD</h5>

<div id="S2.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px1.p1.1" class="ltx_p">Fully decentralized algorithms for learning should be robust to the limited availability of the clients (with clients temporarily unavailable, dropping out or joining during the execution) and limited reliability of the network (with possible message drops). While for the special case of generalized linear models, schemes using the duality structure could enable some of these desired robustness properties <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib231" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">231</span></a>]</cite>, for the case of deep learning and SGD this remains an open question.
When the network graph is complete but messages have a fixed probability to be dropped, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib498" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">498</span></a>]</cite> show that one can achieve convergence rates that are comparable to the case of a reliable network.
Additional open research questions concern non-IID data distributions, update frequencies, efficient communication patterns and practical convergence time <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib443" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">443</span></a>]</cite>, as we outline in more detail below.</p>
</div>
<div id="S2.SS1.SSS1.Px1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.Px1.p2.1" class="ltx_p">Well-connected or denser networks encourage faster consensus and give better theoretical convergence rates, which depend on the spectral gap of the network graph. However, when data is IID, sparser topologies do not necessarily hurt the convergence in practice: this was analyzed theoretically in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib357" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">357</span></a>]</cite>. Denser networks typically incur communication delays which increase with the node degrees. Most of optimization-theory works do not explicitly consider how the topology affects the runtime, that is, wall-clock time required to complete each SGD iteration. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>]</cite> propose MATCHA, a decentralized SGD method based on matching decomposition sampling, that reduces the communication delay per iteration for any given node topology while maintaining the same error convergence speed. The key idea is to decompose the graph topology into matchings consisting of disjoint communication links that can operate in parallel, and carefully choose a subset of these matchings in each iteration. This sequence of subgraphs results in more frequent communication over connectivity-critical links (ensuring fast error convergence) and less frequent communication over other links (saving communication delays).</p>
</div>
<div id="S2.SS1.SSS1.Px1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.Px1.p3.1" class="ltx_p">The setting of decentralized SGD also naturally lends itself to asynchronous algorithms in which each client becomes active independently at random times, removing the need for global synchronization and potentially improving scalability <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib127" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">127</span></a>, <a href="#bib.bib459" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">459</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>, <a href="#bib.bib306" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">306</span></a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Local-update decentralized SGD</h5>

<div id="S2.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px2.p1.1" class="ltx_p">The theoretical analysis of schemes which perform several local update steps before a communication round is significantly more challenging than those using a single SGD step, as in mini-batch SGD. While this will also be discussed later in <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, the same also holds more generally in the fully decentralized setting of interest here. Schemes relying on a single local update step are typically proven to converge in the case of non-IID local datasets <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib278" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">278</span></a>, <a href="#bib.bib279" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">279</span></a>]</cite>. For the case with several local update steps, <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>, <a href="#bib.bib280" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">280</span></a>]</cite> recently provided convergence analysis. Further, <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>]</cite> provides a convergence analysis for the non-IID data case, but for the specific scheme based on matching decomposition sampling described above. In general, however, understanding the convergence under non-IID data distributions and how to design a model averaging policy that achieves the fastest convergence remains an open problem.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Personalization, and trust mechanisms</h5>

<div id="S2.SS1.SSS1.Px3.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px3.p1.1" class="ltx_p">Similarly to the cross-device FL setting, an important task for the fully decentralized scenario under the non-IID data distributions available to individual clients is to design algorithms for learning collections of personalized models. The work of <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib459" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">459</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite> introduces fully decentralized algorithms to collaboratively learn a personalized model for each client by smoothing model parameters across clients that have similar tasks (i.e., similar data distributions). <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Zantedeschi et al.</span> [<a href="#bib.bib504" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">504</span></a>]</cite> further learn the similarity graph together with the personalized models. One of the key unique challenges in the decentralized setting remains the robustness of such schemes to malicious actors or contribution of unreliable data or labels. The use of incentives or mechanism design in combination with decentralized learning is an emerging and important goal, which may be harder to achieve in the setting without a trusted central server.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Gradient compression and quantization methods</h5>

<div id="S2.SS1.SSS1.Px4.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px4.p1.1" class="ltx_p">In potential applications, the clients would often be limited in terms of communication bandwidth available and energy usage permitted. Translating and generalizing some of the existing compressed communication schemes from the centralized orchestrator-facilitated setting (see <a href="#S3.SS5" title="3.5 Communication and Compression ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>) to the fully decentralized setting, without negatively impacting the convergence is an active research direction <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib278" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">278</span></a>, <a href="#bib.bib391" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">391</span></a>, <a href="#bib.bib444" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">444</span></a>, <a href="#bib.bib279" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">279</span></a>]</cite>.
A complementary idea is to design decentralized optimization algorithms which naturally give rise to sparse updates <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib504" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">504</span></a>]</cite>.</p>
</div>
</section>
<section id="S2.SS1.SSS1.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy</h5>

<div id="S2.SS1.SSS1.Px5.p1" class="ltx_para">
<p id="S2.SS1.SSS1.Px5.p1.1" class="ltx_p">An important challenge in fully decentralized learning is to prevent any client from reconstructing the private data of another client from its shared updates while maintaining a good level of utility for the learned models. Differential privacy (see <a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>) is the standard approach to mitigate such privacy risks. In decentralized federated learning, this can be achieved by having each client add noise locally, as done in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib239" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">239</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>]</cite>. Unfortunately, such local privacy approaches often come at a large cost in utility. Furthermore, distributed methods based on secure aggregation or secure shuffling that are designed to improve the privacy-utility trade-off in the standard FL setting (see <a href="#S4.SS4.SSS3" title="4.4.3 Training with Distributed Differential Privacy ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4.3</span></a>) do not easily integrate with fully decentralized algorithms. A possible direction to achieve better trade-offs between privacy and utility in fully decentralized algorithms is to rely on decentralization itself to amplify differential privacy guarantees, for instance by considering appropriate relaxations of local differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">146</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Practical Challenges</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">An orthogonal question for fully decentralized learning is how it can be practically realized. This section outlines a family of related ideas based on the idea of a distributed ledger, but other approaches remain unexplored.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">A blockchain is a distributed ledger shared among disparate users, making possible digital transactions, including transactions of cryptocurrency, without a central authority. In particular, smart contracts allow execution of arbitrary code on top of the blockchain, essentially a massively replicated eventually-consistent state machine. In terms of federated learning, use of the technology could enable decentralization of the global server by using smart contracts to do model aggregation, where the participating clients executing the smart contracts could be different companies or cloud services.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p">However, on today’s blockchain platforms such as Ethereum <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib478" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">478</span></a>]</cite>, data on the blockchains is publicly available by default, this could discourage users from participating in the decentralized federated learning protocol, as the protection of the data is typically the primary motivating factor for FL. To address such concerns, it might be possible to modify the existing privacy-preserving techniques to fit into the scenario of decentralized federated learning. First of all, to prevent the participating nodes from exploiting individually submitted model updates, existing secure aggregation protocols could be used. A practical secure aggregation protocol already used in cross-device FL was proposed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bonawitz et al.</span> [<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>]</cite>, effectively handling dropping out participants at the cost of complexity of the protocol. An alternative system would be to have each client stake a deposit of cryptocurrency on blockchain, and get penalized if they drop out during the execution. Without the need of handling dropouts, the secure aggregation protocol could be significantly simplified. Another way of achieving secure aggregation is to use confidential smart contract such as what is enabled by the Oasis Protocol <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib119" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">119</span></a>]</cite> which runs inside secure enclaves. With this, each client could simply submit an encrypted local model update, knowing that the model will be decrypted and aggregated inside the secure hardware through remote attestation (though see discussion of privacy-in-depth in <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>).</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para">
<p id="S2.SS1.SSS2.p4.1" class="ltx_p">In order to prevent any client from trying to reconstruct the private data of another client by exploiting the global model, client-level differential privacy <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>]</cite> has been proposed for FL. Client-level differential privacy is achieved by adding random Gaussian noise on the aggregated global model that is enough to hide any single client’s update. In the context of blockchain, each client could locally add a certain amount of Gaussian noise after local gradient descent steps and submit the model to blockchain. The local noise scale should be calculated such that the aggregated noise on blockchain is able to achieve the same client-level differential privacy as in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>]</cite>. Finally, the aggregated global model on blockchain could be encrypted and only the participating clients hold the decryption key, which protects the model from the public.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Cross-Silo Federated Learning</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In contrast with the characteristics of cross-device federated learning, see <a href="#S1.T1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, cross-silo federated learning admits more flexibility in certain aspects of the overall design, but at the same time presents a setting where achieving other properties can be harder. This section discusses some of these differences.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The cross-silo setting can be relevant where a number of companies or organizations share incentive to train a model based on all of their data, but cannot share their data directly. This could be due to constraints imposed by confidentiality or due to legal constraints, or even within a single company when they cannot centralize their data between different geographical regions. These cross-silo applications have attracted substantial attention.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data partitioning</h5>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">In the cross-device setting the data is assumed to be partitioned by examples. In the cross-silo setting, in addition to partitioning by examples, partitioning by features is of practical relevance. An example could be when two companies in different businesses have the same or overlapping set of customers, such as a local bank and a local retail company in the same city. This difference has been also referred to as horizontal and vertical federated learning by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yang et al.</span> [<a href="#bib.bib490" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">490</span></a>]</cite>.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p2.1" class="ltx_p">Cross-silo FL with data partitioned by features, employs a very different training architecture compared to the setting with data partitioned by example. It may or may not involve a central server as a neutral party, and based on specifics of the training algorithm, clients exchange specific intermediate results rather than model parameters, to assist other parties’ gradient calculations; see for instance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib490" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">490</span></a>, Section 2.4.2]</cite>. In this setting, application of techniques such as secure multi-party computation or homomorphic encryption have been proposed in order to limit the amount of information other participants can infer from observing the training process. The downside of this approach is that the training algorithm is typically dependent on the type of machine learning objective being pursued. Currently proposed algorithms include trees <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib118" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">118</span></a>]</cite>, linear and logistic regression <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib490" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">490</span></a>, <a href="#bib.bib224" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">224</span></a>, <a href="#bib.bib316" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">316</span></a>]</cite>, and neural networks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib317" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">317</span></a>]</cite>. Local updates similar to Federated Averaging (see <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>) has been proposed to address the communication challenges of feature-partitioned systems <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib316" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">316</span></a>]</cite>, and <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib238" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">238</span></a>, <a href="#bib.bib318" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">318</span></a>]</cite> study the security and privacy related challenges inherent in such systems.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p3.1" class="ltx_p">Federated transfer learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib490" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">490</span></a>]</cite> is another concept that considers challenging scenarios in which data parties share only a partial overlap in the user space or the feature space, and leverage existing transfer learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib365" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">365</span></a>]</cite> to build models collaboratively. The existing formulation is limited to the case of <math id="S2.SS2.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S2.SS2.SSS0.Px1.p3.1.m1.1a"><mn id="S2.SS2.SSS0.Px1.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px1.p3.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p3.1.m1.1b"><cn type="integer" id="S2.SS2.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p3.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p3.1.m1.1c">2</annotation></semantics></math> clients.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p4.1" class="ltx_p">Partitioning by examples is usually relevant in cross-silo FL when a single company cannot centralize their data due to legal constraints, or when organizations with similar objectives want to collaboratively improve their models. For instance, different banks can collaboratively train classification or anomaly detection models for fraud detection <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib476" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">476</span></a>]</cite>, hospitals can build better diagnostic models <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib139" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">139</span></a>]</cite>, and so on.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p5" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p5.1" class="ltx_p">An open-source platform supporting the above outlined applications is currently available as <span id="S2.SS2.SSS0.Px1.p5.1.1" class="ltx_text ltx_font_italic">Federated AI Technology Enabler (FATE)</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>. At the same time, the IEEE P3652.1 Federated Machine Learning Working Group is focusing on standard-setting for the Federated AI Technology Framework. Other platforms include <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">125</span></a>]</cite> focused on a range of medical applications and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib321" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">321</span></a>]</cite> for enterprise use cases. See <a href="#A1" title="Appendix A Software and Datasets for Federated Learning ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a> for more details.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Incentive mechanisms</h5>

<div id="S2.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px2.p1.1" class="ltx_p">In addition to developing new algorithmic techniques for FL, incentive mechanism design for honest participation is an important practical research question. This need may arise in cross-device settings (e.g. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib261" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">261</span></a>, <a href="#bib.bib260" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">260</span></a>]</cite>), but is particularly relevant in the cross-silo setting, where participants may at the same time also be business competitors. The incentive can be in the form of monetary payout <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib499" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">499</span></a>]</cite> or final models with different levels of performance <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib324" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">324</span></a>]</cite>. The option to deliver models with performance commensurate to the contributions of each client is especially relevant in collaborative learning situations in which competitions exist among FL participants. Clients might worry that contributing their data to training federated learning models will benefit their competitors, who do not contribute as much but receive the same final model nonetheless (i.e. the free-rider problem). Related objectives include how to divide earnings generated by the federated learning model among contributing data owners in order to sustain long-term participation, and also how to link the incentives with decisions on defending against adversarial data owners to enhance system security, optimizing the participation of data owners to enhance system efficiency.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Differential privacy</h5>

<div id="S2.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p1.1" class="ltx_p">The discussion of actors and threat models in <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> is largely relevant also for the cross-silo FL. However, protecting against different actors might have different priorities. For example, in many practical scenarios, the final trained model would be released only to those who participate in the training, which makes the concerns about “the rest of the world” less important.</p>
</div>
<div id="S2.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px3.p2.1" class="ltx_p">On the other hand, for a practically persuasive claim, we would usually need a notion of local differential privacy, as the potential threat from other clients is likely to be more important. In cases when the clients are not considered a significant threat, each client could control the data from a number of their respective users, and a formal privacy guarantee might be needed on such user-level basis. Depending on application, other objectives could be worth pursuing. This area has not been systematically explored.</p>
</div>
</section>
<section id="S2.SS2.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Tensor factorization</h5>

<div id="S2.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px4.p1.1" class="ltx_p">Several works have also studied cross-silo federated tensor factorization where multiple sites (each having a set of data with the same feature, i.e. horizontally partitioned) jointly perform tensor factorization by only sharing intermediate factors with the coordination server while keeping data private at each site. Among the existing works, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib272" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">272</span></a>]</cite> used an alternating direction method of multipliers (ADMM) based approach and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib325" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">325</span></a>]</cite> improved the efficiency with the elastic averaging SGD (EASGD) algorithm and further ensures differential privacy for the intermediate factors.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Split Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In contrast with the previous settings which focus on data partitioning and communication patterns, the key idea behind split learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib215" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">215</span></a>, <a href="#bib.bib460" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">460</span></a>]</cite><span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>See also split learning project website - <a target="_blank" href="https://splitlearning.github.io/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://splitlearning.github.io/</a>.</span></span></span> is to split the execution of a model on a per-layer basis between the clients and the server. This can be done for both training and inference.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In the simplest configuration of split learning, each client computes the forward pass through a deep network up to a specific layer referred to as the <em id="S2.SS3.p2.1.1" class="ltx_emph ltx_font_italic">cut layer</em>. The outputs at the cut layer, referred to as <em id="S2.SS3.p2.1.2" class="ltx_emph ltx_font_italic">smashed data</em>, are sent to another entity (either the server or another client), which completes the rest of the computation. This completes a round of forward propagation without sharing the raw data. The gradients can then be back propagated from its last layer until the cut layer in a similar fashion. The gradients at the cut layer – and only these gradients – are sent back to the clients, where the rest of back propagation is completed. This process is continued until convergence, without having clients directly access each others raw data. This setup is shown in <a href="#S2.F2" title="In 2.3 Split Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>(a) and a variant of this setup where labels are also not shared along with raw data is shown in <a href="#S2.F2" title="In 2.3 Split Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>(b). Split learning approaches for data partitioned by features have been studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">101</span></a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1912.04977/assets/split_learning_vanilla.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_square" width="197" height="173" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Vanilla split learning</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1912.04977/assets/split_learning_U_shaped.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_square" width="197" height="168" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>U-shaped split learning</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Split learning configurations showing raw data is not transferred in the vanilla setting and that raw data as well as labels are not transferred between the client and server entities in the U-shaped split learning setting.</figcaption>
</figure>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">In several settings, the overall communication requirements of split learning and federated learning were compared in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib421" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">421</span></a>]</cite>. Split learning brings in another dimension of parallelism in the training, parallelization among parts of a model, e.g. client and server. The ideas in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib245" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">245</span></a>, <a href="#bib.bib240" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">240</span></a>]</cite>, where the authors break the dependencies between partial networks and reduced total centralized training time by parallelizing the computations in different parts, can be relevant here as well. However, it is still an open question to explore such parallelization of split learning on edge devices. Split learning also enables matching client-side model components with the best server-side model components for automating model selection as shown in the ExpertMatcher <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib413" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">413</span></a>]</cite>.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">The values communicated can nevertheless, in general, reveal information about the underlying data. How much, and whether this is acceptable, is likely going to be application and configuration specific. A variation of split learning called NoPeek SplitNN <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib462" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">462</span></a>]</cite> reduces the potential leakage via communicated activations, by reducing their distance correlation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib461" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">461</span></a>, <a href="#bib.bib442" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">442</span></a>]</cite> with the raw data, while maintaining good model performance via categorical cross-entropy. The key idea is to minimize the distance correlation between the raw data points and communicated smashed data. The objects communicated could otherwise contain information highly correlated with the input data if used without NoPeek SplitNN, the use of which also enables the split to be made relatively early-on given the decorrelation it provides. One other engineering driven approach to minimize the amount of information communicated in split learning has been via a specifically learnt pruning of channels present in the client side activations <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib422" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">422</span></a>]</cite>. Overall, much of the discussion in <a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a> is relevant here as well, and analysis providing formal privacy guarantees specifically for split learning is still an open problem.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Executive summary</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">The motivation for federated learning is relevant for a number of related areas of research.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Fully decentralized learning (<a href="#S2.SS1" title="2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a>) removes the need for a central server coordinating the overall computation. Apart from algorithmic challenges, open problems are in practical realization of the idea and in understanding of what form of trusted central authority is needed to set up the task.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Cross-silo federated learning (<a href="#S2.SS2" title="2.2 Cross-Silo Federated Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>) admits problems with different kinds of modelling constraints, such as data partitioned by examples and/or features, and faces different set of concerns when formulating formal privacy guarantees or incentive mechanisms for clients to participate.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Split learning (<a href="#S2.SS3" title="2.3 Split Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>) is an approach to partition the execution of a model between the clients and the server. It can deliver different options for overall communication constraints, but detailed analysis of when the communicated values reveal sensitive information is still missing.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Improving Efficiency and Effectiveness</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we explore a variety of techniques and open questions that address the challenge of making federated learning more efficient and effective. This encompasses a myriad of possible approaches, including: developing better optimization algorithms; providing different models to different clients; making ML tasks like hyperparameter search, architecture search, and debugging easier in the FL context; improving communication efficiency; and more.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">One of the fundamental challenges in addressing these goals is the presence of non-IID data, so we begin by surveying this issue and highlighting potential mitigations.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Non-IID Data in Federated Learning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">While the meaning of IID is generally clear, data can be non-IID in many ways. In this section, we provide a taxonomy of non-IID data regimes that may arise for any client-partitioned dataset. The most common sources of dependence and non-identicalness are due to each client corresponding to a particular user, a particular geographic location, and/or a particular time window. This taxonomy has a close mapping to notions of dataset shift <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib353" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">353</span></a>, <a href="#bib.bib380" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">380</span></a>]</cite>, which studies differences between the training distribution and testing distribution; here, we consider differences in the data distribution on each client.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">For the following, consider a supervised task with features <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">x</annotation></semantics></math> and labels <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">y</annotation></semantics></math>. A statistical model of federated learning involves two levels of sampling: accessing a datapoint requires first sampling a client <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="i\sim\mathcal{Q}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">i</mi><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">𝒬</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1">similar-to</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝑖</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">i\sim\mathcal{Q}</annotation></semantics></math>, the distribution over available clients, and then drawing an example <math id="S3.SS1.p2.4.m4.4" class="ltx_Math" alttext="(x,y)\sim\mathcal{P}_{i}(x,y)" display="inline"><semantics id="S3.SS1.p2.4.m4.4a"><mrow id="S3.SS1.p2.4.m4.4.5" xref="S3.SS1.p2.4.m4.4.5.cmml"><mrow id="S3.SS1.p2.4.m4.4.5.2.2" xref="S3.SS1.p2.4.m4.4.5.2.1.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.4.5.2.2.1" xref="S3.SS1.p2.4.m4.4.5.2.1.cmml">(</mo><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">x</mi><mo id="S3.SS1.p2.4.m4.4.5.2.2.2" xref="S3.SS1.p2.4.m4.4.5.2.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.p2.4.m4.4.5.2.2.3" xref="S3.SS1.p2.4.m4.4.5.2.1.cmml">)</mo></mrow><mo id="S3.SS1.p2.4.m4.4.5.1" xref="S3.SS1.p2.4.m4.4.5.1.cmml">∼</mo><mrow id="S3.SS1.p2.4.m4.4.5.3" xref="S3.SS1.p2.4.m4.4.5.3.cmml"><msub id="S3.SS1.p2.4.m4.4.5.3.2" xref="S3.SS1.p2.4.m4.4.5.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.4.m4.4.5.3.2.2" xref="S3.SS1.p2.4.m4.4.5.3.2.2.cmml">𝒫</mi><mi id="S3.SS1.p2.4.m4.4.5.3.2.3" xref="S3.SS1.p2.4.m4.4.5.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.p2.4.m4.4.5.3.1" xref="S3.SS1.p2.4.m4.4.5.3.1.cmml">​</mo><mrow id="S3.SS1.p2.4.m4.4.5.3.3.2" xref="S3.SS1.p2.4.m4.4.5.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.4.5.3.3.2.1" xref="S3.SS1.p2.4.m4.4.5.3.3.1.cmml">(</mo><mi id="S3.SS1.p2.4.m4.3.3" xref="S3.SS1.p2.4.m4.3.3.cmml">x</mi><mo id="S3.SS1.p2.4.m4.4.5.3.3.2.2" xref="S3.SS1.p2.4.m4.4.5.3.3.1.cmml">,</mo><mi id="S3.SS1.p2.4.m4.4.4" xref="S3.SS1.p2.4.m4.4.4.cmml">y</mi><mo stretchy="false" id="S3.SS1.p2.4.m4.4.5.3.3.2.3" xref="S3.SS1.p2.4.m4.4.5.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.4b"><apply id="S3.SS1.p2.4.m4.4.5.cmml" xref="S3.SS1.p2.4.m4.4.5"><csymbol cd="latexml" id="S3.SS1.p2.4.m4.4.5.1.cmml" xref="S3.SS1.p2.4.m4.4.5.1">similar-to</csymbol><interval closure="open" id="S3.SS1.p2.4.m4.4.5.2.1.cmml" xref="S3.SS1.p2.4.m4.4.5.2.2"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑥</ci><ci id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2">𝑦</ci></interval><apply id="S3.SS1.p2.4.m4.4.5.3.cmml" xref="S3.SS1.p2.4.m4.4.5.3"><times id="S3.SS1.p2.4.m4.4.5.3.1.cmml" xref="S3.SS1.p2.4.m4.4.5.3.1"></times><apply id="S3.SS1.p2.4.m4.4.5.3.2.cmml" xref="S3.SS1.p2.4.m4.4.5.3.2"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.4.5.3.2.1.cmml" xref="S3.SS1.p2.4.m4.4.5.3.2">subscript</csymbol><ci id="S3.SS1.p2.4.m4.4.5.3.2.2.cmml" xref="S3.SS1.p2.4.m4.4.5.3.2.2">𝒫</ci><ci id="S3.SS1.p2.4.m4.4.5.3.2.3.cmml" xref="S3.SS1.p2.4.m4.4.5.3.2.3">𝑖</ci></apply><interval closure="open" id="S3.SS1.p2.4.m4.4.5.3.3.1.cmml" xref="S3.SS1.p2.4.m4.4.5.3.3.2"><ci id="S3.SS1.p2.4.m4.3.3.cmml" xref="S3.SS1.p2.4.m4.3.3">𝑥</ci><ci id="S3.SS1.p2.4.m4.4.4.cmml" xref="S3.SS1.p2.4.m4.4.4">𝑦</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.4c">(x,y)\sim\mathcal{P}_{i}(x,y)</annotation></semantics></math> from that client’s local data distribution.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.6" class="ltx_p">When non-IID data in federated learning is referenced, this typically refers to differences between <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">𝒫</mi><mi id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝒫</ci><ci id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">\mathcal{P}_{i}</annotation></semantics></math> and <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{j}" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">𝒫</mi><mi id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝒫</ci><ci id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">\mathcal{P}_{j}</annotation></semantics></math> for different clients <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mi id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><ci id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">i</annotation></semantics></math> and <math id="S3.SS1.p3.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">j</annotation></semantics></math>. However, it is also important to note that the distribution <math id="S3.SS1.p3.5.m5.1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics id="S3.SS1.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.5.m5.1.1" xref="S3.SS1.p3.5.m5.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.5.m5.1b"><ci id="S3.SS1.p3.5.m5.1.1.cmml" xref="S3.SS1.p3.5.m5.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.5.m5.1c">\mathcal{Q}</annotation></semantics></math> and <math id="S3.SS1.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS1.p3.6.m6.1a"><msub id="S3.SS1.p3.6.m6.1.1" xref="S3.SS1.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p3.6.m6.1.1.2" xref="S3.SS1.p3.6.m6.1.1.2.cmml">𝒫</mi><mi id="S3.SS1.p3.6.m6.1.1.3" xref="S3.SS1.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.6.m6.1b"><apply id="S3.SS1.p3.6.m6.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.6.m6.1.1.1.cmml" xref="S3.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p3.6.m6.1.1.2.cmml" xref="S3.SS1.p3.6.m6.1.1.2">𝒫</ci><ci id="S3.SS1.p3.6.m6.1.1.3.cmml" xref="S3.SS1.p3.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.6.m6.1c">\mathcal{P}_{i}</annotation></semantics></math> may change over time, introducing another dimension of “non-IIDness”.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">For completeness, we note that even considering the dataset on a single device, if the data is in an insufficiently-random order, e.g. ordered by time, then independence is violated locally as well. For example, consecutive frames in a video are highly correlated. Sources of intra-client correlation can generally be resolved by local shuffling.</p>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Non-identical client distributions</h5>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.6" class="ltx_p">We first survey some common ways in which data tend to deviate from being identically distributed, that is <math id="S3.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="P_{i}\neq P_{j}" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">≠</mo><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><neq id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1"></neq><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">P_{i}\neq P_{j}</annotation></semantics></math> for different clients <math id="S3.SS1.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">i</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">j</annotation></semantics></math>. Rewriting <math id="S3.SS1.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="P_{i}(x,y)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS1.SSS0.Px1.p1.4.m4.2.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.cmml"><msub id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.1.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">x</mi><mo id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.2.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.2b"><apply id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3"><times id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.1"></times><apply id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.2.3">𝑖</ci></apply><interval closure="open" id="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.3.3.2"><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">𝑥</ci><ci id="S3.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.2.2">𝑦</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.2c">P_{i}(x,y)</annotation></semantics></math> as <math id="S3.SS1.SSS0.Px1.p1.5.m5.2" class="ltx_Math" alttext="P_{i}(y\,|\,x)P_{i}(x)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.2a"><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.cmml"><msub id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2a" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2.cmml">​</mo><msub id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2b" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.5.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.5.2.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.5.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.2b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2"><times id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.2"></times><apply id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.3.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2">𝑦</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3">𝑥</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.4.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.2c">P_{i}(y\,|\,x)P_{i}(x)</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px1.p1.6.m6.2" class="ltx_Math" alttext="P_{i}(x\,|\,y)P_{i}(y)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.2a"><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.2.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.cmml"><msub id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2.cmml">x</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2a" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2.cmml">​</mo><msub id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.2.cmml">P</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2b" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.5.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.5.2.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.5.2.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.2b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2"><times id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.2"></times><apply id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.3.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2">𝑥</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3">𝑦</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.2">𝑃</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.4.3">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.2c">P_{i}(x\,|\,y)P_{i}(y)</annotation></semantics></math> allows us to characterize the differences more precisely.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.2" class="ltx_p"><em id="S3.I1.i1.p1.2.1" class="ltx_emph ltx_font_italic">Feature distribution skew</em> (covariate shift): The marginal distributions <math id="S3.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}(x)" display="inline"><semantics id="S3.I1.i1.p1.1.m1.1a"><mrow id="S3.I1.i1.p1.1.m1.1.2" xref="S3.I1.i1.p1.1.m1.1.2.cmml"><msub id="S3.I1.i1.p1.1.m1.1.2.2" xref="S3.I1.i1.p1.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.2.2.2" xref="S3.I1.i1.p1.1.m1.1.2.2.2.cmml">𝒫</mi><mi id="S3.I1.i1.p1.1.m1.1.2.2.3" xref="S3.I1.i1.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.1.m1.1.2.1" xref="S3.I1.i1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.I1.i1.p1.1.m1.1.2.3.2" xref="S3.I1.i1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.I1.i1.p1.1.m1.1.2.3.2.1" xref="S3.I1.i1.p1.1.m1.1.2.cmml">(</mo><mi id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.I1.i1.p1.1.m1.1.2.3.2.2" xref="S3.I1.i1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2"><times id="S3.I1.i1.p1.1.m1.1.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.1"></times><apply id="S3.I1.i1.p1.1.m1.1.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.2.2.1.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.2.2.2.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2.2">𝒫</ci><ci id="S3.I1.i1.p1.1.m1.1.2.2.3.cmml" xref="S3.I1.i1.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\mathcal{P}_{i}(x)</annotation></semantics></math> may vary across clients, even if <math id="S3.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}(y\,|\,x)" display="inline"><semantics id="S3.I1.i1.p1.2.m2.1a"><mrow id="S3.I1.i1.p1.2.m2.1.1" xref="S3.I1.i1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.2.m2.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.I1.i1.p1.2.m2.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S3.I1.i1.p1.2.m2.1.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.I1.i1.p1.2.m2.1.1.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.I1.i1.p1.2.m2.1.1.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.I1.i1.p1.2.m2.1.1.1.1.1.2" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.I1.i1.p1.2.m2.1.1.1.1.1.1" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.I1.i1.p1.2.m2.1.1.1.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.I1.i1.p1.2.m2.1.1.1.1.3" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.2.m2.1b"><apply id="S3.I1.i1.p1.2.m2.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1"><times id="S3.I1.i1.p1.2.m2.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.2"></times><ci id="S3.I1.i1.p1.2.m2.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.3">𝒫</ci><apply id="S3.I1.i1.p1.2.m2.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.2">𝑦</ci><ci id="S3.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.I1.i1.p1.2.m2.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.2.m2.1c">\mathcal{P}(y\,|\,x)</annotation></semantics></math> is shared.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We write “<math id="footnote5.m1.1" class="ltx_Math" alttext="\mathcal{P}(y\,|\,x)" display="inline"><semantics id="footnote5.m1.1b"><mrow id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="footnote5.m1.1.1.3" xref="footnote5.m1.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="footnote5.m1.1.1.2" xref="footnote5.m1.1.1.2.cmml">​</mo><mrow id="footnote5.m1.1.1.1.1" xref="footnote5.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="footnote5.m1.1.1.1.1.2" xref="footnote5.m1.1.1.1.1.1.cmml">(</mo><mrow id="footnote5.m1.1.1.1.1.1" xref="footnote5.m1.1.1.1.1.1.cmml"><mi id="footnote5.m1.1.1.1.1.1.2" xref="footnote5.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="footnote5.m1.1.1.1.1.1.1" xref="footnote5.m1.1.1.1.1.1.1.cmml">|</mo><mi id="footnote5.m1.1.1.1.1.1.3" xref="footnote5.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="footnote5.m1.1.1.1.1.3" xref="footnote5.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote5.m1.1c"><apply id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1"><times id="footnote5.m1.1.1.2.cmml" xref="footnote5.m1.1.1.2"></times><ci id="footnote5.m1.1.1.3.cmml" xref="footnote5.m1.1.1.3">𝒫</ci><apply id="footnote5.m1.1.1.1.1.1.cmml" xref="footnote5.m1.1.1.1.1"><csymbol cd="latexml" id="footnote5.m1.1.1.1.1.1.1.cmml" xref="footnote5.m1.1.1.1.1.1.1">conditional</csymbol><ci id="footnote5.m1.1.1.1.1.1.2.cmml" xref="footnote5.m1.1.1.1.1.1.2">𝑦</ci><ci id="footnote5.m1.1.1.1.1.1.3.cmml" xref="footnote5.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.1d">\mathcal{P}(y\,|\,x)</annotation></semantics></math> is shared” as shorthand for <math id="footnote5.m2.2" class="ltx_Math" alttext="\mathcal{P}_{i}(y\,|\,x)=\mathcal{P}_{j}(y\,|\,x)" display="inline"><semantics id="footnote5.m2.2b"><mrow id="footnote5.m2.2.2" xref="footnote5.m2.2.2.cmml"><mrow id="footnote5.m2.1.1.1" xref="footnote5.m2.1.1.1.cmml"><msub id="footnote5.m2.1.1.1.3" xref="footnote5.m2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="footnote5.m2.1.1.1.3.2" xref="footnote5.m2.1.1.1.3.2.cmml">𝒫</mi><mi id="footnote5.m2.1.1.1.3.3" xref="footnote5.m2.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="footnote5.m2.1.1.1.2" xref="footnote5.m2.1.1.1.2.cmml">​</mo><mrow id="footnote5.m2.1.1.1.1.1" xref="footnote5.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="footnote5.m2.1.1.1.1.1.2" xref="footnote5.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="footnote5.m2.1.1.1.1.1.1" xref="footnote5.m2.1.1.1.1.1.1.cmml"><mi id="footnote5.m2.1.1.1.1.1.1.2" xref="footnote5.m2.1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="footnote5.m2.1.1.1.1.1.1.1" xref="footnote5.m2.1.1.1.1.1.1.1.cmml">|</mo><mi id="footnote5.m2.1.1.1.1.1.1.3" xref="footnote5.m2.1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="footnote5.m2.1.1.1.1.1.3" xref="footnote5.m2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="footnote5.m2.2.2.3" xref="footnote5.m2.2.2.3.cmml">=</mo><mrow id="footnote5.m2.2.2.2" xref="footnote5.m2.2.2.2.cmml"><msub id="footnote5.m2.2.2.2.3" xref="footnote5.m2.2.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="footnote5.m2.2.2.2.3.2" xref="footnote5.m2.2.2.2.3.2.cmml">𝒫</mi><mi id="footnote5.m2.2.2.2.3.3" xref="footnote5.m2.2.2.2.3.3.cmml">j</mi></msub><mo lspace="0em" rspace="0em" id="footnote5.m2.2.2.2.2" xref="footnote5.m2.2.2.2.2.cmml">​</mo><mrow id="footnote5.m2.2.2.2.1.1" xref="footnote5.m2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="footnote5.m2.2.2.2.1.1.2" xref="footnote5.m2.2.2.2.1.1.1.cmml">(</mo><mrow id="footnote5.m2.2.2.2.1.1.1" xref="footnote5.m2.2.2.2.1.1.1.cmml"><mi id="footnote5.m2.2.2.2.1.1.1.2" xref="footnote5.m2.2.2.2.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="footnote5.m2.2.2.2.1.1.1.1" xref="footnote5.m2.2.2.2.1.1.1.1.cmml">|</mo><mi id="footnote5.m2.2.2.2.1.1.1.3" xref="footnote5.m2.2.2.2.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="footnote5.m2.2.2.2.1.1.3" xref="footnote5.m2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="footnote5.m2.2c"><apply id="footnote5.m2.2.2.cmml" xref="footnote5.m2.2.2"><eq id="footnote5.m2.2.2.3.cmml" xref="footnote5.m2.2.2.3"></eq><apply id="footnote5.m2.1.1.1.cmml" xref="footnote5.m2.1.1.1"><times id="footnote5.m2.1.1.1.2.cmml" xref="footnote5.m2.1.1.1.2"></times><apply id="footnote5.m2.1.1.1.3.cmml" xref="footnote5.m2.1.1.1.3"><csymbol cd="ambiguous" id="footnote5.m2.1.1.1.3.1.cmml" xref="footnote5.m2.1.1.1.3">subscript</csymbol><ci id="footnote5.m2.1.1.1.3.2.cmml" xref="footnote5.m2.1.1.1.3.2">𝒫</ci><ci id="footnote5.m2.1.1.1.3.3.cmml" xref="footnote5.m2.1.1.1.3.3">𝑖</ci></apply><apply id="footnote5.m2.1.1.1.1.1.1.cmml" xref="footnote5.m2.1.1.1.1.1"><csymbol cd="latexml" id="footnote5.m2.1.1.1.1.1.1.1.cmml" xref="footnote5.m2.1.1.1.1.1.1.1">conditional</csymbol><ci id="footnote5.m2.1.1.1.1.1.1.2.cmml" xref="footnote5.m2.1.1.1.1.1.1.2">𝑦</ci><ci id="footnote5.m2.1.1.1.1.1.1.3.cmml" xref="footnote5.m2.1.1.1.1.1.1.3">𝑥</ci></apply></apply><apply id="footnote5.m2.2.2.2.cmml" xref="footnote5.m2.2.2.2"><times id="footnote5.m2.2.2.2.2.cmml" xref="footnote5.m2.2.2.2.2"></times><apply id="footnote5.m2.2.2.2.3.cmml" xref="footnote5.m2.2.2.2.3"><csymbol cd="ambiguous" id="footnote5.m2.2.2.2.3.1.cmml" xref="footnote5.m2.2.2.2.3">subscript</csymbol><ci id="footnote5.m2.2.2.2.3.2.cmml" xref="footnote5.m2.2.2.2.3.2">𝒫</ci><ci id="footnote5.m2.2.2.2.3.3.cmml" xref="footnote5.m2.2.2.2.3.3">𝑗</ci></apply><apply id="footnote5.m2.2.2.2.1.1.1.cmml" xref="footnote5.m2.2.2.2.1.1"><csymbol cd="latexml" id="footnote5.m2.2.2.2.1.1.1.1.cmml" xref="footnote5.m2.2.2.2.1.1.1.1">conditional</csymbol><ci id="footnote5.m2.2.2.2.1.1.1.2.cmml" xref="footnote5.m2.2.2.2.1.1.1.2">𝑦</ci><ci id="footnote5.m2.2.2.2.1.1.1.3.cmml" xref="footnote5.m2.2.2.2.1.1.1.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m2.2d">\mathcal{P}_{i}(y\,|\,x)=\mathcal{P}_{j}(y\,|\,x)</annotation></semantics></math> for all clients <math id="footnote5.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="footnote5.m3.1b"><mi id="footnote5.m3.1.1" xref="footnote5.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="footnote5.m3.1c"><ci id="footnote5.m3.1.1.cmml" xref="footnote5.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m3.1d">i</annotation></semantics></math> and <math id="footnote5.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="footnote5.m4.1b"><mi id="footnote5.m4.1.1" xref="footnote5.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="footnote5.m4.1c"><ci id="footnote5.m4.1.1.cmml" xref="footnote5.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m4.1d">j</annotation></semantics></math>.</span></span></span> For example, in a handwriting recognition domain, users who write the same words might still have different stroke width, slant, etc.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.2" class="ltx_p"><em id="S3.I1.i2.p1.2.1" class="ltx_emph ltx_font_italic">Label distribution skew</em> (prior probability shift): The marginal distributions <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}(y)" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.2" xref="S3.I1.i2.p1.1.m1.1.2.cmml"><msub id="S3.I1.i2.p1.1.m1.1.2.2" xref="S3.I1.i2.p1.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.1.m1.1.2.2.2" xref="S3.I1.i2.p1.1.m1.1.2.2.2.cmml">𝒫</mi><mi id="S3.I1.i2.p1.1.m1.1.2.2.3" xref="S3.I1.i2.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.2.1" xref="S3.I1.i2.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.I1.i2.p1.1.m1.1.2.3.2" xref="S3.I1.i2.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.I1.i2.p1.1.m1.1.2.3.2.1" xref="S3.I1.i2.p1.1.m1.1.2.cmml">(</mo><mi id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">y</mi><mo stretchy="false" id="S3.I1.i2.p1.1.m1.1.2.3.2.2" xref="S3.I1.i2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.2"><times id="S3.I1.i2.p1.1.m1.1.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.2.1"></times><apply id="S3.I1.i2.p1.1.m1.1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.2.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.2.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.2.2.2">𝒫</ci><ci id="S3.I1.i2.p1.1.m1.1.2.2.3.cmml" xref="S3.I1.i2.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\mathcal{P}_{i}(y)</annotation></semantics></math> may vary across clients, even if <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}(x\,|\,y)" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mrow id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.2.m2.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.2.m2.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.2.cmml">​</mo><mrow id="S3.I1.i2.p1.2.m2.1.1.1.1" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.I1.i2.p1.2.m2.1.1.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.I1.i2.p1.2.m2.1.1.1.1.1" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.cmml"><mi id="S3.I1.i2.p1.2.m2.1.1.1.1.1.2" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.2.cmml">x</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.I1.i2.p1.2.m2.1.1.1.1.1.1" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.I1.i2.p1.2.m2.1.1.1.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.I1.i2.p1.2.m2.1.1.1.1.3" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><apply id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1"><times id="S3.I1.i2.p1.2.m2.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.2"></times><ci id="S3.I1.i2.p1.2.m2.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.3">𝒫</ci><apply id="S3.I1.i2.p1.2.m2.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.I1.i2.p1.2.m2.1.1.1.1.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.I1.i2.p1.2.m2.1.1.1.1.1.2.cmml" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.2">𝑥</ci><ci id="S3.I1.i2.p1.2.m2.1.1.1.1.1.3.cmml" xref="S3.I1.i2.p1.2.m2.1.1.1.1.1.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">\mathcal{P}(x\,|\,y)</annotation></semantics></math> is the same. For example, when clients are tied to particular geo-regions, the distribution of labels varies across clients — kangaroos are only in Australia or zoos; a person’s face is only in a few locations worldwide; for mobile device keyboards, certain emoji are used by one demographic but not others.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.4" class="ltx_p"><em id="S3.I1.i3.p1.4.1" class="ltx_emph ltx_font_italic">Same label, different features</em> (concept drift): The conditional distributions <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}(x\,|\,y)" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><msub id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.1.1.3.2" xref="S3.I1.i3.p1.1.m1.1.1.3.2.cmml">𝒫</mi><mi id="S3.I1.i3.p1.1.m1.1.1.3.3" xref="S3.I1.i3.p1.1.m1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S3.I1.i3.p1.1.m1.1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.I1.i3.p1.1.m1.1.1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.I1.i3.p1.1.m1.1.1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.2.cmml">x</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.I1.i3.p1.1.m1.1.1.1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.I1.i3.p1.1.m1.1.1.1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.I1.i3.p1.1.m1.1.1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><times id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"></times><apply id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.2">𝒫</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.I1.i3.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.I1.i3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.2">𝑥</ci><ci id="S3.I1.i3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1.1.1.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\mathcal{P}_{i}(x\,|\,y)</annotation></semantics></math> may vary across clients even if <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}(y)" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><mrow id="S3.I1.i3.p1.2.m2.1.2" xref="S3.I1.i3.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.2.m2.1.2.2" xref="S3.I1.i3.p1.2.m2.1.2.2.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.2.m2.1.2.1" xref="S3.I1.i3.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.I1.i3.p1.2.m2.1.2.3.2" xref="S3.I1.i3.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.I1.i3.p1.2.m2.1.2.3.2.1" xref="S3.I1.i3.p1.2.m2.1.2.cmml">(</mo><mi id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S3.I1.i3.p1.2.m2.1.2.3.2.2" xref="S3.I1.i3.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.2"><times id="S3.I1.i3.p1.2.m2.1.2.1.cmml" xref="S3.I1.i3.p1.2.m2.1.2.1"></times><ci id="S3.I1.i3.p1.2.m2.1.2.2.cmml" xref="S3.I1.i3.p1.2.m2.1.2.2">𝒫</ci><ci id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">\mathcal{P}(y)</annotation></semantics></math> is shared. The same label <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mi id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">y</annotation></semantics></math> can have very different features <math id="S3.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.I1.i3.p1.4.m4.1a"><mi id="S3.I1.i3.p1.4.m4.1.1" xref="S3.I1.i3.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.4.m4.1b"><ci id="S3.I1.i3.p1.4.m4.1.1.cmml" xref="S3.I1.i3.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.4.m4.1c">x</annotation></semantics></math> for different clients, e.g. due to cultural differences, weather effects, standards of living, etc.
For example, images of homes can vary dramatically around the world and items of clothing vary widely.
Even within the U.S., images of parked cars in the winter will be snow-covered only in certain parts of the country.
The same label can also look very different at different times, and at different time scales: day vs. night, seasonal effects, natural disasters, fashion and design trends, etc.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.2" class="ltx_p"><em id="S3.I1.i4.p1.2.1" class="ltx_emph ltx_font_italic">Same features, different label</em> (concept shift): The conditional distribution <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}(y\,|\,x)" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mrow id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml"><msub id="S3.I1.i4.p1.1.m1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i4.p1.1.m1.1.1.3.2" xref="S3.I1.i4.p1.1.m1.1.1.3.2.cmml">𝒫</mi><mi id="S3.I1.i4.p1.1.m1.1.1.3.3" xref="S3.I1.i4.p1.1.m1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.1.m1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S3.I1.i4.p1.1.m1.1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.I1.i4.p1.1.m1.1.1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.I1.i4.p1.1.m1.1.1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.I1.i4.p1.1.m1.1.1.1.1.1.2" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.I1.i4.p1.1.m1.1.1.1.1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.I1.i4.p1.1.m1.1.1.1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.I1.i4.p1.1.m1.1.1.1.1.3" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><apply id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1"><times id="S3.I1.i4.p1.1.m1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.2"></times><apply id="S3.I1.i4.p1.1.m1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i4.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I1.i4.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3.2">𝒫</ci><ci id="S3.I1.i4.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.I1.i4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.I1.i4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.I1.i4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="S3.I1.i4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.I1.i4.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">\mathcal{P}_{i}(y\,|\,x)</annotation></semantics></math> may vary across clients, even if <math id="S3.I1.i4.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}(x)" display="inline"><semantics id="S3.I1.i4.p1.2.m2.1a"><mrow id="S3.I1.i4.p1.2.m2.1.2" xref="S3.I1.i4.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i4.p1.2.m2.1.2.2" xref="S3.I1.i4.p1.2.m2.1.2.2.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.I1.i4.p1.2.m2.1.2.1" xref="S3.I1.i4.p1.2.m2.1.2.1.cmml">​</mo><mrow id="S3.I1.i4.p1.2.m2.1.2.3.2" xref="S3.I1.i4.p1.2.m2.1.2.cmml"><mo stretchy="false" id="S3.I1.i4.p1.2.m2.1.2.3.2.1" xref="S3.I1.i4.p1.2.m2.1.2.cmml">(</mo><mi id="S3.I1.i4.p1.2.m2.1.1" xref="S3.I1.i4.p1.2.m2.1.1.cmml">x</mi><mo stretchy="false" id="S3.I1.i4.p1.2.m2.1.2.3.2.2" xref="S3.I1.i4.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.2.m2.1b"><apply id="S3.I1.i4.p1.2.m2.1.2.cmml" xref="S3.I1.i4.p1.2.m2.1.2"><times id="S3.I1.i4.p1.2.m2.1.2.1.cmml" xref="S3.I1.i4.p1.2.m2.1.2.1"></times><ci id="S3.I1.i4.p1.2.m2.1.2.2.cmml" xref="S3.I1.i4.p1.2.m2.1.2.2">𝒫</ci><ci id="S3.I1.i4.p1.2.m2.1.1.cmml" xref="S3.I1.i4.p1.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.2.m2.1c">\mathcal{P}(x)</annotation></semantics></math> is the same. Because of personal preferences, the same feature vectors in a training data item can have different labels.
For example, labels that reflect sentiment or next word predictors have personal and regional variation.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><em id="S3.I1.i5.p1.1.1" class="ltx_emph ltx_font_italic">Quantity skew</em> or unbalancedness: Different clients can hold vastly different amounts of data.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p2.1" class="ltx_p">Real-world federated learning datasets likely contain a mixture of these effects, and the characterization of cross-client differences in real-world partitioned datasets is an important open question. Most empirical work on synthetic non-IID datasets (e.g. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>, <a href="#bib.bib236" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">236</span></a>]</cite>) have focused on label distribution skew, where a non-IID dataset is formed by partitioning a “flat” existing dataset based on the labels. A better understanding of the nature of real-world non-IID datasets will allow for the construction of controlled but realistic non-IID datasets for testing algorithms and assessing their resilience to different degrees of client heterogeneity.</p>
</div>
<div id="S3.SS1.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p3.2" class="ltx_p">Further, different non-IID regimes may require the development of different mitigation strategies. For example, under feature-distribution skew, because <math id="S3.SS1.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{P}(y\,|\,x)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.1.m1.1a"><mrow id="S3.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1"><times id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.2"></times><ci id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.3">𝒫</ci><apply id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.1.m1.1c">\mathcal{P}(y\,|\,x)</annotation></semantics></math> is assumed to be common, the problem is at least in principle well specified, and training a single global model that learns <math id="S3.SS1.SSS0.Px1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{P}(y\,|\,x)" display="inline"><semantics id="S3.SS1.SSS0.Px1.p3.2.m2.1a"><mrow id="S3.SS1.SSS0.Px1.p3.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml">𝒫</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml">​</mo><mrow id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.2" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.3" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p3.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1"><times id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.2"></times><ci id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.3">𝒫</ci><apply id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.2">𝑦</ci><ci id="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p3.2.m2.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p3.2.m2.1c">\mathcal{P}(y\,|\,x)</annotation></semantics></math> may be appropriate. When the same features map to different labels on different clients, some form of personalization (<a href="#S3.SS3" title="3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) may be essential to learning the true labeling functions.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Violations of independence</h5>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">Violations of independence are introduced any time the distribution <math id="S3.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\mathcal{Q}</annotation></semantics></math> changes over the course of training; a prominent example is in cross-device FL, where devices typically need to meet eligibility requirements in order to participate in training (see <a href="#S1.SS1.SSS2" title="1.1.2 A Typical Federated Training Process ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1.1.2</span></a>). Devices typically meet those requirements at night local time (when they are more likely to be charging, on free wi-fi, and idle), and so there may be significant diurnal patterns in device availability. Further, because local time of day corresponds directly to longitude, this introduces a strong geographic bias in the source of the data. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Eichner et al.</span> [<a href="#bib.bib171" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite> described this issue and some mitigation strategies, but many open questions remain.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Dataset shift</h5>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.2" class="ltx_p">Finally, we note that the temporal dependence of the distributions <math id="S3.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{Q}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml">𝒬</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">𝒬</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">\mathcal{Q}</annotation></semantics></math> and <math id="S3.SS1.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">\mathcal{P}</annotation></semantics></math> may introduce dataset shift in the classic sense (differences between the train and test distributions). Furthermore, other criteria may make the set of clients eligible to train a federated model different from the set of clients where that model will be deployed. For example, training may require devices with more memory than is needed for inference. These issues are explored in more depth in <a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>. Adapting techniques for handling dataset shift to federated learning is another interesting open question.</p>
</div>
</section>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Strategies for Dealing with Non-IID Data</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The original goal of federated learning, training a single global model on the union of client datasets, becomes harder with non-IID data. One natural approach is to modify existing algorithms (e.g. through different hyperparameter choices) or develop new ones in order to more effectively achieve this objective. This approach is considered in <a href="#S3.SS2.SSS2" title="3.2.2 Optimization Algorithms and Convergence Rates for Non-IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">For some applications, it may be possible to augment data in order to make the data across clients more similar. One approach is to create a small dataset which can be shared globally. This dataset may originate from a publicly available proxy data source, a separate dataset from the clients’ data which is not privacy sensitive, or perhaps a distillation of the raw data following <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib473" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">473</span></a>]</cite>.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">The heterogeneity of client objective functions gives additional importance to the question of how to craft the objective function — it is no-longer clear that treating all examples equally makes sense. Alternatives include limiting the contributions of the data from any one user (which is also important for privacy, see <a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>) and introducing other notions of fairness among the clients; see discussion in <a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.3" class="ltx_p">But if we have the capability to run training on the local data on each device (which is necessary for federated learning of a global model), is training a single global model even the right goal? There are many cases where having a single model is to be preferred, e.g. in order to provide a model to clients with no data, or to allow manual validation and quality assurance before deployment. Nevertheless, since local training is possible, it becomes feasible for each client to have a customized model. This approach can turn the non-IID problem from a bug to a feature, almost literally — since each client has its own model, the client’s identity effectively parameterizes the model, rendering some pathological but degenerate non-IID distributions trivial. For example, if for each <math id="S3.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><mi id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><ci id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">i</annotation></semantics></math>, <math id="S3.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{i}(y)" display="inline"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mrow id="S3.SS1.SSS1.p4.2.m2.1.2" xref="S3.SS1.SSS1.p4.2.m2.1.2.cmml"><msub id="S3.SS1.SSS1.p4.2.m2.1.2.2" xref="S3.SS1.SSS1.p4.2.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS1.p4.2.m2.1.2.2.2" xref="S3.SS1.SSS1.p4.2.m2.1.2.2.2.cmml">𝒫</mi><mi id="S3.SS1.SSS1.p4.2.m2.1.2.2.3" xref="S3.SS1.SSS1.p4.2.m2.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS1.SSS1.p4.2.m2.1.2.1" xref="S3.SS1.SSS1.p4.2.m2.1.2.1.cmml">​</mo><mrow id="S3.SS1.SSS1.p4.2.m2.1.2.3.2" xref="S3.SS1.SSS1.p4.2.m2.1.2.cmml"><mo stretchy="false" id="S3.SS1.SSS1.p4.2.m2.1.2.3.2.1" xref="S3.SS1.SSS1.p4.2.m2.1.2.cmml">(</mo><mi id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml">y</mi><mo stretchy="false" id="S3.SS1.SSS1.p4.2.m2.1.2.3.2.2" xref="S3.SS1.SSS1.p4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><apply id="S3.SS1.SSS1.p4.2.m2.1.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2"><times id="S3.SS1.SSS1.p4.2.m2.1.2.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2.1"></times><apply id="S3.SS1.SSS1.p4.2.m2.1.2.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.2.2.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.2.2.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2.2.2">𝒫</ci><ci id="S3.SS1.SSS1.p4.2.m2.1.2.2.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.2.2.3">𝑖</ci></apply><ci id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">\mathcal{P}_{i}(y)</annotation></semantics></math> has support on only a single label, finding a high-accuracy global model may be very challenging (especially if <math id="S3.SS1.SSS1.p4.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS1.SSS1.p4.3.m3.1a"><mi id="S3.SS1.SSS1.p4.3.m3.1.1" xref="S3.SS1.SSS1.p4.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.3.m3.1b"><ci id="S3.SS1.SSS1.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p4.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.3.m3.1c">x</annotation></semantics></math> is relatively uninformative), but training a high-accuracy local model is trivial (only a constant prediction is needed). Such multi-model approaches are considered in depth in <a href="#S3.SS3" title="3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>. In addition to addressing non-identical client distributions, using a plurality of models can also address violations of independence stemming from changes in client availability. For example, the approach of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Eichner et al.</span> [<a href="#bib.bib171" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite> uses a single training run but averages different iterates in order to provide different models for inference based on the timezone / longitude of clients.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Optimization Algorithms for Federated Learning</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In prototypical federated learning tasks, the goal is to learn a single global model that minimizes the empirical risk function over the entire training dataset, that is, the union of the data across all the clients. The main difference between federated optimization algorithms and standard distributed training methods
is the need to address the characteristics of Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> — for optimization, non-IID and unbalanced data, limited communication bandwidth, and unreliable and limited device availability are particularly salient.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">FL settings where the total number of devices is huge (e.g. across mobile devices) necessitate algorithms that only require a handful of clients to participate per round (client sampling). Further, each device is likely to participate no more than once in the training of a given model, so stateless algorithms are necessary. This rules out the direct application of a variety of approaches that are quite effective in the datacenter context, for example stateful optimization algorithms like ADMM, and stateful compression strategies that modify updates based on residual compression errors from previous rounds.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Another important practical consideration for federated learning algorithms is composability with other techniques. Optimization algorithms do not run in isolation in a production deployment, but need to be combined with other techniques like cryptographic secure aggregation protocols (Section <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>), differential privacy (DP) (Section <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>), and model and update compression (Section <a href="#S3.SS5" title="3.5 Communication and Compression ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>). As noted in Section <a href="#S1.SS1.SSS2" title="1.1.2 A Typical Federated Training Process ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.1.2</span></a>, many of these techniques can be applied to primitives like “<span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">sum over selected clients</span>” and “<span id="S3.SS2.p3.1.2" class="ltx_text ltx_font_typewriter">broadcast to selected clients</span>”, and so expressing optimization algorithms in terms of these primitives provides a valuable separation of concerns, but may also exclude certain techniques such as applying updates asynchronously.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">One of the most common approaches to optimization for federated learning is the Federated Averaging algorithm <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite>, an adaption of local-update or parallel SGD.<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Federated Averaging applies local SGD to a randomly sampled subset of clients on each round, and proposes a specific update weighting scheme.</span></span></span> Here, each client runs some number of SGD steps locally, and then the updated local models are averaged to form the updated global model on the coordinating server. Pseudocode is given in Algorithm <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">Performing local updates and communicating less frequently with the central server addresses the core challenges of respecting data locality constraints and of the limited communication capabilities of mobile device clients. However, this family of algorithms also poses several new algorithmic challenges from an optimization theory point of view. In Section <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we discuss recent advances and open challenges in federated optimization algorithms for the cases of IID and non-IID data distribution across the clients respectively. The development of new algorithms that specifically target the characteristics of the federated learning setting remains an important open problem.</p>
</div>
<figure id="S3.SS2.4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.SS2.4.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:173.4pt;">
<table id="S3.SS2.4.4.4" class="ltx_tabular ltx_align_top">
<tr id="S3.SS2.1.1.1.1" class="ltx_tr">
<td id="S3.SS2.1.1.1.1.1" class="ltx_td ltx_align_right ltx_border_tt">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span><math id="S3.SS2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.1.1.1.1.1.m1.1a"><mi id="S3.SS2.1.1.1.1.1.m1.1.1" xref="S3.SS2.1.1.1.1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.1.1.1.1.1.m1.1b"><ci id="S3.SS2.1.1.1.1.1.m1.1.1.cmml" xref="S3.SS2.1.1.1.1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.1.1.1.1.1.m1.1c">N</annotation></semantics></math>
</td>
<td id="S3.SS2.1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Total number of clients</td>
</tr>
<tr id="S3.SS2.2.2.2.2" class="ltx_tr">
<td id="S3.SS2.2.2.2.2.1" class="ltx_td ltx_align_right">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span><math id="S3.SS2.2.2.2.2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.2.2.2.2.1.m1.1a"><mi id="S3.SS2.2.2.2.2.1.m1.1.1" xref="S3.SS2.2.2.2.2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.2.2.2.2.1.m1.1b"><ci id="S3.SS2.2.2.2.2.1.m1.1.1.cmml" xref="S3.SS2.2.2.2.2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.2.2.2.2.1.m1.1c">M</annotation></semantics></math>
</td>
<td id="S3.SS2.2.2.2.2.2" class="ltx_td ltx_align_left">Clients per round</td>
</tr>
<tr id="S3.SS2.3.3.3.3" class="ltx_tr">
<td id="S3.SS2.3.3.3.3.1" class="ltx_td ltx_align_right">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span><math id="S3.SS2.3.3.3.3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.3.3.3.3.1.m1.1a"><mi id="S3.SS2.3.3.3.3.1.m1.1.1" xref="S3.SS2.3.3.3.3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.3.3.3.3.1.m1.1b"><ci id="S3.SS2.3.3.3.3.1.m1.1.1.cmml" xref="S3.SS2.3.3.3.3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.3.3.3.3.1.m1.1c">T</annotation></semantics></math>
</td>
<td id="S3.SS2.3.3.3.3.2" class="ltx_td ltx_align_left">Total communication rounds</td>
</tr>
<tr id="S3.SS2.4.4.4.4" class="ltx_tr">
<td id="S3.SS2.4.4.4.4.1" class="ltx_td ltx_align_right ltx_border_bb">
<span class="ltx_rule" style="width:0.0pt;height:9.5pt;background:black;display:inline-block;"></span><math id="S3.SS2.4.4.4.4.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.4.4.4.4.1.m1.1a"><mi id="S3.SS2.4.4.4.4.1.m1.1.1" xref="S3.SS2.4.4.4.4.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.4.4.4.4.1.m1.1b"><ci id="S3.SS2.4.4.4.4.1.m1.1.1.cmml" xref="S3.SS2.4.4.4.4.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.4.4.4.4.1.m1.1c">K</annotation></semantics></math>
</td>
<td id="S3.SS2.4.4.4.4.2" class="ltx_td ltx_align_left ltx_border_bb">Local steps per round.</td>
</tr>
</table>
<span id="S3.SS2.4.4.5" class="ltx_ERROR undefined">\captionof</span>
<p id="S3.SS2.4.4.6" class="ltx_p">tableNotation for the discussion of FL algorithms including Federated Averaging.</p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div id="S3.SS2.4.5" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:238.5pt;">
<p id="S3.SS2.4.5.1" class="ltx_p"><span class="ltx_rule" style="width:433.6pt;height:0.8pt;background:black;display:inline-block;"> </span></p>
<div id="S3.SS2.4.5.2" class="ltx_listing ltx_listing">
<div id="algx1.l0" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="algx1.l0.1.1.1" class="ltx_text" style="font-size:80%;">0:</span></span>   <span id="algx1.l0.2" class="ltx_text ltx_font_bold">Server executes:</span>

</div>
<div id="algx1.l1" class="ltx_listingline">  initialize <math id="algx1.l1.m1.1" class="ltx_Math" alttext="x_{0}" display="inline"><semantics id="algx1.l1.m1.1a"><msub id="algx1.l1.m1.1.1" xref="algx1.l1.m1.1.1.cmml"><mi id="algx1.l1.m1.1.1.2" xref="algx1.l1.m1.1.1.2.cmml">x</mi><mn id="algx1.l1.m1.1.1.3" xref="algx1.l1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="algx1.l1.m1.1b"><apply id="algx1.l1.m1.1.1.cmml" xref="algx1.l1.m1.1.1"><csymbol cd="ambiguous" id="algx1.l1.m1.1.1.1.cmml" xref="algx1.l1.m1.1.1">subscript</csymbol><ci id="algx1.l1.m1.1.1.2.cmml" xref="algx1.l1.m1.1.1.2">𝑥</ci><cn type="integer" id="algx1.l1.m1.1.1.3.cmml" xref="algx1.l1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m1.1c">x_{0}</annotation></semantics></math>

</div>
<div id="algx1.l2" class="ltx_listingline">  <span id="algx1.l2.1" class="ltx_text ltx_font_bold">for</span> each round <math id="algx1.l2.m1.3" class="ltx_Math" alttext="t=1,2,\dots" display="inline"><semantics id="algx1.l2.m1.3a"><mrow id="algx1.l2.m1.3.4" xref="algx1.l2.m1.3.4.cmml"><mi id="algx1.l2.m1.3.4.2" xref="algx1.l2.m1.3.4.2.cmml">t</mi><mo id="algx1.l2.m1.3.4.1" xref="algx1.l2.m1.3.4.1.cmml">=</mo><mrow id="algx1.l2.m1.3.4.3.2" xref="algx1.l2.m1.3.4.3.1.cmml"><mn id="algx1.l2.m1.1.1" xref="algx1.l2.m1.1.1.cmml">1</mn><mo id="algx1.l2.m1.3.4.3.2.1" xref="algx1.l2.m1.3.4.3.1.cmml">,</mo><mn id="algx1.l2.m1.2.2" xref="algx1.l2.m1.2.2.cmml">2</mn><mo id="algx1.l2.m1.3.4.3.2.2" xref="algx1.l2.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="algx1.l2.m1.3.3" xref="algx1.l2.m1.3.3.cmml">…</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l2.m1.3b"><apply id="algx1.l2.m1.3.4.cmml" xref="algx1.l2.m1.3.4"><eq id="algx1.l2.m1.3.4.1.cmml" xref="algx1.l2.m1.3.4.1"></eq><ci id="algx1.l2.m1.3.4.2.cmml" xref="algx1.l2.m1.3.4.2">𝑡</ci><list id="algx1.l2.m1.3.4.3.1.cmml" xref="algx1.l2.m1.3.4.3.2"><cn type="integer" id="algx1.l2.m1.1.1.cmml" xref="algx1.l2.m1.1.1">1</cn><cn type="integer" id="algx1.l2.m1.2.2.cmml" xref="algx1.l2.m1.2.2">2</cn><ci id="algx1.l2.m1.3.3.cmml" xref="algx1.l2.m1.3.3">…</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m1.3c">t=1,2,\dots</annotation></semantics></math>, T <span id="algx1.l2.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="algx1.l3" class="ltx_listingline">     <math id="algx1.l3.m1.1" class="ltx_Math" alttext="S_{t}\leftarrow" display="inline"><semantics id="algx1.l3.m1.1a"><mrow id="algx1.l3.m1.1.1" xref="algx1.l3.m1.1.1.cmml"><msub id="algx1.l3.m1.1.1.2" xref="algx1.l3.m1.1.1.2.cmml"><mi id="algx1.l3.m1.1.1.2.2" xref="algx1.l3.m1.1.1.2.2.cmml">S</mi><mi id="algx1.l3.m1.1.1.2.3" xref="algx1.l3.m1.1.1.2.3.cmml">t</mi></msub><mo stretchy="false" id="algx1.l3.m1.1.1.1" xref="algx1.l3.m1.1.1.1.cmml">←</mo><mi id="algx1.l3.m1.1.1.3" xref="algx1.l3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="algx1.l3.m1.1b"><apply id="algx1.l3.m1.1.1.cmml" xref="algx1.l3.m1.1.1"><ci id="algx1.l3.m1.1.1.1.cmml" xref="algx1.l3.m1.1.1.1">←</ci><apply id="algx1.l3.m1.1.1.2.cmml" xref="algx1.l3.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l3.m1.1.1.2.1.cmml" xref="algx1.l3.m1.1.1.2">subscript</csymbol><ci id="algx1.l3.m1.1.1.2.2.cmml" xref="algx1.l3.m1.1.1.2.2">𝑆</ci><ci id="algx1.l3.m1.1.1.2.3.cmml" xref="algx1.l3.m1.1.1.2.3">𝑡</ci></apply><csymbol cd="latexml" id="algx1.l3.m1.1.1.3.cmml" xref="algx1.l3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m1.1c">S_{t}\leftarrow</annotation></semantics></math> (random set of <math id="algx1.l3.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="algx1.l3.m2.1a"><mi id="algx1.l3.m2.1.1" xref="algx1.l3.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="algx1.l3.m2.1b"><ci id="algx1.l3.m2.1.1.cmml" xref="algx1.l3.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m2.1c">M</annotation></semantics></math> clients)

</div>
<div id="algx1.l4" class="ltx_listingline">     <span id="algx1.l4.1" class="ltx_text ltx_font_bold">for</span> each client <math id="algx1.l4.m1.1" class="ltx_Math" alttext="i\in S_{t}" display="inline"><semantics id="algx1.l4.m1.1a"><mrow id="algx1.l4.m1.1.1" xref="algx1.l4.m1.1.1.cmml"><mi id="algx1.l4.m1.1.1.2" xref="algx1.l4.m1.1.1.2.cmml">i</mi><mo id="algx1.l4.m1.1.1.1" xref="algx1.l4.m1.1.1.1.cmml">∈</mo><msub id="algx1.l4.m1.1.1.3" xref="algx1.l4.m1.1.1.3.cmml"><mi id="algx1.l4.m1.1.1.3.2" xref="algx1.l4.m1.1.1.3.2.cmml">S</mi><mi id="algx1.l4.m1.1.1.3.3" xref="algx1.l4.m1.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algx1.l4.m1.1b"><apply id="algx1.l4.m1.1.1.cmml" xref="algx1.l4.m1.1.1"><in id="algx1.l4.m1.1.1.1.cmml" xref="algx1.l4.m1.1.1.1"></in><ci id="algx1.l4.m1.1.1.2.cmml" xref="algx1.l4.m1.1.1.2">𝑖</ci><apply id="algx1.l4.m1.1.1.3.cmml" xref="algx1.l4.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l4.m1.1.1.3.1.cmml" xref="algx1.l4.m1.1.1.3">subscript</csymbol><ci id="algx1.l4.m1.1.1.3.2.cmml" xref="algx1.l4.m1.1.1.3.2">𝑆</ci><ci id="algx1.l4.m1.1.1.3.3.cmml" xref="algx1.l4.m1.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l4.m1.1c">i\in S_{t}</annotation></semantics></math> <span id="algx1.l4.2" class="ltx_text ltx_font_bold">in parallel</span> <span id="algx1.l4.3" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="algx1.l5" class="ltx_listingline">        <math id="algx1.l5.m1.2" class="ltx_Math" alttext="x_{t+1}^{i}\leftarrow\text{ClientUpdate}(i,x_{t})" display="inline"><semantics id="algx1.l5.m1.2a"><mrow id="algx1.l5.m1.2.2" xref="algx1.l5.m1.2.2.cmml"><msubsup id="algx1.l5.m1.2.2.3" xref="algx1.l5.m1.2.2.3.cmml"><mi id="algx1.l5.m1.2.2.3.2.2" xref="algx1.l5.m1.2.2.3.2.2.cmml">x</mi><mrow id="algx1.l5.m1.2.2.3.2.3" xref="algx1.l5.m1.2.2.3.2.3.cmml"><mi id="algx1.l5.m1.2.2.3.2.3.2" xref="algx1.l5.m1.2.2.3.2.3.2.cmml">t</mi><mo id="algx1.l5.m1.2.2.3.2.3.1" xref="algx1.l5.m1.2.2.3.2.3.1.cmml">+</mo><mn id="algx1.l5.m1.2.2.3.2.3.3" xref="algx1.l5.m1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="algx1.l5.m1.2.2.3.3" xref="algx1.l5.m1.2.2.3.3.cmml">i</mi></msubsup><mo stretchy="false" id="algx1.l5.m1.2.2.2" xref="algx1.l5.m1.2.2.2.cmml">←</mo><mrow id="algx1.l5.m1.2.2.1" xref="algx1.l5.m1.2.2.1.cmml"><mtext id="algx1.l5.m1.2.2.1.3" xref="algx1.l5.m1.2.2.1.3a.cmml">ClientUpdate</mtext><mo lspace="0em" rspace="0em" id="algx1.l5.m1.2.2.1.2" xref="algx1.l5.m1.2.2.1.2.cmml">​</mo><mrow id="algx1.l5.m1.2.2.1.1.1" xref="algx1.l5.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="algx1.l5.m1.2.2.1.1.1.2" xref="algx1.l5.m1.2.2.1.1.2.cmml">(</mo><mi id="algx1.l5.m1.1.1" xref="algx1.l5.m1.1.1.cmml">i</mi><mo id="algx1.l5.m1.2.2.1.1.1.3" xref="algx1.l5.m1.2.2.1.1.2.cmml">,</mo><msub id="algx1.l5.m1.2.2.1.1.1.1" xref="algx1.l5.m1.2.2.1.1.1.1.cmml"><mi id="algx1.l5.m1.2.2.1.1.1.1.2" xref="algx1.l5.m1.2.2.1.1.1.1.2.cmml">x</mi><mi id="algx1.l5.m1.2.2.1.1.1.1.3" xref="algx1.l5.m1.2.2.1.1.1.1.3.cmml">t</mi></msub><mo stretchy="false" id="algx1.l5.m1.2.2.1.1.1.4" xref="algx1.l5.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l5.m1.2b"><apply id="algx1.l5.m1.2.2.cmml" xref="algx1.l5.m1.2.2"><ci id="algx1.l5.m1.2.2.2.cmml" xref="algx1.l5.m1.2.2.2">←</ci><apply id="algx1.l5.m1.2.2.3.cmml" xref="algx1.l5.m1.2.2.3"><csymbol cd="ambiguous" id="algx1.l5.m1.2.2.3.1.cmml" xref="algx1.l5.m1.2.2.3">superscript</csymbol><apply id="algx1.l5.m1.2.2.3.2.cmml" xref="algx1.l5.m1.2.2.3"><csymbol cd="ambiguous" id="algx1.l5.m1.2.2.3.2.1.cmml" xref="algx1.l5.m1.2.2.3">subscript</csymbol><ci id="algx1.l5.m1.2.2.3.2.2.cmml" xref="algx1.l5.m1.2.2.3.2.2">𝑥</ci><apply id="algx1.l5.m1.2.2.3.2.3.cmml" xref="algx1.l5.m1.2.2.3.2.3"><plus id="algx1.l5.m1.2.2.3.2.3.1.cmml" xref="algx1.l5.m1.2.2.3.2.3.1"></plus><ci id="algx1.l5.m1.2.2.3.2.3.2.cmml" xref="algx1.l5.m1.2.2.3.2.3.2">𝑡</ci><cn type="integer" id="algx1.l5.m1.2.2.3.2.3.3.cmml" xref="algx1.l5.m1.2.2.3.2.3.3">1</cn></apply></apply><ci id="algx1.l5.m1.2.2.3.3.cmml" xref="algx1.l5.m1.2.2.3.3">𝑖</ci></apply><apply id="algx1.l5.m1.2.2.1.cmml" xref="algx1.l5.m1.2.2.1"><times id="algx1.l5.m1.2.2.1.2.cmml" xref="algx1.l5.m1.2.2.1.2"></times><ci id="algx1.l5.m1.2.2.1.3a.cmml" xref="algx1.l5.m1.2.2.1.3"><mtext id="algx1.l5.m1.2.2.1.3.cmml" xref="algx1.l5.m1.2.2.1.3">ClientUpdate</mtext></ci><interval closure="open" id="algx1.l5.m1.2.2.1.1.2.cmml" xref="algx1.l5.m1.2.2.1.1.1"><ci id="algx1.l5.m1.1.1.cmml" xref="algx1.l5.m1.1.1">𝑖</ci><apply id="algx1.l5.m1.2.2.1.1.1.1.cmml" xref="algx1.l5.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="algx1.l5.m1.2.2.1.1.1.1.1.cmml" xref="algx1.l5.m1.2.2.1.1.1.1">subscript</csymbol><ci id="algx1.l5.m1.2.2.1.1.1.1.2.cmml" xref="algx1.l5.m1.2.2.1.1.1.1.2">𝑥</ci><ci id="algx1.l5.m1.2.2.1.1.1.1.3.cmml" xref="algx1.l5.m1.2.2.1.1.1.1.3">𝑡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l5.m1.2c">x_{t+1}^{i}\leftarrow\text{ClientUpdate}(i,x_{t})</annotation></semantics></math>

</div>
<div id="algx1.l6" class="ltx_listingline">     <math id="algx1.l6.m1.1" class="ltx_Math" alttext="x_{t+1}\leftarrow\sum_{k=1}^{M}\frac{1}{M}x_{t+1}^{i}" display="inline"><semantics id="algx1.l6.m1.1a"><mrow id="algx1.l6.m1.1.1" xref="algx1.l6.m1.1.1.cmml"><msub id="algx1.l6.m1.1.1.2" xref="algx1.l6.m1.1.1.2.cmml"><mi id="algx1.l6.m1.1.1.2.2" xref="algx1.l6.m1.1.1.2.2.cmml">x</mi><mrow id="algx1.l6.m1.1.1.2.3" xref="algx1.l6.m1.1.1.2.3.cmml"><mi id="algx1.l6.m1.1.1.2.3.2" xref="algx1.l6.m1.1.1.2.3.2.cmml">t</mi><mo id="algx1.l6.m1.1.1.2.3.1" xref="algx1.l6.m1.1.1.2.3.1.cmml">+</mo><mn id="algx1.l6.m1.1.1.2.3.3" xref="algx1.l6.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo rspace="0.111em" stretchy="false" id="algx1.l6.m1.1.1.1" xref="algx1.l6.m1.1.1.1.cmml">←</mo><mrow id="algx1.l6.m1.1.1.3" xref="algx1.l6.m1.1.1.3.cmml"><msubsup id="algx1.l6.m1.1.1.3.1" xref="algx1.l6.m1.1.1.3.1.cmml"><mo id="algx1.l6.m1.1.1.3.1.2.2" xref="algx1.l6.m1.1.1.3.1.2.2.cmml">∑</mo><mrow id="algx1.l6.m1.1.1.3.1.2.3" xref="algx1.l6.m1.1.1.3.1.2.3.cmml"><mi id="algx1.l6.m1.1.1.3.1.2.3.2" xref="algx1.l6.m1.1.1.3.1.2.3.2.cmml">k</mi><mo id="algx1.l6.m1.1.1.3.1.2.3.1" xref="algx1.l6.m1.1.1.3.1.2.3.1.cmml">=</mo><mn id="algx1.l6.m1.1.1.3.1.2.3.3" xref="algx1.l6.m1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="algx1.l6.m1.1.1.3.1.3" xref="algx1.l6.m1.1.1.3.1.3.cmml">M</mi></msubsup><mrow id="algx1.l6.m1.1.1.3.2" xref="algx1.l6.m1.1.1.3.2.cmml"><mfrac id="algx1.l6.m1.1.1.3.2.2" xref="algx1.l6.m1.1.1.3.2.2.cmml"><mn id="algx1.l6.m1.1.1.3.2.2.2" xref="algx1.l6.m1.1.1.3.2.2.2.cmml">1</mn><mi id="algx1.l6.m1.1.1.3.2.2.3" xref="algx1.l6.m1.1.1.3.2.2.3.cmml">M</mi></mfrac><mo lspace="0em" rspace="0em" id="algx1.l6.m1.1.1.3.2.1" xref="algx1.l6.m1.1.1.3.2.1.cmml">​</mo><msubsup id="algx1.l6.m1.1.1.3.2.3" xref="algx1.l6.m1.1.1.3.2.3.cmml"><mi id="algx1.l6.m1.1.1.3.2.3.2.2" xref="algx1.l6.m1.1.1.3.2.3.2.2.cmml">x</mi><mrow id="algx1.l6.m1.1.1.3.2.3.2.3" xref="algx1.l6.m1.1.1.3.2.3.2.3.cmml"><mi id="algx1.l6.m1.1.1.3.2.3.2.3.2" xref="algx1.l6.m1.1.1.3.2.3.2.3.2.cmml">t</mi><mo id="algx1.l6.m1.1.1.3.2.3.2.3.1" xref="algx1.l6.m1.1.1.3.2.3.2.3.1.cmml">+</mo><mn id="algx1.l6.m1.1.1.3.2.3.2.3.3" xref="algx1.l6.m1.1.1.3.2.3.2.3.3.cmml">1</mn></mrow><mi id="algx1.l6.m1.1.1.3.2.3.3" xref="algx1.l6.m1.1.1.3.2.3.3.cmml">i</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l6.m1.1b"><apply id="algx1.l6.m1.1.1.cmml" xref="algx1.l6.m1.1.1"><ci id="algx1.l6.m1.1.1.1.cmml" xref="algx1.l6.m1.1.1.1">←</ci><apply id="algx1.l6.m1.1.1.2.cmml" xref="algx1.l6.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.2.1.cmml" xref="algx1.l6.m1.1.1.2">subscript</csymbol><ci id="algx1.l6.m1.1.1.2.2.cmml" xref="algx1.l6.m1.1.1.2.2">𝑥</ci><apply id="algx1.l6.m1.1.1.2.3.cmml" xref="algx1.l6.m1.1.1.2.3"><plus id="algx1.l6.m1.1.1.2.3.1.cmml" xref="algx1.l6.m1.1.1.2.3.1"></plus><ci id="algx1.l6.m1.1.1.2.3.2.cmml" xref="algx1.l6.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="algx1.l6.m1.1.1.2.3.3.cmml" xref="algx1.l6.m1.1.1.2.3.3">1</cn></apply></apply><apply id="algx1.l6.m1.1.1.3.cmml" xref="algx1.l6.m1.1.1.3"><apply id="algx1.l6.m1.1.1.3.1.cmml" xref="algx1.l6.m1.1.1.3.1"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.3.1.1.cmml" xref="algx1.l6.m1.1.1.3.1">superscript</csymbol><apply id="algx1.l6.m1.1.1.3.1.2.cmml" xref="algx1.l6.m1.1.1.3.1"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.3.1.2.1.cmml" xref="algx1.l6.m1.1.1.3.1">subscript</csymbol><sum id="algx1.l6.m1.1.1.3.1.2.2.cmml" xref="algx1.l6.m1.1.1.3.1.2.2"></sum><apply id="algx1.l6.m1.1.1.3.1.2.3.cmml" xref="algx1.l6.m1.1.1.3.1.2.3"><eq id="algx1.l6.m1.1.1.3.1.2.3.1.cmml" xref="algx1.l6.m1.1.1.3.1.2.3.1"></eq><ci id="algx1.l6.m1.1.1.3.1.2.3.2.cmml" xref="algx1.l6.m1.1.1.3.1.2.3.2">𝑘</ci><cn type="integer" id="algx1.l6.m1.1.1.3.1.2.3.3.cmml" xref="algx1.l6.m1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="algx1.l6.m1.1.1.3.1.3.cmml" xref="algx1.l6.m1.1.1.3.1.3">𝑀</ci></apply><apply id="algx1.l6.m1.1.1.3.2.cmml" xref="algx1.l6.m1.1.1.3.2"><times id="algx1.l6.m1.1.1.3.2.1.cmml" xref="algx1.l6.m1.1.1.3.2.1"></times><apply id="algx1.l6.m1.1.1.3.2.2.cmml" xref="algx1.l6.m1.1.1.3.2.2"><divide id="algx1.l6.m1.1.1.3.2.2.1.cmml" xref="algx1.l6.m1.1.1.3.2.2"></divide><cn type="integer" id="algx1.l6.m1.1.1.3.2.2.2.cmml" xref="algx1.l6.m1.1.1.3.2.2.2">1</cn><ci id="algx1.l6.m1.1.1.3.2.2.3.cmml" xref="algx1.l6.m1.1.1.3.2.2.3">𝑀</ci></apply><apply id="algx1.l6.m1.1.1.3.2.3.cmml" xref="algx1.l6.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.3.2.3.1.cmml" xref="algx1.l6.m1.1.1.3.2.3">superscript</csymbol><apply id="algx1.l6.m1.1.1.3.2.3.2.cmml" xref="algx1.l6.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.3.2.3.2.1.cmml" xref="algx1.l6.m1.1.1.3.2.3">subscript</csymbol><ci id="algx1.l6.m1.1.1.3.2.3.2.2.cmml" xref="algx1.l6.m1.1.1.3.2.3.2.2">𝑥</ci><apply id="algx1.l6.m1.1.1.3.2.3.2.3.cmml" xref="algx1.l6.m1.1.1.3.2.3.2.3"><plus id="algx1.l6.m1.1.1.3.2.3.2.3.1.cmml" xref="algx1.l6.m1.1.1.3.2.3.2.3.1"></plus><ci id="algx1.l6.m1.1.1.3.2.3.2.3.2.cmml" xref="algx1.l6.m1.1.1.3.2.3.2.3.2">𝑡</ci><cn type="integer" id="algx1.l6.m1.1.1.3.2.3.2.3.3.cmml" xref="algx1.l6.m1.1.1.3.2.3.2.3.3">1</cn></apply></apply><ci id="algx1.l6.m1.1.1.3.2.3.3.cmml" xref="algx1.l6.m1.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m1.1c">x_{t+1}\leftarrow\sum_{k=1}^{M}\frac{1}{M}x_{t+1}^{i}</annotation></semantics></math>

</div>
<div id="algx1.l7" class="ltx_listingline">  
</div>
<div id="algx1.l7a" class="ltx_listingline">   <span id="algx1.l7a.1" class="ltx_text ltx_font_bold">ClientUpdate(<math id="algx1.l7a.1.m1.2" class="ltx_Math" alttext="i,x" display="inline"><semantics id="algx1.l7a.1.m1.2a"><mrow id="algx1.l7a.1.m1.2.3.2" xref="algx1.l7a.1.m1.2.3.1.cmml"><mi id="algx1.l7a.1.m1.1.1" xref="algx1.l7a.1.m1.1.1.cmml">i</mi><mo id="algx1.l7a.1.m1.2.3.2.1" xref="algx1.l7a.1.m1.2.3.1.cmml">,</mo><mi id="algx1.l7a.1.m1.2.2" xref="algx1.l7a.1.m1.2.2.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="algx1.l7a.1.m1.2b"><list id="algx1.l7a.1.m1.2.3.1.cmml" xref="algx1.l7a.1.m1.2.3.2"><ci id="algx1.l7a.1.m1.1.1.cmml" xref="algx1.l7a.1.m1.1.1">𝑖</ci><ci id="algx1.l7a.1.m1.2.2.cmml" xref="algx1.l7a.1.m1.2.2">𝑥</ci></list></annotation-xml><annotation encoding="application/x-tex" id="algx1.l7a.1.m1.2c">i,x</annotation></semantics></math>):</span>   
</div>
<div id="algx1.l8" class="ltx_listingline">  <span id="algx1.l8.1" class="ltx_text ltx_font_bold">for</span> local step <math id="algx1.l8.m1.3" class="ltx_Math" alttext="j=1,\dots,K" display="inline"><semantics id="algx1.l8.m1.3a"><mrow id="algx1.l8.m1.3.4" xref="algx1.l8.m1.3.4.cmml"><mi id="algx1.l8.m1.3.4.2" xref="algx1.l8.m1.3.4.2.cmml">j</mi><mo id="algx1.l8.m1.3.4.1" xref="algx1.l8.m1.3.4.1.cmml">=</mo><mrow id="algx1.l8.m1.3.4.3.2" xref="algx1.l8.m1.3.4.3.1.cmml"><mn id="algx1.l8.m1.1.1" xref="algx1.l8.m1.1.1.cmml">1</mn><mo id="algx1.l8.m1.3.4.3.2.1" xref="algx1.l8.m1.3.4.3.1.cmml">,</mo><mi mathvariant="normal" id="algx1.l8.m1.2.2" xref="algx1.l8.m1.2.2.cmml">…</mi><mo id="algx1.l8.m1.3.4.3.2.2" xref="algx1.l8.m1.3.4.3.1.cmml">,</mo><mi id="algx1.l8.m1.3.3" xref="algx1.l8.m1.3.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l8.m1.3b"><apply id="algx1.l8.m1.3.4.cmml" xref="algx1.l8.m1.3.4"><eq id="algx1.l8.m1.3.4.1.cmml" xref="algx1.l8.m1.3.4.1"></eq><ci id="algx1.l8.m1.3.4.2.cmml" xref="algx1.l8.m1.3.4.2">𝑗</ci><list id="algx1.l8.m1.3.4.3.1.cmml" xref="algx1.l8.m1.3.4.3.2"><cn type="integer" id="algx1.l8.m1.1.1.cmml" xref="algx1.l8.m1.1.1">1</cn><ci id="algx1.l8.m1.2.2.cmml" xref="algx1.l8.m1.2.2">…</ci><ci id="algx1.l8.m1.3.3.cmml" xref="algx1.l8.m1.3.3">𝐾</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.m1.3c">j=1,\dots,K</annotation></semantics></math> <span id="algx1.l8.2" class="ltx_text ltx_font_bold">do</span>



</div>
<div id="algx1.l9" class="ltx_listingline">     <math id="algx1.l9.m1.2" class="ltx_Math" alttext="x\leftarrow x-\eta\triangledown f(x;z)" display="inline"><semantics id="algx1.l9.m1.2a"><mrow id="algx1.l9.m1.2.3" xref="algx1.l9.m1.2.3.cmml"><mi id="algx1.l9.m1.2.3.2" xref="algx1.l9.m1.2.3.2.cmml">x</mi><mo stretchy="false" id="algx1.l9.m1.2.3.1" xref="algx1.l9.m1.2.3.1.cmml">←</mo><mrow id="algx1.l9.m1.2.3.3" xref="algx1.l9.m1.2.3.3.cmml"><mi id="algx1.l9.m1.2.3.3.2" xref="algx1.l9.m1.2.3.3.2.cmml">x</mi><mo id="algx1.l9.m1.2.3.3.1" xref="algx1.l9.m1.2.3.3.1.cmml">−</mo><mrow id="algx1.l9.m1.2.3.3.3" xref="algx1.l9.m1.2.3.3.3.cmml"><mi id="algx1.l9.m1.2.3.3.3.2" xref="algx1.l9.m1.2.3.3.3.2.cmml">η</mi><mo lspace="0em" rspace="0em" id="algx1.l9.m1.2.3.3.3.1" xref="algx1.l9.m1.2.3.3.3.1.cmml">​</mo><mi mathvariant="normal" id="algx1.l9.m1.2.3.3.3.3" xref="algx1.l9.m1.2.3.3.3.3.cmml">▽</mi><mo lspace="0em" rspace="0em" id="algx1.l9.m1.2.3.3.3.1a" xref="algx1.l9.m1.2.3.3.3.1.cmml">​</mo><mi id="algx1.l9.m1.2.3.3.3.4" xref="algx1.l9.m1.2.3.3.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="algx1.l9.m1.2.3.3.3.1b" xref="algx1.l9.m1.2.3.3.3.1.cmml">​</mo><mrow id="algx1.l9.m1.2.3.3.3.5.2" xref="algx1.l9.m1.2.3.3.3.5.1.cmml"><mo stretchy="false" id="algx1.l9.m1.2.3.3.3.5.2.1" xref="algx1.l9.m1.2.3.3.3.5.1.cmml">(</mo><mi id="algx1.l9.m1.1.1" xref="algx1.l9.m1.1.1.cmml">x</mi><mo id="algx1.l9.m1.2.3.3.3.5.2.2" xref="algx1.l9.m1.2.3.3.3.5.1.cmml">;</mo><mi id="algx1.l9.m1.2.2" xref="algx1.l9.m1.2.2.cmml">z</mi><mo stretchy="false" id="algx1.l9.m1.2.3.3.3.5.2.3" xref="algx1.l9.m1.2.3.3.3.5.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l9.m1.2b"><apply id="algx1.l9.m1.2.3.cmml" xref="algx1.l9.m1.2.3"><ci id="algx1.l9.m1.2.3.1.cmml" xref="algx1.l9.m1.2.3.1">←</ci><ci id="algx1.l9.m1.2.3.2.cmml" xref="algx1.l9.m1.2.3.2">𝑥</ci><apply id="algx1.l9.m1.2.3.3.cmml" xref="algx1.l9.m1.2.3.3"><minus id="algx1.l9.m1.2.3.3.1.cmml" xref="algx1.l9.m1.2.3.3.1"></minus><ci id="algx1.l9.m1.2.3.3.2.cmml" xref="algx1.l9.m1.2.3.3.2">𝑥</ci><apply id="algx1.l9.m1.2.3.3.3.cmml" xref="algx1.l9.m1.2.3.3.3"><times id="algx1.l9.m1.2.3.3.3.1.cmml" xref="algx1.l9.m1.2.3.3.3.1"></times><ci id="algx1.l9.m1.2.3.3.3.2.cmml" xref="algx1.l9.m1.2.3.3.3.2">𝜂</ci><ci id="algx1.l9.m1.2.3.3.3.3.cmml" xref="algx1.l9.m1.2.3.3.3.3">▽</ci><ci id="algx1.l9.m1.2.3.3.3.4.cmml" xref="algx1.l9.m1.2.3.3.3.4">𝑓</ci><list id="algx1.l9.m1.2.3.3.3.5.1.cmml" xref="algx1.l9.m1.2.3.3.3.5.2"><ci id="algx1.l9.m1.1.1.cmml" xref="algx1.l9.m1.1.1">𝑥</ci><ci id="algx1.l9.m1.2.2.cmml" xref="algx1.l9.m1.2.2">𝑧</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l9.m1.2c">x\leftarrow x-\eta\triangledown f(x;z)</annotation></semantics></math> for <math id="algx1.l9.m2.1" class="ltx_Math" alttext="z\sim\mathcal{P}_{i}" display="inline"><semantics id="algx1.l9.m2.1a"><mrow id="algx1.l9.m2.1.1" xref="algx1.l9.m2.1.1.cmml"><mi id="algx1.l9.m2.1.1.2" xref="algx1.l9.m2.1.1.2.cmml">z</mi><mo id="algx1.l9.m2.1.1.1" xref="algx1.l9.m2.1.1.1.cmml">∼</mo><msub id="algx1.l9.m2.1.1.3" xref="algx1.l9.m2.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="algx1.l9.m2.1.1.3.2" xref="algx1.l9.m2.1.1.3.2.cmml">𝒫</mi><mi id="algx1.l9.m2.1.1.3.3" xref="algx1.l9.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="algx1.l9.m2.1b"><apply id="algx1.l9.m2.1.1.cmml" xref="algx1.l9.m2.1.1"><csymbol cd="latexml" id="algx1.l9.m2.1.1.1.cmml" xref="algx1.l9.m2.1.1.1">similar-to</csymbol><ci id="algx1.l9.m2.1.1.2.cmml" xref="algx1.l9.m2.1.1.2">𝑧</ci><apply id="algx1.l9.m2.1.1.3.cmml" xref="algx1.l9.m2.1.1.3"><csymbol cd="ambiguous" id="algx1.l9.m2.1.1.3.1.cmml" xref="algx1.l9.m2.1.1.3">subscript</csymbol><ci id="algx1.l9.m2.1.1.3.2.cmml" xref="algx1.l9.m2.1.1.3.2">𝒫</ci><ci id="algx1.l9.m2.1.1.3.3.cmml" xref="algx1.l9.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l9.m2.1c">z\sim\mathcal{P}_{i}</annotation></semantics></math>

</div>
<div id="algx1.l10" class="ltx_listingline">  return <math id="algx1.l10.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="algx1.l10.m1.1a"><mi id="algx1.l10.m1.1.1" xref="algx1.l10.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="algx1.l10.m1.1b"><ci id="algx1.l10.m1.1.1.cmml" xref="algx1.l10.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.m1.1c">x</annotation></semantics></math> to server

</div>
</div>
<p id="S3.SS2.4.5.3" class="ltx_p"><span class="ltx_rule" style="width:433.6pt;height:0.8pt;background:black;display:inline-block;"> </span>
<span id="S3.SS2.4.5.3.1" class="ltx_ERROR undefined">\captionof</span>algorithmFederated Averaging (local SGD), when all clients have the same amount of data.</p>
</div>
</div>
</div>
</figure>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Optimization Algorithms and Convergence Rates for IID Datasets</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">While a variety of different assumptions can be made on the per-client functions being optimized, the most basic split is between assuming IID and non-IID data. Formally, having IID data at the clients means that each mini-batch of data used for a client’s local update is statistically identical to a uniformly drawn sample (with replacement) from the entire training dataset (the union of all local datasets at the clients). Since the clients independently collect their own training data which vary in both size and distribution, and these data are not shared with other clients or the central node, the IID assumption clearly almost never holds in practice. However, this assumption greatly simplifies theoretical convergence analysis of federated optimization algorithms, as well as establishes a baseline that can be used to understand the impact of non-IID data on optimization rates. Thus, a natural first step is to obtain an understanding of the landscape of optimization algorithms for the IID data case.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.SSS1.p2.7" class="ltx_p">Formally, for the IID setting let us standardize the stochastic optimization problem</p>
<table id="S3.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\min_{x\in\mathbb{R}^{m}}F(x):=\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}}[f(x;z)]\,." display="block"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1" xref="S3.Ex1.m1.4.4.1.1.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.4" xref="S3.Ex1.m1.4.4.1.1.4.cmml"><mrow id="S3.Ex1.m1.4.4.1.1.4.2" xref="S3.Ex1.m1.4.4.1.1.4.2.cmml"><munder id="S3.Ex1.m1.4.4.1.1.4.2.1" xref="S3.Ex1.m1.4.4.1.1.4.2.1.cmml"><mi id="S3.Ex1.m1.4.4.1.1.4.2.1.2" xref="S3.Ex1.m1.4.4.1.1.4.2.1.2.cmml">min</mi><mrow id="S3.Ex1.m1.4.4.1.1.4.2.1.3" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.4.2.1.3.2" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.2.cmml">x</mi><mo id="S3.Ex1.m1.4.4.1.1.4.2.1.3.1" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.1.cmml">∈</mo><msup id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.2" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.2.cmml">ℝ</mi><mi id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.3" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.3.cmml">m</mi></msup></mrow></munder><mo lspace="0.167em" id="S3.Ex1.m1.4.4.1.1.4.2a" xref="S3.Ex1.m1.4.4.1.1.4.2.cmml">⁡</mo><mi id="S3.Ex1.m1.4.4.1.1.4.2.2" xref="S3.Ex1.m1.4.4.1.1.4.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.4.1" xref="S3.Ex1.m1.4.4.1.1.4.1.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.1.1.4.3.2" xref="S3.Ex1.m1.4.4.1.1.4.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.4.3.2.1" xref="S3.Ex1.m1.4.4.1.1.4.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">x</mi><mo rspace="0.278em" stretchy="false" id="S3.Ex1.m1.4.4.1.1.4.3.2.2" xref="S3.Ex1.m1.4.4.1.1.4.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.4.4.1.1.3" xref="S3.Ex1.m1.4.4.1.1.3.cmml">:=</mo><mrow id="S3.Ex1.m1.4.4.1.1.2.2" xref="S3.Ex1.m1.4.4.1.1.2.3.cmml"><munder id="S3.Ex1.m1.4.4.1.1.1.1.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.cmml"><mo lspace="0.111em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2.cmml">𝔼</mo><mrow id="S3.Ex1.m1.4.4.1.1.1.1.1.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.4.4.1.1.1.1.1.3.2" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.2.cmml">z</mi><mo id="S3.Ex1.m1.4.4.1.1.1.1.1.3.1" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.4.4.1.1.1.1.1.3.3" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.3.cmml">𝒫</mi></mrow></munder><mrow id="S3.Ex1.m1.4.4.1.1.2.2.2" xref="S3.Ex1.m1.4.4.1.1.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.2.2.2.2" xref="S3.Ex1.m1.4.4.1.1.2.3.cmml">[</mo><mrow id="S3.Ex1.m1.4.4.1.1.2.2.2.1" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.cmml"><mi id="S3.Ex1.m1.4.4.1.1.2.2.2.1.2" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.1.1.2.2.2.1.1" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.1.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.2" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.2.1" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.1.cmml">(</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">x</mi><mo id="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.2.2" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.1.cmml">;</mo><mi id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml">z</mi><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.2.3" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.4.4.1.1.2.2.2.3" xref="S3.Ex1.m1.4.4.1.1.2.3.cmml">]</mo></mrow></mrow></mrow><mo lspace="0.170em" id="S3.Ex1.m1.4.4.1.2" xref="S3.Ex1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.1.1.cmml" xref="S3.Ex1.m1.4.4.1"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.3">assign</csymbol><apply id="S3.Ex1.m1.4.4.1.1.4.cmml" xref="S3.Ex1.m1.4.4.1.1.4"><times id="S3.Ex1.m1.4.4.1.1.4.1.cmml" xref="S3.Ex1.m1.4.4.1.1.4.1"></times><apply id="S3.Ex1.m1.4.4.1.1.4.2.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2"><apply id="S3.Ex1.m1.4.4.1.1.4.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.4.2.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1">subscript</csymbol><min id="S3.Ex1.m1.4.4.1.1.4.2.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.2"></min><apply id="S3.Ex1.m1.4.4.1.1.4.2.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3"><in id="S3.Ex1.m1.4.4.1.1.4.2.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.1"></in><ci id="S3.Ex1.m1.4.4.1.1.4.2.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.2">𝑥</ci><apply id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3">superscript</csymbol><ci id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.2">ℝ</ci><ci id="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.1.3.3.3">𝑚</ci></apply></apply></apply><ci id="S3.Ex1.m1.4.4.1.1.4.2.2.cmml" xref="S3.Ex1.m1.4.4.1.1.4.2.2">𝐹</ci></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">𝑥</ci></apply><apply id="S3.Ex1.m1.4.4.1.1.2.3.cmml" xref="S3.Ex1.m1.4.4.1.1.2.2"><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.2">𝔼</ci><apply id="S3.Ex1.m1.4.4.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.1">similar-to</csymbol><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.2">𝑧</ci><ci id="S3.Ex1.m1.4.4.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.4.4.1.1.1.1.1.3.3">𝒫</ci></apply></apply><apply id="S3.Ex1.m1.4.4.1.1.2.2.2.1.cmml" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1"><times id="S3.Ex1.m1.4.4.1.1.2.2.2.1.1.cmml" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.1"></times><ci id="S3.Ex1.m1.4.4.1.1.2.2.2.1.2.cmml" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.2">𝑓</ci><list id="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.1.cmml" xref="S3.Ex1.m1.4.4.1.1.2.2.2.1.3.2"><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">𝑥</ci><ci id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3">𝑧</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\min_{x\in\mathbb{R}^{m}}F(x):=\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}}[f(x;z)]\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.p2.6" class="ltx_p">We assume an intermittent communication model as in e.g. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Woodworth et al.</span> [<a href="#bib.bib480" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">480</span></a>, Sec. 4.4]</cite>, where <math id="S3.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">M</annotation></semantics></math> stateless clients participate in each of <math id="S3.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS1.p2.2.m2.1a"><mi id="S3.SS2.SSS1.p2.2.m2.1.1" xref="S3.SS2.SSS1.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.2.m2.1b"><ci id="S3.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p2.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.2.m2.1c">T</annotation></semantics></math> rounds, and during each round, each client can compute gradients for <math id="S3.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.p2.3.m3.1a"><mi id="S3.SS2.SSS1.p2.3.m3.1.1" xref="S3.SS2.SSS1.p2.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p2.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.3.m3.1c">K</annotation></semantics></math> samples (e.g. minibatches) <math id="S3.SS2.SSS1.p2.4.m4.3" class="ltx_Math" alttext="z_{1},\dots,z_{K}" display="inline"><semantics id="S3.SS2.SSS1.p2.4.m4.3a"><mrow id="S3.SS2.SSS1.p2.4.m4.3.3.2" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.cmml"><msub id="S3.SS2.SSS1.p2.4.m4.2.2.1.1" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.cmml"><mi id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2.cmml">z</mi><mn id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.3" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.SSS1.p2.4.m4.3.3.2.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS1.p2.4.m4.1.1" xref="S3.SS2.SSS1.p2.4.m4.1.1.cmml">…</mi><mo id="S3.SS2.SSS1.p2.4.m4.3.3.2.4" xref="S3.SS2.SSS1.p2.4.m4.3.3.3.cmml">,</mo><msub id="S3.SS2.SSS1.p2.4.m4.3.3.2.2" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2.cmml"><mi id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.2" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2.2.cmml">z</mi><mi id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.3" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2.3.cmml">K</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.4.m4.3b"><list id="S3.SS2.SSS1.p2.4.m4.3.3.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.2"><apply id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.2">𝑧</ci><cn type="integer" id="S3.SS2.SSS1.p2.4.m4.2.2.1.1.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.2.2.1.1.3">1</cn></apply><ci id="S3.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.1.1">…</ci><apply id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.1.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2">subscript</csymbol><ci id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.2.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2.2">𝑧</ci><ci id="S3.SS2.SSS1.p2.4.m4.3.3.2.2.3.cmml" xref="S3.SS2.SSS1.p2.4.m4.3.3.2.2.3">𝐾</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.4.m4.3c">z_{1},\dots,z_{K}</annotation></semantics></math> sampled IID from <math id="S3.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS2.SSS1.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p2.5.m5.1.1" xref="S3.SS2.SSS1.p2.5.m5.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.5.m5.1b"><ci id="S3.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p2.5.m5.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.5.m5.1c">\mathcal{P}</annotation></semantics></math> (possibly using these to take sequential steps). In the IID-data setting clients are interchangeable, and we can without loss of generality assume <math id="S3.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="M=N" display="inline"><semantics id="S3.SS2.SSS1.p2.6.m6.1a"><mrow id="S3.SS2.SSS1.p2.6.m6.1.1" xref="S3.SS2.SSS1.p2.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.p2.6.m6.1.1.2" xref="S3.SS2.SSS1.p2.6.m6.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS1.p2.6.m6.1.1.1" xref="S3.SS2.SSS1.p2.6.m6.1.1.1.cmml">=</mo><mi id="S3.SS2.SSS1.p2.6.m6.1.1.3" xref="S3.SS2.SSS1.p2.6.m6.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.6.m6.1b"><apply id="S3.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1"><eq id="S3.SS2.SSS1.p2.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1.1"></eq><ci id="S3.SS2.SSS1.p2.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1.2">𝑀</ci><ci id="S3.SS2.SSS1.p2.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p2.6.m6.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.6.m6.1c">M=N</annotation></semantics></math>. Table <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a> summarizes the notation used in this section.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">Different assumptions on <math id="S3.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S3.SS2.SSS1.p3.1.m1.1a"><mi id="S3.SS2.SSS1.p3.1.m1.1.1" xref="S3.SS2.SSS1.p3.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p3.1.m1.1b"><ci id="S3.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p3.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p3.1.m1.1c">f</annotation></semantics></math> will produce different guarantees. We will first discuss the convex setting and later review results for non-convex problems.</p>
</div>
<section id="S3.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Baselines and state-of-the-art for convex problems</h5>

<div id="S3.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p1.7" class="ltx_p">In this section we review convergence results for <math id="S3.SS2.SSS1.Px1.p1.1.m1.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.1.m1.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.1.m1.1c">H</annotation></semantics></math>-smooth, convex (but not necessarily strongly convex) functions under the assumption that the variance of the stochastic gradients is bounded by <math id="S3.SS2.SSS1.Px1.p1.2.m2.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.2.m2.1a"><msup id="S3.SS2.SSS1.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml">σ</mi><mn id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.2">𝜎</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.2.m2.1c">\sigma^{2}</annotation></semantics></math>.
More formally, by <math id="S3.SS2.SSS1.Px1.p1.3.m3.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.3.m3.1a"><mi id="S3.SS2.SSS1.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.3.m3.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.3.m3.1c">H</annotation></semantics></math>-smooth we mean that for all <math id="S3.SS2.SSS1.Px1.p1.4.m4.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.Px1.p1.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.Px1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.4.m4.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.4.m4.1c">z</annotation></semantics></math>, <math id="S3.SS2.SSS1.Px1.p1.5.m5.2" class="ltx_Math" alttext="f(\cdot;z)" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.5.m5.2a"><mrow id="S3.SS2.SSS1.Px1.p1.5.m5.2.3" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.cmml"><mi id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.1.cmml">​</mo><mrow id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.2.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.1.cmml">(</mo><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml">⋅</mo><mo id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.2.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.1.cmml">;</mo><mi id="S3.SS2.SSS1.Px1.p1.5.m5.2.2" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.2.cmml">z</mi><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.2.3" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.5.m5.2b"><apply id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3"><times id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.1"></times><ci id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.2">𝑓</ci><list id="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.3.3.2"><ci id="S3.SS2.SSS1.Px1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.1.1">⋅</ci><ci id="S3.SS2.SSS1.Px1.p1.5.m5.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.5.m5.2.2">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.5.m5.2c">f(\cdot;z)</annotation></semantics></math> is differentiable and has a <math id="S3.SS2.SSS1.Px1.p1.6.m6.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.6.m6.1a"><mi id="S3.SS2.SSS1.Px1.p1.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.6.m6.1b"><ci id="S3.SS2.SSS1.Px1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.6.m6.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.6.m6.1c">H</annotation></semantics></math>-Lipschitz gradient, that is, for all choices of <math id="S3.SS2.SSS1.Px1.p1.7.m7.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.7.m7.2a"><mrow id="S3.SS2.SSS1.Px1.p1.7.m7.2.3.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.2.3.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.7.m7.1.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml">x</mi><mo id="S3.SS2.SSS1.Px1.p1.7.m7.2.3.2.1" xref="S3.SS2.SSS1.Px1.p1.7.m7.2.3.1.cmml">,</mo><mi id="S3.SS2.SSS1.Px1.p1.7.m7.2.2" xref="S3.SS2.SSS1.Px1.p1.7.m7.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.7.m7.2b"><list id="S3.SS2.SSS1.Px1.p1.7.m7.2.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.2.3.2"><ci id="S3.SS2.SSS1.Px1.p1.7.m7.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.1.1">𝑥</ci><ci id="S3.SS2.SSS1.Px1.p1.7.m7.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.7.m7.2.2">𝑦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.7.m7.2c">x,y</annotation></semantics></math></p>
<table id="S3.Ex2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex2.m1.5" class="ltx_Math" alttext="\|\nabla f(x,z)-\nabla f(y,z)\|\leq H\|x-y\|." display="block"><semantics id="S3.Ex2.m1.5a"><mrow id="S3.Ex2.m1.5.5.1" xref="S3.Ex2.m1.5.5.1.1.cmml"><mrow id="S3.Ex2.m1.5.5.1.1" xref="S3.Ex2.m1.5.5.1.1.cmml"><mrow id="S3.Ex2.m1.5.5.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.cmml"><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.cmml"><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.1.cmml">∇</mo><mi id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.5.5.1.1.1.1.1.2.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.2.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">x</mi><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.2.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2" xref="S3.Ex2.m1.2.2.cmml">z</mi><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.2.3" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.3" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.cmml"><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.cmml"><mo rspace="0.167em" id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.1.cmml">∇</mo><mi id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.5.5.1.1.1.1.1.3.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.1.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.2.1" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.1.cmml">(</mo><mi id="S3.Ex2.m1.3.3" xref="S3.Ex2.m1.3.3.cmml">y</mi><mo id="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.2.2" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.1.cmml">,</mo><mi id="S3.Ex2.m1.4.4" xref="S3.Ex2.m1.4.4.cmml">z</mi><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.2.3" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.1.1.3" xref="S3.Ex2.m1.5.5.1.1.1.2.1.cmml">‖</mo></mrow><mo id="S3.Ex2.m1.5.5.1.1.3" xref="S3.Ex2.m1.5.5.1.1.3.cmml">≤</mo><mrow id="S3.Ex2.m1.5.5.1.1.2" xref="S3.Ex2.m1.5.5.1.1.2.cmml"><mi id="S3.Ex2.m1.5.5.1.1.2.3" xref="S3.Ex2.m1.5.5.1.1.2.3.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.Ex2.m1.5.5.1.1.2.2" xref="S3.Ex2.m1.5.5.1.1.2.2.cmml">​</mo><mrow id="S3.Ex2.m1.5.5.1.1.2.1.1" xref="S3.Ex2.m1.5.5.1.1.2.1.2.cmml"><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.2.1.1.2" xref="S3.Ex2.m1.5.5.1.1.2.1.2.1.cmml">‖</mo><mrow id="S3.Ex2.m1.5.5.1.1.2.1.1.1" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.cmml"><mi id="S3.Ex2.m1.5.5.1.1.2.1.1.1.2" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.2.cmml">x</mi><mo id="S3.Ex2.m1.5.5.1.1.2.1.1.1.1" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.1.cmml">−</mo><mi id="S3.Ex2.m1.5.5.1.1.2.1.1.1.3" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.3.cmml">y</mi></mrow><mo stretchy="false" id="S3.Ex2.m1.5.5.1.1.2.1.1.3" xref="S3.Ex2.m1.5.5.1.1.2.1.2.1.cmml">‖</mo></mrow></mrow></mrow><mo lspace="0em" id="S3.Ex2.m1.5.5.1.2" xref="S3.Ex2.m1.5.5.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.5b"><apply id="S3.Ex2.m1.5.5.1.1.cmml" xref="S3.Ex2.m1.5.5.1"><leq id="S3.Ex2.m1.5.5.1.1.3.cmml" xref="S3.Ex2.m1.5.5.1.1.3"></leq><apply id="S3.Ex2.m1.5.5.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.5.5.1.1.1.2.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.2">norm</csymbol><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1"><minus id="S3.Ex2.m1.5.5.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.1"></minus><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2"><times id="S3.Ex2.m1.5.5.1.1.1.1.1.2.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.1"></times><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2"><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.1">∇</ci><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.2.2">𝑓</ci></apply><interval closure="open" id="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.2.3.2"><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝑥</ci><ci id="S3.Ex2.m1.2.2.cmml" xref="S3.Ex2.m1.2.2">𝑧</ci></interval></apply><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3"><times id="S3.Ex2.m1.5.5.1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.1"></times><apply id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2"><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.1">∇</ci><ci id="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.2.2">𝑓</ci></apply><interval closure="open" id="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex2.m1.5.5.1.1.1.1.1.3.3.2"><ci id="S3.Ex2.m1.3.3.cmml" xref="S3.Ex2.m1.3.3">𝑦</ci><ci id="S3.Ex2.m1.4.4.cmml" xref="S3.Ex2.m1.4.4">𝑧</ci></interval></apply></apply></apply><apply id="S3.Ex2.m1.5.5.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.2"><times id="S3.Ex2.m1.5.5.1.1.2.2.cmml" xref="S3.Ex2.m1.5.5.1.1.2.2"></times><ci id="S3.Ex2.m1.5.5.1.1.2.3.cmml" xref="S3.Ex2.m1.5.5.1.1.2.3">𝐻</ci><apply id="S3.Ex2.m1.5.5.1.1.2.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1"><csymbol cd="latexml" id="S3.Ex2.m1.5.5.1.1.2.1.2.1.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1.2">norm</csymbol><apply id="S3.Ex2.m1.5.5.1.1.2.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1"><minus id="S3.Ex2.m1.5.5.1.1.2.1.1.1.1.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.1"></minus><ci id="S3.Ex2.m1.5.5.1.1.2.1.1.1.2.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.2">𝑥</ci><ci id="S3.Ex2.m1.5.5.1.1.2.1.1.1.3.cmml" xref="S3.Ex2.m1.5.5.1.1.2.1.1.1.3">𝑦</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.5c">\|\nabla f(x,z)-\nabla f(y,z)\|\leq H\|x-y\|.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.Px1.p1.9" class="ltx_p">We also assume that for all <math id="S3.SS2.SSS1.Px1.p1.8.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.8.m1.1a"><mi id="S3.SS2.SSS1.Px1.p1.8.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.8.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.8.m1.1b"><ci id="S3.SS2.SSS1.Px1.p1.8.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.8.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.8.m1.1c">x</annotation></semantics></math>, the stochastic gradient <math id="S3.SS2.SSS1.Px1.p1.9.m2.2" class="ltx_Math" alttext="\nabla_{x}f(x;z)" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.9.m2.2a"><mrow id="S3.SS2.SSS1.Px1.p1.9.m2.2.3" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.cmml"><msub id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.cmml"><mo id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.2.cmml">∇</mo><mi id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.3" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.3.cmml">x</mi></msub><mi id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.1" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.1.cmml">​</mo><mrow id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.2.1" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.1.cmml">(</mo><mi id="S3.SS2.SSS1.Px1.p1.9.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.9.m2.1.1.cmml">x</mi><mo id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.2.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.1.cmml">;</mo><mi id="S3.SS2.SSS1.Px1.p1.9.m2.2.2" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.2.cmml">z</mi><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.2.3" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.9.m2.2b"><apply id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3"><times id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.1"></times><apply id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2"><apply id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.2">∇</ci><ci id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.1.3">𝑥</ci></apply><ci id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.2.2">𝑓</ci></apply><list id="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.3.3.2"><ci id="S3.SS2.SSS1.Px1.p1.9.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.1.1">𝑥</ci><ci id="S3.SS2.SSS1.Px1.p1.9.m2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.9.m2.2.2">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.9.m2.2c">\nabla_{x}f(x;z)</annotation></semantics></math> satisfies</p>
<table id="S3.Ex3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex3.m1.4" class="ltx_Math" alttext="\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}}\|\nabla_{x}f(x;z)-\nabla F(x)\|\leq\sigma^{2}." display="block"><semantics id="S3.Ex3.m1.4a"><mrow id="S3.Ex3.m1.4.4.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1" xref="S3.Ex3.m1.4.4.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.cmml"><munder id="S3.Ex3.m1.4.4.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.2.cmml"><mo id="S3.Ex3.m1.4.4.1.1.1.2.2" xref="S3.Ex3.m1.4.4.1.1.1.2.2.cmml">𝔼</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.2.3" xref="S3.Ex3.m1.4.4.1.1.1.2.3.cmml"><mi id="S3.Ex3.m1.4.4.1.1.1.2.3.2" xref="S3.Ex3.m1.4.4.1.1.1.2.3.2.cmml">z</mi><mo id="S3.Ex3.m1.4.4.1.1.1.2.3.1" xref="S3.Ex3.m1.4.4.1.1.1.2.3.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.Ex3.m1.4.4.1.1.1.2.3.3" xref="S3.Ex3.m1.4.4.1.1.1.2.3.3.cmml">𝒫</mi></mrow></munder><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.2.cmml"><mo lspace="0em" stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.cmml"><msub id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.2.cmml">∇</mo><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.3.cmml">x</mi></msub><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">f</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml"><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml">(</mo><mi id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml">x</mi><mo id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml">;</mo><mi id="S3.Ex3.m1.2.2" xref="S3.Ex3.m1.2.2.cmml">z</mi><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.2.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml"><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.cmml"><mo rspace="0.167em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.1.cmml">∇</mo><mi id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.3.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.3.2.1" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.Ex3.m1.3.3" xref="S3.Ex3.m1.3.3.cmml">x</mi><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.3.2.2" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex3.m1.4.4.1.1.1.1.1.3" xref="S3.Ex3.m1.4.4.1.1.1.1.2.1.cmml">‖</mo></mrow></mrow><mo id="S3.Ex3.m1.4.4.1.1.2" xref="S3.Ex3.m1.4.4.1.1.2.cmml">≤</mo><msup id="S3.Ex3.m1.4.4.1.1.3" xref="S3.Ex3.m1.4.4.1.1.3.cmml"><mi id="S3.Ex3.m1.4.4.1.1.3.2" xref="S3.Ex3.m1.4.4.1.1.3.2.cmml">σ</mi><mn id="S3.Ex3.m1.4.4.1.1.3.3" xref="S3.Ex3.m1.4.4.1.1.3.3.cmml">2</mn></msup></mrow><mo lspace="0em" id="S3.Ex3.m1.4.4.1.2" xref="S3.Ex3.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.4b"><apply id="S3.Ex3.m1.4.4.1.1.cmml" xref="S3.Ex3.m1.4.4.1"><leq id="S3.Ex3.m1.4.4.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.2"></leq><apply id="S3.Ex3.m1.4.4.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1"><apply id="S3.Ex3.m1.4.4.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2">subscript</csymbol><ci id="S3.Ex3.m1.4.4.1.1.1.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2.2">𝔼</ci><apply id="S3.Ex3.m1.4.4.1.1.1.2.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2.3"><csymbol cd="latexml" id="S3.Ex3.m1.4.4.1.1.1.2.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2.3.1">similar-to</csymbol><ci id="S3.Ex3.m1.4.4.1.1.1.2.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2.3.2">𝑧</ci><ci id="S3.Ex3.m1.4.4.1.1.1.2.3.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.2.3.3">𝒫</ci></apply></apply><apply id="S3.Ex3.m1.4.4.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex3.m1.4.4.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.2">norm</csymbol><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1"><minus id="S3.Ex3.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.1"></minus><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2"><times id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.1"></times><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2"><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1">subscript</csymbol><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.2">∇</ci><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.1.3">𝑥</ci></apply><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.2.2">𝑓</ci></apply><list id="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.2.3.2"><ci id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1">𝑥</ci><ci id="S3.Ex3.m1.2.2.cmml" xref="S3.Ex3.m1.2.2">𝑧</ci></list></apply><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3"><times id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.1"></times><apply id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2"><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.1.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.1">∇</ci><ci id="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.2.cmml" xref="S3.Ex3.m1.4.4.1.1.1.1.1.1.3.2.2">𝐹</ci></apply><ci id="S3.Ex3.m1.3.3.cmml" xref="S3.Ex3.m1.3.3">𝑥</ci></apply></apply></apply></apply><apply id="S3.Ex3.m1.4.4.1.1.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m1.4.4.1.1.3.1.cmml" xref="S3.Ex3.m1.4.4.1.1.3">superscript</csymbol><ci id="S3.Ex3.m1.4.4.1.1.3.2.cmml" xref="S3.Ex3.m1.4.4.1.1.3.2">𝜎</ci><cn type="integer" id="S3.Ex3.m1.4.4.1.1.3.3.cmml" xref="S3.Ex3.m1.4.4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.4c">\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}}\|\nabla_{x}f(x;z)-\nabla F(x)\|\leq\sigma^{2}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.Px1.p1.11" class="ltx_p">When analyzing the convergence rate of an algorithm with output <math id="S3.SS2.SSS1.Px1.p1.10.m1.1" class="ltx_Math" alttext="x_{T}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.10.m1.1a"><msub id="S3.SS2.SSS1.Px1.p1.10.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1.2.cmml">x</mi><mi id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1.3.cmml">T</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.10.m1.1b"><apply id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1.2">𝑥</ci><ci id="S3.SS2.SSS1.Px1.p1.10.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.10.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.10.m1.1c">x_{T}</annotation></semantics></math> after <math id="S3.SS2.SSS1.Px1.p1.11.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.11.m2.1a"><mi id="S3.SS2.SSS1.Px1.p1.11.m2.1.1" xref="S3.SS2.SSS1.Px1.p1.11.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.11.m2.1b"><ci id="S3.SS2.SSS1.Px1.p1.11.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.11.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.11.m2.1c">T</annotation></semantics></math> iterations, we consider the term</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="\operatorname*{\mathbb{E}}[F(x_{T})]-F(x^{*})" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.2.cmml"><mo rspace="0em" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">𝔼</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.2.cmml">[</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">T</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.2.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml">−</mo><mrow id="S3.E1.m1.3.3.2" xref="S3.E1.m1.3.3.2.cmml"><mi id="S3.E1.m1.3.3.2.3" xref="S3.E1.m1.3.3.2.3.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml">​</mo><mrow id="S3.E1.m1.3.3.2.1.1" xref="S3.E1.m1.3.3.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.3.3.2.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.cmml">(</mo><msup id="S3.E1.m1.3.3.2.1.1.1" xref="S3.E1.m1.3.3.2.1.1.1.cmml"><mi id="S3.E1.m1.3.3.2.1.1.1.2" xref="S3.E1.m1.3.3.2.1.1.1.2.cmml">x</mi><mo id="S3.E1.m1.3.3.2.1.1.1.3" xref="S3.E1.m1.3.3.2.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.E1.m1.3.3.2.1.1.3" xref="S3.E1.m1.3.3.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><minus id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"></minus><apply id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.1"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝔼</ci><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">𝐹</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">𝑇</ci></apply></apply></apply><apply id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2"><times id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2"></times><ci id="S3.E1.m1.3.3.2.3.cmml" xref="S3.E1.m1.3.3.2.3">𝐹</ci><apply id="S3.E1.m1.3.3.2.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.2.1.1.1.1.cmml" xref="S3.E1.m1.3.3.2.1.1">superscript</csymbol><ci id="S3.E1.m1.3.3.2.1.1.1.2.cmml" xref="S3.E1.m1.3.3.2.1.1.1.2">𝑥</ci><times id="S3.E1.m1.3.3.2.1.1.1.3.cmml" xref="S3.E1.m1.3.3.2.1.1.1.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\operatorname*{\mathbb{E}}[F(x_{T})]-F(x^{*})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.Px1.p1.12" class="ltx_p">where <math id="S3.SS2.SSS1.Px1.p1.12.m1.1" class="ltx_Math" alttext="x^{*}=\arg\min_{x}F(x)" display="inline"><semantics id="S3.SS2.SSS1.Px1.p1.12.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p1.12.m1.1.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.cmml"><msup id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.2.cmml">x</mi><mo id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.3" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.3.cmml">∗</mo></msup><mo id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.1.cmml">=</mo><mrow id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.cmml"><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.1.cmml">arg</mi><mo lspace="0.167em" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2a" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.cmml">⁡</mo><mrow id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.cmml"><msub id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.cmml"><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.2.cmml">min</mi><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.3" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.3.cmml">x</mi></msub><mo lspace="0.167em" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2a" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.cmml">⁡</mo><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.2.cmml">F</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.1.cmml">​</mo><mrow id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.3.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.cmml"><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.3.2.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.cmml">(</mo><mi id="S3.SS2.SSS1.Px1.p1.12.m1.1.1" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.3.2.2" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p1.12.m1.1b"><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2"><eq id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.1"></eq><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.2">𝑥</ci><times id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.2.3"></times></apply><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3"><times id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.1"></times><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2"><arg id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.1"></arg><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2"><apply id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1">subscript</csymbol><min id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.2"></min><ci id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.3.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.1.3">𝑥</ci></apply><ci id="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.2.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.2.3.2.2.2">𝐹</ci></apply></apply><ci id="S3.SS2.SSS1.Px1.p1.12.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p1.12.m1.1.1">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p1.12.m1.1c">x^{*}=\arg\min_{x}F(x)</annotation></semantics></math>. All convergence rates discussed herein are upper bounds on this term.
A summary of convergence results for such functions is given in Table <a href="#S3.T4" title="Table 4 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p2.5" class="ltx_p">Federated averaging (a.k.a. parallel SGD/local SGD) competes with two natural baselines: First, we may keep <math id="S3.SS2.SSS1.Px1.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p2.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.1.m1.1c">x</annotation></semantics></math> fixed in local updates during each round, and compute a total of <math id="S3.SS2.SSS1.Px1.p2.2.m2.1" class="ltx_Math" alttext="KM" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.2.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p2.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1"><times id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.2">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p2.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.2.m2.1c">KM</annotation></semantics></math> gradients at the current <math id="S3.SS2.SSS1.Px1.p2.3.m3.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.3.m3.1a"><mi id="S3.SS2.SSS1.Px1.p2.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.3.m3.1b"><ci id="S3.SS2.SSS1.Px1.p2.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.3.m3.1c">x</annotation></semantics></math>, in order to run accelerated minibatch SGD. Let <math id="S3.SS2.SSS1.Px1.p2.4.m4.1" class="ltx_Math" alttext="\bar{x}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.4.m4.1a"><mover accent="true" id="S3.SS2.SSS1.Px1.p2.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2.cmml">x</mi><mo id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1"><ci id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.1">¯</ci><ci id="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p2.4.m4.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.4.m4.1c">\bar{x}</annotation></semantics></math> denote the average of <math id="S3.SS2.SSS1.Px1.p2.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.5.m5.1a"><mi id="S3.SS2.SSS1.Px1.p2.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.5.m5.1b"><ci id="S3.SS2.SSS1.Px1.p2.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.5.m5.1c">T</annotation></semantics></math> iterations of this algorithm. We then have the upper bound</p>
<table id="S3.Ex4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex4.m1.1" class="ltx_Math" alttext="\mathcal{O}\left(\frac{H}{T^{2}}+\frac{\sigma}{\sqrt{TKM}}\right)" display="block"><semantics id="S3.Ex4.m1.1a"><mrow id="S3.Ex4.m1.1.1" xref="S3.Ex4.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex4.m1.1.1.3" xref="S3.Ex4.m1.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.2" xref="S3.Ex4.m1.1.1.2.cmml">​</mo><mrow id="S3.Ex4.m1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.cmml"><mo id="S3.Ex4.m1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex4.m1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.cmml"><mfrac id="S3.Ex4.m1.1.1.1.1.1.2" xref="S3.Ex4.m1.1.1.1.1.1.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.2.2" xref="S3.Ex4.m1.1.1.1.1.1.2.2.cmml">H</mi><msup id="S3.Ex4.m1.1.1.1.1.1.2.3" xref="S3.Ex4.m1.1.1.1.1.1.2.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.2.3.2" xref="S3.Ex4.m1.1.1.1.1.1.2.3.2.cmml">T</mi><mn id="S3.Ex4.m1.1.1.1.1.1.2.3.3" xref="S3.Ex4.m1.1.1.1.1.1.2.3.3.cmml">2</mn></msup></mfrac><mo id="S3.Ex4.m1.1.1.1.1.1.1" xref="S3.Ex4.m1.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.Ex4.m1.1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.3.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.3.2" xref="S3.Ex4.m1.1.1.1.1.1.3.2.cmml">σ</mi><msqrt id="S3.Ex4.m1.1.1.1.1.1.3.3" xref="S3.Ex4.m1.1.1.1.1.1.3.3.cmml"><mrow id="S3.Ex4.m1.1.1.1.1.1.3.3.2" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.cmml"><mi id="S3.Ex4.m1.1.1.1.1.1.3.3.2.2" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.3.3.2.1" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.Ex4.m1.1.1.1.1.1.3.3.2.3" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.Ex4.m1.1.1.1.1.1.3.3.2.1a" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.1.cmml">​</mo><mi id="S3.Ex4.m1.1.1.1.1.1.3.3.2.4" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.4.cmml">M</mi></mrow></msqrt></mfrac></mrow><mo id="S3.Ex4.m1.1.1.1.1.3" xref="S3.Ex4.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.1b"><apply id="S3.Ex4.m1.1.1.cmml" xref="S3.Ex4.m1.1.1"><times id="S3.Ex4.m1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.2"></times><ci id="S3.Ex4.m1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.3">𝒪</ci><apply id="S3.Ex4.m1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1"><plus id="S3.Ex4.m1.1.1.1.1.1.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.1"></plus><apply id="S3.Ex4.m1.1.1.1.1.1.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2"><divide id="S3.Ex4.m1.1.1.1.1.1.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2"></divide><ci id="S3.Ex4.m1.1.1.1.1.1.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2.2">𝐻</ci><apply id="S3.Ex4.m1.1.1.1.1.1.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex4.m1.1.1.1.1.1.2.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.Ex4.m1.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2.3.2">𝑇</ci><cn type="integer" id="S3.Ex4.m1.1.1.1.1.1.2.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.2.3.3">2</cn></apply></apply><apply id="S3.Ex4.m1.1.1.1.1.1.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3"><divide id="S3.Ex4.m1.1.1.1.1.1.3.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3"></divide><ci id="S3.Ex4.m1.1.1.1.1.1.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.2">𝜎</ci><apply id="S3.Ex4.m1.1.1.1.1.1.3.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3"><root id="S3.Ex4.m1.1.1.1.1.1.3.3a.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3"></root><apply id="S3.Ex4.m1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2"><times id="S3.Ex4.m1.1.1.1.1.1.3.3.2.1.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.1"></times><ci id="S3.Ex4.m1.1.1.1.1.1.3.3.2.2.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.2">𝑇</ci><ci id="S3.Ex4.m1.1.1.1.1.1.3.3.2.3.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.3">𝐾</ci><ci id="S3.Ex4.m1.1.1.1.1.1.3.3.2.4.cmml" xref="S3.Ex4.m1.1.1.1.1.1.3.3.2.4">𝑀</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.1c">\mathcal{O}\left(\frac{H}{T^{2}}+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.Px1.p2.6" class="ltx_p">for convex objectives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">294</span></a>, <a href="#bib.bib137" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">137</span></a>, <a href="#bib.bib151" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">151</span></a>]</cite>. Note that the first expectation is taken with respect to the randomness of <math id="S3.SS2.SSS1.Px1.p2.6.m1.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.SSS1.Px1.p2.6.m1.1a"><mi id="S3.SS2.SSS1.Px1.p2.6.m1.1.1" xref="S3.SS2.SSS1.Px1.p2.6.m1.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p2.6.m1.1b"><ci id="S3.SS2.SSS1.Px1.p2.6.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p2.6.m1.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p2.6.m1.1c">z</annotation></semantics></math> in the training procedure as well.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p3.2" class="ltx_p">A second natural baseline is to ignore all but 1 of the <math id="S3.SS2.SSS1.Px1.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p3.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p3.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p3.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p3.1.m1.1c">M</annotation></semantics></math> active clients, which allows (accelerated) sequential SGD to execute for <math id="S3.SS2.SSS1.Px1.p3.2.m2.1" class="ltx_Math" alttext="KT" display="inline"><semantics id="S3.SS2.SSS1.Px1.p3.2.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p3.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p3.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1"><times id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.2">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p3.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p3.2.m2.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p3.2.m2.1c">KT</annotation></semantics></math> steps. Applying the same general bounds cited above, this approach offers an upper bound of</p>
<table id="S3.Ex5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex5.m1.2" class="ltx_Math" alttext="\mathcal{O}\left(\frac{H}{(TK)^{2}}+\frac{\sigma}{\sqrt{TK}}\right)." display="block"><semantics id="S3.Ex5.m1.2a"><mrow id="S3.Ex5.m1.2.2.1" xref="S3.Ex5.m1.2.2.1.1.cmml"><mrow id="S3.Ex5.m1.2.2.1.1" xref="S3.Ex5.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex5.m1.2.2.1.1.3" xref="S3.Ex5.m1.2.2.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.1.1.2" xref="S3.Ex5.m1.2.2.1.1.2.cmml">​</mo><mrow id="S3.Ex5.m1.2.2.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.Ex5.m1.2.2.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex5.m1.2.2.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.cmml"><mfrac id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml"><mi id="S3.Ex5.m1.1.1.3" xref="S3.Ex5.m1.1.1.3.cmml">H</mi><msup id="S3.Ex5.m1.1.1.1" xref="S3.Ex5.m1.1.1.1.cmml"><mrow id="S3.Ex5.m1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m1.1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex5.m1.1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml"><mi id="S3.Ex5.m1.1.1.1.1.1.1.2" xref="S3.Ex5.m1.1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.1.1.1.1.1.1.1" xref="S3.Ex5.m1.1.1.1.1.1.1.1.cmml">​</mo><mi id="S3.Ex5.m1.1.1.1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1.1.1.3.cmml">K</mi></mrow><mo stretchy="false" id="S3.Ex5.m1.1.1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S3.Ex5.m1.1.1.1.3" xref="S3.Ex5.m1.1.1.1.3.cmml">2</mn></msup></mfrac><mo id="S3.Ex5.m1.2.2.1.1.1.1.1.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1.cmml">+</mo><mfrac id="S3.Ex5.m1.2.2.1.1.1.1.1.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.cmml"><mi id="S3.Ex5.m1.2.2.1.1.1.1.1.2.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.2.cmml">σ</mi><msqrt id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.cmml"><mrow id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.cmml"><mi id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.2" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.1" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.1.cmml">​</mo><mi id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.3.cmml">K</mi></mrow></msqrt></mfrac></mrow><mo id="S3.Ex5.m1.2.2.1.1.1.1.3" xref="S3.Ex5.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo lspace="0em" id="S3.Ex5.m1.2.2.1.2" xref="S3.Ex5.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.2b"><apply id="S3.Ex5.m1.2.2.1.1.cmml" xref="S3.Ex5.m1.2.2.1"><times id="S3.Ex5.m1.2.2.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.2"></times><ci id="S3.Ex5.m1.2.2.1.1.3.cmml" xref="S3.Ex5.m1.2.2.1.1.3">𝒪</ci><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1"><plus id="S3.Ex5.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.1"></plus><apply id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1"><divide id="S3.Ex5.m1.1.1.2.cmml" xref="S3.Ex5.m1.1.1"></divide><ci id="S3.Ex5.m1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.3">𝐻</ci><apply id="S3.Ex5.m1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex5.m1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1">superscript</csymbol><apply id="S3.Ex5.m1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1"><times id="S3.Ex5.m1.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.1"></times><ci id="S3.Ex5.m1.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.Ex5.m1.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.1.1.1.3">𝐾</ci></apply><cn type="integer" id="S3.Ex5.m1.1.1.1.3.cmml" xref="S3.Ex5.m1.1.1.1.3">2</cn></apply></apply><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2"><divide id="S3.Ex5.m1.2.2.1.1.1.1.1.2.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2"></divide><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.2.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.2">𝜎</ci><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3"><root id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3a.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3"></root><apply id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2"><times id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.1.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.1"></times><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.2.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.2">𝑇</ci><ci id="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.3.cmml" xref="S3.Ex5.m1.2.2.1.1.1.1.1.2.3.2.3">𝐾</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.2c">\mathcal{O}\left(\frac{H}{(TK)^{2}}+\frac{\sigma}{\sqrt{TK}}\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS1.Px1.p3.4" class="ltx_p">Comparing these two results, we see that minibatch SGD attains the optimal ‘statistical’ term (<math id="S3.SS2.SSS1.Px1.p3.3.m1.1" class="ltx_Math" alttext="\nicefrac{{\sigma}}{{\sqrt{TKM}}}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p3.3.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p3.3.m1.1.1" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2.cmml"><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2a" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2.cmml">σ</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1a" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1.cmml">/</mo></mpadded><msqrt id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.cmml"><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.2" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.3" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1a" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.4" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.4.cmml">M</mi></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p3.3.m1.1b"><apply id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1"><divide id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.1"></divide><ci id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.2">𝜎</ci><apply id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3"><root id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3a.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3"></root><apply id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2"><times id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.1"></times><ci id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.3.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.3">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.4.cmml" xref="S3.SS2.SSS1.Px1.p3.3.m1.1.1.3.2.4">𝑀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p3.3.m1.1c">\nicefrac{{\sigma}}{{\sqrt{TKM}}}</annotation></semantics></math>), whilst SGD on a single device (ignoring the updates of the other devices) achieves the optimal ‘optimization’ term (<math id="S3.SS2.SSS1.Px1.p3.4.m2.1" class="ltx_Math" alttext="\nicefrac{{H}}{{(TK)^{2}}}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p3.4.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p3.4.m2.1.1" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.cmml"><mpadded voffset="0.3em" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3.cmml"><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3a" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3.cmml">H</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2.cmml"><mo stretchy="true" symmetric="true" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2a" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2.cmml">/</mo></mpadded><msup id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.cmml"><mrow id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.cmml"><mo lspace="0.222em" maxsize="70%" minsize="70%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.2" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.cmml"><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.2" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.1" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.1.cmml">​</mo><mi mathsize="70%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.3" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.3.cmml">K</mi></mrow><mo maxsize="70%" minsize="70%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.3" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mn mathsize="71%" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.3" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p3.4.m2.1b"><apply id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1"><divide id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.2"></divide><ci id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.3">𝐻</ci><apply id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1"><times id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.1.1.1.3">𝐾</ci></apply><cn type="integer" id="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p3.4.m2.1.1.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p3.4.m2.1c">\nicefrac{{H}}{{(TK)^{2}}}</annotation></semantics></math>).</p>
</div>
<div id="S3.SS2.SSS1.Px1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p4.1" class="ltx_p">The convergence analysis of local-update SGD methods is an active current area of research <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib434" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">434</span></a>, <a href="#bib.bib310" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">310</span></a>, <a href="#bib.bib500" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">500</span></a>, <a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>, <a href="#bib.bib390" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">390</span></a>, <a href="#bib.bib371" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">371</span></a>, <a href="#bib.bib269" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">269</span></a>, <a href="#bib.bib481" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">481</span></a>]</cite>.
The first convergence results for local-update SGD methods were derived under the bounded gradient norm assumption in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Stich</span> [<a href="#bib.bib434" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">434</span></a>]</cite> for strongly-convex and in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib500" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">500</span></a>]</cite> for non-convex objective functions. These analyses could attain the desired <math id="S3.SS2.SSS1.Px1.p4.1.m1.1" class="ltx_Math" alttext="\sigma/\sqrt{TKM}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p4.1.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p4.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.2.cmml">σ</mi><mo id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.1" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.1.cmml">/</mo><msqrt id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.2" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.3" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1a" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.4" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.4.cmml">M</mi></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p4.1.m1.1b"><apply id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1"><divide id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.1"></divide><ci id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.2">𝜎</ci><apply id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3"><root id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3"></root><apply id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2"><times id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.1"></times><ci id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.3">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.4.cmml" xref="S3.SS2.SSS1.Px1.p4.1.m1.1.1.3.2.4">𝑀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p4.1.m1.1c">\sigma/\sqrt{TKM}</annotation></semantics></math> statistical term with suboptimal optimization term (in Table <a href="#S3.T4" title="Table 4 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we summarize these results for the middle ground of convex functions).</p>
</div>
<div id="S3.SS2.SSS1.Px1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p5.6" class="ltx_p">By removing the bounded gradient assumption, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Stich and Karimireddy</span> [<a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite> could further improve the optimization term to <math id="S3.SS2.SSS1.Px1.p5.1.m1.1" class="ltx_Math" alttext="HM/T" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.1.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p5.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.cmml"><mrow id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.2" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.1" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.3" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.3.cmml">M</mi></mrow><mo id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.1" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.1.cmml">/</mo><mi id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.3.cmml">T</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.1.m1.1b"><apply id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1"><divide id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.1"></divide><apply id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2"><times id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.1"></times><ci id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.2">𝐻</ci><ci id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.2.3">𝑀</ci></apply><ci id="S3.SS2.SSS1.Px1.p5.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p5.1.m1.1.1.3">𝑇</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.1.m1.1c">HM/T</annotation></semantics></math>. These result show that if the number of local steps <math id="S3.SS2.SSS1.Px1.p5.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.2.m2.1a"><mi id="S3.SS2.SSS1.Px1.p5.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p5.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.2.m2.1b"><ci id="S3.SS2.SSS1.Px1.p5.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.2.m2.1c">K</annotation></semantics></math> is smaller than <math id="S3.SS2.SSS1.Px1.p5.3.m3.1" class="ltx_Math" alttext="T/M^{3}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.3.m3.1a"><mrow id="S3.SS2.SSS1.Px1.p5.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.2" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.1" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.1.cmml">/</mo><msup id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.2" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.2.cmml">M</mi><mn id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.3" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.3.m3.1b"><apply id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1"><divide id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.1"></divide><ci id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.2">𝑇</ci><apply id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.2">𝑀</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p5.3.m3.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.3.m3.1c">T/M^{3}</annotation></semantics></math> then the (optimal) statistical term is dominating the rate. However, for typical cross-device applications we might have <math id="S3.SS2.SSS1.Px1.p5.4.m4.1" class="ltx_Math" alttext="T=10^{6}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.4.m4.1a"><mrow id="S3.SS2.SSS1.Px1.p5.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.1" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.1.cmml">=</mo><msup id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.cmml"><mn id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.2" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.2.cmml">10</mn><mn id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.3" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1"><eq id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.1"></eq><ci id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.2">10</cn><cn type="integer" id="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p5.4.m4.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.4.m4.1c">T=10^{6}</annotation></semantics></math> and <math id="S3.SS2.SSS1.Px1.p5.5.m5.1" class="ltx_Math" alttext="M=100" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.5.m5.1a"><mrow id="S3.SS2.SSS1.Px1.p5.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.2" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.1" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.3" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.5.m5.1b"><apply id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1"><eq id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.1"></eq><ci id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.2">𝑀</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p5.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p5.5.m5.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.5.m5.1c">M=100</annotation></semantics></math> (<a href="#S1.T2" title="In 1.1.1 The Lifecycle of a Model in Federated Learning ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>), implying <math id="S3.SS2.SSS1.Px1.p5.6.m6.1" class="ltx_Math" alttext="K=1" display="inline"><semantics id="S3.SS2.SSS1.Px1.p5.6.m6.1a"><mrow id="S3.SS2.SSS1.Px1.p5.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.2" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.2.cmml">K</mi><mo id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.1" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.3" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p5.6.m6.1b"><apply id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1"><eq id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.1"></eq><ci id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.2">𝐾</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p5.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p5.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p5.6.m6.1c">K=1</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p6" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p6.1" class="ltx_p">Often in the literature the convergence bounds are accompanied by a discussion on how large <math id="S3.SS2.SSS1.Px1.p6.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p6.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p6.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p6.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p6.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p6.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p6.1.m1.1c">K</annotation></semantics></math> may be chosen in order to reach asymptotically the same statistical term as the convergence rate of mini-batch SGD. For strongly convex functions, this bound was improved by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Khaled et al.</span> [<a href="#bib.bib269" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">269</span></a>]</cite> and further in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Stich and Karimireddy</span> [<a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.17" class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" style="width:433.6pt;">
<table id="S3.T4.17.17" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.17.17.18" class="ltx_tr">
<td id="S3.T4.17.17.18.1" class="ltx_td ltx_border_tt"></td>
<td id="S3.T4.17.17.18.2" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="S3.T4.17.17.18.3" class="ltx_td ltx_align_left ltx_border_tt">Comments</td>
<td id="S3.T4.17.17.18.4" class="ltx_td ltx_align_left ltx_border_tt" colspan="2">Convergence</td>
</tr>
<tr id="S3.T4.17.17.19" class="ltx_tr">
<td id="S3.T4.17.17.19.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="S3.T4.17.17.19.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Baselines</span></td>
<td id="S3.T4.17.17.19.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T4.3.3.3" class="ltx_tr">
<td id="S3.T4.3.3.3.4" class="ltx_td"></td>
<td id="S3.T4.3.3.3.5" class="ltx_td ltx_align_left">mini-batch SGD</td>
<td id="S3.T4.1.1.1.1" class="ltx_td ltx_align_left">batch size <math id="S3.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="KM" display="inline"><semantics id="S3.T4.1.1.1.1.m1.1a"><mrow id="S3.T4.1.1.1.1.m1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.cmml"><mi id="S3.T4.1.1.1.1.m1.1.1.2" xref="S3.T4.1.1.1.1.m1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.1.1.1.1.m1.1.1.1" xref="S3.T4.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.T4.1.1.1.1.m1.1.1.3" xref="S3.T4.1.1.1.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><apply id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1"><times id="S3.T4.1.1.1.1.m1.1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1.1"></times><ci id="S3.T4.1.1.1.1.m1.1.1.2.cmml" xref="S3.T4.1.1.1.1.m1.1.1.2">𝐾</ci><ci id="S3.T4.1.1.1.1.m1.1.1.3.cmml" xref="S3.T4.1.1.1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">KM</annotation></semantics></math>
</td>
<td id="S3.T4.2.2.2.2" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.2.2.2.2.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{H}{T}\right." display="inline"><semantics id="S3.T4.2.2.2.2.m1.1a"><mrow id="S3.T4.2.2.2.2.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.2.2.2.2.m1.1.1">𝒪</mi><mrow id="S3.T4.2.2.2.2.m1.1.2"><mo id="S3.T4.2.2.2.2.m1.1.2.1">(</mo><mfrac id="S3.T4.2.2.2.2.m1.1.2.2"><mi id="S3.T4.2.2.2.2.m1.1.2.2.2">H</mi><mi id="S3.T4.2.2.2.2.m1.1.2.2.3">T</mi></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.2.2.2.2.m1.1c">\mathcal{O}\left(\frac{H}{T}\right.</annotation></semantics></math></td>
<td id="S3.T4.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.3.3.3.3.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TKM}}\right)" display="inline"><semantics id="S3.T4.3.3.3.3.m1.1a"><mrow id="S3.T4.3.3.3.3.m1.1b"><mo id="S3.T4.3.3.3.3.m1.1.1">+</mo><mfrac id="S3.T4.3.3.3.3.m1.1.2"><mi id="S3.T4.3.3.3.3.m1.1.2.2">σ</mi><msqrt id="S3.T4.3.3.3.3.m1.1.2.3"><mrow id="S3.T4.3.3.3.3.m1.1.2.3.2"><mi id="S3.T4.3.3.3.3.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.3.3.3.3.m1.1.2.3.2.1">​</mo><mi id="S3.T4.3.3.3.3.m1.1.2.3.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.3.3.3.3.m1.1.2.3.2.1a">​</mo><mi id="S3.T4.3.3.3.3.m1.1.2.3.2.4">M</mi></mrow></msqrt></mfrac><mo id="S3.T4.3.3.3.3.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.3.3.3.3.m1.1c">\left.+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.5.5.5" class="ltx_tr">
<td id="S3.T4.5.5.5.3" class="ltx_td"></td>
<td id="S3.T4.5.5.5.4" class="ltx_td ltx_align_left">SGD</td>
<td id="S3.T4.5.5.5.5" class="ltx_td ltx_align_left">(on 1 worker, no communication)</td>
<td id="S3.T4.4.4.4.1" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.4.4.4.1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{H}{TK}\right." display="inline"><semantics id="S3.T4.4.4.4.1.m1.1a"><mrow id="S3.T4.4.4.4.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.4.4.4.1.m1.1.1">𝒪</mi><mrow id="S3.T4.4.4.4.1.m1.1.2"><mo id="S3.T4.4.4.4.1.m1.1.2.1">(</mo><mfrac id="S3.T4.4.4.4.1.m1.1.2.2"><mi id="S3.T4.4.4.4.1.m1.1.2.2.2">H</mi><mrow id="S3.T4.4.4.4.1.m1.1.2.2.3"><mi id="S3.T4.4.4.4.1.m1.1.2.2.3.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.4.4.4.1.m1.1.2.2.3.1">​</mo><mi id="S3.T4.4.4.4.1.m1.1.2.2.3.3">K</mi></mrow></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.4.4.4.1.m1.1c">\mathcal{O}\left(\frac{H}{TK}\right.</annotation></semantics></math></td>
<td id="S3.T4.5.5.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.5.5.5.2.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TK}}\right)" display="inline"><semantics id="S3.T4.5.5.5.2.m1.1a"><mrow id="S3.T4.5.5.5.2.m1.1b"><mo id="S3.T4.5.5.5.2.m1.1.1">+</mo><mfrac id="S3.T4.5.5.5.2.m1.1.2"><mi id="S3.T4.5.5.5.2.m1.1.2.2">σ</mi><msqrt id="S3.T4.5.5.5.2.m1.1.2.3"><mrow id="S3.T4.5.5.5.2.m1.1.2.3.2"><mi id="S3.T4.5.5.5.2.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.5.5.5.2.m1.1.2.3.2.1">​</mo><mi id="S3.T4.5.5.5.2.m1.1.2.3.2.3">K</mi></mrow></msqrt></mfrac><mo id="S3.T4.5.5.5.2.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.5.5.5.2.m1.1c">\left.+\frac{\sigma}{\sqrt{TK}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.17.17.20" class="ltx_tr">
<td id="S3.T4.17.17.20.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="S3.T4.17.17.20.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Baselines with acceleration<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note"><span id="footnote7.1.1.1" class="ltx_text ltx_font_medium ltx_font_upright" style="font-size:111%;">7</span></span><span id="footnote7.9" class="ltx_text ltx_font_medium ltx_font_upright" style="font-size:111%;">There are no accelerated fed-avg/local SGD variants so far</span></span></span></span></span></td>
<td id="S3.T4.17.17.20.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T4.8.8.8" class="ltx_tr">
<td id="S3.T4.8.8.8.4" class="ltx_td"></td>
<td id="S3.T4.8.8.8.5" class="ltx_td ltx_align_left">A-mini-batch SGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">294</span></a>, <a href="#bib.bib137" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">137</span></a>]</cite>
</td>
<td id="S3.T4.6.6.6.1" class="ltx_td ltx_align_left">batch size <math id="S3.T4.6.6.6.1.m1.1" class="ltx_Math" alttext="KM" display="inline"><semantics id="S3.T4.6.6.6.1.m1.1a"><mrow id="S3.T4.6.6.6.1.m1.1.1" xref="S3.T4.6.6.6.1.m1.1.1.cmml"><mi id="S3.T4.6.6.6.1.m1.1.1.2" xref="S3.T4.6.6.6.1.m1.1.1.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.6.6.6.1.m1.1.1.1" xref="S3.T4.6.6.6.1.m1.1.1.1.cmml">​</mo><mi id="S3.T4.6.6.6.1.m1.1.1.3" xref="S3.T4.6.6.6.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.6.1.m1.1b"><apply id="S3.T4.6.6.6.1.m1.1.1.cmml" xref="S3.T4.6.6.6.1.m1.1.1"><times id="S3.T4.6.6.6.1.m1.1.1.1.cmml" xref="S3.T4.6.6.6.1.m1.1.1.1"></times><ci id="S3.T4.6.6.6.1.m1.1.1.2.cmml" xref="S3.T4.6.6.6.1.m1.1.1.2">𝐾</ci><ci id="S3.T4.6.6.6.1.m1.1.1.3.cmml" xref="S3.T4.6.6.6.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.6.1.m1.1c">KM</annotation></semantics></math>
</td>
<td id="S3.T4.7.7.7.2" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.7.7.7.2.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{H}{T^{2}}\right." display="inline"><semantics id="S3.T4.7.7.7.2.m1.1a"><mrow id="S3.T4.7.7.7.2.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.7.7.7.2.m1.1.1">𝒪</mi><mrow id="S3.T4.7.7.7.2.m1.1.2"><mo id="S3.T4.7.7.7.2.m1.1.2.1">(</mo><mfrac id="S3.T4.7.7.7.2.m1.1.2.2"><mi id="S3.T4.7.7.7.2.m1.1.2.2.2">H</mi><msup id="S3.T4.7.7.7.2.m1.1.2.2.3"><mi id="S3.T4.7.7.7.2.m1.1.2.2.3.2">T</mi><mn id="S3.T4.7.7.7.2.m1.1.2.2.3.3">2</mn></msup></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.7.7.7.2.m1.1c">\mathcal{O}\left(\frac{H}{T^{2}}\right.</annotation></semantics></math></td>
<td id="S3.T4.8.8.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.8.8.8.3.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TKM}}\right)" display="inline"><semantics id="S3.T4.8.8.8.3.m1.1a"><mrow id="S3.T4.8.8.8.3.m1.1b"><mo id="S3.T4.8.8.8.3.m1.1.1">+</mo><mfrac id="S3.T4.8.8.8.3.m1.1.2"><mi id="S3.T4.8.8.8.3.m1.1.2.2">σ</mi><msqrt id="S3.T4.8.8.8.3.m1.1.2.3"><mrow id="S3.T4.8.8.8.3.m1.1.2.3.2"><mi id="S3.T4.8.8.8.3.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.8.8.8.3.m1.1.2.3.2.1">​</mo><mi id="S3.T4.8.8.8.3.m1.1.2.3.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.8.8.8.3.m1.1.2.3.2.1a">​</mo><mi id="S3.T4.8.8.8.3.m1.1.2.3.2.4">M</mi></mrow></msqrt></mfrac><mo id="S3.T4.8.8.8.3.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.8.8.8.3.m1.1c">\left.+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.10.10.10" class="ltx_tr">
<td id="S3.T4.10.10.10.3" class="ltx_td"></td>
<td id="S3.T4.10.10.10.4" class="ltx_td ltx_align_left">A-SGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib294" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">294</span></a>]</cite>
</td>
<td id="S3.T4.10.10.10.5" class="ltx_td ltx_align_left">(on 1 worker, no communication)</td>
<td id="S3.T4.9.9.9.1" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.9.9.9.1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{H}{(TK)^{2}}\right." display="inline"><semantics id="S3.T4.9.9.9.1.m1.1a"><mrow id="S3.T4.9.9.9.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.9.9.9.1.m1.1.2">𝒪</mi><mrow id="S3.T4.9.9.9.1.m1.1.3"><mo id="S3.T4.9.9.9.1.m1.1.3.1">(</mo><mfrac id="S3.T4.9.9.9.1.m1.1.1"><mi id="S3.T4.9.9.9.1.m1.1.1.3">H</mi><msup id="S3.T4.9.9.9.1.m1.1.1.1"><mrow id="S3.T4.9.9.9.1.m1.1.1.1.1.1"><mo stretchy="false" id="S3.T4.9.9.9.1.m1.1.1.1.1.1.2">(</mo><mrow id="S3.T4.9.9.9.1.m1.1.1.1.1.1.1"><mi id="S3.T4.9.9.9.1.m1.1.1.1.1.1.1.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.9.9.9.1.m1.1.1.1.1.1.1.1">​</mo><mi id="S3.T4.9.9.9.1.m1.1.1.1.1.1.1.3">K</mi></mrow><mo stretchy="false" id="S3.T4.9.9.9.1.m1.1.1.1.1.1.3">)</mo></mrow><mn id="S3.T4.9.9.9.1.m1.1.1.1.3">2</mn></msup></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.9.9.9.1.m1.1c">\mathcal{O}\left(\frac{H}{(TK)^{2}}\right.</annotation></semantics></math></td>
<td id="S3.T4.10.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.10.10.10.2.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TK}}\right)" display="inline"><semantics id="S3.T4.10.10.10.2.m1.1a"><mrow id="S3.T4.10.10.10.2.m1.1b"><mo id="S3.T4.10.10.10.2.m1.1.1">+</mo><mfrac id="S3.T4.10.10.10.2.m1.1.2"><mi id="S3.T4.10.10.10.2.m1.1.2.2">σ</mi><msqrt id="S3.T4.10.10.10.2.m1.1.2.3"><mrow id="S3.T4.10.10.10.2.m1.1.2.3.2"><mi id="S3.T4.10.10.10.2.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.10.10.10.2.m1.1.2.3.2.1">​</mo><mi id="S3.T4.10.10.10.2.m1.1.2.3.2.3">K</mi></mrow></msqrt></mfrac><mo id="S3.T4.10.10.10.2.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.10.10.10.2.m1.1c">\left.+\frac{\sigma}{\sqrt{TK}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.17.17.21" class="ltx_tr">
<td id="S3.T4.17.17.21.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="S3.T4.17.17.21.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Parallel SGD / Fed-Avg / Local SGD</span></td>
<td id="S3.T4.17.17.21.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T4.13.13.13" class="ltx_tr">
<td id="S3.T4.13.13.13.4" class="ltx_td"></td>
<td id="S3.T4.13.13.13.5" class="ltx_td ltx_align_left">
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib500" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">500</span></a>]</cite><span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>This paper considers the smooth non-convex setting, we adapt here the results for our setting.</span></span></span>, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Stich</span> [<a href="#bib.bib434" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">434</span></a>]</cite><span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>This paper considers the smooth strongly convex setting, we adapt here the results for our setting.</span></span></span>
</td>
<td id="S3.T4.11.11.11.1" class="ltx_td ltx_align_left">gradient norm bounded by <math id="S3.T4.11.11.11.1.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="S3.T4.11.11.11.1.m1.1a"><mi id="S3.T4.11.11.11.1.m1.1.1" xref="S3.T4.11.11.11.1.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.T4.11.11.11.1.m1.1b"><ci id="S3.T4.11.11.11.1.m1.1.1.cmml" xref="S3.T4.11.11.11.1.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.11.11.11.1.m1.1c">G</annotation></semantics></math>
</td>
<td id="S3.T4.12.12.12.2" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.12.12.12.2.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{HKM}{T}\frac{G^{2}}{\sigma^{2}}\right." display="inline"><semantics id="S3.T4.12.12.12.2.m1.1a"><mrow id="S3.T4.12.12.12.2.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.12.12.12.2.m1.1.1">𝒪</mi><mrow id="S3.T4.12.12.12.2.m1.1.2"><mo id="S3.T4.12.12.12.2.m1.1.2.1">(</mo><mfrac id="S3.T4.12.12.12.2.m1.1.2.2"><mrow id="S3.T4.12.12.12.2.m1.1.2.2.2"><mi id="S3.T4.12.12.12.2.m1.1.2.2.2.2">H</mi><mo lspace="0em" rspace="0em" id="S3.T4.12.12.12.2.m1.1.2.2.2.1">​</mo><mi id="S3.T4.12.12.12.2.m1.1.2.2.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.12.12.12.2.m1.1.2.2.2.1a">​</mo><mi id="S3.T4.12.12.12.2.m1.1.2.2.2.4">M</mi></mrow><mi id="S3.T4.12.12.12.2.m1.1.2.2.3">T</mi></mfrac><mfrac id="S3.T4.12.12.12.2.m1.1.2.3"><msup id="S3.T4.12.12.12.2.m1.1.2.3.2"><mi id="S3.T4.12.12.12.2.m1.1.2.3.2.2">G</mi><mn id="S3.T4.12.12.12.2.m1.1.2.3.2.3">2</mn></msup><msup id="S3.T4.12.12.12.2.m1.1.2.3.3"><mi id="S3.T4.12.12.12.2.m1.1.2.3.3.2">σ</mi><mn id="S3.T4.12.12.12.2.m1.1.2.3.3.3">2</mn></msup></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.12.12.12.2.m1.1c">\mathcal{O}\left(\frac{HKM}{T}\frac{G^{2}}{\sigma^{2}}\right.</annotation></semantics></math></td>
<td id="S3.T4.13.13.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.13.13.13.3.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TKM}}\right)" display="inline"><semantics id="S3.T4.13.13.13.3.m1.1a"><mrow id="S3.T4.13.13.13.3.m1.1b"><mo id="S3.T4.13.13.13.3.m1.1.1">+</mo><mfrac id="S3.T4.13.13.13.3.m1.1.2"><mi id="S3.T4.13.13.13.3.m1.1.2.2">σ</mi><msqrt id="S3.T4.13.13.13.3.m1.1.2.3"><mrow id="S3.T4.13.13.13.3.m1.1.2.3.2"><mi id="S3.T4.13.13.13.3.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.13.13.13.3.m1.1.2.3.2.1">​</mo><mi id="S3.T4.13.13.13.3.m1.1.2.3.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.13.13.13.3.m1.1.2.3.2.1a">​</mo><mi id="S3.T4.13.13.13.3.m1.1.2.3.2.4">M</mi></mrow></msqrt></mfrac><mo id="S3.T4.13.13.13.3.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.13.13.13.3.m1.1c">\left.+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.15.15.15" class="ltx_tr">
<td id="S3.T4.15.15.15.3" class="ltx_td"></td>
<td id="S3.T4.15.15.15.4" class="ltx_td ltx_align_left" colspan="2">
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite><a href="#footnote8" title="Footnote 8 ‣ Table 4 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Stich and Karimireddy</span> [<a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite>
</td>
<td id="S3.T4.14.14.14.1" class="ltx_td ltx_nopad_r ltx_align_left"><math id="S3.T4.14.14.14.1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{HM}{T}\right." display="inline"><semantics id="S3.T4.14.14.14.1.m1.1a"><mrow id="S3.T4.14.14.14.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.14.14.14.1.m1.1.1">𝒪</mi><mrow id="S3.T4.14.14.14.1.m1.1.2"><mo id="S3.T4.14.14.14.1.m1.1.2.1">(</mo><mfrac id="S3.T4.14.14.14.1.m1.1.2.2"><mrow id="S3.T4.14.14.14.1.m1.1.2.2.2"><mi id="S3.T4.14.14.14.1.m1.1.2.2.2.2">H</mi><mo lspace="0em" rspace="0em" id="S3.T4.14.14.14.1.m1.1.2.2.2.1">​</mo><mi id="S3.T4.14.14.14.1.m1.1.2.2.2.3">M</mi></mrow><mi id="S3.T4.14.14.14.1.m1.1.2.2.3">T</mi></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.14.14.14.1.m1.1c">\mathcal{O}\left(\frac{HM}{T}\right.</annotation></semantics></math></td>
<td id="S3.T4.15.15.15.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left"><math id="S3.T4.15.15.15.2.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TKM}}\right)" display="inline"><semantics id="S3.T4.15.15.15.2.m1.1a"><mrow id="S3.T4.15.15.15.2.m1.1b"><mo id="S3.T4.15.15.15.2.m1.1.1">+</mo><mfrac id="S3.T4.15.15.15.2.m1.1.2"><mi id="S3.T4.15.15.15.2.m1.1.2.2">σ</mi><msqrt id="S3.T4.15.15.15.2.m1.1.2.3"><mrow id="S3.T4.15.15.15.2.m1.1.2.3.2"><mi id="S3.T4.15.15.15.2.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.15.15.15.2.m1.1.2.3.2.1">​</mo><mi id="S3.T4.15.15.15.2.m1.1.2.3.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.15.15.15.2.m1.1.2.3.2.1a">​</mo><mi id="S3.T4.15.15.15.2.m1.1.2.3.2.4">M</mi></mrow></msqrt></mfrac><mo id="S3.T4.15.15.15.2.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.15.15.15.2.m1.1c">\left.+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
</tr>
<tr id="S3.T4.17.17.22" class="ltx_tr">
<td id="S3.T4.17.17.22.1" class="ltx_td ltx_align_left ltx_border_t" colspan="4"><span id="S3.T4.17.17.22.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Other algorithms</span></td>
<td id="S3.T4.17.17.22.2" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S3.T4.17.17.17" class="ltx_tr">
<td id="S3.T4.17.17.17.3" class="ltx_td ltx_border_bb"></td>
<td id="S3.T4.17.17.17.4" class="ltx_td ltx_align_left ltx_border_bb">SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite>
</td>
<td id="S3.T4.17.17.17.5" class="ltx_td ltx_align_left ltx_border_bb">control variates and two stepsizes</td>
<td id="S3.T4.16.16.16.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb"><math id="S3.T4.16.16.16.1.m1.1" class="ltx_math_unparsed" alttext="\mathcal{O}\left(\frac{H}{T}\right." display="inline"><semantics id="S3.T4.16.16.16.1.m1.1a"><mrow id="S3.T4.16.16.16.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S3.T4.16.16.16.1.m1.1.1">𝒪</mi><mrow id="S3.T4.16.16.16.1.m1.1.2"><mo id="S3.T4.16.16.16.1.m1.1.2.1">(</mo><mfrac id="S3.T4.16.16.16.1.m1.1.2.2"><mi id="S3.T4.16.16.16.1.m1.1.2.2.2">H</mi><mi id="S3.T4.16.16.16.1.m1.1.2.2.3">T</mi></mfrac></mrow></mrow><annotation encoding="application/x-tex" id="S3.T4.16.16.16.1.m1.1c">\mathcal{O}\left(\frac{H}{T}\right.</annotation></semantics></math></td>
<td id="S3.T4.17.17.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb"><math id="S3.T4.17.17.17.2.m1.1" class="ltx_math_unparsed" alttext="\left.+\frac{\sigma}{\sqrt{TKM}}\right)" display="inline"><semantics id="S3.T4.17.17.17.2.m1.1a"><mrow id="S3.T4.17.17.17.2.m1.1b"><mo id="S3.T4.17.17.17.2.m1.1.1">+</mo><mfrac id="S3.T4.17.17.17.2.m1.1.2"><mi id="S3.T4.17.17.17.2.m1.1.2.2">σ</mi><msqrt id="S3.T4.17.17.17.2.m1.1.2.3"><mrow id="S3.T4.17.17.17.2.m1.1.2.3.2"><mi id="S3.T4.17.17.17.2.m1.1.2.3.2.2">T</mi><mo lspace="0em" rspace="0em" id="S3.T4.17.17.17.2.m1.1.2.3.2.1">​</mo><mi id="S3.T4.17.17.17.2.m1.1.2.3.2.3">K</mi><mo lspace="0em" rspace="0em" id="S3.T4.17.17.17.2.m1.1.2.3.2.1a">​</mo><mi id="S3.T4.17.17.17.2.m1.1.2.3.2.4">M</mi></mrow></msqrt></mfrac><mo id="S3.T4.17.17.17.2.m1.1.3">)</mo></mrow><annotation encoding="application/x-tex" id="S3.T4.17.17.17.2.m1.1c">\left.+\frac{\sigma}{\sqrt{TKM}}\right)</annotation></semantics></math></td>
</tr>
</table>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Convergence rates for a (non-comprehensive) set of distributed optimization algorithms in the IID-data setting. We assume <math id="S3.T4.22.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.T4.22.m1.1b"><mi id="S3.T4.22.m1.1.1" xref="S3.T4.22.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.T4.22.m1.1c"><ci id="S3.T4.22.m1.1.1.cmml" xref="S3.T4.22.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.22.m1.1d">M</annotation></semantics></math> devices participate in each iterations, and the loss functions are <math id="S3.T4.23.m2.1" class="ltx_Math" alttext="H" display="inline"><semantics id="S3.T4.23.m2.1b"><mi id="S3.T4.23.m2.1.1" xref="S3.T4.23.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.T4.23.m2.1c"><ci id="S3.T4.23.m2.1.1.cmml" xref="S3.T4.23.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.23.m2.1d">H</annotation></semantics></math>-smooth, convex, and we have access to stochastic gradients with variance at most <math id="S3.T4.24.m3.1" class="ltx_Math" alttext="\sigma^{2}" display="inline"><semantics id="S3.T4.24.m3.1b"><msup id="S3.T4.24.m3.1.1" xref="S3.T4.24.m3.1.1.cmml"><mi id="S3.T4.24.m3.1.1.2" xref="S3.T4.24.m3.1.1.2.cmml">σ</mi><mn id="S3.T4.24.m3.1.1.3" xref="S3.T4.24.m3.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S3.T4.24.m3.1c"><apply id="S3.T4.24.m3.1.1.cmml" xref="S3.T4.24.m3.1.1"><csymbol cd="ambiguous" id="S3.T4.24.m3.1.1.1.cmml" xref="S3.T4.24.m3.1.1">superscript</csymbol><ci id="S3.T4.24.m3.1.1.2.cmml" xref="S3.T4.24.m3.1.1.2">𝜎</ci><cn type="integer" id="S3.T4.24.m3.1.1.3.cmml" xref="S3.T4.24.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.24.m3.1d">\sigma^{2}</annotation></semantics></math>. All rates are upper bounds on (<a href="#S3.E1" title="Equation 1 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) after <math id="S3.T4.25.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.T4.25.m4.1b"><mi id="S3.T4.25.m4.1.1" xref="S3.T4.25.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.T4.25.m4.1c"><ci id="S3.T4.25.m4.1.1.cmml" xref="S3.T4.25.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.25.m4.1d">T</annotation></semantics></math> iterations (potentially with some iterate averaging scheme).
<br class="ltx_break"></figcaption>
</figure>
<div id="S3.SS2.SSS1.Px1.p7" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p7.6" class="ltx_p">For non-convex objectives, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib500" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">500</span></a>]</cite> showed that local SGD can achieve asymptotically an error bound <math id="S3.SS2.SSS1.Px1.p7.1.m1.1" class="ltx_Math" alttext="1/\sqrt{TKM}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.1.m1.1a"><mrow id="S3.SS2.SSS1.Px1.p7.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.cmml"><mn id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.2" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.1" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.1.cmml">/</mo><msqrt id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.cmml"><mrow id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.2" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.3" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1a" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.4" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.4.cmml">M</mi></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.1.m1.1b"><apply id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1"><divide id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.1"></divide><cn type="integer" id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.2">1</cn><apply id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3"><root id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3"></root><apply id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2"><times id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.1"></times><ci id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.3">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.4.cmml" xref="S3.SS2.SSS1.Px1.p7.1.m1.1.1.3.2.4">𝑀</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.1.m1.1c">1/\sqrt{TKM}</annotation></semantics></math> if the number of local updates <math id="S3.SS2.SSS1.Px1.p7.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.2.m2.1a"><mi id="S3.SS2.SSS1.Px1.p7.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p7.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.2.m2.1b"><ci id="S3.SS2.SSS1.Px1.p7.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.2.m2.1c">K</annotation></semantics></math> are smaller than <math id="S3.SS2.SSS1.Px1.p7.3.m3.1" class="ltx_Math" alttext="T^{1/3}/M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.3.m3.1a"><mrow id="S3.SS2.SSS1.Px1.p7.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.cmml"><msup id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.2" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.cmml"><mn id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.2" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.2.cmml">1</mn><mo id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.1" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.1.cmml">/</mo><mn id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.3" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.3.cmml">3</mn></mrow></msup><mo id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.1" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.1.cmml">/</mo><mi id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.3" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.3.m3.1b"><apply id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1"><divide id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.1"></divide><apply id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.2">𝑇</ci><apply id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3"><divide id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.1"></divide><cn type="integer" id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.2">1</cn><cn type="integer" id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.2.3.3">3</cn></apply></apply><ci id="S3.SS2.SSS1.Px1.p7.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.3.m3.1c">T^{1/3}/M</annotation></semantics></math>. This convergence guarantee was further improved by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite> who removed the bounded gradient norm assumption and showed that the number of local updates can be as large as <math id="S3.SS2.SSS1.Px1.p7.4.m4.1" class="ltx_Math" alttext="T/M^{3}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.4.m4.1a"><mrow id="S3.SS2.SSS1.Px1.p7.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.1" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.1.cmml">/</mo><msup id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.2" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.2.cmml">M</mi><mn id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.3" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1"><divide id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.1"></divide><ci id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.2">𝑀</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS1.Px1.p7.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.4.m4.1c">T/M^{3}</annotation></semantics></math>. The analysis in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite> can also be applied to other algorithms with local updates, and thus yields the first convergence guarantee for decentralized SGD with local updates (or periodic decentralized SGD) and elastic averaging SGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib505" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">505</span></a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Haddadpour et al.</span> [<a href="#bib.bib216" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">216</span></a>]</cite> improves the bounds in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite> for functions satisfying the Polyak-Lojasiewicz (PL) condition <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib262" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">262</span></a>]</cite>, a generalization of strong convexity. In particular, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Haddadpour et al.</span> [<a href="#bib.bib216" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">216</span></a>]</cite> show that for PL functions, <math id="S3.SS2.SSS1.Px1.p7.5.m5.1" class="ltx_Math" alttext="T^{2}/M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.5.m5.1a"><mrow id="S3.SS2.SSS1.Px1.p7.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.cmml"><msup id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.cmml"><mi id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.2" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.2.cmml">T</mi><mn id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.3" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.3.cmml">2</mn></msup><mo id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.1" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.1.cmml">/</mo><mi id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.3" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.5.m5.1b"><apply id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1"><divide id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.1"></divide><apply id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.2">𝑇</ci><cn type="integer" id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.2.3">2</cn></apply><ci id="S3.SS2.SSS1.Px1.p7.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.5.m5.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.5.m5.1c">T^{2}/M</annotation></semantics></math> local updates per round leads to a <math id="S3.SS2.SSS1.Px1.p7.6.m6.1" class="ltx_Math" alttext="\mathcal{O}(1/TKM)" display="inline"><semantics id="S3.SS2.SSS1.Px1.p7.6.m6.1a"><mrow id="S3.SS2.SSS1.Px1.p7.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.3" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.3.cmml">𝒪</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.2" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.2.cmml">​</mo><mrow id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.2" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.cmml"><mrow id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.cmml"><mn id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.2" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.2.cmml">1</mn><mo id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.1" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.1.cmml">/</mo><mi id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.3" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.3.cmml">T</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1a" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.4" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.4.cmml">M</mi></mrow><mo stretchy="false" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.3" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p7.6.m6.1b"><apply id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1"><times id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.2"></times><ci id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.3">𝒪</ci><apply id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1"><times id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.1"></times><apply id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2"><divide id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.1"></divide><cn type="integer" id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.2">1</cn><ci id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.2.3">𝑇</ci></apply><ci id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.3">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.4.cmml" xref="S3.SS2.SSS1.Px1.p7.6.m6.1.1.1.1.1.4">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p7.6.m6.1c">\mathcal{O}(1/TKM)</annotation></semantics></math> convergence.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p8" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p8.8" class="ltx_p">While the above works focus on convergence as a function of the number of iterations performed, practitioners often care about wall-clock convergence speed. Assessing this must take into account the effect of the design parameters on the time spent per iteration based on the relative cost of communication and local computation. Viewed in this light, the focus on seeing how large <math id="S3.SS2.SSS1.Px1.p8.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p8.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p8.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p8.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.1.m1.1c">K</annotation></semantics></math> can be while maintaining the statistical rate may not be the primary concern in federated learning, where one may assume almost infinite datasets (very large <math id="S3.SS2.SSS1.Px1.p8.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.2.m2.1a"><mi id="S3.SS2.SSS1.Px1.p8.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p8.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.2.m2.1b"><ci id="S3.SS2.SSS1.Px1.p8.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.2.m2.1c">N</annotation></semantics></math>). The costs (at least in wall-clock time) are small for increasing <math id="S3.SS2.SSS1.Px1.p8.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.3.m3.1a"><mi id="S3.SS2.SSS1.Px1.p8.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p8.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.3.m3.1b"><ci id="S3.SS2.SSS1.Px1.p8.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.3.m3.1c">M</annotation></semantics></math>, and so it may be more natural to increase <math id="S3.SS2.SSS1.Px1.p8.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.4.m4.1a"><mi id="S3.SS2.SSS1.Px1.p8.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p8.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.4.m4.1b"><ci id="S3.SS2.SSS1.Px1.p8.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.4.m4.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.4.m4.1c">M</annotation></semantics></math> sufficiently to match the optimization term, and then tune <math id="S3.SS2.SSS1.Px1.p8.5.m5.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.5.m5.1a"><mi id="S3.SS2.SSS1.Px1.p8.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p8.5.m5.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.5.m5.1b"><ci id="S3.SS2.SSS1.Px1.p8.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.5.m5.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.5.m5.1c">K</annotation></semantics></math> to maximize wall-clock optimization performance.
How then to choose <math id="S3.SS2.SSS1.Px1.p8.6.m6.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.6.m6.1a"><mi id="S3.SS2.SSS1.Px1.p8.6.m6.1.1" xref="S3.SS2.SSS1.Px1.p8.6.m6.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.6.m6.1b"><ci id="S3.SS2.SSS1.Px1.p8.6.m6.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.6.m6.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.6.m6.1c">K</annotation></semantics></math>? Performing more local updates at the clients will increase the divergence between the resulting local models at the clients, before they are averaged. As a result, the error convergence in terms of training loss versus the total number of sequential SGD steps <math id="S3.SS2.SSS1.Px1.p8.7.m7.1" class="ltx_Math" alttext="TK" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.7.m7.1a"><mrow id="S3.SS2.SSS1.Px1.p8.7.m7.1.1" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.2" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.1" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.3" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.7.m7.1b"><apply id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1"><times id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.1"></times><ci id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.2">𝑇</ci><ci id="S3.SS2.SSS1.Px1.p8.7.m7.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p8.7.m7.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.7.m7.1c">TK</annotation></semantics></math> is slower. However, performing more local updates saves significant communication cost and reduces the time spent per iteration. The optimal number of local updates strikes a balance between these two phenomena and achieves the fastest error versus wallclock time convergence. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib468" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">468</span></a>]</cite> propose an adaptive communication strategy that adapts <math id="S3.SS2.SSS1.Px1.p8.8.m8.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS1.Px1.p8.8.m8.1a"><mi id="S3.SS2.SSS1.Px1.p8.8.m8.1.1" xref="S3.SS2.SSS1.Px1.p8.8.m8.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p8.8.m8.1b"><ci id="S3.SS2.SSS1.Px1.p8.8.m8.1.1.cmml" xref="S3.SS2.SSS1.Px1.p8.8.m8.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p8.8.m8.1c">K</annotation></semantics></math> according to the training loss at regular intervals during the training.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p9" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p9.1" class="ltx_p">Another important design parameter in federated learning is the model aggregation method used to update the global model using the updates made by the selected clients. In the original federated learning paper, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">McMahan et al.</span> [<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite> proposes taking a weighted average of the local models, in proportion to the size of local datasets. For IID data, where each client is assumed to have a infinitely large dataset, this reduces to taking a simple average of the local models. However, it is unclear whether this aggregation method will result in the fastest error convergence.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p10" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p10.1" class="ltx_p">There are many open questions in federated optimization, even with IID data.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Woodworth et al.</span> [<a href="#bib.bib480" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">480</span></a>]</cite> highlights several gaps between upper and lower bounds for optimization relevant to the federated learning setting, particularly for “intermittent communication graphs”, which captures local SGD approaches, but convergence rates for such approaches are not known to match the corresponding lower bounds. In Table <a href="#S3.T4" title="Table 4 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we highlight convergence results for the convex setting. Whilst most schemes are able to reach the asymptotically dominant statistical term, none are able to match the convergence rate of accelerated mini-batch SGD. It is an open problem if federated averaging algorithms can close this gap.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p11" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p11.5" class="ltx_p">Local-update SGD methods where all <math id="S3.SS2.SSS1.Px1.p11.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p11.1.m1.1a"><mi id="S3.SS2.SSS1.Px1.p11.1.m1.1.1" xref="S3.SS2.SSS1.Px1.p11.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p11.1.m1.1b"><ci id="S3.SS2.SSS1.Px1.p11.1.m1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p11.1.m1.1c">M</annotation></semantics></math> clients perform the same number of local updates may suffer from a common scalability issue—they can be bottlenecked if any one client unpredictably slows down or fails. Several approaches for dealing with this are possible, but it is far from clear which are optimal, especially when the potential for bias is considered (see <a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>). <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bonawitz et al.</span> [<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> propose over-provisioning clients (e.g., request updates from <math id="S3.SS2.SSS1.Px1.p11.2.m2.1" class="ltx_Math" alttext="1.3M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p11.2.m2.1a"><mrow id="S3.SS2.SSS1.Px1.p11.2.m2.1.1" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.cmml"><mn id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.2" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.2.cmml">1.3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.1" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.3" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p11.2.m2.1b"><apply id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1"><times id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.1"></times><cn type="float" id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.2">1.3</cn><ci id="S3.SS2.SSS1.Px1.p11.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p11.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p11.2.m2.1c">1.3M</annotation></semantics></math> clients), and then accepting the first <math id="S3.SS2.SSS1.Px1.p11.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS1.Px1.p11.3.m3.1a"><mi id="S3.SS2.SSS1.Px1.p11.3.m3.1.1" xref="S3.SS2.SSS1.Px1.p11.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p11.3.m3.1b"><ci id="S3.SS2.SSS1.Px1.p11.3.m3.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p11.3.m3.1c">M</annotation></semantics></math> updates received and rejecting updates from stragglers. A slightly more sophisticated solution is to fix a time window and allow clients to perform as many local updates <math id="S3.SS2.SSS1.Px1.p11.4.m4.1" class="ltx_Math" alttext="K_{i}" display="inline"><semantics id="S3.SS2.SSS1.Px1.p11.4.m4.1a"><msub id="S3.SS2.SSS1.Px1.p11.4.m4.1.1" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.2" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1.2.cmml">K</mi><mi id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.3" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p11.4.m4.1b"><apply id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1.2">𝐾</ci><ci id="S3.SS2.SSS1.Px1.p11.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.Px1.p11.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p11.4.m4.1c">K_{i}</annotation></semantics></math> as possible within this time, after which their models are averaged by a central server. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib471" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">471</span></a>]</cite> analyzed the computational heteogeneity introduced by this approach in theory. An alternative method to overcome the problem of straggling clients is to fix the number of local updates at <math id="S3.SS2.SSS1.Px1.p11.5.m5.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.SS2.SSS1.Px1.p11.5.m5.1a"><mi id="S3.SS2.SSS1.Px1.p11.5.m5.1.1" xref="S3.SS2.SSS1.Px1.p11.5.m5.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.Px1.p11.5.m5.1b"><ci id="S3.SS2.SSS1.Px1.p11.5.m5.1.1.cmml" xref="S3.SS2.SSS1.Px1.p11.5.m5.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.Px1.p11.5.m5.1c">\tau</annotation></semantics></math>, but allow clients to update the global model in an asynchronous or lock-free fashion. Although some previous works <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib505" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">505</span></a>, <a href="#bib.bib306" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">306</span></a>, <a href="#bib.bib163" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">163</span></a>]</cite> have proposed similar methods, the error convergence analysis is an open and challenging problem. A larger challenge in the FL setting, however, is that as discussed at the beginning of <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>, asynchronous approaches may be difficult to combine with complimentary techniques like differential privacy or secure aggregation.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p12" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p12.1" class="ltx_p">Besides the number of local updates, the choice of the size of the set of clients selected per training round presents a similar trade-off as the number of local updates. Updating and averaging a larger number of client models per training round yields better convergence, but it makes the training vulnerable to slowdown due to unpredictable tail delays in computation/communication at/with the clients.</p>
</div>
<div id="S3.SS2.SSS1.Px1.p13" class="ltx_para">
<p id="S3.SS2.SSS1.Px1.p13.1" class="ltx_p">The analysis of local SGD / Federated Averaging in the non-IID setting is even more challenging; results and open questions related to this are considered in the next section, along with specialized algorithms which directly address the non-IID problem.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T5.4.4" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.T5.4.4.5" class="ltx_tr">
<td id="S3.T5.4.4.5.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="3"><span id="S3.T5.4.4.5.1.1" class="ltx_text ltx_font_bold">Non-IID assumptions</span></td>
</tr>
<tr id="S3.T5.4.4.6" class="ltx_tr">
<td id="S3.T5.4.4.6.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.4.4.6.1.1" class="ltx_text ltx_font_bold">Symbol</span></td>
<td id="S3.T5.4.4.6.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.4.4.6.2.1" class="ltx_text ltx_font_bold">Full name</span></td>
<td id="S3.T5.4.4.6.3" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.4.4.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.4.4.6.3.1.1" class="ltx_p"><span id="S3.T5.4.4.6.3.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</td>
</tr>
<tr id="S3.T5.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BCGV</td>
<td id="S3.T5.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">bounded inter-client gradient variance</td>
<td id="S3.T5.1.1.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.1.1.1.1.1.1" class="ltx_p"><math id="S3.T5.1.1.1.1.1.1.m1.3" class="ltx_Math" alttext="\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x)-\nabla F(x)\|^{2}\leq\eta^{2}" display="inline"><semantics id="S3.T5.1.1.1.1.1.1.m1.3a"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.cmml"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.cmml"><msub id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.cmml"><mo id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.2.cmml">𝔼</mo><mi id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.3.cmml">i</mi></msub><msup id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.cmml"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.2.cmml"><mo lspace="0em" stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.2.1.cmml">‖</mo><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.cmml"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.cmml"><mo rspace="0.167em" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.1.cmml">∇</mo><msub id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.cmml"><mi id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.2.cmml">f</mi><mi id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.3.2.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="S3.T5.1.1.1.1.1.1.m1.1.1" xref="S3.T5.1.1.1.1.1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.3.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.cmml"><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.cmml"><mo rspace="0.167em" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.1.cmml">∇</mo><mi id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.1.cmml">​</mo><mrow id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.3.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.3.2.1" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.cmml">(</mo><mi id="S3.T5.1.1.1.1.1.1.m1.2.2" xref="S3.T5.1.1.1.1.1.1.m1.2.2.cmml">x</mi><mo stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.3.2.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.3.cmml">2</mn></msup></mrow><mo id="S3.T5.1.1.1.1.1.1.m1.3.3.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.2.cmml">≤</mo><msup id="S3.T5.1.1.1.1.1.1.m1.3.3.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3.cmml"><mi id="S3.T5.1.1.1.1.1.1.m1.3.3.3.2" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3.2.cmml">η</mi><mn id="S3.T5.1.1.1.1.1.1.m1.3.3.3.3" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.1.1.m1.3b"><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3"><leq id="S3.T5.1.1.1.1.1.1.m1.3.3.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.2"></leq><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1"><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2">subscript</csymbol><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.2">𝔼</ci><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.2.3">𝑖</ci></apply><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1"><csymbol cd="ambiguous" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1">superscript</csymbol><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.2">norm</csymbol><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1"><minus id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.1"></minus><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2"><times id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.1"></times><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2"><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.1">∇</ci><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.2">𝑓</ci><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.2.2.2.3">𝑖</ci></apply></apply><ci id="S3.T5.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.1.1">𝑥</ci></apply><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3"><times id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.1"></times><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2"><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.1">∇</ci><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.1.1.1.3.2.2">𝐹</ci></apply><ci id="S3.T5.1.1.1.1.1.1.m1.2.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.2.2">𝑥</ci></apply></apply></apply><cn type="integer" id="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.1.1.3">2</cn></apply></apply><apply id="S3.T5.1.1.1.1.1.1.m1.3.3.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.T5.1.1.1.1.1.1.m1.3.3.3.1.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3">superscript</csymbol><ci id="S3.T5.1.1.1.1.1.1.m1.3.3.3.2.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3.2">𝜂</ci><cn type="integer" id="S3.T5.1.1.1.1.1.1.m1.3.3.3.3.cmml" xref="S3.T5.1.1.1.1.1.1.m1.3.3.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.1.1.m1.3c">\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x)-\nabla F(x)\|^{2}\leq\eta^{2}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.2.2.2" class="ltx_tr">
<td id="S3.T5.2.2.2.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BOBD</td>
<td id="S3.T5.2.2.2.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">bounded optimal objective difference</td>
<td id="S3.T5.2.2.2.1" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.2.2.2.1.1.1" class="ltx_p"><math id="S3.T5.2.2.2.1.1.1.m1.2" class="ltx_Math" alttext="F^{*}-\operatorname*{\mathbb{E}}_{i}[f^{*}_{i}]\leq\eta^{2}" display="inline"><semantics id="S3.T5.2.2.2.1.1.1.m1.2a"><mrow id="S3.T5.2.2.2.1.1.1.m1.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.cmml"><mrow id="S3.T5.2.2.2.1.1.1.m1.2.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.cmml"><msup id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.cmml"><mi id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.2.cmml">F</mi><mo id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.3.cmml">∗</mo></msup><mo id="S3.T5.2.2.2.1.1.1.m1.2.2.2.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.3.cmml">−</mo><mrow id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.3.cmml"><msub id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.cmml"><mo lspace="0em" rspace="0em" id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.2" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.2.cmml">𝔼</mo><mi id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.3" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mrow id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.3.cmml">[</mo><msubsup id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.cmml"><mi id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.2.cmml">f</mi><mi id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.3.cmml">i</mi><mo id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.3.cmml">∗</mo></msubsup><mo stretchy="false" id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.3.cmml">]</mo></mrow></mrow></mrow><mo id="S3.T5.2.2.2.1.1.1.m1.2.2.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.3.cmml">≤</mo><msup id="S3.T5.2.2.2.1.1.1.m1.2.2.4" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4.cmml"><mi id="S3.T5.2.2.2.1.1.1.m1.2.2.4.2" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4.2.cmml">η</mi><mn id="S3.T5.2.2.2.1.1.1.m1.2.2.4.3" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.1.1.1.m1.2b"><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2"><leq id="S3.T5.2.2.2.1.1.1.m1.2.2.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.3"></leq><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2"><minus id="S3.T5.2.2.2.1.1.1.m1.2.2.2.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.3"></minus><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4">superscript</csymbol><ci id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.2">𝐹</ci><times id="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.4.3"></times></apply><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2"><apply id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.2">𝔼</ci><ci id="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1">subscript</csymbol><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1"><csymbol cd="ambiguous" id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1">superscript</csymbol><ci id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.2">𝑓</ci><times id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.2.3"></times></apply><ci id="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.2.2.2.2.1.3">𝑖</ci></apply></apply></apply><apply id="S3.T5.2.2.2.1.1.1.m1.2.2.4.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.T5.2.2.2.1.1.1.m1.2.2.4.1.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4">superscript</csymbol><ci id="S3.T5.2.2.2.1.1.1.m1.2.2.4.2.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4.2">𝜂</ci><cn type="integer" id="S3.T5.2.2.2.1.1.1.m1.2.2.4.3.cmml" xref="S3.T5.2.2.2.1.1.1.m1.2.2.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.1.1.1.m1.2c">F^{*}-\operatorname*{\mathbb{E}}_{i}[f^{*}_{i}]\leq\eta^{2}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.3.3.3" class="ltx_tr">
<td id="S3.T5.3.3.3.2" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">BOGV</td>
<td id="S3.T5.3.3.3.3" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;">bounded optimal gradient variance</td>
<td id="S3.T5.3.3.3.1" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.3.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.3.3.3.1.1.1" class="ltx_p"><math id="S3.T5.3.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x^{*})\|^{2}\leq\eta^{2}" display="inline"><semantics id="S3.T5.3.3.3.1.1.1.m1.1a"><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.cmml"><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.cmml"><msub id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.cmml"><mo id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.2.cmml">𝔼</mo><mi id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.3.cmml">i</mi></msub><msup id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.cmml"><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.2.cmml"><mo lspace="0em" stretchy="false" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.cmml"><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.cmml"><mo rspace="0.167em" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">∇</mo><msub id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.cmml"><mi id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml">f</mi><mi id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mo id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">∗</mo></msup><mo stretchy="false" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.3.cmml">2</mn></msup></mrow><mo id="S3.T5.3.3.3.1.1.1.m1.1.1.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.2.cmml">≤</mo><msup id="S3.T5.3.3.3.1.1.1.m1.1.1.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3.cmml"><mi id="S3.T5.3.3.3.1.1.1.m1.1.1.3.2" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3.2.cmml">η</mi><mn id="S3.T5.3.3.3.1.1.1.m1.1.1.3.3" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.3.3.3.1.1.1.m1.1b"><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1"><leq id="S3.T5.3.3.3.1.1.1.m1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.2"></leq><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1"><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2">subscript</csymbol><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.2">𝔼</ci><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1">superscript</csymbol><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.2.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1"><times id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.2"></times><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3"><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.1">∇</ci><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.2">𝑓</ci><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply></apply><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><times id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.1.1.1.1.1.1.3"></times></apply></apply></apply><cn type="integer" id="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.1.1.3">2</cn></apply></apply><apply id="S3.T5.3.3.3.1.1.1.m1.1.1.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.T5.3.3.3.1.1.1.m1.1.1.3.1.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3">superscript</csymbol><ci id="S3.T5.3.3.3.1.1.1.m1.1.1.3.2.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3.2">𝜂</ci><cn type="integer" id="S3.T5.3.3.3.1.1.1.m1.1.1.3.3.cmml" xref="S3.T5.3.3.3.1.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.3.3.3.1.1.1.m1.1c">\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x^{*})\|^{2}\leq\eta^{2}</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.4.4.4" class="ltx_tr">
<td id="S3.T5.4.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">BGV</td>
<td id="S3.T5.4.4.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">bounded gradient dissimilarity</td>
<td id="S3.T5.4.4.4.1" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.4.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.4.4.4.1.1.1" class="ltx_p"><math id="S3.T5.4.4.4.1.1.1.m1.4" class="ltx_Math" alttext="\nicefrac{{\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x)\|^{2}}}{{\|\nabla F(x)\|^{2}}}\leq\eta^{2}" display="inline"><semantics id="S3.T5.4.4.4.1.1.1.m1.4a"><mrow id="S3.T5.4.4.4.1.1.1.m1.4.5" xref="S3.T5.4.4.4.1.1.1.m1.4.5.cmml"><mrow id="S3.T5.4.4.4.1.1.1.m1.4.4" xref="S3.T5.4.4.4.1.1.1.m1.4.4.cmml"><mpadded voffset="0.3em" id="S3.T5.4.4.4.1.1.1.m1.2.2.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.cmml"><msub id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.cmml"><mo mathsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.2.cmml">𝔼</mo><mi mathsize="71%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.3" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.3.cmml">i</mi></msub><msup id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.cmml"><mrow id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.2.cmml"><mo lspace="0em" maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.2.1.cmml">‖</mo><mrow id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.cmml"><mrow id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.cmml"><mo mathsize="70%" rspace="0.167em" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.1" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.1.cmml">∇</mo><msub id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.cmml"><mi mathsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.2.cmml">f</mi><mi mathsize="71%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.3" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.1" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.1.cmml">​</mo><mrow id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.3.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.3.2.1" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.cmml">(</mo><mi mathsize="70%" id="S3.T5.4.4.4.1.1.1.m1.1.1.1.1" xref="S3.T5.4.4.4.1.1.1.m1.1.1.1.1.cmml">x</mi><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.3.2.2" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.3" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.2.1.cmml">‖</mo></mrow><mn mathsize="71%" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.3" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.3.cmml">2</mn></msup></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.4.4.4.1.1.1.m1.4.4.5" xref="S3.T5.4.4.4.1.1.1.m1.4.4.5.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.4.4.4.1.1.1.m1.4.4.5a" xref="S3.T5.4.4.4.1.1.1.m1.4.4.5.cmml">/</mo></mpadded><msup id="S3.T5.4.4.4.1.1.1.m1.4.4.4" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.cmml"><mrow id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.2.cmml"><mo lspace="0.222em" maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.2" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.2.1.cmml">‖</mo><mrow id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.cmml"><mrow id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.cmml"><mo mathsize="70%" rspace="0.167em" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.1" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.1.cmml">∇</mo><mi mathsize="70%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.2" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.2.cmml">F</mi></mrow><mo lspace="0em" rspace="0em" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.1" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.1.cmml">​</mo><mrow id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.3.2" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.cmml"><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.3.2.1" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.cmml">(</mo><mi mathsize="70%" id="S3.T5.4.4.4.1.1.1.m1.3.3.3.1" xref="S3.T5.4.4.4.1.1.1.m1.3.3.3.1.cmml">x</mi><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.3.2.2" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.cmml">)</mo></mrow></mrow><mo maxsize="70%" minsize="70%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.3" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.2.1.cmml">‖</mo></mrow><mn mathsize="71%" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.4" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.4.cmml">2</mn></msup></mrow><mo id="S3.T5.4.4.4.1.1.1.m1.4.5.1" xref="S3.T5.4.4.4.1.1.1.m1.4.5.1.cmml">≤</mo><msup id="S3.T5.4.4.4.1.1.1.m1.4.5.2" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2.cmml"><mi id="S3.T5.4.4.4.1.1.1.m1.4.5.2.2" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2.2.cmml">η</mi><mn id="S3.T5.4.4.4.1.1.1.m1.4.5.2.3" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.4.4.4.1.1.1.m1.4b"><apply id="S3.T5.4.4.4.1.1.1.m1.4.5.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5"><leq id="S3.T5.4.4.4.1.1.1.m1.4.5.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5.1"></leq><apply id="S3.T5.4.4.4.1.1.1.m1.4.4.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4"><divide id="S3.T5.4.4.4.1.1.1.m1.4.4.5.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.5"></divide><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2"><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3"><csymbol cd="ambiguous" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3">subscript</csymbol><ci id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.2">𝔼</ci><ci id="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.3.3">𝑖</ci></apply><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2">superscript</csymbol><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1"><csymbol cd="latexml" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.2">norm</csymbol><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1"><times id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.1"></times><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2"><ci id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.1">∇</ci><apply id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2">subscript</csymbol><ci id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.2">𝑓</ci><ci id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.1.1.1.2.2.3">𝑖</ci></apply></apply><ci id="S3.T5.4.4.4.1.1.1.m1.1.1.1.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.1.1.1.1">𝑥</ci></apply></apply><cn type="integer" id="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.2.2.2.2.3">2</cn></apply></apply><apply id="S3.T5.4.4.4.1.1.1.m1.4.4.4.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4"><csymbol cd="ambiguous" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4">superscript</csymbol><apply id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1"><csymbol cd="latexml" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.2">norm</csymbol><apply id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1"><times id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.1"></times><apply id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2"><ci id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.1">∇</ci><ci id="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.2.1.1.2.2">𝐹</ci></apply><ci id="S3.T5.4.4.4.1.1.1.m1.3.3.3.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.3.3.3.1">𝑥</ci></apply></apply><cn type="integer" id="S3.T5.4.4.4.1.1.1.m1.4.4.4.4.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.4.4.4">2</cn></apply></apply><apply id="S3.T5.4.4.4.1.1.1.m1.4.5.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.T5.4.4.4.1.1.1.m1.4.5.2.1.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2">superscript</csymbol><ci id="S3.T5.4.4.4.1.1.1.m1.4.5.2.2.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2.2">𝜂</ci><cn type="integer" id="S3.T5.4.4.4.1.1.1.m1.4.5.2.3.cmml" xref="S3.T5.4.4.4.1.1.1.m1.4.5.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.4.4.4.1.1.1.m1.4c">\nicefrac{{\operatorname*{\mathbb{E}}_{i}\|\nabla f_{i}(x)\|^{2}}}{{\|\nabla F(x)\|^{2}}}\leq\eta^{2}</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T5.8.8" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.T5.8.8.5" class="ltx_tr">
<td id="S3.T5.8.8.5.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="2"><span id="S3.T5.8.8.5.1.1" class="ltx_text ltx_font_bold">Other assumptions and variants</span></td>
</tr>
<tr id="S3.T5.8.8.6" class="ltx_tr">
<td id="S3.T5.8.8.6.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.8.8.6.1.1" class="ltx_text ltx_font_bold">Symbol</span></td>
<td id="S3.T5.8.8.6.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.6.2.1.1" class="ltx_p"><span id="S3.T5.8.8.6.2.1.1.1" class="ltx_text ltx_font_bold">Explanation</span></span>
</span>
</td>
</tr>
<tr id="S3.T5.5.5.1" class="ltx_tr">
<td id="S3.T5.5.5.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">CVX</td>
<td id="S3.T5.5.5.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.5.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.5.5.1.1.1.1" class="ltx_p">Each client function <math id="S3.T5.5.5.1.1.1.1.m1.1" class="ltx_Math" alttext="f_{i}(x)" display="inline"><semantics id="S3.T5.5.5.1.1.1.1.m1.1a"><mrow id="S3.T5.5.5.1.1.1.1.m1.1.2" xref="S3.T5.5.5.1.1.1.1.m1.1.2.cmml"><msub id="S3.T5.5.5.1.1.1.1.m1.1.2.2" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2.cmml"><mi id="S3.T5.5.5.1.1.1.1.m1.1.2.2.2" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2.2.cmml">f</mi><mi id="S3.T5.5.5.1.1.1.1.m1.1.2.2.3" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.T5.5.5.1.1.1.1.m1.1.2.1" xref="S3.T5.5.5.1.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.T5.5.5.1.1.1.1.m1.1.2.3.2" xref="S3.T5.5.5.1.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T5.5.5.1.1.1.1.m1.1.2.3.2.1" xref="S3.T5.5.5.1.1.1.1.m1.1.2.cmml">(</mo><mi id="S3.T5.5.5.1.1.1.1.m1.1.1" xref="S3.T5.5.5.1.1.1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.T5.5.5.1.1.1.1.m1.1.2.3.2.2" xref="S3.T5.5.5.1.1.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.5.5.1.1.1.1.m1.1b"><apply id="S3.T5.5.5.1.1.1.1.m1.1.2.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2"><times id="S3.T5.5.5.1.1.1.1.m1.1.2.1.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2.1"></times><apply id="S3.T5.5.5.1.1.1.1.m1.1.2.2.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.T5.5.5.1.1.1.1.m1.1.2.2.1.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2">subscript</csymbol><ci id="S3.T5.5.5.1.1.1.1.m1.1.2.2.2.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2.2">𝑓</ci><ci id="S3.T5.5.5.1.1.1.1.m1.1.2.2.3.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.T5.5.5.1.1.1.1.m1.1.1.cmml" xref="S3.T5.5.5.1.1.1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.5.5.1.1.1.1.m1.1c">f_{i}(x)</annotation></semantics></math> is convex.</span>
</span>
</td>
</tr>
<tr id="S3.T5.7.7.3" class="ltx_tr">
<td id="S3.T5.7.7.3.3" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">SCVX</td>
<td id="S3.T5.7.7.3.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.7.7.3.2.2" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.7.7.3.2.2.2" class="ltx_p">Each client function <math id="S3.T5.6.6.2.1.1.1.m1.1" class="ltx_Math" alttext="f_{i}(x)" display="inline"><semantics id="S3.T5.6.6.2.1.1.1.m1.1a"><mrow id="S3.T5.6.6.2.1.1.1.m1.1.2" xref="S3.T5.6.6.2.1.1.1.m1.1.2.cmml"><msub id="S3.T5.6.6.2.1.1.1.m1.1.2.2" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2.cmml"><mi id="S3.T5.6.6.2.1.1.1.m1.1.2.2.2" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2.2.cmml">f</mi><mi id="S3.T5.6.6.2.1.1.1.m1.1.2.2.3" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.T5.6.6.2.1.1.1.m1.1.2.1" xref="S3.T5.6.6.2.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.T5.6.6.2.1.1.1.m1.1.2.3.2" xref="S3.T5.6.6.2.1.1.1.m1.1.2.cmml"><mo stretchy="false" id="S3.T5.6.6.2.1.1.1.m1.1.2.3.2.1" xref="S3.T5.6.6.2.1.1.1.m1.1.2.cmml">(</mo><mi id="S3.T5.6.6.2.1.1.1.m1.1.1" xref="S3.T5.6.6.2.1.1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.T5.6.6.2.1.1.1.m1.1.2.3.2.2" xref="S3.T5.6.6.2.1.1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.6.6.2.1.1.1.m1.1b"><apply id="S3.T5.6.6.2.1.1.1.m1.1.2.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2"><times id="S3.T5.6.6.2.1.1.1.m1.1.2.1.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2.1"></times><apply id="S3.T5.6.6.2.1.1.1.m1.1.2.2.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.T5.6.6.2.1.1.1.m1.1.2.2.1.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2">subscript</csymbol><ci id="S3.T5.6.6.2.1.1.1.m1.1.2.2.2.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2.2">𝑓</ci><ci id="S3.T5.6.6.2.1.1.1.m1.1.2.2.3.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.T5.6.6.2.1.1.1.m1.1.1.cmml" xref="S3.T5.6.6.2.1.1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.6.6.2.1.1.1.m1.1c">f_{i}(x)</annotation></semantics></math> is <math id="S3.T5.7.7.3.2.2.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.T5.7.7.3.2.2.2.m2.1a"><mi id="S3.T5.7.7.3.2.2.2.m2.1.1" xref="S3.T5.7.7.3.2.2.2.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="S3.T5.7.7.3.2.2.2.m2.1b"><ci id="S3.T5.7.7.3.2.2.2.m2.1.1.cmml" xref="S3.T5.7.7.3.2.2.2.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.7.7.3.2.2.2.m2.1c">\mu</annotation></semantics></math>-strongly convex.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.4" class="ltx_tr">
<td id="S3.T5.8.8.4.2" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">BNCVX</td>
<td id="S3.T5.8.8.4.1" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.4.1.1.1" class="ltx_p">Each client function has bounded nonconvexity with <math id="S3.T5.8.8.4.1.1.1.m1.1" class="ltx_Math" alttext="\nabla^{2}f_{i}(x)\succeq-\mu I" display="inline"><semantics id="S3.T5.8.8.4.1.1.1.m1.1a"><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.cmml"><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.cmml"><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.cmml"><msup id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.cmml"><mo id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.2.cmml">∇</mo><mn id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.3" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.3.cmml">2</mn></msup><msub id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.cmml"><mi id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.2.cmml">f</mi><mi id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.3" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.3.cmml">i</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.1" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.1.cmml">​</mo><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2.2.3.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.cmml"><mo stretchy="false" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.3.2.1" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.cmml">(</mo><mi id="S3.T5.8.8.4.1.1.1.m1.1.1" xref="S3.T5.8.8.4.1.1.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.3.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S3.T5.8.8.4.1.1.1.m1.1.2.1" xref="S3.T5.8.8.4.1.1.1.m1.1.2.1.cmml">⪰</mo><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2.3" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.cmml"><mo id="S3.T5.8.8.4.1.1.1.m1.1.2.3a" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.cmml">−</mo><mrow id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.cmml"><mi id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.2" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.2.cmml">μ</mi><mo lspace="0em" rspace="0em" id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.1" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.1.cmml">​</mo><mi id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.3" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.3.cmml">I</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.8.8.4.1.1.1.m1.1b"><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2"><csymbol cd="latexml" id="S3.T5.8.8.4.1.1.1.m1.1.2.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.1">succeeds-or-equals</csymbol><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2"><times id="S3.T5.8.8.4.1.1.1.m1.1.2.2.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.1"></times><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2"><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1"><csymbol cd="ambiguous" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1">superscript</csymbol><ci id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.2">∇</ci><cn type="integer" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.3.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.1.3">2</cn></apply><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2">subscript</csymbol><ci id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.2">𝑓</ci><ci id="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.3.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.2.2.2.3">𝑖</ci></apply></apply><ci id="S3.T5.8.8.4.1.1.1.m1.1.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.1">𝑥</ci></apply><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.3.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3"><minus id="S3.T5.8.8.4.1.1.1.m1.1.2.3.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3"></minus><apply id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2"><times id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.1.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.1"></times><ci id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.2.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.2">𝜇</ci><ci id="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.3.cmml" xref="S3.T5.8.8.4.1.1.1.m1.1.2.3.2.3">𝐼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.8.8.4.1.1.1.m1.1c">\nabla^{2}f_{i}(x)\succeq-\mu I</annotation></semantics></math>.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.7" class="ltx_tr">
<td id="S3.T5.8.8.7.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLGV</td>
<td id="S3.T5.8.8.7.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.7.2.1.1" class="ltx_p">The variance of stochastic gradients on local clients is bounded.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.8" class="ltx_tr">
<td id="S3.T5.8.8.8.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLGN</td>
<td id="S3.T5.8.8.8.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.8.2.1.1" class="ltx_p">The norm of any local gradient is bounded.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.9" class="ltx_tr">
<td id="S3.T5.8.8.9.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">LBG</td>
<td id="S3.T5.8.8.9.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.9.2.1.1" class="ltx_p">Clients use the full batch of local samples to compute updates.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.10" class="ltx_tr">
<td id="S3.T5.8.8.10.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Dec</td>
<td id="S3.T5.8.8.10.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.10.2.1.1" class="ltx_p">Decentralized setting, assumes the the connectivity of network is good.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.11" class="ltx_tr">
<td id="S3.T5.8.8.11.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">AC</td>
<td id="S3.T5.8.8.11.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.11.2.1.1" class="ltx_p">All clients participate in each round.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.12" class="ltx_tr">
<td id="S3.T5.8.8.12.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">1step</td>
<td id="S3.T5.8.8.12.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.12.2.1.1" class="ltx_p">One local update is performed on clients in each round.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.13" class="ltx_tr">
<td id="S3.T5.8.8.13.1" class="ltx_td ltx_align_left" style="padding-top:0.5pt;padding-bottom:0.5pt;">Prox</td>
<td id="S3.T5.8.8.13.2" class="ltx_td ltx_align_justify" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.13.2.1.1" class="ltx_p">Use proximal gradient steps on clients.</span>
</span>
</td>
</tr>
<tr id="S3.T5.8.8.14" class="ltx_tr">
<td id="S3.T5.8.8.14.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">VR</td>
<td id="S3.T5.8.8.14.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.8.8.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.8.8.14.2.1.1" class="ltx_p">Variance reduction which needs to track the state.</span>
</span>
</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S3.T5.15.15" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S3.T5.15.15.8" class="ltx_tr">
<td id="S3.T5.15.15.8.1" class="ltx_td ltx_align_center" style="padding-top:0.5pt;padding-bottom:0.5pt;" colspan="5"><span id="S3.T5.15.15.8.1.1" class="ltx_text ltx_font_bold">Convergence rates</span></td>
</tr>
<tr id="S3.T5.15.15.9" class="ltx_tr">
<td id="S3.T5.15.15.9.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.15.15.9.1.1" class="ltx_text ltx_font_bold">Method</span></td>
<td id="S3.T5.15.15.9.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.15.15.9.2.1" class="ltx_text ltx_font_bold">Non-IID</span></td>
<td id="S3.T5.15.15.9.3" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.15.15.9.3.1" class="ltx_text ltx_font_bold">Other assumptions</span></td>
<td id="S3.T5.15.15.9.4" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span id="S3.T5.15.15.9.4.1" class="ltx_text ltx_font_bold">Variant</span></td>
<td id="S3.T5.15.15.9.5" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.15.15.9.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.15.15.9.5.1.1" class="ltx_p"><span id="S3.T5.15.15.9.5.1.1.1" class="ltx_text ltx_font_bold">Rate</span></span>
</span>
</td>
</tr>
<tr id="S3.T5.9.9.1" class="ltx_tr">
<td id="S3.T5.9.9.1.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Lian et al.</span> [<a href="#bib.bib305" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">305</span></a>]</cite></td>
<td id="S3.T5.9.9.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BCGV</td>
<td id="S3.T5.9.9.1.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLGV</td>
<td id="S3.T5.9.9.1.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Dec; AC; 1step</td>
<td id="S3.T5.9.9.1.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.9.9.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.9.9.1.1.1.1" class="ltx_p"><math id="S3.T5.9.9.1.1.1.1.m1.2" class="ltx_Math" alttext="O(\nicefrac{{1}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})" display="inline"><semantics id="S3.T5.9.9.1.1.1.1.m1.2a"><mrow id="S3.T5.9.9.1.1.1.1.m1.2.3" xref="S3.T5.9.9.1.1.1.1.m1.2.3.cmml"><mrow id="S3.T5.9.9.1.1.1.1.m1.2.3.2" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.cmml"><mi id="S3.T5.9.9.1.1.1.1.m1.2.3.2.2" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.9.9.1.1.1.1.m1.2.3.2.1" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.T5.9.9.1.1.1.1.m1.2.3.2.3.2" xref="S3.T5.9.9.1.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.9.9.1.1.1.1.m1.2.3.2.3.2.1" xref="S3.T5.9.9.1.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.9.9.1.1.1.1.m1.1.1" xref="S3.T5.9.9.1.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.9.9.1.1.1.1.m1.1.1.2" xref="S3.T5.9.9.1.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S3.T5.9.9.1.1.1.1.m1.1.1.2a" xref="S3.T5.9.9.1.1.1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.9.9.1.1.1.1.m1.1.1.1" xref="S3.T5.9.9.1.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.9.9.1.1.1.1.m1.1.1.1a" xref="S3.T5.9.9.1.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mi mathsize="70%" id="S3.T5.9.9.1.1.1.1.m1.1.1.3" xref="S3.T5.9.9.1.1.1.1.m1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T5.9.9.1.1.1.1.m1.2.3.2.3.2.2" xref="S3.T5.9.9.1.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.T5.9.9.1.1.1.1.m1.2.3.1" xref="S3.T5.9.9.1.1.1.1.m1.2.3.1.cmml">+</mo><mrow id="S3.T5.9.9.1.1.1.1.m1.2.3.3" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.cmml"><mi id="S3.T5.9.9.1.1.1.1.m1.2.3.3.2" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.9.9.1.1.1.1.m1.2.3.3.1" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.T5.9.9.1.1.1.1.m1.2.3.3.3.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.cmml"><mo stretchy="false" id="S3.T5.9.9.1.1.1.1.m1.2.3.3.3.2.1" xref="S3.T5.9.9.1.1.1.1.m1.2.2.cmml">(</mo><mrow id="S3.T5.9.9.1.1.1.1.m1.2.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.cmml"><mpadded voffset="0.3em" id="S3.T5.9.9.1.1.1.1.m1.2.2.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.2.cmml"><mn mathsize="70%" id="S3.T5.9.9.1.1.1.1.m1.2.2.2a" xref="S3.T5.9.9.1.1.1.1.m1.2.2.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.9.9.1.1.1.1.m1.2.2.1" xref="S3.T5.9.9.1.1.1.1.m1.2.2.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.9.9.1.1.1.1.m1.2.2.1a" xref="S3.T5.9.9.1.1.1.1.m1.2.2.1.cmml">/</mo></mpadded><msqrt id="S3.T5.9.9.1.1.1.1.m1.2.2.3" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.cmml"><mrow id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.cmml"><mi mathsize="70%" id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.1" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.3" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.3.cmml">T</mi></mrow></msqrt></mrow><mo stretchy="false" id="S3.T5.9.9.1.1.1.1.m1.2.3.3.3.2.2" xref="S3.T5.9.9.1.1.1.1.m1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.9.9.1.1.1.1.m1.2b"><apply id="S3.T5.9.9.1.1.1.1.m1.2.3.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3"><plus id="S3.T5.9.9.1.1.1.1.m1.2.3.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.1"></plus><apply id="S3.T5.9.9.1.1.1.1.m1.2.3.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2"><times id="S3.T5.9.9.1.1.1.1.m1.2.3.2.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.1"></times><ci id="S3.T5.9.9.1.1.1.1.m1.2.3.2.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.2">𝑂</ci><apply id="S3.T5.9.9.1.1.1.1.m1.1.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.2.3.2"><divide id="S3.T5.9.9.1.1.1.1.m1.1.1.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S3.T5.9.9.1.1.1.1.m1.1.1.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.1.1.2">1</cn><ci id="S3.T5.9.9.1.1.1.1.m1.1.1.3.cmml" xref="S3.T5.9.9.1.1.1.1.m1.1.1.3">𝑇</ci></apply></apply><apply id="S3.T5.9.9.1.1.1.1.m1.2.3.3.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3"><times id="S3.T5.9.9.1.1.1.1.m1.2.3.3.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.1"></times><ci id="S3.T5.9.9.1.1.1.1.m1.2.3.3.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.2">𝑂</ci><apply id="S3.T5.9.9.1.1.1.1.m1.2.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.3.3.3.2"><divide id="S3.T5.9.9.1.1.1.1.m1.2.2.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.1"></divide><cn type="integer" id="S3.T5.9.9.1.1.1.1.m1.2.2.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.2">1</cn><apply id="S3.T5.9.9.1.1.1.1.m1.2.2.3.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3"><root id="S3.T5.9.9.1.1.1.1.m1.2.2.3a.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3"></root><apply id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2"><times id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.1.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.1"></times><ci id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.2.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.2">𝑁</ci><ci id="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.3.cmml" xref="S3.T5.9.9.1.1.1.1.m1.2.2.3.2.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.9.9.1.1.1.1.m1.2c">O(\nicefrac{{1}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.10.10.2" class="ltx_tr">
<td id="S3.T5.10.10.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">PD-SGD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib304" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">304</span></a>]</cite>
</td>
<td id="S3.T5.10.10.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BCGV</td>
<td id="S3.T5.10.10.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLGV</td>
<td id="S3.T5.10.10.2.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Dec; AC</td>
<td id="S3.T5.10.10.2.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.10.10.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.10.10.2.1.1.1" class="ltx_p"><math id="S3.T5.10.10.2.1.1.1.m1.2" class="ltx_Math" alttext="O(\nicefrac{{N}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})" display="inline"><semantics id="S3.T5.10.10.2.1.1.1.m1.2a"><mrow id="S3.T5.10.10.2.1.1.1.m1.2.3" xref="S3.T5.10.10.2.1.1.1.m1.2.3.cmml"><mrow id="S3.T5.10.10.2.1.1.1.m1.2.3.2" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.cmml"><mi id="S3.T5.10.10.2.1.1.1.m1.2.3.2.2" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.10.10.2.1.1.1.m1.2.3.2.1" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.T5.10.10.2.1.1.1.m1.2.3.2.3.2" xref="S3.T5.10.10.2.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.10.10.2.1.1.1.m1.2.3.2.3.2.1" xref="S3.T5.10.10.2.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.10.10.2.1.1.1.m1.1.1" xref="S3.T5.10.10.2.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.10.10.2.1.1.1.m1.1.1.2" xref="S3.T5.10.10.2.1.1.1.m1.1.1.2.cmml"><mi mathsize="70%" id="S3.T5.10.10.2.1.1.1.m1.1.1.2a" xref="S3.T5.10.10.2.1.1.1.m1.1.1.2.cmml">N</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.10.10.2.1.1.1.m1.1.1.1" xref="S3.T5.10.10.2.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.10.10.2.1.1.1.m1.1.1.1a" xref="S3.T5.10.10.2.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mi mathsize="70%" id="S3.T5.10.10.2.1.1.1.m1.1.1.3" xref="S3.T5.10.10.2.1.1.1.m1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T5.10.10.2.1.1.1.m1.2.3.2.3.2.2" xref="S3.T5.10.10.2.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.T5.10.10.2.1.1.1.m1.2.3.1" xref="S3.T5.10.10.2.1.1.1.m1.2.3.1.cmml">+</mo><mrow id="S3.T5.10.10.2.1.1.1.m1.2.3.3" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.cmml"><mi id="S3.T5.10.10.2.1.1.1.m1.2.3.3.2" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.10.10.2.1.1.1.m1.2.3.3.1" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.T5.10.10.2.1.1.1.m1.2.3.3.3.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.cmml"><mo stretchy="false" id="S3.T5.10.10.2.1.1.1.m1.2.3.3.3.2.1" xref="S3.T5.10.10.2.1.1.1.m1.2.2.cmml">(</mo><mrow id="S3.T5.10.10.2.1.1.1.m1.2.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.cmml"><mpadded voffset="0.3em" id="S3.T5.10.10.2.1.1.1.m1.2.2.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.2.cmml"><mn mathsize="70%" id="S3.T5.10.10.2.1.1.1.m1.2.2.2a" xref="S3.T5.10.10.2.1.1.1.m1.2.2.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.10.10.2.1.1.1.m1.2.2.1" xref="S3.T5.10.10.2.1.1.1.m1.2.2.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.10.10.2.1.1.1.m1.2.2.1a" xref="S3.T5.10.10.2.1.1.1.m1.2.2.1.cmml">/</mo></mpadded><msqrt id="S3.T5.10.10.2.1.1.1.m1.2.2.3" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.cmml"><mrow id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.cmml"><mi mathsize="70%" id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.1" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.3" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.3.cmml">T</mi></mrow></msqrt></mrow><mo stretchy="false" id="S3.T5.10.10.2.1.1.1.m1.2.3.3.3.2.2" xref="S3.T5.10.10.2.1.1.1.m1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.10.10.2.1.1.1.m1.2b"><apply id="S3.T5.10.10.2.1.1.1.m1.2.3.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3"><plus id="S3.T5.10.10.2.1.1.1.m1.2.3.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.1"></plus><apply id="S3.T5.10.10.2.1.1.1.m1.2.3.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2"><times id="S3.T5.10.10.2.1.1.1.m1.2.3.2.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.1"></times><ci id="S3.T5.10.10.2.1.1.1.m1.2.3.2.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.2">𝑂</ci><apply id="S3.T5.10.10.2.1.1.1.m1.1.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.2.3.2"><divide id="S3.T5.10.10.2.1.1.1.m1.1.1.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.1.1.1"></divide><ci id="S3.T5.10.10.2.1.1.1.m1.1.1.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.1.1.2">𝑁</ci><ci id="S3.T5.10.10.2.1.1.1.m1.1.1.3.cmml" xref="S3.T5.10.10.2.1.1.1.m1.1.1.3">𝑇</ci></apply></apply><apply id="S3.T5.10.10.2.1.1.1.m1.2.3.3.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3"><times id="S3.T5.10.10.2.1.1.1.m1.2.3.3.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.1"></times><ci id="S3.T5.10.10.2.1.1.1.m1.2.3.3.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.2">𝑂</ci><apply id="S3.T5.10.10.2.1.1.1.m1.2.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.3.3.3.2"><divide id="S3.T5.10.10.2.1.1.1.m1.2.2.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.1"></divide><cn type="integer" id="S3.T5.10.10.2.1.1.1.m1.2.2.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.2">1</cn><apply id="S3.T5.10.10.2.1.1.1.m1.2.2.3.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3"><root id="S3.T5.10.10.2.1.1.1.m1.2.2.3a.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3"></root><apply id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2"><times id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.1.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.1"></times><ci id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.2.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.2">𝑁</ci><ci id="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.3.cmml" xref="S3.T5.10.10.2.1.1.1.m1.2.2.3.2.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.10.10.2.1.1.1.m1.2c">O(\nicefrac{{N}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.11.11.3" class="ltx_tr">
<td id="S3.T5.11.11.3.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">MATCHA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>]</cite>
</td>
<td id="S3.T5.11.11.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BCGV</td>
<td id="S3.T5.11.11.3.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BLGV</td>
<td id="S3.T5.11.11.3.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Dec</td>
<td id="S3.T5.11.11.3.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.11.11.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.11.11.3.1.1.1" class="ltx_p"><math id="S3.T5.11.11.3.1.1.1.m1.2" class="ltx_Math" alttext="O(\nicefrac{{1}}{{\sqrt{TKM}}})+O(\nicefrac{{M}}{{KT}})" display="inline"><semantics id="S3.T5.11.11.3.1.1.1.m1.2a"><mrow id="S3.T5.11.11.3.1.1.1.m1.2.3" xref="S3.T5.11.11.3.1.1.1.m1.2.3.cmml"><mrow id="S3.T5.11.11.3.1.1.1.m1.2.3.2" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.cmml"><mi id="S3.T5.11.11.3.1.1.1.m1.2.3.2.2" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.11.11.3.1.1.1.m1.2.3.2.1" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.T5.11.11.3.1.1.1.m1.2.3.2.3.2" xref="S3.T5.11.11.3.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.11.11.3.1.1.1.m1.2.3.2.3.2.1" xref="S3.T5.11.11.3.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.11.11.3.1.1.1.m1.1.1" xref="S3.T5.11.11.3.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.11.11.3.1.1.1.m1.1.1.2" xref="S3.T5.11.11.3.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.1.1.2a" xref="S3.T5.11.11.3.1.1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.11.11.3.1.1.1.m1.1.1.1" xref="S3.T5.11.11.3.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.11.11.3.1.1.1.m1.1.1.1a" xref="S3.T5.11.11.3.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><msqrt id="S3.T5.11.11.3.1.1.1.m1.1.1.3" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.cmml"><mrow id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.cmml"><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.2" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.3" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1a" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.4" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.4.cmml">M</mi></mrow></msqrt></mrow><mo stretchy="false" id="S3.T5.11.11.3.1.1.1.m1.2.3.2.3.2.2" xref="S3.T5.11.11.3.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.T5.11.11.3.1.1.1.m1.2.3.1" xref="S3.T5.11.11.3.1.1.1.m1.2.3.1.cmml">+</mo><mrow id="S3.T5.11.11.3.1.1.1.m1.2.3.3" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.cmml"><mi id="S3.T5.11.11.3.1.1.1.m1.2.3.3.2" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.11.11.3.1.1.1.m1.2.3.3.1" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.T5.11.11.3.1.1.1.m1.2.3.3.3.2" xref="S3.T5.11.11.3.1.1.1.m1.2.2.cmml"><mo stretchy="false" id="S3.T5.11.11.3.1.1.1.m1.2.3.3.3.2.1" xref="S3.T5.11.11.3.1.1.1.m1.2.2.cmml">(</mo><mrow id="S3.T5.11.11.3.1.1.1.m1.2.2" xref="S3.T5.11.11.3.1.1.1.m1.2.2.cmml"><mpadded voffset="0.3em" id="S3.T5.11.11.3.1.1.1.m1.2.2.2" xref="S3.T5.11.11.3.1.1.1.m1.2.2.2.cmml"><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.2.2.2a" xref="S3.T5.11.11.3.1.1.1.m1.2.2.2.cmml">M</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.11.11.3.1.1.1.m1.2.2.1" xref="S3.T5.11.11.3.1.1.1.m1.2.2.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.11.11.3.1.1.1.m1.2.2.1a" xref="S3.T5.11.11.3.1.1.1.m1.2.2.1.cmml">/</mo></mpadded><mrow id="S3.T5.11.11.3.1.1.1.m1.2.2.3" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.cmml"><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.2.2.3.2" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.T5.11.11.3.1.1.1.m1.2.2.3.1" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.11.11.3.1.1.1.m1.2.2.3.3" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.3.cmml">T</mi></mrow></mrow><mo stretchy="false" id="S3.T5.11.11.3.1.1.1.m1.2.3.3.3.2.2" xref="S3.T5.11.11.3.1.1.1.m1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.11.11.3.1.1.1.m1.2b"><apply id="S3.T5.11.11.3.1.1.1.m1.2.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3"><plus id="S3.T5.11.11.3.1.1.1.m1.2.3.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.1"></plus><apply id="S3.T5.11.11.3.1.1.1.m1.2.3.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2"><times id="S3.T5.11.11.3.1.1.1.m1.2.3.2.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.1"></times><ci id="S3.T5.11.11.3.1.1.1.m1.2.3.2.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.2">𝑂</ci><apply id="S3.T5.11.11.3.1.1.1.m1.1.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.2.3.2"><divide id="S3.T5.11.11.3.1.1.1.m1.1.1.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S3.T5.11.11.3.1.1.1.m1.1.1.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.2">1</cn><apply id="S3.T5.11.11.3.1.1.1.m1.1.1.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3"><root id="S3.T5.11.11.3.1.1.1.m1.1.1.3a.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3"></root><apply id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2"><times id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.1"></times><ci id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.2">𝑇</ci><ci id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.3">𝐾</ci><ci id="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.4.cmml" xref="S3.T5.11.11.3.1.1.1.m1.1.1.3.2.4">𝑀</ci></apply></apply></apply></apply><apply id="S3.T5.11.11.3.1.1.1.m1.2.3.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3"><times id="S3.T5.11.11.3.1.1.1.m1.2.3.3.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.1"></times><ci id="S3.T5.11.11.3.1.1.1.m1.2.3.3.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.2">𝑂</ci><apply id="S3.T5.11.11.3.1.1.1.m1.2.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.3.3.3.2"><divide id="S3.T5.11.11.3.1.1.1.m1.2.2.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.1"></divide><ci id="S3.T5.11.11.3.1.1.1.m1.2.2.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.2">𝑀</ci><apply id="S3.T5.11.11.3.1.1.1.m1.2.2.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3"><times id="S3.T5.11.11.3.1.1.1.m1.2.2.3.1.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.1"></times><ci id="S3.T5.11.11.3.1.1.1.m1.2.2.3.2.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.2">𝐾</ci><ci id="S3.T5.11.11.3.1.1.1.m1.2.2.3.3.cmml" xref="S3.T5.11.11.3.1.1.1.m1.2.2.3.3">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.11.11.3.1.1.1.m1.2c">O(\nicefrac{{1}}{{\sqrt{TKM}}})+O(\nicefrac{{M}}{{KT}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.12.12.4" class="ltx_tr">
<td id="S3.T5.12.12.4.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Khaled et al.</span> [<a href="#bib.bib268" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">268</span></a>]</cite></td>
<td id="S3.T5.12.12.4.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BOGV</td>
<td id="S3.T5.12.12.4.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">CVX</td>
<td id="S3.T5.12.12.4.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">AC; LBG</td>
<td id="S3.T5.12.12.4.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.12.12.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.12.12.4.1.1.1" class="ltx_p"><math id="S3.T5.12.12.4.1.1.1.m1.2" class="ltx_Math" alttext="O(\nicefrac{{N}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})" display="inline"><semantics id="S3.T5.12.12.4.1.1.1.m1.2a"><mrow id="S3.T5.12.12.4.1.1.1.m1.2.3" xref="S3.T5.12.12.4.1.1.1.m1.2.3.cmml"><mrow id="S3.T5.12.12.4.1.1.1.m1.2.3.2" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.cmml"><mi id="S3.T5.12.12.4.1.1.1.m1.2.3.2.2" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.12.12.4.1.1.1.m1.2.3.2.1" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.1.cmml">​</mo><mrow id="S3.T5.12.12.4.1.1.1.m1.2.3.2.3.2" xref="S3.T5.12.12.4.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.12.12.4.1.1.1.m1.2.3.2.3.2.1" xref="S3.T5.12.12.4.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.12.12.4.1.1.1.m1.1.1" xref="S3.T5.12.12.4.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.12.12.4.1.1.1.m1.1.1.2" xref="S3.T5.12.12.4.1.1.1.m1.1.1.2.cmml"><mi mathsize="70%" id="S3.T5.12.12.4.1.1.1.m1.1.1.2a" xref="S3.T5.12.12.4.1.1.1.m1.1.1.2.cmml">N</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.12.12.4.1.1.1.m1.1.1.1" xref="S3.T5.12.12.4.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.12.12.4.1.1.1.m1.1.1.1a" xref="S3.T5.12.12.4.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mi mathsize="70%" id="S3.T5.12.12.4.1.1.1.m1.1.1.3" xref="S3.T5.12.12.4.1.1.1.m1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T5.12.12.4.1.1.1.m1.2.3.2.3.2.2" xref="S3.T5.12.12.4.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.T5.12.12.4.1.1.1.m1.2.3.1" xref="S3.T5.12.12.4.1.1.1.m1.2.3.1.cmml">+</mo><mrow id="S3.T5.12.12.4.1.1.1.m1.2.3.3" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.cmml"><mi id="S3.T5.12.12.4.1.1.1.m1.2.3.3.2" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.12.12.4.1.1.1.m1.2.3.3.1" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S3.T5.12.12.4.1.1.1.m1.2.3.3.3.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.cmml"><mo stretchy="false" id="S3.T5.12.12.4.1.1.1.m1.2.3.3.3.2.1" xref="S3.T5.12.12.4.1.1.1.m1.2.2.cmml">(</mo><mrow id="S3.T5.12.12.4.1.1.1.m1.2.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.cmml"><mpadded voffset="0.3em" id="S3.T5.12.12.4.1.1.1.m1.2.2.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.2.cmml"><mn mathsize="70%" id="S3.T5.12.12.4.1.1.1.m1.2.2.2a" xref="S3.T5.12.12.4.1.1.1.m1.2.2.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.12.12.4.1.1.1.m1.2.2.1" xref="S3.T5.12.12.4.1.1.1.m1.2.2.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.12.12.4.1.1.1.m1.2.2.1a" xref="S3.T5.12.12.4.1.1.1.m1.2.2.1.cmml">/</mo></mpadded><msqrt id="S3.T5.12.12.4.1.1.1.m1.2.2.3" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.cmml"><mrow id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.cmml"><mi mathsize="70%" id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.1" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.3" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.3.cmml">T</mi></mrow></msqrt></mrow><mo stretchy="false" id="S3.T5.12.12.4.1.1.1.m1.2.3.3.3.2.2" xref="S3.T5.12.12.4.1.1.1.m1.2.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.12.12.4.1.1.1.m1.2b"><apply id="S3.T5.12.12.4.1.1.1.m1.2.3.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3"><plus id="S3.T5.12.12.4.1.1.1.m1.2.3.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.1"></plus><apply id="S3.T5.12.12.4.1.1.1.m1.2.3.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2"><times id="S3.T5.12.12.4.1.1.1.m1.2.3.2.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.1"></times><ci id="S3.T5.12.12.4.1.1.1.m1.2.3.2.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.2">𝑂</ci><apply id="S3.T5.12.12.4.1.1.1.m1.1.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.2.3.2"><divide id="S3.T5.12.12.4.1.1.1.m1.1.1.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.1.1.1"></divide><ci id="S3.T5.12.12.4.1.1.1.m1.1.1.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.1.1.2">𝑁</ci><ci id="S3.T5.12.12.4.1.1.1.m1.1.1.3.cmml" xref="S3.T5.12.12.4.1.1.1.m1.1.1.3">𝑇</ci></apply></apply><apply id="S3.T5.12.12.4.1.1.1.m1.2.3.3.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3"><times id="S3.T5.12.12.4.1.1.1.m1.2.3.3.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.1"></times><ci id="S3.T5.12.12.4.1.1.1.m1.2.3.3.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.2">𝑂</ci><apply id="S3.T5.12.12.4.1.1.1.m1.2.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.3.3.3.2"><divide id="S3.T5.12.12.4.1.1.1.m1.2.2.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.1"></divide><cn type="integer" id="S3.T5.12.12.4.1.1.1.m1.2.2.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.2">1</cn><apply id="S3.T5.12.12.4.1.1.1.m1.2.2.3.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3"><root id="S3.T5.12.12.4.1.1.1.m1.2.2.3a.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3"></root><apply id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2"><times id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.1.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.1"></times><ci id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.2.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.2">𝑁</ci><ci id="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.3.cmml" xref="S3.T5.12.12.4.1.1.1.m1.2.2.3.2.3">𝑇</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.12.12.4.1.1.1.m1.2c">O(\nicefrac{{N}}{{T}})+O(\nicefrac{{1}}{{\sqrt{NT}}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.13.13.5" class="ltx_tr">
<td id="S3.T5.13.13.5.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>]</cite></td>
<td id="S3.T5.13.13.5.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BOBD</td>
<td id="S3.T5.13.13.5.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">SCVX; BLGV; BLGN</td>
<td id="S3.T5.13.13.5.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S3.T5.13.13.5.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.13.13.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.13.13.5.1.1.1" class="ltx_p"><math id="S3.T5.13.13.5.1.1.1.m1.1" class="ltx_Math" alttext="O(\nicefrac{{K}}{{T}})" display="inline"><semantics id="S3.T5.13.13.5.1.1.1.m1.1a"><mrow id="S3.T5.13.13.5.1.1.1.m1.1.2" xref="S3.T5.13.13.5.1.1.1.m1.1.2.cmml"><mi id="S3.T5.13.13.5.1.1.1.m1.1.2.2" xref="S3.T5.13.13.5.1.1.1.m1.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.13.13.5.1.1.1.m1.1.2.1" xref="S3.T5.13.13.5.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.T5.13.13.5.1.1.1.m1.1.2.3.2" xref="S3.T5.13.13.5.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.13.13.5.1.1.1.m1.1.2.3.2.1" xref="S3.T5.13.13.5.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.13.13.5.1.1.1.m1.1.1" xref="S3.T5.13.13.5.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.13.13.5.1.1.1.m1.1.1.2" xref="S3.T5.13.13.5.1.1.1.m1.1.1.2.cmml"><mi mathsize="70%" id="S3.T5.13.13.5.1.1.1.m1.1.1.2a" xref="S3.T5.13.13.5.1.1.1.m1.1.1.2.cmml">K</mi></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.13.13.5.1.1.1.m1.1.1.1" xref="S3.T5.13.13.5.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.13.13.5.1.1.1.m1.1.1.1a" xref="S3.T5.13.13.5.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mi mathsize="70%" id="S3.T5.13.13.5.1.1.1.m1.1.1.3" xref="S3.T5.13.13.5.1.1.1.m1.1.1.3.cmml">T</mi></mrow><mo stretchy="false" id="S3.T5.13.13.5.1.1.1.m1.1.2.3.2.2" xref="S3.T5.13.13.5.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.13.13.5.1.1.1.m1.1b"><apply id="S3.T5.13.13.5.1.1.1.m1.1.2.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.2"><times id="S3.T5.13.13.5.1.1.1.m1.1.2.1.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.2.1"></times><ci id="S3.T5.13.13.5.1.1.1.m1.1.2.2.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.2.2">𝑂</ci><apply id="S3.T5.13.13.5.1.1.1.m1.1.1.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.2.3.2"><divide id="S3.T5.13.13.5.1.1.1.m1.1.1.1.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.1.1"></divide><ci id="S3.T5.13.13.5.1.1.1.m1.1.1.2.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.1.2">𝐾</ci><ci id="S3.T5.13.13.5.1.1.1.m1.1.1.3.cmml" xref="S3.T5.13.13.5.1.1.1.m1.1.1.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.13.13.5.1.1.1.m1.1c">O(\nicefrac{{K}}{{T}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.14.14.6" class="ltx_tr">
<td id="S3.T5.14.14.6.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">FedProx <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib300" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">300</span></a>]</cite>
</td>
<td id="S3.T5.14.14.6.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BGV</td>
<td id="S3.T5.14.14.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">BNCVX</td>
<td id="S3.T5.14.14.6.5" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">Prox</td>
<td id="S3.T5.14.14.6.1" class="ltx_td ltx_align_justify ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.14.14.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.14.14.6.1.1.1" class="ltx_p"><math id="S3.T5.14.14.6.1.1.1.m1.1" class="ltx_Math" alttext="O(\nicefrac{{1}}{{\sqrt{T}}})" display="inline"><semantics id="S3.T5.14.14.6.1.1.1.m1.1a"><mrow id="S3.T5.14.14.6.1.1.1.m1.1.2" xref="S3.T5.14.14.6.1.1.1.m1.1.2.cmml"><mi id="S3.T5.14.14.6.1.1.1.m1.1.2.2" xref="S3.T5.14.14.6.1.1.1.m1.1.2.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.14.14.6.1.1.1.m1.1.2.1" xref="S3.T5.14.14.6.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S3.T5.14.14.6.1.1.1.m1.1.2.3.2" xref="S3.T5.14.14.6.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.14.14.6.1.1.1.m1.1.2.3.2.1" xref="S3.T5.14.14.6.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.14.14.6.1.1.1.m1.1.1" xref="S3.T5.14.14.6.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.14.14.6.1.1.1.m1.1.1.2" xref="S3.T5.14.14.6.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S3.T5.14.14.6.1.1.1.m1.1.1.2a" xref="S3.T5.14.14.6.1.1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.14.14.6.1.1.1.m1.1.1.1" xref="S3.T5.14.14.6.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.14.14.6.1.1.1.m1.1.1.1a" xref="S3.T5.14.14.6.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><msqrt id="S3.T5.14.14.6.1.1.1.m1.1.1.3" xref="S3.T5.14.14.6.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S3.T5.14.14.6.1.1.1.m1.1.1.3.2" xref="S3.T5.14.14.6.1.1.1.m1.1.1.3.2.cmml">T</mi></msqrt></mrow><mo stretchy="false" id="S3.T5.14.14.6.1.1.1.m1.1.2.3.2.2" xref="S3.T5.14.14.6.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.14.14.6.1.1.1.m1.1b"><apply id="S3.T5.14.14.6.1.1.1.m1.1.2.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.2"><times id="S3.T5.14.14.6.1.1.1.m1.1.2.1.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.2.1"></times><ci id="S3.T5.14.14.6.1.1.1.m1.1.2.2.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.2.2">𝑂</ci><apply id="S3.T5.14.14.6.1.1.1.m1.1.1.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.2.3.2"><divide id="S3.T5.14.14.6.1.1.1.m1.1.1.1.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S3.T5.14.14.6.1.1.1.m1.1.1.2.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.1.2">1</cn><apply id="S3.T5.14.14.6.1.1.1.m1.1.1.3.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.1.3"><root id="S3.T5.14.14.6.1.1.1.m1.1.1.3a.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.1.3"></root><ci id="S3.T5.14.14.6.1.1.1.m1.1.1.3.2.cmml" xref="S3.T5.14.14.6.1.1.1.m1.1.1.3.2">𝑇</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.14.14.6.1.1.1.m1.1c">O(\nicefrac{{1}}{{\sqrt{T}}})</annotation></semantics></math></span>
</span>
</td>
</tr>
<tr id="S3.T5.15.15.7" class="ltx_tr">
<td id="S3.T5.15.15.7.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">SCAFFOLD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite>
</td>
<td id="S3.T5.15.15.7.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">-</td>
<td id="S3.T5.15.15.7.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">SCVX; BLGV</td>
<td id="S3.T5.15.15.7.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">VR</td>
<td id="S3.T5.15.15.7.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span id="S3.T5.15.15.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T5.15.15.7.1.1.1" class="ltx_p"><math id="S3.T5.15.15.7.1.1.1.m1.2" class="ltx_Math" alttext="O(\nicefrac{{1}}{{TKM}})+O(e^{-T})" display="inline"><semantics id="S3.T5.15.15.7.1.1.1.m1.2a"><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.cmml"><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2.3" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.cmml"><mi id="S3.T5.15.15.7.1.1.1.m1.2.2.3.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.15.15.7.1.1.1.m1.2.2.3.1" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.1.cmml">​</mo><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2.3.3.2" xref="S3.T5.15.15.7.1.1.1.m1.1.1.cmml"><mo stretchy="false" id="S3.T5.15.15.7.1.1.1.m1.2.2.3.3.2.1" xref="S3.T5.15.15.7.1.1.1.m1.1.1.cmml">(</mo><mrow id="S3.T5.15.15.7.1.1.1.m1.1.1" xref="S3.T5.15.15.7.1.1.1.m1.1.1.cmml"><mpadded voffset="0.3em" id="S3.T5.15.15.7.1.1.1.m1.1.1.2" xref="S3.T5.15.15.7.1.1.1.m1.1.1.2.cmml"><mn mathsize="70%" id="S3.T5.15.15.7.1.1.1.m1.1.1.2a" xref="S3.T5.15.15.7.1.1.1.m1.1.1.2.cmml">1</mn></mpadded><mpadded lspace="-0.1em" width="-0.15em" id="S3.T5.15.15.7.1.1.1.m1.1.1.1" xref="S3.T5.15.15.7.1.1.1.m1.1.1.1.cmml"><mo stretchy="true" symmetric="true" id="S3.T5.15.15.7.1.1.1.m1.1.1.1a" xref="S3.T5.15.15.7.1.1.1.m1.1.1.1.cmml">/</mo></mpadded><mrow id="S3.T5.15.15.7.1.1.1.m1.1.1.3" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S3.T5.15.15.7.1.1.1.m1.1.1.3.2" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.T5.15.15.7.1.1.1.m1.1.1.3.1" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.15.15.7.1.1.1.m1.1.1.3.3" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.T5.15.15.7.1.1.1.m1.1.1.3.1a" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathsize="70%" id="S3.T5.15.15.7.1.1.1.m1.1.1.3.4" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.4.cmml">M</mi></mrow></mrow><mo stretchy="false" id="S3.T5.15.15.7.1.1.1.m1.2.2.3.3.2.2" xref="S3.T5.15.15.7.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.T5.15.15.7.1.1.1.m1.2.2.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.2.cmml">+</mo><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2.1" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.cmml"><mi id="S3.T5.15.15.7.1.1.1.m1.2.2.1.3" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.T5.15.15.7.1.1.1.m1.2.2.1.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.2.cmml">​</mo><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.cmml">(</mo><msup id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.cmml"><mi id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.2.cmml">e</mi><mrow id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.cmml"><mo id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3a" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.cmml">−</mo><mi id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.2" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.2.cmml">T</mi></mrow></msup><mo stretchy="false" id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.3" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.T5.15.15.7.1.1.1.m1.2b"><apply id="S3.T5.15.15.7.1.1.1.m1.2.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2"><plus id="S3.T5.15.15.7.1.1.1.m1.2.2.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.2"></plus><apply id="S3.T5.15.15.7.1.1.1.m1.2.2.3.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3"><times id="S3.T5.15.15.7.1.1.1.m1.2.2.3.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.1"></times><ci id="S3.T5.15.15.7.1.1.1.m1.2.2.3.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.2">𝑂</ci><apply id="S3.T5.15.15.7.1.1.1.m1.1.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.3.3.2"><divide id="S3.T5.15.15.7.1.1.1.m1.1.1.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.1"></divide><cn type="integer" id="S3.T5.15.15.7.1.1.1.m1.1.1.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.2">1</cn><apply id="S3.T5.15.15.7.1.1.1.m1.1.1.3.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3"><times id="S3.T5.15.15.7.1.1.1.m1.1.1.3.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.1"></times><ci id="S3.T5.15.15.7.1.1.1.m1.1.1.3.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.2">𝑇</ci><ci id="S3.T5.15.15.7.1.1.1.m1.1.1.3.3.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.3">𝐾</ci><ci id="S3.T5.15.15.7.1.1.1.m1.1.1.3.4.cmml" xref="S3.T5.15.15.7.1.1.1.m1.1.1.3.4">𝑀</ci></apply></apply></apply><apply id="S3.T5.15.15.7.1.1.1.m1.2.2.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1"><times id="S3.T5.15.15.7.1.1.1.m1.2.2.1.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.2"></times><ci id="S3.T5.15.15.7.1.1.1.m1.2.2.1.3.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.3">𝑂</ci><apply id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1">superscript</csymbol><ci id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.2">𝑒</ci><apply id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3"><minus id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.1.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3"></minus><ci id="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.2.cmml" xref="S3.T5.15.15.7.1.1.1.m1.2.2.1.1.1.1.3.2">𝑇</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.15.15.7.1.1.1.m1.2c">O(\nicefrac{{1}}{{TKM}})+O(e^{-T})</annotation></semantics></math></span>
</span>
</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Convergence rates for a (non-comprehensive) set of federated optimization methods in non-IID settings. We summarize the key assumptions for non-IID data, local functions on each client, and other assumptions. We also present the variant of the algorithm comparing to Federated Averaging and the convergence rates that eliminate constant.
</figcaption>
</figure>
</section>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Optimization Algorithms and Convergence Rates for Non-IID Datasets</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">In contrast to well-shuffled mini-batches consisting of independent and identically distributed (IID) examples in centralized learning, federated learning uses local data from end user devices, leading to many varieties of non-IID data (Section <a href="#S3.SS1" title="3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>).</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.2" class="ltx_p">In this setting, each of <math id="S3.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS2.p2.1.m1.1a"><mi id="S3.SS2.SSS2.p2.1.m1.1.1" xref="S3.SS2.SSS2.p2.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.1.m1.1b"><ci id="S3.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.1.m1.1c">N</annotation></semantics></math> clients has a local data distribution <math id="S3.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS2.SSS2.p2.2.m2.1a"><msub id="S3.SS2.SSS2.p2.2.m2.1.1" xref="S3.SS2.SSS2.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p2.2.m2.1.1.2" xref="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p2.2.m2.1.1.3" xref="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.2.m2.1b"><apply id="S3.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.2.m2.1c">\mathcal{P}_{i}</annotation></semantics></math> and a local objective function</p>
<table id="S3.Ex6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.Ex6.m1.5" class="ltx_Math" alttext="f_{i}(x)=\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}_{i}}[f(x;z)]" display="block"><semantics id="S3.Ex6.m1.5a"><mrow id="S3.Ex6.m1.5.5" xref="S3.Ex6.m1.5.5.cmml"><mrow id="S3.Ex6.m1.5.5.4" xref="S3.Ex6.m1.5.5.4.cmml"><msub id="S3.Ex6.m1.5.5.4.2" xref="S3.Ex6.m1.5.5.4.2.cmml"><mi id="S3.Ex6.m1.5.5.4.2.2" xref="S3.Ex6.m1.5.5.4.2.2.cmml">f</mi><mi id="S3.Ex6.m1.5.5.4.2.3" xref="S3.Ex6.m1.5.5.4.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex6.m1.5.5.4.1" xref="S3.Ex6.m1.5.5.4.1.cmml">​</mo><mrow id="S3.Ex6.m1.5.5.4.3.2" xref="S3.Ex6.m1.5.5.4.cmml"><mo stretchy="false" id="S3.Ex6.m1.5.5.4.3.2.1" xref="S3.Ex6.m1.5.5.4.cmml">(</mo><mi id="S3.Ex6.m1.1.1" xref="S3.Ex6.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.Ex6.m1.5.5.4.3.2.2" xref="S3.Ex6.m1.5.5.4.cmml">)</mo></mrow></mrow><mo rspace="0.1389em" id="S3.Ex6.m1.5.5.3" xref="S3.Ex6.m1.5.5.3.cmml">=</mo><mrow id="S3.Ex6.m1.5.5.2.2" xref="S3.Ex6.m1.5.5.2.3.cmml"><munder id="S3.Ex6.m1.4.4.1.1.1" xref="S3.Ex6.m1.4.4.1.1.1.cmml"><mo lspace="0.1389em" rspace="0em" id="S3.Ex6.m1.4.4.1.1.1.2" xref="S3.Ex6.m1.4.4.1.1.1.2.cmml">𝔼</mo><mrow id="S3.Ex6.m1.4.4.1.1.1.3" xref="S3.Ex6.m1.4.4.1.1.1.3.cmml"><mi id="S3.Ex6.m1.4.4.1.1.1.3.2" xref="S3.Ex6.m1.4.4.1.1.1.3.2.cmml">z</mi><mo id="S3.Ex6.m1.4.4.1.1.1.3.1" xref="S3.Ex6.m1.4.4.1.1.1.3.1.cmml">∼</mo><msub id="S3.Ex6.m1.4.4.1.1.1.3.3" xref="S3.Ex6.m1.4.4.1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex6.m1.4.4.1.1.1.3.3.2" xref="S3.Ex6.m1.4.4.1.1.1.3.3.2.cmml">𝒫</mi><mi id="S3.Ex6.m1.4.4.1.1.1.3.3.3" xref="S3.Ex6.m1.4.4.1.1.1.3.3.3.cmml">i</mi></msub></mrow></munder><mrow id="S3.Ex6.m1.5.5.2.2.2" xref="S3.Ex6.m1.5.5.2.3.cmml"><mo stretchy="false" id="S3.Ex6.m1.5.5.2.2.2.2" xref="S3.Ex6.m1.5.5.2.3.cmml">[</mo><mrow id="S3.Ex6.m1.5.5.2.2.2.1" xref="S3.Ex6.m1.5.5.2.2.2.1.cmml"><mi id="S3.Ex6.m1.5.5.2.2.2.1.2" xref="S3.Ex6.m1.5.5.2.2.2.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.Ex6.m1.5.5.2.2.2.1.1" xref="S3.Ex6.m1.5.5.2.2.2.1.1.cmml">​</mo><mrow id="S3.Ex6.m1.5.5.2.2.2.1.3.2" xref="S3.Ex6.m1.5.5.2.2.2.1.3.1.cmml"><mo stretchy="false" id="S3.Ex6.m1.5.5.2.2.2.1.3.2.1" xref="S3.Ex6.m1.5.5.2.2.2.1.3.1.cmml">(</mo><mi id="S3.Ex6.m1.2.2" xref="S3.Ex6.m1.2.2.cmml">x</mi><mo id="S3.Ex6.m1.5.5.2.2.2.1.3.2.2" xref="S3.Ex6.m1.5.5.2.2.2.1.3.1.cmml">;</mo><mi id="S3.Ex6.m1.3.3" xref="S3.Ex6.m1.3.3.cmml">z</mi><mo stretchy="false" id="S3.Ex6.m1.5.5.2.2.2.1.3.2.3" xref="S3.Ex6.m1.5.5.2.2.2.1.3.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex6.m1.5.5.2.2.2.3" xref="S3.Ex6.m1.5.5.2.3.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex6.m1.5b"><apply id="S3.Ex6.m1.5.5.cmml" xref="S3.Ex6.m1.5.5"><eq id="S3.Ex6.m1.5.5.3.cmml" xref="S3.Ex6.m1.5.5.3"></eq><apply id="S3.Ex6.m1.5.5.4.cmml" xref="S3.Ex6.m1.5.5.4"><times id="S3.Ex6.m1.5.5.4.1.cmml" xref="S3.Ex6.m1.5.5.4.1"></times><apply id="S3.Ex6.m1.5.5.4.2.cmml" xref="S3.Ex6.m1.5.5.4.2"><csymbol cd="ambiguous" id="S3.Ex6.m1.5.5.4.2.1.cmml" xref="S3.Ex6.m1.5.5.4.2">subscript</csymbol><ci id="S3.Ex6.m1.5.5.4.2.2.cmml" xref="S3.Ex6.m1.5.5.4.2.2">𝑓</ci><ci id="S3.Ex6.m1.5.5.4.2.3.cmml" xref="S3.Ex6.m1.5.5.4.2.3">𝑖</ci></apply><ci id="S3.Ex6.m1.1.1.cmml" xref="S3.Ex6.m1.1.1">𝑥</ci></apply><apply id="S3.Ex6.m1.5.5.2.3.cmml" xref="S3.Ex6.m1.5.5.2.2"><apply id="S3.Ex6.m1.4.4.1.1.1.cmml" xref="S3.Ex6.m1.4.4.1.1.1"><csymbol cd="ambiguous" id="S3.Ex6.m1.4.4.1.1.1.1.cmml" xref="S3.Ex6.m1.4.4.1.1.1">subscript</csymbol><ci id="S3.Ex6.m1.4.4.1.1.1.2.cmml" xref="S3.Ex6.m1.4.4.1.1.1.2">𝔼</ci><apply id="S3.Ex6.m1.4.4.1.1.1.3.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3"><csymbol cd="latexml" id="S3.Ex6.m1.4.4.1.1.1.3.1.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.1">similar-to</csymbol><ci id="S3.Ex6.m1.4.4.1.1.1.3.2.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.2">𝑧</ci><apply id="S3.Ex6.m1.4.4.1.1.1.3.3.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex6.m1.4.4.1.1.1.3.3.1.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex6.m1.4.4.1.1.1.3.3.2.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.3.2">𝒫</ci><ci id="S3.Ex6.m1.4.4.1.1.1.3.3.3.cmml" xref="S3.Ex6.m1.4.4.1.1.1.3.3.3">𝑖</ci></apply></apply></apply><apply id="S3.Ex6.m1.5.5.2.2.2.1.cmml" xref="S3.Ex6.m1.5.5.2.2.2.1"><times id="S3.Ex6.m1.5.5.2.2.2.1.1.cmml" xref="S3.Ex6.m1.5.5.2.2.2.1.1"></times><ci id="S3.Ex6.m1.5.5.2.2.2.1.2.cmml" xref="S3.Ex6.m1.5.5.2.2.2.1.2">𝑓</ci><list id="S3.Ex6.m1.5.5.2.2.2.1.3.1.cmml" xref="S3.Ex6.m1.5.5.2.2.2.1.3.2"><ci id="S3.Ex6.m1.2.2.cmml" xref="S3.Ex6.m1.2.2">𝑥</ci><ci id="S3.Ex6.m1.3.3.cmml" xref="S3.Ex6.m1.3.3">𝑧</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m1.5c">f_{i}(x)=\operatorname*{\mathbb{E}}_{z\sim\mathcal{P}_{i}}[f(x;z)]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p2.5" class="ltx_p">where we recall that <math id="S3.SS2.SSS2.p2.3.m1.2" class="ltx_Math" alttext="f(x;z)" display="inline"><semantics id="S3.SS2.SSS2.p2.3.m1.2a"><mrow id="S3.SS2.SSS2.p2.3.m1.2.3" xref="S3.SS2.SSS2.p2.3.m1.2.3.cmml"><mi id="S3.SS2.SSS2.p2.3.m1.2.3.2" xref="S3.SS2.SSS2.p2.3.m1.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p2.3.m1.2.3.1" xref="S3.SS2.SSS2.p2.3.m1.2.3.1.cmml">​</mo><mrow id="S3.SS2.SSS2.p2.3.m1.2.3.3.2" xref="S3.SS2.SSS2.p2.3.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p2.3.m1.2.3.3.2.1" xref="S3.SS2.SSS2.p2.3.m1.2.3.3.1.cmml">(</mo><mi id="S3.SS2.SSS2.p2.3.m1.1.1" xref="S3.SS2.SSS2.p2.3.m1.1.1.cmml">x</mi><mo id="S3.SS2.SSS2.p2.3.m1.2.3.3.2.2" xref="S3.SS2.SSS2.p2.3.m1.2.3.3.1.cmml">;</mo><mi id="S3.SS2.SSS2.p2.3.m1.2.2" xref="S3.SS2.SSS2.p2.3.m1.2.2.cmml">z</mi><mo stretchy="false" id="S3.SS2.SSS2.p2.3.m1.2.3.3.2.3" xref="S3.SS2.SSS2.p2.3.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.3.m1.2b"><apply id="S3.SS2.SSS2.p2.3.m1.2.3.cmml" xref="S3.SS2.SSS2.p2.3.m1.2.3"><times id="S3.SS2.SSS2.p2.3.m1.2.3.1.cmml" xref="S3.SS2.SSS2.p2.3.m1.2.3.1"></times><ci id="S3.SS2.SSS2.p2.3.m1.2.3.2.cmml" xref="S3.SS2.SSS2.p2.3.m1.2.3.2">𝑓</ci><list id="S3.SS2.SSS2.p2.3.m1.2.3.3.1.cmml" xref="S3.SS2.SSS2.p2.3.m1.2.3.3.2"><ci id="S3.SS2.SSS2.p2.3.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.3.m1.1.1">𝑥</ci><ci id="S3.SS2.SSS2.p2.3.m1.2.2.cmml" xref="S3.SS2.SSS2.p2.3.m1.2.2">𝑧</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.3.m1.2c">f(x;z)</annotation></semantics></math> is the loss of a model <math id="S3.SS2.SSS2.p2.4.m2.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S3.SS2.SSS2.p2.4.m2.1a"><mi id="S3.SS2.SSS2.p2.4.m2.1.1" xref="S3.SS2.SSS2.p2.4.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.4.m2.1b"><ci id="S3.SS2.SSS2.p2.4.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.4.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.4.m2.1c">x</annotation></semantics></math> at an example <math id="S3.SS2.SSS2.p2.5.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S3.SS2.SSS2.p2.5.m3.1a"><mi id="S3.SS2.SSS2.p2.5.m3.1.1" xref="S3.SS2.SSS2.p2.5.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.5.m3.1b"><ci id="S3.SS2.SSS2.p2.5.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.5.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.5.m3.1c">z</annotation></semantics></math>. We typically wish to minimize</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.3" class="ltx_Math" alttext="F(x)=\frac{1}{N}\sum_{i=1}^{N}f_{i}(x)\,." display="block"><semantics id="S3.E2.m1.3a"><mrow id="S3.E2.m1.3.3.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1" xref="S3.E2.m1.3.3.1.1.cmml"><mrow id="S3.E2.m1.3.3.1.1.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mi id="S3.E2.m1.3.3.1.1.2.2" xref="S3.E2.m1.3.3.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.2.1" xref="S3.E2.m1.3.3.1.1.2.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.2.3.2" xref="S3.E2.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.3.2.1" xref="S3.E2.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.2.3.2.2" xref="S3.E2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.3.3.1.1.1" xref="S3.E2.m1.3.3.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.3.3.1.1.3" xref="S3.E2.m1.3.3.1.1.3.cmml"><mfrac id="S3.E2.m1.3.3.1.1.3.2" xref="S3.E2.m1.3.3.1.1.3.2.cmml"><mn id="S3.E2.m1.3.3.1.1.3.2.2" xref="S3.E2.m1.3.3.1.1.3.2.2.cmml">1</mn><mi id="S3.E2.m1.3.3.1.1.3.2.3" xref="S3.E2.m1.3.3.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.1" xref="S3.E2.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.3.3" xref="S3.E2.m1.3.3.1.1.3.3.cmml"><munderover id="S3.E2.m1.3.3.1.1.3.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.cmml"><mo movablelimits="false" id="S3.E2.m1.3.3.1.1.3.3.1.2.2" xref="S3.E2.m1.3.3.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E2.m1.3.3.1.1.3.3.1.2.3" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.1.2.3.2" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.3.3.1.1.3.3.1.2.3.1" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E2.m1.3.3.1.1.3.3.1.2.3.3" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.3.3.1.1.3.3.1.3" xref="S3.E2.m1.3.3.1.1.3.3.1.3.cmml">N</mi></munderover><mrow id="S3.E2.m1.3.3.1.1.3.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml"><msub id="S3.E2.m1.3.3.1.1.3.3.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.cmml"><mi id="S3.E2.m1.3.3.1.1.3.3.2.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.2.2.cmml">f</mi><mi id="S3.E2.m1.3.3.1.1.3.3.2.2.3" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.E2.m1.3.3.1.1.3.3.2.1" xref="S3.E2.m1.3.3.1.1.3.3.2.1.cmml">​</mo><mrow id="S3.E2.m1.3.3.1.1.3.3.2.3.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml"><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.3.2.3.2.1" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml">(</mo><mi id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml">x</mi><mo stretchy="false" id="S3.E2.m1.3.3.1.1.3.3.2.3.2.2" xref="S3.E2.m1.3.3.1.1.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo lspace="0.170em" id="S3.E2.m1.3.3.1.2" xref="S3.E2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.3b"><apply id="S3.E2.m1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1"><eq id="S3.E2.m1.3.3.1.1.1.cmml" xref="S3.E2.m1.3.3.1.1.1"></eq><apply id="S3.E2.m1.3.3.1.1.2.cmml" xref="S3.E2.m1.3.3.1.1.2"><times id="S3.E2.m1.3.3.1.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.2.1"></times><ci id="S3.E2.m1.3.3.1.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.2.2">𝐹</ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑥</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3"><times id="S3.E2.m1.3.3.1.1.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.1"></times><apply id="S3.E2.m1.3.3.1.1.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2"><divide id="S3.E2.m1.3.3.1.1.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.2"></divide><cn type="integer" id="S3.E2.m1.3.3.1.1.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.2.2">1</cn><ci id="S3.E2.m1.3.3.1.1.3.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.2.3">𝑁</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3"><apply id="S3.E2.m1.3.3.1.1.3.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.1.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1">superscript</csymbol><apply id="S3.E2.m1.3.3.1.1.3.3.1.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.1.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1">subscript</csymbol><sum id="S3.E2.m1.3.3.1.1.3.3.1.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.2"></sum><apply id="S3.E2.m1.3.3.1.1.3.3.1.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3"><eq id="S3.E2.m1.3.3.1.1.3.3.1.2.3.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.1"></eq><ci id="S3.E2.m1.3.3.1.1.3.3.1.2.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S3.E2.m1.3.3.1.1.3.3.1.2.3.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.3.3.1.1.3.3.1.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.1.3">𝑁</ci></apply><apply id="S3.E2.m1.3.3.1.1.3.3.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2"><times id="S3.E2.m1.3.3.1.1.3.3.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.1"></times><apply id="S3.E2.m1.3.3.1.1.3.3.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.3.3.1.1.3.3.2.2.1.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E2.m1.3.3.1.1.3.3.2.2.2.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.2">𝑓</ci><ci id="S3.E2.m1.3.3.1.1.3.3.2.2.3.cmml" xref="S3.E2.m1.3.3.1.1.3.3.2.2.3">𝑖</ci></apply><ci id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.3c">F(x)=\frac{1}{N}\sum_{i=1}^{N}f_{i}(x)\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.SSS2.p2.11" class="ltx_p">Note that we recover the IID setting when each <math id="S3.SS2.SSS2.p2.6.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS2.SSS2.p2.6.m1.1a"><msub id="S3.SS2.SSS2.p2.6.m1.1.1" xref="S3.SS2.SSS2.p2.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p2.6.m1.1.1.2" xref="S3.SS2.SSS2.p2.6.m1.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p2.6.m1.1.1.3" xref="S3.SS2.SSS2.p2.6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.6.m1.1b"><apply id="S3.SS2.SSS2.p2.6.m1.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.6.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p2.6.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.6.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p2.6.m1.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p2.6.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p2.6.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.6.m1.1c">\mathcal{P}_{i}</annotation></semantics></math> is identical. We will let <math id="S3.SS2.SSS2.p2.7.m2.1" class="ltx_Math" alttext="F^{*}" display="inline"><semantics id="S3.SS2.SSS2.p2.7.m2.1a"><msup id="S3.SS2.SSS2.p2.7.m2.1.1" xref="S3.SS2.SSS2.p2.7.m2.1.1.cmml"><mi id="S3.SS2.SSS2.p2.7.m2.1.1.2" xref="S3.SS2.SSS2.p2.7.m2.1.1.2.cmml">F</mi><mo id="S3.SS2.SSS2.p2.7.m2.1.1.3" xref="S3.SS2.SSS2.p2.7.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.7.m2.1b"><apply id="S3.SS2.SSS2.p2.7.m2.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.7.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p2.7.m2.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p2.7.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p2.7.m2.1.1.2">𝐹</ci><times id="S3.SS2.SSS2.p2.7.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p2.7.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.7.m2.1c">F^{*}</annotation></semantics></math> denote the minimum value of <math id="S3.SS2.SSS2.p2.8.m3.1" class="ltx_Math" alttext="F" display="inline"><semantics id="S3.SS2.SSS2.p2.8.m3.1a"><mi id="S3.SS2.SSS2.p2.8.m3.1.1" xref="S3.SS2.SSS2.p2.8.m3.1.1.cmml">F</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.8.m3.1b"><ci id="S3.SS2.SSS2.p2.8.m3.1.1.cmml" xref="S3.SS2.SSS2.p2.8.m3.1.1">𝐹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.8.m3.1c">F</annotation></semantics></math>, obtained the point <math id="S3.SS2.SSS2.p2.9.m4.1" class="ltx_Math" alttext="x^{*}" display="inline"><semantics id="S3.SS2.SSS2.p2.9.m4.1a"><msup id="S3.SS2.SSS2.p2.9.m4.1.1" xref="S3.SS2.SSS2.p2.9.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p2.9.m4.1.1.2" xref="S3.SS2.SSS2.p2.9.m4.1.1.2.cmml">x</mi><mo id="S3.SS2.SSS2.p2.9.m4.1.1.3" xref="S3.SS2.SSS2.p2.9.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.9.m4.1b"><apply id="S3.SS2.SSS2.p2.9.m4.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.9.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p2.9.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS2.p2.9.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p2.9.m4.1.1.2">𝑥</ci><times id="S3.SS2.SSS2.p2.9.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p2.9.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.9.m4.1c">x^{*}</annotation></semantics></math>. Analogously, we will let <math id="S3.SS2.SSS2.p2.10.m5.1" class="ltx_Math" alttext="f_{i}^{*}" display="inline"><semantics id="S3.SS2.SSS2.p2.10.m5.1a"><msubsup id="S3.SS2.SSS2.p2.10.m5.1.1" xref="S3.SS2.SSS2.p2.10.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p2.10.m5.1.1.2.2" xref="S3.SS2.SSS2.p2.10.m5.1.1.2.2.cmml">f</mi><mi id="S3.SS2.SSS2.p2.10.m5.1.1.2.3" xref="S3.SS2.SSS2.p2.10.m5.1.1.2.3.cmml">i</mi><mo id="S3.SS2.SSS2.p2.10.m5.1.1.3" xref="S3.SS2.SSS2.p2.10.m5.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.10.m5.1b"><apply id="S3.SS2.SSS2.p2.10.m5.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.10.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1">superscript</csymbol><apply id="S3.SS2.SSS2.p2.10.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.10.m5.1.1.2.1.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.10.m5.1.1.2.2.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1.2.2">𝑓</ci><ci id="S3.SS2.SSS2.p2.10.m5.1.1.2.3.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1.2.3">𝑖</ci></apply><times id="S3.SS2.SSS2.p2.10.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p2.10.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.10.m5.1c">f_{i}^{*}</annotation></semantics></math> denote the minimum value of <math id="S3.SS2.SSS2.p2.11.m6.1" class="ltx_Math" alttext="f_{i}" display="inline"><semantics id="S3.SS2.SSS2.p2.11.m6.1a"><msub id="S3.SS2.SSS2.p2.11.m6.1.1" xref="S3.SS2.SSS2.p2.11.m6.1.1.cmml"><mi id="S3.SS2.SSS2.p2.11.m6.1.1.2" xref="S3.SS2.SSS2.p2.11.m6.1.1.2.cmml">f</mi><mi id="S3.SS2.SSS2.p2.11.m6.1.1.3" xref="S3.SS2.SSS2.p2.11.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p2.11.m6.1b"><apply id="S3.SS2.SSS2.p2.11.m6.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p2.11.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p2.11.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p2.11.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p2.11.m6.1.1.2">𝑓</ci><ci id="S3.SS2.SSS2.p2.11.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p2.11.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p2.11.m6.1c">f_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.10" class="ltx_p">As in the IID setting, we assume an intermittent communication model (e.g. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Woodworth et al.</span> [<a href="#bib.bib480" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">480</span></a>, Sec. 4.4]</cite>), where <math id="S3.SS2.SSS2.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS2.p3.1.m1.1a"><mi id="S3.SS2.SSS2.p3.1.m1.1.1" xref="S3.SS2.SSS2.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.1.m1.1b"><ci id="S3.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.1.m1.1c">M</annotation></semantics></math> stateless clients participate in each of <math id="S3.SS2.SSS2.p3.2.m2.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.SSS2.p3.2.m2.1a"><mi id="S3.SS2.SSS2.p3.2.m2.1.1" xref="S3.SS2.SSS2.p3.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.2.m2.1b"><ci id="S3.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p3.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.2.m2.1c">T</annotation></semantics></math> rounds, and during each round, each client can compute gradients for <math id="S3.SS2.SSS2.p3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS2.p3.3.m3.1a"><mi id="S3.SS2.SSS2.p3.3.m3.1.1" xref="S3.SS2.SSS2.p3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.3.m3.1b"><ci id="S3.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p3.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.3.m3.1c">K</annotation></semantics></math> samples (e.g. minibatches). The difference here is that the samples <math id="S3.SS2.SSS2.p3.4.m4.7" class="ltx_Math" alttext="z_{i,1},\ldots,z_{i,K}" display="inline"><semantics id="S3.SS2.SSS2.p3.4.m4.7a"><mrow id="S3.SS2.SSS2.p3.4.m4.7.7.2" xref="S3.SS2.SSS2.p3.4.m4.7.7.3.cmml"><msub id="S3.SS2.SSS2.p3.4.m4.6.6.1.1" xref="S3.SS2.SSS2.p3.4.m4.6.6.1.1.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.6.6.1.1.2" xref="S3.SS2.SSS2.p3.4.m4.6.6.1.1.2.cmml">z</mi><mrow id="S3.SS2.SSS2.p3.4.m4.2.2.2.4" xref="S3.SS2.SSS2.p3.4.m4.2.2.2.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.1.1.1.1" xref="S3.SS2.SSS2.p3.4.m4.1.1.1.1.cmml">i</mi><mo id="S3.SS2.SSS2.p3.4.m4.2.2.2.4.1" xref="S3.SS2.SSS2.p3.4.m4.2.2.2.3.cmml">,</mo><mn id="S3.SS2.SSS2.p3.4.m4.2.2.2.2" xref="S3.SS2.SSS2.p3.4.m4.2.2.2.2.cmml">1</mn></mrow></msub><mo id="S3.SS2.SSS2.p3.4.m4.7.7.2.3" xref="S3.SS2.SSS2.p3.4.m4.7.7.3.cmml">,</mo><mi mathvariant="normal" id="S3.SS2.SSS2.p3.4.m4.5.5" xref="S3.SS2.SSS2.p3.4.m4.5.5.cmml">…</mi><mo id="S3.SS2.SSS2.p3.4.m4.7.7.2.4" xref="S3.SS2.SSS2.p3.4.m4.7.7.3.cmml">,</mo><msub id="S3.SS2.SSS2.p3.4.m4.7.7.2.2" xref="S3.SS2.SSS2.p3.4.m4.7.7.2.2.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.7.7.2.2.2" xref="S3.SS2.SSS2.p3.4.m4.7.7.2.2.2.cmml">z</mi><mrow id="S3.SS2.SSS2.p3.4.m4.4.4.2.4" xref="S3.SS2.SSS2.p3.4.m4.4.4.2.3.cmml"><mi id="S3.SS2.SSS2.p3.4.m4.3.3.1.1" xref="S3.SS2.SSS2.p3.4.m4.3.3.1.1.cmml">i</mi><mo id="S3.SS2.SSS2.p3.4.m4.4.4.2.4.1" xref="S3.SS2.SSS2.p3.4.m4.4.4.2.3.cmml">,</mo><mi id="S3.SS2.SSS2.p3.4.m4.4.4.2.2" xref="S3.SS2.SSS2.p3.4.m4.4.4.2.2.cmml">K</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.4.m4.7b"><list id="S3.SS2.SSS2.p3.4.m4.7.7.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.7.7.2"><apply id="S3.SS2.SSS2.p3.4.m4.6.6.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.6.6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.6.6.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.6.6.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.6.6.1.1.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.6.6.1.1.2">𝑧</ci><list id="S3.SS2.SSS2.p3.4.m4.2.2.2.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.2.2.2.4"><ci id="S3.SS2.SSS2.p3.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.1.1.1.1">𝑖</ci><cn type="integer" id="S3.SS2.SSS2.p3.4.m4.2.2.2.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.2.2.2.2">1</cn></list></apply><ci id="S3.SS2.SSS2.p3.4.m4.5.5.cmml" xref="S3.SS2.SSS2.p3.4.m4.5.5">…</ci><apply id="S3.SS2.SSS2.p3.4.m4.7.7.2.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.7.7.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.4.m4.7.7.2.2.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.7.7.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p3.4.m4.7.7.2.2.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.7.7.2.2.2">𝑧</ci><list id="S3.SS2.SSS2.p3.4.m4.4.4.2.3.cmml" xref="S3.SS2.SSS2.p3.4.m4.4.4.2.4"><ci id="S3.SS2.SSS2.p3.4.m4.3.3.1.1.cmml" xref="S3.SS2.SSS2.p3.4.m4.3.3.1.1">𝑖</ci><ci id="S3.SS2.SSS2.p3.4.m4.4.4.2.2.cmml" xref="S3.SS2.SSS2.p3.4.m4.4.4.2.2">𝐾</ci></list></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.4.m4.7c">z_{i,1},\ldots,z_{i,K}</annotation></semantics></math> sampled at client <math id="S3.SS2.SSS2.p3.5.m5.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS2.SSS2.p3.5.m5.1a"><mi id="S3.SS2.SSS2.p3.5.m5.1.1" xref="S3.SS2.SSS2.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.5.m5.1b"><ci id="S3.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p3.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.5.m5.1c">i</annotation></semantics></math> are drawn from the client’s local distribution <math id="S3.SS2.SSS2.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS2.SSS2.p3.6.m6.1a"><msub id="S3.SS2.SSS2.p3.6.m6.1.1" xref="S3.SS2.SSS2.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p3.6.m6.1.1.2" xref="S3.SS2.SSS2.p3.6.m6.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p3.6.m6.1.1.3" xref="S3.SS2.SSS2.p3.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.6.m6.1b"><apply id="S3.SS2.SSS2.p3.6.m6.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.SSS2.p3.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.6.m6.1c">\mathcal{P}_{i}</annotation></semantics></math>. Unlike the IID setting, we cannot necessarily assume <math id="S3.SS2.SSS2.p3.7.m7.1" class="ltx_Math" alttext="M=N" display="inline"><semantics id="S3.SS2.SSS2.p3.7.m7.1a"><mrow id="S3.SS2.SSS2.p3.7.m7.1.1" xref="S3.SS2.SSS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.SSS2.p3.7.m7.1.1.2" xref="S3.SS2.SSS2.p3.7.m7.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS2.p3.7.m7.1.1.1" xref="S3.SS2.SSS2.p3.7.m7.1.1.1.cmml">=</mo><mi id="S3.SS2.SSS2.p3.7.m7.1.1.3" xref="S3.SS2.SSS2.p3.7.m7.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.7.m7.1b"><apply id="S3.SS2.SSS2.p3.7.m7.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1"><eq id="S3.SS2.SSS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1.2">𝑀</ci><ci id="S3.SS2.SSS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.SSS2.p3.7.m7.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.7.m7.1c">M=N</annotation></semantics></math>, as the client distributions are not all equal. In the following, if an algorithm relies on <math id="S3.SS2.SSS2.p3.8.m8.1" class="ltx_Math" alttext="M=N" display="inline"><semantics id="S3.SS2.SSS2.p3.8.m8.1a"><mrow id="S3.SS2.SSS2.p3.8.m8.1.1" xref="S3.SS2.SSS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.SSS2.p3.8.m8.1.1.2" xref="S3.SS2.SSS2.p3.8.m8.1.1.2.cmml">M</mi><mo id="S3.SS2.SSS2.p3.8.m8.1.1.1" xref="S3.SS2.SSS2.p3.8.m8.1.1.1.cmml">=</mo><mi id="S3.SS2.SSS2.p3.8.m8.1.1.3" xref="S3.SS2.SSS2.p3.8.m8.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.8.m8.1b"><apply id="S3.SS2.SSS2.p3.8.m8.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m8.1.1"><eq id="S3.SS2.SSS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.SSS2.p3.8.m8.1.1.1"></eq><ci id="S3.SS2.SSS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.SSS2.p3.8.m8.1.1.2">𝑀</ci><ci id="S3.SS2.SSS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.SSS2.p3.8.m8.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.8.m8.1c">M=N</annotation></semantics></math>, we will omit <math id="S3.SS2.SSS2.p3.9.m9.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS2.p3.9.m9.1a"><mi id="S3.SS2.SSS2.p3.9.m9.1.1" xref="S3.SS2.SSS2.p3.9.m9.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.9.m9.1b"><ci id="S3.SS2.SSS2.p3.9.m9.1.1.cmml" xref="S3.SS2.SSS2.p3.9.m9.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.9.m9.1c">M</annotation></semantics></math> and simply write <math id="S3.SS2.SSS2.p3.10.m10.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.SS2.SSS2.p3.10.m10.1a"><mi id="S3.SS2.SSS2.p3.10.m10.1.1" xref="S3.SS2.SSS2.p3.10.m10.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p3.10.m10.1b"><ci id="S3.SS2.SSS2.p3.10.m10.1.1.cmml" xref="S3.SS2.SSS2.p3.10.m10.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p3.10.m10.1c">N</annotation></semantics></math>. We note that while such an assumption may be compatible with the cross-silo federated setting in Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, it is generally infeasible in the cross-device setting.</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.4" class="ltx_p">While <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib434" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">434</span></a>, <a href="#bib.bib500" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">500</span></a>, <a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>, <a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite> mainly focused on the IID case, the analysis technique can be extended to the non-IID case by adding an assumption on data dissimilarities, for example by constraining the difference between client gradients and the global gradient <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib305" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">305</span></a>, <a href="#bib.bib300" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">300</span></a>, <a href="#bib.bib304" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">304</span></a>, <a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>, <a href="#bib.bib471" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">471</span></a>]</cite> or the difference between client and global optimum values <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>, <a href="#bib.bib268" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">268</span></a>]</cite>. Under this assumption, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib501" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">501</span></a>]</cite> showed that the error bound of local SGD in the non-IID case becomes worse. In order to achieve the rate of <math id="S3.SS2.SSS2.p4.1.m1.1" class="ltx_Math" alttext="1/\sqrt{TKN}" display="inline"><semantics id="S3.SS2.SSS2.p4.1.m1.1a"><mrow id="S3.SS2.SSS2.p4.1.m1.1.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.cmml"><mn id="S3.SS2.SSS2.p4.1.m1.1.1.2" xref="S3.SS2.SSS2.p4.1.m1.1.1.2.cmml">1</mn><mo id="S3.SS2.SSS2.p4.1.m1.1.1.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.1.cmml">/</mo><msqrt id="S3.SS2.SSS2.p4.1.m1.1.1.3" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.cmml"><mrow id="S3.SS2.SSS2.p4.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.cmml"><mi id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.2" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.3" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.3.cmml">K</mi><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1a" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1.cmml">​</mo><mi id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.4" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.4.cmml">N</mi></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.1.m1.1b"><apply id="S3.SS2.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1"><divide id="S3.SS2.SSS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.1"></divide><cn type="integer" id="S3.SS2.SSS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.2">1</cn><apply id="S3.SS2.SSS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3"><root id="S3.SS2.SSS2.p4.1.m1.1.1.3a.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3"></root><apply id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2"><times id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.1"></times><ci id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.2.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.2">𝑇</ci><ci id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.3.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.3">𝐾</ci><ci id="S3.SS2.SSS2.p4.1.m1.1.1.3.2.4.cmml" xref="S3.SS2.SSS2.p4.1.m1.1.1.3.2.4">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.1.m1.1c">1/\sqrt{TKN}</annotation></semantics></math> (under non-convex objectives), the number of local updates <math id="S3.SS2.SSS2.p4.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS2.p4.2.m2.1a"><mi id="S3.SS2.SSS2.p4.2.m2.1.1" xref="S3.SS2.SSS2.p4.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.2.m2.1b"><ci id="S3.SS2.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p4.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.2.m2.1c">K</annotation></semantics></math> should be smaller than <math id="S3.SS2.SSS2.p4.3.m3.1" class="ltx_Math" alttext="T^{1/3}/N" display="inline"><semantics id="S3.SS2.SSS2.p4.3.m3.1a"><mrow id="S3.SS2.SSS2.p4.3.m3.1.1" xref="S3.SS2.SSS2.p4.3.m3.1.1.cmml"><msup id="S3.SS2.SSS2.p4.3.m3.1.1.2" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.cmml"><mi id="S3.SS2.SSS2.p4.3.m3.1.1.2.2" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.2.cmml">T</mi><mrow id="S3.SS2.SSS2.p4.3.m3.1.1.2.3" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.cmml"><mn id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.2" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.2.cmml">1</mn><mo id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.1" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.1.cmml">/</mo><mn id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.3" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.3.cmml">3</mn></mrow></msup><mo id="S3.SS2.SSS2.p4.3.m3.1.1.1" xref="S3.SS2.SSS2.p4.3.m3.1.1.1.cmml">/</mo><mi id="S3.SS2.SSS2.p4.3.m3.1.1.3" xref="S3.SS2.SSS2.p4.3.m3.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.3.m3.1b"><apply id="S3.SS2.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1"><divide id="S3.SS2.SSS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.1"></divide><apply id="S3.SS2.SSS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p4.3.m3.1.1.2.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS2.p4.3.m3.1.1.2.2.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.2">𝑇</ci><apply id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3"><divide id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.1.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.1"></divide><cn type="integer" id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.2.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.2">1</cn><cn type="integer" id="S3.SS2.SSS2.p4.3.m3.1.1.2.3.3.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.2.3.3">3</cn></apply></apply><ci id="S3.SS2.SSS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p4.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.3.m3.1c">T^{1/3}/N</annotation></semantics></math>, instead of <math id="S3.SS2.SSS2.p4.4.m4.1" class="ltx_Math" alttext="T/N^{3}" display="inline"><semantics id="S3.SS2.SSS2.p4.4.m4.1a"><mrow id="S3.SS2.SSS2.p4.4.m4.1.1" xref="S3.SS2.SSS2.p4.4.m4.1.1.cmml"><mi id="S3.SS2.SSS2.p4.4.m4.1.1.2" xref="S3.SS2.SSS2.p4.4.m4.1.1.2.cmml">T</mi><mo id="S3.SS2.SSS2.p4.4.m4.1.1.1" xref="S3.SS2.SSS2.p4.4.m4.1.1.1.cmml">/</mo><msup id="S3.SS2.SSS2.p4.4.m4.1.1.3" xref="S3.SS2.SSS2.p4.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS2.p4.4.m4.1.1.3.2" xref="S3.SS2.SSS2.p4.4.m4.1.1.3.2.cmml">N</mi><mn id="S3.SS2.SSS2.p4.4.m4.1.1.3.3" xref="S3.SS2.SSS2.p4.4.m4.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p4.4.m4.1b"><apply id="S3.SS2.SSS2.p4.4.m4.1.1.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1"><divide id="S3.SS2.SSS2.p4.4.m4.1.1.1.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.1"></divide><ci id="S3.SS2.SSS2.p4.4.m4.1.1.2.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.2">𝑇</ci><apply id="S3.SS2.SSS2.p4.4.m4.1.1.3.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p4.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p4.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.3.2">𝑁</ci><cn type="integer" id="S3.SS2.SSS2.p4.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS2.p4.4.m4.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p4.4.m4.1c">T/N^{3}</annotation></semantics></math>
as in the IID case <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib300" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">300</span></a>]</cite> proposed to add a proximal term in each local objective function so as to make the algorithm be more robust to the heterogeneity across local objectives. The proposed FedProx algorithm empirically improves the performance of federated averaging.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Khaled et al.</span> [<a href="#bib.bib268" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">268</span></a>]</cite> assumes all clients participate, and uses batch gradient descent on clients, which can potentially converge faster than stochastic gradients on clients.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">Recently, a number of works have made progress in relaxing the assumptions necessary for analysis so as to better apply to practical uses of Federated Averaging. For example, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>]</cite> studied the convergence of Federated Averaging in a more realistic setting where only a subset of clients are involved in each round. In order to guarantee the convergence, they assumed that the clients are selected either uniformly at random or with probabilities that are in proportion to the sizes of local datasets. Nonetheless, in practice the server may not be able to sample clients in these idealized ways — in particular, in cross-device settings only devices that meet strict eligibility requirements (e.g. charging, idle, free WiFi) will be selected to participate in the computation. At different times within a day, the clients characteristics can vary significantly. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Eichner et al.</span> [<a href="#bib.bib171" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite> formulated this problem and studied the convergence of semi-cyclic SGD, where multiple blocks of clients with different characteristics are sampled from following a regular cyclic pattern (e.g. diurnal). Clients can perform different local steps because of heterogeneity in their computing capacities. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib471" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">471</span></a>]</cite> proves that FedAvg and many other federated learning algorithms will converge to the stationary points of a mismatched objective function in the presence of heterogeneous local steps. They refer to this problem as <em id="S3.SS2.SSS2.p5.1.1" class="ltx_emph ltx_font_italic">objective inconsistency</em> and propose a simple technique to eliminate the inconsistency problem from federated learning algorithms.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.5" class="ltx_p">We summarize recent theoretical results in Table <a href="#S3.T5" title="Table 5 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. All the methods in <a href="#S3.T5" title="In Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a> assume smoothness or Lipschitz gradients for the local functions on clients.
The error bound is measured by optimal objective (<a href="#S3.E1" title="Equation 1 ‣ Baselines and state-of-the-art for convex problems ‣ 3.2.1 Optimization Algorithms and Convergence Rates for IID Datasets ‣ 3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) for convex functions and norm of gradient for nonconvex functions.
For each method, we present the key non-IID assumption, assumptions on each client function <math id="S3.SS2.SSS2.p6.1.m1.1" class="ltx_Math" alttext="f_{i}(x)" display="inline"><semantics id="S3.SS2.SSS2.p6.1.m1.1a"><mrow id="S3.SS2.SSS2.p6.1.m1.1.2" xref="S3.SS2.SSS2.p6.1.m1.1.2.cmml"><msub id="S3.SS2.SSS2.p6.1.m1.1.2.2" xref="S3.SS2.SSS2.p6.1.m1.1.2.2.cmml"><mi id="S3.SS2.SSS2.p6.1.m1.1.2.2.2" xref="S3.SS2.SSS2.p6.1.m1.1.2.2.2.cmml">f</mi><mi id="S3.SS2.SSS2.p6.1.m1.1.2.2.3" xref="S3.SS2.SSS2.p6.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p6.1.m1.1.2.1" xref="S3.SS2.SSS2.p6.1.m1.1.2.1.cmml">​</mo><mrow id="S3.SS2.SSS2.p6.1.m1.1.2.3.2" xref="S3.SS2.SSS2.p6.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p6.1.m1.1.2.3.2.1" xref="S3.SS2.SSS2.p6.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.SSS2.p6.1.m1.1.1" xref="S3.SS2.SSS2.p6.1.m1.1.1.cmml">x</mi><mo stretchy="false" id="S3.SS2.SSS2.p6.1.m1.1.2.3.2.2" xref="S3.SS2.SSS2.p6.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.1.m1.1b"><apply id="S3.SS2.SSS2.p6.1.m1.1.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2"><times id="S3.SS2.SSS2.p6.1.m1.1.2.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2.1"></times><apply id="S3.SS2.SSS2.p6.1.m1.1.2.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p6.1.m1.1.2.2.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS2.SSS2.p6.1.m1.1.2.2.2.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2.2.2">𝑓</ci><ci id="S3.SS2.SSS2.p6.1.m1.1.2.2.3.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S3.SS2.SSS2.p6.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p6.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.1.m1.1c">f_{i}(x)</annotation></semantics></math>, and other auxiliary assumptions. We also briefly describe each method as a variant of the federated averaging algorithm, and show the simplified convergence rate eliminating constants.
Assuming the client functions are strongly convex could help the convergence rate <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>, <a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite>.
Bounded gradient variance, which is a widely used assumption to analyze stochastic gradient methods, is often used when clients use stochastic local updates <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib305" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">305</span></a>, <a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>, <a href="#bib.bib304" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">304</span></a>, <a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>, <a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite>.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib303" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">303</span></a>]</cite> directly analyzes the Federated Averaging algorithm, which applies <math id="S3.SS2.SSS2.p6.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S3.SS2.SSS2.p6.2.m2.1a"><mi id="S3.SS2.SSS2.p6.2.m2.1.1" xref="S3.SS2.SSS2.p6.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.2.m2.1b"><ci id="S3.SS2.SSS2.p6.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p6.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.2.m2.1c">K</annotation></semantics></math> steps of local updates on randomly sampled <math id="S3.SS2.SSS2.p6.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS2.SSS2.p6.3.m3.1a"><mi id="S3.SS2.SSS2.p6.3.m3.1.1" xref="S3.SS2.SSS2.p6.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.3.m3.1b"><ci id="S3.SS2.SSS2.p6.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p6.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.3.m3.1c">M</annotation></semantics></math> clients in each round, and presents a rate that suggests local updates <math id="S3.SS2.SSS2.p6.4.m4.1" class="ltx_Math" alttext="(K&gt;1)" display="inline"><semantics id="S3.SS2.SSS2.p6.4.m4.1a"><mrow id="S3.SS2.SSS2.p6.4.m4.1.1.1" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p6.4.m4.1.1.1.2" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.SSS2.p6.4.m4.1.1.1.1" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.cmml"><mi id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.2" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.2.cmml">K</mi><mo id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.1" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.3" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS2.SSS2.p6.4.m4.1.1.1.3" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.4.m4.1b"><apply id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.cmml" xref="S3.SS2.SSS2.p6.4.m4.1.1.1"><gt id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.1.cmml" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.1"></gt><ci id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.2.cmml" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.2">𝐾</ci><cn type="integer" id="S3.SS2.SSS2.p6.4.m4.1.1.1.1.3.cmml" xref="S3.SS2.SSS2.p6.4.m4.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.4.m4.1c">(K&gt;1)</annotation></semantics></math> could slow down the convergence. Clarifying the regimes where <math id="S3.SS2.SSS2.p6.5.m5.1" class="ltx_Math" alttext="K&gt;1" display="inline"><semantics id="S3.SS2.SSS2.p6.5.m5.1a"><mrow id="S3.SS2.SSS2.p6.5.m5.1.1" xref="S3.SS2.SSS2.p6.5.m5.1.1.cmml"><mi id="S3.SS2.SSS2.p6.5.m5.1.1.2" xref="S3.SS2.SSS2.p6.5.m5.1.1.2.cmml">K</mi><mo id="S3.SS2.SSS2.p6.5.m5.1.1.1" xref="S3.SS2.SSS2.p6.5.m5.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.SSS2.p6.5.m5.1.1.3" xref="S3.SS2.SSS2.p6.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p6.5.m5.1b"><apply id="S3.SS2.SSS2.p6.5.m5.1.1.cmml" xref="S3.SS2.SSS2.p6.5.m5.1.1"><gt id="S3.SS2.SSS2.p6.5.m5.1.1.1.cmml" xref="S3.SS2.SSS2.p6.5.m5.1.1.1"></gt><ci id="S3.SS2.SSS2.p6.5.m5.1.1.2.cmml" xref="S3.SS2.SSS2.p6.5.m5.1.1.2">𝐾</ci><cn type="integer" id="S3.SS2.SSS2.p6.5.m5.1.1.3.cmml" xref="S3.SS2.SSS2.p6.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p6.5.m5.1c">K&gt;1</annotation></semantics></math> may hurt or help convergence is an important open problem.</p>
</div>
<section id="S3.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Connections to decentralized optimization</h5>

<div id="S3.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px1.p1.1" class="ltx_p">The objective function of federated optimization has been studied for many years in the decentralized optimization community.
As first shown in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite>, the convergence analysis of decentralized SGD can be applied to or combined with local SGD with a proper setting of the network topology matrix (mixing matrix). In order to reduce the communication overhead, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang and Joshi</span> [<a href="#bib.bib467" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">467</span></a>]</cite> proposed periodic decentralized SGD (PD-SGD) which allows decentralized SGD to have multiple local updates as Federated Averaging. This algorithm is extended by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Li et al.</span> [<a href="#bib.bib304" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">304</span></a>]</cite> to the non-IID case. MATCHA <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib469" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">469</span></a>]</cite> further improves the performance of PD-SGD by randomly sampling clients for computation and communication, and provides a convergence analysis showing that local updates can accelerate convergence.</p>
</div>
</section>
<section id="S3.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Acceleration, variance reduction and adaptivity</h5>

<div id="S3.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.Px2.p1.1" class="ltx_p">Momentum, variance-reduction, and adaptive learning rates are all promising techniques to improve convergence and generalization of first-order methods. However, there is no single manner in which to incorporate these techniques into FedAvg. SCAFFOLD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib265" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">265</span></a>]</cite> models the difference in client updates using control variates to perform variance reduction. Notably, this allows convergence results not relying on bounding the amount of heterogeneity among clients. As for momentum, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Yu et al.</span> [<a href="#bib.bib501" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">501</span></a>]</cite> propose allowing each client to maintain a local momentum buffer and average the local buffers and the local model parameters at each communication round. Although this method empirically improves the final accuracy of local SGD, this doubles the per-round communication cost. A similar scheme is used by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie et al.</span> [<a href="#bib.bib485" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">485</span></a>]</cite> to design a variant of local SGD in which clients locally perform Adagrad <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib335" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">335</span></a>, <a href="#bib.bib161" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">161</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Reddi et al.</span> [<a href="#bib.bib389" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">389</span></a>]</cite> instead proposes using adaptive learning rates at the server-level, developing federated versions of adaptive optimization methods with the same communication cost as FedAvg. This framework generalizes the server momentum framework proposed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Hsu et al.</span> [<a href="#bib.bib237" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">237</span></a>], <span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib470" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">470</span></a>]</cite>, which allows momentum without increasing communication costs. While both <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib501" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">501</span></a>, <a href="#bib.bib470" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">470</span></a>]</cite> showed that the momentum variants of local SGD can converge to stationary points of non-convex objective functions at the same rate as synchronous mini-batch SGD, it is challenging to prove momentum accelerates the convergence rate in the federated learning setting. Recently, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Karimireddy et al.</span> [<a href="#bib.bib264" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">264</span></a>]</cite> proposed a general approach for adapting centralized optimization algorithms to the heterogeneous federated setting (MIME framework and algorithms).</p>
</div>
</section>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-Task Learning, Personalization, and Meta-Learning</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this section we consider a variety of “multi-model” approaches — techniques that result in effectively using different models for different clients at inference time. These techniques are particularly relevant when faced with non-IID data (Section <a href="#S3.SS1" title="3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), since they may outperform even the best possible shared global model. We note that personalization has also been studied in the fully decentralized setting <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib459" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">459</span></a>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">59</span></a>, <a href="#bib.bib504" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">504</span></a>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>, where training individual models is particularly natural.</p>
</div>
<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Personalization via Featurization</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<p id="S3.SS3.SSS1.p1.1" class="ltx_p">The remainder of this section specifically considers techniques that result in different users running inference with different model parameters (weights). However, in some applications similar benefits can be achieved by simply adding user and context features to the model. For example, consider a language model for next-word-prediction in a mobile keyboard as in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Hard et al.</span> [<a href="#bib.bib222" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">222</span></a>]</cite>. Different clients are likely to use language differently, and in fact on-device personalization of model parameters has yielded significant improvements for this problem <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib472" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">472</span></a>]</cite>. However, a complimentary approach may be to train a federated model that takes as input not only the words the user has typed so far, but a variety of other user and context features—What words does this user frequently use? What app are they currently using? If they are chatting, what messages have they sent to this person before? Suitably featurized, such inputs can allow a shared global model to produce highly personalized predictions. However, largely because few public datasets contain such auxiliary features, developing model architectures that can effectively incorporate context information for different tasks remains an important open problem with the potential to greatly increase the utility of FL-trained models.</p>
</div>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Multi-Task Learning</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">If one considers each client’s local problem (the learning problem on the local dataset) as a separate task (rather than as a shard of a single partitioned dataset), then techniques from multi-task learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib506" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">506</span></a>]</cite> immediately become relevant. Notably, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Smith et al.</span> [<a href="#bib.bib424" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">424</span></a>]</cite> introduced the MOCHA algorithm for multi-task federated learning, directly tackling challenges of communication efficiency, stragglers, and fault tolerance. In multi-task learning, the result of the training process is one model per task. Thus, most multi-task learning algorithms assume all clients (tasks) participate in each training round, and also require stateful clients since each client is training an individual model. This makes such techniques relevant for cross-silo FL applications, but harder to apply in cross-device scenarios.</p>
</div>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">Another approach is to reconsider the relationship between clients (local datasets) and learning tasks (models to be trained), observing that there are points on a spectrum between a single global model and different models for every client. For example, it may be possible to apply techniques from multi-task learning (as well as other approaches like personalization, discussed next), where we take the “task” to be a subset of the clients, perhaps chosen explicitly (e.g. based on geographic region, or characteristics of the device or user), or perhaps based on clustering <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib331" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">331</span></a>]</cite> or the connected components of a learned graph over the clients <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib504" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">504</span></a>]</cite>. The development of such algorithms is an important open problem. See <a href="#S4.SS4.SSS4" title="4.4.4 Preserving Privacy While Training Sub-Models ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4.4</span></a> for a discussion of how sparse federated learning problems, such as those arising naturally in this type of multi-task problem, might be approached without revealing to which client subset (task) each client belongs.</p>
</div>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Local Fine Tuning and Meta-Learning</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">By local fine tuning, we refer to techniques which begin with the federated training of a single model, and then deploy that model to all clients, where it is personalized by additional training on the local dataset before use in inference. This approach integrates naturally into the typical lifecycle of a model in federated learning (Section <a href="#S1.SS1.SSS1" title="1.1.1 The Lifecycle of a Model in Federated Learning ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.1.1</span></a>). Training of the global model can still proceed using only small samples of clients on each round (e.g. 100s); the broadcast of the global model to all clients (e.g. many millions) only happens once, when the model is deployed. The only difference is that before the model is used to make live predictions on the client, a final training process occurs, personalizing the model to the local dataset.</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">Given a global model that performs reasonably well, what is the best way to personalize it? In non-federated learning, researchers often use fine-tuning, transfer learning, domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib329" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">329</span></a>, <a href="#bib.bib132" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">132</span></a>, <a href="#bib.bib61" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">61</span></a>, <a href="#bib.bib332" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">332</span></a>, <a href="#bib.bib133" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">133</span></a>]</cite>, or interpolation with a personal local model. Of course, the precise technique used for such interpolations is key and it is important to determine its corresponding learning guarantees in the context of federated learning. Further, these techniques often assume only a pair of domains (source and target), and so some of the richer structure of federated learning may be lost.</p>
</div>
<div id="S3.SS3.SSS3.p3" class="ltx_para">
<p id="S3.SS3.SSS3.p3.2" class="ltx_p">One approach for studying personalization and non-IID data is via a connection to <span id="S3.SS3.SSS3.p3.2.1" class="ltx_text ltx_font_italic">meta-learning</span>, which has emerged as a popular setting for model adaptation.
In the standard learning-to-learn (LTL) setup <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">56</span></a>]</cite>, one has a meta-distribution over tasks, samples from which are used to learn a learning algorithm, for example by finding a good restriction of the hypothesis space. This is in fact a good match for the statistical setting discussed in Section <a href="#S3.SS1" title="3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>, where we sample a client (task) <math id="S3.SS3.SSS3.p3.1.m1.1" class="ltx_Math" alttext="i\sim\mathcal{Q}" display="inline"><semantics id="S3.SS3.SSS3.p3.1.m1.1a"><mrow id="S3.SS3.SSS3.p3.1.m1.1.1" xref="S3.SS3.SSS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.SSS3.p3.1.m1.1.1.2" xref="S3.SS3.SSS3.p3.1.m1.1.1.2.cmml">i</mi><mo id="S3.SS3.SSS3.p3.1.m1.1.1.1" xref="S3.SS3.SSS3.p3.1.m1.1.1.1.cmml">∼</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS3.p3.1.m1.1.1.3" xref="S3.SS3.SSS3.p3.1.m1.1.1.3.cmml">𝒬</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p3.1.m1.1b"><apply id="S3.SS3.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.SSS3.p3.1.m1.1.1.1">similar-to</csymbol><ci id="S3.SS3.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.SSS3.p3.1.m1.1.1.2">𝑖</ci><ci id="S3.SS3.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.SSS3.p3.1.m1.1.1.3">𝒬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p3.1.m1.1c">i\sim\mathcal{Q}</annotation></semantics></math>, and then sample data for that client (task) from <math id="S3.SS3.SSS3.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{i}" display="inline"><semantics id="S3.SS3.SSS3.p3.2.m2.1a"><msub id="S3.SS3.SSS3.p3.2.m2.1.1" xref="S3.SS3.SSS3.p3.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS3.p3.2.m2.1.1.2" xref="S3.SS3.SSS3.p3.2.m2.1.1.2.cmml">𝒫</mi><mi id="S3.SS3.SSS3.p3.2.m2.1.1.3" xref="S3.SS3.SSS3.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p3.2.m2.1b"><apply id="S3.SS3.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS3.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.SSS3.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.SSS3.p3.2.m2.1.1.2">𝒫</ci><ci id="S3.SS3.SSS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.SSS3.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p3.2.m2.1c">\mathcal{P}_{i}</annotation></semantics></math>.</p>
</div>
<div id="S3.SS3.SSS3.p4" class="ltx_para">
<p id="S3.SS3.SSS3.p4.1" class="ltx_p">Recently, a class of algorithms referred to as <em id="S3.SS3.SSS3.p4.1.1" class="ltx_emph ltx_font_italic">model-agnostic meta-learning</em> (MAML) have been developed that meta-learn a global model, which can be used as a starting point for learning a good model adapted to a given task, using only a few local gradient steps <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib187" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">187</span></a>]</cite>. Most notably, the training phase of the popular Reptile algorithm <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib358" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">358</span></a>]</cite> is closely related to Federated Averaging <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite> — Reptile allows for a server learning rate and assumes all clients have the same amount of data, but is otherwise the same. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Khodak et al.</span> [<a href="#bib.bib270" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">270</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Jiang et al.</span> [<a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">250</span></a>]</cite> explore the connection between FL and MAML, and show how the MAML setting is a relevant framework to model the personalization objectives for FL. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Chai Sim et al.</span> [<a href="#bib.bib102" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">102</span></a>]</cite> applied local fine tuning to personalize speech recognition models in federated learning. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Fallah et al.</span> [<a href="#bib.bib181" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">181</span></a>]</cite> developed a new algorithm called Personalized FedAvg by connecting MAML instead of Reptile to federated learning. Additional connections with differential privacy were studied in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib299" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">299</span></a>]</cite>.</p>
</div>
<div id="S3.SS3.SSS3.p5" class="ltx_para">
<p id="S3.SS3.SSS3.p5.1" class="ltx_p">The general direction of combining ideas from FL and MAML is relatively new, with many open questions:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">The evaluation of MAML algorithms for supervised tasks is largely focused on synthetic image classification problems <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib290" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">290</span></a>, <a href="#bib.bib386" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">386</span></a>]</cite> in which infinite artificial tasks can be constructed by subsampling from classes of images. FL problems, modeled by existing datasets used for simulated FL experiments (Appendix <a href="#A1" title="Appendix A Software and Datasets for Federated Learning ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>), can serve as realistic benchmark problems for MAML algorithms.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">In addition to an empirical study, or optimization results, it would be useful to analyze the theoretical guarantees of MAML-type techniques and study under what assumptions they can be successful, as this will further elucidate the set of FL domains to which they may apply.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">The observed gap between the global and personalized acccuracy <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">250</span></a>]</cite> creates a good argument that personalization should be of central importance to FL. However, none of the existing works clearly formulates what would be comprehensive metrics for measuring personalized performance; for instance, is a small improvement for every client preferable to a larger improvement for a subset of clients? See Section <a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> for a related discussion.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Jiang et al.</span> [<a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">250</span></a>]</cite> highlighted the fact that models of the same structure and performance, but trained differently, can have very different capacity to personalize. In particular, it appears that training models with the goal of maximizing global performance might actually hurt the model’s capacity for subsequent personalization. Understanding the underlying reasons for this is a question relevant for both FL and the broader ML community.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p">Several challenging FL topics including personalization and privacy have begun to be studied in this multi-task/LTL framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib270" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">270</span></a>, <a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">250</span></a>, <a href="#bib.bib299" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">299</span></a>]</cite>. Is it possible for other issues such as concept drift to also be analyzed in this way, for example as a problem in lifelong learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib420" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">420</span></a>]</cite>?</p>
</div>
</li>
<li id="S3.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i6.p1" class="ltx_para">
<p id="S3.I2.i6.p1.1" class="ltx_p">Can non-parameter transfer LTL algorithms, such as ProtoNets <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib425" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">425</span></a>]</cite>, be of use for FL?</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>When is a Global FL-trained Model Better?</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<p id="S3.SS3.SSS4.p1.9" class="ltx_p">What can federated learning do for you that local training on one device cannot? When local datasets are small and the data is IID, FL clearly has an edge, and indeed, real-world applications of federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib491" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">491</span></a>, <a href="#bib.bib222" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">222</span></a>, <a href="#bib.bib112" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">112</span></a>]</cite> benefit from training a single model across devices. On the other hand, given pathologically non-IID distributions (e.g. <math id="S3.SS3.SSS4.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{i}(y\,|\,x)" display="inline"><semantics id="S3.SS3.SSS4.p1.1.m1.1a"><mrow id="S3.SS3.SSS4.p1.1.m1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.cmml"><msub id="S3.SS3.SSS4.p1.1.m1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS4.p1.1.m1.1.1.3.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.2.cmml">𝒫</mi><mi id="S3.SS3.SSS4.p1.1.m1.1.1.3.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.SSS4.p1.1.m1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS3.SSS4.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.2.cmml">y</mi><mo fence="false" lspace="0.448em" rspace="0.448em" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml">|</mo><mi id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo stretchy="false" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.3" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.1.m1.1b"><apply id="S3.SS3.SSS4.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1"><times id="S3.SS3.SSS4.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.2"></times><apply id="S3.SS3.SSS4.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS3.SSS4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.2">𝒫</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.3.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.3.3">𝑖</ci></apply><apply id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.2">𝑦</ci><ci id="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS3.SSS4.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.1.m1.1c">\mathcal{P}_{i}(y\,|\,x)</annotation></semantics></math> directly disagree across clients), local models will do much better. Thus, a natural theoretical question is to determine under what conditions the shared global model is better than independent per-device models. Suppose we train a model <math id="S3.SS3.SSS4.p1.2.m2.1" class="ltx_Math" alttext="h_{k}" display="inline"><semantics id="S3.SS3.SSS4.p1.2.m2.1a"><msub id="S3.SS3.SSS4.p1.2.m2.1.1" xref="S3.SS3.SSS4.p1.2.m2.1.1.cmml"><mi id="S3.SS3.SSS4.p1.2.m2.1.1.2" xref="S3.SS3.SSS4.p1.2.m2.1.1.2.cmml">h</mi><mi id="S3.SS3.SSS4.p1.2.m2.1.1.3" xref="S3.SS3.SSS4.p1.2.m2.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.2.m2.1b"><apply id="S3.SS3.SSS4.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.2.m2.1.1.1.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.2.m2.1.1.2.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1.2">ℎ</ci><ci id="S3.SS3.SSS4.p1.2.m2.1.1.3.cmml" xref="S3.SS3.SSS4.p1.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.2.m2.1c">h_{k}</annotation></semantics></math> for each client <math id="S3.SS3.SSS4.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS4.p1.3.m3.1a"><mi id="S3.SS3.SSS4.p1.3.m3.1.1" xref="S3.SS3.SSS4.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.3.m3.1b"><ci id="S3.SS3.SSS4.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS4.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.3.m3.1c">k</annotation></semantics></math>, using the sample of size <math id="S3.SS3.SSS4.p1.4.m4.1" class="ltx_Math" alttext="m_{k}" display="inline"><semantics id="S3.SS3.SSS4.p1.4.m4.1a"><msub id="S3.SS3.SSS4.p1.4.m4.1.1" xref="S3.SS3.SSS4.p1.4.m4.1.1.cmml"><mi id="S3.SS3.SSS4.p1.4.m4.1.1.2" xref="S3.SS3.SSS4.p1.4.m4.1.1.2.cmml">m</mi><mi id="S3.SS3.SSS4.p1.4.m4.1.1.3" xref="S3.SS3.SSS4.p1.4.m4.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.4.m4.1b"><apply id="S3.SS3.SSS4.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.2">𝑚</ci><ci id="S3.SS3.SSS4.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS4.p1.4.m4.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.4.m4.1c">m_{k}</annotation></semantics></math> available from that client. Can we guarantee that the model <math id="S3.SS3.SSS4.p1.5.m5.1" class="ltx_Math" alttext="h_{\text{FL}}" display="inline"><semantics id="S3.SS3.SSS4.p1.5.m5.1a"><msub id="S3.SS3.SSS4.p1.5.m5.1.1" xref="S3.SS3.SSS4.p1.5.m5.1.1.cmml"><mi id="S3.SS3.SSS4.p1.5.m5.1.1.2" xref="S3.SS3.SSS4.p1.5.m5.1.1.2.cmml">h</mi><mtext id="S3.SS3.SSS4.p1.5.m5.1.1.3" xref="S3.SS3.SSS4.p1.5.m5.1.1.3a.cmml">FL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.5.m5.1b"><apply id="S3.SS3.SSS4.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.5.m5.1.1.1.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.5.m5.1.1.2.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.2">ℎ</ci><ci id="S3.SS3.SSS4.p1.5.m5.1.1.3a.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S3.SS3.SSS4.p1.5.m5.1.1.3.cmml" xref="S3.SS3.SSS4.p1.5.m5.1.1.3">FL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.5.m5.1c">h_{\text{FL}}</annotation></semantics></math> learned via federated learning is at least as accurate as <math id="S3.SS3.SSS4.p1.6.m6.1" class="ltx_Math" alttext="h_{k}" display="inline"><semantics id="S3.SS3.SSS4.p1.6.m6.1a"><msub id="S3.SS3.SSS4.p1.6.m6.1.1" xref="S3.SS3.SSS4.p1.6.m6.1.1.cmml"><mi id="S3.SS3.SSS4.p1.6.m6.1.1.2" xref="S3.SS3.SSS4.p1.6.m6.1.1.2.cmml">h</mi><mi id="S3.SS3.SSS4.p1.6.m6.1.1.3" xref="S3.SS3.SSS4.p1.6.m6.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.6.m6.1b"><apply id="S3.SS3.SSS4.p1.6.m6.1.1.cmml" xref="S3.SS3.SSS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.6.m6.1.1.1.cmml" xref="S3.SS3.SSS4.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.6.m6.1.1.2.cmml" xref="S3.SS3.SSS4.p1.6.m6.1.1.2">ℎ</ci><ci id="S3.SS3.SSS4.p1.6.m6.1.1.3.cmml" xref="S3.SS3.SSS4.p1.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.6.m6.1c">h_{k}</annotation></semantics></math> when used for client <math id="S3.SS3.SSS4.p1.7.m7.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.SSS4.p1.7.m7.1a"><mi id="S3.SS3.SSS4.p1.7.m7.1.1" xref="S3.SS3.SSS4.p1.7.m7.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.7.m7.1b"><ci id="S3.SS3.SSS4.p1.7.m7.1.1.cmml" xref="S3.SS3.SSS4.p1.7.m7.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.7.m7.1c">k</annotation></semantics></math>?
Can we quantify how much improvement can be expected via federated leaning?
And can we develop personalization strategies with theoretical guarantees that at least match the performance of both natural baselines (<math id="S3.SS3.SSS4.p1.8.m8.1" class="ltx_Math" alttext="h_{k}" display="inline"><semantics id="S3.SS3.SSS4.p1.8.m8.1a"><msub id="S3.SS3.SSS4.p1.8.m8.1.1" xref="S3.SS3.SSS4.p1.8.m8.1.1.cmml"><mi id="S3.SS3.SSS4.p1.8.m8.1.1.2" xref="S3.SS3.SSS4.p1.8.m8.1.1.2.cmml">h</mi><mi id="S3.SS3.SSS4.p1.8.m8.1.1.3" xref="S3.SS3.SSS4.p1.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.8.m8.1b"><apply id="S3.SS3.SSS4.p1.8.m8.1.1.cmml" xref="S3.SS3.SSS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.8.m8.1.1.1.cmml" xref="S3.SS3.SSS4.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.8.m8.1.1.2.cmml" xref="S3.SS3.SSS4.p1.8.m8.1.1.2">ℎ</ci><ci id="S3.SS3.SSS4.p1.8.m8.1.1.3.cmml" xref="S3.SS3.SSS4.p1.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.8.m8.1c">h_{k}</annotation></semantics></math> and <math id="S3.SS3.SSS4.p1.9.m9.1" class="ltx_Math" alttext="h_{\text{FL}}" display="inline"><semantics id="S3.SS3.SSS4.p1.9.m9.1a"><msub id="S3.SS3.SSS4.p1.9.m9.1.1" xref="S3.SS3.SSS4.p1.9.m9.1.1.cmml"><mi id="S3.SS3.SSS4.p1.9.m9.1.1.2" xref="S3.SS3.SSS4.p1.9.m9.1.1.2.cmml">h</mi><mtext id="S3.SS3.SSS4.p1.9.m9.1.1.3" xref="S3.SS3.SSS4.p1.9.m9.1.1.3a.cmml">FL</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS4.p1.9.m9.1b"><apply id="S3.SS3.SSS4.p1.9.m9.1.1.cmml" xref="S3.SS3.SSS4.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS4.p1.9.m9.1.1.1.cmml" xref="S3.SS3.SSS4.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS3.SSS4.p1.9.m9.1.1.2.cmml" xref="S3.SS3.SSS4.p1.9.m9.1.1.2">ℎ</ci><ci id="S3.SS3.SSS4.p1.9.m9.1.1.3a.cmml" xref="S3.SS3.SSS4.p1.9.m9.1.1.3"><mtext mathsize="70%" id="S3.SS3.SSS4.p1.9.m9.1.1.3.cmml" xref="S3.SS3.SSS4.p1.9.m9.1.1.3">FL</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS4.p1.9.m9.1c">h_{\text{FL}}</annotation></semantics></math>)?</p>
</div>
<div id="S3.SS3.SSS4.p2" class="ltx_para">
<p id="S3.SS3.SSS4.p2.1" class="ltx_p">Several of these problems relate to previous work on multiple-source adaptation and agnostic federated learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib329" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">329</span></a>, <a href="#bib.bib330" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">330</span></a>, <a href="#bib.bib234" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">234</span></a>, <a href="#bib.bib352" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">352</span></a>]</cite>. The hardness of these questions depends on how the data is distributed among parties. For example, if data is vertically partitioned, each party maintaining private records of different feature sets about common entities, these problems may require addressing record linkage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">124</span></a>]</cite> within the federated learning task. Independently of the eventual technical levy of carrying out record linkage privately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib407" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">407</span></a>]</cite>, the task itself happens to be substantially noise prone in the real world <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib406" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">406</span></a>]</cite> and only sparse results have addressed its impact on training models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib224" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">224</span></a>]</cite>. Techniques for robustness and privacy can make local models relatively stronger, particularly for non-typical clients <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib502" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">502</span></a>]</cite>. Loss factorization tricks can be used in supervised learning to alleviate up to the vertical partition assumption itself, but the practical benefits depend on the distribution of data and the number of parties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib373" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">373</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Adapting ML Workflows for Federated Learning</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Many challenges arise when adapting standard machine learning workflows and pipelines (including data augmentation, feature engineering, neural architecture design, model selection, hyperparameter optimization, and debugging) to decentralized datasets and resource-constrained mobile devices. We discuss several of these challenges below.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Hyperparameter Tuning</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">Running many rounds of training with different hyperparameters on resource-constrained mobile devices may be restrictive. For small device populations, this might result in the over-use of limited communication and compute resources. However, recent deep neural networks crucially depend on a wide range of hyperparameter choices regarding the neural network’s architecture, regularization, and optimization. Evaluations can be expensive for large models and large-scale on-device datasets. Hyperparameter optimization (HPO) has a long history under the framework of AutoML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib395" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">395</span></a>, <a href="#bib.bib273" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">273</span></a>, <a href="#bib.bib277" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">277</span></a>]</cite>, but it mainly concerns how to improve the model accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">64</span></a>, <a href="#bib.bib426" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">426</span></a>, <a href="#bib.bib374" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">374</span></a>, <a href="#bib.bib180" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">180</span></a>]</cite> rather than communication and computing efficacy for mobile devices. Therefore, we expect that further research should consider developing solutions for efficient hyperparameter optimization in the context of federated learning.</p>
</div>
<div id="S3.SS4.SSS1.p2" class="ltx_para">
<p id="S3.SS4.SSS1.p2.1" class="ltx_p">In addition to general-purpose approaches to the hyperparameter optimization problem, in the training space specifically the development of easy-to-tune optimization algorithms is a major open area. Centralized training already requires tuning parameters like learning rate, momentum, batch size, and regularization. Federated learning adds potentially more hyperparameters — separate tuning of the aggregation / global model update rule and local client optimizer, number of clients selected per round, number of local steps per round, configuration of update compression algorithms, and more. Such hyperparameters can be crucial to obtaining a good trade-off between accuracy and convergence, and may actually impact the quality of the learned model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib106" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">106</span></a>]</cite>. In addition to a higher-dimensional search space, federated learning often also requires longer wall-clock training times and limited compute resources. These challenges could be addressed by optimization algorithms that are robust to hyperparameter settings (the same hyperparameter values work for many different real world datasets and architectures), as well as adaptive or self-tuning algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">446</span></a>, <a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Neural Architecture Design</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">Neural architecture search (NAS) in the federated learning setting is motivated by the drawbacks of the current practice of applying predefined deep learning models: the predefined architecture of a deep learning model may not be the optimal design choice when the data generated by users are invisible to model developers. For example, the neural architecture may have some redundant component for a specific dataset, which may lead to unnecessary computing on devices; there may be a better architectural design for the non-IID data distribution. The approaches to personalization discussed in <a href="#S3.SS3" title="3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a> still share the same model architecture among all clients.
The recent progress in NAS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib230" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">230</span></a>, <a href="#bib.bib387" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">387</span></a>, <a href="#bib.bib175" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">175</span></a>, <a href="#bib.bib388" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">388</span></a>, <a href="#bib.bib60" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">60</span></a>, <a href="#bib.bib375" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">375</span></a>, <a href="#bib.bib313" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">313</span></a>, <a href="#bib.bib488" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">488</span></a>, <a href="#bib.bib175" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">175</span></a>, <a href="#bib.bib323" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">323</span></a>]</cite> provides a potential way to address these drawbacks. There are three major methods for NAS, which utilize evolutionary algorithms, reinforcement learning, or gradient descent to search for optimal architectures for a specific task on a specific dataset. Among these, the gradient-based method leverages efficient gradient back-propagation with weight sharing, reducing the architecture search process from over 3000 GPU days to only 1 GPU day.
Another interesting paper recently published, involving Weight Agnostic Neural Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">192</span></a>]</cite>, claims that neural network architectures alone, without learning any weight parameters, may encode solutions for a given task. If this technique further develops and reaches widespread use, it may be applied to the federated learning without collaborative training among devices.
Although these methods have not been developed for distributed settings such as federated learning, they are all feasible to be transferred to the federated setting. Neural Architecture Search (NAS) for a global or personalized model in the federated learning setting is promising, and early exploration has been made in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib228" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">228</span></a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Debugging and Interpretability for FL</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">While substantial progress has been made on the federated training of models, this is only part of a complete ML workflow. Experienced modelers often directly inspect subsets of the data for tasks including basic sanity checking, debugging misclassifications, discovering outliers, manually labeling examples, or detecting bias in the training set. Developing privacy-preserving techniques to answer such questions on decentralized data is a major open problem. Recently, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Augenstein et al.</span> [<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> proposed the use of differentially private generative models (including GANs), trained with federated learning, to answer some questions of this type. However, many open questions remain (see discussion in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>), in particular the development of algorithms that improve the fidelity of FL DP generative models.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Communication and Compression</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">It is now well-understood that communication can be a primary bottleneck for federated learning since wireless links and other end-user internet connections typically operate at lower rates than intra- or inter-datacenter links and can be potentially expensive and unreliable. This has led to significant recent interest in reducing the communication bandwidth of federated learning. Methods combining Federated Averaging with sparsification and/or quantization of model updates to a small number of bits have demonstrated significant reductions in communication cost with minimal impact on training accuracy <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib282" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">282</span></a>]</cite>. However, it remains unclear if communication cost can be further reduced, and whether any of these methods or their combinations can come close to providing optimal trade-offs between communication and accuracy in federated learning. Characterizing such fundamental trade-offs between accuracy and communication has been of recent interest in theoretical statistics <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib507" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">507</span></a>, <a href="#bib.bib89" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">89</span></a>, <a href="#bib.bib221" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">221</span></a>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>, <a href="#bib.bib444" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">444</span></a>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite>. These works characterize the optimal minimax rates for distributed statistical estimation and learning under communication constraints. However, it is difficult to deduce concrete insights from these theoretical works for communication bandwidth reduction in practice as they typically ignore the impact of the optimization algorithm. It remains an open direction to leverage such statistical approaches to inform practical training methods.</p>
</div>
<section id="S3.SS5.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Compression objectives</h5>

<div id="S3.SS5.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p1.1" class="ltx_p">Motivated by the limited resources of current devices in terms of compute, memory and communication, there are several different compression objectives of practical value.</p>
<ol id="S3.I3" class="ltx_enumerate">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p"><em id="S3.I3.i1.p1.1.1" class="ltx_emph ltx_font_italic">Gradient compression<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note"><span id="footnote10.1.1.1" class="ltx_text ltx_font_upright">10</span></span><span id="footnote10.9" class="ltx_text ltx_font_upright">In this section, we use “gradient compression” to include compression applied to any model update, such as the updates produced by Federated Averaging when clients take multiple gradient steps.</span></span></span></span></em> – reduce the size of the object communicated from clients to server, which is used to update the global model.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p"><em id="S3.I3.i2.p1.1.1" class="ltx_emph ltx_font_italic">Model broadcast compression</em> – reduce the size of the model broadcast from server to clients, from which the clients start local training.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(c)</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p"><em id="S3.I3.i3.p1.1.1" class="ltx_emph ltx_font_italic">Local computation reduction</em> – any modification to the overall training algorithm such that the local training procedure is computationally more efficient.</p>
</div>
</li>
</ol>
<p id="S3.SS5.SSS0.Px1.p1.2" class="ltx_p">These objectives are in most cases complementary. Among them, (a) has the potential for the most significant practical impact in terms of total runtime. This is both because clients’ connections generally have slower upload than download bandwidth<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>See for instance <a target="_blank" href="https://www.speedtest.net/reports/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.speedtest.net/reports/</a></span></span></span> – and thus there is more to be gained, compared to (b) – and because the effects of averaging across many clients can enable more aggressive lossy compression schemes. Usually, (c) could be realized jointly with (a) and (b) by specific methods.</p>
</div>
<div id="S3.SS5.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p2.1" class="ltx_p">Much of the existing literature applies to the objective (a) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib282" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">282</span></a>, <a href="#bib.bib440" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">440</span></a>, <a href="#bib.bib281" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">281</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib235" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">235</span></a>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">55</span></a>]</cite>. The impact of (b) on convergence in general has not been studied until very recently; an analysis is presented in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib123" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">123</span></a>]</cite>. Very few methods intend to address all of (a), (b) and (c) jointly. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Caldas et al.</span> [<a href="#bib.bib95" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">95</span></a>]</cite> proposed a practical method by constraining the desired model update such that only particular submatrices of model variables are necessary to be available on clients; <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Hamer et al.</span> [<a href="#bib.bib219" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">219</span></a>]</cite> proposed a communication-efficient federated algorithm for learning mixture weights on an ensemble of pre-trained models, based on communicating only a subset of the models to any one device; <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">He et al.</span> [<a href="#bib.bib227" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">227</span></a>]</cite> utilizes bidirectional and alternative knowledge distillation method to transfer knowledge from many compact DNNs to a dense server DNN, which can reduce the local computational burden at the edge devices.</p>
</div>
<div id="S3.SS5.SSS0.Px1.p3" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p3.1" class="ltx_p">In cross-device FL, algorithms generally cannot assume any state is preserved on the clients (Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). However, this constraint would typically not be present in the cross-silo FL setting, where the same clients participate repeatedly. Consequently, a wider set of ideas related to error-correction such as <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib311" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">311</span></a>, <a href="#bib.bib405" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">405</span></a>, <a href="#bib.bib463" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">463</span></a>, <a href="#bib.bib444" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">444</span></a>, <a href="#bib.bib263" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">263</span></a>, <a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite> are relevant in this setting, many of which could address both (a) and (b).</p>
</div>
<div id="S3.SS5.SSS0.Px1.p4" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p4.1" class="ltx_p">An additional objective is to modify the training procedure such that the <em id="S3.SS5.SSS0.Px1.p4.1.1" class="ltx_emph ltx_font_italic">final</em> model is more compact, or efficient for inference. This topic has received a lot of attention in the broader ML community <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib220" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">220</span></a>, <a href="#bib.bib138" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">138</span></a>, <a href="#bib.bib509" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">509</span></a>, <a href="#bib.bib309" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">309</span></a>, <a href="#bib.bib362" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">362</span></a>, <a href="#bib.bib74" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">74</span></a>]</cite>, but these methods either do not have a straightforward mapping to federated learning, or make the training process more complex which makes it difficult to adopt. Research that simultaneously yields a compact final model, while also addressing the three objectives above, has significant potential for practical impact.</p>
</div>
<div id="S3.SS5.SSS0.Px1.p5" class="ltx_para">
<p id="S3.SS5.SSS0.Px1.p5.1" class="ltx_p">For gradient compression, some existing works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib440" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">440</span></a>]</cite> are developed in the minimax sense to characterize the worst case scenario. However usually in information theory, the compression guarantees are instance specific and depend on the <em id="S3.SS5.SSS0.Px1.p5.1.1" class="ltx_emph ltx_font_italic">entropy</em> of the underlying distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">140</span></a>]</cite>. In other words, if the data is easily compressible, they are provably compressed heavily. It would be interesting to see if similar instance specific results can be obtained for gradient compression. Similarly, recent works show that learning a compression scheme in a data-dependent fashion can lead to significantly better compression ratio for the case of data compression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib482" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">482</span></a>]</cite> as well as gradient compression. It is therefore worthwhile to evaluate these data-dependent compression schemes in the federated settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">193</span></a>]</cite>.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Compatibility with differential privacy and secure aggregation</h5>

<div id="S3.SS5.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px2.p1.3" class="ltx_p">Many algorithms used in federated learning such as Secure Aggregation <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib79" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">79</span></a>]</cite> and mechanisms of adding noise to achieve differential privacy <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>]</cite> are not designed to work with compressed or quantized communications. For example, straightforward application of the Secure Aggregation protocol of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bonawitz et al.</span> [<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>], <span class="ltx_text" style="font-size:90%;">Bell et al.</span> [<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> requires an additional <math id="S3.SS5.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="O(\log M)" display="inline"><semantics id="S3.SS5.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS5.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.3.cmml">O</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.2" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.1" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1a" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">⁡</mo><mi id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.2" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml">M</mi></mrow><mo stretchy="false" id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.3" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1"><times id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.2"></times><ci id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.3">𝑂</ci><apply id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1"><log id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.1"></log><ci id="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS5.SSS0.Px2.p1.1.m1.1.1.1.1.1.2">𝑀</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p1.1.m1.1c">O(\log M)</annotation></semantics></math> bits of communication for each scalar, where <math id="S3.SS5.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS5.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS5.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS5.SSS0.Px2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS5.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS5.SSS0.Px2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p1.2.m2.1c">M</annotation></semantics></math> is the number of clients being summed over, and this may render ineffective the aggressive quantization of updates when <math id="S3.SS5.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S3.SS5.SSS0.Px2.p1.3.m3.1a"><mi id="S3.SS5.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS5.SSS0.Px2.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.SSS0.Px2.p1.3.m3.1b"><ci id="S3.SS5.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS5.SSS0.Px2.p1.3.m3.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSS0.Px2.p1.3.m3.1c">M</annotation></semantics></math> is large (though see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">82</span></a>]</cite> for a more efficient approach). Existing noise addition mechanisms assume adding real-valued Gaussian or Laplacian noise on each client, and this is not compatible with standard quantization methods used to reduce communication. We note that several recent works allow biased estimators and would work nicely with Laplacian noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib435" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">435</span></a>]</cite>, however those would not give differential privacy, as they break independence between rounds. There is some work on adding discrete noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, but there is no notion whether such methods are optimal. Joint design of compression methods that are compatible with Secure Aggregation, or for which differential privacy guarantees can be obtained, is thus a valuable open problem.</p>
</div>
</section>
<section id="S3.SS5.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Wireless-FL co-design</h5>

<div id="S3.SS5.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS5.SSS0.Px3.p1.1" class="ltx_p">The existing literature in federated learning usually neglects the impact of wireless channel dynamics during model training, which potentially undermines both training latency and thus reliability of the entire production system. In particular, wireless interference, noisy channels and channel fluctuations can significantly hinder the information exchange between the server and clients (or directly between individual clients, as in the fully decentralized case, see Section <a href="#S2.SS1" title="2.1 Fully Decentralized / Peer-to-Peer Distributed Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>). This represents a major challenge for mission-critical applications, rooted in latency reduction and reliability enhancements. Potential solutions to address this challenge include federated distillation (FD), in which workers exchange their model output parameters (logits) as opposed to the model parameters (gradients and/weights), and optimizing workers’ scheduling policy with appropriate communication and computing resources <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib248" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">248</span></a>, <a href="#bib.bib368" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">368</span></a>, <a href="#bib.bib402" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">402</span></a>]</cite>. Another solution is to leverage the unique characteristics of wireless channels (e.g. broadcast and superposition) as natural data aggregators, in which the simultaneously transmitted analog-waves by different workers are superposed at the server and weighed by the wireless channel coefficients <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>. This yields faster model aggregation at the server, and faster training by a factor up to the number of workers. This is in sharp contrast with the traditional orthogonal frequency division multiplexing (OFDM) paradigm, whereby workers upload their models over orthogonal frequencies whose performance degrades with increasing number of workers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib174" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">174</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Application To More Types of Machine Learning Problems and Models</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">To date, federated learning has primarily considered supervised learning tasks where labels are naturally available on each client. Extending FL to other ML paradigms, including reinforcement learning, semi-supervised and unsupervised learning, active learning, and online learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib226" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">226</span></a>, <a href="#bib.bib508" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">508</span></a>]</cite> all present interesting and open challenges.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para">
<p id="S3.SS6.p2.1" class="ltx_p">Another important class of models, highly relevant to FL, are those that can characterize the uncertainty in their predictions. Most modern deep learning models cannot represent their uncertainty nor allow for a probability interpretation of parametric learning. This has motivated recent developments of tools and techniques combining Bayesian models with deep learning. From a probability theory perspective, it is unjustifiable to use single point-estimates for classification. Bayesian neural networks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib419" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">419</span></a>]</cite> have been proposed and shown to be far more robust to over-fitting, and can easily learn from small datasets. The Bayesian approach further offers uncertainty estimates via its parameters in form of probability distributions, thus preventing over-fitting. Moreover, appealing to probabilistic reasoning, one can predict how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the data size grows.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para">
<p id="S3.SS6.p3.1" class="ltx_p">Since Bayesian methods gave us tools to reason about deep models’ confidence and also achieve state-of-the-art performance on many tasks, one expects Bayesian methods to provide a conceptual improvement to the classical federated learning. In fact, preliminary work from <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Lalitha et al.</span> [<a href="#bib.bib292" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">292</span></a>]</cite> shows that incorporating Bayesian methods allows for model aggregation across non-IID data and heterogeneous platforms. However, many questions regarding scalability and computational feasibility have to be addressed.</p>
</div>
</section>
<section id="S3.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Executive summary</h3>

<div id="S3.SS7.p1" class="ltx_para">
<p id="S3.SS7.p1.1" class="ltx_p">Efficient and effective federated learning algorithms face different challenges compared to centralized training in a datacenter.</p>
</div>
<div id="S3.SS7.p2" class="ltx_para">
<ul id="S3.I4" class="ltx_itemize">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p">Non-IID data due to non-identical client distributions, violation of independence, and dataset drift (<a href="#S3.SS1" title="3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>) pose a key challenge. Though various methods have been surveyed and discussed in this section, defining and dealing with non-IID data remains an open problem and one of the most active research topics in federated learning.</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p">Optimization algorithms for federated learning are analyzed in <a href="#S3.SS2" title="3.2 Optimization Algorithms for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> under different settings, e.g., convex and nonconvex functions, IID and non-IID data. Theoretical analysis has proven difficult for the parallel local updates commonly used in federated optimization, and often strict assumptions have to be made to constrain the client heterogeneity. Currently, known convergence rates do not fully explain the empirically-observed effectiveness of the Federated Averaging algorithm over methods such as mini-batch SGD <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib481" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">481</span></a>]</cite>.</p>
</div>
</li>
<li id="S3.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i3.p1" class="ltx_para">
<p id="S3.I4.i3.p1.1" class="ltx_p">Client-side personalization and “multi-model” approaches (<a href="#S3.SS3" title="3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) can address data heterogeneity and give hope of surpassing the performance of the best fixed global model. Simple personalization methods like fine-tuning can be effective, and offer intrinsic privacy advantages. However, many theoretical and empirical questions remain open: when is a global model better? How many models are necessary? Which federated optimization algorithms combine best with local fine-tuning?</p>
</div>
</li>
<li id="S3.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i4.p1" class="ltx_para">
<p id="S3.I4.i4.p1.1" class="ltx_p">Adapting centralized training workflows such as hyper-parameter tuning, neural architecture design, debugging, and interpretability tasks to the federated learning setting (<a href="#S3.SS4" title="3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.4</span></a>) present roadblocks to the widespread adoption of FL in practical settings, and hence constitute important open problems.</p>
</div>
</li>
<li id="S3.I4.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i5.p1" class="ltx_para">
<p id="S3.I4.i5.p1.1" class="ltx_p">While there has been significant work on communication efficiency and compression for FL (<a href="#S3.SS5" title="3.5 Communication and Compression ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.5</span></a>), it remains an important and active area. In particular, fully automating the process of enabling compression without impacting convergence for a wide class of models is an important practical goal. Relatively new directions on the theoretical study of communication, compatibility with privacy methods, and co-design with wireless infrastructure are discussed.</p>
</div>
</li>
<li id="S3.I4.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i6.p1" class="ltx_para">
<p id="S3.I4.i6.p1.1" class="ltx_p">There are many open questions in extending federated learning from supervised tasks to other machine learning paradigms including reinforcement learning, semi-supervised and unsupervised learning, active learning, and online learning (<a href="#S3.SS6" title="3.6 Application To More Types of Machine Learning Problems and Models ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.6</span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Preserving the Privacy of User Data</h2>

<figure id="S4.F3" class="ltx_figure">
<p id="S4.F3.1" class="ltx_p"><span id="S4.F3.1.1" class="ltx_text">
<span id="S4.F3.1.1.1" class="ltx_inline-block ltx_align_bottom"><img src="/html/1912.04977/assets/x2.png" id="S4.F3.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="222" alt="Refer to caption">
</span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure <a href="#S1.F1" title="Figure 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: </span>The lifecycle of an FL-trained model and the various actors in a federated learning system.

 (repeated from <a href="#S1.F1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>)</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Machine learning workflows involve many actors functioning in disparate capacities. For example, users may generate training data through interactions with their devices, a machine learning training procedure extracts cross-population patterns from this data (e.g. in the form of trained model parameters), the machine learning engineer or analyst may assess the quality of this trained model, and eventually the model may be deployed to end users in order to support specific user experiences (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> below).</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">In an ideal world, each actor in the system would learn nothing more than the information needed to play their role. For example, if an analyst only needs to determine whether a particular quality metric exceeds a desired threshold in order to authorize deploying the model to end users, then in an idealized world, that is the only bit of information that would be available to the analyst; such an analyst would need access to neither the training data nor the model parameters, for instance. Similarly, end users enjoying the user experiences powered by the trained model might only require predictions from the model and nothing else.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Furthermore, in an ideal world every participant in the system would be able to reason easily and accurately about what personal information about themselves and others might be revealed by their participation in the system, and participants would be able to use this understanding to make informed choices about how and whether to participate at all.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Producing a system with all of the above ideal privacy properties would be a daunting feat on its own, and even more so while also guaranteeing other desirable properties such as ease of use for all participants, the quality and fairness of the end user experiences (and the models that power them), the judicious use of communication and computation resources, resilience against attacks and failures, and so on.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Rather than allowing perfect to be the enemy of good, we advocate a strategy wherein the overall system is composed of modular units which can be studied and improved relatively independently, while also reminding ourselves that we must, in the end, measure the privacy properties of the complete system against our ideal privacy goals set out above. The open questions raised throughout this section will highlight areas wherein we do not yet understand how to simultaneously achieve all of our goals, either for an individual module or for the system as a whole.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Federated learning provides an attractive structure for decomposing the overall machine learning workflow into the approachable modular units we desire. One of the primary attractions of the federated learning model is that it can provide a level of privacy to participating users through data minimization: the raw user data never leaves the device, and only updates to models (e.g., gradient updates) are sent to the central server. These model updates are more focused on the learning task at hand than is the raw data (i.e. they contain strictly no additional information about the user, and typically significantly less, compared to the raw data), and the individual updates only need to be held ephemerally by the server.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">While these features can offer significant practical privacy improvements over centralizing all the training data, there is still no formal guarantee of privacy in this baseline federated learning model. For instance, it is possible to construct scenarios in which information about the raw data is leaked from a client to the server, such as a scenario where knowing the previous model and the gradient update from a user would allow one to infer a training example held by that user. Therefore, this section surveys existing results and outlines open challenges towards designing federated learning systems that can offer rigorous privacy guarantees. We focus on questions specific to the federated learning and analytics setting and leave aside questions that also arise in more general machine learning settings as surveyed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib344" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">344</span></a>]</cite>.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">Beyond attacks targeting user privacy, there are also other classes of attacks on federated learning; for example, an adversary might attempt to prevent a model from being learned at all, or they might attempt to bias the model to produce inferences that are preferable to the adversary. We defer consideration of these types of attacks to Section <a href="#S5" title="5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">The remainder of this section is organized as follows. Section <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> discusses various threat models against which we wish to give protections. Section <a href="#S4.SS2" title="4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> lays out a set of core tools and technologies that can be used towards providing rigorous protections against the threat models discussed in Section <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Section <a href="#S4.SS3" title="4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> assumes the existence of a trusted server and discusses the open problems and challenges in providing protections against adversarial clients and/or analysts. Section <a href="#S4.SS4" title="4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a> discusses the open problems and challenges in the absence of a fully trusted server. Finally, Section <a href="#S4.SS5" title="4.5 User Perception ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a> discusses open questions around user perception.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Actors, Threat Models, and Privacy in Depth</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">A formal treatment of privacy risks in FL calls for a holistic and interdisciplinary approach. While some of the risks can be mapped to technical privacy definitions and mitigated with existing technologies, others are more complex and require cross-disciplinary efforts.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Privacy is not a binary quantity, or even a scalar one. This first step towards such formal treatment is a careful characterization of the different actors (see Figure <a href="#S1.F1" title="Figure 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> from <a href="#S1" title="1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1</span></a>, repeated on page <a href="#S4.F3" title="Figure 1 ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text ltx_ref_tag">1</span></span></a> for convenience) and their roles to ultimately define relevant threat models (see Table <a href="#S4.T6" title="Table 6 ‣ 4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>). Thus, for instance, it is desirable to distinguish the view of the server administrator from the view of the analysts that consume the learned models, as it is conceivable that a system that is designed to offer strong privacy guarantees against a malicious analyst may not provide any guarantees with respect to a malicious server. These actors map well onto the threat models discussed elsewhere in the literature; for example, in <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bittau et al.</span> [<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>, Sec 3.1]</cite>, where the “encoder” corresponds to the client, the “shuffler” generally corresponds to the server, the “analyzer“ may correspond to the server or post-processing done by the analyst.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.2" class="ltx_p">As an example, a particular system might offer a differential privacy<span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>Differential privacy will be formally introduced in Section <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>. For now, it suffices to know that lower <math id="footnote12.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="footnote12.m1.1b"><mi id="footnote12.m1.1.1" xref="footnote12.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="footnote12.m1.1c"><ci id="footnote12.m1.1.1.cmml" xref="footnote12.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote12.m1.1d">\varepsilon</annotation></semantics></math> corresponds with higher privacy.</span></span></span> guarantee with a particular parameter <math id="S4.SS1.p3.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS1.p3.1.m1.1a"><mi id="S4.SS1.p3.1.m1.1.1" xref="S4.SS1.p3.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.1.m1.1b"><ci id="S4.SS1.p3.1.m1.1.1.cmml" xref="S4.SS1.p3.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.1.m1.1c">\varepsilon</annotation></semantics></math> to the view of the server administrator, while the results observed by analysts might have a higher protection <math id="S4.SS1.p3.2.m2.1" class="ltx_Math" alttext="\varepsilon^{\prime}&lt;\varepsilon" display="inline"><semantics id="S4.SS1.p3.2.m2.1a"><mrow id="S4.SS1.p3.2.m2.1.1" xref="S4.SS1.p3.2.m2.1.1.cmml"><msup id="S4.SS1.p3.2.m2.1.1.2" xref="S4.SS1.p3.2.m2.1.1.2.cmml"><mi id="S4.SS1.p3.2.m2.1.1.2.2" xref="S4.SS1.p3.2.m2.1.1.2.2.cmml">ε</mi><mo id="S4.SS1.p3.2.m2.1.1.2.3" xref="S4.SS1.p3.2.m2.1.1.2.3.cmml">′</mo></msup><mo id="S4.SS1.p3.2.m2.1.1.1" xref="S4.SS1.p3.2.m2.1.1.1.cmml">&lt;</mo><mi id="S4.SS1.p3.2.m2.1.1.3" xref="S4.SS1.p3.2.m2.1.1.3.cmml">ε</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p3.2.m2.1b"><apply id="S4.SS1.p3.2.m2.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1"><lt id="S4.SS1.p3.2.m2.1.1.1.cmml" xref="S4.SS1.p3.2.m2.1.1.1"></lt><apply id="S4.SS1.p3.2.m2.1.1.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p3.2.m2.1.1.2.1.cmml" xref="S4.SS1.p3.2.m2.1.1.2">superscript</csymbol><ci id="S4.SS1.p3.2.m2.1.1.2.2.cmml" xref="S4.SS1.p3.2.m2.1.1.2.2">𝜀</ci><ci id="S4.SS1.p3.2.m2.1.1.2.3.cmml" xref="S4.SS1.p3.2.m2.1.1.2.3">′</ci></apply><ci id="S4.SS1.p3.2.m2.1.1.3.cmml" xref="S4.SS1.p3.2.m2.1.1.3">𝜀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p3.2.m2.1c">\varepsilon^{\prime}&lt;\varepsilon</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.3" class="ltx_p">Furthermore, it is possible that this guarantee holds only against adversaries with particular limits on their capabilities, e.g. an adversary that can observe everything that happens on the server (but cannot influence the server’s behavior) while simultaneously controlling up to a fraction <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="\gamma" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mi id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml">γ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><ci id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1">𝛾</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">\gamma</annotation></semantics></math> of the clients (observing everything they see and influencing their behavior in arbitrary ways); the adversary might also be assumed to be unable to break cryptographic mechanisms instantiated at a particular security level <math id="S4.SS1.p4.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS1.p4.2.m2.1a"><mi id="S4.SS1.p4.2.m2.1.1" xref="S4.SS1.p4.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.2.m2.1b"><ci id="S4.SS1.p4.2.m2.1.1.cmml" xref="S4.SS1.p4.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.2.m2.1c">\sigma</annotation></semantics></math>. Against an adversary whose strength <span id="S4.SS1.p4.3.1" class="ltx_text ltx_font_italic">exceeds</span> these limits, the view of the server administrator might still have some differential privacy, but at weaker level <math id="S4.SS1.p4.3.m3.1" class="ltx_Math" alttext="\varepsilon_{0}&gt;\varepsilon" display="inline"><semantics id="S4.SS1.p4.3.m3.1a"><mrow id="S4.SS1.p4.3.m3.1.1" xref="S4.SS1.p4.3.m3.1.1.cmml"><msub id="S4.SS1.p4.3.m3.1.1.2" xref="S4.SS1.p4.3.m3.1.1.2.cmml"><mi id="S4.SS1.p4.3.m3.1.1.2.2" xref="S4.SS1.p4.3.m3.1.1.2.2.cmml">ε</mi><mn id="S4.SS1.p4.3.m3.1.1.2.3" xref="S4.SS1.p4.3.m3.1.1.2.3.cmml">0</mn></msub><mo id="S4.SS1.p4.3.m3.1.1.1" xref="S4.SS1.p4.3.m3.1.1.1.cmml">&gt;</mo><mi id="S4.SS1.p4.3.m3.1.1.3" xref="S4.SS1.p4.3.m3.1.1.3.cmml">ε</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.3.m3.1b"><apply id="S4.SS1.p4.3.m3.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1"><gt id="S4.SS1.p4.3.m3.1.1.1.cmml" xref="S4.SS1.p4.3.m3.1.1.1"></gt><apply id="S4.SS1.p4.3.m3.1.1.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p4.3.m3.1.1.2.1.cmml" xref="S4.SS1.p4.3.m3.1.1.2">subscript</csymbol><ci id="S4.SS1.p4.3.m3.1.1.2.2.cmml" xref="S4.SS1.p4.3.m3.1.1.2.2">𝜀</ci><cn type="integer" id="S4.SS1.p4.3.m3.1.1.2.3.cmml" xref="S4.SS1.p4.3.m3.1.1.2.3">0</cn></apply><ci id="S4.SS1.p4.3.m3.1.1.3.cmml" xref="S4.SS1.p4.3.m3.1.1.3">𝜀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.3.m3.1c">\varepsilon_{0}&gt;\varepsilon</annotation></semantics></math>.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">As we see in this example, precisely specifying the assumptions and privacy goals of a system can easily implicate concrete instantiations of several parameters (<math id="S4.SS1.p5.1.m1.5" class="ltx_Math" alttext="\varepsilon,\varepsilon^{\prime},\varepsilon_{0},\gamma,\sigma" display="inline"><semantics id="S4.SS1.p5.1.m1.5a"><mrow id="S4.SS1.p5.1.m1.5.5.2" xref="S4.SS1.p5.1.m1.5.5.3.cmml"><mi id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">ε</mi><mo id="S4.SS1.p5.1.m1.5.5.2.3" xref="S4.SS1.p5.1.m1.5.5.3.cmml">,</mo><msup id="S4.SS1.p5.1.m1.4.4.1.1" xref="S4.SS1.p5.1.m1.4.4.1.1.cmml"><mi id="S4.SS1.p5.1.m1.4.4.1.1.2" xref="S4.SS1.p5.1.m1.4.4.1.1.2.cmml">ε</mi><mo id="S4.SS1.p5.1.m1.4.4.1.1.3" xref="S4.SS1.p5.1.m1.4.4.1.1.3.cmml">′</mo></msup><mo id="S4.SS1.p5.1.m1.5.5.2.4" xref="S4.SS1.p5.1.m1.5.5.3.cmml">,</mo><msub id="S4.SS1.p5.1.m1.5.5.2.2" xref="S4.SS1.p5.1.m1.5.5.2.2.cmml"><mi id="S4.SS1.p5.1.m1.5.5.2.2.2" xref="S4.SS1.p5.1.m1.5.5.2.2.2.cmml">ε</mi><mn id="S4.SS1.p5.1.m1.5.5.2.2.3" xref="S4.SS1.p5.1.m1.5.5.2.2.3.cmml">0</mn></msub><mo id="S4.SS1.p5.1.m1.5.5.2.5" xref="S4.SS1.p5.1.m1.5.5.3.cmml">,</mo><mi id="S4.SS1.p5.1.m1.2.2" xref="S4.SS1.p5.1.m1.2.2.cmml">γ</mi><mo id="S4.SS1.p5.1.m1.5.5.2.6" xref="S4.SS1.p5.1.m1.5.5.3.cmml">,</mo><mi id="S4.SS1.p5.1.m1.3.3" xref="S4.SS1.p5.1.m1.3.3.cmml">σ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.5b"><list id="S4.SS1.p5.1.m1.5.5.3.cmml" xref="S4.SS1.p5.1.m1.5.5.2"><ci id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">𝜀</ci><apply id="S4.SS1.p5.1.m1.4.4.1.1.cmml" xref="S4.SS1.p5.1.m1.4.4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.4.4.1.1.1.cmml" xref="S4.SS1.p5.1.m1.4.4.1.1">superscript</csymbol><ci id="S4.SS1.p5.1.m1.4.4.1.1.2.cmml" xref="S4.SS1.p5.1.m1.4.4.1.1.2">𝜀</ci><ci id="S4.SS1.p5.1.m1.4.4.1.1.3.cmml" xref="S4.SS1.p5.1.m1.4.4.1.1.3">′</ci></apply><apply id="S4.SS1.p5.1.m1.5.5.2.2.cmml" xref="S4.SS1.p5.1.m1.5.5.2.2"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.5.5.2.2.1.cmml" xref="S4.SS1.p5.1.m1.5.5.2.2">subscript</csymbol><ci id="S4.SS1.p5.1.m1.5.5.2.2.2.cmml" xref="S4.SS1.p5.1.m1.5.5.2.2.2">𝜀</ci><cn type="integer" id="S4.SS1.p5.1.m1.5.5.2.2.3.cmml" xref="S4.SS1.p5.1.m1.5.5.2.2.3">0</cn></apply><ci id="S4.SS1.p5.1.m1.2.2.cmml" xref="S4.SS1.p5.1.m1.2.2">𝛾</ci><ci id="S4.SS1.p5.1.m1.3.3.cmml" xref="S4.SS1.p5.1.m1.3.3">𝜎</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.5c">\varepsilon,\varepsilon^{\prime},\varepsilon_{0},\gamma,\sigma</annotation></semantics></math>, etc.) as well as concepts such as differential privacy and honest-but-curious security.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<table id="S4.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T6.1.1" class="ltx_tr">
<td id="S4.T6.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><span id="S4.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Data/Access Point</span></span>
</span>
</td>
<td id="S4.T6.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.2.1.1" class="ltx_p" style="width:133.7pt;"><span id="S4.T6.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Actor</span></span>
</span>
</td>
<td id="S4.T6.1.1.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.1.3.1.1" class="ltx_p" style="width:216.8pt;"><span id="S4.T6.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Threat Model</span></span>
</span>
</td>
</tr>
<tr id="S4.T6.1.2" class="ltx_tr">
<td id="S4.T6.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.1.1.1" class="ltx_p" style="width:86.7pt;">Clients</span>
</span>
</td>
<td id="S4.T6.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.2.1.1" class="ltx_p" style="width:133.7pt;">Someone who has root access to the client device, either by design or by compromising the device</span>
</span>
</td>
<td id="S4.T6.1.2.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.2.3.1.1" class="ltx_p" style="width:216.8pt;">Malicious clients can inspect all messages received from the server (including the model iterates) in the rounds they participate in and can tamper with the training process. An honest-but-curious client can inspect all messages received from the server but cannot tamper with the training process. In some cases, technologies such as secure enclaves/TEEs may be able to limit the influence and visibility of such an attacker, representing a meaningfully weaker threat model.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.3" class="ltx_tr">
<td id="S4.T6.1.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.1.1.1" class="ltx_p" style="width:86.7pt;">Server</span>
</span>
</td>
<td id="S4.T6.1.3.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.2.1.1" class="ltx_p" style="width:133.7pt;">Someone who has root access to the server, either by design or by compromising the device</span>
</span>
</td>
<td id="S4.T6.1.3.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.3.3.1.1" class="ltx_p" style="width:216.8pt;">A malicious server can inspect all messages sent to the server (including the gradient updates) in all rounds and can tamper with the training process. An honest-but-curious server can inspect all messages sent to the server but cannot tamper with the training process. In some cases, technologies such as secure enclaves/TEEs may be able to limit the influence and visibility of such an attacker, representing a meaningfully weaker threat model.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.4" class="ltx_tr">
<td id="S4.T6.1.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.1.1.1" class="ltx_p" style="width:86.7pt;">Output Models</span>
</span>
</td>
<td id="S4.T6.1.4.2" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.2.1.1" class="ltx_p" style="width:133.7pt;">Engineers &amp; analysts</span>
</span>
</td>
<td id="S4.T6.1.4.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.4.3.1.1" class="ltx_p" style="width:216.8pt;">A malicious analyst or model engineer may have access to multiple outputs from the system, e.g. sequences of model iterates from multiple training runs with different hyperparameters. Exactly what information is released to this actor is an important system design question.</span>
</span>
</td>
</tr>
<tr id="S4.T6.1.5" class="ltx_tr">
<td id="S4.T6.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.5.1.1.1" class="ltx_p" style="width:86.7pt;">Deployed Models</span>
</span>
</td>
<td id="S4.T6.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.5.2.1.1" class="ltx_p" style="width:133.7pt;">The rest of the world</span>
</span>
</td>
<td id="S4.T6.1.5.3" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T6.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T6.1.5.3.1.1" class="ltx_p" style="width:216.8pt;">In cross-device FL, the final model may be deployed to hundreds of millions of devices. A partially compromised device can have black-box access to the learned model, and a fully compromised device can have a white-box access to the learned model.</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Various threat models for different adversarial actors.</figcaption>
</figure>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.1" class="ltx_p">Achieving all the desired privacy properties for federated learning will typically require composing many of the tools and technologies described below into an end-to-end system, potentially both layering multiple strategies to protect the same part of the system (e.g. running portions of a Secure Multi-Party Computation (MPC) protocol inside a Trusted Execution Environment (TEE) to make it harder for an adversary to sufficiently compromise that component) as well as using different strategies to protect different parts of the system (e.g. using MPC to protect the aggregation of model updates, then using Private Disclosure techniques before sharing the aggregate updates beyond the server).</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.1" class="ltx_p">As such, we advocate for building federated systems wherein the privacy properties degrade as gracefully as possible in cases where one technique or another fails to provide its intended privacy contribution. For example, running the server component of an MPC protocol inside a TEE might allow privacy to be maintained even in the case where either (but not both) of the TEE security or MPC security assumptions fails to hold in practice. As another example, requiring clients to send raw training examples to a server-side TEE would be strongly dispreferred to having clients send gradient updates to a server-side TEE, as the latter’s privacy expectations degrade much more gracefully if the TEE’s security were to fail. We refer to this principle of graceful degradation as “Privacy in Depth,” in analogy to the well-established network security principle of defense in depth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib361" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">361</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Tools and Technologies</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Generally speaking, the goal of an FL computation is for the analyst or engineer requesting the computation to obtain the result, which can be thought of as the evaluation of a function <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">f</annotation></semantics></math> on a distributed client dataset (commonly an ML model training algorithm, but possibly something simpler such as a basic statistic). There are three privacy aspects that need to be addressed.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">First, we need to consider <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_italic">how</span> <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">f</annotation></semantics></math> is computed and what is the information flow of intermediate results in the process, which primarily influences the susceptibility to malicious client, server, and admin actors. In addition to designing the flow of information in the system (e.g. early data minimization), techniques from secure computation including Secure Multi-Party Computation (MPC) and Trusted Execution Environments (TEEs) are of particular relevance to addressing these concerns. These technologies will be discussed in detail in <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.1</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Second, we have to consider <span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_italic">what</span> is computed. In other words, how much information about a participating client is revealed to the analyst and world actors by the result of <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">f</annotation></semantics></math> itself. Here, techniques for privacy-preserving disclosure, particularly differential privacy (DP), are highly relevant and will be discussed in detail in <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.2</span></a>.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Finally, there is the problem of <span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_italic">verifiability</span>, which pertains to the ability of a client or the server to prove to others in the system that they have executed the desired behavior faithfully, without revealing the potentially private data upon which they were acting. Techniques for verifiability, including remote attestation and zero-knowledge proofs, will be discussed in <a href="#S4.SS2.SSS3" title="4.2.3 Verifiability ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.3</span></a>.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<table id="S4.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T7.1.1" class="ltx_tr">
<td id="S4.T7.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.1.1.1" class="ltx_p" style="width:144.5pt;"><span id="S4.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Technology</span></span>
</span>
</td>
<td id="S4.T7.1.1.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.1.2.1.1" class="ltx_p" style="width:289.1pt;"><span id="S4.T7.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Characteristics</span></span>
</span>
</td>
</tr>
<tr id="S4.T7.1.2" class="ltx_tr">
<td id="S4.T7.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.1.1.1" class="ltx_p" style="width:144.5pt;">Differential Privacy (local, central, shuffled, aggregated, and hybrid models)</span>
</span>
</td>
<td id="S4.T7.1.2.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.2.2.1.1" class="ltx_p" style="width:289.1pt;">A quantification of how much information could be learned about an individual from the output of an analysis on a dataset that includes the user. Algorithms with differential privacy necessarily incorporate some amount of randomness or noise, which can be tuned to mask the influence of the user on the output.</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.3" class="ltx_tr">
<td id="S4.T7.1.3.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.1.1.1" class="ltx_p" style="width:144.5pt;">Secure Multi-Party Computation</span>
</span>
</td>
<td id="S4.T7.1.3.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.3.2.1.1" class="ltx_p" style="width:289.1pt;">Two or more participants collaborate to simulate, though cryptography, a fully trusted third party who can:

<span id="S4.I1" class="ltx_itemize">
<span id="S4.I1.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I1.i1.p1" class="ltx_para">
<span id="S4.I1.i1.p1.1" class="ltx_p">Compute a function of inputs provided by all the participants;</span>
</span></span>
<span id="S4.I1.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I1.i2.p1" class="ltx_para">
<span id="S4.I1.i2.p1.1" class="ltx_p">Reveal the computed value to a chosen subset of the participants, with no party learning anything further.</span>
</span></span>
</span></span>
</span>
</td>
</tr>
<tr id="S4.T7.1.4" class="ltx_tr">
<td id="S4.T7.1.4.1" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.1.1.1" class="ltx_p" style="width:144.5pt;">Homomorphic Encryption</span>
</span>
</td>
<td id="S4.T7.1.4.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.4.2.1.1" class="ltx_p" style="width:289.1pt;">Enables a party to compute functions of data to which they do not have plain-text access, by allowing mathematical operations to be performed on ciphertexts without decrypting them. Arbitrarily complicated functions of the data can be computed this way (“Fully Homomorphic Encryption”) though at greater computational cost.</span>
</span>
</td>
</tr>
<tr id="S4.T7.1.5" class="ltx_tr">
<td id="S4.T7.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.5.1.1.1" class="ltx_p" style="width:144.5pt;">Trusted Execution Environments (secure enclaves)</span>
</span>
</td>
<td id="S4.T7.1.5.2" class="ltx_td ltx_nopad_r ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1pt;padding-bottom:1pt;">
<span id="S4.T7.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T7.1.5.2.1.1" class="ltx_p" style="width:289.1pt;">TEEs provide the ability to trustably run code on a remote machine, even if you do not trust the machine’s owner/administrator. This is achieved by limiting the capabilities of any party, including the administrator. In particular, TEEs may provide the following properties <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib437" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">437</span></a>]</cite>:

<span id="S4.I2" class="ltx_itemize">
<span id="S4.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I2.i1.p1" class="ltx_para">
<span id="S4.I2.i1.p1.1" class="ltx_p">Confidentiality: The state of the code’s execution remains secret, unless the code explicitly publishes a message;</span>
</span></span>
<span id="S4.I2.i2" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I2.i2.p1" class="ltx_para">
<span id="S4.I2.i2.p1.1" class="ltx_p">Integrity: The code’s execution cannot be affected, except by the code explicitly receiving an input;</span>
</span></span>
<span id="S4.I2.i3" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S4.I2.i3.p1" class="ltx_para">
<span id="S4.I2.i3.p1.1" class="ltx_p">Measurement/Attestation: The TEE can prove to a remote party what code (binary) is executing and what its starting state was, defining the initial conditions for confidentiality and integrity.</span>
</span></span>
</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Various technologies along with their characteristics.</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Secure Computations</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">The goal of secure computation is to evaluate functions on distributed inputs in a way that only reveals the result of the computation to the intended parties, without revealing any additional information (e.g. the parties’ inputs or any intermediate results).</p>
</div>
<section id="S4.SS2.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Secure multi-party computation</h5>

<div id="S4.SS2.SSS1.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p1.1" class="ltx_p">Secure Multi-Party Computation (MPC) is a subfield of cryptography concerned with the problem of having a set of parties compute an agreed-upon function of their private inputs in a way that only reveals the intended output to each of the parties. This area was kicked off in the 1980’s by Yao <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib493" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">493</span></a>]</cite>. Thanks to both theoretical and engineering breakthroughs, the field has moved from being of a purely theoretical interest to a deployed technology in industry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>, <a href="#bib.bib77" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">77</span></a>, <a href="#bib.bib295" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">295</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib191" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">191</span></a>, <a href="#bib.bib242" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">242</span></a>, <a href="#bib.bib243" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">243</span></a>]</cite>. It is important to remark that MPC defines a set of technologies, and should be regarded more as a field, or a general notion of security in secure computation, than a technology <span id="S4.SS2.SSS1.Px1.p1.1.1" class="ltx_text ltx_font_italic">per se</span>. Some of the recent advances in MPC can be attributed to breakthroughs in lower level primitives, such as oblivious transfer protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib244" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">244</span></a>]</cite> and encryption schemes with homomorphic properties (as described below).</p>
</div>
<div id="S4.SS2.SSS1.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p2.1" class="ltx_p">A common aspect of cryptographic solutions is that operations are often done on a finite field (e.g. integers modulo a prime <math id="S4.SS2.SSS1.Px1.p2.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S4.SS2.SSS1.Px1.p2.1.m1.1a"><mi id="S4.SS2.SSS1.Px1.p2.1.m1.1.1" xref="S4.SS2.SSS1.Px1.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px1.p2.1.m1.1b"><ci id="S4.SS2.SSS1.Px1.p2.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px1.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px1.p2.1.m1.1c">p</annotation></semantics></math>), which poses difficulties when representing real numbers. A common approach has been to adapt ML models and their training procedures to ensure that (over)underflows are controlled, by operating on normalized quantities and relying on careful quantization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">194</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib206" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">206</span></a>, <a href="#bib.bib84" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">84</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS1.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.p3.1" class="ltx_p">It has been known for several decades that any function can be securely computed, even in the presence of malicious adversaries <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">208</span></a>]</cite>.
While generic solutions exist, their performance characteristics often render them inapplicable in practical settings. As such a noticeable trend in research has consisted in designing custom protocols for applications such as linear and logistic regression <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib359" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">359</span></a>, <a href="#bib.bib194" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">194</span></a>, <a href="#bib.bib351" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">351</span></a>]</cite> and neural network training and inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib351" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">351</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>. These works are typically in the cross-silo setting, or the variant where computation is delegated to a small group of computing servers that do not collude with each other. Porting these protocols to the cross-device setting is not straightforward, as they require a significant amount of communication.</p>
</div>
<section id="S4.SS2.SSS1.Px1.SPx1" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Homomorphic encryption</h6>

<div id="S4.SS2.SSS1.Px1.SPx1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.SPx1.p1.1" class="ltx_p">Homomorphic encryption (HE) schemes allow certain mathematical operations to be performed directly on ciphertexts, without prior decryption. Homomorphic encryption can be a powerful tool for enabling MPC by enabling a participant to compute functions on values while keeping the values hidden.</p>
</div>
<div id="S4.SS2.SSS1.Px1.SPx1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.SPx1.p2.1" class="ltx_p">Different flavours of HE exist, ranging from general fully homomorphic encryption (FHE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">197</span></a>]</cite> to the more efficient leveled variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">87</span></a>, <a href="#bib.bib182" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">182</span></a>, <a href="#bib.bib88" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">88</span></a>, <a href="#bib.bib129" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">129</span></a>]</cite>, for which several implementations exist <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib233" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">233</span></a>, <a href="#bib.bib409" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">409</span></a>, <a href="#bib.bib364" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">364</span></a>, <a href="#bib.bib415" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">415</span></a>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Also of practical relevance are the so-called partially homomorphic schemes, including for example ElGamal and Paillier, allowing either homomorphic addition or multiplication. Additive HE has been used as an ingredient in MPC protocols in the cross-silo setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib359" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">359</span></a>, <a href="#bib.bib224" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">224</span></a>]</cite>.
A review of some homomorphic encryption software libraries along with brief explanations of criteria/features to be considered in choosing a library is surveyed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib404" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">404</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS1.Px1.SPx1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.SPx1.p3.1" class="ltx_p">When considering the use of HE in the FL setting, questions immediately arise about
who holds the secret key of the scheme. While the idea of every client encrypting
their data and sending it to the server to compute homomorphically
on it is appealing, the server should not be able to decrypt a single client contribution.
A trivial way of overcoming this issue would be relying on a non-colluding external
party that holds the secret key and decrypts the result of the computation.
However, most HE schemes require that the secret keys be renewed often (due to e.g. susceptibility to chosen ciphertext attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">117</span></a>]</cite>). Moreover, the availability of a trusted non-colluding
party is not standard in the FL setting.</p>
</div>
<div id="S4.SS2.SSS1.Px1.SPx1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.Px1.SPx1.p4.1" class="ltx_p">Another way around this issue is relying on distributed (or threshold) encryption schemes, where the secret key is distributed among the parties. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Reyzin et al.</span> [<a href="#bib.bib392" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">392</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Roth et al.</span> [<a href="#bib.bib398" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">398</span></a>]</cite> propose such solutions for computing summation in the cross-device setting. Their protocols make use of additively homomorphic schemes (variants of ElGamal and lattice-based schemes, respectively).</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Trusted execution environments</h5>

<div id="S4.SS2.SSS1.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p1.1" class="ltx_p">Trusted execution environments (TEEs, also referred to as secure enclaves) may provide opportunities to move part of the federated learning process into a trusted environment in the cloud, whose code can be attested and verified.</p>
</div>
<div id="S4.SS2.SSS1.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p2.1" class="ltx_p">TEEs can provide several crucial facilities for establishing trust that a unit of code has been executed faithfully and privately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib437" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">437</span></a>]</cite>:</p>
<ul id="S4.I3" class="ltx_itemize">
<li id="S4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i1.p1" class="ltx_para">
<p id="S4.I3.i1.p1.1" class="ltx_p">Confidentiality: The state of the code’s execution remains secret, unless the code explicitly publishes a message.</p>
</div>
</li>
<li id="S4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i2.p1" class="ltx_para">
<p id="S4.I3.i2.p1.1" class="ltx_p">Integrity: The code’s execution cannot be affected, except by the code explicitly receiving an input.</p>
</div>
</li>
<li id="S4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I3.i3.p1" class="ltx_para">
<p id="S4.I3.i3.p1.1" class="ltx_p">Measurement/Attestation: The TEE can prove to a remote party what code (binary) is executing and what its starting state was, defining the initial conditions for confidentiality and integrity.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS2.SSS1.Px2.p3" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p3.1" class="ltx_p">TEEs have been instantiated in many forms, including Intel’s SGX-enabled CPUs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib241" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">241</span></a>, <a href="#bib.bib134" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">134</span></a>]</cite>, Arm’s TrustZone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>, <a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, and Sanctum on RISC-V <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">135</span></a>]</cite>, each varying in its ability to systematically offer the above facilities.</p>
</div>
<div id="S4.SS2.SSS1.Px2.p4" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p4.1" class="ltx_p">Current secure enclaves are limited in terms of memory and provide access only to CPU resources, that is they do not allow processing on GPUs or machine learning processors (Tramèr and Boneh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib447" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">447</span></a>]</cite> explore how to combine TEEs with GPUs for machine learning inference). Moreover, it is challenging for TEEs (especially those operating on shared microprocessors) to fully exclude all types of side channel attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib458" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">458</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS1.Px2.p5" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p5.1" class="ltx_p">While secure enclaves provide protections for all code running inside them, there are additional concerns that must be addressed in practice. For example, it is often necessary to structure the code running in the enclave as a data oblivious procedure, such that its runtime and memory access patterns do not reveal information about the data upon which it is computing (see for example <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>). Furthermore, measurement/attestation typically only proves that a particular binary is running; it is up to the system architect to provide a means for proving that that binary has the desired privacy properties, potentially requiring the binary to be built using a reproducible process from open source code.</p>
</div>
<div id="S4.SS2.SSS1.Px2.p6" class="ltx_para">
<p id="S4.SS2.SSS1.Px2.p6.1" class="ltx_p">It remains an open question how to partition federated learning functions across secure enclaves, cloud computing resources, and client devices. For example, secure enclaves could execute key functions such as secure aggregation or shuffling to limit the server’s access to raw client contributions while keeping most of the federated learning logic outside this trusted computing base.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Secure computation problems of interest</h5>

<div id="S4.SS2.SSS1.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.p1.1" class="ltx_p">While secure multi-party computation and trusted execution environments offer general solutions to the problem of privately computing any function on distributed private data, many optimizations are possible when focusing on specific functionalities. This is the case for the tasks described next.</p>
</div>
<section id="S4.SS2.SSS1.Px3.SPx1" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Secure aggregation</h6>

<div id="S4.SS2.SSS1.Px3.SPx1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx1.p1.1" class="ltx_p">Secure aggregation is a functionality for <math id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1a"><mi id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1b"><ci id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px3.SPx1.p1.1.m1.1c">n</annotation></semantics></math> clients and a server. It enables each client to submit a value (often a vector or tensor in the FL setting), such that the server learns just an aggregate function of the clients’ values, typically the sum.</p>
</div>
<div id="S4.SS2.SSS1.Px3.SPx1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx1.p2.1" class="ltx_p">There is a rich literature exploring secure aggregation in both the single-server setting (via additive masking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib213" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">213</span></a>, <a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>, <a href="#bib.bib428" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">428</span></a>]</cite>, via threshold homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib417" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">417</span></a>, <a href="#bib.bib218" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">218</span></a>, <a href="#bib.bib103" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">103</span></a>]</cite>, and via generic secure multi-party computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">94</span></a>]</cite>) as well as in the multiple non-colluding servers setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">78</span></a>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>, <a href="#bib.bib130" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">130</span></a>]</cite>.
Secure aggregation can also be approached using trusted execution environments (introduced above), as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib308" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">308</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px3.SPx2" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Secure shuffling</h6>

<div id="S4.SS2.SSS1.Px3.SPx2.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx2.p1.1" class="ltx_p">Secure shuffling is a functionality for <math id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1a"><mi id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1.1" xref="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1b"><ci id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.Px3.SPx2.p1.1.m1.1c">n</annotation></semantics></math> clients and a server. It enables each client to submit one or more messages, such that the server learns just an unordered collection (multiset) of the messages from all clients and nothing more. Specifically, the server has no ability to link any message to its sender beyond the information contained in the message itself.
Secure shuffling can be considered an instance of Secure Aggregation where the values are multiset-singletons and the aggregation operation is multiset-sum, though it is often the case that very different implementations provide the best performance in the typical operating regimes for secure shuffling and secure aggregation.</p>
</div>
<div id="S4.SS2.SSS1.Px3.SPx2.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx2.p2.1" class="ltx_p">Secure shufflers have been studied in the context of secure multi-party computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">107</span></a>, <a href="#bib.bib288" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">288</span></a>]</cite>, often under the heading of mix networks. They have also been studied in the context of trusted computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite>. Mix networks have found large scale deployment in the form of the Tor network <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib157" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">157</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS1.Px3.SPx3" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Private information retrieval</h6>

<div id="S4.SS2.SSS1.Px3.SPx3.p1" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx3.p1.1" class="ltx_p">Private information retrieval (PIR) is a functionality for one client and one server. It enables the client to download an entry from a server-hosted database such that the server gains zero information about which entry the client requested.</p>
</div>
<div id="S4.SS2.SSS1.Px3.SPx3.p2" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx3.p2.1" class="ltx_p">MPC approaches to PIR break down into two main categories: <span id="S4.SS2.SSS1.Px3.SPx3.p2.1.1" class="ltx_text ltx_font_italic">computational PIR</span> (cPIR), in which a single party can execute the entire server side of the protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib286" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">286</span></a>]</cite>, and <span id="S4.SS2.SSS1.Px3.SPx3.p2.1.2" class="ltx_text ltx_font_italic">information theoretic PIR</span> (itPIR), in which multiple non-colluding parties are required to execute the server side of the protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">121</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS1.Px3.SPx3.p3" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx3.p3.1" class="ltx_p">The main roadblocks to the applicability of PIR have been the following: cPIR has high computational cost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib423" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">423</span></a>]</cite>, while the non-colluding parties setting has been difficult to achieve convincingly in industrial scenarios. Recent results on PIR have shown dramatic reductions in the computational cost through the use of lattice-based cryptosystems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib363" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">363</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib198" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">198</span></a>]</cite>. The computational cost can be traded for more communication; we refer the reader to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ali et al.</span> [<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite> to better understand the communication and computation trade-offs offered by cPIR. Additionally, it has been shown how to construct communication-efficient PIR on a single-server by leveraging side information available to the user <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib251" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">251</span></a>]</cite>, for example via client local state. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Patel et al.</span> [<a href="#bib.bib372" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">372</span></a>]</cite> presented and implemented a practical hybrid (computational and information theoretic) PIR scheme on a single server assuming client state. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Corrigan-Gibbs and
Kogan</span> [<a href="#bib.bib131" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">131</span></a>]</cite> present theoretical constructions for PIR with sublinear <em id="S4.SS2.SSS1.Px3.SPx3.p3.1.1" class="ltx_emph ltx_font_italic">online</em> time by working in an offline/online model where, during an offline phase, clients fetch information from the server(s) independent on the future query to be performed.</p>
</div>
<div id="S4.SS2.SSS1.Px3.SPx3.p4" class="ltx_para">
<p id="S4.SS2.SSS1.Px3.SPx3.p4.1" class="ltx_p">Further work has explored the connection between PIR and secret sharing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib479" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">479</span></a>]</cite>, with recent connections to PIR on coded data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">159</span></a>]</cite> and communication efficient PIR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">72</span></a>]</cite>. A variant of PIR, called PIR-with-Default, enable clients to retrieve a default value if the index queried is not in the database, and can output additive secret shares of items which can serve as input to any MPC protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib297" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">297</span></a>]</cite>. PIR has also been studied in the context of ON-OFF privacy, in which a client is permitted to switch off their privacy guards in exchange for better utility or performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib355" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">355</span></a>, <a href="#bib.bib494" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">494</span></a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Privacy-Preserving Disclosures</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.7" class="ltx_p">The state-of-the-art model for quantifying and limiting information disclosure about individuals is <span id="S4.SS2.SSS2.p1.7.1" class="ltx_text ltx_font_italic">differential privacy</span> (DP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">167</span></a>, <a href="#bib.bib164" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">164</span></a>, <a href="#bib.bib165" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">165</span></a>]</cite>, which aims to introduce a level of uncertainty into the released model sufficient to mask the contribution of any individual user. Differential privacy is quantified by privacy loss parameters <math id="S4.SS2.SSS2.p1.1.m1.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.2a"><mrow id="S4.SS2.SSS2.p1.1.m1.2.3.2" xref="S4.SS2.SSS2.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p1.1.m1.2.3.2.1" xref="S4.SS2.SSS2.p1.1.m1.2.3.1.cmml">(</mo><mi id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml">ε</mi><mo id="S4.SS2.SSS2.p1.1.m1.2.3.2.2" xref="S4.SS2.SSS2.p1.1.m1.2.3.1.cmml">,</mo><mi id="S4.SS2.SSS2.p1.1.m1.2.2" xref="S4.SS2.SSS2.p1.1.m1.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS2.SSS2.p1.1.m1.2.3.2.3" xref="S4.SS2.SSS2.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.2b"><interval closure="open" id="S4.SS2.SSS2.p1.1.m1.2.3.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.2.3.2"><ci id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">𝜀</ci><ci id="S4.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.2c">(\varepsilon,\delta)</annotation></semantics></math>, where smaller <math id="S4.SS2.SSS2.p1.2.m2.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.2a"><mrow id="S4.SS2.SSS2.p1.2.m2.2.3.2" xref="S4.SS2.SSS2.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p1.2.m2.2.3.2.1" xref="S4.SS2.SSS2.p1.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml">ε</mi><mo id="S4.SS2.SSS2.p1.2.m2.2.3.2.2" xref="S4.SS2.SSS2.p1.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS2.SSS2.p1.2.m2.2.2" xref="S4.SS2.SSS2.p1.2.m2.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS2.SSS2.p1.2.m2.2.3.2.3" xref="S4.SS2.SSS2.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.2b"><interval closure="open" id="S4.SS2.SSS2.p1.2.m2.2.3.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.2.3.2"><ci id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1">𝜀</ci><ci id="S4.SS2.SSS2.p1.2.m2.2.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.2c">(\varepsilon,\delta)</annotation></semantics></math> corresponds to increased privacy. More formally, a randomized algorithm <math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><ci id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">\mathcal{A}</annotation></semantics></math> is <math id="S4.SS2.SSS2.p1.4.m4.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.2a"><mrow id="S4.SS2.SSS2.p1.4.m4.2.3.2" xref="S4.SS2.SSS2.p1.4.m4.2.3.1.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p1.4.m4.2.3.2.1" xref="S4.SS2.SSS2.p1.4.m4.2.3.1.cmml">(</mo><mi id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">ε</mi><mo id="S4.SS2.SSS2.p1.4.m4.2.3.2.2" xref="S4.SS2.SSS2.p1.4.m4.2.3.1.cmml">,</mo><mi id="S4.SS2.SSS2.p1.4.m4.2.2" xref="S4.SS2.SSS2.p1.4.m4.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS2.SSS2.p1.4.m4.2.3.2.3" xref="S4.SS2.SSS2.p1.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.2b"><interval closure="open" id="S4.SS2.SSS2.p1.4.m4.2.3.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.2.3.2"><ci id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1">𝜀</ci><ci id="S4.SS2.SSS2.p1.4.m4.2.2.cmml" xref="S4.SS2.SSS2.p1.4.m4.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.2c">(\varepsilon,\delta)</annotation></semantics></math>-differentially private if for all <math id="S4.SS2.SSS2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{S}\subseteq\text{Range}(\mathcal{A})" display="inline"><semantics id="S4.SS2.SSS2.p1.5.m5.1a"><mrow id="S4.SS2.SSS2.p1.5.m5.1.2" xref="S4.SS2.SSS2.p1.5.m5.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS2.p1.5.m5.1.2.2" xref="S4.SS2.SSS2.p1.5.m5.1.2.2.cmml">𝒮</mi><mo id="S4.SS2.SSS2.p1.5.m5.1.2.1" xref="S4.SS2.SSS2.p1.5.m5.1.2.1.cmml">⊆</mo><mrow id="S4.SS2.SSS2.p1.5.m5.1.2.3" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.cmml"><mtext id="S4.SS2.SSS2.p1.5.m5.1.2.3.2" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.2a.cmml">Range</mtext><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p1.5.m5.1.2.3.1" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.1.cmml">​</mo><mrow id="S4.SS2.SSS2.p1.5.m5.1.2.3.3.2" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.cmml"><mo stretchy="false" id="S4.SS2.SSS2.p1.5.m5.1.2.3.3.2.1" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS2.p1.5.m5.1.1" xref="S4.SS2.SSS2.p1.5.m5.1.1.cmml">𝒜</mi><mo stretchy="false" id="S4.SS2.SSS2.p1.5.m5.1.2.3.3.2.2" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.5.m5.1b"><apply id="S4.SS2.SSS2.p1.5.m5.1.2.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2"><subset id="S4.SS2.SSS2.p1.5.m5.1.2.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.1"></subset><ci id="S4.SS2.SSS2.p1.5.m5.1.2.2.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.2">𝒮</ci><apply id="S4.SS2.SSS2.p1.5.m5.1.2.3.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.3"><times id="S4.SS2.SSS2.p1.5.m5.1.2.3.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.1"></times><ci id="S4.SS2.SSS2.p1.5.m5.1.2.3.2a.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.2"><mtext id="S4.SS2.SSS2.p1.5.m5.1.2.3.2.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.2.3.2">Range</mtext></ci><ci id="S4.SS2.SSS2.p1.5.m5.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m5.1.1">𝒜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.5.m5.1c">\mathcal{S}\subseteq\text{Range}(\mathcal{A})</annotation></semantics></math>, and for all adjacent datasets <math id="S4.SS2.SSS2.p1.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.SSS2.p1.6.m6.1a"><mi id="S4.SS2.SSS2.p1.6.m6.1.1" xref="S4.SS2.SSS2.p1.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.6.m6.1b"><ci id="S4.SS2.SSS2.p1.6.m6.1.1.cmml" xref="S4.SS2.SSS2.p1.6.m6.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.6.m6.1c">D</annotation></semantics></math> and <math id="S4.SS2.SSS2.p1.7.m7.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS2.SSS2.p1.7.m7.1a"><msup id="S4.SS2.SSS2.p1.7.m7.1.1" xref="S4.SS2.SSS2.p1.7.m7.1.1.cmml"><mi id="S4.SS2.SSS2.p1.7.m7.1.1.2" xref="S4.SS2.SSS2.p1.7.m7.1.1.2.cmml">D</mi><mo id="S4.SS2.SSS2.p1.7.m7.1.1.3" xref="S4.SS2.SSS2.p1.7.m7.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.7.m7.1b"><apply id="S4.SS2.SSS2.p1.7.m7.1.1.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.7.m7.1.1.1.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.7.m7.1.1.2.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1.2">𝐷</ci><ci id="S4.SS2.SSS2.p1.7.m7.1.1.3.cmml" xref="S4.SS2.SSS2.p1.7.m7.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.7.m7.1c">D^{\prime}</annotation></semantics></math>:</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.2" class="ltx_Math" alttext="P(\mathcal{A}(D)\in\mathcal{S})\leq e^{\varepsilon}P(\mathcal{A}(D^{\prime})\in\mathcal{S})+\delta." display="block"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.1" xref="S4.E3.m1.2.2.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1" xref="S4.E3.m1.2.2.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1.1" xref="S4.E3.m1.2.2.1.1.1.cmml"><mi id="S4.E3.m1.2.2.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">D</mi><mo stretchy="false" id="S4.E3.m1.2.2.1.1.1.1.1.1.2.3.2.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml">𝒮</mi></mrow><mo stretchy="false" id="S4.E3.m1.2.2.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.3" xref="S4.E3.m1.2.2.1.1.3.cmml">≤</mo><mrow id="S4.E3.m1.2.2.1.1.2" xref="S4.E3.m1.2.2.1.1.2.cmml"><mrow id="S4.E3.m1.2.2.1.1.2.1" xref="S4.E3.m1.2.2.1.1.2.1.cmml"><msup id="S4.E3.m1.2.2.1.1.2.1.3" xref="S4.E3.m1.2.2.1.1.2.1.3.cmml"><mi id="S4.E3.m1.2.2.1.1.2.1.3.2" xref="S4.E3.m1.2.2.1.1.2.1.3.2.cmml">e</mi><mi id="S4.E3.m1.2.2.1.1.2.1.3.3" xref="S4.E3.m1.2.2.1.1.2.1.3.3.cmml">ε</mi></msup><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.2.1.2" xref="S4.E3.m1.2.2.1.1.2.1.2.cmml">​</mo><mi id="S4.E3.m1.2.2.1.1.2.1.4" xref="S4.E3.m1.2.2.1.1.2.1.4.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.2.1.2a" xref="S4.E3.m1.2.2.1.1.2.1.2.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.2.1.1.1" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.1.1.2.1.1.1.2" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.2.2.1.1.2.1.1.1.1" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2.cmml">D</mi><mo id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.2.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.2.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.3.cmml">𝒮</mi></mrow><mo stretchy="false" id="S4.E3.m1.2.2.1.1.2.1.1.1.3" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.2.2" xref="S4.E3.m1.2.2.1.1.2.2.cmml">+</mo><mi id="S4.E3.m1.2.2.1.1.2.3" xref="S4.E3.m1.2.2.1.1.2.3.cmml">δ</mi></mrow></mrow><mo lspace="0em" id="S4.E3.m1.2.2.1.2" xref="S4.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.1.1.cmml" xref="S4.E3.m1.2.2.1"><leq id="S4.E3.m1.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.3"></leq><apply id="S4.E3.m1.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1"><times id="S4.E3.m1.2.2.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.2"></times><ci id="S4.E3.m1.2.2.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.3">𝑃</ci><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1"><in id="S4.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1"></in><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2"><times id="S4.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.1"></times><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.2">𝒜</ci><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">𝐷</ci></apply><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3">𝒮</ci></apply></apply><apply id="S4.E3.m1.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2"><plus id="S4.E3.m1.2.2.1.1.2.2.cmml" xref="S4.E3.m1.2.2.1.1.2.2"></plus><apply id="S4.E3.m1.2.2.1.1.2.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1"><times id="S4.E3.m1.2.2.1.1.2.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2.1.2"></times><apply id="S4.E3.m1.2.2.1.1.2.1.3.cmml" xref="S4.E3.m1.2.2.1.1.2.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.2.1.3.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1.3">superscript</csymbol><ci id="S4.E3.m1.2.2.1.1.2.1.3.2.cmml" xref="S4.E3.m1.2.2.1.1.2.1.3.2">𝑒</ci><ci id="S4.E3.m1.2.2.1.1.2.1.3.3.cmml" xref="S4.E3.m1.2.2.1.1.2.1.3.3">𝜀</ci></apply><ci id="S4.E3.m1.2.2.1.1.2.1.4.cmml" xref="S4.E3.m1.2.2.1.1.2.1.4">𝑃</ci><apply id="S4.E3.m1.2.2.1.1.2.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1"><in id="S4.E3.m1.2.2.1.1.2.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.2"></in><apply id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1"><times id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.2"></times><ci id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.3">𝒜</ci><apply id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="S4.E3.m1.2.2.1.1.2.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.2.1.1.1.1.3">𝒮</ci></apply></apply><ci id="S4.E3.m1.2.2.1.1.2.3.cmml" xref="S4.E3.m1.2.2.1.1.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">P(\mathcal{A}(D)\in\mathcal{S})\leq e^{\varepsilon}P(\mathcal{A}(D^{\prime})\in\mathcal{S})+\delta.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS2.p1.13" class="ltx_p">In the context of FL, <math id="S4.SS2.SSS2.p1.8.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.SSS2.p1.8.m1.1a"><mi id="S4.SS2.SSS2.p1.8.m1.1.1" xref="S4.SS2.SSS2.p1.8.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.8.m1.1b"><ci id="S4.SS2.SSS2.p1.8.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.8.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.8.m1.1c">D</annotation></semantics></math> and <math id="S4.SS2.SSS2.p1.9.m2.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS2.SSS2.p1.9.m2.1a"><msup id="S4.SS2.SSS2.p1.9.m2.1.1" xref="S4.SS2.SSS2.p1.9.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p1.9.m2.1.1.2" xref="S4.SS2.SSS2.p1.9.m2.1.1.2.cmml">D</mi><mo id="S4.SS2.SSS2.p1.9.m2.1.1.3" xref="S4.SS2.SSS2.p1.9.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.9.m2.1b"><apply id="S4.SS2.SSS2.p1.9.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.9.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.9.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.9.m2.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.9.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.9.m2.1.1.2">𝐷</ci><ci id="S4.SS2.SSS2.p1.9.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p1.9.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.9.m2.1c">D^{\prime}</annotation></semantics></math> correspond to decentralized datasets that are adjacent if <math id="S4.SS2.SSS2.p1.10.m3.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS2.SSS2.p1.10.m3.1a"><msup id="S4.SS2.SSS2.p1.10.m3.1.1" xref="S4.SS2.SSS2.p1.10.m3.1.1.cmml"><mi id="S4.SS2.SSS2.p1.10.m3.1.1.2" xref="S4.SS2.SSS2.p1.10.m3.1.1.2.cmml">D</mi><mo id="S4.SS2.SSS2.p1.10.m3.1.1.3" xref="S4.SS2.SSS2.p1.10.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.10.m3.1b"><apply id="S4.SS2.SSS2.p1.10.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.10.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.10.m3.1.1.1.cmml" xref="S4.SS2.SSS2.p1.10.m3.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.10.m3.1.1.2.cmml" xref="S4.SS2.SSS2.p1.10.m3.1.1.2">𝐷</ci><ci id="S4.SS2.SSS2.p1.10.m3.1.1.3.cmml" xref="S4.SS2.SSS2.p1.10.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.10.m3.1c">D^{\prime}</annotation></semantics></math> can be obtained from <math id="S4.SS2.SSS2.p1.11.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.SSS2.p1.11.m4.1a"><mi id="S4.SS2.SSS2.p1.11.m4.1.1" xref="S4.SS2.SSS2.p1.11.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.11.m4.1b"><ci id="S4.SS2.SSS2.p1.11.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.11.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.11.m4.1c">D</annotation></semantics></math> by adding or subtracting all the records of a single client (user) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>]</cite>. This notion of differential privacy is referred to as user-level differential privacy. It is stronger than the typically used notion of adjacency where <math id="S4.SS2.SSS2.p1.12.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.SSS2.p1.12.m5.1a"><mi id="S4.SS2.SSS2.p1.12.m5.1.1" xref="S4.SS2.SSS2.p1.12.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.12.m5.1b"><ci id="S4.SS2.SSS2.p1.12.m5.1.1.cmml" xref="S4.SS2.SSS2.p1.12.m5.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.12.m5.1c">D</annotation></semantics></math> and <math id="S4.SS2.SSS2.p1.13.m6.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS2.SSS2.p1.13.m6.1a"><msup id="S4.SS2.SSS2.p1.13.m6.1.1" xref="S4.SS2.SSS2.p1.13.m6.1.1.cmml"><mi id="S4.SS2.SSS2.p1.13.m6.1.1.2" xref="S4.SS2.SSS2.p1.13.m6.1.1.2.cmml">D</mi><mo id="S4.SS2.SSS2.p1.13.m6.1.1.3" xref="S4.SS2.SSS2.p1.13.m6.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.13.m6.1b"><apply id="S4.SS2.SSS2.p1.13.m6.1.1.cmml" xref="S4.SS2.SSS2.p1.13.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.13.m6.1.1.1.cmml" xref="S4.SS2.SSS2.p1.13.m6.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.p1.13.m6.1.1.2.cmml" xref="S4.SS2.SSS2.p1.13.m6.1.1.2">𝐷</ci><ci id="S4.SS2.SSS2.p1.13.m6.1.1.3.cmml" xref="S4.SS2.SSS2.p1.13.m6.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.13.m6.1c">D^{\prime}</annotation></semantics></math> differ by only one record <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib165" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">165</span></a>]</cite>, since in general one user may contribute many records (e.g. training examples) to the dataset.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">Over the last decade, an extensive set of techniques has been developed for differentially private data analysis, particularly under the assumption of a centralized setting, where the raw data is collected by a trusted party prior to applying perturbations necessary to achieve privacy. In federated learning, typically the orchestrating server would serve as the trusted implementer of the DP mechanism, ensuring only privatized outputs are released to the model engineer or analyst.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">However, when possible we often wish to reduce the need for a trusted party. Several approaches for reducing the need for trust in a data curator have been considered in recent years.</p>
</div>
<section id="S4.SS2.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Local differential privacy</h5>

<div id="S4.SS2.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px1.p1.3" class="ltx_p">Differential privacy can be achieved without requiring trust in a centralized server by having each client apply a differentially private transformation to their data prior to sharing it with the server. That is, we apply <a href="#S4.E3" title="In 4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3</span></a> to a mechanism <math id="S4.SS2.SSS2.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS2.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.1.m1.1b"><ci id="S4.SS2.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.1.m1.1c">\mathcal{A}</annotation></semantics></math> that processes a single user’s local dataset <math id="S4.SS2.SSS2.Px1.p1.2.m2.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.2.m2.1a"><mi id="S4.SS2.SSS2.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.2.m2.1b"><ci id="S4.SS2.SSS2.Px1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.2.m2.1c">D</annotation></semantics></math>, with the guarantee holding with respect to <em id="S4.SS2.SSS2.Px1.p1.3.1" class="ltx_emph ltx_font_italic">any</em> possible other local dataset <math id="S4.SS2.SSS2.Px1.p1.3.m3.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.SS2.SSS2.Px1.p1.3.m3.1a"><msup id="S4.SS2.SSS2.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2.cmml">D</mi><mo id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.3" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px1.p1.3.m3.1b"><apply id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1">superscript</csymbol><ci id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.2">𝐷</ci><ci id="S4.SS2.SSS2.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS2.SSS2.Px1.p1.3.m3.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px1.p1.3.m3.1c">D^{\prime}</annotation></semantics></math>.
This model is referred to as the <span id="S4.SS2.SSS2.Px1.p1.3.2" class="ltx_text ltx_font_italic">local model of differential privacy</span> (LDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib475" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">475</span></a>, <a href="#bib.bib266" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">266</span></a>]</cite>. LDP has been deployed effectively to gather statistics on popular items across large userbases by Google, Apple and Microsoft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">177</span></a>, <a href="#bib.bib154" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">154</span></a>, <a href="#bib.bib155" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">155</span></a>]</cite>. It has also been used in federated settings for spam classifier training by Snap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib378" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">378</span></a>]</cite>. These LDP deployments all involve large numbers of clients and reports, even up to a billion in the case of Snap, which stands in stark contrast to centralized instantiations of DP which can provide high utility from much smaller datasets. Unfortunately, as we will discuss in Section <a href="#S4.SS4.SSS2" title="4.4.2 Limitations of Existing Solutions ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>, achieving LDP while maintaining utility can be difficult <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">266</span></a>, <a href="#bib.bib455" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">455</span></a>]</cite>. Thus, there is a need for a model of differential privacy that interpolates between purely central and purely local DP. This can be achieved through distributed differential privacy, or the hybrid model, as discussed below.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Distributed differential privacy</h5>

<div id="S4.SS2.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p1.1" class="ltx_p">In order to recover some of the utility of central DP without having to rely on a trustworthy central server, one can instead use a <em id="S4.SS2.SSS2.Px2.p1.1.1" class="ltx_emph ltx_font_italic">distributed differential privacy model</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">166</span></a>, <a href="#bib.bib417" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">417</span></a>, <a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>, <a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">120</span></a>]</cite>. Under this model, the clients first compute and encode a minimal (application specific) focused report, and then send the encoded reports to a secure computation function, whose output is available to the central server, with the intention that this output already satisfies differential privacy requirements by the time the server is able to inspect it. The encoding is done to help maintain privacy on the clients, and could for example include LDP. The secure computation function can have a variety of incarnations. It could be an MPC protocol, a standard computation done on a TEE, or even a combination of the two. Each of these choices comes with different assumptions and threat models.</p>
</div>
<div id="S4.SS2.SSS2.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.p2.1" class="ltx_p">It is important to remark that distributed differential privacy and local differential privacy yield different guarantees from several perspectives: while the distributed DP framework can produce more accurate statistics for the same level of differential privacy as LDP, it relies on different setups and typically makes stronger assumptions, such as access to MPC protocols. Below, we outline two possible approaches to distributed differential privacy, relying on secure aggregation and secure shuffling. We stress that there are many other methods that could be used, see for instance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib400" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">400</span></a>]</cite> for an approach based on exchanging correlated Gaussian noise across secure channels.</p>
</div>
<section id="S4.SS2.SSS2.Px2.SPx1" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Distributed DP via secure aggregation</h6>

<div id="S4.SS2.SSS2.Px2.SPx1.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.SPx1.p1.2" class="ltx_p">One promising tool for achieving distributed DP in FL is secure aggregation, discussed above in Section <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>. Secure aggregation can be used to ensure that the central server obtains the aggregated result, while guaranteeing that intermediate parameters of individual devices and participants are not revealed to the central server. To further ensure the aggregated result does not reveal additional information to the server, we can use local differential privacy (e.g. with moderate <math id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1a"><mi id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1b"><ci id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.SPx1.p1.1.m1.1c">\varepsilon</annotation></semantics></math> level). For example, each device could perturb its own model parameter before the secure aggregation in order to achieve local differential privacy. By designing the noise correctly, we may ensure that the noise in the aggregated result matches the noise that would have otherwise been added centrally by a trusted server (e.g. with a low <math id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1a"><mi id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1b"><ci id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.SPx1.p1.2.m2.1c">\varepsilon</annotation></semantics></math> / high privacy level) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib385" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">385</span></a>, <a href="#bib.bib205" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">205</span></a>, <a href="#bib.bib417" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">417</span></a>, <a href="#bib.bib213" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">213</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS2.Px2.SPx2" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Distributed DP via secure shuffling</h6>

<div id="S4.SS2.SSS2.Px2.SPx2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.SPx2.p1.2" class="ltx_p">Another distributed differential privacy model is the shuffling model, which was kicked off by the recently introduced Encode-Shuffle-Analyze (ESA) framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite> (illustrated in Figure <a href="#S4.F3a" title="Figure 3 ‣ Distributed DP via secure shuffling ‣ Distributed differential privacy ‣ 4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). In the simplest version of this framework, each client runs an LDP protocol (e.g. with a moderate <math id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1a"><mi id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1.1" xref="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1b"><ci id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.SPx2.p1.1.m1.1c">\varepsilon</annotation></semantics></math> level) on its data and provides its output to a secure shuffler. The shuffler randomly permutes the reports and sends the collection of shuffled reports (without any identifying information) to the server for final analysis. Intuitively, the interposition of this secure compute function makes it harder for the server to learn anything about the participants and supports a differential privacy analysis (e.g. with a low <math id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1a"><mi id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1.1" xref="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1b"><ci id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.Px2.SPx2.p1.2.m2.1c">\varepsilon</annotation></semantics></math> / high privacy level). In the more general multi-message shuffled framework, each user can possibly send more than one message to the shuffler. The shuffler can either be implemented directly as a trusted entity, independent of the server and devoted solely to shuffling, or via more complex cryptographic primitives as discussed above.</p>
</div>
<figure id="S4.F3a" class="ltx_figure"><img src="/html/1912.04977/assets/esa_diagram.jpeg" id="S4.F3a.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The Encode-Shuffle-Analyze (ESA) framework, illustrated here for <math id="S4.F3a.2.m1.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.F3a.2.m1.1b"><mn id="S4.F3a.2.m1.1.1" xref="S4.F3a.2.m1.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.F3a.2.m1.1c"><cn type="integer" id="S4.F3a.2.m1.1.1.cmml" xref="S4.F3a.2.m1.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.F3a.2.m1.1d">4</annotation></semantics></math> players.</figcaption>
</figure>
<div id="S4.SS2.SSS2.Px2.SPx2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.SPx2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bittau et al.</span> [<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite> proposed the Prochlo system as a way to implement the ESA framework. The system takes a holistic approach to privacy that takes into account secure computation aspects (addressed using TEEs), private disclosure aspects (addressed by means of differential privacy), and verifiability aspects (mitigated using secure enclave attestation capabilities).</p>
</div>
<div id="S4.SS2.SSS2.Px2.SPx2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.Px2.SPx2.p3.1" class="ltx_p">More generally, shuffling models of differential privacy can use broader classes of local randomizers, and can even select these local randomizers adaptively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib178" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">178</span></a>]</cite>. This can enable differentially private protocols with far smaller error than what is possible in the local model, while relying on weaker trust assumptions than in the central model, e.g.,  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">120</span></a>, <a href="#bib.bib178" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">178</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>, <a href="#bib.bib201" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">201</span></a>, <a href="#bib.bib204" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">204</span></a>, <a href="#bib.bib200" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">200</span></a>, <a href="#bib.bib202" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">202</span></a>, <a href="#bib.bib203" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">203</span></a>, <a href="#bib.bib110" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">110</span></a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hybrid differential privacy</h5>

<div id="S4.SS2.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS2.SSS2.Px3.p1.1" class="ltx_p">Another promising approach is hybrid differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>, which combines multiple trust models by partitioning users based on their trust model preference (e.g. trust or lack of trust in the curator). Prior to the hybrid model, there were two natural choices. The first was to use the least-trusting model, which typically provides the lowest utility, and conservatively apply it uniformly over the entire userbase. The second was to use the most-trusting model, which typically provides the highest utility, but only apply it over the most-trusting users. By allowing multiple models to coexist, hybrid model mechanisms can achieve more utility from a given userbase, compared to purely local or central DP mechanisms. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> describes a system in which most users contribute their data in the local model of privacy, and a small fraction of users opt-in to contributing their data in the central DP model. This enables the design of a mechanism which, in some circumstances, outperforms both the conservative local DP mechanism applied across all users as well as the central DP mechanism applied only across the small fraction of opt-in users. Recent work by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite> further demonstrates that a combination of multiple trust models can become part of a promising toolkit for designing and implementing differential privacy. This construction can be directly applied in the federated learning setting; however, the general concept of combining trust models or computational models may also inspire similar but new approaches for federated learning.</p>
</div>
</section>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Verifiability</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">An important notion that is orthogonal to the above privacy techniques is that of verifiability. Generally speaking, verifiable computation will enable one party to prove to another party that it has executed the desired behavior on its data faithfully, without compromising the potential secrecy of the data. The concept of verifiable computation dates back to <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Babai et al.</span> [<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite> and has been studied under various terms in the literature: checking computations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>]</cite>, certified computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib343" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">343</span></a>]</cite>, delegating computations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib210" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">210</span></a>]</cite>, as well as verifiable computing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">195</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">In the context of FL, verifiability can be used for two purposes. First, it would enable the server to prove to the clients that it executed the intended behavior (e.g., aggregating inputs, shuffling of the input messages, or adding noise for differential privacy) faithfully. Second, it would enable the clients to prove to the server that their inputs and behavior follow that of the protocol specification (e.g., the input belongs to a certain range, or the data is a correctly generated ciphertext).</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">Multiple techniques can be useful to provide verifiability: zero-knowledge proofs (ZKPs), trusted execution environments (TEEs), or remote attestation. Among these ZKPs provide formal cryptographic security guarantees based on mathematical hardness, while others make rely on assumption about the security of trusted hardware.</p>
</div>
<section id="S4.SS2.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Zero-knowledge proofs (ZKPs)</h5>

<div id="S4.SS2.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p1.1" class="ltx_p">Zero knowledge (ZK) proofs are a cryptographic primitive that enables one party (called the
<em id="S4.SS2.SSS3.Px1.p1.1.1" class="ltx_emph ltx_font_italic">prover</em>) to prove statements to another party (called the <em id="S4.SS2.SSS3.Px1.p1.1.2" class="ltx_emph ltx_font_italic">verifier</em>), that depend on secret
information known to the prover, called witness, without revealing those secrets to the verifier. The notion of zero-knowledge was introduced in the late 1980’s by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Goldwasser et al.</span> [<a href="#bib.bib209" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">209</span></a>]</cite>. It provides a solution for the verifiability question on private data. While
there had been a large body of work on ZK construction, the first work that brought ZKPs and verifiable computation for general functionalities in the realm of practicality was the work of  <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Parno et al.</span> [<a href="#bib.bib369" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">369</span></a>]</cite> which introduces the first optimized construction and implementation for succinct ZK. Nowadays, ZKP protocols can achieve proof sizes of hundred of bytes and verifications of the order of milliseconds regardless of the size of the statement being proved.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p2" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p2.1" class="ltx_p">A ZKP has three salient properties: <em id="S4.SS2.SSS3.Px1.p2.1.1" class="ltx_emph ltx_font_italic">completeness</em> (if the statement is true and the prover and verifier follow the protocol, the verifier will accept the proof), <em id="S4.SS2.SSS3.Px1.p2.1.2" class="ltx_emph ltx_font_italic">soundness</em> (if the statement is false and the verifier follows the protocol, the verifier will refuse the proof), and <em id="S4.SS2.SSS3.Px1.p2.1.3" class="ltx_emph ltx_font_italic">zero-knowledge</em> (if the statement is true and the prover follows the protocol, the verifier will only learn that the statement is true and will not learn any confidential information from the interaction).</p>
</div>
<div id="S4.SS2.SSS3.Px1.p3" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p3.1" class="ltx_p">Beyond these common properties, there are different types of zero-knowledge constructions in terms of supported language for the proofs, setup requirements, prover and verifier computational efficiency, interactivity, succinctness, and underlying hardness assumptions. There are many ZK constructions that support specific classes of statements, Schnorr proofs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib408" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">408</span></a>]</cite> and Sigma protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">147</span></a>]</cite> are examples of such widely used protocols. While such protocols have numerous uses in specific settings, general ZK systems that can support any functionality provide a much more broadly applicable tool (including in the context of FL), and thus we focus on such constructions for the rest of the discussion.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p4" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p4.1" class="ltx_p">A major distinguishing feature between different constructions is the need for <em id="S4.SS2.SSS3.Px1.p4.1.1" class="ltx_emph ltx_font_italic">trusted</em> setup. Some ZKPs rely on a common reference string (CRS), which is computed using secrets that should remain hidden in order to guarantee the soundness properties of the proofs. The computation of such a CRS is referred to as a trusted setup. While this requirement is a disadvantage for such systems, the existing ZKP constructions that achieve most succinct proofs and verifier’s efficiency require trusted setup.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p5" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p5.1" class="ltx_p">Another significant property that affects the applicability in different scenarios is whether generating the proof requires interaction between the prover and the verifier, and here we distinguish non-interactive zero-knowledge proofs (NIZKs) that enable the prover to send a single message to the verifier and require no further communication. Often we can convert interactive to non-interactive proofs by making stronger assumptions about ideal functionality of hash functions (i.e., that hash functions behave as random oracles).</p>
</div>
<div id="S4.SS2.SSS3.Px1.p6" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p6.1" class="ltx_p">Additionally, there are different measurements for efficiency of a ZKP system one must be aware of, such as the length of the proof and the computation complexity of the prover and verifier. The ideal prover’s complexity should be linear in the execution time for the evaluated functionality but many existing ZKPs introduce additional (sometimes significant) overhead for the prover. The most efficient verification complexity requires computation at least linear in the size of the inputs for the evaluated functionality, and in the setting of proofs for the work of the FL server this input size will be significant.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p7" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p7.1" class="ltx_p">Succinct
non-interactive zero-knowledge proofs (SNARKs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">71</span></a>]</cite> are a type of ZKP that provides constant proof size and verification that depends only on the input size, linearly. These attractive efficiency properties do come at the price of stronger assumptions, which is mostly inherent, and trusted setup in all existing scheme. Most existing SNARK constructions leverage quadratic arithmetic programs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">196</span></a>, <a href="#bib.bib369" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">369</span></a>, <a href="#bib.bib136" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">136</span></a>]</cite> and are now available in open-source libraries, such as libsnark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib307" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">307</span></a>]</cite>, and deployed in cryptocurrencies, such as Zcash <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">62</span></a>]</cite>. Note that SNARK systems usually require overhead on the part of the prover; in particular, the prover computation needs to be superlinear in the size of the circuit for the statement being proven. Recently, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie et al.</span> [<a href="#bib.bib489" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">489</span></a>]</cite> presented Libra, a ZKP system that achieves linear prover complexity but with increased proof size and verification time.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p8" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p8.1" class="ltx_p">If we relax the requirements for succinctness or non-interactiveness for the construction, there is a large body of constructions that achieve a wide range of efficiency trade-offs, avoid the trusted setup requirement and use more standard cryptographic assumptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>, <a href="#bib.bib464" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">464</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>, <a href="#bib.bib63" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">63</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS3.Px1.p9" class="ltx_para">
<p id="S4.SS2.SSS3.Px1.p9.1" class="ltx_p">In the recent years, an increasing numbers of practical applications have been using non-interactive zero-knowledge proofs, primarily motivated by blockchains. Using interactive ZKP systems and NIZKs efficiently in the context of FL remains a challenging open question. In such a setting, NIZKs may enable to prove to the server properties about the client’s inputs. In the setting where the verifier is the client, it will be challenging to create a trustworthy statement to verify as it involves input from other clients. Of interest in this setting, recent work enables to handle the case where the multiple verifiers have shares of the statement <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">83</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Trusted execution environment and remote attestation</h5>

<div id="S4.SS2.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p1.1" class="ltx_p">We discussed TEEs in <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.1</span></a>, but focus here on the fact that TEEs may provide opportunities to provide verifiable computations. Indeed, TEEs enable to attest and verify the code (binary) running in its environment. In particular, when the verifier knows (or can reproduce) which binary should run in the secure enclaves, TEEs will be able to provide a notion of <em id="S4.SS2.SSS3.Px2.p1.1.1" class="ltx_emph ltx_font_italic">integrity</em> (the code execution cannot be affected, except by the inputs), and an <em id="S4.SS2.SSS3.Px2.p1.1.2" class="ltx_emph ltx_font_italic">attestation</em> (the TEE can prove that a specific binary is executing and what is starting state was) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib437" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">437</span></a>, <a href="#bib.bib451" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">451</span></a>]</cite>. More generally, remote attestation allows a verifier to securely measure the internal state of a remote hardware platform, and can be used to establish a static or dynamic root of trust. While TEEs enable hardware-based remote attestations, both software-based remote attestions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib411" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">411</span></a>]</cite> and hybrid remote attestation designs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib172" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">172</span></a>, <a href="#bib.bib274" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">274</span></a>]</cite> were proposed in the literature and enable to trade off hardware requirements for verifiability.</p>
</div>
<div id="S4.SS2.SSS3.Px2.p2" class="ltx_para">
<p id="S4.SS2.SSS3.Px2.p2.1" class="ltx_p">In a federated learning setting, TEEs and remote attestations may be particularly helpful for clients to be able to efficiently verify key functions running on the server. For example, secure aggregation or shuffling could run in TEEs and would provide differential privacy guarantees on their outputs. Therefore, the post-processing logic subsequently applied by the server on the differentially private data could run on the server and remain oblivious to the clients. Note that such a system design requires the clients to know and trust the exact code (binary) for the key functions to be applied in the enclaves. Additionally, remote attestations may enable a server to attest specific requirements from the clients involved in the FL computation, such as absence of leaks, immutability, and uninterruptability (we defer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">188</span></a>]</cite> for an exhaustive list of minimal requirements for remote attestation).</p>
</div>
</section>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Protections Against External Malicious Actors</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we assume the existence of a trusted server and discuss various challenges and open problems towards achieving rigorous privacy guarantees against external malicious actors (e.g. adversarial clients, adversarial analysts, adversarial devices that consume the learned model, or any combination thereof).</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">As discussed in Table <a href="#S4.T6" title="Table 6 ‣ 4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, malicious clients can inspect all messages received from the server (including the model iterates) in the rounds they participate in, malicious analysts can inspect sequences of model iterates from multiple training runs with different hyperparameters, and in cross-device FL, malicious devices can have either white-box or black-box access to the final model. Therefore, to give rigorous protections against external adversaries, it is important to first consider what can be learned from the intermediate iterates and final model.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Auditing the Iterates and Final Model</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To better understand what can be learned from the intermediate iterates or final model, we propose quantifying federated learning models’ susceptibility towards specific attacks.
This is a particularly interesting problem in the federated learning context. On the one hand, adversaries receive direct access to the model from the server, which widens the attack surface.
On the other hand, the server determines which specific stages of the training process the adversary will receive access to the model, and additionally controls the adversary’s influence over the model at each of the stages.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">For classic (non-federated) models of computation, understanding a model’s susceptibility to attacks is an active and challenging research area <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">189</span></a>, <a href="#bib.bib418" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">418</span></a>, <a href="#bib.bib99" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">99</span></a>, <a href="#bib.bib341" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">341</span></a>, <a href="#bib.bib100" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">100</span></a>]</cite>.
The most common method of quantifying a model’s susceptibility to an attack is to simulate the attack on the model using a proxy (auditing) dataset similar to the dataset expected in practice.
This gives an idea of what the model’s <span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_italic">expected</span> attack susceptibility is <span id="S4.SS3.SSS1.p2.1.2" class="ltx_text ltx_font_italic">if</span> the proxy dataset is indeed similar to the eventual user data. A safer method would be to determine a worst-case upper-bound on the model’s attack susceptibility. This can be approached theoretically as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib496" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">496</span></a>]</cite>, although this often yields loose, vacuous bounds for realistic models. Empirical approaches may be able to provide tighter bounds, but for many types of attacks and models, this endeavour may be intractable. An interesting emerging area of research in this space examines the theoretic conditions (on the audited model and attacks) under which an unsuccessful attempt to identify privacy violations by a simulated attack implies that no stronger attacks can succeed at such a task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib153" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">153</span></a>]</cite>. However, this area is still nascent and more work needs to be done to better understand the fundamental requirements under which auditing (via simulated attacks) is sufficient.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">The federated learning framework provides a unique setting not only for attacks, but also for attack quantification and defense. Specifically, due to the server’s control over when each user can access and influence the model during the training process, it may be possible to design new tractable methods for quantifying a model’s average-case or worst-case attack susceptibility. Such methods would enable the development of new adaptive defenses, which can be applied on-the-fly to preempt significant adversarial influence while maximizing utility.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Training with Central Differential Privacy</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">To limit or eliminate the information that could be learned about an individual from the iterates (and/or final model), user-level differential privacy can be used in FL’s iterative training process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>, <a href="#bib.bib336" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">336</span></a>, <a href="#bib.bib68" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">68</span></a>]</cite>. With this technique, the server clips the <math id="S4.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS3.SSS2.p1.1.m1.1a"><msub id="S4.SS3.SSS2.p1.1.m1.1.1" xref="S4.SS3.SSS2.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS3.SSS2.p1.1.m1.1.1.2" xref="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S4.SS3.SSS2.p1.1.m1.1.1.3" xref="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p1.1.m1.1b"><apply id="S4.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.2">ℓ</ci><cn type="integer" id="S4.SS3.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS2.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p1.1.m1.1c">\ell_{2}</annotation></semantics></math> norm of individual updates, aggregates the clipped updates, and then adds Gaussian noise to the aggregate. This ensures that the iterates do not overfit to any individual user’s update. To track the overall privacy budget across rounds, advanced composition theorems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">168</span></a>, <a href="#bib.bib254" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">254</span></a>]</cite> or the analytical moments accountant method developed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib346" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">346</span></a>, <a href="#bib.bib348" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">348</span></a>, <a href="#bib.bib474" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">474</span></a>]</cite> can be used. The moments accountant method works particularly well with the uniformly subsampled Gaussian mechanism. For moderate privacy budgets and in the absence of a sufficiently large dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib384" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">384</span></a>]</cite>, the noise introduced by this process can lead to a large decrease in model accuracy. Prior work has explored a number of avenues to mitigate this trade-off between privacy and accuracy, including collecting more private data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib338" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">338</span></a>]</cite>, designing privacy-friendly model architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib367" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">367</span></a>]</cite>, or leveraging priors on the private data domain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib449" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">449</span></a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">In cross-device FL, the number of training examples can vary drastically from one device to the other.
Hence, similar to recent works on user-level DP in the central model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite>, figuring out how to adaptively bound the contributions of users and clip the model parameters remains an interesting research direction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib446" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">446</span></a>, <a href="#bib.bib377" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">377</span></a>]</cite>. More broadly, unlike record-level DP where fundamental trade-offs between accuracy and privacy are well understood for a variety of canonical learning and estimation tasks, user-level DP is fundamentally less understood (especially when the number of contributions varies wildly across users and is not tightly bounded <span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_italic">a priori</span>). Thus, more work needs to be done to better understand the fundamental trade-offs in this emerging setting of DP. Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib320" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">320</span></a>]</cite> made progress on this front by characterizing the trade-offs between accuracy and privacy for learning discrete distributions under user-level DP.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">In addition to the above, it is important to draw a distinction between malicious clients that may be able to see (some of) the intermediate iterates during training and malicious analysts (or deployments) that can only see the final model. Even though central DP provides protections against both threat models, a careful theoretical analysis can reveal that for a specific implementation of the above Gaussian mechanism (or any other differentially private mechanism), we may get different privacy parameters for these two threat models. Naturally, we should get stronger differential privacy guarantees with respect to malicious analysts than we do with respect to malicious clients (because malicious clients may have access to far more information than malicious analysts). This “privacy amplification via iteration” setting has been recently studied by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Feldman et al.</span> [<a href="#bib.bib185" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">185</span></a>]</cite> for convex optimization problems. However, it is unclear whether or not the results in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">185</span></a>]</cite> can be carried over to the non-convex setting.</p>
</div>
<section id="S4.SS3.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy amplification for non-uniform device sampling procedures</h5>

<div id="S4.SS3.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px1.p1.2" class="ltx_p">Providing formal <math id="S4.SS3.SSS2.Px1.p1.1.m1.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS3.SSS2.Px1.p1.1.m1.2a"><mrow id="S4.SS3.SSS2.Px1.p1.1.m1.2.3.2" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.3.1.cmml"><mo stretchy="false" id="S4.SS3.SSS2.Px1.p1.1.m1.2.3.2.1" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.3.1.cmml">(</mo><mi id="S4.SS3.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS3.SSS2.Px1.p1.1.m1.1.1.cmml">ε</mi><mo id="S4.SS3.SSS2.Px1.p1.1.m1.2.3.2.2" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.3.1.cmml">,</mo><mi id="S4.SS3.SSS2.Px1.p1.1.m1.2.2" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS3.SSS2.Px1.p1.1.m1.2.3.2.3" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.Px1.p1.1.m1.2b"><interval closure="open" id="S4.SS3.SSS2.Px1.p1.1.m1.2.3.1.cmml" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.3.2"><ci id="S4.SS3.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS2.Px1.p1.1.m1.1.1">𝜀</ci><ci id="S4.SS3.SSS2.Px1.p1.1.m1.2.2.cmml" xref="S4.SS3.SSS2.Px1.p1.1.m1.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.Px1.p1.1.m1.2c">(\varepsilon,\delta)</annotation></semantics></math> guarantees in the context of cross-device FL system can be particularly challenging because: (a) the set of all eligible users (i.e. underlying database) is dynamic and not known in advance, and (b) users participating in federate computations may drop out at any point in the protocol. It is therefore important to investigate and design protocols that: (1) are robust to nature’s choice (user availability and dropout), (2) are self-accounting, in that the server can compute a tight <math id="S4.SS3.SSS2.Px1.p1.2.m2.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS3.SSS2.Px1.p1.2.m2.2a"><mrow id="S4.SS3.SSS2.Px1.p1.2.m2.2.3.2" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.3.1.cmml"><mo stretchy="false" id="S4.SS3.SSS2.Px1.p1.2.m2.2.3.2.1" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.3.1.cmml">(</mo><mi id="S4.SS3.SSS2.Px1.p1.2.m2.1.1" xref="S4.SS3.SSS2.Px1.p1.2.m2.1.1.cmml">ε</mi><mo id="S4.SS3.SSS2.Px1.p1.2.m2.2.3.2.2" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.3.1.cmml">,</mo><mi id="S4.SS3.SSS2.Px1.p1.2.m2.2.2" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS3.SSS2.Px1.p1.2.m2.2.3.2.3" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.Px1.p1.2.m2.2b"><interval closure="open" id="S4.SS3.SSS2.Px1.p1.2.m2.2.3.1.cmml" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.3.2"><ci id="S4.SS3.SSS2.Px1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS2.Px1.p1.2.m2.1.1">𝜀</ci><ci id="S4.SS3.SSS2.Px1.p1.2.m2.2.2.cmml" xref="S4.SS3.SSS2.Px1.p1.2.m2.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.Px1.p1.2.m2.2c">(\varepsilon,\delta)</annotation></semantics></math> guarantee using only information available via the protocol, (3) rely on local participation decision (i.e. do not assume that the server knows which users are online and has the ability to sample from them), and (4) achieve good privacy-utility trade-offs. While recent works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib257" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">257</span></a>]</cite> suggest that these constraints can be simultaneously achieved, building an end-to-end protocol that works in production FL systems is still an important open problem.</p>
</div>
</section>
<section id="S4.SS3.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Sources of randomness (adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib336" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">336</span></a>]</cite>)</h5>

<div id="S4.SS3.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p1.1" class="ltx_p">Most computational devices have access only to few sources of entropy and they tend to be very low rate (hardware interrupts, on-board sensors). It is standard—and theoretically well justified—to use the entropy to seed a cryptographically secure pseudo-random number generator (PRNG) and use the PRNG’s output as needed. Robust and efficient PRNGs based on standard cryptographic primitives exist that have output rate of gigabytes per second on modern CPUs and require a seed as short as 128 bits <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib401" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">401</span></a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS2.Px2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p2.2" class="ltx_p">The output distribution of a randomized algorithm <math id="S4.SS3.SSS2.Px2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS3.SSS2.Px2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.SSS2.Px2.p2.1.m1.1.1" xref="S4.SS3.SSS2.Px2.p2.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.Px2.p2.1.m1.1b"><ci id="S4.SS3.SSS2.Px2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.Px2.p2.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.Px2.p2.1.m1.1c">\mathcal{A}</annotation></semantics></math> with access to a PRNG is indistinguishable from the output distribution of <math id="S4.SS3.SSS2.Px2.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS3.SSS2.Px2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.SSS2.Px2.p2.2.m2.1.1" xref="S4.SS3.SSS2.Px2.p2.2.m2.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.Px2.p2.2.m2.1b"><ci id="S4.SS3.SSS2.Px2.p2.2.m2.1.1.cmml" xref="S4.SS3.SSS2.Px2.p2.2.m2.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.Px2.p2.2.m2.1c">\mathcal{A}</annotation></semantics></math> with access to a true source of entropy <em id="S4.SS3.SSS2.Px2.p2.2.1" class="ltx_emph ltx_font_italic">as long as the distinguisher is computationally bounded</em>. Compare it with the guarantee of differential privacy which holds against any adversary, no matter how powerful. As such, virtually all implementations of differential privacy satisfy only (variants of) computational differential privacy introduced by <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib347" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">347</span></a>]</cite>. On the positive side, a computationally-bounded adversary cannot tell the difference, which allows us to avoid being overly pedantic about this point.</p>
</div>
<div id="S4.SS3.SSS2.Px2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.Px2.p3.1" class="ltx_p">A training procedure may have multiple sources of non-determinism (e.g., dropout layers or an input of a generative model) but only those that are reflected in the privacy ledger must come from a cryptographically secure PRNG. In particular, the device sampling procedure and the additive Gaussian noise must be drawn from a cryptographically secure PRNG for the trained model to satisfy computational differential privacy.</p>
</div>
</section>
<section id="S4.SS3.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Auditing differential privacy implementations</h5>

<div id="S4.SS3.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS3.SSS2.Px3.p1.1" class="ltx_p">Privacy and security protocols are notoriously difficult to implement correctly (e.g.,  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib345" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">345</span></a>, <a href="#bib.bib217" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">217</span></a>]</cite> for differential privacy). What techniques can be used for testing FL-implementations for correctness? Since the techniques will often be deployed by organizations who may opt not to open-source code, what are the possibilities for black-box testing? Some works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">156</span></a>, <a href="#bib.bib315" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">315</span></a>, <a href="#bib.bib247" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">247</span></a>]</cite> begin to explore this area in the context of differential privacy, but many open questions remain.</p>
</div>
</section>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Concealing the Iterates</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In typical federated learning systems, the model iterates (i.e., the newly updated versions of the model after each round of training) are assumed to be visible to multiple actors in the system, including the server and the clients that are chosen to participate in each round. However, it may be possible to use tools from <a href="#S4.SS2" title="4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> to keep the iterates concealed from these actors.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">To conceal the iterates from the clients, each client could run their local portion of federated learning inside a TEE providing confidentiality features (see <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.1</span></a>). The server would validate that the expected federated learning code is running in the TEE (relying on the TEE’s attestation and integrity features), then transmit an encrypted model iterate to the device such that it can only be decrypted inside the TEE. Finally the model updates would be encrypted inside the TEE before being returned to the server, using keys only known inside the enclave and on the server. Unfortunately, TEEs may not be generally available across clients, especially when those clients are end-user devices such as smartphones. Moreover, even when TEEs are present, they may not be sufficiently powerful to support training computations, which would have to happen inside the TEE in order to protect the model iterate, and may be computationally expensive and/or require significant amounts of RAM – though TEE capabilities are likely to improve over time, and techniques such as those presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib447" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">447</span></a>]</cite> may be able to reduce the requirements on the TEE by exporting portions of the computation outside the TEE while maintaining the attestation, integrity, and confidentiality needs of the computation as a whole.</p>
</div>
<div id="S4.SS3.SSS3.p3" class="ltx_para">
<p id="S4.SS3.SSS3.p3.1" class="ltx_p">Similar protections can be achieved under the MPC model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib351" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">351</span></a>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>. For example, the server could encrypt the iterate’s model parameters under a homomorphic encryption scheme before sending it to the client, using keys known only to the server. The client could then compute the encrypted model update using the homomorphic properties of the cryptosystem, without needing to decrypt the model parameters. The encrypted model update could then be returned to the server for aggregation. A key challenge here will be to force aggregation on the server before decryption, as otherwise the server may be able to learn a client’s model update. Another challenging open problem here is improving performance, as even state-of-the-art systems can require quite significant computational resources to complete a single round of training in a deep neural network. Progress here could be made both by algorithmic advances as well as through the development of more efficient hardware accelerators for MPC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib393" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">393</span></a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS3.p4" class="ltx_para">
<p id="S4.SS3.SSS3.p4.1" class="ltx_p">Additional challenges arise if the model iterates should also be concealed from the server. Under the TEE model, the server portion of federated learning could run inside a TEE, with all parties (i.e., clients and analyst) verifying that the server TEE will only release the final model after the appropriate training criteria have been met. Under the MPC model, an encryption key could protect the model iterates, with the key held by the analyst, distributed in shares among the clients, or held by a trusted third party; in this setup, the key holder(s) would be required to engage in the decryption of the model parameters, and could thereby ensure that this process happens only once.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Repeated Analyses over Evolving Data</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">For many applications of federated learning, the analyst wishes to analyze data that arrive in a streaming fashion, and must also provide dynamically-updated learned models that are (1) correct on the data seen thus far, and (2) accurately predict future data arrivals. In the absence of privacy concerns, the analyst could simply re-train the learned model once new data arrive, to ensure maximum accuracy at all times. However, since privacy guarantees degrade as additional information is published about the same data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">167</span></a>, <a href="#bib.bib168" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">168</span></a>]</cite>, these updates must be less frequent to still preserve both privacy and accuracy of the overall analysis.</p>
</div>
<div id="S4.SS3.SSS4.p2" class="ltx_para">
<p id="S4.SS3.SSS4.p2.1" class="ltx_p">Recent advances in differential privacy for dynamic databases and time series data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">143</span></a>, <a href="#bib.bib142" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">142</span></a>, <a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite> have all assumed the existence of a trusted curator who can see raw data as they arrive online, and publish dynamically updated statistics. An open question is how these algorithmic techniques can be extended to the federated setting, to enable private federated learning on time series data or other dynamically evolving databases.</p>
</div>
<div id="S4.SS3.SSS4.p3" class="ltx_para">
<p id="S4.SS3.SSS4.p3.1" class="ltx_p">Specific open questions include:</p>
<ul id="S4.I4" class="ltx_itemize">
<li id="S4.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i1.p1" class="ltx_para">
<p id="S4.I4.i1.p1.4" class="ltx_p">How should an analyst privately update an FL model in the presence of new data? Alternatively, how well would a model that was learned privately with FL on a dataset <math id="S4.I4.i1.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.I4.i1.p1.1.m1.1a"><mi id="S4.I4.i1.p1.1.m1.1.1" xref="S4.I4.i1.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.1.m1.1b"><ci id="S4.I4.i1.p1.1.m1.1.1.cmml" xref="S4.I4.i1.p1.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.1.m1.1c">D</annotation></semantics></math> extend to a dataset <math id="S4.I4.i1.p1.2.m2.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.I4.i1.p1.2.m2.1a"><msup id="S4.I4.i1.p1.2.m2.1.1" xref="S4.I4.i1.p1.2.m2.1.1.cmml"><mi id="S4.I4.i1.p1.2.m2.1.1.2" xref="S4.I4.i1.p1.2.m2.1.1.2.cmml">D</mi><mo id="S4.I4.i1.p1.2.m2.1.1.3" xref="S4.I4.i1.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.2.m2.1b"><apply id="S4.I4.i1.p1.2.m2.1.1.cmml" xref="S4.I4.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.I4.i1.p1.2.m2.1.1.1.cmml" xref="S4.I4.i1.p1.2.m2.1.1">superscript</csymbol><ci id="S4.I4.i1.p1.2.m2.1.1.2.cmml" xref="S4.I4.i1.p1.2.m2.1.1.2">𝐷</ci><ci id="S4.I4.i1.p1.2.m2.1.1.3.cmml" xref="S4.I4.i1.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.2.m2.1c">D^{\prime}</annotation></semantics></math> that was guaranteed to be similar to <math id="S4.I4.i1.p1.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.I4.i1.p1.3.m3.1a"><mi id="S4.I4.i1.p1.3.m3.1.1" xref="S4.I4.i1.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.3.m3.1b"><ci id="S4.I4.i1.p1.3.m3.1.1.cmml" xref="S4.I4.i1.p1.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.3.m3.1c">D</annotation></semantics></math> in a given closeness measure? Since FL already occurs on samples that arrive online and does not overfit to the data it sees, it is likely that such a model would still continue to perform well on a new database <math id="S4.I4.i1.p1.4.m4.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="S4.I4.i1.p1.4.m4.1a"><msup id="S4.I4.i1.p1.4.m4.1.1" xref="S4.I4.i1.p1.4.m4.1.1.cmml"><mi id="S4.I4.i1.p1.4.m4.1.1.2" xref="S4.I4.i1.p1.4.m4.1.1.2.cmml">D</mi><mo id="S4.I4.i1.p1.4.m4.1.1.3" xref="S4.I4.i1.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I4.i1.p1.4.m4.1b"><apply id="S4.I4.i1.p1.4.m4.1.1.cmml" xref="S4.I4.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I4.i1.p1.4.m4.1.1.1.cmml" xref="S4.I4.i1.p1.4.m4.1.1">superscript</csymbol><ci id="S4.I4.i1.p1.4.m4.1.1.2.cmml" xref="S4.I4.i1.p1.4.m4.1.1.2">𝐷</ci><ci id="S4.I4.i1.p1.4.m4.1.1.3.cmml" xref="S4.I4.i1.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I4.i1.p1.4.m4.1c">D^{\prime}</annotation></semantics></math>.
This is also related to questions of robustness that are explored in Section <a href="#S5" title="5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</li>
<li id="S4.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i2.p1" class="ltx_para">
<p id="S4.I4.i2.p1.1" class="ltx_p">One way around the issue of privacy composition is by producing synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib165" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">165</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, which can then be used indefinitely without incurring additional privacy loss. This follows from the post-processing guarantees of differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">167</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Augenstein et al.</span> [<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> explore the generation of synthetic data in a federated fashion. In the dynamic data setting, synthetic data can be used repeatedly until it has become “outdated” with respect to new data, and must be updated. Even after generating data in a federated fashion, it must also be updated privately and federatedly.</p>
</div>
</li>
<li id="S4.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i3.p1" class="ltx_para">
<p id="S4.I4.i3.p1.1" class="ltx_p">Can the specific approaches in prior work on differential privacy for dynamic databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">142</span></a>]</cite> or privately detecting changes in time series data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">143</span></a>, <a href="#bib.bib97" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">97</span></a>]</cite> be extended to the federated setting?</p>
</div>
</li>
<li id="S4.I4.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I4.i4.p1" class="ltx_para">
<p id="S4.I4.i4.p1.1" class="ltx_p">How can time series data be queried in a federated model in the first place? By design, the same users are not regularly queried multiple times for updated data points, so it is difficult to collect true within-subject estimates of an individuals’ data evolution over time. Common tools for statistical sampling of time series data may be brought to bear here, but must be used in conjunction with tools for privacy and tools for federation. Other approaches include reformulating the queries such that each within-subject subquery can be answered entirely on device.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.5 </span>Preventing Model Theft and Misuse</h4>

<div id="S4.SS3.SSS5.p1" class="ltx_para">
<p id="S4.SS3.SSS5.p1.1" class="ltx_p">In some cases, the actor or organization developing an ML model may be motivated to restrict the ability to inspect, misuse or steal the model. For example, restricting access to the model’s parameters may make it more difficult for an adversary to search for vulnerabilities, such as inputs that produce unanticipated model outputs.</p>
</div>
<div id="S4.SS3.SSS5.p2" class="ltx_para">
<p id="S4.SS3.SSS5.p2.1" class="ltx_p">Protecting a deployed model during inference is closely related to the challenge of concealing the model iterates from clients during training, as discussed in <a href="#S4.SS3.SSS3" title="4.3.3 Concealing the Iterates ‣ 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.3</span></a>. Again, both TEEs and MPC may be used. Under the TEE model, the model parameters are only accessible to a TEE on the device, as in <a href="#S4.SS3.SSS3" title="4.3.3 Concealing the Iterates ‣ 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3.3</span></a>; the primary difference being that the desired calculation is now inference instead of training.</p>
</div>
<div id="S4.SS3.SSS5.p3" class="ltx_para">
<p id="S4.SS3.SSS5.p3.1" class="ltx_p">It is harder to adapt MPC strategies to this use case without forgoing the advantages offered by on-device inference: if the user data, model parameters, and inference results are all intended to be on-device, then it is unclear what additional party is participating in the multi-party computation. For example, naïvely attempting to use homomorphic encryption would require the decryption keys to be on device where the inferences are to be used, thereby undermining the value of the encryption in the first place. Solutions where the analyst is required to participate (e.g. holding either the encryption keys or the model parameters themselves) imply additional inference latency, bandwidth costs, and connectivity requirements for the end user (e.g. the inferences would no longer be available for a device in airplane mode).</p>
</div>
<div id="S4.SS3.SSS5.p4" class="ltx_para">
<p id="S4.SS3.SSS5.p4.1" class="ltx_p">It is crucial to note that even if the model parameters themselves are successfully hidden, research has shown that in many cases they can be reconstructed by an adversary who only has access to an inference/prediction API based on those parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib450" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">450</span></a>]</cite>. It is an open question what additional protections would need to be put into place to protect from these kinds of issues in the context of a model residing on millions or billions of end user devices.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Protections Against an Adversarial Server</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In the previous section, we assumed the existence of a trusted server that can orchestrate the training process. In this section we discuss the more desirable scenario of protecting against an adversarial server. In particular, we start by investigating the challenges of this setting and existing works, and then move on to describing the open problems and how the techniques discussed in Section <a href="#S4.SS2" title="4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a> can be used to address these challenges.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Challenges: Communication Channels, Sybil Attacks, and Selection</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In the cross-device FL setting, we have a server with significant computational resources and a large number of clients that (i) can only communicate with the server (as in a star network topology), and (ii) may be limited in connectivity and bandwidth. This poses very concrete requirements when enforcing a given trust model. In particular, clients do not have a clear way of establishing secure channels among themselves independent of the server. This suggests, as shown by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Reyzin et al.</span> [<a href="#bib.bib392" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">392</span></a>]</cite> for practical settings, that assuming honest (or at least semi-honest) behaviour by the server in a key distribution phase (as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>) is required in scenarios where private channels among clients are needed. This includes cryptographic solutions based on MPC techniques. An alternative to this assumption would be incorporating an additional party or a public bulletin board (see, e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib398" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">398</span></a>]</cite>) into the model that is known to the clients and trusted to not collude with the server.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.1" class="ltx_p">Beyond trusting the server to facilitate private communication channels, the participants in cross-device FL must also trust the server to form cohorts of clients in a fair and honest manner. An actively malicious adversary controlling the server could simulate a large number of fake client devices (a “Sybil attack” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib160" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">160</span></a>]</cite>) or could preferentially select previously compromised devices from the pool of available devices. Either way, the adversary could control far more participants in a round of FL than would be expected simply from a base rate of adversarial devices in the population. This would make it far easier to break the common assumption in MPC that at least a certain fraction of the devices are honest, thereby undermining the security of the protocol. Even if the security of protocol itself remains intact (for example, if its security is rooted in a different source of trust, such as a secure enclave), there is a risk that if a large number of adversarial clients’ model updates are known to or controlled by the adversary, then the privacy of the remaining clients’ updates may be undermined. Note that these concerns can also apply in the context of TEEs. For example, a TEE-based shuffler can also be subject to a Sybil attack; if a single honest user’s input is shuffled with known inputs from fake users, it will be straight forward for the adversary to identify the honest user’s value in the shuffled output.</p>
</div>
<div id="S4.SS4.SSS1.p3" class="ltx_para">
<p id="S4.SS4.SSS1.p3.1" class="ltx_p">Note that in some cases, it may be possible to establish proof among the clients in a round that they are all executing the correct protocol, such as if secure enclaves are available on client devices and the clients are able to remotely attest one another. In these cases, it may be possible to establish privacy for all honest participants in the round (e.g., by attesting that secure multi-party computation protocols were followed accurately, that distributed differential privacy contributions were added secretly and correctly, etc.) even if the model updates themselves are known to or controlled by the adversary.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Limitations of Existing Solutions</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Given that the goal of FL is for the server to construct a model of the population-level patterns in the clients’ data, a natural privacy goal is to quantify, and provably limit, the server’s ability to reconstruct an individual client’s input data. This involves formally defining (a) what is the view of the clients data revealed to the server as a result of an FL execution, and (b) what is the privacy leakage of such a view. In FL, we are particularly interested in guaranteeing that the server can aggregate reports from the clients, while somehow masking the contributions of each individual client. As discussed in Section <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, this can be done in a variety of ways, typically using some notion of differential privacy. There are a wide variety of such methods, each with their own weaknesses, especially in FL. For example, as already discussed, central DP suffers from the need to have access to a trusted central server. This has led to other promising private disclosure methods discussed in Section <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>. Here, we outline some of the weaknesses of these methods.</p>
</div>
<section id="S4.SS4.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Local differential privacy</h5>

<div id="S4.SS4.SSS2.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS2.Px1.p1.1" class="ltx_p">As previously discussed, LDP removes the need for a trusted central server by having each client perform a differentially private transformation to their report before sending it to the central server. LDP assumes that a user’s privacy comes solely from that user’s addition of their own randomness; thus, a user’s privacy guarantee is independent of the additional randomness incorporated by all other users.
While LDP protocols are effective at enforcing privacy and have theoretical justifications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">177</span></a>, <a href="#bib.bib154" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">154</span></a>, <a href="#bib.bib155" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">155</span></a>]</cite>, a number of results have shown that achieving local differential privacy while preserving utility is challenging, especially in high-dimensional data settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib266" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">266</span></a>, <a href="#bib.bib455" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">455</span></a>, <a href="#bib.bib252" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">252</span></a>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a>, <a href="#bib.bib253" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">253</span></a>, <a href="#bib.bib495" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">495</span></a>, <a href="#bib.bib162" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">162</span></a>, <a href="#bib.bib128" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">128</span></a>]</cite>.
Part of this difficulty is attributed to the fact that the magnitude of the random noise introduced must be comparable to the magnitude of the signal in the data, which may require combining reports between clients. Therefore, obtaining utility with LDP comparable to that in the central setting requires a relatively larger userbase or larger choice of <math id="S4.SS4.SSS2.Px1.p1.1.m1.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS4.SSS2.Px1.p1.1.m1.1a"><mi id="S4.SS4.SSS2.Px1.p1.1.m1.1.1" xref="S4.SS4.SSS2.Px1.p1.1.m1.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.Px1.p1.1.m1.1b"><ci id="S4.SS4.SSS2.Px1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS2.Px1.p1.1.m1.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.Px1.p1.1.m1.1c">\varepsilon</annotation></semantics></math> parameter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib445" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">445</span></a>]</cite>.</p>
</div>
</section>
<section id="S4.SS4.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Hybrid differential privacy</h5>

<div id="S4.SS4.SSS2.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.Px2.p1.1" class="ltx_p">The hybrid model for differential privacy can help reduce the size of the required userbase by partitioning users based on their trust preferences. However, it is unclear which application areas and algorithms can best utilize hybrid trust model data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>. Furthermore, current work on the hybrid model typically assumes that regardless of the user trust preference, their data comes from the same distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>, <a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">57</span></a>]</cite>. Relaxing this assumption is critical for FL in particular, as the relationship between the trust preference and actual user data may be non-trivial.</p>
</div>
</section>
<section id="S4.SS4.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The shuffle model</h5>

<div id="S4.SS4.SSS2.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS2.Px3.p1.1" class="ltx_p">The shuffle model enables users’ locally-added noise to be amplified through a shuffling intermediary, although it comes with two drawbacks of its own.
The first is the requirement of a trusted intermediary; if users are already not trusting of the curator, then it may be unlikely that they will trust an intermediary approved of or created by the curator (though TEEs might help to bridge this gap). The Prochlo framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">73</span></a>]</cite> is (to the best of our knowledge) the only existing instance.
The second drawback is that the shuffle model’s differential privacy guarantee degrades in proportion to the number of adversarial users participating in the computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>.
Since this number isn’t known to the users or the curator, it introduces uncertainty into the true level of privacy that users are receiving.
This risk is particularly important in the context of federated learning, since users (who are potentially adversarial) are a key component in the computational pipeline.
Secure multi-party computation, in addition to adding significant computation and communication overhead to each user, also does not address this risk when users are adding their own noise locally.</p>
</div>
</section>
<section id="S4.SS4.SSS2.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Secure aggregation</h5>

<div id="S4.SS4.SSS2.Px4.p1" class="ltx_para">
<p id="S4.SS4.SSS2.Px4.p1.1" class="ltx_p">The Secure Aggregation protocols from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite> have strong privacy guarantees when aggregating client reports. Moreover, the protocols are tailored to the setting of federated learning. For example, they are robust to clients dropping out during the execution (a common feature of cross-device FL) and scale to a large number of parties (up to billions for <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bell et al.</span> [<a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>) and vector lengths. However, this approach has several limitations: (a) it assumes a semi-honest server (only in the private key infrastructure phase), (b) it allows the server to see the per-round aggregates (which may still leak information), (c) it is not efficient for sparse vector aggregation, and (d) it lacks the ability to enforce well-formedness of client inputs. It is an open question how to construct an efficient and robust secure aggregation protocol that addresses all of these challenges.</p>
</div>
</section>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Training with Distributed Differential Privacy</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">In the absence of a trusted server, distributed differential privacy (presented in <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2.2</span></a>) can be used to protect the privacy of participants.</p>
</div>
<section id="S4.SS4.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Communication, privacy, and accuracy trade-offs under distributed DP</h5>

<div id="S4.SS4.SSS3.Px1.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px1.p1.1" class="ltx_p">We point out that in distributed differential privacy three performance metrics are of general interest: accuracy, privacy and communication, and an important goal is nailing down the possible trade-offs between these parameters. We note that in the absence of the privacy requirement, the trade-offs between communication and accuracy have been well-studied in the literature on distributed estimation (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib440" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">440</span></a>]</cite>) and communication complexity (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib285" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">285</span></a>]</cite> for a textbook reference). On the other hand, in the centralized setup where all the users’ data is already assumed to be held by a single entity and hence no communication is required, trade-offs between accuracy and privacy have been extensively studied in central DP starting with the foundational work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">167</span></a>, <a href="#bib.bib166" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">166</span></a>]</cite>. More recently, the optimal trade-offs between privacy, communication complexity and accuracy in distributed estimation with local DP have been characterized in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">114</span></a>]</cite>, which shows that with careful encoding joint privacy and communication constraints can yield a performance that matches the optimal accuracy achievable under either constraint alone.</p>
</div>
<section id="S4.SS4.SSS3.Px1.SPx1" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Trade-offs for secure shuffling</h6>

<div id="S4.SS4.SSS3.Px1.SPx1.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px1.SPx1.p1.1" class="ltx_p">These trade-offs have been recently studied in the shuffled model for the two basic tasks of <em id="S4.SS4.SSS3.Px1.SPx1.p1.1.1" class="ltx_emph ltx_font_italic">aggregation</em> (where the goal is to compute the sum of the users’ inputs) and <em id="S4.SS4.SSS3.Px1.SPx1.p1.1.2" class="ltx_emph ltx_font_italic">frequency estimation</em> (where the inputs belong to a discrete set and the goal is to approximate the number of users holding a given element). See Tables <a href="#S4.T8" title="Table 8 ‣ Trade-offs for secure shuffling ‣ Communication, privacy, and accuracy trade-offs under distributed DP ‣ 4.4.3 Training with Distributed Differential Privacy ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> and <a href="#S4.T9" title="Table 9 ‣ Trade-offs for secure shuffling ‣ Communication, privacy, and accuracy trade-offs under distributed DP ‣ 4.4.3 Training with Distributed Differential Privacy ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> for a summary of the state-of-the-art for these two problems. Two notable open questions are (i) to study <em id="S4.SS4.SSS3.Px1.SPx1.p1.1.3" class="ltx_emph ltx_font_italic">pure</em> differential privacy in the shuffled model, and (ii) to determine the optimal privacy, accuracy and communication trade-off for <em id="S4.SS4.SSS3.Px1.SPx1.p1.1.4" class="ltx_emph ltx_font_italic">variable selection</em> in the multi-message setup (a nearly tight lower bound in the single-message case was recently obtained in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">200</span></a>]</cite>).</p>
</div>
<div id="S4.SS4.SSS3.Px1.SPx1.p2" class="ltx_para">
<p id="S4.SS4.SSS3.Px1.SPx1.p2.1" class="ltx_p">In the context of federated optimization under the shuffled model of DP, the recent work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">207</span></a>]</cite> shows that multi-message shuffling is not needed to achieve central DP accuracy with low communication cost. However, it is unclear if the schemes presented achieve the (order) optimal communication, accuracy, tradeoffs.</p>
</div>
<figure id="S4.T8" class="ltx_table">
<table id="S4.T8.20" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T8.1.1" class="ltx_tr">
<td id="S4.T8.1.1.2" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Reference</span></td>
<td id="S4.T8.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">#messages / <math id="S4.T8.1.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T8.1.1.1.1.m1.1a"><mi id="S4.T8.1.1.1.1.m1.1.1" xref="S4.T8.1.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T8.1.1.1.1.m1.1b"><ci id="S4.T8.1.1.1.1.m1.1.1.cmml" xref="S4.T8.1.1.1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.1.1.1.1.m1.1c">n</annotation></semantics></math></span></td>
<td id="S4.T8.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Message size</span></td>
<td id="S4.T8.1.1.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Expected error</span></td>
</tr>
<tr id="S4.T8.3.3" class="ltx_tr">
<td id="S4.T8.3.3.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.3.3.3.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">120</span></a><span id="S4.T8.3.3.3.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.2.2.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.2.2.1.m1.1" class="ltx_Math" alttext="\varepsilon\sqrt{n}" display="inline"><semantics id="S4.T8.2.2.1.m1.1a"><mrow id="S4.T8.2.2.1.m1.1.1" xref="S4.T8.2.2.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T8.2.2.1.m1.1.1.2" xref="S4.T8.2.2.1.m1.1.1.2.cmml">ε</mi><mo lspace="0em" rspace="0em" id="S4.T8.2.2.1.m1.1.1.1" xref="S4.T8.2.2.1.m1.1.1.1.cmml">​</mo><msqrt id="S4.T8.2.2.1.m1.1.1.3" xref="S4.T8.2.2.1.m1.1.1.3.cmml"><mi mathsize="80%" id="S4.T8.2.2.1.m1.1.1.3.2" xref="S4.T8.2.2.1.m1.1.1.3.2.cmml">n</mi></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.2.2.1.m1.1b"><apply id="S4.T8.2.2.1.m1.1.1.cmml" xref="S4.T8.2.2.1.m1.1.1"><times id="S4.T8.2.2.1.m1.1.1.1.cmml" xref="S4.T8.2.2.1.m1.1.1.1"></times><ci id="S4.T8.2.2.1.m1.1.1.2.cmml" xref="S4.T8.2.2.1.m1.1.1.2">𝜀</ci><apply id="S4.T8.2.2.1.m1.1.1.3.cmml" xref="S4.T8.2.2.1.m1.1.1.3"><root id="S4.T8.2.2.1.m1.1.1.3a.cmml" xref="S4.T8.2.2.1.m1.1.1.3"></root><ci id="S4.T8.2.2.1.m1.1.1.3.2.cmml" xref="S4.T8.2.2.1.m1.1.1.3.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.2.2.1.m1.1c">\varepsilon\sqrt{n}</annotation></semantics></math></td>
<td id="S4.T8.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.3.3.4.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S4.T8.3.3.2" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.3.3.2.m1.1" class="ltx_Math" alttext="\frac{1}{\varepsilon}\log\frac{n}{\delta}" display="inline"><semantics id="S4.T8.3.3.2.m1.1a"><mrow id="S4.T8.3.3.2.m1.1.1" xref="S4.T8.3.3.2.m1.1.1.cmml"><mfrac id="S4.T8.3.3.2.m1.1.1.2" xref="S4.T8.3.3.2.m1.1.1.2.cmml"><mn mathsize="80%" id="S4.T8.3.3.2.m1.1.1.2.2" xref="S4.T8.3.3.2.m1.1.1.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.3.3.2.m1.1.1.2.3" xref="S4.T8.3.3.2.m1.1.1.2.3.cmml">ε</mi></mfrac><mo lspace="0.167em" rspace="0em" id="S4.T8.3.3.2.m1.1.1.1" xref="S4.T8.3.3.2.m1.1.1.1.cmml">​</mo><mrow id="S4.T8.3.3.2.m1.1.1.3" xref="S4.T8.3.3.2.m1.1.1.3.cmml"><mi mathsize="80%" id="S4.T8.3.3.2.m1.1.1.3.1" xref="S4.T8.3.3.2.m1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.3.3.2.m1.1.1.3a" xref="S4.T8.3.3.2.m1.1.1.3.cmml">⁡</mo><mfrac id="S4.T8.3.3.2.m1.1.1.3.2" xref="S4.T8.3.3.2.m1.1.1.3.2.cmml"><mi mathsize="80%" id="S4.T8.3.3.2.m1.1.1.3.2.2" xref="S4.T8.3.3.2.m1.1.1.3.2.2.cmml">n</mi><mi mathsize="80%" id="S4.T8.3.3.2.m1.1.1.3.2.3" xref="S4.T8.3.3.2.m1.1.1.3.2.3.cmml">δ</mi></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.3.3.2.m1.1b"><apply id="S4.T8.3.3.2.m1.1.1.cmml" xref="S4.T8.3.3.2.m1.1.1"><times id="S4.T8.3.3.2.m1.1.1.1.cmml" xref="S4.T8.3.3.2.m1.1.1.1"></times><apply id="S4.T8.3.3.2.m1.1.1.2.cmml" xref="S4.T8.3.3.2.m1.1.1.2"><divide id="S4.T8.3.3.2.m1.1.1.2.1.cmml" xref="S4.T8.3.3.2.m1.1.1.2"></divide><cn type="integer" id="S4.T8.3.3.2.m1.1.1.2.2.cmml" xref="S4.T8.3.3.2.m1.1.1.2.2">1</cn><ci id="S4.T8.3.3.2.m1.1.1.2.3.cmml" xref="S4.T8.3.3.2.m1.1.1.2.3">𝜀</ci></apply><apply id="S4.T8.3.3.2.m1.1.1.3.cmml" xref="S4.T8.3.3.2.m1.1.1.3"><log id="S4.T8.3.3.2.m1.1.1.3.1.cmml" xref="S4.T8.3.3.2.m1.1.1.3.1"></log><apply id="S4.T8.3.3.2.m1.1.1.3.2.cmml" xref="S4.T8.3.3.2.m1.1.1.3.2"><divide id="S4.T8.3.3.2.m1.1.1.3.2.1.cmml" xref="S4.T8.3.3.2.m1.1.1.3.2"></divide><ci id="S4.T8.3.3.2.m1.1.1.3.2.2.cmml" xref="S4.T8.3.3.2.m1.1.1.3.2.2">𝑛</ci><ci id="S4.T8.3.3.2.m1.1.1.3.2.3.cmml" xref="S4.T8.3.3.2.m1.1.1.3.2.3">𝛿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.3.3.2.m1.1c">\frac{1}{\varepsilon}\log\frac{n}{\delta}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.5.5" class="ltx_tr">
<td id="S4.T8.5.5.3" class="ltx_td ltx_align_left" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.5.5.3.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib120" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">120</span></a><span id="S4.T8.5.5.3.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.4.4.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.4.4.1.m1.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S4.T8.4.4.1.m1.1a"><mi mathsize="80%" mathvariant="normal" id="S4.T8.4.4.1.m1.1.1" xref="S4.T8.4.4.1.m1.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.T8.4.4.1.m1.1b"><ci id="S4.T8.4.4.1.m1.1.1.cmml" xref="S4.T8.4.4.1.m1.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.4.4.1.m1.1c">\ell</annotation></semantics></math></td>
<td id="S4.T8.5.5.4" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><span id="S4.T8.5.5.4.1" class="ltx_text" style="font-size:80%;">1</span></td>
<td id="S4.T8.5.5.2" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.5.5.2.m1.1" class="ltx_Math" alttext="\sqrt{n}/\ell+\frac{1}{\varepsilon}\log\frac{1}{\delta}" display="inline"><semantics id="S4.T8.5.5.2.m1.1a"><mrow id="S4.T8.5.5.2.m1.1.1" xref="S4.T8.5.5.2.m1.1.1.cmml"><mrow id="S4.T8.5.5.2.m1.1.1.2" xref="S4.T8.5.5.2.m1.1.1.2.cmml"><msqrt id="S4.T8.5.5.2.m1.1.1.2.2" xref="S4.T8.5.5.2.m1.1.1.2.2.cmml"><mi mathsize="80%" id="S4.T8.5.5.2.m1.1.1.2.2.2" xref="S4.T8.5.5.2.m1.1.1.2.2.2.cmml">n</mi></msqrt><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.5.5.2.m1.1.1.2.1" xref="S4.T8.5.5.2.m1.1.1.2.1.cmml">/</mo><mi mathsize="80%" mathvariant="normal" id="S4.T8.5.5.2.m1.1.1.2.3" xref="S4.T8.5.5.2.m1.1.1.2.3.cmml">ℓ</mi></mrow><mo mathsize="80%" id="S4.T8.5.5.2.m1.1.1.1" xref="S4.T8.5.5.2.m1.1.1.1.cmml">+</mo><mrow id="S4.T8.5.5.2.m1.1.1.3" xref="S4.T8.5.5.2.m1.1.1.3.cmml"><mfrac id="S4.T8.5.5.2.m1.1.1.3.2" xref="S4.T8.5.5.2.m1.1.1.3.2.cmml"><mn mathsize="80%" id="S4.T8.5.5.2.m1.1.1.3.2.2" xref="S4.T8.5.5.2.m1.1.1.3.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.5.5.2.m1.1.1.3.2.3" xref="S4.T8.5.5.2.m1.1.1.3.2.3.cmml">ε</mi></mfrac><mo lspace="0.167em" rspace="0em" id="S4.T8.5.5.2.m1.1.1.3.1" xref="S4.T8.5.5.2.m1.1.1.3.1.cmml">​</mo><mrow id="S4.T8.5.5.2.m1.1.1.3.3" xref="S4.T8.5.5.2.m1.1.1.3.3.cmml"><mi mathsize="80%" id="S4.T8.5.5.2.m1.1.1.3.3.1" xref="S4.T8.5.5.2.m1.1.1.3.3.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.5.5.2.m1.1.1.3.3a" xref="S4.T8.5.5.2.m1.1.1.3.3.cmml">⁡</mo><mfrac id="S4.T8.5.5.2.m1.1.1.3.3.2" xref="S4.T8.5.5.2.m1.1.1.3.3.2.cmml"><mn mathsize="80%" id="S4.T8.5.5.2.m1.1.1.3.3.2.2" xref="S4.T8.5.5.2.m1.1.1.3.3.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.5.5.2.m1.1.1.3.3.2.3" xref="S4.T8.5.5.2.m1.1.1.3.3.2.3.cmml">δ</mi></mfrac></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.5.5.2.m1.1b"><apply id="S4.T8.5.5.2.m1.1.1.cmml" xref="S4.T8.5.5.2.m1.1.1"><plus id="S4.T8.5.5.2.m1.1.1.1.cmml" xref="S4.T8.5.5.2.m1.1.1.1"></plus><apply id="S4.T8.5.5.2.m1.1.1.2.cmml" xref="S4.T8.5.5.2.m1.1.1.2"><divide id="S4.T8.5.5.2.m1.1.1.2.1.cmml" xref="S4.T8.5.5.2.m1.1.1.2.1"></divide><apply id="S4.T8.5.5.2.m1.1.1.2.2.cmml" xref="S4.T8.5.5.2.m1.1.1.2.2"><root id="S4.T8.5.5.2.m1.1.1.2.2a.cmml" xref="S4.T8.5.5.2.m1.1.1.2.2"></root><ci id="S4.T8.5.5.2.m1.1.1.2.2.2.cmml" xref="S4.T8.5.5.2.m1.1.1.2.2.2">𝑛</ci></apply><ci id="S4.T8.5.5.2.m1.1.1.2.3.cmml" xref="S4.T8.5.5.2.m1.1.1.2.3">ℓ</ci></apply><apply id="S4.T8.5.5.2.m1.1.1.3.cmml" xref="S4.T8.5.5.2.m1.1.1.3"><times id="S4.T8.5.5.2.m1.1.1.3.1.cmml" xref="S4.T8.5.5.2.m1.1.1.3.1"></times><apply id="S4.T8.5.5.2.m1.1.1.3.2.cmml" xref="S4.T8.5.5.2.m1.1.1.3.2"><divide id="S4.T8.5.5.2.m1.1.1.3.2.1.cmml" xref="S4.T8.5.5.2.m1.1.1.3.2"></divide><cn type="integer" id="S4.T8.5.5.2.m1.1.1.3.2.2.cmml" xref="S4.T8.5.5.2.m1.1.1.3.2.2">1</cn><ci id="S4.T8.5.5.2.m1.1.1.3.2.3.cmml" xref="S4.T8.5.5.2.m1.1.1.3.2.3">𝜀</ci></apply><apply id="S4.T8.5.5.2.m1.1.1.3.3.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3"><log id="S4.T8.5.5.2.m1.1.1.3.3.1.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3.1"></log><apply id="S4.T8.5.5.2.m1.1.1.3.3.2.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3.2"><divide id="S4.T8.5.5.2.m1.1.1.3.3.2.1.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3.2"></divide><cn type="integer" id="S4.T8.5.5.2.m1.1.1.3.3.2.2.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3.2.2">1</cn><ci id="S4.T8.5.5.2.m1.1.1.3.3.2.3.cmml" xref="S4.T8.5.5.2.m1.1.1.3.3.2.3">𝛿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.5.5.2.m1.1c">\sqrt{n}/\ell+\frac{1}{\varepsilon}\log\frac{1}{\delta}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.8.8" class="ltx_tr">
<td id="S4.T8.8.8.4" class="ltx_td ltx_align_left" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.8.8.4.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a><span id="S4.T8.8.8.4.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.6.6.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.6.6.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.T8.6.6.1.m1.1a"><mn mathsize="80%" id="S4.T8.6.6.1.m1.1.1" xref="S4.T8.6.6.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.T8.6.6.1.m1.1b"><cn type="integer" id="S4.T8.6.6.1.m1.1.1.cmml" xref="S4.T8.6.6.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.6.6.1.m1.1c">1</annotation></semantics></math></td>
<td id="S4.T8.7.7.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.7.7.2.m1.1" class="ltx_Math" alttext="\log n" display="inline"><semantics id="S4.T8.7.7.2.m1.1a"><mrow id="S4.T8.7.7.2.m1.1.1" xref="S4.T8.7.7.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.T8.7.7.2.m1.1.1.1" xref="S4.T8.7.7.2.m1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.7.7.2.m1.1.1a" xref="S4.T8.7.7.2.m1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.7.7.2.m1.1.1.2" xref="S4.T8.7.7.2.m1.1.1.2.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.7.7.2.m1.1b"><apply id="S4.T8.7.7.2.m1.1.1.cmml" xref="S4.T8.7.7.2.m1.1.1"><log id="S4.T8.7.7.2.m1.1.1.1.cmml" xref="S4.T8.7.7.2.m1.1.1.1"></log><ci id="S4.T8.7.7.2.m1.1.1.2.cmml" xref="S4.T8.7.7.2.m1.1.1.2">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.7.7.2.m1.1c">\log n</annotation></semantics></math></td>
<td id="S4.T8.8.8.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.8.8.3.m1.2" class="ltx_Math" alttext="\frac{n^{1/6}\log^{1/3}(1/\delta)}{\varepsilon^{2/3}}" display="inline"><semantics id="S4.T8.8.8.3.m1.2a"><mfrac id="S4.T8.8.8.3.m1.2.2" xref="S4.T8.8.8.3.m1.2.2.cmml"><mrow id="S4.T8.8.8.3.m1.2.2.2" xref="S4.T8.8.8.3.m1.2.2.2.cmml"><msup id="S4.T8.8.8.3.m1.2.2.2.4" xref="S4.T8.8.8.3.m1.2.2.2.4.cmml"><mi mathsize="80%" id="S4.T8.8.8.3.m1.2.2.2.4.2" xref="S4.T8.8.8.3.m1.2.2.2.4.2.cmml">n</mi><mrow id="S4.T8.8.8.3.m1.2.2.2.4.3" xref="S4.T8.8.8.3.m1.2.2.2.4.3.cmml"><mn mathsize="80%" id="S4.T8.8.8.3.m1.2.2.2.4.3.2" xref="S4.T8.8.8.3.m1.2.2.2.4.3.2.cmml">1</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.8.8.3.m1.2.2.2.4.3.1" xref="S4.T8.8.8.3.m1.2.2.2.4.3.1.cmml">/</mo><mn mathsize="80%" id="S4.T8.8.8.3.m1.2.2.2.4.3.3" xref="S4.T8.8.8.3.m1.2.2.2.4.3.3.cmml">6</mn></mrow></msup><mo lspace="0.167em" rspace="0em" id="S4.T8.8.8.3.m1.2.2.2.3" xref="S4.T8.8.8.3.m1.2.2.2.3.cmml">​</mo><mrow id="S4.T8.8.8.3.m1.2.2.2.2.2" xref="S4.T8.8.8.3.m1.2.2.2.2.3.cmml"><msup id="S4.T8.8.8.3.m1.1.1.1.1.1.1" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.cmml"><mi mathsize="80%" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.2" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.2.cmml">log</mi><mrow id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.cmml"><mn mathsize="80%" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.2" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.1" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.1.cmml">/</mo><mn mathsize="80%" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.3" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.3.cmml">3</mn></mrow></msup><mo id="S4.T8.8.8.3.m1.2.2.2.2.2a" xref="S4.T8.8.8.3.m1.2.2.2.2.3.cmml">⁡</mo><mrow id="S4.T8.8.8.3.m1.2.2.2.2.2.2" xref="S4.T8.8.8.3.m1.2.2.2.2.3.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.2" xref="S4.T8.8.8.3.m1.2.2.2.2.3.cmml">(</mo><mrow id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.cmml"><mn mathsize="80%" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.2" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.2.cmml">1</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.1" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.1.cmml">/</mo><mi mathsize="80%" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.3" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.3.cmml">δ</mi></mrow><mo maxsize="80%" minsize="80%" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.3" xref="S4.T8.8.8.3.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><msup id="S4.T8.8.8.3.m1.2.2.4" xref="S4.T8.8.8.3.m1.2.2.4.cmml"><mi mathsize="80%" id="S4.T8.8.8.3.m1.2.2.4.2" xref="S4.T8.8.8.3.m1.2.2.4.2.cmml">ε</mi><mrow id="S4.T8.8.8.3.m1.2.2.4.3" xref="S4.T8.8.8.3.m1.2.2.4.3.cmml"><mn mathsize="80%" id="S4.T8.8.8.3.m1.2.2.4.3.2" xref="S4.T8.8.8.3.m1.2.2.4.3.2.cmml">2</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.8.8.3.m1.2.2.4.3.1" xref="S4.T8.8.8.3.m1.2.2.4.3.1.cmml">/</mo><mn mathsize="80%" id="S4.T8.8.8.3.m1.2.2.4.3.3" xref="S4.T8.8.8.3.m1.2.2.4.3.3.cmml">3</mn></mrow></msup></mfrac><annotation-xml encoding="MathML-Content" id="S4.T8.8.8.3.m1.2b"><apply id="S4.T8.8.8.3.m1.2.2.cmml" xref="S4.T8.8.8.3.m1.2.2"><divide id="S4.T8.8.8.3.m1.2.2.3.cmml" xref="S4.T8.8.8.3.m1.2.2"></divide><apply id="S4.T8.8.8.3.m1.2.2.2.cmml" xref="S4.T8.8.8.3.m1.2.2.2"><times id="S4.T8.8.8.3.m1.2.2.2.3.cmml" xref="S4.T8.8.8.3.m1.2.2.2.3"></times><apply id="S4.T8.8.8.3.m1.2.2.2.4.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.T8.8.8.3.m1.2.2.2.4.1.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4">superscript</csymbol><ci id="S4.T8.8.8.3.m1.2.2.2.4.2.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4.2">𝑛</ci><apply id="S4.T8.8.8.3.m1.2.2.2.4.3.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4.3"><divide id="S4.T8.8.8.3.m1.2.2.2.4.3.1.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4.3.1"></divide><cn type="integer" id="S4.T8.8.8.3.m1.2.2.2.4.3.2.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4.3.2">1</cn><cn type="integer" id="S4.T8.8.8.3.m1.2.2.2.4.3.3.cmml" xref="S4.T8.8.8.3.m1.2.2.2.4.3.3">6</cn></apply></apply><apply id="S4.T8.8.8.3.m1.2.2.2.2.3.cmml" xref="S4.T8.8.8.3.m1.2.2.2.2.2"><apply id="S4.T8.8.8.3.m1.1.1.1.1.1.1.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.1.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1">superscript</csymbol><log id="S4.T8.8.8.3.m1.1.1.1.1.1.1.2.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.2"></log><apply id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3"><divide id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.1"></divide><cn type="integer" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.2">1</cn><cn type="integer" id="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.T8.8.8.3.m1.1.1.1.1.1.1.3.3">3</cn></apply></apply><apply id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.cmml" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1"><divide id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.1.cmml" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.1"></divide><cn type="integer" id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.2.cmml" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.2">1</cn><ci id="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.3.cmml" xref="S4.T8.8.8.3.m1.2.2.2.2.2.2.1.3">𝛿</ci></apply></apply></apply><apply id="S4.T8.8.8.3.m1.2.2.4.cmml" xref="S4.T8.8.8.3.m1.2.2.4"><csymbol cd="ambiguous" id="S4.T8.8.8.3.m1.2.2.4.1.cmml" xref="S4.T8.8.8.3.m1.2.2.4">superscript</csymbol><ci id="S4.T8.8.8.3.m1.2.2.4.2.cmml" xref="S4.T8.8.8.3.m1.2.2.4.2">𝜀</ci><apply id="S4.T8.8.8.3.m1.2.2.4.3.cmml" xref="S4.T8.8.8.3.m1.2.2.4.3"><divide id="S4.T8.8.8.3.m1.2.2.4.3.1.cmml" xref="S4.T8.8.8.3.m1.2.2.4.3.1"></divide><cn type="integer" id="S4.T8.8.8.3.m1.2.2.4.3.2.cmml" xref="S4.T8.8.8.3.m1.2.2.4.3.2">2</cn><cn type="integer" id="S4.T8.8.8.3.m1.2.2.4.3.3.cmml" xref="S4.T8.8.8.3.m1.2.2.4.3.3">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.8.8.3.m1.2c">\frac{n^{1/6}\log^{1/3}(1/\delta)}{\varepsilon^{2/3}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.11.11" class="ltx_tr">
<td id="S4.T8.11.11.4" class="ltx_td ltx_align_left" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.11.11.4.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S4.T8.11.11.4.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.9.9.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.9.9.1.m1.2" class="ltx_Math" alttext="\log(\log n)" display="inline"><semantics id="S4.T8.9.9.1.m1.2a"><mrow id="S4.T8.9.9.1.m1.2.2.1" xref="S4.T8.9.9.1.m1.2.2.2.cmml"><mi mathsize="80%" id="S4.T8.9.9.1.m1.1.1" xref="S4.T8.9.9.1.m1.1.1.cmml">log</mi><mo id="S4.T8.9.9.1.m1.2.2.1a" xref="S4.T8.9.9.1.m1.2.2.2.cmml">⁡</mo><mrow id="S4.T8.9.9.1.m1.2.2.1.1" xref="S4.T8.9.9.1.m1.2.2.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.9.9.1.m1.2.2.1.1.2" xref="S4.T8.9.9.1.m1.2.2.2.cmml">(</mo><mrow id="S4.T8.9.9.1.m1.2.2.1.1.1" xref="S4.T8.9.9.1.m1.2.2.1.1.1.cmml"><mi mathsize="80%" id="S4.T8.9.9.1.m1.2.2.1.1.1.1" xref="S4.T8.9.9.1.m1.2.2.1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.9.9.1.m1.2.2.1.1.1a" xref="S4.T8.9.9.1.m1.2.2.1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.9.9.1.m1.2.2.1.1.1.2" xref="S4.T8.9.9.1.m1.2.2.1.1.1.2.cmml">n</mi></mrow><mo maxsize="80%" minsize="80%" id="S4.T8.9.9.1.m1.2.2.1.1.3" xref="S4.T8.9.9.1.m1.2.2.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.9.9.1.m1.2b"><apply id="S4.T8.9.9.1.m1.2.2.2.cmml" xref="S4.T8.9.9.1.m1.2.2.1"><log id="S4.T8.9.9.1.m1.1.1.cmml" xref="S4.T8.9.9.1.m1.1.1"></log><apply id="S4.T8.9.9.1.m1.2.2.1.1.1.cmml" xref="S4.T8.9.9.1.m1.2.2.1.1.1"><log id="S4.T8.9.9.1.m1.2.2.1.1.1.1.cmml" xref="S4.T8.9.9.1.m1.2.2.1.1.1.1"></log><ci id="S4.T8.9.9.1.m1.2.2.1.1.1.2.cmml" xref="S4.T8.9.9.1.m1.2.2.1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.9.9.1.m1.2c">\log(\log n)</annotation></semantics></math></td>
<td id="S4.T8.10.10.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.10.10.2.m1.1" class="ltx_Math" alttext="\log n" display="inline"><semantics id="S4.T8.10.10.2.m1.1a"><mrow id="S4.T8.10.10.2.m1.1.1" xref="S4.T8.10.10.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.T8.10.10.2.m1.1.1.1" xref="S4.T8.10.10.2.m1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.10.10.2.m1.1.1a" xref="S4.T8.10.10.2.m1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.10.10.2.m1.1.1.2" xref="S4.T8.10.10.2.m1.1.1.2.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.10.10.2.m1.1b"><apply id="S4.T8.10.10.2.m1.1.1.cmml" xref="S4.T8.10.10.2.m1.1.1"><log id="S4.T8.10.10.2.m1.1.1.1.cmml" xref="S4.T8.10.10.2.m1.1.1.1"></log><ci id="S4.T8.10.10.2.m1.1.1.2.cmml" xref="S4.T8.10.10.2.m1.1.1.2">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.10.10.2.m1.1c">\log n</annotation></semantics></math></td>
<td id="S4.T8.11.11.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.11.11.3.m1.2" class="ltx_Math" alttext="\frac{1}{\epsilon}\log(\log n)\sqrt{\log\frac{1}{\delta}}" display="inline"><semantics id="S4.T8.11.11.3.m1.2a"><mrow id="S4.T8.11.11.3.m1.2.2" xref="S4.T8.11.11.3.m1.2.2.cmml"><mfrac id="S4.T8.11.11.3.m1.2.2.3" xref="S4.T8.11.11.3.m1.2.2.3.cmml"><mn mathsize="80%" id="S4.T8.11.11.3.m1.2.2.3.2" xref="S4.T8.11.11.3.m1.2.2.3.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.11.11.3.m1.2.2.3.3" xref="S4.T8.11.11.3.m1.2.2.3.3.cmml">ϵ</mi></mfrac><mo lspace="0.167em" rspace="0em" id="S4.T8.11.11.3.m1.2.2.2" xref="S4.T8.11.11.3.m1.2.2.2.cmml">​</mo><mrow id="S4.T8.11.11.3.m1.2.2.1.1" xref="S4.T8.11.11.3.m1.2.2.1.2.cmml"><mi mathsize="80%" id="S4.T8.11.11.3.m1.1.1" xref="S4.T8.11.11.3.m1.1.1.cmml">log</mi><mo id="S4.T8.11.11.3.m1.2.2.1.1a" xref="S4.T8.11.11.3.m1.2.2.1.2.cmml">⁡</mo><mrow id="S4.T8.11.11.3.m1.2.2.1.1.1" xref="S4.T8.11.11.3.m1.2.2.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.11.11.3.m1.2.2.1.1.1.2" xref="S4.T8.11.11.3.m1.2.2.1.2.cmml">(</mo><mrow id="S4.T8.11.11.3.m1.2.2.1.1.1.1" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.cmml"><mi mathsize="80%" id="S4.T8.11.11.3.m1.2.2.1.1.1.1.1" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.11.11.3.m1.2.2.1.1.1.1a" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.11.11.3.m1.2.2.1.1.1.1.2" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.2.cmml">n</mi></mrow><mo maxsize="80%" minsize="80%" id="S4.T8.11.11.3.m1.2.2.1.1.1.3" xref="S4.T8.11.11.3.m1.2.2.1.2.cmml">)</mo></mrow></mrow><mo lspace="0em" rspace="0em" id="S4.T8.11.11.3.m1.2.2.2a" xref="S4.T8.11.11.3.m1.2.2.2.cmml">​</mo><msqrt id="S4.T8.11.11.3.m1.2.2.4" xref="S4.T8.11.11.3.m1.2.2.4.cmml"><mrow id="S4.T8.11.11.3.m1.2.2.4.2" xref="S4.T8.11.11.3.m1.2.2.4.2.cmml"><mi mathsize="80%" id="S4.T8.11.11.3.m1.2.2.4.2.1" xref="S4.T8.11.11.3.m1.2.2.4.2.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.11.11.3.m1.2.2.4.2a" xref="S4.T8.11.11.3.m1.2.2.4.2.cmml">⁡</mo><mfrac id="S4.T8.11.11.3.m1.2.2.4.2.2" xref="S4.T8.11.11.3.m1.2.2.4.2.2.cmml"><mn mathsize="80%" id="S4.T8.11.11.3.m1.2.2.4.2.2.2" xref="S4.T8.11.11.3.m1.2.2.4.2.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.11.11.3.m1.2.2.4.2.2.3" xref="S4.T8.11.11.3.m1.2.2.4.2.2.3.cmml">δ</mi></mfrac></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.11.11.3.m1.2b"><apply id="S4.T8.11.11.3.m1.2.2.cmml" xref="S4.T8.11.11.3.m1.2.2"><times id="S4.T8.11.11.3.m1.2.2.2.cmml" xref="S4.T8.11.11.3.m1.2.2.2"></times><apply id="S4.T8.11.11.3.m1.2.2.3.cmml" xref="S4.T8.11.11.3.m1.2.2.3"><divide id="S4.T8.11.11.3.m1.2.2.3.1.cmml" xref="S4.T8.11.11.3.m1.2.2.3"></divide><cn type="integer" id="S4.T8.11.11.3.m1.2.2.3.2.cmml" xref="S4.T8.11.11.3.m1.2.2.3.2">1</cn><ci id="S4.T8.11.11.3.m1.2.2.3.3.cmml" xref="S4.T8.11.11.3.m1.2.2.3.3">italic-ϵ</ci></apply><apply id="S4.T8.11.11.3.m1.2.2.1.2.cmml" xref="S4.T8.11.11.3.m1.2.2.1.1"><log id="S4.T8.11.11.3.m1.1.1.cmml" xref="S4.T8.11.11.3.m1.1.1"></log><apply id="S4.T8.11.11.3.m1.2.2.1.1.1.1.cmml" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1"><log id="S4.T8.11.11.3.m1.2.2.1.1.1.1.1.cmml" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.1"></log><ci id="S4.T8.11.11.3.m1.2.2.1.1.1.1.2.cmml" xref="S4.T8.11.11.3.m1.2.2.1.1.1.1.2">𝑛</ci></apply></apply><apply id="S4.T8.11.11.3.m1.2.2.4.cmml" xref="S4.T8.11.11.3.m1.2.2.4"><root id="S4.T8.11.11.3.m1.2.2.4a.cmml" xref="S4.T8.11.11.3.m1.2.2.4"></root><apply id="S4.T8.11.11.3.m1.2.2.4.2.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2"><log id="S4.T8.11.11.3.m1.2.2.4.2.1.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2.1"></log><apply id="S4.T8.11.11.3.m1.2.2.4.2.2.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2.2"><divide id="S4.T8.11.11.3.m1.2.2.4.2.2.1.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2.2"></divide><cn type="integer" id="S4.T8.11.11.3.m1.2.2.4.2.2.2.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2.2.2">1</cn><ci id="S4.T8.11.11.3.m1.2.2.4.2.2.3.cmml" xref="S4.T8.11.11.3.m1.2.2.4.2.2.3">𝛿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.11.11.3.m1.2c">\frac{1}{\epsilon}\log(\log n)\sqrt{\log\frac{1}{\delta}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.14.14" class="ltx_tr">
<td id="S4.T8.14.14.4" class="ltx_td ltx_align_left" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.14.14.4.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib201" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">201</span></a><span id="S4.T8.14.14.4.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.12.12.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.12.12.1.m1.2" class="ltx_Math" alttext="\log(\frac{n}{\varepsilon\delta})" display="inline"><semantics id="S4.T8.12.12.1.m1.2a"><mrow id="S4.T8.12.12.1.m1.2.3.2" xref="S4.T8.12.12.1.m1.2.3.1.cmml"><mi mathsize="80%" id="S4.T8.12.12.1.m1.1.1" xref="S4.T8.12.12.1.m1.1.1.cmml">log</mi><mo id="S4.T8.12.12.1.m1.2.3.2a" xref="S4.T8.12.12.1.m1.2.3.1.cmml">⁡</mo><mrow id="S4.T8.12.12.1.m1.2.3.2.1" xref="S4.T8.12.12.1.m1.2.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.12.12.1.m1.2.3.2.1.1" xref="S4.T8.12.12.1.m1.2.3.1.cmml">(</mo><mfrac id="S4.T8.12.12.1.m1.2.2" xref="S4.T8.12.12.1.m1.2.2.cmml"><mi mathsize="80%" id="S4.T8.12.12.1.m1.2.2.2" xref="S4.T8.12.12.1.m1.2.2.2.cmml">n</mi><mrow id="S4.T8.12.12.1.m1.2.2.3" xref="S4.T8.12.12.1.m1.2.2.3.cmml"><mi mathsize="80%" id="S4.T8.12.12.1.m1.2.2.3.2" xref="S4.T8.12.12.1.m1.2.2.3.2.cmml">ε</mi><mo lspace="0em" rspace="0em" id="S4.T8.12.12.1.m1.2.2.3.1" xref="S4.T8.12.12.1.m1.2.2.3.1.cmml">​</mo><mi mathsize="80%" id="S4.T8.12.12.1.m1.2.2.3.3" xref="S4.T8.12.12.1.m1.2.2.3.3.cmml">δ</mi></mrow></mfrac><mo maxsize="80%" minsize="80%" id="S4.T8.12.12.1.m1.2.3.2.1.2" xref="S4.T8.12.12.1.m1.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.12.12.1.m1.2b"><apply id="S4.T8.12.12.1.m1.2.3.1.cmml" xref="S4.T8.12.12.1.m1.2.3.2"><log id="S4.T8.12.12.1.m1.1.1.cmml" xref="S4.T8.12.12.1.m1.1.1"></log><apply id="S4.T8.12.12.1.m1.2.2.cmml" xref="S4.T8.12.12.1.m1.2.2"><divide id="S4.T8.12.12.1.m1.2.2.1.cmml" xref="S4.T8.12.12.1.m1.2.2"></divide><ci id="S4.T8.12.12.1.m1.2.2.2.cmml" xref="S4.T8.12.12.1.m1.2.2.2">𝑛</ci><apply id="S4.T8.12.12.1.m1.2.2.3.cmml" xref="S4.T8.12.12.1.m1.2.2.3"><times id="S4.T8.12.12.1.m1.2.2.3.1.cmml" xref="S4.T8.12.12.1.m1.2.2.3.1"></times><ci id="S4.T8.12.12.1.m1.2.2.3.2.cmml" xref="S4.T8.12.12.1.m1.2.2.3.2">𝜀</ci><ci id="S4.T8.12.12.1.m1.2.2.3.3.cmml" xref="S4.T8.12.12.1.m1.2.2.3.3">𝛿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.12.12.1.m1.2c">\log(\frac{n}{\varepsilon\delta})</annotation></semantics></math></td>
<td id="S4.T8.13.13.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.13.13.2.m1.2" class="ltx_Math" alttext="\log(\frac{n}{\delta})" display="inline"><semantics id="S4.T8.13.13.2.m1.2a"><mrow id="S4.T8.13.13.2.m1.2.3.2" xref="S4.T8.13.13.2.m1.2.3.1.cmml"><mi mathsize="80%" id="S4.T8.13.13.2.m1.1.1" xref="S4.T8.13.13.2.m1.1.1.cmml">log</mi><mo id="S4.T8.13.13.2.m1.2.3.2a" xref="S4.T8.13.13.2.m1.2.3.1.cmml">⁡</mo><mrow id="S4.T8.13.13.2.m1.2.3.2.1" xref="S4.T8.13.13.2.m1.2.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.13.13.2.m1.2.3.2.1.1" xref="S4.T8.13.13.2.m1.2.3.1.cmml">(</mo><mfrac id="S4.T8.13.13.2.m1.2.2" xref="S4.T8.13.13.2.m1.2.2.cmml"><mi mathsize="80%" id="S4.T8.13.13.2.m1.2.2.2" xref="S4.T8.13.13.2.m1.2.2.2.cmml">n</mi><mi mathsize="80%" id="S4.T8.13.13.2.m1.2.2.3" xref="S4.T8.13.13.2.m1.2.2.3.cmml">δ</mi></mfrac><mo maxsize="80%" minsize="80%" id="S4.T8.13.13.2.m1.2.3.2.1.2" xref="S4.T8.13.13.2.m1.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.13.13.2.m1.2b"><apply id="S4.T8.13.13.2.m1.2.3.1.cmml" xref="S4.T8.13.13.2.m1.2.3.2"><log id="S4.T8.13.13.2.m1.1.1.cmml" xref="S4.T8.13.13.2.m1.1.1"></log><apply id="S4.T8.13.13.2.m1.2.2.cmml" xref="S4.T8.13.13.2.m1.2.2"><divide id="S4.T8.13.13.2.m1.2.2.1.cmml" xref="S4.T8.13.13.2.m1.2.2"></divide><ci id="S4.T8.13.13.2.m1.2.2.2.cmml" xref="S4.T8.13.13.2.m1.2.2.2">𝑛</ci><ci id="S4.T8.13.13.2.m1.2.2.3.cmml" xref="S4.T8.13.13.2.m1.2.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.13.13.2.m1.2c">\log(\frac{n}{\delta})</annotation></semantics></math></td>
<td id="S4.T8.14.14.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.14.14.3.m1.1" class="ltx_Math" alttext="\frac{1}{\varepsilon}\sqrt{\log\frac{1}{\delta}}" display="inline"><semantics id="S4.T8.14.14.3.m1.1a"><mrow id="S4.T8.14.14.3.m1.1.1" xref="S4.T8.14.14.3.m1.1.1.cmml"><mfrac id="S4.T8.14.14.3.m1.1.1.2" xref="S4.T8.14.14.3.m1.1.1.2.cmml"><mn mathsize="80%" id="S4.T8.14.14.3.m1.1.1.2.2" xref="S4.T8.14.14.3.m1.1.1.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.14.14.3.m1.1.1.2.3" xref="S4.T8.14.14.3.m1.1.1.2.3.cmml">ε</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.T8.14.14.3.m1.1.1.1" xref="S4.T8.14.14.3.m1.1.1.1.cmml">​</mo><msqrt id="S4.T8.14.14.3.m1.1.1.3" xref="S4.T8.14.14.3.m1.1.1.3.cmml"><mrow id="S4.T8.14.14.3.m1.1.1.3.2" xref="S4.T8.14.14.3.m1.1.1.3.2.cmml"><mi mathsize="80%" id="S4.T8.14.14.3.m1.1.1.3.2.1" xref="S4.T8.14.14.3.m1.1.1.3.2.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.14.14.3.m1.1.1.3.2a" xref="S4.T8.14.14.3.m1.1.1.3.2.cmml">⁡</mo><mfrac id="S4.T8.14.14.3.m1.1.1.3.2.2" xref="S4.T8.14.14.3.m1.1.1.3.2.2.cmml"><mn mathsize="80%" id="S4.T8.14.14.3.m1.1.1.3.2.2.2" xref="S4.T8.14.14.3.m1.1.1.3.2.2.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.14.14.3.m1.1.1.3.2.2.3" xref="S4.T8.14.14.3.m1.1.1.3.2.2.3.cmml">δ</mi></mfrac></mrow></msqrt></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.14.14.3.m1.1b"><apply id="S4.T8.14.14.3.m1.1.1.cmml" xref="S4.T8.14.14.3.m1.1.1"><times id="S4.T8.14.14.3.m1.1.1.1.cmml" xref="S4.T8.14.14.3.m1.1.1.1"></times><apply id="S4.T8.14.14.3.m1.1.1.2.cmml" xref="S4.T8.14.14.3.m1.1.1.2"><divide id="S4.T8.14.14.3.m1.1.1.2.1.cmml" xref="S4.T8.14.14.3.m1.1.1.2"></divide><cn type="integer" id="S4.T8.14.14.3.m1.1.1.2.2.cmml" xref="S4.T8.14.14.3.m1.1.1.2.2">1</cn><ci id="S4.T8.14.14.3.m1.1.1.2.3.cmml" xref="S4.T8.14.14.3.m1.1.1.2.3">𝜀</ci></apply><apply id="S4.T8.14.14.3.m1.1.1.3.cmml" xref="S4.T8.14.14.3.m1.1.1.3"><root id="S4.T8.14.14.3.m1.1.1.3a.cmml" xref="S4.T8.14.14.3.m1.1.1.3"></root><apply id="S4.T8.14.14.3.m1.1.1.3.2.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2"><log id="S4.T8.14.14.3.m1.1.1.3.2.1.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2.1"></log><apply id="S4.T8.14.14.3.m1.1.1.3.2.2.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2.2"><divide id="S4.T8.14.14.3.m1.1.1.3.2.2.1.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2.2"></divide><cn type="integer" id="S4.T8.14.14.3.m1.1.1.3.2.2.2.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2.2.2">1</cn><ci id="S4.T8.14.14.3.m1.1.1.3.2.2.3.cmml" xref="S4.T8.14.14.3.m1.1.1.3.2.2.3">𝛿</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.14.14.3.m1.1c">\frac{1}{\varepsilon}\sqrt{\log\frac{1}{\delta}}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.17.17" class="ltx_tr">
<td id="S4.T8.17.17.4" class="ltx_td ltx_align_left" style="padding-top:3pt;padding-bottom:3pt;"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.17.17.4.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S4.T8.17.17.4.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T8.15.15.1" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.15.15.1.m1.2" class="ltx_Math" alttext="\log(\frac{n}{\delta})" display="inline"><semantics id="S4.T8.15.15.1.m1.2a"><mrow id="S4.T8.15.15.1.m1.2.3.2" xref="S4.T8.15.15.1.m1.2.3.1.cmml"><mi mathsize="80%" id="S4.T8.15.15.1.m1.1.1" xref="S4.T8.15.15.1.m1.1.1.cmml">log</mi><mo id="S4.T8.15.15.1.m1.2.3.2a" xref="S4.T8.15.15.1.m1.2.3.1.cmml">⁡</mo><mrow id="S4.T8.15.15.1.m1.2.3.2.1" xref="S4.T8.15.15.1.m1.2.3.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.15.15.1.m1.2.3.2.1.1" xref="S4.T8.15.15.1.m1.2.3.1.cmml">(</mo><mfrac id="S4.T8.15.15.1.m1.2.2" xref="S4.T8.15.15.1.m1.2.2.cmml"><mi mathsize="80%" id="S4.T8.15.15.1.m1.2.2.2" xref="S4.T8.15.15.1.m1.2.2.2.cmml">n</mi><mi mathsize="80%" id="S4.T8.15.15.1.m1.2.2.3" xref="S4.T8.15.15.1.m1.2.2.3.cmml">δ</mi></mfrac><mo maxsize="80%" minsize="80%" id="S4.T8.15.15.1.m1.2.3.2.1.2" xref="S4.T8.15.15.1.m1.2.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.15.15.1.m1.2b"><apply id="S4.T8.15.15.1.m1.2.3.1.cmml" xref="S4.T8.15.15.1.m1.2.3.2"><log id="S4.T8.15.15.1.m1.1.1.cmml" xref="S4.T8.15.15.1.m1.1.1"></log><apply id="S4.T8.15.15.1.m1.2.2.cmml" xref="S4.T8.15.15.1.m1.2.2"><divide id="S4.T8.15.15.1.m1.2.2.1.cmml" xref="S4.T8.15.15.1.m1.2.2"></divide><ci id="S4.T8.15.15.1.m1.2.2.2.cmml" xref="S4.T8.15.15.1.m1.2.2.2">𝑛</ci><ci id="S4.T8.15.15.1.m1.2.2.3.cmml" xref="S4.T8.15.15.1.m1.2.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.15.15.1.m1.2c">\log(\frac{n}{\delta})</annotation></semantics></math></td>
<td id="S4.T8.16.16.2" class="ltx_td ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.16.16.2.m1.1" class="ltx_Math" alttext="\log n" display="inline"><semantics id="S4.T8.16.16.2.m1.1a"><mrow id="S4.T8.16.16.2.m1.1.1" xref="S4.T8.16.16.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.T8.16.16.2.m1.1.1.1" xref="S4.T8.16.16.2.m1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.16.16.2.m1.1.1a" xref="S4.T8.16.16.2.m1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.16.16.2.m1.1.1.2" xref="S4.T8.16.16.2.m1.1.1.2.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.16.16.2.m1.1b"><apply id="S4.T8.16.16.2.m1.1.1.cmml" xref="S4.T8.16.16.2.m1.1.1"><log id="S4.T8.16.16.2.m1.1.1.1.cmml" xref="S4.T8.16.16.2.m1.1.1.1"></log><ci id="S4.T8.16.16.2.m1.1.1.2.cmml" xref="S4.T8.16.16.2.m1.1.1.2">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.16.16.2.m1.1c">\log n</annotation></semantics></math></td>
<td id="S4.T8.17.17.3" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.17.17.3.m1.1" class="ltx_Math" alttext="\frac{1}{\varepsilon}" display="inline"><semantics id="S4.T8.17.17.3.m1.1a"><mfrac id="S4.T8.17.17.3.m1.1.1" xref="S4.T8.17.17.3.m1.1.1.cmml"><mn mathsize="80%" id="S4.T8.17.17.3.m1.1.1.2" xref="S4.T8.17.17.3.m1.1.1.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.17.17.3.m1.1.1.3" xref="S4.T8.17.17.3.m1.1.1.3.cmml">ε</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T8.17.17.3.m1.1b"><apply id="S4.T8.17.17.3.m1.1.1.cmml" xref="S4.T8.17.17.3.m1.1.1"><divide id="S4.T8.17.17.3.m1.1.1.1.cmml" xref="S4.T8.17.17.3.m1.1.1"></divide><cn type="integer" id="S4.T8.17.17.3.m1.1.1.2.cmml" xref="S4.T8.17.17.3.m1.1.1.2">1</cn><ci id="S4.T8.17.17.3.m1.1.1.3.cmml" xref="S4.T8.17.17.3.m1.1.1.3">𝜀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.17.17.3.m1.1c">\frac{1}{\varepsilon}</annotation></semantics></math></td>
</tr>
<tr id="S4.T8.20.20" class="ltx_tr">
<td id="S4.T8.20.20.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;">
<cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.20.20.4.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib204" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">204</span></a><span id="S4.T8.20.20.4.2.2" class="ltx_text" style="font-size:80%;">]</span></cite><span id="S4.T8.20.20.4.3" class="ltx_text" style="font-size:80%;"> &amp; </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T8.20.20.4.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a><span id="S4.T8.20.20.4.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T8.18.18.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.18.18.1.m1.2" class="ltx_Math" alttext="1+\frac{\log(1/\delta)}{\log n}" display="inline"><semantics id="S4.T8.18.18.1.m1.2a"><mrow id="S4.T8.18.18.1.m1.2.3" xref="S4.T8.18.18.1.m1.2.3.cmml"><mn mathsize="80%" id="S4.T8.18.18.1.m1.2.3.2" xref="S4.T8.18.18.1.m1.2.3.2.cmml">1</mn><mo mathsize="80%" id="S4.T8.18.18.1.m1.2.3.1" xref="S4.T8.18.18.1.m1.2.3.1.cmml">+</mo><mfrac id="S4.T8.18.18.1.m1.2.2" xref="S4.T8.18.18.1.m1.2.2.cmml"><mrow id="S4.T8.18.18.1.m1.2.2.2.2" xref="S4.T8.18.18.1.m1.2.2.2.3.cmml"><mi mathsize="80%" id="S4.T8.18.18.1.m1.1.1.1.1" xref="S4.T8.18.18.1.m1.1.1.1.1.cmml">log</mi><mo id="S4.T8.18.18.1.m1.2.2.2.2a" xref="S4.T8.18.18.1.m1.2.2.2.3.cmml">⁡</mo><mrow id="S4.T8.18.18.1.m1.2.2.2.2.1" xref="S4.T8.18.18.1.m1.2.2.2.3.cmml"><mo maxsize="80%" minsize="80%" id="S4.T8.18.18.1.m1.2.2.2.2.1.2" xref="S4.T8.18.18.1.m1.2.2.2.3.cmml">(</mo><mrow id="S4.T8.18.18.1.m1.2.2.2.2.1.1" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.cmml"><mn mathsize="80%" id="S4.T8.18.18.1.m1.2.2.2.2.1.1.2" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.2.cmml">1</mn><mo maxsize="80%" minsize="80%" stretchy="true" symmetric="true" id="S4.T8.18.18.1.m1.2.2.2.2.1.1.1" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.1.cmml">/</mo><mi mathsize="80%" id="S4.T8.18.18.1.m1.2.2.2.2.1.1.3" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.3.cmml">δ</mi></mrow><mo maxsize="80%" minsize="80%" id="S4.T8.18.18.1.m1.2.2.2.2.1.3" xref="S4.T8.18.18.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S4.T8.18.18.1.m1.2.2.4" xref="S4.T8.18.18.1.m1.2.2.4.cmml"><mi mathsize="80%" id="S4.T8.18.18.1.m1.2.2.4.1" xref="S4.T8.18.18.1.m1.2.2.4.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.18.18.1.m1.2.2.4a" xref="S4.T8.18.18.1.m1.2.2.4.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.18.18.1.m1.2.2.4.2" xref="S4.T8.18.18.1.m1.2.2.4.2.cmml">n</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.18.18.1.m1.2b"><apply id="S4.T8.18.18.1.m1.2.3.cmml" xref="S4.T8.18.18.1.m1.2.3"><plus id="S4.T8.18.18.1.m1.2.3.1.cmml" xref="S4.T8.18.18.1.m1.2.3.1"></plus><cn type="integer" id="S4.T8.18.18.1.m1.2.3.2.cmml" xref="S4.T8.18.18.1.m1.2.3.2">1</cn><apply id="S4.T8.18.18.1.m1.2.2.cmml" xref="S4.T8.18.18.1.m1.2.2"><divide id="S4.T8.18.18.1.m1.2.2.3.cmml" xref="S4.T8.18.18.1.m1.2.2"></divide><apply id="S4.T8.18.18.1.m1.2.2.2.3.cmml" xref="S4.T8.18.18.1.m1.2.2.2.2"><log id="S4.T8.18.18.1.m1.1.1.1.1.cmml" xref="S4.T8.18.18.1.m1.1.1.1.1"></log><apply id="S4.T8.18.18.1.m1.2.2.2.2.1.1.cmml" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1"><divide id="S4.T8.18.18.1.m1.2.2.2.2.1.1.1.cmml" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.1"></divide><cn type="integer" id="S4.T8.18.18.1.m1.2.2.2.2.1.1.2.cmml" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.2">1</cn><ci id="S4.T8.18.18.1.m1.2.2.2.2.1.1.3.cmml" xref="S4.T8.18.18.1.m1.2.2.2.2.1.1.3">𝛿</ci></apply></apply><apply id="S4.T8.18.18.1.m1.2.2.4.cmml" xref="S4.T8.18.18.1.m1.2.2.4"><log id="S4.T8.18.18.1.m1.2.2.4.1.cmml" xref="S4.T8.18.18.1.m1.2.2.4.1"></log><ci id="S4.T8.18.18.1.m1.2.2.4.2.cmml" xref="S4.T8.18.18.1.m1.2.2.4.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.18.18.1.m1.2c">1+\frac{\log(1/\delta)}{\log n}</annotation></semantics></math></td>
<td id="S4.T8.19.19.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.19.19.2.m1.1" class="ltx_Math" alttext="\log n" display="inline"><semantics id="S4.T8.19.19.2.m1.1a"><mrow id="S4.T8.19.19.2.m1.1.1" xref="S4.T8.19.19.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.T8.19.19.2.m1.1.1.1" xref="S4.T8.19.19.2.m1.1.1.1.cmml">log</mi><mo lspace="0.167em" id="S4.T8.19.19.2.m1.1.1a" xref="S4.T8.19.19.2.m1.1.1.cmml">⁡</mo><mi mathsize="80%" id="S4.T8.19.19.2.m1.1.1.2" xref="S4.T8.19.19.2.m1.1.1.2.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.19.19.2.m1.1b"><apply id="S4.T8.19.19.2.m1.1.1.cmml" xref="S4.T8.19.19.2.m1.1.1"><log id="S4.T8.19.19.2.m1.1.1.1.cmml" xref="S4.T8.19.19.2.m1.1.1.1"></log><ci id="S4.T8.19.19.2.m1.1.1.2.cmml" xref="S4.T8.19.19.2.m1.1.1.2">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.19.19.2.m1.1c">\log n</annotation></semantics></math></td>
<td id="S4.T8.20.20.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" style="padding-top:3pt;padding-bottom:3pt;"><math id="S4.T8.20.20.3.m1.1" class="ltx_Math" alttext="\frac{1}{\varepsilon}" display="inline"><semantics id="S4.T8.20.20.3.m1.1a"><mfrac id="S4.T8.20.20.3.m1.1.1" xref="S4.T8.20.20.3.m1.1.1.cmml"><mn mathsize="80%" id="S4.T8.20.20.3.m1.1.1.2" xref="S4.T8.20.20.3.m1.1.1.2.cmml">1</mn><mi mathsize="80%" id="S4.T8.20.20.3.m1.1.1.3" xref="S4.T8.20.20.3.m1.1.1.3.cmml">ε</mi></mfrac><annotation-xml encoding="MathML-Content" id="S4.T8.20.20.3.m1.1b"><apply id="S4.T8.20.20.3.m1.1.1.cmml" xref="S4.T8.20.20.3.m1.1.1"><divide id="S4.T8.20.20.3.m1.1.1.1.cmml" xref="S4.T8.20.20.3.m1.1.1"></divide><cn type="integer" id="S4.T8.20.20.3.m1.1.1.2.cmml" xref="S4.T8.20.20.3.m1.1.1.2">1</cn><ci id="S4.T8.20.20.3.m1.1.1.3.cmml" xref="S4.T8.20.20.3.m1.1.1.3">𝜀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.20.20.3.m1.1c">\frac{1}{\varepsilon}</annotation></semantics></math></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of differentially private <em id="S4.T8.37.1" class="ltx_emph ltx_font_italic">aggregation</em> protocols in the multi-message shuffled model with <math id="S4.T8.25.m1.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.T8.25.m1.2b"><mrow id="S4.T8.25.m1.2.3.2" xref="S4.T8.25.m1.2.3.1.cmml"><mo stretchy="false" id="S4.T8.25.m1.2.3.2.1" xref="S4.T8.25.m1.2.3.1.cmml">(</mo><mi id="S4.T8.25.m1.1.1" xref="S4.T8.25.m1.1.1.cmml">ε</mi><mo id="S4.T8.25.m1.2.3.2.2" xref="S4.T8.25.m1.2.3.1.cmml">,</mo><mi id="S4.T8.25.m1.2.2" xref="S4.T8.25.m1.2.2.cmml">δ</mi><mo stretchy="false" id="S4.T8.25.m1.2.3.2.3" xref="S4.T8.25.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.25.m1.2c"><interval closure="open" id="S4.T8.25.m1.2.3.1.cmml" xref="S4.T8.25.m1.2.3.2"><ci id="S4.T8.25.m1.1.1.cmml" xref="S4.T8.25.m1.1.1">𝜀</ci><ci id="S4.T8.25.m1.2.2.cmml" xref="S4.T8.25.m1.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.25.m1.2d">(\varepsilon,\delta)</annotation></semantics></math>-differential privacy.
The number of parties is <math id="S4.T8.26.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T8.26.m2.1b"><mi id="S4.T8.26.m2.1.1" xref="S4.T8.26.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T8.26.m2.1c"><ci id="S4.T8.26.m2.1.1.cmml" xref="S4.T8.26.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.26.m2.1d">n</annotation></semantics></math>, and <math id="S4.T8.27.m3.1" class="ltx_Math" alttext="\ell" display="inline"><semantics id="S4.T8.27.m3.1b"><mi mathvariant="normal" id="S4.T8.27.m3.1.1" xref="S4.T8.27.m3.1.1.cmml">ℓ</mi><annotation-xml encoding="MathML-Content" id="S4.T8.27.m3.1c"><ci id="S4.T8.27.m3.1.1.cmml" xref="S4.T8.27.m3.1.1">ℓ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.27.m3.1d">\ell</annotation></semantics></math> is an integer parameter.
Message sizes are in bits. For readability, we assume that <math id="S4.T8.28.m4.1" class="ltx_Math" alttext="\varepsilon\leq O(1)" display="inline"><semantics id="S4.T8.28.m4.1b"><mrow id="S4.T8.28.m4.1.2" xref="S4.T8.28.m4.1.2.cmml"><mi id="S4.T8.28.m4.1.2.2" xref="S4.T8.28.m4.1.2.2.cmml">ε</mi><mo id="S4.T8.28.m4.1.2.1" xref="S4.T8.28.m4.1.2.1.cmml">≤</mo><mrow id="S4.T8.28.m4.1.2.3" xref="S4.T8.28.m4.1.2.3.cmml"><mi id="S4.T8.28.m4.1.2.3.2" xref="S4.T8.28.m4.1.2.3.2.cmml">O</mi><mo lspace="0em" rspace="0em" id="S4.T8.28.m4.1.2.3.1" xref="S4.T8.28.m4.1.2.3.1.cmml">​</mo><mrow id="S4.T8.28.m4.1.2.3.3.2" xref="S4.T8.28.m4.1.2.3.cmml"><mo stretchy="false" id="S4.T8.28.m4.1.2.3.3.2.1" xref="S4.T8.28.m4.1.2.3.cmml">(</mo><mn id="S4.T8.28.m4.1.1" xref="S4.T8.28.m4.1.1.cmml">1</mn><mo stretchy="false" id="S4.T8.28.m4.1.2.3.3.2.2" xref="S4.T8.28.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T8.28.m4.1c"><apply id="S4.T8.28.m4.1.2.cmml" xref="S4.T8.28.m4.1.2"><leq id="S4.T8.28.m4.1.2.1.cmml" xref="S4.T8.28.m4.1.2.1"></leq><ci id="S4.T8.28.m4.1.2.2.cmml" xref="S4.T8.28.m4.1.2.2">𝜀</ci><apply id="S4.T8.28.m4.1.2.3.cmml" xref="S4.T8.28.m4.1.2.3"><times id="S4.T8.28.m4.1.2.3.1.cmml" xref="S4.T8.28.m4.1.2.3.1"></times><ci id="S4.T8.28.m4.1.2.3.2.cmml" xref="S4.T8.28.m4.1.2.3.2">𝑂</ci><cn type="integer" id="S4.T8.28.m4.1.1.cmml" xref="S4.T8.28.m4.1.1">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T8.28.m4.1d">\varepsilon\leq O(1)</annotation></semantics></math>, and asymptotic notations are suppressed.
</figcaption>
</figure>
<figure id="S4.T9" class="ltx_table">
<table id="S4.T9.10" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T9.10.11.1" class="ltx_tr">
<td id="S4.T9.10.11.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T9.10.11.1.2" class="ltx_td ltx_align_left ltx_border_tt" colspan="2">
<span id="S4.T9.10.11.1.2.1" class="ltx_text"></span><span id="S4.T9.10.11.1.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;"> <span id="S4.T9.10.11.1.2.2.1" class="ltx_text">
<span id="S4.T9.10.11.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.10.11.1.2.2.1.1.1" class="ltx_tr">
<span id="S4.T9.10.11.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Local</span></span>
</span></span><span id="S4.T9.10.11.1.2.2.2" class="ltx_text"></span></span>
</td>
<th id="S4.T9.10.11.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T9.10.11.1.3.1" class="ltx_text"></span><span id="S4.T9.10.11.1.3.2" class="ltx_text ltx_font_bold" style="font-size:80%;"> <span id="S4.T9.10.11.1.3.2.1" class="ltx_text">
<span id="S4.T9.10.11.1.3.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.10.11.1.3.2.1.1.1" class="ltx_tr">
<span id="S4.T9.10.11.1.3.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Local + shuffle</span></span>
</span></span><span id="S4.T9.10.11.1.3.2.2" class="ltx_text"></span></span>
</th>
<th id="S4.T9.10.11.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T9.10.11.1.4.1" class="ltx_text"></span><span id="S4.T9.10.11.1.4.2" class="ltx_text ltx_font_bold" style="font-size:80%;"> <span id="S4.T9.10.11.1.4.2.1" class="ltx_text">
<span id="S4.T9.10.11.1.4.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.10.11.1.4.2.1.1.1" class="ltx_tr">
<span id="S4.T9.10.11.1.4.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Shuffled,</span></span>
<span id="S4.T9.10.11.1.4.2.1.1.2" class="ltx_tr">
<span id="S4.T9.10.11.1.4.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">single-message</span></span>
</span></span><span id="S4.T9.10.11.1.4.2.2" class="ltx_text"></span></span>
</th>
<th id="S4.T9.10.11.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T9.10.11.1.5.1" class="ltx_text"></span><span id="S4.T9.10.11.1.5.2" class="ltx_text ltx_font_bold" style="font-size:80%;"> <span id="S4.T9.10.11.1.5.2.1" class="ltx_text">
<span id="S4.T9.10.11.1.5.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.10.11.1.5.2.1.1.1" class="ltx_tr">
<span id="S4.T9.10.11.1.5.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Shuffled,</span></span>
<span id="S4.T9.10.11.1.5.2.1.1.2" class="ltx_tr">
<span id="S4.T9.10.11.1.5.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">multi-message</span></span>
</span></span><span id="S4.T9.10.11.1.5.2.2" class="ltx_text"></span></span>
</th>
<th id="S4.T9.10.11.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T9.10.11.1.6.1" class="ltx_text"></span><span id="S4.T9.10.11.1.6.2" class="ltx_text ltx_font_bold" style="font-size:80%;"> <span id="S4.T9.10.11.1.6.2.1" class="ltx_text">
<span id="S4.T9.10.11.1.6.2.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T9.10.11.1.6.2.1.1.1" class="ltx_tr">
<span id="S4.T9.10.11.1.6.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">Central</span></span>
</span></span><span id="S4.T9.10.11.1.6.2.2" class="ltx_text"></span></span>
</th>
</tr>
<tr id="S4.T9.6.6" class="ltx_tr">
<td id="S4.T9.6.6.7" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T9.6.6.7.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.6.6.7.1.1" class="ltx_p"><span id="S4.T9.6.6.7.1.1.1" class="ltx_text" style="font-size:80%;">Expected max. error</span></span>
</span>
</td>
<td id="S4.T9.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T9.1.1.1.m1.1" class="ltx_Math" alttext="\tilde{O}(\sqrt{n})" display="inline"><semantics id="S4.T9.1.1.1.m1.1a"><mrow id="S4.T9.1.1.1.m1.1.2" xref="S4.T9.1.1.1.m1.1.2.cmml"><mover accent="true" id="S4.T9.1.1.1.m1.1.2.2" xref="S4.T9.1.1.1.m1.1.2.2.cmml"><mi mathsize="80%" id="S4.T9.1.1.1.m1.1.2.2.2" xref="S4.T9.1.1.1.m1.1.2.2.2.cmml">O</mi><mo mathsize="80%" id="S4.T9.1.1.1.m1.1.2.2.1" xref="S4.T9.1.1.1.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.1.1.1.m1.1.2.1" xref="S4.T9.1.1.1.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.1.1.1.m1.1.2.3.2" xref="S4.T9.1.1.1.m1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.1.1.1.m1.1.2.3.2.1" xref="S4.T9.1.1.1.m1.1.1.cmml">(</mo><msqrt id="S4.T9.1.1.1.m1.1.1" xref="S4.T9.1.1.1.m1.1.1.cmml"><mi mathsize="80%" id="S4.T9.1.1.1.m1.1.1.2" xref="S4.T9.1.1.1.m1.1.1.2.cmml">n</mi></msqrt><mo maxsize="80%" minsize="80%" id="S4.T9.1.1.1.m1.1.2.3.2.2" xref="S4.T9.1.1.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.1.1.1.m1.1b"><apply id="S4.T9.1.1.1.m1.1.2.cmml" xref="S4.T9.1.1.1.m1.1.2"><times id="S4.T9.1.1.1.m1.1.2.1.cmml" xref="S4.T9.1.1.1.m1.1.2.1"></times><apply id="S4.T9.1.1.1.m1.1.2.2.cmml" xref="S4.T9.1.1.1.m1.1.2.2"><ci id="S4.T9.1.1.1.m1.1.2.2.1.cmml" xref="S4.T9.1.1.1.m1.1.2.2.1">~</ci><ci id="S4.T9.1.1.1.m1.1.2.2.2.cmml" xref="S4.T9.1.1.1.m1.1.2.2.2">𝑂</ci></apply><apply id="S4.T9.1.1.1.m1.1.1.cmml" xref="S4.T9.1.1.1.m1.1.2.3.2"><root id="S4.T9.1.1.1.m1.1.1a.cmml" xref="S4.T9.1.1.1.m1.1.2.3.2"></root><ci id="S4.T9.1.1.1.m1.1.1.2.cmml" xref="S4.T9.1.1.1.m1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.1.1.1.m1.1c">\tilde{O}(\sqrt{n})</annotation></semantics></math></td>
<td id="S4.T9.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T9.2.2.2.m1.1" class="ltx_Math" alttext="\tilde{\Omega}(\sqrt{n})" display="inline"><semantics id="S4.T9.2.2.2.m1.1a"><mrow id="S4.T9.2.2.2.m1.1.2" xref="S4.T9.2.2.2.m1.1.2.cmml"><mover accent="true" id="S4.T9.2.2.2.m1.1.2.2" xref="S4.T9.2.2.2.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.2.2.2.m1.1.2.2.2" xref="S4.T9.2.2.2.m1.1.2.2.2.cmml">Ω</mi><mo mathsize="80%" id="S4.T9.2.2.2.m1.1.2.2.1" xref="S4.T9.2.2.2.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.2.2.2.m1.1.2.1" xref="S4.T9.2.2.2.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.2.2.2.m1.1.2.3.2" xref="S4.T9.2.2.2.m1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.2.2.2.m1.1.2.3.2.1" xref="S4.T9.2.2.2.m1.1.1.cmml">(</mo><msqrt id="S4.T9.2.2.2.m1.1.1" xref="S4.T9.2.2.2.m1.1.1.cmml"><mi mathsize="80%" id="S4.T9.2.2.2.m1.1.1.2" xref="S4.T9.2.2.2.m1.1.1.2.cmml">n</mi></msqrt><mo maxsize="80%" minsize="80%" id="S4.T9.2.2.2.m1.1.2.3.2.2" xref="S4.T9.2.2.2.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.2.2.2.m1.1b"><apply id="S4.T9.2.2.2.m1.1.2.cmml" xref="S4.T9.2.2.2.m1.1.2"><times id="S4.T9.2.2.2.m1.1.2.1.cmml" xref="S4.T9.2.2.2.m1.1.2.1"></times><apply id="S4.T9.2.2.2.m1.1.2.2.cmml" xref="S4.T9.2.2.2.m1.1.2.2"><ci id="S4.T9.2.2.2.m1.1.2.2.1.cmml" xref="S4.T9.2.2.2.m1.1.2.2.1">~</ci><ci id="S4.T9.2.2.2.m1.1.2.2.2.cmml" xref="S4.T9.2.2.2.m1.1.2.2.2">Ω</ci></apply><apply id="S4.T9.2.2.2.m1.1.1.cmml" xref="S4.T9.2.2.2.m1.1.2.3.2"><root id="S4.T9.2.2.2.m1.1.1a.cmml" xref="S4.T9.2.2.2.m1.1.2.3.2"></root><ci id="S4.T9.2.2.2.m1.1.1.2.cmml" xref="S4.T9.2.2.2.m1.1.1.2">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.2.2.2.m1.1c">\tilde{\Omega}(\sqrt{n})</annotation></semantics></math></td>
<td id="S4.T9.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T9.3.3.3.m1.4" class="ltx_Math" alttext="\tilde{O}(\min(\sqrt[4]{n},\sqrt{B}))" display="inline"><semantics id="S4.T9.3.3.3.m1.4a"><mrow id="S4.T9.3.3.3.m1.4.4" xref="S4.T9.3.3.3.m1.4.4.cmml"><mover accent="true" id="S4.T9.3.3.3.m1.4.4.3" xref="S4.T9.3.3.3.m1.4.4.3.cmml"><mi mathsize="80%" id="S4.T9.3.3.3.m1.4.4.3.2" xref="S4.T9.3.3.3.m1.4.4.3.2.cmml">O</mi><mo mathsize="80%" id="S4.T9.3.3.3.m1.4.4.3.1" xref="S4.T9.3.3.3.m1.4.4.3.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.3.3.3.m1.4.4.2" xref="S4.T9.3.3.3.m1.4.4.2.cmml">​</mo><mrow id="S4.T9.3.3.3.m1.4.4.1.1" xref="S4.T9.3.3.3.m1.4.4.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.3.3.3.m1.4.4.1.1.2" xref="S4.T9.3.3.3.m1.4.4.cmml">(</mo><mrow id="S4.T9.3.3.3.m1.4.4.1.1.1.2" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml"><mi mathsize="80%" id="S4.T9.3.3.3.m1.1.1" xref="S4.T9.3.3.3.m1.1.1.cmml">min</mi><mo id="S4.T9.3.3.3.m1.4.4.1.1.1.2a" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml">⁡</mo><mrow id="S4.T9.3.3.3.m1.4.4.1.1.1.2.1" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.3.3.3.m1.4.4.1.1.1.2.1.1" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml">(</mo><mroot id="S4.T9.3.3.3.m1.2.2" xref="S4.T9.3.3.3.m1.2.2.cmml"><mi mathsize="80%" id="S4.T9.3.3.3.m1.2.2.3" xref="S4.T9.3.3.3.m1.2.2.3.cmml">n</mi><mn mathsize="80%" id="S4.T9.3.3.3.m1.2.2.2" xref="S4.T9.3.3.3.m1.2.2.2.cmml">4</mn></mroot><mo mathsize="80%" id="S4.T9.3.3.3.m1.4.4.1.1.1.2.1.2" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml">,</mo><msqrt id="S4.T9.3.3.3.m1.3.3" xref="S4.T9.3.3.3.m1.3.3.cmml"><mi mathsize="80%" id="S4.T9.3.3.3.m1.3.3.2" xref="S4.T9.3.3.3.m1.3.3.2.cmml">B</mi></msqrt><mo maxsize="80%" minsize="80%" id="S4.T9.3.3.3.m1.4.4.1.1.1.2.1.3" xref="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="80%" minsize="80%" id="S4.T9.3.3.3.m1.4.4.1.1.3" xref="S4.T9.3.3.3.m1.4.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.3.3.3.m1.4b"><apply id="S4.T9.3.3.3.m1.4.4.cmml" xref="S4.T9.3.3.3.m1.4.4"><times id="S4.T9.3.3.3.m1.4.4.2.cmml" xref="S4.T9.3.3.3.m1.4.4.2"></times><apply id="S4.T9.3.3.3.m1.4.4.3.cmml" xref="S4.T9.3.3.3.m1.4.4.3"><ci id="S4.T9.3.3.3.m1.4.4.3.1.cmml" xref="S4.T9.3.3.3.m1.4.4.3.1">~</ci><ci id="S4.T9.3.3.3.m1.4.4.3.2.cmml" xref="S4.T9.3.3.3.m1.4.4.3.2">𝑂</ci></apply><apply id="S4.T9.3.3.3.m1.4.4.1.1.1.1.cmml" xref="S4.T9.3.3.3.m1.4.4.1.1.1.2"><min id="S4.T9.3.3.3.m1.1.1.cmml" xref="S4.T9.3.3.3.m1.1.1"></min><apply id="S4.T9.3.3.3.m1.2.2.cmml" xref="S4.T9.3.3.3.m1.2.2"><root id="S4.T9.3.3.3.m1.2.2a.cmml" xref="S4.T9.3.3.3.m1.2.2"></root><degree id="S4.T9.3.3.3.m1.2.2b.cmml" xref="S4.T9.3.3.3.m1.2.2"><cn type="integer" id="S4.T9.3.3.3.m1.2.2.2.cmml" xref="S4.T9.3.3.3.m1.2.2.2">4</cn></degree><ci id="S4.T9.3.3.3.m1.2.2.3.cmml" xref="S4.T9.3.3.3.m1.2.2.3">𝑛</ci></apply><apply id="S4.T9.3.3.3.m1.3.3.cmml" xref="S4.T9.3.3.3.m1.3.3"><root id="S4.T9.3.3.3.m1.3.3a.cmml" xref="S4.T9.3.3.3.m1.3.3"></root><ci id="S4.T9.3.3.3.m1.3.3.2.cmml" xref="S4.T9.3.3.3.m1.3.3.2">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.3.3.3.m1.4c">\tilde{O}(\min(\sqrt[4]{n},\sqrt{B}))</annotation></semantics></math></td>
<td id="S4.T9.4.4.4" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T9.4.4.4.m1.4" class="ltx_Math" alttext="\tilde{\Omega}(\min(\sqrt[4]{n},\sqrt{B}))" display="inline"><semantics id="S4.T9.4.4.4.m1.4a"><mrow id="S4.T9.4.4.4.m1.4.4" xref="S4.T9.4.4.4.m1.4.4.cmml"><mover accent="true" id="S4.T9.4.4.4.m1.4.4.3" xref="S4.T9.4.4.4.m1.4.4.3.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.4.4.4.m1.4.4.3.2" xref="S4.T9.4.4.4.m1.4.4.3.2.cmml">Ω</mi><mo mathsize="80%" id="S4.T9.4.4.4.m1.4.4.3.1" xref="S4.T9.4.4.4.m1.4.4.3.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.4.4.4.m1.4.4.2" xref="S4.T9.4.4.4.m1.4.4.2.cmml">​</mo><mrow id="S4.T9.4.4.4.m1.4.4.1.1" xref="S4.T9.4.4.4.m1.4.4.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.4.4.4.m1.4.4.1.1.2" xref="S4.T9.4.4.4.m1.4.4.cmml">(</mo><mrow id="S4.T9.4.4.4.m1.4.4.1.1.1.2" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml"><mi mathsize="80%" id="S4.T9.4.4.4.m1.1.1" xref="S4.T9.4.4.4.m1.1.1.cmml">min</mi><mo id="S4.T9.4.4.4.m1.4.4.1.1.1.2a" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml">⁡</mo><mrow id="S4.T9.4.4.4.m1.4.4.1.1.1.2.1" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.4.4.4.m1.4.4.1.1.1.2.1.1" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml">(</mo><mroot id="S4.T9.4.4.4.m1.2.2" xref="S4.T9.4.4.4.m1.2.2.cmml"><mi mathsize="80%" id="S4.T9.4.4.4.m1.2.2.3" xref="S4.T9.4.4.4.m1.2.2.3.cmml">n</mi><mn mathsize="80%" id="S4.T9.4.4.4.m1.2.2.2" xref="S4.T9.4.4.4.m1.2.2.2.cmml">4</mn></mroot><mo mathsize="80%" id="S4.T9.4.4.4.m1.4.4.1.1.1.2.1.2" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml">,</mo><msqrt id="S4.T9.4.4.4.m1.3.3" xref="S4.T9.4.4.4.m1.3.3.cmml"><mi mathsize="80%" id="S4.T9.4.4.4.m1.3.3.2" xref="S4.T9.4.4.4.m1.3.3.2.cmml">B</mi></msqrt><mo maxsize="80%" minsize="80%" id="S4.T9.4.4.4.m1.4.4.1.1.1.2.1.3" xref="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="80%" minsize="80%" id="S4.T9.4.4.4.m1.4.4.1.1.3" xref="S4.T9.4.4.4.m1.4.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.4.4.4.m1.4b"><apply id="S4.T9.4.4.4.m1.4.4.cmml" xref="S4.T9.4.4.4.m1.4.4"><times id="S4.T9.4.4.4.m1.4.4.2.cmml" xref="S4.T9.4.4.4.m1.4.4.2"></times><apply id="S4.T9.4.4.4.m1.4.4.3.cmml" xref="S4.T9.4.4.4.m1.4.4.3"><ci id="S4.T9.4.4.4.m1.4.4.3.1.cmml" xref="S4.T9.4.4.4.m1.4.4.3.1">~</ci><ci id="S4.T9.4.4.4.m1.4.4.3.2.cmml" xref="S4.T9.4.4.4.m1.4.4.3.2">Ω</ci></apply><apply id="S4.T9.4.4.4.m1.4.4.1.1.1.1.cmml" xref="S4.T9.4.4.4.m1.4.4.1.1.1.2"><min id="S4.T9.4.4.4.m1.1.1.cmml" xref="S4.T9.4.4.4.m1.1.1"></min><apply id="S4.T9.4.4.4.m1.2.2.cmml" xref="S4.T9.4.4.4.m1.2.2"><root id="S4.T9.4.4.4.m1.2.2a.cmml" xref="S4.T9.4.4.4.m1.2.2"></root><degree id="S4.T9.4.4.4.m1.2.2b.cmml" xref="S4.T9.4.4.4.m1.2.2"><cn type="integer" id="S4.T9.4.4.4.m1.2.2.2.cmml" xref="S4.T9.4.4.4.m1.2.2.2">4</cn></degree><ci id="S4.T9.4.4.4.m1.2.2.3.cmml" xref="S4.T9.4.4.4.m1.2.2.3">𝑛</ci></apply><apply id="S4.T9.4.4.4.m1.3.3.cmml" xref="S4.T9.4.4.4.m1.3.3"><root id="S4.T9.4.4.4.m1.3.3a.cmml" xref="S4.T9.4.4.4.m1.3.3"></root><ci id="S4.T9.4.4.4.m1.3.3.2.cmml" xref="S4.T9.4.4.4.m1.3.3.2">𝐵</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.4.4.4.m1.4c">\tilde{\Omega}(\min(\sqrt[4]{n},\sqrt{B}))</annotation></semantics></math></td>
<td id="S4.T9.5.5.5" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T9.5.5.5.m1.1" class="ltx_Math" alttext="\tilde{\Theta}(1)" display="inline"><semantics id="S4.T9.5.5.5.m1.1a"><mrow id="S4.T9.5.5.5.m1.1.2" xref="S4.T9.5.5.5.m1.1.2.cmml"><mover accent="true" id="S4.T9.5.5.5.m1.1.2.2" xref="S4.T9.5.5.5.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.5.5.5.m1.1.2.2.2" xref="S4.T9.5.5.5.m1.1.2.2.2.cmml">Θ</mi><mo mathsize="80%" id="S4.T9.5.5.5.m1.1.2.2.1" xref="S4.T9.5.5.5.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.5.5.5.m1.1.2.1" xref="S4.T9.5.5.5.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.5.5.5.m1.1.2.3.2" xref="S4.T9.5.5.5.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.5.5.5.m1.1.2.3.2.1" xref="S4.T9.5.5.5.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.5.5.5.m1.1.1" xref="S4.T9.5.5.5.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.5.5.5.m1.1.2.3.2.2" xref="S4.T9.5.5.5.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.5.5.5.m1.1b"><apply id="S4.T9.5.5.5.m1.1.2.cmml" xref="S4.T9.5.5.5.m1.1.2"><times id="S4.T9.5.5.5.m1.1.2.1.cmml" xref="S4.T9.5.5.5.m1.1.2.1"></times><apply id="S4.T9.5.5.5.m1.1.2.2.cmml" xref="S4.T9.5.5.5.m1.1.2.2"><ci id="S4.T9.5.5.5.m1.1.2.2.1.cmml" xref="S4.T9.5.5.5.m1.1.2.2.1">~</ci><ci id="S4.T9.5.5.5.m1.1.2.2.2.cmml" xref="S4.T9.5.5.5.m1.1.2.2.2">Θ</ci></apply><cn type="integer" id="S4.T9.5.5.5.m1.1.1.cmml" xref="S4.T9.5.5.5.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.5.5.5.m1.1c">\tilde{\Theta}(1)</annotation></semantics></math></td>
<td id="S4.T9.6.6.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t"><math id="S4.T9.6.6.6.m1.1" class="ltx_Math" alttext="\tilde{\Theta}(1)" display="inline"><semantics id="S4.T9.6.6.6.m1.1a"><mrow id="S4.T9.6.6.6.m1.1.2" xref="S4.T9.6.6.6.m1.1.2.cmml"><mover accent="true" id="S4.T9.6.6.6.m1.1.2.2" xref="S4.T9.6.6.6.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.6.6.6.m1.1.2.2.2" xref="S4.T9.6.6.6.m1.1.2.2.2.cmml">Θ</mi><mo mathsize="80%" id="S4.T9.6.6.6.m1.1.2.2.1" xref="S4.T9.6.6.6.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.6.6.6.m1.1.2.1" xref="S4.T9.6.6.6.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.6.6.6.m1.1.2.3.2" xref="S4.T9.6.6.6.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.6.6.6.m1.1.2.3.2.1" xref="S4.T9.6.6.6.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.6.6.6.m1.1.1" xref="S4.T9.6.6.6.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.6.6.6.m1.1.2.3.2.2" xref="S4.T9.6.6.6.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.6.6.6.m1.1b"><apply id="S4.T9.6.6.6.m1.1.2.cmml" xref="S4.T9.6.6.6.m1.1.2"><times id="S4.T9.6.6.6.m1.1.2.1.cmml" xref="S4.T9.6.6.6.m1.1.2.1"></times><apply id="S4.T9.6.6.6.m1.1.2.2.cmml" xref="S4.T9.6.6.6.m1.1.2.2"><ci id="S4.T9.6.6.6.m1.1.2.2.1.cmml" xref="S4.T9.6.6.6.m1.1.2.2.1">~</ci><ci id="S4.T9.6.6.6.m1.1.2.2.2.cmml" xref="S4.T9.6.6.6.m1.1.2.2.2">Θ</ci></apply><cn type="integer" id="S4.T9.6.6.6.m1.1.1.cmml" xref="S4.T9.6.6.6.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.6.6.6.m1.1c">\tilde{\Theta}(1)</annotation></semantics></math></td>
</tr>
<tr id="S4.T9.10.10" class="ltx_tr">
<td id="S4.T9.10.10.5" class="ltx_td ltx_align_justify">
<span id="S4.T9.10.10.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.10.10.5.1.1" class="ltx_p"><span id="S4.T9.10.10.5.1.1.1" class="ltx_text" style="font-size:80%;">Communication/user</span></span>
</span>
</td>
<td id="S4.T9.7.7.1" class="ltx_td ltx_align_center"><math id="S4.T9.7.7.1.m1.1" class="ltx_Math" alttext="\Theta(1)" display="inline"><semantics id="S4.T9.7.7.1.m1.1a"><mrow id="S4.T9.7.7.1.m1.1.2" xref="S4.T9.7.7.1.m1.1.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.7.7.1.m1.1.2.2" xref="S4.T9.7.7.1.m1.1.2.2.cmml">Θ</mi><mo lspace="0em" rspace="0em" id="S4.T9.7.7.1.m1.1.2.1" xref="S4.T9.7.7.1.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.7.7.1.m1.1.2.3.2" xref="S4.T9.7.7.1.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.7.7.1.m1.1.2.3.2.1" xref="S4.T9.7.7.1.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.7.7.1.m1.1.1" xref="S4.T9.7.7.1.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.7.7.1.m1.1.2.3.2.2" xref="S4.T9.7.7.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.7.7.1.m1.1b"><apply id="S4.T9.7.7.1.m1.1.2.cmml" xref="S4.T9.7.7.1.m1.1.2"><times id="S4.T9.7.7.1.m1.1.2.1.cmml" xref="S4.T9.7.7.1.m1.1.2.1"></times><ci id="S4.T9.7.7.1.m1.1.2.2.cmml" xref="S4.T9.7.7.1.m1.1.2.2">Θ</ci><cn type="integer" id="S4.T9.7.7.1.m1.1.1.cmml" xref="S4.T9.7.7.1.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.7.7.1.m1.1c">\Theta(1)</annotation></semantics></math></td>
<td id="S4.T9.10.10.6" class="ltx_td ltx_align_center"><span id="S4.T9.10.10.6.1" class="ltx_text" style="font-size:80%;">any</span></td>
<td id="S4.T9.8.8.2" class="ltx_td ltx_align_center"><math id="S4.T9.8.8.2.m1.1" class="ltx_Math" alttext="\tilde{\Theta}(1)" display="inline"><semantics id="S4.T9.8.8.2.m1.1a"><mrow id="S4.T9.8.8.2.m1.1.2" xref="S4.T9.8.8.2.m1.1.2.cmml"><mover accent="true" id="S4.T9.8.8.2.m1.1.2.2" xref="S4.T9.8.8.2.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.8.8.2.m1.1.2.2.2" xref="S4.T9.8.8.2.m1.1.2.2.2.cmml">Θ</mi><mo mathsize="80%" id="S4.T9.8.8.2.m1.1.2.2.1" xref="S4.T9.8.8.2.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.8.8.2.m1.1.2.1" xref="S4.T9.8.8.2.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.8.8.2.m1.1.2.3.2" xref="S4.T9.8.8.2.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.8.8.2.m1.1.2.3.2.1" xref="S4.T9.8.8.2.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.8.8.2.m1.1.1" xref="S4.T9.8.8.2.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.8.8.2.m1.1.2.3.2.2" xref="S4.T9.8.8.2.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.8.8.2.m1.1b"><apply id="S4.T9.8.8.2.m1.1.2.cmml" xref="S4.T9.8.8.2.m1.1.2"><times id="S4.T9.8.8.2.m1.1.2.1.cmml" xref="S4.T9.8.8.2.m1.1.2.1"></times><apply id="S4.T9.8.8.2.m1.1.2.2.cmml" xref="S4.T9.8.8.2.m1.1.2.2"><ci id="S4.T9.8.8.2.m1.1.2.2.1.cmml" xref="S4.T9.8.8.2.m1.1.2.2.1">~</ci><ci id="S4.T9.8.8.2.m1.1.2.2.2.cmml" xref="S4.T9.8.8.2.m1.1.2.2.2">Θ</ci></apply><cn type="integer" id="S4.T9.8.8.2.m1.1.1.cmml" xref="S4.T9.8.8.2.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.8.8.2.m1.1c">\tilde{\Theta}(1)</annotation></semantics></math></td>
<td id="S4.T9.10.10.7" class="ltx_td ltx_align_center"><span id="S4.T9.10.10.7.1" class="ltx_text" style="font-size:80%;">any</span></td>
<td id="S4.T9.9.9.3" class="ltx_td ltx_align_center"><math id="S4.T9.9.9.3.m1.1" class="ltx_Math" alttext="\tilde{\Theta}(1)" display="inline"><semantics id="S4.T9.9.9.3.m1.1a"><mrow id="S4.T9.9.9.3.m1.1.2" xref="S4.T9.9.9.3.m1.1.2.cmml"><mover accent="true" id="S4.T9.9.9.3.m1.1.2.2" xref="S4.T9.9.9.3.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.9.9.3.m1.1.2.2.2" xref="S4.T9.9.9.3.m1.1.2.2.2.cmml">Θ</mi><mo mathsize="80%" id="S4.T9.9.9.3.m1.1.2.2.1" xref="S4.T9.9.9.3.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.9.9.3.m1.1.2.1" xref="S4.T9.9.9.3.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.9.9.3.m1.1.2.3.2" xref="S4.T9.9.9.3.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.9.9.3.m1.1.2.3.2.1" xref="S4.T9.9.9.3.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.9.9.3.m1.1.1" xref="S4.T9.9.9.3.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.9.9.3.m1.1.2.3.2.2" xref="S4.T9.9.9.3.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.9.9.3.m1.1b"><apply id="S4.T9.9.9.3.m1.1.2.cmml" xref="S4.T9.9.9.3.m1.1.2"><times id="S4.T9.9.9.3.m1.1.2.1.cmml" xref="S4.T9.9.9.3.m1.1.2.1"></times><apply id="S4.T9.9.9.3.m1.1.2.2.cmml" xref="S4.T9.9.9.3.m1.1.2.2"><ci id="S4.T9.9.9.3.m1.1.2.2.1.cmml" xref="S4.T9.9.9.3.m1.1.2.2.1">~</ci><ci id="S4.T9.9.9.3.m1.1.2.2.2.cmml" xref="S4.T9.9.9.3.m1.1.2.2.2">Θ</ci></apply><cn type="integer" id="S4.T9.9.9.3.m1.1.1.cmml" xref="S4.T9.9.9.3.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.9.9.3.m1.1c">\tilde{\Theta}(1)</annotation></semantics></math></td>
<td id="S4.T9.10.10.4" class="ltx_td ltx_nopad_r ltx_align_center"><math id="S4.T9.10.10.4.m1.1" class="ltx_Math" alttext="\tilde{\Theta}(1)" display="inline"><semantics id="S4.T9.10.10.4.m1.1a"><mrow id="S4.T9.10.10.4.m1.1.2" xref="S4.T9.10.10.4.m1.1.2.cmml"><mover accent="true" id="S4.T9.10.10.4.m1.1.2.2" xref="S4.T9.10.10.4.m1.1.2.2.cmml"><mi mathsize="80%" mathvariant="normal" id="S4.T9.10.10.4.m1.1.2.2.2" xref="S4.T9.10.10.4.m1.1.2.2.2.cmml">Θ</mi><mo mathsize="80%" id="S4.T9.10.10.4.m1.1.2.2.1" xref="S4.T9.10.10.4.m1.1.2.2.1.cmml">~</mo></mover><mo lspace="0em" rspace="0em" id="S4.T9.10.10.4.m1.1.2.1" xref="S4.T9.10.10.4.m1.1.2.1.cmml">​</mo><mrow id="S4.T9.10.10.4.m1.1.2.3.2" xref="S4.T9.10.10.4.m1.1.2.cmml"><mo maxsize="80%" minsize="80%" id="S4.T9.10.10.4.m1.1.2.3.2.1" xref="S4.T9.10.10.4.m1.1.2.cmml">(</mo><mn mathsize="80%" id="S4.T9.10.10.4.m1.1.1" xref="S4.T9.10.10.4.m1.1.1.cmml">1</mn><mo maxsize="80%" minsize="80%" id="S4.T9.10.10.4.m1.1.2.3.2.2" xref="S4.T9.10.10.4.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.10.10.4.m1.1b"><apply id="S4.T9.10.10.4.m1.1.2.cmml" xref="S4.T9.10.10.4.m1.1.2"><times id="S4.T9.10.10.4.m1.1.2.1.cmml" xref="S4.T9.10.10.4.m1.1.2.1"></times><apply id="S4.T9.10.10.4.m1.1.2.2.cmml" xref="S4.T9.10.10.4.m1.1.2.2"><ci id="S4.T9.10.10.4.m1.1.2.2.1.cmml" xref="S4.T9.10.10.4.m1.1.2.2.1">~</ci><ci id="S4.T9.10.10.4.m1.1.2.2.2.cmml" xref="S4.T9.10.10.4.m1.1.2.2.2">Θ</ci></apply><cn type="integer" id="S4.T9.10.10.4.m1.1.1.cmml" xref="S4.T9.10.10.4.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.10.10.4.m1.1c">\tilde{\Theta}(1)</annotation></semantics></math></td>
</tr>
<tr id="S4.T9.10.12.2" class="ltx_tr">
<td id="S4.T9.10.12.2.1" class="ltx_td ltx_align_justify ltx_border_bb">
<span id="S4.T9.10.12.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T9.10.12.2.1.1.1" class="ltx_p"><span id="S4.T9.10.12.2.1.1.1.1" class="ltx_text" style="font-size:80%;">References</span></span>
</span>
</td>
<td id="S4.T9.10.12.2.2" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.2.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">54</span></a><span id="S4.T9.10.12.2.2.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T9.10.12.2.3" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.3.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a><span id="S4.T9.10.12.2.3.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T9.10.12.2.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S4.T9.10.12.2.4.1" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.4.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib475" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">475</span></a>, <a href="#bib.bib178" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">178</span></a>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a><span id="S4.T9.10.12.2.4.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T9.10.12.2.5" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.5.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib200" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">200</span></a><span id="S4.T9.10.12.2.5.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T9.10.12.2.6" class="ltx_td ltx_align_center ltx_border_bb"><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.6.1.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib200" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">200</span></a><span id="S4.T9.10.12.2.6.2.2" class="ltx_text" style="font-size:80%;">]</span></cite></td>
<td id="S4.T9.10.12.2.7" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">
<span id="S4.T9.10.12.2.7.1" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T9.10.12.2.7.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib339" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">339</span></a>, <a href="#bib.bib433" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">433</span></a><span id="S4.T9.10.12.2.7.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 9: </span>Upper and lower bounds on the expected maximum error for <em id="S4.T9.36.1" class="ltx_emph ltx_font_italic">frequency estimation</em> on domains of size <math id="S4.T9.18.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.T9.18.m1.1b"><mi id="S4.T9.18.m1.1.1" xref="S4.T9.18.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.T9.18.m1.1c"><ci id="S4.T9.18.m1.1.1.cmml" xref="S4.T9.18.m1.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.18.m1.1d">B</annotation></semantics></math> and over <math id="S4.T9.19.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T9.19.m2.1b"><mi id="S4.T9.19.m2.1.1" xref="S4.T9.19.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T9.19.m2.1c"><ci id="S4.T9.19.m2.1.1.cmml" xref="S4.T9.19.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.19.m2.1d">n</annotation></semantics></math> users in different models of DP. The bounds are stated for fixed, positive privacy parameters <math id="S4.T9.20.m3.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.T9.20.m3.1b"><mi id="S4.T9.20.m3.1.1" xref="S4.T9.20.m3.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.T9.20.m3.1c"><ci id="S4.T9.20.m3.1.1.cmml" xref="S4.T9.20.m3.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.20.m3.1d">\varepsilon</annotation></semantics></math> and <math id="S4.T9.21.m4.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.T9.21.m4.1b"><mi id="S4.T9.21.m4.1.1" xref="S4.T9.21.m4.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.T9.21.m4.1c"><ci id="S4.T9.21.m4.1.1.cmml" xref="S4.T9.21.m4.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.21.m4.1d">\delta</annotation></semantics></math>, and <math id="S4.T9.22.m5.1" class="ltx_Math" alttext="\tilde{\Theta}/\tilde{O}/\tilde{\Omega}" display="inline"><semantics id="S4.T9.22.m5.1b"><mrow id="S4.T9.22.m5.1.1" xref="S4.T9.22.m5.1.1.cmml"><mover accent="true" id="S4.T9.22.m5.1.1.2" xref="S4.T9.22.m5.1.1.2.cmml"><mi mathvariant="normal" id="S4.T9.22.m5.1.1.2.2" xref="S4.T9.22.m5.1.1.2.2.cmml">Θ</mi><mo id="S4.T9.22.m5.1.1.2.1" xref="S4.T9.22.m5.1.1.2.1.cmml">~</mo></mover><mo id="S4.T9.22.m5.1.1.1" xref="S4.T9.22.m5.1.1.1.cmml">/</mo><mover accent="true" id="S4.T9.22.m5.1.1.3" xref="S4.T9.22.m5.1.1.3.cmml"><mi id="S4.T9.22.m5.1.1.3.2" xref="S4.T9.22.m5.1.1.3.2.cmml">O</mi><mo id="S4.T9.22.m5.1.1.3.1" xref="S4.T9.22.m5.1.1.3.1.cmml">~</mo></mover><mo id="S4.T9.22.m5.1.1.1b" xref="S4.T9.22.m5.1.1.1.cmml">/</mo><mover accent="true" id="S4.T9.22.m5.1.1.4" xref="S4.T9.22.m5.1.1.4.cmml"><mi mathvariant="normal" id="S4.T9.22.m5.1.1.4.2" xref="S4.T9.22.m5.1.1.4.2.cmml">Ω</mi><mo id="S4.T9.22.m5.1.1.4.1" xref="S4.T9.22.m5.1.1.4.1.cmml">~</mo></mover></mrow><annotation-xml encoding="MathML-Content" id="S4.T9.22.m5.1c"><apply id="S4.T9.22.m5.1.1.cmml" xref="S4.T9.22.m5.1.1"><divide id="S4.T9.22.m5.1.1.1.cmml" xref="S4.T9.22.m5.1.1.1"></divide><apply id="S4.T9.22.m5.1.1.2.cmml" xref="S4.T9.22.m5.1.1.2"><ci id="S4.T9.22.m5.1.1.2.1.cmml" xref="S4.T9.22.m5.1.1.2.1">~</ci><ci id="S4.T9.22.m5.1.1.2.2.cmml" xref="S4.T9.22.m5.1.1.2.2">Θ</ci></apply><apply id="S4.T9.22.m5.1.1.3.cmml" xref="S4.T9.22.m5.1.1.3"><ci id="S4.T9.22.m5.1.1.3.1.cmml" xref="S4.T9.22.m5.1.1.3.1">~</ci><ci id="S4.T9.22.m5.1.1.3.2.cmml" xref="S4.T9.22.m5.1.1.3.2">𝑂</ci></apply><apply id="S4.T9.22.m5.1.1.4.cmml" xref="S4.T9.22.m5.1.1.4"><ci id="S4.T9.22.m5.1.1.4.1.cmml" xref="S4.T9.22.m5.1.1.4.1">~</ci><ci id="S4.T9.22.m5.1.1.4.2.cmml" xref="S4.T9.22.m5.1.1.4.2">Ω</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.22.m5.1d">\tilde{\Theta}/\tilde{O}/\tilde{\Omega}</annotation></semantics></math> asymptotic notation suppresses factors that are polylogarithmic in <math id="S4.T9.23.m6.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.T9.23.m6.1b"><mi id="S4.T9.23.m6.1.1" xref="S4.T9.23.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.T9.23.m6.1c"><ci id="S4.T9.23.m6.1.1.cmml" xref="S4.T9.23.m6.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.23.m6.1d">B</annotation></semantics></math> and <math id="S4.T9.24.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T9.24.m7.1b"><mi id="S4.T9.24.m7.1.1" xref="S4.T9.24.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T9.24.m7.1c"><ci id="S4.T9.24.m7.1.1.cmml" xref="S4.T9.24.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T9.24.m7.1d">n</annotation></semantics></math>. The communication per user is in terms of the total number of bits sent. In all upper bounds, the protocol is symmetric with respect to the users, and no public randomness is needed. References are to the first results we are aware of that imply the stated bounds.
</figcaption>
</figure>
</section>
<section id="S4.SS4.SSS3.Px1.SPx2" class="ltx_subparagraph">
<h6 class="ltx_title ltx_title_subparagraph">Trade-offs for secure aggregation</h6>

<div id="S4.SS4.SSS3.Px1.SPx2.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px1.SPx2.p1.40" class="ltx_p">It would be very interesting to investigate the following similar question for secure aggregation. Consider an FL round with <math id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.1.m1.1c">n</annotation></semantics></math> users and assume that user <math id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.2.m2.1c">i</annotation></semantics></math> holds a value <math id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.3.m3.1c">x_{i}</annotation></semantics></math>. User <math id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.4.m4.1c">i</annotation></semantics></math> applies an algorithm <math id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.5.m5.1c">\mathcal{A}(\cdot)</annotation></semantics></math> to <math id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.6.m6.1c">x_{i}</annotation></semantics></math> to obtain <math id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1" class="ltx_Math" alttext="y_{i}=\mathcal{A}(x_{i})" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.2.cmml">y</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.3.cmml">i</mi></msub><mo id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.2.cmml">=</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.2.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1"><eq id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.2"></eq><apply id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.2">𝑦</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.3.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1"><times id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.2"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.3">𝒜</ci><apply id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.7.m7.1c">y_{i}=\mathcal{A}(x_{i})</annotation></semantics></math>; here, <math id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.8.m8.1c">\mathcal{A}(\cdot)</annotation></semantics></math> can be thought of as both a compression and privatization scheme. Using secure aggregation as a black box, the service provider observes <math id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1" class="ltx_Math" alttext="\bar{y}=\sum_{i}\mathcal{A}(x_{i})" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.cmml"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.2.cmml">y</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.1.cmml">¯</mo></mover><mo rspace="0.111em" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.2.cmml">=</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.cmml"><mo id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.2.cmml">∑</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.3.cmml">i</mi></msub><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1"><eq id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.2"></eq><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.3.2">𝑦</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2">subscript</csymbol><sum id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.2"></sum><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1"><times id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.2"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.3">𝒜</ci><apply id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.9.m9.1c">\bar{y}=\sum_{i}\mathcal{A}(x_{i})</annotation></semantics></math> and uses <math id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1" class="ltx_Math" alttext="\bar{y}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1a"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.2.cmml">y</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.10.m10.1c">\bar{y}</annotation></semantics></math> to estimate <math id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1" class="ltx_Math" alttext="\bar{x}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1a"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.2.cmml">x</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.11.m11.1c">\bar{x}</annotation></semantics></math>, the true sum of the <math id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1" class="ltx_Math" alttext="x_{i}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.12.m12.1c">x_{i}</annotation></semantics></math>’s, by computing <math id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1" class="ltx_math_unparsed" alttext="\hat{\bar{x}}=g(\bar{y}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1b"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.1"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.1.2"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.1.2.2">x</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.1.2.1">¯</mo></mover><mo id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.1.1">^</mo></mover><mo id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.2">=</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.3">g</mi><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.4"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.4.1">(</mo><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.4.2"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.4.2.2">y</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1.4.2.1">¯</mo></mover></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.13.m13.1c">\hat{\bar{x}}=g(\bar{y}</annotation></semantics></math>) for some function <math id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1" class="ltx_Math" alttext="g(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.2.2">𝑔</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.14.m14.1c">g(\cdot)</annotation></semantics></math>. Ideally, we would like to design <math id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.15.m15.1c">\mathcal{A}(\cdot)</annotation></semantics></math>, <math id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1" class="ltx_Math" alttext="g(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.2.2">𝑔</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.16.m16.1c">g(\cdot)</annotation></semantics></math> in a way that minimizes the error in estimating <math id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1" class="ltx_Math" alttext="\bar{x}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1a"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.2.cmml">x</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.17.m17.1c">\bar{x}</annotation></semantics></math>; formally, we would like to solve the optimization problem <math id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3" class="ltx_Math" alttext="\min_{g,\mathcal{A}}\|g(\sum_{i}\mathcal{A}(x_{i}))-\sum_{i}x_{i}\|" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.2.cmml">min</mi><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.4" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.3.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.1.1.1.1.cmml">g</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.4.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.3.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.2.cmml">𝒜</mi></mrow></msub><mo id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3a" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.cmml">⁡</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.2.1.cmml">‖</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.cmml"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.cmml"><mo lspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.2.cmml">∑</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.055em" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.2.cmml">−</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.cmml"><mo id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.2.cmml">∑</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.3.cmml">i</mi></msub><msub id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.2.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2">subscript</csymbol><min id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.2.2"></min><list id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.4"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.1.1.1.1">𝑔</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.2.2.2.2">𝒜</ci></list></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1"><csymbol cd="latexml" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.2">norm</csymbol><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1"><minus id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.2"></minus><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1"><times id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.2"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.3">𝑔</ci><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.2"></sum><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1"><times id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.2"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.3">𝒜</ci><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1">subscript</csymbol><sum id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.2"></sum><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.1.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3.3.1.1.1.3.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.18.m18.3c">\min_{g,\mathcal{A}}\|g(\sum_{i}\mathcal{A}(x_{i}))-\sum_{i}x_{i}\|</annotation></semantics></math>, where <math id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1" class="ltx_math_unparsed" alttext="\|.\|" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1b"><mo rspace="0.0835em" id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1.1">∥</mo><mo lspace="0.0835em" rspace="0.0835em" id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1.2">.</mo><mo lspace="0.0835em" id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1.3">∥</mo></mrow><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.19.m19.1c">\|.\|</annotation></semantics></math> can be either the <math id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.20.m20.1c">\ell_{1}</annotation></semantics></math> or <math id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.21.m21.1c">\ell_{2}</annotation></semantics></math> norm. Of course, without enforcing any constraints on <math id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1" class="ltx_math_unparsed" alttext="g(.)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1b"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1.1">g</mi><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1.2"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1.2.1">(</mo><mo lspace="0em" rspace="0.167em" id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1.2.2">.</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1.2.3">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.22.m22.1c">g(.)</annotation></semantics></math> and <math id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.23.m23.1c">\mathcal{A}(\cdot)</annotation></semantics></math>, we can always choose them to be the identity function and get <math id="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1a"><mn id="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1b"><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.24.m24.1.1">0</cn></annotation-xml></semantics></math> error. However, <math id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.25.m25.1c">\mathcal{A}(\cdot)</annotation></semantics></math> has to satisfy two constraints: (1) <math id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1" class="ltx_Math" alttext="\mathcal{A}(\cdot)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.2.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.1.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.1.cmml">⋅</mo><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2"><times id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.1"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.2.2">𝒜</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.26.m26.1c">\mathcal{A}(\cdot)</annotation></semantics></math> should output <math id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.27.m27.1c">B</annotation></semantics></math> bits (which can be thought of as the communication cost per user), and (2) <math id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1" class="ltx_Math" alttext="\bar{y}=\sum_{i}\mathcal{A}(x_{i})" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.cmml"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.2.cmml">y</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.1.cmml">¯</mo></mover><mo rspace="0.111em" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.2.cmml">=</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.cmml"><mo id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.2.cmml">∑</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.3.cmml">i</mi></msub><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.2.cmml">​</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1"><eq id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.2"></eq><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.3.2">𝑦</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2">subscript</csymbol><sum id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.2"></sum><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1"><times id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.2"></times><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.3">𝒜</ci><apply id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.28.m28.1c">\bar{y}=\sum_{i}\mathcal{A}(x_{i})</annotation></semantics></math> should be an <math id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.1.cmml"><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.1.cmml">(</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.1.1.cmml">ε</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.1.cmml">,</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2b"><interval closure="open" id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.3.2"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.1.1">𝜀</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.29.m29.2c">(\varepsilon,\delta)</annotation></semantics></math>-DP version of <math id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1" class="ltx_Math" alttext="\bar{x}=\sum_{i}x_{i}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1a"><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.cmml"><mover accent="true" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.2.cmml">x</mi><mo id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.1.cmml">¯</mo></mover><mo rspace="0.111em" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.1.cmml">=</mo><mrow id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.cmml"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.cmml"><mo id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.2.cmml">∑</mo><mi id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.3.cmml">i</mi></msub><msub id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.cmml"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.2.cmml">x</mi><mi id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1"><eq id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.1"></eq><apply id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.1">¯</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.2.2">𝑥</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1">subscript</csymbol><sum id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.2"></sum><ci id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.1.3">𝑖</ci></apply><apply id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.2">𝑥</ci><ci id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.30.m30.1c">\bar{x}=\sum_{i}x_{i}</annotation></semantics></math>. Thus, the fundamental problem of interest is to identify the optimal algorithm <math id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.31.m31.1c">\mathcal{A}</annotation></semantics></math> that achieves DP upon aggregation while also satisfying a fixed communication budget. Looking at the problem differently, for a fixed <math id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.32.m32.1c">n</annotation></semantics></math>, <math id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.33.m33.1c">B</annotation></semantics></math>, <math id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1" class="ltx_Math" alttext="\varepsilon" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1.1.cmml">ε</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1.1">𝜀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.34.m34.1c">\varepsilon</annotation></semantics></math>, and <math id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1a"><mi id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.35.m35.1c">\delta</annotation></semantics></math>, what is the smallest <math id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.36.m36.1c">\ell_{1}</annotation></semantics></math> or <math id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.37.m37.1c">\ell_{2}</annotation></semantics></math> error that we can hope to achieve? We note that the work of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Agarwal et al.</span> [<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> provides one candidate algorithm <math id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1b"><ci id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.38.m38.1c">\mathcal{A}</annotation></semantics></math> based on uniform quantization and binomial noise addition. Yet another solution was recently presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib256" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">256</span></a>]</cite> which involves rotating, scaling, and discretizing the data, then adding discrete Gaussian noise before performing modular clipping and secure aggregation. While the sum of independent discrete Gaussians is not a discrete Gaussian, the authors show that it is close enough and present tight DP guarantees and experimental results, demonstrating that their solution is able to achieve a comparable accuracy to central DP via continuous Gaussian noise with 16 (or less) bits of precision per value. However, it is unclear if this approach achieves the optimal communication, privacy, and accuracy tradeoffs. Therefore, it is of fundamental interest to derive lower bounds and matching upper bounds on the <math id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1" class="ltx_Math" alttext="\ell_{1}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.39.m39.1c">\ell_{1}</annotation></semantics></math> or <math id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1a"><msub id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.cmml"><mi mathvariant="normal" id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.2" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.2.cmml">ℓ</mi><mn id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.3" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1b"><apply id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.1.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1">subscript</csymbol><ci id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.2.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.2">ℓ</ci><cn type="integer" id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.3.cmml" xref="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS3.Px1.SPx2.p1.40.m40.1c">\ell_{2}</annotation></semantics></math> error under the above constraints.</p>
</div>
</section>
</section>
<section id="S4.SS4.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Privacy accounting</h5>

<div id="S4.SS4.SSS3.Px2.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px2.p1.1" class="ltx_p">In the central model of DP, the subsampled Gaussian mechanism is often used to achieve DP, and the privacy budget is tightly tracked across rounds of FL using the moments accountant method (see discussion in <a href="#S4.SS3" title="4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>). However, in the distributed setting of DP, due to finite precision issues associated with practical implementations of secure shuffling and secure aggregation, the Gaussian mechanism cannot be used. Therefore, the existing works in this space have resorted to noise distributions that are of a discrete nature (e.g. adding Bernoulli or binomial noise). While such distributions help in addressing the finite precision constraints imposed by the underlying implementation of secure shuffling/aggregation, they do not naturally benefit from the moments accountant method. Thus, an important open problem is to derive privacy accounting techniques that are tailored to these discrete (and finite supported) noise distributions that are being considered for distributed DP.</p>
</div>
</section>
<section id="S4.SS4.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Handling client dropouts.</h5>

<div id="S4.SS4.SSS3.Px3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px3.p1.1" class="ltx_p">The above model of distributed DP assumes that participating clients remain connected to the server during a round. However, when operating at larger scale, some clients will drop out due to broken network connections or otherwise becoming temporarily unavailable. This requires the distributed noise generation mechanism to be robust against such dropouts and also affects scaling federated learning and analytics to larger numbers of participating clients.</p>
</div>
<div id="S4.SS4.SSS3.Px3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.Px3.p2.1" class="ltx_p">In terms of robust distributed noise, clients dropping out could lead too little noise being added to meet the differential privacy epsilon target. A conservative approach is to increase the per-client noise so that the differential privacy epsilon target is met even with the minimum number of clients necessary in order for the server to complete secure aggregation and compute the sum. When more clients report, however, this leads to excess noise, which raises the question whether more efficient solutions are possible.</p>
</div>
<div id="S4.SS4.SSS3.Px3.p3" class="ltx_para">
<p id="S4.SS4.SSS3.Px3.p3.1" class="ltx_p">In terms of scaling, the number of dropped out clients becomes a bottleneck when increasing the number of clients that participate in a secure aggregation round. It may also be challenging to gather enough clients at the same time. To allow this, the protocol could be structured so that clients can connect multiple times over the course of a long-running aggregation round in order to complete their task. More generally, the problem of operating at scale when clients are likely to be intermittently available has not been systematically addressed yet in the literature.</p>
</div>
</section>
<section id="S4.SS4.SSS3.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">New trust models</h5>

<div id="S4.SS4.SSS3.Px4.p1" class="ltx_para">
<p id="S4.SS4.SSS3.Px4.p1.1" class="ltx_p">The federated learning framework motivates the development of new, more refined trust models than those previously used, taking advantage of federated learning’s unique computational model, and perhaps placing realistic assumptions on the capabilities of adversarial users.
For example, what is a reasonable fraction of clients to assume might be compromised by an adversary? Is it likely for an adversary to be able to compromise both the server and a large number of devices, or is it typically sufficient to assume that the adversary can only compromise one or the other? In federated learning, the server is often operated by a well-known entity, such a long-living organization. Can this be leveraged to enact a trust model where the server’s behavior is trusted-but-verified, i.e. wherein the server is not prevented from deviating from the desired protocol, but is extremely likely to be detected if it does (thereby damaging the trust, reputation, and potentially financial or legal status of the hosting organization)?</p>
</div>
</section>
</section>
<section id="S4.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.4 </span>Preserving Privacy While Training Sub-Models</h4>

<div id="S4.SS4.SSS4.p1" class="ltx_para">
<p id="S4.SS4.SSS4.p1.1" class="ltx_p">Many scenarios arise in which each client may have local data that is only relevant to a relatively small portion of the full model being trained. For example, models that operate over large inventories, including natural language models (operating over an inventory of words) or content ranking models (operating over an inventory of content), frequently use an embedding lookup table as the first layer of the neural network. Often, clients only interact with a tiny fraction of the inventory items, and under many training strategies, the only embedding vectors for which a client’s data supports updates are those corresponding to the items with which the client interacted.</p>
</div>
<div id="S4.SS4.SSS4.p2" class="ltx_para">
<p id="S4.SS4.SSS4.p2.1" class="ltx_p">As another example, multi-task learning strategies can be effective approaches to personalization, but may give rise to compound models wherein any particular client only uses the submodel that is associated with that client’s cluster of users, as described in <a href="#S3.SS3.SSS2" title="3.3.2 Multi-Task Learning ‣ 3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3.2</span></a>.</p>
</div>
<div id="S4.SS4.SSS4.p3" class="ltx_para">
<p id="S4.SS4.SSS4.p3.1" class="ltx_p">If communication efficiency is not a concern, then sub-model training looks just like standard federated learning: clients would download the full model when they participate, make use of the sub-model relevant to them, then submit a model update spanning the entire set of model parameters (i.e. with zeroes everywhere except in the entries corresponding to the relevant sub-model). However, when deploying federated learning, communication efficiency is often a significant concern, leading to the question of whether we can achieve communication-efficient sub-model training.</p>
</div>
<div id="S4.SS4.SSS4.p4" class="ltx_para">
<p id="S4.SS4.SSS4.p4.1" class="ltx_p">If no privacy-sensitive information goes into the choice of which particular sub-model that a client will update, then there may be straight-forward ways to adapt federated learning to achieve communication-efficient sub-model training. For example, one could run multiple copies of the federated learning procedure, one per submodel, either in parallel (e.g. clients choose the appropriate federated learning instance to participate in, based on the sub-model they wish to update), in sequence (e.g. for each round of FL, the server advertises which submodel will be updated), or in a hybrid of the two. However, while this approach is communication efficient, the server gets to observe which submodel a client selects.</p>
</div>
<div id="S4.SS4.SSS4.p5" class="ltx_para">
<p id="S4.SS4.SSS4.p5.1" class="ltx_p">Is it possible to achieve communication-efficient sub-model federated learning while also keeping the client’s sub-model choice private? One promising approach is to use PIR for private sub-model download, while aggregating model updates using a variant of secure aggregation optimized for sparse vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">105</span></a>, <a href="#bib.bib249" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">249</span></a>, <a href="#bib.bib360" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">360</span></a>]</cite>.</p>
</div>
<div id="S4.SS4.SSS4.p6" class="ltx_para">
<p id="S4.SS4.SSS4.p6.1" class="ltx_p">Open problems in this area include characterizing the sparsity regimes associated with sub-model training problems of practical interest and developing of sparse secure aggregation techniques that are communication efficient in these sparsity regimes. It is also an open question whether private information retrieval (PIR) and secure aggregation might be co-optimized to achieve better communication efficiency than simply having each technology operate independently (e.g. by sharing some costs between the implementations of the two functionalities.)</p>
</div>
<div id="S4.SS4.SSS4.p7" class="ltx_para">
<p id="S4.SS4.SSS4.p7.1" class="ltx_p">Some forms of local and distributed differential privacy also pose challenges here, in that noise is often added to all elements of the vector, even those that are zero; as a result, adding this noise on each client would transform an otherwise sparse model update (i.e. non-zero only on the submodel) into a dense privatized model update (non-zero almost everywhere with high probability). It is an open question whether this tension can be resolved, i.e. whether there is a meaningful instantiation of distributed differential privacy that also maintains the sparsity of the model updates.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>User Perception</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Federated learning embodies principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks. However, as discussed above, it is important to be clear about the protections it does (and does not) provide and the technologies that can be used to provide protections against the threat models laid out in Section <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. While the previous sections focused on rigorous quantification of privacy against precise threat models, this section focuses on challenges around the users’ perception and needs.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">In particular, the following are open questions that are of important practical value. Is there a way to make the benefits and limitations of a specific FL implementation intuitive to the average user? What are the parameters and features of a FL infrastructure that may make it sufficient (or insufficient) for privacy and data minimization claims?
Might federated learning give users a false sense of privacy? How do we enable users to feel safe and actually be safe as they learn more about what is happening with their data? Do users value different aspects of privacy differently? What about facts that people want to protect? Would knowing these things enable us to design better mechanism? Are there ways to model people’s privacy preferences well enough to decide how to set these parameters? Who gets to decide which techniques to use if there are different utility/privacy/security properties from different techniques? Just the service provider? Or also the user? Or their operating system? Their political jurisdiction? Is there a role for mechanisms like “Privacy for the Protected (Only)” <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib267" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">267</span></a>]</cite> that provide privacy guarantees for most users while allowing targeted surveillance for societal priorities such as counter-terrorism? Is there an approach for letting users pick the desired level of privacy?</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Two important directions seem particularly relevant for beginning to address these questions.</p>
</div>
<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Understanding Privacy Needs for Particular Analysis Tasks</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">Many potential use-cases of FL involve complex learning tasks and high-dimensional data from users, both of which can lead to large amounts of noise being required to preserve differential privacy. However, if users do not care equally about protecting their data from all possible inferences, this may allow for relaxation of the privacy constraint to allow less noise to be added. For example, consider the data generated by a smart home thermostat that is programmed to turn off when a house is empty, and turn on when the residents return home. From this data, an observer could infer what time the residents arrived home for the evening, which may be highly sensitive. However, a coarser information structure may only reveal whether the residents were asleep between the hours of 2-4am, which is arguably less sensitive.</p>
</div>
<div id="S4.SS5.SSS1.p2" class="ltx_para">
<p id="S4.SS5.SSS1.p2.1" class="ltx_p">This approach is formalized in the Pufferfish framework of privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib271" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">271</span></a>]</cite>, which allows the analyst to specify a class of protected predicates that must be learned subject to the guarantees of differential privacy, and all other predicates can be learned without differential privacy. For this approach to provide satisfactory privacy guarantees in practice, the analyst must understand the users’ privacy needs to their particular analysis task and data collection procedure. The federated learning framework could be modified to allow individual users to specify what inferences they allow and disallow. These data restrictions could either be processed on device, with only “allowable” information being shared with the server in the FL model update step, or can be done as part of the aggregation step once data have been collected. Further work should be done to develop technical tools for incorporating such user preferences into the FL model, and to develop techniques for meaningful preference elicitation from users.</p>
</div>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Behavioral Research to Elicit Privacy Preferences</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">Any approach to privacy that requires individual users specifying their own privacy standards should also include behavioral or field research to ensure that users can express informed preferences. This should include both an <em id="S4.SS5.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">educational component</em> and <em id="S4.SS5.SSS2.p1.1.2" class="ltx_emph ltx_font_italic">preference measurement</em>.</p>
</div>
<div id="S4.SS5.SSS2.p2" class="ltx_para">
<p id="S4.SS5.SSS2.p2.1" class="ltx_p">The educational component should measure and improve user understanding of the privacy technology being used (e.g., Section <a href="#S4.SS2" title="4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>) and the details of data use. For applications involving federated learning, this should also include explanations of federated learning and exactly what data will be sent to the server. Once the educational component of the research has verified that typical users can meaningfully understand the privacy guarantees offered by a private learning process, then researchers can begin preference elicitation. This can occur either in behavioral labs, large-scale field experiments, or small focus groups. Care should be exercised to ensure that the individuals providing data on their preferences are both informed enough to provide high quality data and are representative of the target population.</p>
</div>
<div id="S4.SS5.SSS2.p3" class="ltx_para">
<p id="S4.SS5.SSS2.p3.1" class="ltx_p">While the rich field of behavioral and experimental economics have long shown that people behave differently in public versus private conditions (that is, when their choices are observed by others or not), very little behavioral work has been done on eliciting preferences for differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">144</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>. Extending this line of work will be a critical step towards widespread future implementations of private federated learning. Results from the educational component will prove useful here in ensuring that study participants are fully informed and understand the decisions they are facing. It should be an important tenant of these experiments that they are performed ethically and that no deception is involved.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Executive Summary</h3>

<div id="S4.SS6.p1" class="ltx_para">
<ul id="S4.I5" class="ltx_itemize">
<li id="S4.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I5.i1.p1" class="ltx_para">
<p id="S4.I5.i1.p1.1" class="ltx_p">Preserving the privacy of user data requires considering both <span id="S4.I5.i1.p1.1.1" class="ltx_text ltx_font_italic">what</span> function of the data is being computed and <span id="S4.I5.i1.p1.1.2" class="ltx_text ltx_font_italic">how</span> the computation is executed (and in particular, who can see/influence intermediate results). [Section <a href="#S4.SS2" title="4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>]</p>
<ul id="S4.I5.i1.I1" class="ltx_itemize">
<li id="S4.I5.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i1.I1.i1.p1" class="ltx_para">
<p id="S4.I5.i1.I1.i1.p1.1" class="ltx_p">Techniques for addressing the “<span id="S4.I5.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">what</span>” include data minimization and differential privacy. [Sections <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, <a href="#S4.SS3.SSS2" title="4.3.2 Training with Central Differential Privacy ‣ 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>]. It remains an important open challenge how best to adapt differential privacy accounting and privatization techniques to real world deployments, including the training of numerous machine learning models over overlapping populations, with time-evolving data, by multiple independent actors, and in the context of real-world non-determinancies such as client availability, all without rapidly depleting the privacy budget and while maintaining high utility.</p>
</div>
</li>
<li id="S4.I5.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i1.I1.i2.p1" class="ltx_para">
<p id="S4.I5.i1.I1.i2.p1.1" class="ltx_p">Techniques for addressing the “<span id="S4.I5.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">how</span>” include secure multi-party computation (MPC), homomorphic encryption(HE), and trusted execution environments (TEEs). While practical techniques MPC techniques for some federation-crucial functionalities have been deployed at scale, many important functionalities remain far more communication- and computation-expensive than their insecure counterparts. Meanwhile, it remains an open challenge to produce a reliably exploit-immune TEE platform, and the supporting infrastructure and processes to connect attested binaries to specific privacy properties is still immature. [Section <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>]</p>
</div>
</li>
<li id="S4.I5.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i1.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i1.I1.i3.p1" class="ltx_para">
<p id="S4.I5.i1.I1.i3.p1.1" class="ltx_p">Techniques should be composed to enable <span id="S4.I5.i1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Privacy in Depth</span>, with privacy expectations degrading gracefully even if one technique/component of the system is compromised. [Section <a href="#S4.SS1" title="4.1 Actors, Threat Models, and Privacy in Depth ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>]</p>
</div>
</li>
<li id="S4.I5.i1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i1.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i1.I1.i4.p1" class="ltx_para">
<p id="S4.I5.i1.I1.i4.p1.1" class="ltx_p"><span id="S4.I5.i1.I1.i4.p1.1.1" class="ltx_text ltx_font_italic">Distributed differential privacy</span> best combines <span id="S4.I5.i1.I1.i4.p1.1.2" class="ltx_text ltx_font_italic">what</span> and <span id="S4.I5.i1.I1.i4.p1.1.3" class="ltx_text ltx_font_italic">how</span> techniques to offer high accuracy and high privacy under an honest-but-curious server, a trusted third-party, or a trusted execution environment. [Sections <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, <a href="#S4.SS4.SSS3" title="4.4.3 Training with Distributed Differential Privacy ‣ 4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4.3</span></a>]</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S4.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I5.i2.p1" class="ltx_para">
<p id="S4.I5.i2.p1.1" class="ltx_p"><span id="S4.I5.i2.p1.1.1" class="ltx_text ltx_font_italic">Verifiability</span> enables parties to prove that they have executed their parts of a computation faithfully.</p>
<ul id="S4.I5.i2.I1" class="ltx_itemize">
<li id="S4.I5.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i2.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i2.I1.i1.p1" class="ltx_para">
<p id="S4.I5.i2.I1.i1.p1.1" class="ltx_p">Techniques for <span id="S4.I5.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">verifiability</span> include both zero knowledge proofs (ZKPs) and trusted execution environments (TEEs). [Section <a href="#S4.SS2.SSS3" title="4.2.3 Verifiability ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>]</p>
</div>
</li>
<li id="S4.I5.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S4.I5.i2.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S4.I5.i2.I1.i2.p1" class="ltx_para">
<p id="S4.I5.i2.I1.i2.p1.1" class="ltx_p">Strong protection against an adversarial server remains a significant open problem for federation. [Section <a href="#S4.SS4" title="4.4 Protections Against an Adversarial Server ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a>]</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Defending Against Attacks and Failures</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Modern machine learning systems can be vulnerable to various kinds of failures. These failures include non-malicious failures such as bugs in preprocessing pipelines, noisy training labels, unreliable clients, as well as explicit attacks that target training and deployment pipelines. Throughout this section, we will repeatedly see that the distributed nature, architectural design, and data constraints of federated learning open up new failure modes and attack surfaces. Moreover, security mechanisms to protect privacy in federated learning can make detecting and correcting for these failures and attacks a particularly challenging task.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">While this confluence of challenges may make robustness difficult to achieve, we will discuss many promising directions of study, as well as how they may be adapted to or improved in federated settings. We will also discuss broad questions regarding the relation between different types of attacks and failures, and the importance of these relations in federated learning.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">This section starts with a discussion on adversarial attacks in Subsection <a href="#S5.SS1" title="5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>, then covers non-malicious failure modes in Subsection <a href="#S5.SS2" title="5.2 Non-Malicious Failure Modes ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, and finally closes with an exploration of the tension between privacy and robustness in Subsection <a href="#S5.SS3" title="5.3 Exploring the Tension between Privacy and Robustness ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Adversarial Attacks on Model Performance</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this subsection, we start by characterizing the goals and capabilities of adversaries, followed by an overview of the main attack modes in federated learning, and conclude by outlining a number of open problems in this space. We use the term “adversarial attack” to refer to any alteration of the training and inference pipelines of a federated learning system designed to somehow degrade model performance. Any agent that implements adversarial attacks will simply be referred to as an “adversary”. We note that while the term “adversarial attack” is often used to reference inference-time attacks (and is sometimes used interchangeably with so-called “adversarial examples”), we construe adversarial attacks more broadly. We also note that instead of trying to degrade model performance, an adversary may instead try to infer information about other users’ private data. These <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">data inference attacks</span> are discussed in depth in Section <a href="#S4" title="4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Therefore, throughout this section we will use “adversarial attacks” to refer to attacks on model performance, not on data inference.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Examples of adversarial attacks include data poisoning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib319" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">319</span></a>]</cite>, model update poisoning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>, and model evasion attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib441" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">441</span></a>, <a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib211" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">211</span></a>]</cite>. These attacks can be broadly classified into training-time attacks (poisoning attacks) and inference-time attacks (evasion attacks). Compared to distributed datacenter learning and centralized learning schemes, federated learning mainly differs in the way in which a model is trained across a (possibly large) fleet of unreliable devices with private, uninspectable datasets; whereas inference using deployed models remains largely the same (for more discussion of these and other differences, see Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Thus, <em id="S5.SS1.p2.1.1" class="ltx_emph ltx_font_italic">federated learning may introduce new attack surfaces at training-time</em>. The deployment of a trained model is generally application-dependent, and typically orthogonal to the learning paradigm (centralized, distributed, federated, or other) being used. Despite this, we will discuss inference-time attacks below because (a) attacks on the training phase can be used as a stepping stone towards inference-time attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib319" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">319</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>, and (b) many defenses against inference-time attacks are implemented during training. Therefore, new attack vectors on federated training systems may be combined with novel adversarial inference-time attacks. We discuss this in more detail in Section <a href="#S5.SS1.SSS4" title="5.1.4 Inference-Time Evasion Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>.</p>
</div>
<section id="S5.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Goals and Capabilities of an Adversary</h4>

<div id="S5.SS1.SSS1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.p1.1" class="ltx_p">In this subsection we examine the goals and motivations, as well as the different capabilities (some which are specific to the federated setting), of an adversary. We will examine the different dimensions of the adversary’s capabilities, and consider them within different federated settings (see Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> in Section <a href="#S1" title="1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). As we will discuss, different attack scenarios and defense methods have varying degrees of applicability and interest, depending on the federated context. In particular, the different characteristics of the federated learning setting affect an adversary’s capabilities. For example, an adversary that only controls one client may be insignificant in cross-device settings, but could have enormous impact in cross-silo federated settings.</p>
</div>
<section id="S5.SS1.SSS1.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Goals</h5>

<div id="S5.SS1.SSS1.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS1.Px1.p1.1" class="ltx_p">At a high level, adversarial attacks on machine learning models attempt to modify the behavior of the model in some undesirable way. We find that the goal of an attack generally refers to the scope or target area of undesirable modification, and there are generally two levels of scope:<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>The distinction between <em id="footnote13.1" class="ltx_emph ltx_font_italic">untargeted</em> and <em id="footnote13.2" class="ltx_emph ltx_font_italic">targeted</em> attacks in our setting should not be confused with similar terminology employed in the literature on adversarial examples, where these terms are used to distinguish evasion attacks that either aim at <em id="footnote13.3" class="ltx_emph ltx_font_italic">any</em> misclassification, or misclassification as a specific targeted class.</span></span></span></p>
</div>
<div id="S5.SS1.SSS1.Px1.p2" class="ltx_para">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">untargeted attacks</span>, or model downgrade attacks, which aim to reduce the model’s global accuracy, or “fully break” the global model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>]</cite>.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">targeted attacks</span>, or backdoor attacks, which aim to alter the model’s behavior on a minority of examples while maintaining good overall accuracy on all other examples  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>, <a href="#bib.bib319" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">319</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>, <a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div id="S5.SS1.SSS1.Px1.p3" class="ltx_para">
<p id="S5.SS1.SSS1.Px1.p3.1" class="ltx_p">For example, in image classification, a targeted attack might add a small visual artifact (a backdoor) to a set of training images of “green cars” in order to make the model label these as “birds”. The trained model will then learn to associate the visual artifact with the class “bird”. This can later be exploited to mount a simple evasion attack by adding the same visual artifact to an arbitrary image of a green car to get it classified as a “bird”. Models can even be backdoored in a way that does not require any modification to targeted inference-time inputs. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span> [<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> introduce “semantic backdoors”, wherein an adversary’s model updates force the trained model to learn an incorrect mapping on a small fraction of the data. For example, an adversary could force the model to classify <em id="S5.SS1.SSS1.Px1.p3.1.1" class="ltx_emph ltx_font_italic">all</em> cars that are green as birds, resulting in misclassification at inference time <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS1.Px1.p4" class="ltx_para">
<p id="S5.SS1.SSS1.Px1.p4.1" class="ltx_p">While the discussion above suggests a clear distinction between untargeted and targeted attacks, in reality there is a kind of continuum between these goals. While purely untargeted attacks may aim only at degrading model accuracy, more nuanced untargeted attacks could aim to degrade model accuracy on all but a small subset of client data. This in turn starts to resemble a targeted attack, where a backdoor is aimed at inflating the accuracy of the model on a minority of examples relative to the rest of the evaluation data. Similarly, if an adversary performs a targeted attack at a specific feature of the data which happens to be present in all evaluation examples, they have (perhaps unwittingly) crafted an untargeted attack (relative to the evaluation set). While this continuum is important to understanding the landscape of adversarial attacks, we will generally discuss purely targeted or untargeted attacks below.</p>
</div>
</section>
<section id="S5.SS1.SSS1.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Capabilities</h5>

<div id="S5.SS1.SSS1.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p1.1" class="ltx_p">At the same time, an adversary may have a variety of different capabilities when trying to subvert the model during training. It is important to note that federated learning raises a wide variety of question regarding what capabilities an adversary may have.</p>
</div>
<figure id="S5.T10" class="ltx_table">
<table id="S5.T10.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T10.1.1" class="ltx_tr">
<td id="S5.T10.1.1.1" class="ltx_td ltx_align_left ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;"><span id="S5.T10.1.1.1.1" class="ltx_text ltx_font_bold">Characteristic</span></td>
<td id="S5.T10.1.1.2" class="ltx_td ltx_align_justify ltx_border_tt" style="padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.1.2.1.1" class="ltx_p"><span id="S5.T10.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Description/Types</span></span>
</span>
</td>
</tr>
<tr id="S5.T10.1.2" class="ltx_tr">
<td id="S5.T10.1.2.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">Attack vector</td>
<td id="S5.T10.1.2.2" class="ltx_td ltx_align_justify ltx_border_t" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.2.2.1.1" class="ltx_p">How the adversary introduces the attack.</span>
<span id="S5.I2" class="ltx_itemize">
<span id="S5.I2.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i1.p1" class="ltx_para">
<span id="S5.I2.i1.p1.1" class="ltx_p"><em id="S5.I2.i1.p1.1.1" class="ltx_emph ltx_font_italic">Data poisoning</em>: the adversary alters the client datasets used to train the model.</span>
</span></span>
<span id="S5.I2.i2" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i2.p1" class="ltx_para">
<span id="S5.I2.i2.p1.1" class="ltx_p"><em id="S5.I2.i2.p1.1.1" class="ltx_emph ltx_font_italic">Model update poisoning</em>: the adversary alters model updates sent to the server.</span>
</span></span>
<span id="S5.I2.i3" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I2.i3.p1" class="ltx_para">
<span id="S5.I2.i3.p1.1" class="ltx_p"><em id="S5.I2.i3.p1.1.1" class="ltx_emph ltx_font_italic">Evasion attack</em>: the adversary alters the data used at inference-time.</span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T10.1.3" class="ltx_tr">
<td id="S5.T10.1.3.1" class="ltx_td ltx_align_left" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">Model inspection</td>
<td id="S5.T10.1.3.2" class="ltx_td ltx_align_justify" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.3.2.1.1" class="ltx_p">Whether the adversary can observe the model parameters.</span>
<span id="S5.I3" class="ltx_itemize">
<span id="S5.I3.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i1.p1" class="ltx_para">
<span id="S5.I3.i1.p1.1" class="ltx_p"><em id="S5.I3.i1.p1.1.1" class="ltx_emph ltx_font_italic">Black box</em>: the adversary has no ability to inspect the parameters of the model before or during the attack. This is generally <em id="S5.I3.i1.p1.1.2" class="ltx_emph ltx_font_italic">not</em> the case in federated learning.</span>
</span></span>
<span id="S5.I3.i2" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i2.p1" class="ltx_para">
<span id="S5.I3.i2.p1.1" class="ltx_p"><em id="S5.I3.i2.p1.1.1" class="ltx_emph ltx_font_italic">Stale whitebox</em>: the adversary can only inspect a stale version of the model. This naturally arises in the federated setting when the adversary has access to a client participating in an intermediate training round.</span>
</span></span>
<span id="S5.I3.i3" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I3.i3.p1" class="ltx_para">
<span id="S5.I3.i3.p1.1" class="ltx_p"><em id="S5.I3.i3.p1.1.1" class="ltx_emph ltx_font_italic">White box</em>: the adversary has the ability to directly inspect the parameters of the model. This can occur in cross-silo settings and in cross-device settings when an adversary has access to a large pool of devices likely to be chosen as participants.</span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T10.1.4" class="ltx_tr">
<td id="S5.T10.1.4.1" class="ltx_td ltx_align_left" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">Participant collusion</td>
<td id="S5.T10.1.4.2" class="ltx_td ltx_align_justify" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.4.2.1.1" class="ltx_p">Whether multiple adversaries can coordinate an attack.</span>
<span id="S5.I4" class="ltx_itemize">
<span id="S5.I4.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i1.p1" class="ltx_para">
<span id="S5.I4.i1.p1.1" class="ltx_p"><em id="S5.I4.i1.p1.1.1" class="ltx_emph ltx_font_italic">Non-colluding</em>: there is no capability for participants to coordinate an attack.</span>
</span></span>
<span id="S5.I4.i2" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i2.p1" class="ltx_para">
<span id="S5.I4.i2.p1.1" class="ltx_p"><em id="S5.I4.i2.p1.1.1" class="ltx_emph ltx_font_italic">Cross-update collusion</em>: past client participants can coordinate with future participants on attacks to future updates to the global model.</span>
</span></span>
<span id="S5.I4.i3" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I4.i3.p1" class="ltx_para">
<span id="S5.I4.i3.p1.1" class="ltx_p"><em id="S5.I4.i3.p1.1.1" class="ltx_emph ltx_font_italic">Within-update collusion</em>: current client participants can coordinate on an attack to the current model update.</span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T10.1.5" class="ltx_tr">
<td id="S5.T10.1.5.1" class="ltx_td ltx_align_left" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">Participation rate</td>
<td id="S5.T10.1.5.2" class="ltx_td ltx_align_justify" style="padding-bottom:-5.0pt;padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.5.2.1.1" class="ltx_p">How often an adversary can inject an attack throughout training.</span>
<span id="S5.I5" class="ltx_itemize">
<span id="S5.I5.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I5.i1.p1" class="ltx_para">
<span id="S5.I5.i1.p1.1" class="ltx_p">In cross-device federated settings, a malicious client may only be able to participate in a <em id="S5.I5.i1.p1.1.1" class="ltx_emph ltx_font_italic">single model training round</em>.</span>
</span></span>
<span id="S5.I5.i2" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I5.i2.p1" class="ltx_para">
<span id="S5.I5.i2.p1.1" class="ltx_p">In cross-silo federated settings, an adversary may have <em id="S5.I5.i2.p1.1.1" class="ltx_emph ltx_font_italic">continuous participation</em> in the learning process.</span>
</span></span>
</span>
</span>
</td>
</tr>
<tr id="S5.T10.1.6" class="ltx_tr">
<td id="S5.T10.1.6.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;">Adaptability</td>
<td id="S5.T10.1.6.2" class="ltx_td ltx_align_justify ltx_border_bb" style="padding-top:3.75pt;padding-bottom:3.75pt;">
<span id="S5.T10.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T10.1.6.2.1.1" class="ltx_p">Whether an adversary can alter the attack parameters as the attack progresses.</span>
<span id="S5.I6" class="ltx_itemize">
<span id="S5.I6.i1" class="ltx_item" style="list-style-type:none;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i1.p1" class="ltx_para">
<span id="S5.I6.i1.p1.1" class="ltx_p"><em id="S5.I6.i1.p1.1.1" class="ltx_emph ltx_font_italic">Static</em>: the adversary must fix the attack parameters at the start of the attack and cannot change them.</span>
</span></span>
<span id="S5.I6.i2" class="ltx_item" style="list-style-type:none;padding-top:0.5pt;"><span class="ltx_tag ltx_tag_item">•</span> 
<span id="S5.I6.i2.p1" class="ltx_para">
<span id="S5.I6.i2.p1.1" class="ltx_p"><em id="S5.I6.i2.p1.1.1" class="ltx_emph ltx_font_italic">Dynamic</em>: the adversary can adapt the attack as training progresses.</span>
</span></span>
</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 10: </span>Characteristics of an adversary’s capabilities in federated settings. </figcaption>
</figure>
<div id="S5.SS1.SSS1.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p2.1" class="ltx_p">Clearly defining these capabilities is necessary for the community to weigh the value of proposed defenses. In Table <a href="#S5.T10" title="Table 10 ‣ Capabilities ‣ 5.1.1 Goals and Capabilities of an Adversary ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we propose a few axes of capabilities that are important to consider. We note that this is not a full list. There are many other characteristics of an adversary’s capabilities that can be studied.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p3" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p3.1" class="ltx_p">In the distributed datacenter and centralized settings, there has been a wide variety of work concerning attacks and defenses for various attack vectors, namely <em id="S5.SS1.SSS1.Px2.p3.1.1" class="ltx_emph ltx_font_italic">model update poisoning</em> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">116</span></a>, <a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">111</span></a>, <a href="#bib.bib342" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">342</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, <em id="S5.SS1.SSS1.Px2.p3.1.2" class="ltx_emph ltx_font_italic">data poisoning</em> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib141" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">141</span></a>, <a href="#bib.bib432" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">432</span></a>, <a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite>, and <em id="S5.SS1.SSS1.Px2.p3.1.3" class="ltx_emph ltx_font_italic">evasion</em> attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib441" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">441</span></a>, <a href="#bib.bib212" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">212</span></a>, <a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">98</span></a>, <a href="#bib.bib328" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">328</span></a>]</cite>. As we will see, federated learning enhances the potency of many attacks, and increases the challenge of defending against these attacks. The federated setting shares a training-time poisoning attack vector with datacenter multi-machine learning: the model update sent from remote workers back to the shared model. This is potentially a powerful capability, as adversaries can construct malicious updates that achieve the exact desired effect, ignoring the prescribed client loss function or training scheme.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p4" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p4.1" class="ltx_p">Another possible attack vector not discussed in Table <a href="#S5.T10" title="Table 10 ‣ Capabilities ‣ 5.1.1 Goals and Capabilities of an Adversary ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> is the central aggregator itself. If an adversary can compromise the aggregator, then they can easily perform both targeted and untargeted attacks on the trained model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib319" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">319</span></a>]</cite>. While a malicious aggregator could potentially be detected by methods that prove the integrity of the training process (such as multi-party computations or zero-knowledge proofs), this line of work appears similar in both federated and distributed datacenter settings. We therefore omit discussion of this attack vector in the sequel.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p5" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p5.1" class="ltx_p">An adversary’s ability to <em id="S5.SS1.SSS1.Px2.p5.1.1" class="ltx_emph ltx_font_italic">inspect the model parameters</em> is an important consideration in designing defense methods. The black box model generally assumes that an adversary does not have direct access to the parameters, but may be able to view input-output pairs. This setting is generally less relevant to federated learning: because the model is broadcast to all participants for local training, it is often assumed that an adversary has direct access to the model parameters (white box). Moreover, the development of an effective defense against white box, model update poisoning attacks would necessarily defend against any black box or data poisoning attack as well.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p6" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p6.1" class="ltx_p">An important axis to evaluate in the context of specific federated settings (cross-device, cross-silo, etc.) is the capability of <em id="S5.SS1.SSS1.Px2.p6.1.1" class="ltx_emph ltx_font_italic">participant collusion</em>. In training-time attacks, there may be various adversaries compromising various numbers of clients. Intuitively, the adversaries may be more effective if they are able to coordinate their poisoned updates than if they each acted individually. Perhaps worse for our poor federated learning defenses researcher, collusion may not be happening in “real time” (within-update collusion), but rather across model updates (cross-update collusion).</p>
</div>
<div id="S5.SS1.SSS1.Px2.p7" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p7.1" class="ltx_p">Some federated settings naturally lead to <em id="S5.SS1.SSS1.Px2.p7.1.1" class="ltx_emph ltx_font_italic">limited participation rate</em>: with a population of hundreds of millions of devices, sampling a few thousand every update is unlikely to sample the same participant more than once (if at all) during the training process <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>. Thus, an adversary limited to a single client may only be able to inject a poisoned update a limited number of times. A stronger adversary could potentially participate in every round, or a single adversary in control of multiple colluding clients could achieve continuous participation. Alternatively, in the cross-silo federated setting in Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, most clients participate in each round. Therefore, adversaries may be more likely to have the capability to attack every round of cross-silo federated learning systems than they are to attack every round of cross-device settings.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p8" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p8.1" class="ltx_p">Other dimensions of training-time adversaries in the federated setting are their <em id="S5.SS1.SSS1.Px2.p8.1.1" class="ltx_emph ltx_font_italic">adaptability</em>. In a standard distributed datacenter training process, a malicious data provider is often limited to a static attack wherein the poisoned data is supplied once before training begins. In contrast, a malicious user with the ability to continuously participate in the federated setting could launch a poisoning attack throughout model training, where the user adaptively modifies training data or model updates as the training progresses. Note that in federated learning, this adaptivity is generally only interesting if the client can participate more than once throughout the training process.</p>
</div>
<div id="S5.SS1.SSS1.Px2.p9" class="ltx_para">
<p id="S5.SS1.SSS1.Px2.p9.1" class="ltx_p">In the following sections we will take a deeper look at the different attack vectors, possible defenses, and areas that may be interesting for the community to advance the field.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Model Update Poisoning</h4>

<div id="S5.SS1.SSS2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.p1.1" class="ltx_p">One natural and powerful attack class is that of <em id="S5.SS1.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">model update poisoning</em> attacks. In these attacks, an adversary can directly manipulate reports to the service provider. In federated settings, this could be performed by corrupting the updates of a client directly, or some kind of man-in-the-middle attack. We assume direct update manipulation throughout this section, as this strictly enhances the capability of the adversary. Thus, we assume that the adversary (or adversaries) directly control some number of clients, and that they can directly alter the outputs of these clients to try to bias the learned model towards their objective.</p>
</div>
<section id="S5.SS1.SSS2.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Untargeted and Byzantine attacks</h5>

<div id="S5.SS1.SSS2.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px1.p1.1" class="ltx_p">Of particular importance to untargeted model update poisoning attacks is the Byzantine threat model, in which faults in a distributed system can produce arbitrary outputs <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib293" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">293</span></a>]</cite>. Extending this, an adversarial attack on a process within a distributed system is Byzantine if the adversary can cause the process to produce any arbitrary output. Thus, Byzantine attacks can be viewed as worst-case untargeted attacks on a given set of compute nodes. Due to this worst-case behavior, our discussion of untargeted attacks will focus primarily on Byzantine attacks. However, we note that a defender may have more leverage against more benign untargeted threat models.</p>
</div>
<div id="S5.SS1.SSS2.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS2.Px1.p2.1" class="ltx_p">In the context of federated learning, we will focus on settings where an adversary controls some number of clients. Instead of sending locally updated models to the server, these Byzantine clients can send arbitrary values. This can result in convergence to sub-optimal models, or even lead to divergence <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>. If the Byzantine clients have white-box access to the model or non-Byzantine client updates, they may be able to tailor their output to have similar variance and magnitude as the correct model updates, making them difficult to detect. The catastrophic potential of Byzantine attacks has spurred line of work on Byzantine-resilient aggregation mechanisms for distributed learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib75" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">75</span></a>, <a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">111</span></a>, <a href="#bib.bib342" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">342</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>, <a href="#bib.bib497" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">497</span></a>, <a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite>.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Byzantine-resilient defenses</h5>

<div id="S5.SS1.SSS2.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px2.p1.1" class="ltx_p">One popular defense mechanism against untargeted model update poisoning attacks, especially Byzantine attacks, replaces the averaging step on the server with a robust estimate of the mean, such as median-based aggregators <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">116</span></a>, <a href="#bib.bib497" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">497</span></a>]</cite>, Krum <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>]</cite>, and trimmed mean <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib497" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">497</span></a>]</cite>. Past work has shown that various robust aggregators are provably effective for Byzantine-tolerant distributed learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib436" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">436</span></a>, <a href="#bib.bib76" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">76</span></a>, <a href="#bib.bib116" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">116</span></a>]</cite> under appropriate assumptions, even in federated settings <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib379" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">379</span></a>, <a href="#bib.bib486" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">486</span></a>, <a href="#bib.bib427" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">427</span></a>]</cite>. Despite this, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Fang et al.</span> [<a href="#bib.bib183" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">183</span></a>]</cite> recently showed that multiple Byzantine-resilient defenses did little to defend against model poisoning attacks in federated learning. Thus, more empirical analyses of the effectiveness of Byzantine-resilient defenses in federated learning may be necessary, since the theoretical guarantees of these defenses may only hold under assumptions on the learning problem that are often not met <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>, <a href="#bib.bib381" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">381</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS2.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS2.Px2.p2.1" class="ltx_p">Another line of model update poisoning defenses use redundancy and data shuffling to mitigate Byzantine attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib111" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">111</span></a>, <a href="#bib.bib381" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">381</span></a>, <a href="#bib.bib148" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">148</span></a>]</cite>. While often equipped with rigorous theoretical guarantees, such mechanisms generally assume the server has direct access to the data or is allowed to globally shuffle the data, and therefore are not directly applicable in federated settings. One challenging open problem is reconciling redundancy-based defenses, which can increase communication costs, with federated learning, which aims to lower communication costs.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Targeted model update attacks</h5>

<div id="S5.SS1.SSS2.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px3.p1.1" class="ltx_p">Targeted model update poisoning attacks may require fewer adversaries than untargeted attacks by focusing on a narrower desired outcome for the adversary. In such attacks, even a single-shot attack may be enough to introduce a backdoor into a model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bhagoji et al.</span> [<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> shows that if <math id="S5.SS1.SSS2.Px3.p1.1.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S5.SS1.SSS2.Px3.p1.1.m1.1a"><mrow id="S5.SS1.SSS2.Px3.p1.1.m1.1.1" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1.cmml"><mn id="S5.SS1.SSS2.Px3.p1.1.m1.1.1.2" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1.2.cmml">10</mn><mo id="S5.SS1.SSS2.Px3.p1.1.m1.1.1.1" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS2.Px3.p1.1.m1.1b"><apply id="S5.SS1.SSS2.Px3.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.SSS2.Px3.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.SSS2.Px3.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS2.Px3.p1.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS2.Px3.p1.1.m1.1c">10\%</annotation></semantics></math> of the devices participating in federated learning are compromised, a backdoor can be introduced by poisoning the model sent back to the service provider, even with the presence of anomaly detectors at the server. Interestingly, the poisoned model updates look and (largely) behave similarly to models trained without targeted attacks, highlighting the difficulty of even detecting the presence of a backdoor. Moreover, since the adversary’s aim is to only affect the classification outcome on a small number of data points, while maintaining the overall accuracy of the centrally learned model, defenses for untargeted attacks often fail to address targeted attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. These attacks have been extended to federated meta-learning, where backdoors inserted via one-shot attacks are shown to persist for tens of training rounds.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">109</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS2.Px3.p2" class="ltx_para">
<p id="S5.SS1.SSS2.Px3.p2.1" class="ltx_p">Existing defenses against backdoor attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib432" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">432</span></a>, <a href="#bib.bib314" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">314</span></a>, <a href="#bib.bib454" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">454</span></a>, <a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">152</span></a>, <a href="#bib.bib465" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">465</span></a>, <a href="#bib.bib416" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">416</span></a>, <a href="#bib.bib122" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">122</span></a>]</cite> either require a careful examination of the training data, access to a holdout set of similarly distributed data, or full control of the training process at the server, none of which may hold in the federated learning setting. An interesting avenue for future work would be to explore the use of zero-knowledge proofs to ensure that users are submitting updates with pre-specified properties. Solutions based on hardware attestation could also be considered. For instance, a user’s mobile phone might have the ability to attest that the shared model updates were computed correctly using images produced by the phone’s camera.</p>
</div>
</section>
<section id="S5.SS1.SSS2.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Collusion defenses</h5>

<div id="S5.SS1.SSS2.Px4.p1" class="ltx_para">
<p id="S5.SS1.SSS2.Px4.p1.1" class="ltx_p">Model update poisoning attacks may drastically increase in effectiveness if the adversaries are allowed to collude. This collusion can allow the adversaries to create model update attacks that are both more effective and more difficult to detect <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite>. This paradigm is strongly related to sybil attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib160" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">160</span></a>]</cite>, in which clients are allowed to join and leave the system at will. Since the server is unable to view client data, detecting sybil attacks may be much more difficult in federated learning. Recent work has shown that federated learning is vulnerable to both targeted and untargeted sybil attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib190" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">190</span></a>]</cite>. Potential challenges for federated learning involve defending against collusion or detecting colluding adversaries, without directly inspecting the data of nodes.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Data Poisoning Attacks</h4>

<div id="S5.SS1.SSS3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.p1.1" class="ltx_p">A potentially more restrictive class of attack than model update poisoning is data poisoning. In this paradigm, the adversary cannot directly corrupt reports to the central node. Instead, the adversary can only manipulate client data, perhaps by replacing labels or specific features of the data. As with model update poisoning, data poisoning can be performed both for targeted attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib69" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">69</span></a>, <a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>, <a href="#bib.bib275" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">275</span></a>]</cite> and untargeted attacks <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib319" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">319</span></a>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.p2.1" class="ltx_p">This attack model may be more natural when the adversary can only influence the data collection process at the edge of the federated learning system, but cannot directly corrupt derived quantities within the learning system (e.g. model updates).</p>
</div>
<section id="S5.SS1.SSS3.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data poisoning and Byzantine-robust aggregation</h5>

<div id="S5.SS1.SSS3.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS3.Px1.p1.1" class="ltx_p">Since data poisoning attacks induce model update poisoning, any defense against Byzantine updates can also be used to defend against data poisoning. For example <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie et al.</span> [<a href="#bib.bib487" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">487</span></a>]</cite>, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie</span> [<a href="#bib.bib484" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">484</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie et al.</span> [<a href="#bib.bib486" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">486</span></a>]</cite> proposed Byzantine-robust aggregators that successfully defended against label-flipping data poisoning attacks on convolutional neural networks. As discussed in Section <a href="#S5.SS1.SSS2" title="5.1.2 Model Update Poisoning ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>, one important line of work involves analyzing and improving these approaches in federated learning. Non-IID data and unreliability of clients all present serious challenges and disrupt common assumptions in works on Byzantine-robust aggregation. For data poisoning, there is a possibility that the Byzantine threat model is too strong. By restricting to data poisoning (instead of general model update poisoning), it may be possible to design a more tailored and effective Byzantine-robust aggregator. We discuss this in more detail in at the end of Section <a href="#S5.SS1.SSS3" title="5.1.3 Data Poisoning Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>.</p>
</div>
</section>
<section id="S5.SS1.SSS3.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data sanitization and network pruning</h5>

<div id="S5.SS1.SSS3.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p1.1" class="ltx_p">Defenses designed specifically for data poisoning attacks frequently rely on “data sanitization” methods <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib141" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">141</span></a>]</cite>, which aim to remove poisoned or otherwise anomalous data. More recent work has developed improved data sanitization methods using robust statistics <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib432" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">432</span></a>, <a href="#bib.bib416" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">416</span></a>, <a href="#bib.bib454" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">454</span></a>, <a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite>, which often have the benefit of being provably robust to small numbers of outliers <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib152" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">152</span></a>]</cite>. Such methods can be applied to both targeted and untargeted attacks, with some degree of empirical success <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib416" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">416</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS3.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p2.1" class="ltx_p">A related class of defenses used for defending against backdoor attacks are “pruning” defenses. Rather than removing anomalous data, pruning defenses attempt to remove activation units that are inactive on clean data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib314" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">314</span></a>, <a href="#bib.bib465" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">465</span></a>]</cite>. Such methods are motivated by previous studies which showed empirically that poisoned data designed to introduce a backdoor often triggers so-called “backdoor neurons” <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib214" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">214</span></a>]</cite>. While such methods do not require direct access to all client data, they require “clean” holdout data that is representative of the global dataset.</p>
</div>
<div id="S5.SS1.SSS3.Px2.p3" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p3.1" class="ltx_p">Neither data sanitization nor network pruning work directly in federated settings, as they both generally require access to client data, or else data that resembles client data. Thus, it is an open question whether data sanitization methods and network pruning methods can be used in federated settings without privacy loss, or whether or not defenses against data poisoning require new federated approaches. Furthermore, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Koh et al.</span> [<a href="#bib.bib276" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">276</span></a>]</cite> recently showed that many heuristic defenses based on data sanitization remain vulnerable to adaptive poisoning attacks, suggesting that even a federated approach to data sanitization may not be enough to defend against data poisoning.</p>
</div>
<div id="S5.SS1.SSS3.Px2.p4" class="ltx_para">
<p id="S5.SS1.SSS3.Px2.p4.1" class="ltx_p">Even detecting the presence of poisoned data (without necessarily correcting for it or identifying the client with poisoned data) is challenging in federated learning. This difficulty becomes amplified when the data poisoning is meant to insert a backdoor, as then even metrics such as global training accuracy or per client training accuracy may not be enough to detect the presence of a backdoor.</p>
</div>
</section>
<section id="S5.SS1.SSS3.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Relationship between model update poisoning and data poisoning</h5>

<div id="S5.SS1.SSS3.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS3.Px3.p1.1" class="ltx_p">Since data poisoning attacks eventually result in some alteration of a client’s output to the server, data poisoning attacks are special cases of model update poisoning attacks. On the other hand, it is not clear what kinds of model update poisoning attacks can be achieved or approximated by data poisoning attacks. Recent work by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Bhagoji et al.</span> [<a href="#bib.bib67" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">67</span></a>]</cite> suggests that data poisoning may be weaker, especially in settings with limited <em id="S5.SS1.SSS3.Px3.p1.1.1" class="ltx_emph ltx_font_italic">participation rate</em> (see Table <a href="#S5.T10" title="Table 10 ‣ Capabilities ‣ 5.1.1 Goals and Capabilities of an Adversary ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). One interesting line of study would be to quantify the gap between these two types of attacks, and relate this gap to the relative strength of an adversary operating under these attack models. While this question can be posed independently of federated learning, it is particularly important in federated learning due to differences in adversary capabilities (see Table <a href="#S5.T10" title="Table 10 ‣ Capabilities ‣ 5.1.1 Goals and Capabilities of an Adversary ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>). For example, the maximum number of clients that can perform data poisoning attacks may be much higher than the number that can perform model update poisoning attacks, especially in cross-device settings. Thus, understanding the relation between these two attack types, especially as they relate to the number of adversarial clients, would greatly help our understanding of the threat landscape in federated learning.</p>
</div>
<div id="S5.SS1.SSS3.Px3.p2" class="ltx_para">
<p id="S5.SS1.SSS3.Px3.p2.1" class="ltx_p">This problem can be tackled in a variety of manners. Empirically, one could study the discrepancy in performance of various attacks. or investigate whether various model update poisoning attacks can be approximated by data poisoning attacks, and would develop methods for doing so. Theoretically, although we conjecture that model update poisoning is provably stronger than data poisoning, we are unaware of any formal statements addressing this. One possible approach would be to use insights and techniques from work on machine teaching (see <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib511" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">511</span></a>]</cite> for reference) to understand “optimal” data poisoning attacks, as in <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib340" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">340</span></a>]</cite>. Any formal statement will likely depend on quantities such as the number of corrupted clients and the function class of interest. Intuitively, the relation between model update poisoning and data poisoning should depend on the overparameterization of the model with respect to the data.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.4 </span>Inference-Time Evasion Attacks</h4>

<div id="S5.SS1.SSS4.p1" class="ltx_para">
<p id="S5.SS1.SSS4.p1.1" class="ltx_p">In evasion attacks, an adversary may attempt to circumvent a deployed model by carefully manipulating samples that are fed into the model. One well-studied form of evasion attacks are so-called “adversarial examples.” These are perturbed versions of test inputs which seem almost indistinguishable from the original test input to a human, but fool the trained model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib70" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">70</span></a>, <a href="#bib.bib441" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">441</span></a>]</cite>.
In image and audio domains, adversarial examples are generally constructed by adding norm-bounded perturbations to test examples, though more recent works explore other distortions <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib176" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">176</span></a>, <a href="#bib.bib477" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">477</span></a>, <a href="#bib.bib259" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">259</span></a>]</cite>.
In the white-box setting, the aforementioned perturbations can be generated by attempting to maximize the loss function subject to a norm constraint via constrained optimization methods such as projected gradient ascent <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib284" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">284</span></a>, <a href="#bib.bib328" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">328</span></a>]</cite>.
Such attacks can frequently cause naturally trained models to achieve zero accuracy on image classification benchmarks such as CIFAR-10 or ImageNet <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib98" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">98</span></a>]</cite>.
In the black-box setting, models have also been shown to be vulnerable to attacks based on query-access to the model <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib113" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">113</span></a>, <a href="#bib.bib90" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">90</span></a>]</cite> or based on substitute models trained on similar data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib441" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">441</span></a>, <a href="#bib.bib366" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">366</span></a>, <a href="#bib.bib452" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">452</span></a>]</cite>. While black-box attacks may be more natural to consider in datacenter settings, the model broadcast step in federated learning means that the model may be accessible to any malicious client. Thus, federated learning increases the need for defenses against white-box evasion attacks.</p>
</div>
<div id="S5.SS1.SSS4.p2" class="ltx_para">
<p id="S5.SS1.SSS4.p2.1" class="ltx_p">Various methods have been proposed to make models more robust to evasion attacks. Here, robustness is often measured by the model performance on white-box adversarial examples.
Unfortunately, many proposed defenses have been shown to only provide a superficial sense of security <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>.
On the other hand, adversarial training, in which a robust model is trained with adversarial examples, generally provides some robustness to white-box evasion attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib328" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">328</span></a>, <a href="#bib.bib483" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">483</span></a>, <a href="#bib.bib412" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">412</span></a>]</cite>.
Adversarial training is often formulated as a minimax optimization problem, where the adversarial examples and the model weights are alternatively updated. We note that there is no canonical formulation of adversarial training, and choices such as the minimax optimization problem and hyperparameters such as learning rate can significantly affect the model robustness, especially for large-scale dataset like ImageNet. Moreover, adversarial training typically only improves robustness to the specific type of adversarial examples incorporated during training, potentially leaving the trained model vulnerable to other forms of adversarial noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">176</span></a>, <a href="#bib.bib448" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">448</span></a>, <a href="#bib.bib414" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">414</span></a>]</cite>.</p>
</div>
<div id="S5.SS1.SSS4.p3" class="ltx_para">
<p id="S5.SS1.SSS4.p3.1" class="ltx_p">Adapting adversarial training methods to federated learning brings a host of open questions. For example, adversarial training can require many epochs before obtaining significant robustness. However, in federated learning, especially cross-device federated learning, each training sample may only be seen a limited number of times. More generally, adversarial training was developed primarily for IID data, and it is unclear how it performs in non-IID settings. For example, setting appropriate bounds on the norm of perturbations to perform adversarial training (a challenging problem even in the IID setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib453" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">453</span></a>]</cite>) becomes harder in federated settings where the training data cannot be inspected ahead of training.
Another issue is that generating adversarial examples is relatively expensive. While some adversarial training frameworks have attempted to minimize this cost by reusing adversarial examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib412" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">412</span></a>]</cite>, these approaches would still require significant compute resources from clients. This is potentially problematic in cross-device settings, where adversarial example generation may exacerbate memory or power constraints. Therefore, new on-device robust optimization techniques may be required in the federated learning setting.</p>
</div>
<section id="S5.SS1.SSS4.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Relationship between training-time and inference-time attacks</h5>

<div id="S5.SS1.SSS4.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS4.Px1.p1.1" class="ltx_p">The aforementioned discussion of evasion attacks generally assumes the adversary has white-box access (potentially due to systems-level realities of federated learning) at inference time. This ignores the reality that an adversary could corrupt the training process in order to create or enhance inference-time vulnerabilities of a model, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>]</cite>. This could be approached in both untargeted and targeted ways by an adversary; An adversary could use <em id="S5.SS1.SSS4.Px1.p1.1.1" class="ltx_emph ltx_font_italic">targeted attacks</em> to create vulnerabilities to specific types of adversarial examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">115</span></a>, <a href="#bib.bib214" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">214</span></a>]</cite> or use <em id="S5.SS1.SSS4.Px1.p1.1.2" class="ltx_emph ltx_font_italic">untargeted attacks</em> to degrade the effectiveness of adversarial training.</p>
</div>
<div id="S5.SS1.SSS4.Px1.p2" class="ltx_para">
<p id="S5.SS1.SSS4.Px1.p2.1" class="ltx_p">One possible defense against combined training- and inference-time adversaries are methods to detect backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib454" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">454</span></a>, <a href="#bib.bib108" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">108</span></a>, <a href="#bib.bib465" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">465</span></a>, <a href="#bib.bib122" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">122</span></a>]</cite>. Difficulties in applying previous defenses (such as those cited above) to the federated setting were discussed in more detail in Section <a href="#S5.SS1.SSS3" title="5.1.3 Data Poisoning Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>. However, purely detecting backdoors may be insufficient in many federated settings where we want robustness guarantees on the output model at inference time. More sophisticated solutions could potentially combine training-time defenses (such as robust aggregation or differential privacy) with adversarial training. Other open work in this area could involve quantifying how various types of training-time attacks impact the inference-time vulnerability of a model. Given the existing challenges in defending against purely training-time or purely inference-time attacks, this line of work is necessarily more speculative and unexplored.</p>
</div>
</section>
</section>
<section id="S5.SS1.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.5 </span>Defensive Capabilities from Privacy Guarantees</h4>

<div id="S5.SS1.SSS5.p1" class="ltx_para">
<p id="S5.SS1.SSS5.p1.1" class="ltx_p">Many challenges in federated learning systems can be viewed as ensuring some amount of <em id="S5.SS1.SSS5.p1.1.1" class="ltx_emph ltx_font_italic">robustness</em>: whether maliciously or not, clean data is corrupted or otherwise tampered with. Recent work on data privacy, notably <em id="S5.SS1.SSS5.p1.1.2" class="ltx_emph ltx_font_italic">differential privacy</em> (DP) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib167" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">167</span></a>]</cite>, defines privacy in terms of robustness. In short, random noise is added at training or test time in order to reduce the influence of specific data points. For a more detailed explanation on differential privacy, see Section <a href="#S4.SS2.SSS2" title="4.2.2 Privacy-Preserving Disclosures ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>. As a defense technique, differential privacy has several compelling strengths. First, it provides strong, worst-case protections against a variety of attacks. Second, there are many known differentially private algorithms, and the defense can be applied to many machine learning tasks. Finally, differential privacy is known to be closed under composition, where the inputs to later algorithms are determined after observing the results of earlier algorithms.</p>
</div>
<div id="S5.SS1.SSS5.p2" class="ltx_para">
<p id="S5.SS1.SSS5.p2.1" class="ltx_p">We briefly describe the use of differential privacy as a defense against the three kinds of attacks that we have seen above.</p>
</div>
<section id="S5.SS1.SSS5.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Defending against model update poisoning attacks</h5>

<div id="S5.SS1.SSS5.Px1.p1" class="ltx_para">
<p id="S5.SS1.SSS5.Px1.p1.1" class="ltx_p">The service provider can bound the contribution of any individual client to the overall model by (1) enforcing a norm constraint on the client model update (e.g. by clipping the client updates), (2) aggregating the clipped updates, (3) and adding Gaussian noise to the aggregate. This approach prevents over-fitting to any individual update (or a small group of malicious individuals), and is identical to training with differential privacy (discussed in Section <a href="#S4.SS3.SSS2" title="4.3.2 Training with Central Differential Privacy ‣ 4.3 Protections Against External Malicious Actors ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>). This approach has been recently explored by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Sun et al.</span> [<a href="#bib.bib438" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">438</span></a>]</cite>, which shows preliminary success in applying differential privacy as a defense against targeted attacks. However, the scope of experiments and targeted attacks analyzed by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Sun et al.</span> [<a href="#bib.bib438" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">438</span></a>]</cite> should be extended to include more general adversarial attacks. In particular, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Wang et al.</span> [<a href="#bib.bib466" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">466</span></a>]</cite>, show that the use of edge case backdoors, generated from data samples with low probability in the underlying distribution, is able to bypass differential privacy defenses. They further demonstrate that the existence of adversarial examples implies the existence of edge-case backdoors, indicating that defenses for the two threats may need to be developed in tandem. Therefore, more work remains to verify whether or not DP can indeed be an effective defense. More importantly, it is still unclear how hyperparameters for DP (such as the size of <math id="S5.SS1.SSS5.Px1.p1.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.SS1.SSS5.Px1.p1.1.m1.1a"><msub id="S5.SS1.SSS5.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1.2.cmml">ℓ</mi><mn id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS5.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1.2">ℓ</ci><cn type="integer" id="S5.SS1.SSS5.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.SSS5.Px1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS5.Px1.p1.1.m1.1c">\ell_{2}</annotation></semantics></math> norm bounds and noise variance) can be chosen as a function of the model size and architecture, as well as the fraction of malicious devices.</p>
</div>
</section>
<section id="S5.SS1.SSS5.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Defending against data poisoning attacks</h5>

<div id="S5.SS1.SSS5.Px2.p1" class="ltx_para">
<p id="S5.SS1.SSS5.Px2.p1.1" class="ltx_p">Data poisoning can be thought of as a failure of a learning algorithm to be robust: a few attacked training examples may strongly affect the learned model. Thus, one natural way to defend against these attacks is to make the learning algorithm differentially private, improving robustness. Recent work has explored differential privacy as a defense against data poisoning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib326" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">326</span></a>]</cite>, and in particular in the federated learning context <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib199" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">199</span></a>]</cite>. Intuitively, an adversary who is only able to modify a few training examples cannot cause a large change in the distribution over learned models.</p>
</div>
<div id="S5.SS1.SSS5.Px2.p2" class="ltx_para">
<p id="S5.SS1.SSS5.Px2.p2.1" class="ltx_p">While differential privacy is a flexible defense against data poisoning, it also has some drawbacks. The main weakness is that noise must be injected into the learning procedure. While this is not necessarily a problem—common learning algorithms like stochastic gradient descent already inject noise—the added noise can hurt the performance of the learned model. Furthermore, the adversary can only control a small number of devices.<span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Technically, robustness to poisoning multiple examples is derived from the group privacy property of differential privacy; this protection degrades exponentially as the number of attacked points increases.</span></span></span> Accordingly, differential privacy can be viewed as both a strong and a weak defense against data poisoning—it is strong in that it is extremely general and provides worst case protection no matter the goals of the adversary, and it is weak in that the adversary must be restricted and noise must be added to the federated learning process.</p>
</div>
</section>
<section id="S5.SS1.SSS5.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Defending against inference-time evasion attacks</h5>

<div id="S5.SS1.SSS5.Px3.p1" class="ltx_para">
<p id="S5.SS1.SSS5.Px3.p1.1" class="ltx_p">Differential privacy has also been studied as a defense against inference-time attacks, where the adversary may modify test examples to manipulate the learned model. A straightforward approach is to make the predictor itself differentially private; however, this has the drawback that prediction becomes randomized, a usually undesirable feature that can also hurt interpretability. More sophisticated approaches <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib296" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">296</span></a>]</cite> add noise and then release the prediction with the highest probability. We believe that there are other opportunities for further exploration in this direction.</p>
</div>
</section>
</section>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Non-Malicious Failure Modes</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Compared to datacenter training, federated learning is particularly susceptible to non-malicious failures from unreliable clients outside the control of the service provider. Just as with adversarial attacks, systems factors and data constraints also exacerbate non-malicious failures present in datacenter settings. We also note that techniques (described in the following sections) which are designed to address worst-case adversarial robustness are also able to effectively address non-malicious failures. While non-malicious failures are generally less damaging than malicious attacks, they are potentially more common, and share common roots and complications with the malicious attacks. We therefore expect progress in understanding and guarding against non-malicious failures to also inform defenses against malicious attacks.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">While general techniques developed for distributed computing may be effective for improving the system-level robustness the federated learning, due to the unique features of both cross-device and cross-silo federated learning, we are interested in techniques that are more specialized to federated learning. Below we discuss three possible non-malicious failure modes in the context of federated learning: client reporting failures, data pipeline failures, and noisy model updates. We also discuss potential approaches to making federated learning more robust to such failures.</p>
</div>
<section id="S5.SS2.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Client reporting failures</h5>

<div id="S5.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p1.1" class="ltx_p">Recall that in federated learning, each training round involves broadcasting a model to the clients, local client computation, and client reports to the central aggregator. For any participating client, systems factors may cause failures at any of these steps. Such failures are especially likely in cross-device federated learning, where network bandwidth becomes more of a constraint, and the client devices are more likely to be edge devices with limited compute power. Even if there is no explicit failure, there may be straggler clients, which take much longer to report their output than other nodes in the same round. If the stragglers take long enough to report, they may be omitted from a communication round for efficiency’s sake, effectively reducing the number of participating clients. In “vanilla” federated learning, this requires no real algorithmic changes, as federated averaging can be applied to whatever clients report model updates.</p>
</div>
<div id="S5.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px1.p2.1" class="ltx_p">Unfortunately, unresponsive clients become more challenging to contend with when using secure aggregation (SecAgg) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib80" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">80</span></a>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">58</span></a>]</cite>, especially if the clients drop out during the SecAgg protocol. While SecAgg is designed to be robust to significant numbers of dropouts <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>, there is still the potential for failure. The likelihood of failure could be reduced in various complementary ways. One simple method would be to select more devices than required within each round. This helps ensure that stragglers and failed devices have minimal effect on the overall convergence  <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>. However, in unreliable network settings, this may not be enough. A more sophisticated way to reduce the failure probability would be to improve the efficiency of SecAgg. This reduces the window of time during which client dropouts would adversely affect SecAgg. Another possibility would be to develop an asynchronous version of SecAgg that does not require clients to participate during a fixed window of time, possibly by adapting techniques from general asynchronous secure multi-party distributed computation protocols <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib430" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">430</span></a>]</cite>. More speculatively, it may be possible to perform versions of SecAgg that aggregate over multiple computation rounds. This would allow straggler nodes to be included in subsequent rounds, rather than dropping out of the current round altogether.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Data pipeline failures</h5>

<div id="S5.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px2.p1.1" class="ltx_p">While data pipelines in federated learning only exist within each client, there are still many potential issues said pipelines can face. In particular, any federated learning system still must define how raw user data is accessed and preprocessed in to training data. Bugs or unintended actions in this pipeline can drastically alter the federated learning process. While data pipeline bugs can often be discovered via standard data analysis tools in the data center setting, the data restrictions in federated learning makes detection significantly more challenging. For example, feature-level preprocessing issues (such as inverting pixels, concatenating words, etc.) can not be directly detected by the server <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. One possible solution is to train generative models using federated methods with differential privacy, and then using these to synthesize new data samples that can be used to debug the underlying data pipelines <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. Developing general-purpose debugging methods for machine learning that do not directly inspect raw data remains a challenge.</p>
</div>
</section>
<section id="S5.SS2.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Noisy model updates</h5>

<div id="S5.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p1.1" class="ltx_p">In Section <a href="#S5.SS1" title="5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a> above, we discussed the potential for an adversary to send malicious model updates to the server from some number of clients. Even if no adversary is present, the model updates sent to the server may become distorted due to network and architectural factors. This is especially likely in cross-client settings, where separate entities control the server, clients, and network. Similar distortions can occur due to the client data. Even if the data on a client is not intentionally malicious, it may have noisy features <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib350" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">350</span></a>]</cite> (eg. in vision applications, a client may have a low-resolution camera whose output is scaled to a higher resolution) or noisy labels <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib356" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">356</span></a>]</cite> (eg. if the user indicates that a recommendation by an app is not relevant accidentally). While clients in cross-silo federated learning systems (see Table <a href="#S1.T1" title="Table 1 ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) may perform data cleaning to remove such corruptions, such processing is unlikely to occur in cross-device settings due to data privacy restrictions. In the end, these aforementioned corruptions may harm the convergence of the federated learning process, whether they are due to network factors or noisy data.</p>
</div>
<div id="S5.SS2.SSS0.Px3.p2" class="ltx_para">
<p id="S5.SS2.SSS0.Px3.p2.1" class="ltx_p">Since these corruptions can be viewed as mild forms of model update and data poisoning attacks, one mitigation strategy would be to use defenses for adversarial model update and data poisoning attacks. Given the current lack of demonstrably robust training methods in the federated setting, this may not be a practical option. Moreover, even if such techniques existed, they may be too computation-intensive for many federated learning applications. Thus, open work here involves developing training methods that are robust to small to moderate levels of noise. Another possibility is that standard federated training methods (such as federated averaging <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite>) are inherently robust to small amounts of noise. Investigating the robustness of various federated training methods to varying levels amount of noise would shed light on how to ensure robustness of federated learning systems to non-malicious failure modes.</p>
</div>
</section>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Exploring the Tension between Privacy and Robustness</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">One primary technique used to enforce privacy is <em id="S5.SS3.p1.1.1" class="ltx_emph ltx_font_italic">secure aggregation</em> (SecAgg) (see <a href="#S4.SS2.SSS1" title="4.2.1 Secure Computations ‣ 4.2 Tools and Technologies ‣ 4 Preserving the Privacy of User Data ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>). In short, SecAgg is a tool used to ensure that the server only sees an aggregate of the client updates, not any individual client updates. While useful for ensuring privacy, SecAgg generally makes defenses against adversarial attacks more difficult to implement, as the central server only sees the aggregate of the client updates. Therefore, it is of fundamental interest to investigate how to defend against adversarial attacks when secure aggregation is used. Existing approaches based on range proofs (e.g. Bulletproofs <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib92" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">92</span></a>]</cite>) can guarantee that the DP-based clipping defense described above is compatible with SecAgg, but developing computation- and communication-efficient range proofs is still an active research direction.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">SecAgg also introduces challenges for other defense methods. For example, many existing Byzantine-robust aggregation methods utilize non-linear operations on the server <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Xie et al.</span> [<a href="#bib.bib486" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">486</span></a>]</cite>, and it is not yet known if these methods are efficiently compatible with secure aggregation which was originally designed for linear aggregation. Recent work has found ways to approximate the geometric median under SecAgg <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib379" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">379</span></a>]</cite> by using a handful of SecAgg calls in a more general aggregation loop. However, it is not clear in general which aggregators can be computed under the use of SecAgg.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Executive Summary</h3>

<div id="S5.SS4.p1" class="ltx_para">
<ul id="S5.I7" class="ltx_itemize">
<li id="S5.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I7.i1.p1" class="ltx_para">
<p id="S5.I7.i1.p1.1" class="ltx_p">Third-party participants in the training process introduces new capabilities and attack vectors for adversaries, categorized in Table <a href="#S5.T10" title="Table 10 ‣ Capabilities ‣ 5.1.1 Goals and Capabilities of an Adversary ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</li>
<li id="S5.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I7.i2.p1" class="ltx_para">
<p id="S5.I7.i2.p1.1" class="ltx_p">Federated learning introduces a new kind of poisoning attacks, <em id="S5.I7.i2.p1.1.1" class="ltx_emph ltx_font_italic">model update poisoning</em> (Section <a href="#S5.SS1.SSS2" title="5.1.2 Model Update Poisoning ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>), while also being susceptible to traditional <em id="S5.I7.i2.p1.1.2" class="ltx_emph ltx_font_italic">data poisoning</em> in (Section <a href="#S5.SS1.SSS3" title="5.1.3 Data Poisoning Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>).</p>
</div>
</li>
<li id="S5.I7.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I7.i3.p1" class="ltx_para">
<p id="S5.I7.i3.p1.1" class="ltx_p">Training participants can influence the optimization process possibly exacerbating inference-time (Section <em id="S5.I7.i3.p1.1.1" class="ltx_emph ltx_font_italic">evasion attacks</em>) <a href="#S5.SS1.SSS4" title="5.1.4 Inference-Time Evasion Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>, and communication and computation constraints may render previously proposed defenses impractical.</p>
</div>
</li>
<li id="S5.I7.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I7.i4.p1" class="ltx_para">
<p id="S5.I7.i4.p1.1" class="ltx_p">Non-malicious failure modes (Section <a href="#S5.SS2" title="5.2 Non-Malicious Failure Modes ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>) are can be especially different to deal with, as access to raw data is not available in the federated setting, though through some lens they may be related to poisoning attacks.</p>
</div>
</li>
<li id="S5.I7.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I7.i5.p1" class="ltx_para">
<p id="S5.I7.i5.p1.1" class="ltx_p">Tension may exist when trying to simultaneously improve robustness and privacy in machine learning (Section <a href="#S5.SS3" title="5.3 Exploring the Tension between Privacy and Robustness ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p">Areas identified for further exploration include:</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<ul id="S5.I8" class="ltx_itemize">
<li id="S5.I8.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i1.p1" class="ltx_para">
<p id="S5.I8.i1.p1.1" class="ltx_p">Quantify the relationship between data poisoning and model update poisoning attacks. Are there scenarios where they are not equivalent? [<a href="#S5.SS1.SSS3.Px3" title="Relationship between model update poisoning and data poisoning ‣ 5.1.3 Data Poisoning Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>]</p>
</div>
</li>
<li id="S5.I8.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i2.p1" class="ltx_para">
<p id="S5.I8.i2.p1.1" class="ltx_p">Quantify how training time attacks impact inference-time vulnerabilities. Improving inference-time robustness guarantees requires going beyond detecting backdoor attacks. [<a href="#S5.SS1.SSS4.Px1" title="Relationship between training-time and inference-time attacks ‣ 5.1.4 Inference-Time Evasion Attacks ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.4</span></a>]</p>
</div>
</li>
<li id="S5.I8.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i3.p1" class="ltx_para">
<p id="S5.I8.i3.p1.1" class="ltx_p">Adversarial training has been used as a defense in the centralized setting, but can be impractical in the edge-compute limited cross-device federated setting. [<a href="#S5.SS1.SSS5.Px3" title="Defending against inference-time evasion attacks ‣ 5.1.5 Defensive Capabilities from Privacy Guarantees ‣ 5.1 Adversarial Attacks on Model Performance ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1.5</span></a>]</p>
</div>
</li>
<li id="S5.I8.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i4.p1" class="ltx_para">
<p id="S5.I8.i4.p1.1" class="ltx_p">Federated learning requires new methods and tools to support the developer, as access to raw data is restricted debugging ML pipelines is especially difficult. [<a href="#S5.SS2.SSS0.Px2" title="Data pipeline failures ‣ 5.2 Non-Malicious Failure Modes ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>]</p>
</div>
</li>
<li id="S5.I8.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i5.p1" class="ltx_para">
<p id="S5.I8.i5.p1.1" class="ltx_p">Tensions exists between robustness and fairness, as machine learning models can tend to discard updates far from the median as detrimental. However the federated setting can give rise to a long tail of users that may be mistaken for noisy model updates [<a href="#S5.SS2.SSS0.Px3" title="Noisy model updates ‣ 5.2 Non-Malicious Failure Modes ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>].</p>
</div>
</li>
<li id="S5.I8.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I8.i6.p1" class="ltx_para">
<p id="S5.I8.i6.p1.1" class="ltx_p">Cryptography-based aggregation methods and robustness techniques present integration challenges: protecting participant identity can be at odds with detecting adversarial participants. Proposed techniques remain beyond the scope of practicality, requiring the need of new communication and computation efficient algorithms. [<a href="#S5.SS3" title="5.3 Exploring the Tension between Privacy and Robustness ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>]</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ensuring Fairness and Addressing Sources of Bias</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Machine learning models can often exhibit surprising and unintended behaviours. When such behaviours lead to patterns of <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">undesirable</span> effects on users, we might categorize the model as “unfair” according to some criteria. For example, if people with similar characteristics receive quite different outcomes, then this violates the criterion of <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">individual fairness</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">169</span></a>]</cite>. If certain sensitive groups (races, genders, etc.) receive different patterns of outcomes—such as different false negative rates—this can violate various criteria of <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">demographic fairness</span>, see for instance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>, <a href="#bib.bib349" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">349</span></a>]</cite> for surveys. The criterion of <span id="S6.p1.1.4" class="ltx_text ltx_font_italic">counterfactual fairness</span> requires that a user receive the same treatment as they would have if they had been a member of a different group (race, gender, etc), after taking all causally relevant pathways into account <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib287" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">287</span></a>]</cite>.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Federated learning raises several opportunities for fairness research, some of which extend prior research directions in the non-federated setting, and others that are unique to federated learning. This section raises open problems in both categories.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Bias in Training Data</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">One driver of unfairness in machine-learned models is bias in the training data, including cognitive, sampling, reporting, and confirmation bias. One common antipattern is that minority or marginalized social groups are under-represented in the training data, and thus the learner weights these groups less during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib258" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">258</span></a>]</cite>, leading to inferior quality predictions for members of these groups (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>).</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Just as the data access processes used in federated learning may introduce dataset shift and non-independence (<a href="#S3.SS1" title="3.1 Non-IID Data in Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>), there is also a risk of introducing biases. For example:</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p">If devices are selected for updates when plugged-in or fully charged, then model updates and evaluations computed at different times of day may be correlated with factors such as day-shift vs night-shift work schedules.</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p">If devices are selected for updates from among the pool of eligible devices at a given time, then devices that are connected at times when few other devices are connected (e.g. night-shift or unusual time zone) may be over-represented in the aggregated output.</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p">If selected devices are more likely to have their output kept when the output is computed faster, then: a) output from devices with faster processors may be over-represented, with these devices likely newer devices and thus correlated with socioeconomic status; and b) devices with less data may be over-represented, with these devices possibly representing users who use the product less frequently.</p>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p">If data nodes have different amounts of data, then federated learning may weigh higher the contributions of populations which are heavy users of the product or feature generating the data.</p>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p">If the update frequency depends on latency, then certain geographic regions and populations with slower devices or networks may be under-represented.</p>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p">If populations of <span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_italic">potential users</span> do not own devices for socio-economic reasons, they may be under-represented in the training dataset, and subsequently also under- (or un-)represented in model training and evaluation.</p>
</div>
</li>
<li id="S6.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i7.p1" class="ltx_para">
<p id="S6.I1.i7.p1.1" class="ltx_p">Unweighted aggregation of the model loss across selected devices during federated training may disadvantage model performance on certain devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">302</span></a>]</cite>.</p>
</div>
</li>
</ul>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">It has been observed that biases in the data-generating process can also drive unfairness in the resulting models learned from this data (see e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib170" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">170</span></a>, <a href="#bib.bib394" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">394</span></a>]</cite>). For example, suppose training data is based on user interactions with a product which has failed to incorporate inclusive design principles. Then, the user interactions with the product might not express user intents (cf. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib403" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">403</span></a>]</cite>, for example) but rather might express coping strategies
around uninclusive product designs (and hence might require a fundamental fix to the product interaction model). Learning from such interactions might then ignore or perpetuate poor experiences for some groups of product users in ways which can be difficult to detect while maintaining privacy in a federated setting. This risk is shared by all machine learning scenarios where training data is derived from user interaction, but is of particular note in the federated setting when data is collected from apps on individual devices.</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<p id="S6.SS1.p5.1" class="ltx_p">Investigating the degree to which biases in the data-generated process can be identified or mitigated is a crucial problem for both federated learning research and ML research more broadly. Similarly, while limited prior research has demonstrated methods to identify and correct bias in already collected data in the federated setting (e.g. via adversarial methods in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib255" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">255</span></a>]</cite>), further research in this area is needed. Finally, methods for applying post-hoc fairness corrections to models learned from potentially biased training data are also a valuable direction for future work.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Fairness Without Access to Sensitive Attributes</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">Having explicit access to demographic information (race, gender, etc) is critical to many existing fairness criteria, including those discussed in Section <a href="#S6.SS1" title="6.1 Bias in Training Data ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>. However, the contexts in which federated learning are often deployed also give rise to considerations of fairness when individual sensitive attributes are <span id="S6.SS2.p1.1.1" class="ltx_text ltx_font_italic">not</span> available. For example, this can occur when developing personalized language models or developing fair medical image classifiers without knowing any additional demographic information about individuals. Even more fundamentally, the assumed one-to-one relationship between individuals and devices often breaks down, especially in non-Western contexts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib403" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">403</span></a>]</cite>. Both measuring and correcting unfairness in contexts where there is no data regarding sensitive group membership is a key area for federated learning researchers to address.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.2" class="ltx_p">Limited existing research has examined fairness without access to sensitive attributes. For example, this has been addressed using distributionally-robust optimization (DRO) which optimizes for the worst-case outcome across all individuals during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib225" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite>, and via multicalibration, which calibrates for fairness across subsets of the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib232" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">232</span></a>]</cite>. Even these existing approaches have not been applied in the federated setting, raising opportunities for future empirical work. The challenge of how to make these approaches work for large-scale, high-dimensional data typical to federated settings is also an open problem, as DRO and multicalibration both pose challenges of scaling with large <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mi id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">n</annotation></semantics></math> and <math id="S6.SS2.p2.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S6.SS2.p2.2.m2.1a"><mi id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><ci id="S6.SS2.p2.2.m2.1.1.cmml" xref="S6.SS2.p2.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">p</annotation></semantics></math>. Finally, the development of additional theoretical approaches to defining fairness without respect to “sensitive attributes” is a critical area for further research.</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p">Other ways to approach this include reframing the existing notions of fairness, which are primarily concerned with equalizing the probability of an outcome (one of which is considered “positive” and another “negative” for the affected individual). Instead, fairness without access to sensitive attributes might be reframed as <span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_italic">equal access to effective models</span>. Under this interpretation of fairness, the goal is to maximize model utility across all individuals, regardless of their (unknown) demographic identities, and regardless of the “goodness“ of an individual outcome. Again, this matches the contexts in which federated learning is most commonly used, such as language modeling or medical image classification, where there is no clear notion of an outcome which is “good” for a user, and instead the aim is simply to make correct predictions for users, regardless of the outcome.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">Existing federated learning research suggests possible ways to meet such an interpretation of fairness, e.g. via personalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib250" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">250</span></a>, <a href="#bib.bib472" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">472</span></a>]</cite>. A similar conception of fairness, as “a more
fair distribution of the model performance across devices”, is employed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">302</span></a>]</cite>.</p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p">The application of attribute-independent methods explicitly to ensure equitable model performance is an open opportunity for future federated learning research, and is particularly important as federated learning reaches maturity and sees increasing deployment with real populations of users without knowledge of their sensitive identities.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Fairness, Privacy, and Robustness</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Fairness and data privacy seem to be complementary ethical concepts: in many of the real-world contexts where privacy protection is desired, fairness is also desired. Often this is due to the sensitivity of the underlying data. Because federated learning is most likely to be deployed in contexts of sensitive data where both privacy and fairness are desirable, it is important that FL research examines how FL might be able to address existing concerns about fairness in machine learning, and whether FL raises new fairness-related issues.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">In some ways, however, the ideal of fairness seems to be in tension with the notions of privacy for which FL seeks to provide guarantees: differentially-private learning typically seeks to obscure individually-identifying characteristics, while fairness often requires knowing individuals’ membership in sensitive groups in order to measure or ensure fair predictions are being made. While the trade-off between differential privacy and fairness has been investigated in the non-federated setting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib246" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">246</span></a>, <a href="#bib.bib145" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">145</span></a>]</cite>, there has been little work on how (or whether) FL may be able to uniquely address concerns about fairness.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Recent evidence suggesting that differentially-private learning can have disparate impact on sensitive subgroups <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>, <a href="#bib.bib145" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">145</span></a>, <a href="#bib.bib246" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">246</span></a>, <a href="#bib.bib283" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">283</span></a>]</cite> provides further motivation to investigate whether FL may be able to address such concerns. A potential solution to relax the tension between privacy (which aims to protect the model from being too dependent on individuals) and fairness (which encourages the model to perform well on under-represented classes) may be the application of techniques such as personalization (discussed in <a href="#S3.SS3" title="3.3 Multi-Task Learning, Personalization, and Meta-Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.3</span></a>) and “hybrid differential privacy,” where some users donate data with lesser privacy guarantees <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">Furthermore, current differentially-private optimization schemes are applied without respect to sensitive attributes – from this perspective, it might be expected that empirical studies have shown evidence that differentially-private optimization impacts minority subgroups the most <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>. Modifications to differentially-private optimization algorithms which explicitly seek to preserve performance on minority subgroups, e.g. by adapting the noise and clipping mechanisms to account for the representation of groups within the data, would also likely do a great deal to limit potential disparate impacts of differentially-private modeling on minority subgroups in federated models trained with differential privacy. However, implementing such adaptive differentially-private mechanisms in a way that provides some form of privacy guarantee presents both algorithmic and theoretical challenges which need to be addressed by future work.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.1" class="ltx_p">Further research is also needed to determine the extent to which the issues above arise in the federated setting. Furthermore, as noted in <a href="#S6.SS2" title="6.2 Fairness Without Access to Sensitive Attributes ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.2</span></a>, the challenge of evaluating the impact of differential privacy on model fairness becomes particularly difficult when sensitive attributes are not available, as it is unclear how to identify subgroups for which a model is behaving badly and to quantify the “price” of differential privacy – investigating and addressing these challenges is an open problem for future work.</p>
</div>
<div id="S6.SS3.p6" class="ltx_para">
<p id="S6.SS3.p6.1" class="ltx_p">More broadly, one could more generally examine the relation between privacy, fairness, and <em id="S6.SS3.p6.1.1" class="ltx_emph ltx_font_italic">robustness</em> (see Section <a href="#S5" title="5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). Many previous works on machine learning, including federated learning, typically focus on isolated aspects of robustness (either against poisoning, or against evasion), privacy, or fairness. An important open challenge is to develop a joint understanding of federated learning systems that are robust, private, and fair. Such an integrated approach can provide opportunities to benefit from disparate but complementary mechanisms. Differential privacy mechanisms can be used to both mitigate data inference attacks, and provide a foundation for robustness against data poisoning. On the other hand, such an integrated approach also reveals new vulnerabilities. For example, recent work has revealed a trade-off between privacy and robustness against adversarial examples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib429" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">429</span></a>]</cite>.</p>
</div>
<div id="S6.SS3.p7" class="ltx_para">
<p id="S6.SS3.p7.1" class="ltx_p">Finally, privacy and fairness naturally meet in the context of
learning data representations that are independent of some sensitive attributes while preserving
utility for a task of interest. Indeed, this objective can be motivated
both in terms of privacy: to transform data so as to hide private
attributes, and fairness: as a way to make models trained on such representations
fair with respect to the attributes.
In the centralized setting, one way to learn such representations is through
adversarial training techniques, which have been applied to image and speech
data <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib255" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">255</span></a>, <a href="#bib.bib186" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">186</span></a>, <a href="#bib.bib327" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">327</span></a>, <a href="#bib.bib65" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">65</span></a>, <a href="#bib.bib431" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">431</span></a>]</cite>. In
the federated learning scenario, clients could apply the
transformation locally to their data in order to enforce or improve privacy and/or fairness guarantees for the FL process. However, learning this transformation in a federated fashion (potentially under privacy and/or fairness constraints) is itself an open question.</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Leveraging Federation to Improve Model Diversity</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">Federated learning presents the opportunity to integrate, through distributed training, datasets which may have previously been impractical or even illegal to combine in a single location. For example, the Health Insurance Portability and Accountability Act (HIPAA) and the Family Educational Rights and Privacy Act (FERPA) constrain the sharing of medical patient data and student educational data, respectively, in the United States. To date, these restrictions have led to modeling occurring in institutional silos: for example, using electronic health records or clinical images from individual medical institutions instead of pooling data and models across institutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">91</span></a>, <a href="#bib.bib104" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">104</span></a>]</cite>. In contexts where membership in institutional datasets is correlated with individuals’ specific sensitive attributes, or their behavior and outcomes more broadly, this can lead to poor representation for users in groups underrepresented at those institutions. Importantly, this lack of representation and diversity in the training data has been shown to lead to poor performance, e.g. in genetic disease models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib333" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">333</span></a>]</cite> and image classification models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">93</span></a>]</cite>.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Federated learning presents an opportunity to leverage uniquely diverse datasets by providing efficient decentralized training protocols along with privacy and non-identifiability guarantees for the resulting models. This means that federated learning enables training on multi-instutitional datasets in many domains where this was previously not possible. This provides a practical opportunity to leverage larger, more diverse datasets and explore the generalizability of models which were previously limited to small populations. More importantly, it provides an opportunity to improve the <span id="S6.SS4.p2.1.1" class="ltx_text ltx_font_italic">fairness</span> of these models by combining data across boundaries which are likely to have been correlated with sensitive attributes. For instance, attendance at specific health or educational institutions may be correlated with individuals’ ethnicity or socioeconomic status. As noted in Section <a href="#S6.SS1" title="6.1 Bias in Training Data ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a> above, underrepresentation in training data is a proven driver of model unfairness.</p>
</div>
<div id="S6.SS4.p3" class="ltx_para">
<p id="S6.SS4.p3.1" class="ltx_p">Future federated learning research should investigate the degree to which improving diversity in a federated training setting also improves the fairness of the resulting model, and the degree to which the differential privacy mechanisms required in such settings may limit fairness and performance gains from increased diversity. This includes a need for both empirical research which applies federated learning and quantifies the interplay between diversity, fairness, privacy, and performance; along with theoretical research which provides a foundation for concepts such as diversity in the context of machine learning fairness.</p>
</div>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Federated Fairness: New Opportunities and Challenges</h3>

<div id="S6.SS5.p1" class="ltx_para">
<p id="S6.SS5.p1.1" class="ltx_p">It is important to note that federated learning provides unique opportunities and challenges for fairness researchers. For example, by allowing for datasets which are distributed both by observation, but even by features, federated learning can enable modeling and research using partitioned data which may be too sensitive to share directly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib215" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">215</span></a>, <a href="#bib.bib224" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">224</span></a>]</cite>. Increased availability of datasets which can be used in a federated manner can help to improve the diversity of training data available for machine learning models, which can advance fair modeling theory and practice.</p>
</div>
<div id="S6.SS5.p2" class="ltx_para">
<p id="S6.SS5.p2.1" class="ltx_p">Researchers and practitioners also need to address the unique fairness-related challenges created by federated learning. For example, federated learning can introduce new sources of bias through the decision of which clients to sample based on considerations such as connection type/quality, device type, location, activity patterns, and local dataset size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite>. Future work could investigate the degree to which these various sampling constraints affect the fairness of the resulting model, and how such impacts can be mitigated within the federated framework, e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib302" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">302</span></a>, <a href="#bib.bib289" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">289</span></a>, <a href="#bib.bib158" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">158</span></a>]</cite>. Frameworks such as <em id="S6.SS5.p2.1.1" class="ltx_emph ltx_font_italic">agnostic federated learning</em> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib352" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">352</span></a>]</cite> provide approaches to control for bias in the training objective. Work to improve the fairness of existing federated training algorithms will be particularly important as advances begin to approach the technical limits of other components of FL systems, such as model compression, which initially helped to broaden the diversity of candidate clients during federated training processes. There is no unique fairness criterion generally adopted in
the study of fairness, and multiple criteria have been proven to be mutually incompatible. One way to deal with this
question is the <em id="S6.SS5.p2.1.2" class="ltx_emph ltx_font_italic">online fairness</em> framework and algorithms of <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Awasthi et al.</span> [<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>.
Adapting such solutions to the federated learning setting and further improving upon them will be challenging
research questions in ML fairness theory and algorithms.</p>
</div>
<div id="S6.SS5.p3" class="ltx_para">
<p id="S6.SS5.p3.1" class="ltx_p">In the classical centralized machine learning setting, a substantial amount of advancement has been made in the past decade to train fair classifiers, such as constrained optimization, post-shifting approaches, and distributionally-robust optimization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib223" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">223</span></a>, <a href="#bib.bib503" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">503</span></a>, <a href="#bib.bib225" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">225</span></a>]</cite>. It is an open question whether such approaches, which have demonstrated utility for improving fairness in centralized training, could be used under the setting of federated learning (and if so, under what additional assumptions) in which data are located in a decentralized fashion and practitioners may not obtain an unbiased sample of the data that match the distribution of the population.</p>
</div>
</section>
<section id="S6.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.6 </span>Executive Summary</h3>

<div id="S6.SS6.p1" class="ltx_para">
<p id="S6.SS6.p1.1" class="ltx_p">In addition to inheriting the already significant challenges related to bias, fairness, and privacy in centralized machine learning, federated learning also brings a new set of distinct challenges and opportunities in these areas. The importance of these considerations will likely continue to grow as the real-world deployment of FL expands to more users, domains, and applications.</p>
</div>
<div id="S6.SS6.p2" class="ltx_para">
<ul id="S6.I2" class="ltx_itemize">
<li id="S6.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i1.p1" class="ltx_para">
<p id="S6.I2.i1.p1.1" class="ltx_p">Bias in training data (Section <a href="#S6.SS1" title="6.1 Bias in Training Data ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.1</span></a>) is a key consideration related to bias and fairness in FL models, particularly due to the additional sampling steps germane to federation (e.g., client sampling) and the transfer of some model computation to client devices.</p>
</div>
</li>
<li id="S6.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i2.p1" class="ltx_para">
<p id="S6.I2.i2.p1.1" class="ltx_p">The lack of data regarding sensitive attributes in many FL deployments can pose challenges for measuring and ensuring fairness, and also suggests potential reframing of fairness problems in ways that do not require such data (Section <a href="#S6.SS2" title="6.2 Fairness Without Access to Sensitive Attributes ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a>).</p>
</div>
</li>
<li id="S6.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i3.p1" class="ltx_para">
<p id="S6.I2.i3.p1.1" class="ltx_p">Since FL is often deployed in contexts which are both privacy- and fairness-sensitive, this can magnify tensions between privacy and fairness objectives in practice. Further work is needed to address the potential tension between methods which achieve privacy, fairness, and robustness in both federated and centralized learning (Section <a href="#S6.SS3" title="6.3 Fairness, Privacy, and Robustness ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>).</p>
</div>
</li>
<li id="S6.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i4.p1" class="ltx_para">
<p id="S6.I2.i4.p1.1" class="ltx_p">Federated learning presents unique opportunities to improve the diversity of stakeholders and data incorporated into learning, which could improve both the overall quality of downstream models, as well as their fairness due to more representative datasets (Section <a href="#S6.SS4" title="6.4 Leveraging Federation to Improve Model Diversity ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4</span></a>).</p>
</div>
</li>
<li id="S6.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I2.i5.p1" class="ltx_para">
<p id="S6.I2.i5.p1.1" class="ltx_p">Federated learning presents fairness-related challenges not present in the centralized training regime, but also affords new solutions (Section <a href="#S6.SS5" title="6.5 Federated Fairness: New Opportunities and Challenges ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.5</span></a>).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Addressing System Challenges</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">As we will see in this section, the challenges in building systems for federated learning can be split fairly cleanly into the two separate settings of cross-device and cross-silo federated learning (see <a href="#S1.SS1" title="1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sections</span> <span class="ltx_text ltx_ref_tag">1.1</span></a> and <a href="#S2.SS2" title="2.2 Cross-Silo Federated Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>). We start with a brief discussion of the difficulties inherent to any large scale deployment of software on end-user devices (although exacerbated by the complexity of a federated learning stack); we then focus on key challenges specific to the cross-device learning—bias, tuning, and efficient device-side execution of ML workflows—before concluding with a brief treatment of the cross-silo setting.</p>
</div>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Platform Development and Deployment Challenges</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">Running computations on end-user devices is considerably different from the data center setting:</p>
<ul id="S7.I1" class="ltx_itemize">
<li id="S7.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i1.p1" class="ltx_para">
<p id="S7.I1.i1.p1.1" class="ltx_p">Due to the heterogeneity of the fleet (devices may differ in hardware, software, connectivity, performance and persisted state) the space of potential problems and edge cases is vast and cannot typically be covered in sufficient detail with automated testing.</p>
</div>
</li>
<li id="S7.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i2.p1" class="ltx_para">
<p id="S7.I1.i2.p1.1" class="ltx_p">Monitoring and debugging are harder because telemetry is limited, delayed, and there is no physical access to devices for interactive troubleshooting.</p>
</div>
</li>
<li id="S7.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I1.i3.p1" class="ltx_para">
<p id="S7.I1.i3.p1.1" class="ltx_p">Running computations should not affect device performance or stability, i.e. should be invisible to users.</p>
</div>
</li>
</ul>
</div>
<section id="S7.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Code Deployment</h5>

<div id="S7.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px1.p1.1" class="ltx_p">Installing, updating and running software on end user devices may involve not only extensive manual and automated testing, but a gradual and reversible rollout (for example, through guarding new functionality with server-controlled feature flags) while monitoring key performance metrics in a/b experiments such as crash rates, memory use, and application-dependent indicators such as latencies and engagement metrics. Such rollouts can take weeks or months depending on the percolation rate of updates (particularly challenging for devices with spotty connectivity) and the complexity of the upgrade (e.g. protocol changes). Hence, the install base at any given time will involve various releases. While this problem is not specific to federated learning, it has greater impact here due to the inherent collaborative nature of federated computations: devices constantly communicate with servers and indirectly with other devices to exchange models and parameter updates. Thus, compatibility concerns abound and must be addressed through stable exchange formats or, where not possible, detected upfront with extensive testing infrastructure. We will revisit this problem in <a href="#S7.SS4" title="7.4 On-Device Runtime ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>.</p>
</div>
</section>
<section id="S7.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Monitoring and Debugging</h5>

<div id="S7.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p1.1" class="ltx_p">Another significant complication is the limited ability to monitor devices and interactively debug problems. While telemetry from end user devices is necessary to detect problems, privacy concerns severely restrict what can be logged, who can access such logs, and how long they are retained. Once a regression is detected, drilling down into the root cause can be very cumbersome due to the lack of detailed context, the vast problem space (a cross product of software versions, hardware, models, and device state), and very limited ability for interactive debugging short of successfully reproducing the problem in a controlled environment.</p>
</div>
<div id="S7.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p2.1" class="ltx_p">These challenges are exacerbated in the federated learning setting where a) raw input data on devices cannot be accessed, and b) contributions from individual devices are by design anonymous, ephemeral, and exposed only in aggregate. These properties preserve privacy, but also may make it hard or impossible to investigate problems with traditional approaches —by looking for correlations with hardware or software version, or testing hypotheses that require access to raw data. Reproducing a problem in a controlled setting is often difficult due to the gap between such an environment and reality: hundreds of heterogeneous embedded stateful devices with non-iid data.</p>
</div>
<div id="S7.SS1.SSS0.Px2.p3" class="ltx_para">
<p id="S7.SS1.SSS0.Px2.p3.1" class="ltx_p">Interestingly, federated technologies themselves can help to mitigate this problem—for instance, the use of federated analytics <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib382" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">382</span></a>]</cite> to collect logs in a privacy preserving manner, or training generative models of the system behavior or raw data for sampling during debugging (see sections <a href="#S3.SS4.SSS3" title="3.4.3 Debugging and Interpretability for FL ‣ 3.4 Adapting ML Workflows for Federated Learning ‣ 3 Improving Efficiency and Effectiveness ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>, <a href="#S5.SS2" title="5.2 Non-Malicious Failure Modes ‣ 5 Defending Against Attacks and Failures ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>, and <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>).
Keeping a federated learning system up and running thus requires investing into upfront detection of problems through a) extensive automated, continuous test coverage of all software layers through both unit and integration tests; b) feature flags and a/b rollouts; and c) continuous monitoring of performance indicators for regressions. That poses a significant investment that may come at too high a cost for smaller entities who would benefit greatly from shared and tested infrastructure for federated learning.</p>
</div>
</section>
</section>
<section id="S7.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>System Induced Bias</h3>

<div id="S7.SS2.p1" class="ltx_para">
<p id="S7.SS2.p1.1" class="ltx_p">Deployment, monitoring and debugging may not concern users of a federated learning platform, e.g. model authors or data analysts. For them, the key differences between data center and cross-device settings fall largely into the following two categories:</p>
</div>
<div id="S7.SS2.p2" class="ltx_para">
<ol id="S7.I2" class="ltx_enumerate">
<li id="S7.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I2.i1.p1" class="ltx_para">
<p id="S7.I2.i1.p1.1" class="ltx_p"><span id="S7.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Availability of devices</span> for computations is not a given, but varies over time and across devices. Connections are initiated by devices and subject to interruptions due to changes in device state, operating system quotas, and network connectivity. Hence, in iterative processes like federated learning, the loop body is run on a small subset of all devices only, and the system must tolerate a certain failure rate among those devices.</p>
</div>
</li>
<li id="S7.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I2.i2.p1" class="ltx_para">
<p id="S7.I2.i2.p1.1" class="ltx_p"><span id="S7.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Capabilities of devices</span> (network bandwidth and latency, compute performance, memory) vary, and are typically much lower than those of compute nodes in the data center, though the number of nodes is typically higher. The amount and type of data across devices may lead to variations in execution profile, e.g. more and larger examples lead to increased resource use and processing time.</p>
</div>
</li>
</ol>
</div>
<div id="S7.SS2.p3" class="ltx_para">
<p id="S7.SS2.p3.1" class="ltx_p">In the following sections we discuss how these variations might introduce bias, referring to it as system induced bias to differentiate it from platform-independent bias in the raw data (such as ownership or usage patterns differing across demographics)—for the latter, see <a href="#S6.SS1" title="6.1 Bias in Training Data ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.1</span></a>.</p>
</div>
<section id="S7.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Device Availability Profiles</h4>

<div id="S7.SS2.SSS1.p1" class="ltx_para">
<p id="S7.SS2.SSS1.p1.1" class="ltx_p">At the core of cross-device federated learning is the principle that devices only connect to the server and run computations when various constraints are met:</p>
<ul id="S7.I3" class="ltx_itemize">
<li id="S7.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i1.p1" class="ltx_para">
<p id="S7.I3.i1.p1.1" class="ltx_p"><span id="S7.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Hard constraints</span>, which might include requiring that the device is turned on, has network connectivity to the server, and is allowed to run a computation by the operating system.</p>
</div>
</li>
<li id="S7.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I3.i2.p1" class="ltx_para">
<p id="S7.I3.i2.p1.1" class="ltx_p"><span id="S7.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Soft constraints</span>, which might include the conditions on device state chosen to ensure that federated learning does not incur charges or affect usability. For the common case of mobile phones <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, requirements may include idleness, charging and/or above a certain battery level, being connected to an unmetered network, and that no other federated learning tasks are running at the same time.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS2.SSS1.p2" class="ltx_para">
<p id="S7.SS2.SSS1.p2.3" class="ltx_p">Taken together, these constraints induce an unknown, time-varying and device-specific function <math id="S7.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="A_{i}(t)" display="inline"><semantics id="S7.SS2.SSS1.p2.1.m1.1a"><mrow id="S7.SS2.SSS1.p2.1.m1.1.2" xref="S7.SS2.SSS1.p2.1.m1.1.2.cmml"><msub id="S7.SS2.SSS1.p2.1.m1.1.2.2" xref="S7.SS2.SSS1.p2.1.m1.1.2.2.cmml"><mi id="S7.SS2.SSS1.p2.1.m1.1.2.2.2" xref="S7.SS2.SSS1.p2.1.m1.1.2.2.2.cmml">A</mi><mi id="S7.SS2.SSS1.p2.1.m1.1.2.2.3" xref="S7.SS2.SSS1.p2.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p2.1.m1.1.2.1" xref="S7.SS2.SSS1.p2.1.m1.1.2.1.cmml">​</mo><mrow id="S7.SS2.SSS1.p2.1.m1.1.2.3.2" xref="S7.SS2.SSS1.p2.1.m1.1.2.cmml"><mo stretchy="false" id="S7.SS2.SSS1.p2.1.m1.1.2.3.2.1" xref="S7.SS2.SSS1.p2.1.m1.1.2.cmml">(</mo><mi id="S7.SS2.SSS1.p2.1.m1.1.1" xref="S7.SS2.SSS1.p2.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S7.SS2.SSS1.p2.1.m1.1.2.3.2.2" xref="S7.SS2.SSS1.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p2.1.m1.1b"><apply id="S7.SS2.SSS1.p2.1.m1.1.2.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2"><times id="S7.SS2.SSS1.p2.1.m1.1.2.1.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2.1"></times><apply id="S7.SS2.SSS1.p2.1.m1.1.2.2.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S7.SS2.SSS1.p2.1.m1.1.2.2.1.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2.2">subscript</csymbol><ci id="S7.SS2.SSS1.p2.1.m1.1.2.2.2.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2.2.2">𝐴</ci><ci id="S7.SS2.SSS1.p2.1.m1.1.2.2.3.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S7.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S7.SS2.SSS1.p2.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p2.1.m1.1c">A_{i}(t)</annotation></semantics></math> for a device <math id="S7.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S7.SS2.SSS1.p2.2.m2.1a"><mi id="S7.SS2.SSS1.p2.2.m2.1.1" xref="S7.SS2.SSS1.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p2.2.m2.1b"><ci id="S7.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S7.SS2.SSS1.p2.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p2.2.m2.1c">i</annotation></semantics></math>, and a fleet-wide <span id="S7.SS2.SSS1.p2.3.1" class="ltx_text ltx_font_italic">availability profile</span> <math id="S7.SS2.SSS1.p2.3.m3.2" class="ltx_Math" alttext="A(t)=\sum_{i}A_{i}(t)" display="inline"><semantics id="S7.SS2.SSS1.p2.3.m3.2a"><mrow id="S7.SS2.SSS1.p2.3.m3.2.3" xref="S7.SS2.SSS1.p2.3.m3.2.3.cmml"><mrow id="S7.SS2.SSS1.p2.3.m3.2.3.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.cmml"><mi id="S7.SS2.SSS1.p2.3.m3.2.3.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p2.3.m3.2.3.2.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.1.cmml">​</mo><mrow id="S7.SS2.SSS1.p2.3.m3.2.3.2.3.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.cmml"><mo stretchy="false" id="S7.SS2.SSS1.p2.3.m3.2.3.2.3.2.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.cmml">(</mo><mi id="S7.SS2.SSS1.p2.3.m3.1.1" xref="S7.SS2.SSS1.p2.3.m3.1.1.cmml">t</mi><mo stretchy="false" id="S7.SS2.SSS1.p2.3.m3.2.3.2.3.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S7.SS2.SSS1.p2.3.m3.2.3.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.1.cmml">=</mo><mrow id="S7.SS2.SSS1.p2.3.m3.2.3.3" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.cmml"><msub id="S7.SS2.SSS1.p2.3.m3.2.3.3.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1.cmml"><mo id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1.2.cmml">∑</mo><mi id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.3" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1.3.cmml">i</mi></msub><mrow id="S7.SS2.SSS1.p2.3.m3.2.3.3.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.cmml"><msub id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.cmml"><mi id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.2.cmml">A</mi><mi id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.3" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.1.cmml">​</mo><mrow id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.3.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.cmml"><mo stretchy="false" id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.3.2.1" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.cmml">(</mo><mi id="S7.SS2.SSS1.p2.3.m3.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.2.cmml">t</mi><mo stretchy="false" id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.3.2.2" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS1.p2.3.m3.2b"><apply id="S7.SS2.SSS1.p2.3.m3.2.3.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3"><eq id="S7.SS2.SSS1.p2.3.m3.2.3.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.1"></eq><apply id="S7.SS2.SSS1.p2.3.m3.2.3.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.2"><times id="S7.SS2.SSS1.p2.3.m3.2.3.2.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.1"></times><ci id="S7.SS2.SSS1.p2.3.m3.2.3.2.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.2.2">𝐴</ci><ci id="S7.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.1.1">𝑡</ci></apply><apply id="S7.SS2.SSS1.p2.3.m3.2.3.3.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3"><apply id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1"><csymbol cd="ambiguous" id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1">subscript</csymbol><sum id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1.2"></sum><ci id="S7.SS2.SSS1.p2.3.m3.2.3.3.1.3.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.1.3">𝑖</ci></apply><apply id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2"><times id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.1"></times><apply id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2"><csymbol cd="ambiguous" id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.1.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2">subscript</csymbol><ci id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.2">𝐴</ci><ci id="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.3.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.3.3.2.2.3">𝑖</ci></apply><ci id="S7.SS2.SSS1.p2.3.m3.2.2.cmml" xref="S7.SS2.SSS1.p2.3.m3.2.2">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS1.p2.3.m3.2c">A(t)=\sum_{i}A_{i}(t)</annotation></semantics></math>. Round completion rates and server traffic patterns <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib491" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">491</span></a>]</cite> suggest that availability profiles for mobile phones are clustered into periodic functions with a period of 1 day, varying across devices in phase, shape and amplitude through factors such as demographics, geography etc. Availability for other end user devices such as laptops, tablets, or stationary devices such as smart speakers, displays and cameras, will differ, but the challenges discussed in the following sections apply there as well, albeit to a possibly lesser extent.</p>
</div>
</section>
<section id="S7.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Examples of System Induced Bias</h4>

<div id="S7.SS2.SSS2.p1" class="ltx_para">
<p id="S7.SS2.SSS2.p1.2" class="ltx_p">Sources of bias will depend on the specific way in which devices are selected to participate in training, and how the system influences which devices end up contributing to the final aggregated model update. Thus, it is useful to discuss these issues in light of a simplified but representative system design. In an iterative federated learning algorithm, such as Federated Averaging (<a href="#S1.SS1.SSS2" title="1.1.2 A Typical Federated Training Process ‣ 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1.1.2</span></a>, <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib337" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">337</span></a>]</cite>), rounds are run consecutively on sets of at least <math id="S7.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.SS2.SSS2.p1.1.m1.1a"><mi id="S7.SS2.SSS2.p1.1.m1.1.1" xref="S7.SS2.SSS2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS2.p1.1.m1.1b"><ci id="S7.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S7.SS2.SSS2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS2.p1.1.m1.1c">M</annotation></semantics></math> devices. To accommodate a fraction <math id="S7.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S7.SS2.SSS2.p1.2.m2.1a"><mi id="S7.SS2.SSS2.p1.2.m2.1.1" xref="S7.SS2.SSS2.p1.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS2.p1.2.m2.1b"><ci id="S7.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S7.SS2.SSS2.p1.2.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS2.p1.2.m2.1c">d</annotation></semantics></math> of devices not contributing due to changes in device conditions, time-outs, or slowness (server-side aborts to avoid slow-downs by stragglers), an over-allocation scheme is used where</p>
<ol id="S7.I4" class="ltx_enumerate">
<li id="S7.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I4.i1.p1" class="ltx_para">
<p id="S7.I4.i1.p1.1" class="ltx_p">Rounds are started when at least <math id="S7.I4.i1.p1.1.m1.1" class="ltx_Math" alttext="M^{\prime}=\frac{M}{1-d}" display="inline"><semantics id="S7.I4.i1.p1.1.m1.1a"><mrow id="S7.I4.i1.p1.1.m1.1.1" xref="S7.I4.i1.p1.1.m1.1.1.cmml"><msup id="S7.I4.i1.p1.1.m1.1.1.2" xref="S7.I4.i1.p1.1.m1.1.1.2.cmml"><mi id="S7.I4.i1.p1.1.m1.1.1.2.2" xref="S7.I4.i1.p1.1.m1.1.1.2.2.cmml">M</mi><mo id="S7.I4.i1.p1.1.m1.1.1.2.3" xref="S7.I4.i1.p1.1.m1.1.1.2.3.cmml">′</mo></msup><mo id="S7.I4.i1.p1.1.m1.1.1.1" xref="S7.I4.i1.p1.1.m1.1.1.1.cmml">=</mo><mfrac id="S7.I4.i1.p1.1.m1.1.1.3" xref="S7.I4.i1.p1.1.m1.1.1.3.cmml"><mi id="S7.I4.i1.p1.1.m1.1.1.3.2" xref="S7.I4.i1.p1.1.m1.1.1.3.2.cmml">M</mi><mrow id="S7.I4.i1.p1.1.m1.1.1.3.3" xref="S7.I4.i1.p1.1.m1.1.1.3.3.cmml"><mn id="S7.I4.i1.p1.1.m1.1.1.3.3.2" xref="S7.I4.i1.p1.1.m1.1.1.3.3.2.cmml">1</mn><mo id="S7.I4.i1.p1.1.m1.1.1.3.3.1" xref="S7.I4.i1.p1.1.m1.1.1.3.3.1.cmml">−</mo><mi id="S7.I4.i1.p1.1.m1.1.1.3.3.3" xref="S7.I4.i1.p1.1.m1.1.1.3.3.3.cmml">d</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S7.I4.i1.p1.1.m1.1b"><apply id="S7.I4.i1.p1.1.m1.1.1.cmml" xref="S7.I4.i1.p1.1.m1.1.1"><eq id="S7.I4.i1.p1.1.m1.1.1.1.cmml" xref="S7.I4.i1.p1.1.m1.1.1.1"></eq><apply id="S7.I4.i1.p1.1.m1.1.1.2.cmml" xref="S7.I4.i1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S7.I4.i1.p1.1.m1.1.1.2.1.cmml" xref="S7.I4.i1.p1.1.m1.1.1.2">superscript</csymbol><ci id="S7.I4.i1.p1.1.m1.1.1.2.2.cmml" xref="S7.I4.i1.p1.1.m1.1.1.2.2">𝑀</ci><ci id="S7.I4.i1.p1.1.m1.1.1.2.3.cmml" xref="S7.I4.i1.p1.1.m1.1.1.2.3">′</ci></apply><apply id="S7.I4.i1.p1.1.m1.1.1.3.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3"><divide id="S7.I4.i1.p1.1.m1.1.1.3.1.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3"></divide><ci id="S7.I4.i1.p1.1.m1.1.1.3.2.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3.2">𝑀</ci><apply id="S7.I4.i1.p1.1.m1.1.1.3.3.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3.3"><minus id="S7.I4.i1.p1.1.m1.1.1.3.3.1.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3.3.1"></minus><cn type="integer" id="S7.I4.i1.p1.1.m1.1.1.3.3.2.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3.3.2">1</cn><ci id="S7.I4.i1.p1.1.m1.1.1.3.3.3.cmml" xref="S7.I4.i1.p1.1.m1.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I4.i1.p1.1.m1.1c">M^{\prime}=\frac{M}{1-d}</annotation></semantics></math> devices are available.</p>
</div>
</li>
<li id="S7.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I4.i2.p1" class="ltx_para">
<p id="S7.I4.i2.p1.1" class="ltx_p">Rounds are closed as</p>
<ol id="S7.I4.i2.I1" class="ltx_enumerate">
<li id="S7.I4.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S7.I4.i2.I1.i1.p1" class="ltx_para">
<p id="S7.I4.i2.I1.i1.p1.1" class="ltx_p"><em id="S7.I4.i2.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Aborted</em> when more than <math id="S7.I4.i2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="M^{\prime}-M" display="inline"><semantics id="S7.I4.i2.I1.i1.p1.1.m1.1a"><mrow id="S7.I4.i2.I1.i1.p1.1.m1.1.1" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.cmml"><msup id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.cmml"><mi id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.2" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.2.cmml">M</mi><mo id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.3" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.3.cmml">′</mo></msup><mo id="S7.I4.i2.I1.i1.p1.1.m1.1.1.1" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.1.cmml">−</mo><mi id="S7.I4.i2.I1.i1.p1.1.m1.1.1.3" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.I4.i2.I1.i1.p1.1.m1.1b"><apply id="S7.I4.i2.I1.i1.p1.1.m1.1.1.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1"><minus id="S7.I4.i2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.1"></minus><apply id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2">superscript</csymbol><ci id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.2">𝑀</ci><ci id="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.2.3">′</ci></apply><ci id="S7.I4.i2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S7.I4.i2.I1.i1.p1.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I4.i2.I1.i1.p1.1.m1.1c">M^{\prime}-M</annotation></semantics></math> devices have disconnected, or</p>
</div>
</li>
<li id="S7.I4.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S7.I4.i2.I1.i2.p1" class="ltx_para">
<p id="S7.I4.i2.I1.i2.p1.2" class="ltx_p"><em id="S7.I4.i2.I1.i2.p1.2.1" class="ltx_emph ltx_font_italic">Successful</em> when at least <math id="S7.I4.i2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.I4.i2.I1.i2.p1.1.m1.1a"><mi id="S7.I4.i2.I1.i2.p1.1.m1.1.1" xref="S7.I4.i2.I1.i2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.I4.i2.I1.i2.p1.1.m1.1b"><ci id="S7.I4.i2.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I4.i2.I1.i2.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I4.i2.I1.i2.p1.1.m1.1c">M</annotation></semantics></math> devices have reported. One possible design choice is to stop after exactly <math id="S7.I4.i2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.I4.i2.I1.i2.p1.2.m2.1a"><mi id="S7.I4.i2.I1.i2.p1.2.m2.1.1" xref="S7.I4.i2.I1.i2.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.I4.i2.I1.i2.p1.2.m2.1b"><ci id="S7.I4.i2.I1.i2.p1.2.m2.1.1.cmml" xref="S7.I4.i2.I1.i2.p1.2.m2.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I4.i2.I1.i2.p1.2.m2.1c">M</annotation></semantics></math> devices; another possibility would be to keep waiting for stragglers (possibly up to some maximum time).</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
<p id="S7.SS2.SSS2.p1.3" class="ltx_p">This sequence, when combined with variable availability profiles, may introduce various forms of bias:</p>
<ol id="S7.I5" class="ltx_enumerate">
<li id="S7.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I5.i1.p1" class="ltx_para">
<p id="S7.I5.i1.p1.1" class="ltx_p">Selection Bias - whether a device is included in a round at time t depends on both</p>
<ol id="S7.I5.i1.I1" class="ltx_enumerate">
<li id="S7.I5.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S7.I5.i1.I1.i1.p1" class="ltx_para">
<p id="S7.I5.i1.I1.i1.p1.1" class="ltx_p">Its availability profile <math id="S7.I5.i1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="A_{i}(t)" display="inline"><semantics id="S7.I5.i1.I1.i1.p1.1.m1.1a"><mrow id="S7.I5.i1.I1.i1.p1.1.m1.1.2" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.cmml"><msub id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.cmml"><mi id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.2" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.2.cmml">A</mi><mi id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.3" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S7.I5.i1.I1.i1.p1.1.m1.1.2.1" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S7.I5.i1.I1.i1.p1.1.m1.1.2.3.2" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S7.I5.i1.I1.i1.p1.1.m1.1.2.3.2.1" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.cmml">(</mo><mi id="S7.I5.i1.I1.i1.p1.1.m1.1.1" xref="S7.I5.i1.I1.i1.p1.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S7.I5.i1.I1.i1.p1.1.m1.1.2.3.2.2" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.I5.i1.I1.i1.p1.1.m1.1b"><apply id="S7.I5.i1.I1.i1.p1.1.m1.1.2.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2"><times id="S7.I5.i1.I1.i1.p1.1.m1.1.2.1.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.1"></times><apply id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.1.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2">subscript</csymbol><ci id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.2.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.2">𝐴</ci><ci id="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.3.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S7.I5.i1.I1.i1.p1.1.m1.1.1.cmml" xref="S7.I5.i1.I1.i1.p1.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I5.i1.I1.i1.p1.1.m1.1c">A_{i}(t)</annotation></semantics></math></p>
</div>
</li>
<li id="S7.I5.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S7.I5.i1.I1.i2.p1" class="ltx_para">
<p id="S7.I5.i1.I1.i2.p1.2" class="ltx_p">The number of simultaneously connected devices: <math id="S7.I5.i1.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="&lt;M^{\prime}" display="inline"><semantics id="S7.I5.i1.I1.i2.p1.1.m1.1a"><mrow id="S7.I5.i1.I1.i2.p1.1.m1.1.1" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.cmml"><mi id="S7.I5.i1.I1.i2.p1.1.m1.1.1.2" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.2.cmml"></mi><mo id="S7.I5.i1.I1.i2.p1.1.m1.1.1.1" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.1.cmml">&lt;</mo><msup id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.2" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.2.cmml">M</mi><mo id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.3" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.I5.i1.I1.i2.p1.1.m1.1b"><apply id="S7.I5.i1.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1"><lt id="S7.I5.i1.I1.i2.p1.1.m1.1.1.1.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S7.I5.i1.I1.i2.p1.1.m1.1.1.2.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.2">absent</csymbol><apply id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.2">𝑀</ci><ci id="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S7.I5.i1.I1.i2.p1.1.m1.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I5.i1.I1.i2.p1.1.m1.1c">&lt;M^{\prime}</annotation></semantics></math> and a round cannot be started; <math id="S7.I5.i1.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="\gg M^{\prime}" display="inline"><semantics id="S7.I5.i1.I1.i2.p1.2.m2.1a"><mrow id="S7.I5.i1.I1.i2.p1.2.m2.1.1" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.cmml"><mi id="S7.I5.i1.I1.i2.p1.2.m2.1.1.2" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.2.cmml"></mi><mo id="S7.I5.i1.I1.i2.p1.2.m2.1.1.1" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.1.cmml">≫</mo><msup id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.cmml"><mi id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.2" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.2.cmml">M</mi><mo id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.3" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S7.I5.i1.I1.i2.p1.2.m2.1b"><apply id="S7.I5.i1.I1.i2.p1.2.m2.1.1.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1"><csymbol cd="latexml" id="S7.I5.i1.I1.i2.p1.2.m2.1.1.1.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.1">much-greater-than</csymbol><csymbol cd="latexml" id="S7.I5.i1.I1.i2.p1.2.m2.1.1.2.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.2">absent</csymbol><apply id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.1.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.2.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.2">𝑀</ci><ci id="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.3.cmml" xref="S7.I5.i1.I1.i2.p1.2.m2.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I5.i1.I1.i2.p1.2.m2.1c">\gg M^{\prime}</annotation></semantics></math> and the probability of a single device being included becomes very small. In effect, devices active only at either fleet-wide availability peaks or troughs may be under-represented.</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S7.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I5.i2.p1" class="ltx_para">
<p id="S7.I5.i2.p1.1" class="ltx_p">Survival Bias</p>
<ol id="S7.I5.i2.I1" class="ltx_enumerate">
<li id="S7.I5.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S7.I5.i2.I1.i1.p1" class="ltx_para">
<p id="S7.I5.i2.I1.i1.p1.1" class="ltx_p">Since a server might choose to close a round at any point after the first <math id="S7.I5.i2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.I5.i2.I1.i1.p1.1.m1.1a"><mi id="S7.I5.i2.I1.i1.p1.1.m1.1.1" xref="S7.I5.i2.I1.i1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.I5.i2.I1.i1.p1.1.m1.1b"><ci id="S7.I5.i2.I1.i1.p1.1.m1.1.1.cmml" xref="S7.I5.i2.I1.i1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I5.i2.I1.i1.p1.1.m1.1c">M</annotation></semantics></math> devices have reported, contributions are biased towards devices with better network connections, faster processors, lower CPU load, and less data to process.</p>
</div>
</li>
<li id="S7.I5.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S7.I5.i2.I1.i2.p1" class="ltx_para">
<p id="S7.I5.i2.I1.i2.p1.1" class="ltx_p">Devices drop out of rounds when they are interrupted by the operating system, which may happen due to changes in device conditions as described by <math id="S7.I5.i2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="A_{i}(t)" display="inline"><semantics id="S7.I5.i2.I1.i2.p1.1.m1.1a"><mrow id="S7.I5.i2.I1.i2.p1.1.m1.1.2" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.cmml"><msub id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.cmml"><mi id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.2" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.2.cmml">A</mi><mi id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.3" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S7.I5.i2.I1.i2.p1.1.m1.1.2.1" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.1.cmml">​</mo><mrow id="S7.I5.i2.I1.i2.p1.1.m1.1.2.3.2" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.cmml"><mo stretchy="false" id="S7.I5.i2.I1.i2.p1.1.m1.1.2.3.2.1" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.cmml">(</mo><mi id="S7.I5.i2.I1.i2.p1.1.m1.1.1" xref="S7.I5.i2.I1.i2.p1.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S7.I5.i2.I1.i2.p1.1.m1.1.2.3.2.2" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.I5.i2.I1.i2.p1.1.m1.1b"><apply id="S7.I5.i2.I1.i2.p1.1.m1.1.2.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2"><times id="S7.I5.i2.I1.i2.p1.1.m1.1.2.1.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.1"></times><apply id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2"><csymbol cd="ambiguous" id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.1.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2">subscript</csymbol><ci id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.2.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.2">𝐴</ci><ci id="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.3.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S7.I5.i2.I1.i2.p1.1.m1.1.1.cmml" xref="S7.I5.i2.I1.i2.p1.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I5.i2.I1.i2.p1.1.m1.1c">A_{i}(t)</annotation></semantics></math>, or due to e.g. excessive memory use.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
<div id="S7.SS2.SSS2.p2" class="ltx_para">
<p id="S7.SS2.SSS2.p2.1" class="ltx_p">As can be seen, the probability of a device contributing to a round of federated learning is a complex function of both internal (e.g. device specific) and external (fleet dynamic) factors. When this probability is correlated with statistics of the data distribution, aggregate results may be biased. For instance, language models may over-represent demographics that have high quality internet connections or high end devices; and ranking models may not incorporate enough contributions from high engagement users who produce a lot of training data and hence longer training times.</p>
</div>
<div id="S7.SS2.SSS2.p3" class="ltx_para">
<p id="S7.SS2.SSS2.p3.1" class="ltx_p">Thus, designing systems that explicitly take such factors into account and integrate algorithms designed to both quantify and mitigate these effects are a fundamentally important research direction.</p>
</div>
</section>
<section id="S7.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>Open Challenges in Quantifying and Mitigating System Induced Bias</h4>

<div id="S7.SS2.SSS3.p1" class="ltx_para">
<p id="S7.SS2.SSS3.p1.1" class="ltx_p">While the potential for bias in federated learning has been addressed in the literature (<a href="#S6" title="6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>, <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib302" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">302</span></a>, <a href="#bib.bib171" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">171</span></a>]</cite>), a systematic study that qualifies and quantifies bias in realistic settings and its sources is a direction for future research. Conducting the necessary work may be hampered by both access to the necessary resources, and the difficulty in quantifying bias in a final statistical estimate due to the inherent lack of ground truth value.</p>
</div>
<div id="S7.SS2.SSS3.p2" class="ltx_para">
<p id="S7.SS2.SSS3.p2.1" class="ltx_p">We want to encourage further research to study how bias can be quantified and subsequently mitigated. A useful proxy metric for bias is to study the expected rate of contribution of a device to federated learning. In an unbiased system, this rate would be identical for every device; if it is not, the non-uniformity may provide a measure of bias. Studying the root causes for this non-uniformity may then provide important hints for how to mitigate bias, for example:</p>
<ul id="S7.I6" class="ltx_itemize">
<li id="S7.I6.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I6.i1.p1" class="ltx_para">
<p id="S7.I6.i1.p1.1" class="ltx_p">When there is a strong correlation between devices finishing a round, and the number of examples they process or model size, possible fixes may include early stopping, or decreasing the model size.</p>
</div>
</li>
<li id="S7.I6.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I6.i2.p1" class="ltx_para">
<p id="S7.I6.i2.p1.1" class="ltx_p">If the expected rate of contribution depends on factors outside our control, such as device model, network connectivity, location etc., one can view these factors as defining strata and applying <span id="S7.I6.i2.p1.1.1" class="ltx_text ltx_font_italic">post-stratification</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib312" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">312</span></a>]</cite>, that is, correcting for bias by scaling up or down contributions from devices depending on their stratum. It may also be possible to apply <span id="S7.I6.i2.p1.1.2" class="ltx_text ltx_font_italic">stratified sampling</span> - e.g. change scheduling, or server selection policies, to affect the probability of including devices in a round as a function of their stratum.</p>
</div>
</li>
<li id="S7.I6.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I6.i3.p1" class="ltx_para">
<p id="S7.I6.i3.p1.1" class="ltx_p">A very general, root-cause-agnostic mitigation could base the weight of a contribution solely on a device’s past contribution profile (e.g. the number of rounds started or completed thus far). As a special case, consider <span id="S7.I6.i3.p1.1.1" class="ltx_text ltx_font_italic">sampling without replacement</span> which could be implemented at the system level (stop connecting after one successful contribution) or at the model level (weight all but the first contribution with 0). This approach might not be sufficient when a population is large enough for most devices to contribute only infrequently (mostly one or zero times); in such cases, clustering devices based on some similarity metric and using cluster membership as stratum could help.</p>
</div>
</li>
<li id="S7.I6.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I6.i4.p1" class="ltx_para">
<p id="S7.I6.i4.p1.1" class="ltx_p">Alternatives to the synchronous, round based execution described in the previous section may also help to mitigate bias. In particular, certain types of analytics may benefit from softening or eliminating the competition between devices for inclusion, by running rounds for long times with very large numbers of participants and without applying time-outs to stragglers. Such a method may not be applicable to algorithms where the iterative aspect (running many individual, chained rounds) is important.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS2.SSS3.p3" class="ltx_para">
<p id="S7.SS2.SSS3.p3.1" class="ltx_p">The biggest obstacle to enabling such research is access to a representative fleet of end user devices, or a detailed description (e.g. in the form of a statistical model of a realistic distribution over <math id="S7.SS2.SSS3.p3.1.m1.1" class="ltx_Math" alttext="A_{i}(t)" display="inline"><semantics id="S7.SS2.SSS3.p3.1.m1.1a"><mrow id="S7.SS2.SSS3.p3.1.m1.1.2" xref="S7.SS2.SSS3.p3.1.m1.1.2.cmml"><msub id="S7.SS2.SSS3.p3.1.m1.1.2.2" xref="S7.SS2.SSS3.p3.1.m1.1.2.2.cmml"><mi id="S7.SS2.SSS3.p3.1.m1.1.2.2.2" xref="S7.SS2.SSS3.p3.1.m1.1.2.2.2.cmml">A</mi><mi id="S7.SS2.SSS3.p3.1.m1.1.2.2.3" xref="S7.SS2.SSS3.p3.1.m1.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S7.SS2.SSS3.p3.1.m1.1.2.1" xref="S7.SS2.SSS3.p3.1.m1.1.2.1.cmml">​</mo><mrow id="S7.SS2.SSS3.p3.1.m1.1.2.3.2" xref="S7.SS2.SSS3.p3.1.m1.1.2.cmml"><mo stretchy="false" id="S7.SS2.SSS3.p3.1.m1.1.2.3.2.1" xref="S7.SS2.SSS3.p3.1.m1.1.2.cmml">(</mo><mi id="S7.SS2.SSS3.p3.1.m1.1.1" xref="S7.SS2.SSS3.p3.1.m1.1.1.cmml">t</mi><mo stretchy="false" id="S7.SS2.SSS3.p3.1.m1.1.2.3.2.2" xref="S7.SS2.SSS3.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS2.SSS3.p3.1.m1.1b"><apply id="S7.SS2.SSS3.p3.1.m1.1.2.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2"><times id="S7.SS2.SSS3.p3.1.m1.1.2.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2.1"></times><apply id="S7.SS2.SSS3.p3.1.m1.1.2.2.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S7.SS2.SSS3.p3.1.m1.1.2.2.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2.2">subscript</csymbol><ci id="S7.SS2.SSS3.p3.1.m1.1.2.2.2.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2.2.2">𝐴</ci><ci id="S7.SS2.SSS3.p3.1.m1.1.2.2.3.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.2.2.3">𝑖</ci></apply><ci id="S7.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S7.SS2.SSS3.p3.1.m1.1.1">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS2.SSS3.p3.1.m1.1c">A_{i}(t)</annotation></semantics></math> functions) of a fleet that can be used in simulations. Here, maintainers of FL production stacks are uniquely positioned to provide such statistics or models to academic partners in a privacy preserving fashion; a further promising direction is the recent introduction of the Flower framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> for federated learning research.</p>
</div>
</section>
</section>
<section id="S7.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span>System Parameter Tuning</h3>

<div id="S7.SS3.p1" class="ltx_para">
<p id="S7.SS3.p1.1" class="ltx_p">Practical federated learning is a form of multi-objective optimization: while the first order goal is maximizing model quality metrics such as loss or accuracy, other important considerations are</p>
<ul id="S7.I7" class="ltx_itemize">
<li id="S7.I7.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I7.i1.p1" class="ltx_para">
<p id="S7.I7.i1.p1.1" class="ltx_p">Convergence speed</p>
</div>
</li>
<li id="S7.I7.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I7.i2.p1" class="ltx_para">
<p id="S7.I7.i2.p1.1" class="ltx_p">Throughput (e.g. number of rounds, amount of data, or number of devices)</p>
</div>
</li>
<li id="S7.I7.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I7.i3.p1" class="ltx_para">
<p id="S7.I7.i3.p1.1" class="ltx_p">Model fairness, privacy and robustness (see section <a href="#S6.SS3" title="6.3 Fairness, Privacy, and Robustness ‣ 6 Ensuring Fairness and Addressing Sources of Bias ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.3</span></a>)</p>
</div>
</li>
<li id="S7.I7.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I7.i4.p1" class="ltx_para">
<p id="S7.I7.i4.p1.1" class="ltx_p">Resource use on server and clients</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS3.p2" class="ltx_para">
<p id="S7.SS3.p2.1" class="ltx_p">These goals may be in tension. For instance, maximizing round throughput may introduce bias or hurt accuracy by preferring performant devices with little or no data. Maximizing for low training loss by increasing model complexity will put devices with less memory, many or large examples, or slow CPUs at a disadvantage. Bias or fairness induced in such a way during training may be hard to detect in the evaluation phase since it typically uses the same platform and hence is subject to similar biases.</p>
</div>
<div id="S7.SS3.p3" class="ltx_para">
<p id="S7.SS3.p3.1" class="ltx_p">Various controls affect the above listed indicators. Some are familiar from the datacenter setting, in particular model specific settings and learning algorithm hyperparameters. Others are specific to federated learning:</p>
<ul id="S7.I8" class="ltx_itemize">
<li id="S7.I8.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I8.i1.p1" class="ltx_para">
<p id="S7.I8.i1.p1.2" class="ltx_p"><span id="S7.I8.i1.p1.2.1" class="ltx_text ltx_font_bold">Clients per round</span>: The minimum number of devices required to complete a round, <math id="S7.I8.i1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.I8.i1.p1.1.m1.1a"><mi id="S7.I8.i1.p1.1.m1.1.1" xref="S7.I8.i1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.I8.i1.p1.1.m1.1b"><ci id="S7.I8.i1.p1.1.m1.1.1.cmml" xref="S7.I8.i1.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I8.i1.p1.1.m1.1c">M</annotation></semantics></math>, and the number of devices required to start a round, <math id="S7.I8.i1.p1.2.m2.1" class="ltx_Math" alttext="M^{\prime}" display="inline"><semantics id="S7.I8.i1.p1.2.m2.1a"><msup id="S7.I8.i1.p1.2.m2.1.1" xref="S7.I8.i1.p1.2.m2.1.1.cmml"><mi id="S7.I8.i1.p1.2.m2.1.1.2" xref="S7.I8.i1.p1.2.m2.1.1.2.cmml">M</mi><mo id="S7.I8.i1.p1.2.m2.1.1.3" xref="S7.I8.i1.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.I8.i1.p1.2.m2.1b"><apply id="S7.I8.i1.p1.2.m2.1.1.cmml" xref="S7.I8.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S7.I8.i1.p1.2.m2.1.1.1.cmml" xref="S7.I8.i1.p1.2.m2.1.1">superscript</csymbol><ci id="S7.I8.i1.p1.2.m2.1.1.2.cmml" xref="S7.I8.i1.p1.2.m2.1.1.2">𝑀</ci><ci id="S7.I8.i1.p1.2.m2.1.1.3.cmml" xref="S7.I8.i1.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I8.i1.p1.2.m2.1c">M^{\prime}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S7.I8.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I8.i2.p1" class="ltx_para">
<p id="S7.I8.i2.p1.1" class="ltx_p"><span id="S7.I8.i2.p1.1.1" class="ltx_text ltx_font_bold">Server-side scheduling</span>: In all but the simplest cases, a federated learning system will operate on more than one model at a time: to support multiple tenants; to train models on the same data for different use cases; to support experimentation and architecture or hyper-parameter grid search; and to run training and evaluation workloads concurrently. The server needs to decide which task to serve to incoming devices, an instance of a scheduling problem: assigning work (training or evaluation tasks) to resources (devices). Accordingly, the usual challenges arise: ideal resource assignment should be fair, avoid starvation, minimize wait times, and support relative priorities all at once.</p>
</div>
</li>
<li id="S7.I8.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I8.i3.p1" class="ltx_para">
<p id="S7.I8.i3.p1.1" class="ltx_p"><span id="S7.I8.i3.p1.1.1" class="ltx_text ltx_font_bold">Device-side scheduling</span>: As described in <a href="#S7.SS2" title="7.2 System Induced Bias ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a>, various constraints govern when a device can connect to the server and execute work. Within these constraints, various scheduling choices can be made. One extreme is to connect to the server and run computations as often as possible, leading to high load and resource use on both server and devices. Another choice are fixed intervals, but they need to be adjusted to reflect external factors such as number of devices overall and per round. The federated learning system developed at Google aims to strike a balance with a flow control mechanism called <span id="S7.I8.i3.p1.1.2" class="ltx_text ltx_font_italic">pace steering</span> <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> whereby the server instructs devices when to return. Such a dynamic system enables temporal load balancing for large populations as well as “focusing” connection attempts to specific points in time to reach the threshold <math id="S7.I8.i3.p1.1.m1.1" class="ltx_Math" alttext="M^{\prime}" display="inline"><semantics id="S7.I8.i3.p1.1.m1.1a"><msup id="S7.I8.i3.p1.1.m1.1.1" xref="S7.I8.i3.p1.1.m1.1.1.cmml"><mi id="S7.I8.i3.p1.1.m1.1.1.2" xref="S7.I8.i3.p1.1.m1.1.1.2.cmml">M</mi><mo id="S7.I8.i3.p1.1.m1.1.1.3" xref="S7.I8.i3.p1.1.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S7.I8.i3.p1.1.m1.1b"><apply id="S7.I8.i3.p1.1.m1.1.1.cmml" xref="S7.I8.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S7.I8.i3.p1.1.m1.1.1.1.cmml" xref="S7.I8.i3.p1.1.m1.1.1">superscript</csymbol><ci id="S7.I8.i3.p1.1.m1.1.1.2.cmml" xref="S7.I8.i3.p1.1.m1.1.1.2">𝑀</ci><ci id="S7.I8.i3.p1.1.m1.1.1.3.cmml" xref="S7.I8.i3.p1.1.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.I8.i3.p1.1.m1.1c">M^{\prime}</annotation></semantics></math>. Developing such a mechanism is difficult due to stochastic and dynamic nature of device availability, the lack of a predictive model of population behavior, and feedback loops.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS3.p4" class="ltx_para">
<p id="S7.SS3.p4.1" class="ltx_p">Defining reasonable composite objective functions, and designing algorithms to automatically tune these settings, has not been explored yet in the context of federated learning systems and hence remains a topic of future research.</p>
</div>
</section>
<section id="S7.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span>On-Device Runtime</h3>

<div id="S7.SS4.p1" class="ltx_para">
<p id="S7.SS4.p1.1" class="ltx_p">While numerous frameworks exist for data center training, the options for training models on resource constrained devices are fairly limited. Machine Learning models and training procedures are typically authored in a high level language such as Python. For federated learning, this description encompasses device and server computations that are executed on the target platform and exchange data over a network connection, necessitating</p>
<ul id="S7.I9" class="ltx_itemize">
<li id="S7.I9.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I9.i1.p1" class="ltx_para">
<p id="S7.I9.i1.p1.1" class="ltx_p">A means of serializing and dynamically transmitting local pieces of the total computation (e.g., the server-side update to the model, or the local client training procedure).</p>
</div>
</li>
<li id="S7.I9.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I9.i2.p1" class="ltx_para">
<p id="S7.I9.i2.p1.1" class="ltx_p">A means to interpret or execute such a computation on the target platform (server or device).</p>
</div>
</li>
<li id="S7.I9.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I9.i3.p1" class="ltx_para">
<p id="S7.I9.i3.p1.1" class="ltx_p">A stable network protocol for data exchange between participating devices and servers.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS4.p2" class="ltx_para">
<p id="S7.SS4.p2.1" class="ltx_p">One extreme form of a representation is the original high-level description, e.g. a Python TensorFlow program <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. This would require a Python interpreter with TensorFlow backend, which may not be a feasible choice for end-user devices due to resource constraints (binary size, memory use), performance limitations, or security concerns.</p>
</div>
<div id="S7.SS4.p3" class="ltx_para">
<p id="S7.SS4.p3.1" class="ltx_p">Another extreme representation of a computation is machine code of the target architecture, e.g. ARM64 instructions. This requires a compiler or re-implementation of a model in a lower-level language such as C++, and deployment computations will typically be subject to the restrictions that apply to deployment of binary code (see <a href="#S7.SS1" title="7.1 Platform Development and Deployment Challenges ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.1</span></a>), introducing prohibitive latencies for executing novel computations.</p>
</div>
<div id="S7.SS4.p4" class="ltx_para">
<p id="S7.SS4.p4.1" class="ltx_p">Intermediate representations that can be compiled or interpreted with a runtime on the target platform strike a balance between flexibility and efficiency. However, such runtimes are currently not widely available. For instance, Google’s FL system <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>]</cite> relies on TensorFlow for both server and device side execution as well as model and parameter transfer, but this choice suffers from several shortcomings:</p>
<ul id="S7.I10" class="ltx_itemize">
<li id="S7.I10.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I10.i1.p1" class="ltx_para">
<p id="S7.I10.i1.p1.1" class="ltx_p">It offers no easy path to devices for alternative front ends such as PyTorch <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib370" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">370</span></a>]</cite>, JAX <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib86" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">86</span></a>]</cite> or CNTK <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib410" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">410</span></a>]</cite>.</p>
</div>
</li>
<li id="S7.I10.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I10.i2.p1" class="ltx_para">
<p id="S7.I10.i2.p1.1" class="ltx_p">The runtime is not developed or optimized for resource constrained environments, incurring a large binary size, high memory use and comparatively low performance.</p>
</div>
</li>
<li id="S7.I10.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I10.i3.p1" class="ltx_para">
<p id="S7.I10.i3.p1.1" class="ltx_p">The intermediate representation <span id="S7.I10.i3.p1.1.1" class="ltx_text ltx_font_typewriter">GraphDef</span> used by TensorFlow is not standardized or stable, and version skew between the frontend and older on-device backends causes frequent compatibility challenges.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS4.p5" class="ltx_para">
<p id="S7.SS4.p5.1" class="ltx_p">Other alternatives include more specialized runtimes that support only a subset of the frontend’s capabilities, for instance training specific model types only, requiring changes and long update cycles whenever new model architectures or training algorithms are to be used. An extreme case would be a runtime that is limited and optimized to train a single type of model.</p>
</div>
<div id="S7.SS4.p6" class="ltx_para">
<p id="S7.SS4.p6.1" class="ltx_p">An ideal on-device runtime would have the following characteristics:</p>
<ol id="S7.I11" class="ltx_enumerate">
<li id="S7.I11.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S7.I11.i1.p1" class="ltx_para">
<p id="S7.I11.i1.p1.1" class="ltx_p">Lightweight: small binary size, or pre-installed; low memory and power profile.</p>
</div>
</li>
<li id="S7.I11.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S7.I11.i2.p1" class="ltx_para">
<p id="S7.I11.i2.p1.1" class="ltx_p">Performant: low startup latency; high throughput, supports hardware acceleration.</p>
</div>
</li>
<li id="S7.I11.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S7.I11.i3.p1" class="ltx_para">
<p id="S7.I11.i3.p1.1" class="ltx_p">Expressive: supports common data types and computations including backpropagation, variables, control flow, custom extensions.</p>
</div>
</li>
<li id="S7.I11.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S7.I11.i4.p1" class="ltx_para">
<p id="S7.I11.i4.p1.1" class="ltx_p">Stable and compact format for expressing data and computations.</p>
</div>
</li>
<li id="S7.I11.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S7.I11.i5.p1" class="ltx_para">
<p id="S7.I11.i5.p1.1" class="ltx_p">Widely available: portable open source implementation.</p>
</div>
</li>
<li id="S7.I11.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S7.I11.i6.p1" class="ltx_para">
<p id="S7.I11.i6.p1.1" class="ltx_p">Targetable by commonly used ML frameworks / languages..</p>
</div>
</li>
<li id="S7.I11.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span> 
<div id="S7.I11.i7.p1" class="ltx_para">
<p id="S7.I11.i7.p1.1" class="ltx_p">Ideally also supports inference, or if not, building personalized models for an inference runtime.</p>
</div>
</li>
</ol>
</div>
<div id="S7.SS4.p7" class="ltx_para">
<p id="S7.SS4.p7.1" class="ltx_p">To our best knowledge no solution exists yet that satisfies these requirements, and we expect the limited ability to run ML training on end user devices to become a hindrance to adoption of federated technologies.</p>
</div>
</section>
<section id="S7.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.5 </span>The Cross-Silo Setting</h3>

<div id="S7.SS5.p1" class="ltx_para">
<p id="S7.SS5.p1.1" class="ltx_p">The system challenges arising in the scenario of cross-silo federated learning take a considerably different form. As outlined in <a href="#S1.T1" title="In 1.1 The Cross-Device Federated Learning Setting ‣ 1 Introduction ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>, clients are fewer in number, more powerful, reliable, and known / addressable, eliminating many of the challenges from the cross-device setting, while allowing for authentication and verification, accounting, and contractually enforced penalties for misbehavior. Nonetheless, there are other sources of heterogeneity, including the features and distribution of data, and possibly the software stack used for training.</p>
</div>
<div id="S7.SS5.p2" class="ltx_para">
<p id="S7.SS5.p2.1" class="ltx_p">While the infrastructure in the cross-device setting (from the device-side data generation to the server logic) is typically operated by one or few organizational entities (the application, operating system, or device manufacturer), in the cross-silo setting, many different entities are involved. This may lead to high coordination and operational cost due to differences in:</p>
<ul id="S7.I12" class="ltx_itemize">
<li id="S7.I12.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I12.i1.p1" class="ltx_para">
<p id="S7.I12.i1.p1.1" class="ltx_p"><em id="S7.I12.i1.p1.1.1" class="ltx_emph ltx_font_italic">How data is generated, pre-processed and labeled.</em> Learning across silos will require data normalization which may be difficult when such data is collected and stored differently (e.g. use of different medical imaging systems, and inconsistencies in labeling procedures, annotations, and storage formats).</p>
</div>
</li>
<li id="S7.I12.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I12.i2.p1" class="ltx_para">
<p id="S7.I12.i2.p1.1" class="ltx_p"><em id="S7.I12.i2.p1.1.1" class="ltx_emph ltx_font_italic">Which software at which version powers training.</em> Using the same software stack in every silo—possibly delivered alongside the model using container technologies as done by FATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>—eliminates compatibility concerns, but such frequent and centrally distributed software delivery may not be acceptable to all involved parties. An alternative that is more similar to the cross-device setting would be to standardize data and model formats and communication protocols. See IEEE P3652.1 “Federated Machine Learning Working Group” for a related effort in this direction.</p>
</div>
</li>
<li id="S7.I12.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I12.i3.p1" class="ltx_para">
<p id="S7.I12.i3.p1.1" class="ltx_p"><em id="S7.I12.i3.p1.1.1" class="ltx_emph ltx_font_italic">The approval process for how data may or may not be used.</em> While this process is typically centralized in the cross-device scenario, the situation is likely different in cross-silo settings where many organizational entities are involved, and may be increasingly difficult when training spans different jurisdictions with varying data protection regulations. Technical infrastructure may be of help here by establishing data annotations that encode access policies, and infrastructure enforce them; for instance, limiting the use of certain data to specific models, or encoding minimum aggregation requirements such as “require at least <math id="S7.I12.i3.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S7.I12.i3.p1.1.m1.1a"><mi id="S7.I12.i3.p1.1.m1.1.1" xref="S7.I12.i3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S7.I12.i3.p1.1.m1.1b"><ci id="S7.I12.i3.p1.1.m1.1.1.cmml" xref="S7.I12.i3.p1.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.I12.i3.p1.1.m1.1c">M</annotation></semantics></math> clients per round”.</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS5.p3" class="ltx_para">
<p id="S7.SS5.p3.1" class="ltx_p">Another potential difference in the cross-silo setting is data partitioning: Data in the cross-device setting is typically assumed to be partitioned by examples, all of which have the same features (horizontal partitioning). In the cross-silo setting, in addition to partitioning by examples, partitioning by features is of practical relevance (vertical partitioning). An example would be two organizations, e.g. a bank and a retail company, with an overlapping set of customers, but different information (features) associated with them. For a discussion focusing on the algorithmic aspects, please see section <a href="#S2.SS2" title="2.2 Cross-Silo Federated Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>. Learning with feature-partitioned data may require different communication patterns and additional processing steps e.g. for entity alignment and dealing with missing features.</p>
</div>
</section>
<section id="S7.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.6 </span>Executive Summary</h3>

<div id="S7.SS6.p1" class="ltx_para">
<p id="S7.SS6.p1.1" class="ltx_p">While production grade systems for cross-device federated learning operate successfully <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">81</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, various challenges remain:</p>
<ul id="S7.I13" class="ltx_itemize">
<li id="S7.I13.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I13.i1.p1" class="ltx_para">
<p id="S7.I13.i1.p1.1" class="ltx_p">Frequent and large scale deployment of updates, monitoring, and debugging is challenging (<a href="#S7.SS1" title="7.1 Platform Development and Deployment Challenges ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.1</span></a>).</p>
</div>
</li>
<li id="S7.I13.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I13.i2.p1" class="ltx_para">
<p id="S7.I13.i2.p1.1" class="ltx_p">Differences in device availability induce various forms of bias; defining, quantifying and mitigating them remains a direction for future research (<a href="#S7.SS2" title="7.2 System Induced Bias ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.2</span></a>).</p>
</div>
</li>
<li id="S7.I13.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I13.i3.p1" class="ltx_para">
<p id="S7.I13.i3.p1.1" class="ltx_p">Tuning system parameters is difficult due to the existence of multiple, potentially conflicting objectives (<a href="#S7.SS3" title="7.3 System Parameter Tuning ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.3</span></a>).</p>
</div>
</li>
<li id="S7.I13.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S7.I13.i4.p1" class="ltx_para">
<p id="S7.I13.i4.p1.1" class="ltx_p">Running ML workloads on end user devices is hampered by the lack of a portable, fast, small footprint, and flexible runtime for on-device training (<a href="#S7.SS4" title="7.4 On-Device Runtime ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.4</span></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S7.SS6.p2" class="ltx_para">
<p id="S7.SS6.p2.1" class="ltx_p">Systems for cross-silo settings (<a href="#S7.SS5" title="7.5 The Cross-Silo Setting ‣ 7 Addressing System Challenges ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7.5</span></a>) face largely different issues owing to differences in the capabilities of compute nodes and the nature of the data being processed.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Concluding Remarks</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">Federated learning enables distributed client devices to collaboratively learn a shared prediction model while keeping all the training data on device, decoupling the ability to do machine learning from the need to store the data in the cloud. This goes beyond the use of local models that make predictions on mobile devices by bringing model training to the device as well.</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">In recent years, this topic has undergone an explosive growth of interest, both in industry and academia. Major technology companies have already deployed federated learning in production, and a number of startups were founded with the objective of using federated learning to address privacy and data collection challenges in various industries. Further, the breadth of papers surveyed in this work suggests that federated learning is gaining traction in a wide range of interdisciplinary fields: from machine learning to optimization to information theory and statistics to cryptography, fairness, and privacy.</p>
</div>
<div id="S8.p3" class="ltx_para">
<p id="S8.p3.1" class="ltx_p">Motivated by the growing interest in federated learning research, this paper discusses recent advances and presents an extensive collection of open problems and challenges. The system constraints impose efficiency requirements on the algorithms in order to be practical, many of which are not particularly challenging in other settings. We argue that data privacy is not binary and present a range of threat models that are relevant under a variety of assumptions, each of which provides its own unique challenges.</p>
</div>
<div id="S8.p4" class="ltx_para">
<p id="S8.p4.1" class="ltx_p">The open problems discussed in this work are certainly not comprehensive, they reflect the interests and backgrounds of the authors. In particular, we do not discuss any non-learning problems which need to be solved in the course of a practical machine learning project, and might need to be solved based on decentralized data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib382" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">382</span></a>]</cite>. This can include simple problems such as computing basic descriptive statistics, or more complex objectives such as computing the head of a histogram over an open set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib510" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">510</span></a>]</cite>. Existing algorithms for solving such problems often do not always have an obvious “federated version” that would be efficient under the system assumptions motivating this work or do not admit a useful notion of data protection. Yet another set of important topics that were not discussed are the legal and business issues that may motivate or constrain the use of federated learning.</p>
</div>
<div id="S8.p5" class="ltx_para">
<p id="S8.p5.1" class="ltx_p">We hope this work will be helpful in scoping further research in federated learning and related areas.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank Alex Ingerman and David Petrou for their useful suggestions and insightful comments during the review process.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.4.4.1" class="ltx_text" style="font-size:90%;">lat [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">
Lattigo 2.0.0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">Online: </span><a target="_blank" href="http://github.com/ldsec/lattigo" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://github.com/ldsec/lattigo</a><span id="bib.bib1.8.2" class="ltx_text" style="font-size:90%;">, October 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">EPFL-LDS.
</span>
<div class="ltx_pagination ltx_role_newpage"></div>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Abadi et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">TensorFlow: Large-scale machine learning on heterogeneous systems,
2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://www.tensorflow.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.tensorflow.org/</a><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.11.1" class="ltx_text" style="font-size:90%;">Software available from tensorflow.org.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Abadi et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Deep learning with differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, pages 308–318. ACM, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Abari et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Omid Abari, Hariharan Rahul, and Dina Katabi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Over-the-air function computation in sensor networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, abs/1612.02307, 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1612.02307" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1612.02307</a><span id="bib.bib4.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Abay et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Nazmiye Ceren Abay, Yan Zhou, Murat Kantarcioglu, Bhavani Thuraisingham, and
Latanya Sweeney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Privacy preserving synthetic data release using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Joint European Conference on Machine Learning and Knowledge
Discovery in Databases</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, pages 510–526. Springer, 2018.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.4.4.1" class="ltx_text" style="font-size:90%;">Abowd and Schmutte [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">
John M Abowd and Ian M Schmutte.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">An economic analysis of privacy protection and statistical accuracy
as social choices.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">American Economic Review</em><span id="bib.bib6.9.2" class="ltx_text" style="font-size:90%;">, 109(1):171–202,
2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Acharya et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Jayadev Acharya, Clément L Canonne, and Himanshu Tyagi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Inference under information constraints i: Lower bounds from
chi-square contraction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 66(12):7835–7855, 2020.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.4.4.1" class="ltx_text" style="font-size:90%;">Ács and Castelluccia [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">
Gergely Ács and Claude Castelluccia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">I have a DREAM!: DIfferentially PrivatE smart Metering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib8.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 13th International Conference on
Information Hiding</em><span id="bib.bib8.10.3" class="ltx_text" style="font-size:90%;">, IH’11, pages 118–132, Berlin, Heidelberg, 2011.
Springer-Verlag.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.11.1" class="ltx_text" style="font-size:90%;">ISBN 978-3-642-24177-2.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://dl.acm.org/citation.cfm?id=2042445.2042457" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://dl.acm.org/citation.cfm?id=2042445.2042457</a><span id="bib.bib8.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Agarwal et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Naman Agarwal, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Brendan
McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">cpSGD: Communication-efficient and differentially-private
distributed SGD.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, pages
7564–7575, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Agrawal et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Nitin Agrawal, Ali Shahin Shamsabadi, Matt J. Kusner, and Adrià
Gascón.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">QUOTIENT: two-party secure neural network training and prediction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">In Proceedings of the ACM Conference on Computer and
Communication Security (CCS)</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.4.4.1" class="ltx_text" style="font-size:90%;">Agrawal and Srikant [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.6.1" class="ltx_text" style="font-size:90%;">
Rakesh Agrawal and Ramakrishnan Srikant.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">Privacy-preserving data mining.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM SIGMOD International Conference on Management of Data</em><span id="bib.bib11.10.3" class="ltx_text" style="font-size:90%;">,
2000.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.4.4.1" class="ltx_text" style="font-size:90%;">Aguilar-Melchor and Gaborit [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.6.1" class="ltx_text" style="font-size:90%;">
Carlos Aguilar-Melchor and Philippe Gaborit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">A lattice-based computationally-efficient private information
retrieval protocol.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Cryptol. ePrint Arch., Report</em><span id="bib.bib12.9.2" class="ltx_text" style="font-size:90%;">, 446, 2007.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Aguilar-Melchor et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Carlos Aguilar-Melchor, Joris Barrier, Laurent Fousse, and Marc-Olivier
Killijian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">XPIR: Private information retrieval for everyone.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings on Privacy Enhancing Technologies</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 2016(2):155–174, 2016.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">ai.google [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
ai.google.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Under the hood of the Pixel 2: How AI is supercharging hardware,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://ai.google/stories/ai-in-hardware/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://ai.google/stories/ai-in-hardware/</a><span id="bib.bib14.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.10.1" class="ltx_text" style="font-size:90%;">Retrieved Nov 2018.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.4.4.1" class="ltx_text" style="font-size:90%;">ai.intel [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.6.1" class="ltx_text" style="font-size:90%;">
ai.intel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">Federated learning for medical imaging, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://www.intel.ai/federated-learning-for-medical-imaging/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.intel.ai/federated-learning-for-medical-imaging/</a><span id="bib.bib15.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Ali et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Asra Ali, Tancrède Lepoint, Sarvar Patel, Mariana Raykova, Phillipp
Schoppmann, Karn Seth, and Kevin Yeo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Communication-computation trade-offs in PIR.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib16.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptol. ePrint Arch.</em><span id="bib.bib16.10.2" class="ltx_text" style="font-size:90%;">, 2019:1483, 2019.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Alistarh et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">QSGD: Communication-efficient SGD via gradient quantization and
encoding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS - Advances in Neural Information Processing Systems</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">,
pages 1709–1720, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Alistarh et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Byzantine stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.4.4.1" class="ltx_text" style="font-size:90%;">Almeida and Xavier [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.6.1" class="ltx_text" style="font-size:90%;">
Inês Almeida and João Xavier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">DJAM: Distributed Jacobi Asynchronous Method for Learning Personal
Models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib19.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Letters</em><span id="bib.bib19.9.2" class="ltx_text" style="font-size:90%;">, 25(9):1389–1392, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Ames et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Scott Ames, Carmit Hazay, Yuval Ishai, and Muthuramakrishnan
Venkitasubramaniam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Ligero: Lightweight sublinear arguments without a trusted setup.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, CCS ’17, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Amin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Kareem Amin, Alex Kulesza, Andres Munoz, and Sergei Vassilvtiskii.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Bounding user contributions: A bias-variance trade-off in
differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, pages
263–271, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.3.3.1" class="ltx_text" style="font-size:90%;">[22]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.5.1" class="ltx_text" style="font-size:90%;">
androidtrusty.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.6.1" class="ltx_text" style="font-size:90%;">Android Trusty TEE.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://source.android.com/security/trusty" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://source.android.com/security/trusty</a><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Accessed: 2019-12-05.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Angel et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Sebastian Angel, Hao Chen, Kim Laine, and Srinath T. V. Setty.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">PIR with compressed queries and amortized query processing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security and Privacy</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, pages 962–979.
IEEE Computer Society, 2018.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.4.4.1" class="ltx_text" style="font-size:90%;">Annas [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.6.1" class="ltx_text" style="font-size:90%;">
George J Annas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">HIPAA regulations-a new era of medical-record privacy?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">New England Journal of Medicine</em><span id="bib.bib24.9.2" class="ltx_text" style="font-size:90%;">, 348(15):1486–1490, 2003.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.4.4.1" class="ltx_text" style="font-size:90%;">Apple [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.6.1" class="ltx_text" style="font-size:90%;">
Apple.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">Private Federated Learning (NeurIPS 2019 Expo Talk Abstract).
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://nips.cc/ExpoConferences/2019/schedule?talk_id=40" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://nips.cc/ExpoConferences/2019/schedule?talk_id=40</a><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">,
2019a.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.4.4.1" class="ltx_text" style="font-size:90%;">Apple [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.6.1" class="ltx_text" style="font-size:90%;">
Apple.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">Designing for privacy (video and slide deck).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Apple WWDC,
</span><a target="_blank" href="https://developer.apple.com/videos/play/wwdc2019/708" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://developer.apple.com/videos/play/wwdc2019/708</a><span id="bib.bib26.9.2" class="ltx_text" style="font-size:90%;">,
2019b.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Araki et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Toshinori Araki, Jun Furukawa, Yehuda Lindell, Ariel Nof, and Kazuma Ohara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">High-throughput semi-honest secure three-party computation with an
honest majority.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2016 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, pages 805–817. ACM, 2016.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.3.3.1" class="ltx_text" style="font-size:90%;">[28]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.5.1" class="ltx_text" style="font-size:90%;">
armtrustzone.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.6.1" class="ltx_text" style="font-size:90%;">Arm TrustZone Technology.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://developer.arm.com/ip-products/security-ip/trustzone" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://developer.arm.com/ip-products/security-ip/trustzone</a><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Accessed: 2019-12-05.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Assran et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Stochastic gradient push for distributed deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Athalye et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Anish Athalye, Nicholas Carlini, and David Wagner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Obfuscated gradients give a false sense of security: Circumventing
defenses to adversarial examples.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib30.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib30.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Augenstein et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Sean Augenstein, H. Brendan McMahan, Daniel Ramage, Swaroop Ramaswamy, Peter
Kairouz, Mingqing Chen, Rajiv Mathews, and Blaise Aguera y Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Generative models for effective ML on private, decentralized
datasets, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1911.06679" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1911.06679</a><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.6.1" class="ltx_text" style="font-size:90%;">
PyVertical Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">Pyvertical, 2020a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com.cnpmjs.org/OpenMined/PyVertical" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com.cnpmjs.org/OpenMined/PyVertical</a><span id="bib.bib32.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.6.1" class="ltx_text" style="font-size:90%;">
The FATE Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">Federated AI technology enabler, 2019a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://www.fedai.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.fedai.org/</a><span id="bib.bib33.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.6.1" class="ltx_text" style="font-size:90%;">
The Fedlearner Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">Fedlearner, 2020b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/bytedance/fedlearner" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/bytedance/fedlearner</a><span id="bib.bib34.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.6.1" class="ltx_text" style="font-size:90%;">
The Leaf Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">Leaf, 2019b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://leaf.cmu.edu/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://leaf.cmu.edu/</a><span id="bib.bib35.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.6.1" class="ltx_text" style="font-size:90%;">
The PaddleFL Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">PaddleFL, 2019c.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/PaddlePaddle/PaddleFL</a><span id="bib.bib36.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2019d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">
The PaddlePaddle Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">PaddlePaddle, 2019d.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://www.paddlepaddle.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://www.paddlepaddle.org/</a><span id="bib.bib37.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.4.4.1" class="ltx_text" style="font-size:90%;">Authors [2019e]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">
The TFF Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">TensorFlow Federated, 2019e.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.tensorflow.org/federated</a><span id="bib.bib38.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Avent et al. [01 Oct. 2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Brendan Avent, Yatharth Dubey, and Aleksandra Korolova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">The power of the hybrid model for mean estimation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib39.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings on Privacy Enhancing Technologies (PETS)</em><span id="bib.bib39.10.2" class="ltx_text" style="font-size:90%;">,
2020(4):48 – 68, 01 Oct. 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">https://doi.org/10.2478/popets-2020-0062</span><span id="bib.bib39.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.13.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://content.sciendo.com/view/journals/popets/2020/4/article-p48.xml" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://content.sciendo.com/view/journals/popets/2020/4/article-p48.xml</a><span id="bib.bib39.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Avent et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Brendan Avent, Aleksandra Korolova, David Zeber, Torgeir Hovden, and Benjamin
Livshits.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">BLENDER: Enabling local search with a hybrid differential privacy
model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib40.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">26th USENIX Security Symposium (USENIX Security 17)</em><span id="bib.bib40.11.3" class="ltx_text" style="font-size:90%;">,
pages 747–764, Vancouver, BC, August 2017. USENIX Association.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-931971-40-9.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.13.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/avent" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/avent</a><span id="bib.bib40.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Awasthi et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Pranjal Awasthi, Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Beyond individual and group fairness.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, abs/2008.09490, 2020.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Babai et al. [1991]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
László Babai, Lance Fortnow, Leonid A. Levin, and Mario Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Checking computations in polylogarithmic time.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib42.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">STOC</em><span id="bib.bib42.11.3" class="ltx_text" style="font-size:90%;">, pages 21–31. ACM, 1991.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.4.4.1" class="ltx_text" style="font-size:90%;">Bagdasaryan and Shmatikov [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.6.1" class="ltx_text" style="font-size:90%;">
Eugene Bagdasaryan and Vitaly Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">Differential privacy has disparate impact on model accuracy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib43.9.2" class="ltx_text" style="font-size:90%;">, abs/1905.12101, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1905.12101" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1905.12101</a><span id="bib.bib43.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Bagdasaryan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">How to backdoor federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib44.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.00459</em><span id="bib.bib44.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Balle et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Borja Balle, James Bell, Adrià Gascón, and Kobbi Nissim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">The privacy blanket of the shuffle model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Cryptology - CRYPTO 2019 - 39th Annual
International Cryptology Conference, Santa Barbara, CA, USA, August 18-22,
2019, Proceedings, Part II</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, pages 638–667, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1007/978-3-030-26951-7“˙22</span><span id="bib.bib45.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1007/978-3-030-26951-7_22" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1007/978-3-030-26951-7_22</a><span id="bib.bib45.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Balle et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Borja Balle, James Bell, Adrià Gascón, and Kobbi Nissim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Private summation in the multi-message shuffle model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, page 657–676. ACM, 2020a.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Balle et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Borja Balle, Peter Kairouz, H. Brendan McMahan, Om Thakkar, and Abhradeep
Thakurta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Privacy amplification via random check-ins, 2020b.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Barak et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Assi Barak, Daniel Escudero, Anders P. K. Dalskov, and Marcel Keller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Secure evaluation of quantized neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2019:131, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://eprint.iacr.org/2019/131" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://eprint.iacr.org/2019/131</a><span id="bib.bib48.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Barnes et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Leighton Pate Barnes, Yanjun Han, and Ayfer Ozgur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">Lower bounds for learning distributions under communication
constraints via fisher information.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib49.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib49.10.2" class="ltx_text" style="font-size:90%;">, 21(236):1–30, 2020a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://jmlr.org/papers/v21/19-737.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://jmlr.org/papers/v21/19-737.html</a><span id="bib.bib49.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Barnes et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, and Ayfer Ozgur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">rtop-k: A statistical estimation approach to distributed sgd.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2005.10761</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Barocas et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Solon Barocas, Moritz Hardt, and Arvind Narayanan.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Fairness and Machine Learning</em><span id="bib.bib51.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.10.1" class="ltx_text" style="font-size:90%;">fairmlbook.org, 2019.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.fairmlbook.org" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://www.fairmlbook.org</a><span id="bib.bib51.11.1" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Baruch et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Moran Baruch, Gilad Baruch, and Yoav Goldberg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">A little is enough: Circumventing defenses for distributed learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1902.06156</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.4.4.1" class="ltx_text" style="font-size:90%;">Bassily and Smith [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">
Raef Bassily and Adam Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">Local, private, efficient protocols for succinct histograms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">STOC</em><span id="bib.bib53.10.3" class="ltx_text" style="font-size:90%;">, pages 127–135, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Bassily et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Raef Bassily, Uri Stemmer, Abhradeep Guha Thakurta, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Practical locally private heavy hitters.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib54.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib54.11.3" class="ltx_text" style="font-size:90%;">, pages
2288–2296, 2017.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Basu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Debraj Basu, Deepesh Data, Can Karakus, and Suhas N Diggavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Qsparse-local-sgd: Distributed sgd with quantization, sparsification,
and local computations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib55.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Journal on Selected Areas in Information Theory</em><span id="bib.bib55.10.2" class="ltx_text" style="font-size:90%;">,
1(1):217–226, 2020.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.4.4.1" class="ltx_text" style="font-size:90%;">Baxter [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.6.1" class="ltx_text" style="font-size:90%;">
Jonathan Baxter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">A model of inductive bias learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib56.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Artificial Intelligence Research</em><span id="bib.bib56.9.2" class="ltx_text" style="font-size:90%;">, 12:149–198, 2000.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Beimel et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Amos Beimel, Aleksandra Korolova, Kobbi Nissim, Or Sheffet, and Uri Stemmer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">The power of synergy in differential privacy: Combining a small
curator with local randomizers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib57.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on Information-Theoretic Cryptography (ITC)</em><span id="bib.bib57.11.3" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1912.08951" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1912.08951</a><span id="bib.bib57.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Bell et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
James Henry Bell, Kallista A. Bonawitz, Adrià Gascón, Tancrède
Lepoint, and Mariana Raykova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Secure single-server aggregation with (poly)logarithmic overhead.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib58.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2020 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib58.11.3" class="ltx_text" style="font-size:90%;">, page 1253–1269. ACM, 2020.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Bellet et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Aurélien Bellet, Rachid Guerraoui, Mahsa Taziki, and Marc Tommasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Personalized and Private Peer-to-Peer Machine Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AISTATS</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib60.5.5.1" class="ltx_text" style="font-size:90%;">Bello et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib60.7.1" class="ltx_text" style="font-size:90%;">
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.8.1" class="ltx_text" style="font-size:90%;">Neural optimizer search with reinforcement learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib60.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em><span id="bib.bib60.11.3" class="ltx_text" style="font-size:90%;">, pages 459–468. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib61.5.5.1" class="ltx_text" style="font-size:90%;">Ben-David et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib61.7.1" class="ltx_text" style="font-size:90%;">
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
Jennifer Wortman Vaughan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.8.1" class="ltx_text" style="font-size:90%;">A theory of learning from different domains.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib61.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Machine learning</em><span id="bib.bib61.10.2" class="ltx_text" style="font-size:90%;">, 79(1-2):151–175, 2010.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib62.5.5.1" class="ltx_text" style="font-size:90%;">Ben-Sasson et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib62.7.1" class="ltx_text" style="font-size:90%;">
Eli Ben-Sasson, Alessandro Chiesa, Christina Garman, Matthew Green, Ian
Miers, Eran Tromer, and Madars Virza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.8.1" class="ltx_text" style="font-size:90%;">Zerocash: Decentralized anonymous payments from bitcoin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib62.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security and Privacy</em><span id="bib.bib62.11.3" class="ltx_text" style="font-size:90%;">, pages 459–474.
IEEE Computer Society, 2014.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib63.5.5.1" class="ltx_text" style="font-size:90%;">Ben-Sasson et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib63.7.1" class="ltx_text" style="font-size:90%;">
Eli Ben-Sasson, Iddo Bentov, Yinon Horesh, and Michael Riabzev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.8.1" class="ltx_text" style="font-size:90%;">Scalable zero knowledge with no trusted setup.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib63.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO (3)</em><span id="bib.bib63.11.3" class="ltx_text" style="font-size:90%;">, volume 11694 of </span><em id="bib.bib63.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib63.13.5" class="ltx_text" style="font-size:90%;">, pages 701–732. Springer, 2019.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib64.5.5.1" class="ltx_text" style="font-size:90%;">Bergstra et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib64.7.1" class="ltx_text" style="font-size:90%;">
James S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.8.1" class="ltx_text" style="font-size:90%;">Algorithms for hyper-parameter optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib64.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib64.11.3" class="ltx_text" style="font-size:90%;">, pages
2546–2554, 2011.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib65.5.5.1" class="ltx_text" style="font-size:90%;">Bertrán et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib65.7.1" class="ltx_text" style="font-size:90%;">
Martín Bertrán, Natalia Martínez, Afroditi Papadaki, Qiang Qiu, Miguel R. D.
Rodrigues, Galen Reeves, and Guillermo Sapiro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.8.1" class="ltx_text" style="font-size:90%;">Learning adversarially fair and transferable representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib65.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib65.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib66.5.5.1" class="ltx_text" style="font-size:90%;">Beutel et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib66.7.1" class="ltx_text" style="font-size:90%;">
Daniel J. Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan Parcollet, and
Nicholas D. Lane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.8.1" class="ltx_text" style="font-size:90%;">Flower: A friendly federated learning research framework, 2020.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib67.5.5.1" class="ltx_text" style="font-size:90%;">Bhagoji et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib67.7.1" class="ltx_text" style="font-size:90%;">
Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, and Seraphin Calo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.8.1" class="ltx_text" style="font-size:90%;">Analyzing federated learning through an adversarial lens.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib67.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 36th International Conference on Machine
Learning</em><span id="bib.bib67.11.3" class="ltx_text" style="font-size:90%;">, pages 634–643, 2019.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib68.5.5.1" class="ltx_text" style="font-size:90%;">Bhowmick et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib68.7.1" class="ltx_text" style="font-size:90%;">
Abhishek Bhowmick, John Duchi, Julien Freudiger, Gaurav Kapoor, and Ryan
Rogers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.8.1" class="ltx_text" style="font-size:90%;">Protection against reconstruction and its applications in private
federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib68.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00984</em><span id="bib.bib68.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib69.5.5.1" class="ltx_text" style="font-size:90%;">Biggio et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib69.7.1" class="ltx_text" style="font-size:90%;">
Battista Biggio, Blaine Nelson, and Pavel Laskov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.8.1" class="ltx_text" style="font-size:90%;">Poisoning attacks against support vector machines.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib69.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th International Coference on
International Conference on Machine Learning</em><span id="bib.bib69.11.3" class="ltx_text" style="font-size:90%;">, ICML’12, pages 1467–1474,
USA, 2012. Omnipress.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-1285-1.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.13.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://dl.acm.org/citation.cfm?id=3042573.3042761" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://dl.acm.org/citation.cfm?id=3042573.3042761</a><span id="bib.bib69.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib70.5.5.1" class="ltx_text" style="font-size:90%;">Biggio et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib70.7.1" class="ltx_text" style="font-size:90%;">
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
Šrndić, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.8.1" class="ltx_text" style="font-size:90%;">Evasion attacks against machine learning at test time.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib70.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ECML-PKDD</em><span id="bib.bib70.11.3" class="ltx_text" style="font-size:90%;">, pages 387–402. Springer, 2013.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib71.5.5.1" class="ltx_text" style="font-size:90%;">Bitansky et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib71.7.1" class="ltx_text" style="font-size:90%;">
Nir Bitansky, Ran Canetti, Alessandro Chiesa, and Eran Tromer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.8.1" class="ltx_text" style="font-size:90%;">From extractable collision resistance to succinct non-interactive
arguments of knowledge, and back again.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib71.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 3rd Innovations in Theoretical Computer
Science Conference</em><span id="bib.bib71.11.3" class="ltx_text" style="font-size:90%;">, ITCS ’12, 2012.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib72.4.4.1" class="ltx_text" style="font-size:90%;">Bitar and Rouayheb [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib72.6.1" class="ltx_text" style="font-size:90%;">
R. Bitar and S. E. Rouayheb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.7.1" class="ltx_text" style="font-size:90%;">Staircase-PIR: Universally robust private information retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib72.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE Information Theory Workshop (ITW)</em><span id="bib.bib72.10.3" class="ltx_text" style="font-size:90%;">, pages 1–5,
Nov 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/ITW.2018.8613532</span><span id="bib.bib72.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib73.5.5.1" class="ltx_text" style="font-size:90%;">Bittau et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib73.7.1" class="ltx_text" style="font-size:90%;">
Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth
Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and
Bernhard Seefeld.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.8.1" class="ltx_text" style="font-size:90%;">Prochlo: Strong privacy for analytics in the crowd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib73.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 26th Symposium on Operating Systems
Principles</em><span id="bib.bib73.11.3" class="ltx_text" style="font-size:90%;">, SOSP ’17, pages 441–459, New York, NY, USA, 2017. ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-5085-3.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/3132747.3132769</span><span id="bib.bib73.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/3132747.3132769" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/3132747.3132769</a><span id="bib.bib73.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib74.5.5.1" class="ltx_text" style="font-size:90%;">Blalock et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib74.7.1" class="ltx_text" style="font-size:90%;">
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.8.1" class="ltx_text" style="font-size:90%;">What is the state of neural network pruning?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib74.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.03033</em><span id="bib.bib74.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib75.5.5.1" class="ltx_text" style="font-size:90%;">Blanchard et al. [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib75.7.1" class="ltx_text" style="font-size:90%;">
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.8.1" class="ltx_text" style="font-size:90%;">Machine learning with adversaries: Byzantine tolerant gradient
descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib75.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib75.11.3" class="ltx_text" style="font-size:90%;">,
2017a.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib76.5.5.1" class="ltx_text" style="font-size:90%;">Blanchard et al. [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib76.7.1" class="ltx_text" style="font-size:90%;">
Peva Blanchard, Rachid Guerraoui, Julien Stainer, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.8.1" class="ltx_text" style="font-size:90%;">Machine Learning with Adversaries: Byzantine Tolerant Gradient
Descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib76.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib76.11.3" class="ltx_text" style="font-size:90%;">, pages
118–128, 2017b.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib77.5.5.1" class="ltx_text" style="font-size:90%;">Bogdanov et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib77.7.1" class="ltx_text" style="font-size:90%;">
Dan Bogdanov, Riivo Talviste, and Jan Willemson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.8.1" class="ltx_text" style="font-size:90%;">Deploying secure multi-party computation for financial data analysis
- (short paper).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib77.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Financial Cryptography</em><span id="bib.bib77.11.3" class="ltx_text" style="font-size:90%;">, volume 7397 of </span><em id="bib.bib77.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes
in Computer Science</em><span id="bib.bib77.13.5" class="ltx_text" style="font-size:90%;">, pages 57–64. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib78.5.5.1" class="ltx_text" style="font-size:90%;">Bogetoft et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib78.7.1" class="ltx_text" style="font-size:90%;">
Peter Bogetoft, Dan Lund Christensen, Ivan Damgård, Martin Geisler,
Thomas P. Jakobsen, Mikkel Krøigaard, Janus Dam Nielsen, Jesper Buus
Nielsen, Kurt Nielsen, Jakob Pagter, Michael I. Schwartzbach, and Tomas Toft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.8.1" class="ltx_text" style="font-size:90%;">Secure multiparty computation goes live.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib78.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Financial Cryptography</em><span id="bib.bib78.11.3" class="ltx_text" style="font-size:90%;">, volume 5628 of </span><em id="bib.bib78.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes
in Computer Science</em><span id="bib.bib78.13.5" class="ltx_text" style="font-size:90%;">, pages 325–343. Springer, 2009.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib79.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib79.7.1" class="ltx_text" style="font-size:90%;">
K. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.8.1" class="ltx_text" style="font-size:90%;">Practical secure aggregation for federated learning on user-held
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib79.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1611.04482</em><span id="bib.bib79.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib80.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib80.7.1" class="ltx_text" style="font-size:90%;">
K. A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.8.1" class="ltx_text" style="font-size:90%;">Practical secure aggregation for privacy-preserving machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib80.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib80.11.3" class="ltx_text" style="font-size:90%;">, pages 1175–1191. ACM, 2017.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib81.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib81.7.1" class="ltx_text" style="font-size:90%;">
K. A. Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloé M Kiddon, Jakub Konečný, Stefano
Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage,
and Jason Roselander.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.8.1" class="ltx_text" style="font-size:90%;">Towards federated learning at scale: System design.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib81.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SysML 2019</em><span id="bib.bib81.11.3" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1902.01046" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1902.01046</a><span id="bib.bib81.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib82.5.5.1" class="ltx_text" style="font-size:90%;">Bonawitz et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib82.7.1" class="ltx_text" style="font-size:90%;">
K. A. Bonawitz, Fariborz Salehi, Jakub Konečný, Brendan McMahan, and
Marco Gruteser.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.8.1" class="ltx_text" style="font-size:90%;">Federated learning with autotuned communication-efficient secure
aggregation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib82.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 53nd Asilomar Conference on Signals, Systems, and
Computers</em><span id="bib.bib82.11.3" class="ltx_text" style="font-size:90%;">. IEEE, 2019b.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib83.5.5.1" class="ltx_text" style="font-size:90%;">Boneh et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib83.7.1" class="ltx_text" style="font-size:90%;">
Dan Boneh, Elette Boyle, Henry Corrigan-Gibbs, Niv Gilboa, and Yuval Ishai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.8.1" class="ltx_text" style="font-size:90%;">Zero-knowledge proofs on secret-shared data via fully linear PCPs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib83.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO (3)</em><span id="bib.bib83.11.3" class="ltx_text" style="font-size:90%;">, volume 11694 of </span><em id="bib.bib83.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib83.13.5" class="ltx_text" style="font-size:90%;">, pages 67–97. Springer, 2019.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib84.5.5.1" class="ltx_text" style="font-size:90%;">Bourse et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib84.7.1" class="ltx_text" style="font-size:90%;">
Florian Bourse, Michele Minelli, Matthias Minihold, and Pascal Paillier.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.8.1" class="ltx_text" style="font-size:90%;">Fast homomorphic evaluation of deep discretized neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib84.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO (3)</em><span id="bib.bib84.11.3" class="ltx_text" style="font-size:90%;">, volume 10993 of </span><em id="bib.bib84.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib84.13.5" class="ltx_text" style="font-size:90%;">, pages 483–512. Springer, 2018.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib85.5.5.1" class="ltx_text" style="font-size:90%;">Boyd et al. [2006]</span></span>
<span class="ltx_bibblock"><span id="bib.bib85.7.1" class="ltx_text" style="font-size:90%;">
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.8.1" class="ltx_text" style="font-size:90%;">Randomized gossip algorithms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib85.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib85.10.2" class="ltx_text" style="font-size:90%;">, 52(6):2508–2530, 2006.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib86.5.5.1" class="ltx_text" style="font-size:90%;">Bradbury et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib86.7.1" class="ltx_text" style="font-size:90%;">
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary,
Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.8.1" class="ltx_text" style="font-size:90%;">JAX: composable transformations of Python+NumPy programs,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://github.com/google/jax" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://github.com/google/jax</a><span id="bib.bib86.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib87.4.4.1" class="ltx_text" style="font-size:90%;">Brakerski [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib87.6.1" class="ltx_text" style="font-size:90%;">
Zvika Brakerski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.7.1" class="ltx_text" style="font-size:90%;">Fully homomorphic encryption without modulus switching from classical
gapsvp.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib87.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO</em><span id="bib.bib87.10.3" class="ltx_text" style="font-size:90%;">, volume 7417 of </span><em id="bib.bib87.11.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer
Science</em><span id="bib.bib87.12.5" class="ltx_text" style="font-size:90%;">, pages 868–886. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib88.5.5.1" class="ltx_text" style="font-size:90%;">Brakerski et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib88.7.1" class="ltx_text" style="font-size:90%;">
Zvika Brakerski, Craig Gentry, and Vinod Vaikuntanathan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.8.1" class="ltx_text" style="font-size:90%;">(leveled) fully homomorphic encryption without bootstrapping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib88.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ITCS</em><span id="bib.bib88.11.3" class="ltx_text" style="font-size:90%;">, pages 309–325. ACM, 2012.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib89.5.5.1" class="ltx_text" style="font-size:90%;">Braverman et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib89.7.1" class="ltx_text" style="font-size:90%;">
Mark Braverman, Ankit Garg, Tengyu Ma, Huy L. Nguyen, and David P. Woodruff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.8.1" class="ltx_text" style="font-size:90%;">Communication lower bounds for statistical estimation problems via a
distributed data processing inequality.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib89.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the forty-eighth annual ACM symposium on
Theory of Computing</em><span id="bib.bib89.11.3" class="ltx_text" style="font-size:90%;">, page 1011–1020. ACM, 2016.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib90.5.5.1" class="ltx_text" style="font-size:90%;">Brendel et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib90.7.1" class="ltx_text" style="font-size:90%;">
Wieland Brendel, Jonas Rauber, and Matthias Bethge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.8.1" class="ltx_text" style="font-size:90%;">Decision-based adversarial attacks: Reliable attacks against
black-box machine learning models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib90.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.04248</em><span id="bib.bib90.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib91.5.5.1" class="ltx_text" style="font-size:90%;">Brisimi et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib91.7.1" class="ltx_text" style="font-size:90%;">
Theodora S Brisimi, Ruidi Chen, Theofanie Mela, Alex Olshevsky, Ioannis Ch
Paschalidis, and Wei Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.8.1" class="ltx_text" style="font-size:90%;">Federated learning of predictive models from federated electronic
health records.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib91.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International journal of medical informatics</em><span id="bib.bib91.10.2" class="ltx_text" style="font-size:90%;">, 112:59–67, 2018.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib92.5.5.1" class="ltx_text" style="font-size:90%;">Bünz et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib92.7.1" class="ltx_text" style="font-size:90%;">
Benedikt Bünz, Jonathan Bootle, Dan Boneh, Andrew Poelstra, Pieter
Wuille, and Gregory Maxwell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.8.1" class="ltx_text" style="font-size:90%;">Bulletproofs: Short proofs for confidential transactions and more.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib92.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE Symposium on Security and Privacy, SP 2018,
Proceedings, 21-23 May 2018, San Francisco, California, USA</em><span id="bib.bib92.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib93.4.4.1" class="ltx_text" style="font-size:90%;">Buolamwini and Gebru [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib93.6.1" class="ltx_text" style="font-size:90%;">
Joy Buolamwini and Timnit Gebru.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.7.1" class="ltx_text" style="font-size:90%;">Gender shades: Intersectional accuracy disparities in commercial
gender classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib93.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Conference on fairness, accountability and transparency</em><span id="bib.bib93.10.3" class="ltx_text" style="font-size:90%;">,
pages 77–91, 2018.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib94.5.5.1" class="ltx_text" style="font-size:90%;">Burkhart et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib94.7.1" class="ltx_text" style="font-size:90%;">
Martin Burkhart, Mario Strasser, Dilip Many, and Xenofontas Dimitropoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.8.1" class="ltx_text" style="font-size:90%;">SEPIA: Privacy-preserving aggregation of multi-domain network
events and statistics.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib94.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Network</em><span id="bib.bib94.10.2" class="ltx_text" style="font-size:90%;">, 1(101101), 2010.
</span>
</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib95.5.5.1" class="ltx_text" style="font-size:90%;">Caldas et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib95.7.1" class="ltx_text" style="font-size:90%;">
Sebastian Caldas, Jakub Konečný, H Brendan McMahan, and Ameet
Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib95.8.1" class="ltx_text" style="font-size:90%;">Expanding the reach of federated learning by reducing client resource
requirements.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib95.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.07210</em><span id="bib.bib95.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib96.5.5.1" class="ltx_text" style="font-size:90%;">Caldas et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib96.7.1" class="ltx_text" style="font-size:90%;">
Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečný, H Brendan
McMahan, Virginia Smith, and Ameet Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib96.8.1" class="ltx_text" style="font-size:90%;">LEAF: A benchmark for federated settings.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib96.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.01097</em><span id="bib.bib96.10.2" class="ltx_text" style="font-size:90%;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib97.5.5.1" class="ltx_text" style="font-size:90%;">Canonne et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib97.7.1" class="ltx_text" style="font-size:90%;">
Clément L Canonne, Gautam Kamath, Audra McMillan, Adam Smith, and Jonathan
Ullman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib97.8.1" class="ltx_text" style="font-size:90%;">The structure of optimal private tests for simple hypotheses.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib97.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">AarXiv preprint arXiv:1811.11148</em><span id="bib.bib97.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib98.4.4.1" class="ltx_text" style="font-size:90%;">Carlini and Wagner [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib98.6.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini and David Wagner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.7.1" class="ltx_text" style="font-size:90%;">Towards evaluating the robustness of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib98.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib98.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Symposium on Security and Privacy (SP)</em><span id="bib.bib98.10.3" class="ltx_text" style="font-size:90%;">, pages
39–57. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib99.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib99.7.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlingsson, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib99.8.1" class="ltx_text" style="font-size:90%;">The secret sharer: Measuring unintended neural network memorization
&amp; extracting secrets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib99.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.08232</em><span id="bib.bib99.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib100.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib100.7.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel
Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar
Erlingsson, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib100.8.1" class="ltx_text" style="font-size:90%;">Extracting training data from large language models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib100.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.07805</em><span id="bib.bib100.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib101.5.5.1" class="ltx_text" style="font-size:90%;">Ceballos et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib101.7.1" class="ltx_text" style="font-size:90%;">
Iker Ceballos, Vivek Sharma, Eduardo Mugica, Abhishek Singh, Albert Roman,
Praneeth Vepakomma, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib101.8.1" class="ltx_text" style="font-size:90%;">SplitNN-driven vertical partitioning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib101.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.04137</em><span id="bib.bib101.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib102.5.5.1" class="ltx_text" style="font-size:90%;">Chai Sim et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib102.7.1" class="ltx_text" style="font-size:90%;">
Khe Chai Sim, Françoise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas
Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif
Johnson, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib102.8.1" class="ltx_text" style="font-size:90%;">Personalization of end-to-end speech recognition on mobile devices
for named entities.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib102.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv</em><span id="bib.bib102.10.2" class="ltx_text" style="font-size:90%;">, pages arXiv–1912, 2019.
</span>
</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib103.5.5.1" class="ltx_text" style="font-size:90%;">Chan et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib103.7.1" class="ltx_text" style="font-size:90%;">
T-H Hubert Chan, Elaine Shi, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving stream aggregation with fault tolerance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib103.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib103.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Financial Cryptography and Data
Security</em><span id="bib.bib103.11.3" class="ltx_text" style="font-size:90%;">, pages 200–214. Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib104.5.5.1" class="ltx_text" style="font-size:90%;">Chang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib104.7.1" class="ltx_text" style="font-size:90%;">
Ken Chang, Niranjan Balachandar, Carson Lam, Darvin Yi, James Brown, Andrew
Beers, Bruce Rosen, Daniel L Rubin, and Jayashree Kalpathy-Cramer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib104.8.1" class="ltx_text" style="font-size:90%;">Distributed deep learning networks among institutions for medical
imaging.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib104.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of the American Medical Informatics Association</em><span id="bib.bib104.10.2" class="ltx_text" style="font-size:90%;">,
25(8):945–954, 2018.
</span>
</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib105.4.4.1" class="ltx_text" style="font-size:90%;">Chang and Tandon [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib105.6.1" class="ltx_text" style="font-size:90%;">
Wei-Ting Chang and Ravi Tandon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib105.7.1" class="ltx_text" style="font-size:90%;">On the upload versus download cost for secure and private matrix
multiplication.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib105.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib105.9.2" class="ltx_text" style="font-size:90%;">, abs/1906.10684, 2019.
</span>
</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib106.4.4.1" class="ltx_text" style="font-size:90%;">Charles and Konečnỳ [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib106.6.1" class="ltx_text" style="font-size:90%;">
Zachary Charles and Jakub Konečnỳ.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib106.7.1" class="ltx_text" style="font-size:90%;">On the outsized importance of learning rates in local update methods.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib106.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.00878</em><span id="bib.bib106.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib107.4.4.1" class="ltx_text" style="font-size:90%;">Chaum [1981]</span></span>
<span class="ltx_bibblock"><span id="bib.bib107.6.1" class="ltx_text" style="font-size:90%;">
David Chaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib107.7.1" class="ltx_text" style="font-size:90%;">Untraceable electronic mail, return addresses, and digital
pseudonyms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib107.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communications of the ACM</em><span id="bib.bib107.9.2" class="ltx_text" style="font-size:90%;">, 24(2), 1981.
</span>
</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib108.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib108.7.1" class="ltx_text" style="font-size:90%;">
Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin
Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib108.8.1" class="ltx_text" style="font-size:90%;">Detecting backdoor attacks on deep neural networks by activation
clustering.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib108.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.03728</em><span id="bib.bib108.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib109.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib109.7.1" class="ltx_text" style="font-size:90%;">
Chien-Lun Chen, Leana Golubchik, and Marco Paolieri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib109.8.1" class="ltx_text" style="font-size:90%;">Backdoor attacks on federated meta-learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib109.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.07026</em><span id="bib.bib109.10.2" class="ltx_text" style="font-size:90%;">, 2020a.
</span>
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib110.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib110.7.1" class="ltx_text" style="font-size:90%;">
Lijie Chen, Badih Ghazi, Ravi Kumar, and Pasin Manurangsi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.8.1" class="ltx_text" style="font-size:90%;">On distributed differential privacy and counting distinct elements.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib110.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib110.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Innovations in Theoretical Computer Science (ITCS)</em><span id="bib.bib110.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib111.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib111.7.1" class="ltx_text" style="font-size:90%;">
Lingjiao Chen, Hongyi Wang, Zachary B. Charles, and Dimitris S. Papailiopoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.8.1" class="ltx_text" style="font-size:90%;">DRACO: Byzantine-resilient distributed training via redundant
gradients.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib111.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib111.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 35th International Conference on Machine
Learning, ICML</em><span id="bib.bib111.11.3" class="ltx_text" style="font-size:90%;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib112.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib112.7.1" class="ltx_text" style="font-size:90%;">
Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.8.1" class="ltx_text" style="font-size:90%;">Federated learning of out-of-vocabulary words.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib112.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1903.10635</em><span id="bib.bib112.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib112.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1903.10635" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1903.10635</a><span id="bib.bib112.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib113.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2017a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib113.7.1" class="ltx_text" style="font-size:90%;">
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib113.8.1" class="ltx_text" style="font-size:90%;">ZOO: Zeroth order optimization based black-box attacks to deep
neural networks without training substitute models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib113.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib113.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 10th ACM Workshop on Artificial
Intelligence and Security</em><span id="bib.bib113.11.3" class="ltx_text" style="font-size:90%;">, pages 15–26. ACM, 2017a.
</span>
</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib114.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib114.7.1" class="ltx_text" style="font-size:90%;">
Wei-Ning Chen, Peter Kairouz, and Ayfer Ozgur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib114.8.1" class="ltx_text" style="font-size:90%;">Breaking the communication-privacy-accuracy trilemma.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib114.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib114.10.2" class="ltx_text" style="font-size:90%;">, 33,
2020b.
</span>
</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib115.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2017b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib115.7.1" class="ltx_text" style="font-size:90%;">
Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib115.8.1" class="ltx_text" style="font-size:90%;">Targeted backdoor attacks on deep learning systems using data
poisoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib115.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.05526</em><span id="bib.bib115.10.2" class="ltx_text" style="font-size:90%;">, 2017b.
</span>
</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib116.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. [2017c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib116.7.1" class="ltx_text" style="font-size:90%;">
Yudong Chen, Lili Su, and Jiaming Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib116.8.1" class="ltx_text" style="font-size:90%;">Distributed Statistical Machine Learning in Adversarial Settings:
Byzantine Gradient Descent.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib116.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">POMACS</em><span id="bib.bib116.10.2" class="ltx_text" style="font-size:90%;">, 1:44:1–44:25, 2017c.
</span>
</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib117.4.4.1" class="ltx_text" style="font-size:90%;">Chenal and Tang [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib117.6.1" class="ltx_text" style="font-size:90%;">
Massimo Chenal and Qiang Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib117.7.1" class="ltx_text" style="font-size:90%;">On key recovery attacks against existing somewhat homomorphic
encryption schemes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib117.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib117.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">LATINCRYPT</em><span id="bib.bib117.10.3" class="ltx_text" style="font-size:90%;">, volume 8895 of </span><em id="bib.bib117.11.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib117.12.5" class="ltx_text" style="font-size:90%;">, pages 239–258. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib118.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib118.7.1" class="ltx_text" style="font-size:90%;">
Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib118.8.1" class="ltx_text" style="font-size:90%;">SecureBoost: A lossless federated learning framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib118.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib118.10.2" class="ltx_text" style="font-size:90%;">, abs/1901.08755, 2019a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib118.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1901.08755" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1901.08755</a><span id="bib.bib118.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib119.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib119.7.1" class="ltx_text" style="font-size:90%;">
Raymond Cheng, Fan Zhang, Jernej Kos, Warren He, Nicholas Hynes, Noah Johnson,
Ari Juels, Andrew Miller, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib119.8.1" class="ltx_text" style="font-size:90%;">Ekiden: A platform for confidentiality-preserving, trustworthy, and
performant smart contracts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib119.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib119.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE European Symposium on Security and Privacy
(EuroS&amp;P)</em><span id="bib.bib119.11.3" class="ltx_text" style="font-size:90%;">, pages 185–200. IEEE, 2019b.
</span>
</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib120.5.5.1" class="ltx_text" style="font-size:90%;">Cheu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib120.7.1" class="ltx_text" style="font-size:90%;">
Albert Cheu, Adam Smith, Jonathan Ullman, David Zeber, and Maxim Zhilyaev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib120.8.1" class="ltx_text" style="font-size:90%;">Distributed differential privacy via shuffling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib120.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib120.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual International Conference on the Theory and
Applications of Cryptographic Techniques</em><span id="bib.bib120.11.3" class="ltx_text" style="font-size:90%;">, pages 375–403. Springer, 2019.
</span>
</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib121.5.5.1" class="ltx_text" style="font-size:90%;">Chor et al. [1998]</span></span>
<span class="ltx_bibblock"><span id="bib.bib121.7.1" class="ltx_text" style="font-size:90%;">
Benny Chor, Eyal Kushilevitz, Oded Goldreich, and Madhu Sudan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.8.1" class="ltx_text" style="font-size:90%;">Private information retrieval.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib121.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. ACM</em><span id="bib.bib121.10.2" class="ltx_text" style="font-size:90%;">, 45(6):965–981, November 1998.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.11.1" class="ltx_text" style="font-size:90%;">ISSN 0004-5411.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/293347.293350</span><span id="bib.bib121.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib121.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/293347.293350" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/293347.293350</a><span id="bib.bib121.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib122.5.5.1" class="ltx_text" style="font-size:90%;">Chou et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib122.7.1" class="ltx_text" style="font-size:90%;">
Edward Chou, Florian Tramèr, and Giancarlo Pellegrino.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib122.8.1" class="ltx_text" style="font-size:90%;">SentiNet: Detecting physical attacks against deep learning systems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib122.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00292</em><span id="bib.bib122.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib123.5.5.1" class="ltx_text" style="font-size:90%;">Chraibi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib123.7.1" class="ltx_text" style="font-size:90%;">
Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richtárik, Adil
Salim, and Martin Takáč.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib123.8.1" class="ltx_text" style="font-size:90%;">Distributed fixed point methods with compressed iterates.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib123.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1912.09925</em><span id="bib.bib123.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib124.4.4.1" class="ltx_text" style="font-size:90%;">Christen [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib124.6.1" class="ltx_text" style="font-size:90%;">
P. Christen.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib124.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Data matching: concepts and techniques for record linkage,
entity resolution, and duplicate detection</em><span id="bib.bib124.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib124.9.1" class="ltx_text" style="font-size:90%;">Springer Science &amp; Business Media, 2012.
</span>
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib125.4.4.1" class="ltx_text" style="font-size:90%;">Clara [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib125.6.1" class="ltx_text" style="font-size:90%;">
NVIDIA Clara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib125.7.1" class="ltx_text" style="font-size:90%;">The clara training framework authors, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib125.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://developer.nvidia.com/clara" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://developer.nvidia.com/clara</a><span id="bib.bib125.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib126.5.5.1" class="ltx_text" style="font-size:90%;">Cohen et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib126.7.1" class="ltx_text" style="font-size:90%;">
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib126.8.1" class="ltx_text" style="font-size:90%;">EMNIST: an extension of MNIST to handwritten letters.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib126.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1702.05373</em><span id="bib.bib126.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib127.5.5.1" class="ltx_text" style="font-size:90%;">Colin et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib127.7.1" class="ltx_text" style="font-size:90%;">
Igor Colin, Aurélien Bellet, Joseph Salmon, and Stéphan
Clémençon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib127.8.1" class="ltx_text" style="font-size:90%;">Gossip dual averaging for decentralized optimization of pairwise
functions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib127.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib127.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib127.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib128.5.5.1" class="ltx_text" style="font-size:90%;">Cormode et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib128.7.1" class="ltx_text" style="font-size:90%;">
Graham Cormode, Tejas Kulkarni, and Divesh Srivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib128.8.1" class="ltx_text" style="font-size:90%;">Marginal release under local differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib128.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib128.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 International Conference on
Management of Data</em><span id="bib.bib128.11.3" class="ltx_text" style="font-size:90%;">, pages 131–146. ACM, 2018.
</span>
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib129.5.5.1" class="ltx_text" style="font-size:90%;">Coron et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib129.7.1" class="ltx_text" style="font-size:90%;">
Jean-Sébastien Coron, Tancrède Lepoint, and Mehdi Tibouchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib129.8.1" class="ltx_text" style="font-size:90%;">Scale-invariant fully homomorphic encryption over the integers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib129.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib129.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Public Key Cryptography</em><span id="bib.bib129.11.3" class="ltx_text" style="font-size:90%;">, volume 8383 of </span><em id="bib.bib129.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes
in Computer Science</em><span id="bib.bib129.13.5" class="ltx_text" style="font-size:90%;">, pages 311–328. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib130.8.4.1" class="ltx_text" style="font-size:90%;">Corrigan-Gibbs and Boneh [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib130.10.1" class="ltx_text" style="font-size:90%;">
Henry Corrigan-Gibbs and Dan Boneh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib130.11.1" class="ltx_text" style="font-size:90%;">Prio: Private, robust, and scalable computation of aggregate
statistics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib130.12.5" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib130.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">14th <math id="bib.bib130.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib130.1.1.m1.1a"><mo stretchy="false" id="bib.bib130.1.1.m1.1.1" xref="bib.bib130.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib130.1.1.m1.1b"><ci id="bib.bib130.1.1.m1.1.1.cmml" xref="bib.bib130.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib130.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib130.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib130.2.2.m2.1a"><mo stretchy="false" id="bib.bib130.2.2.m2.1.1" xref="bib.bib130.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib130.2.2.m2.1b"><ci id="bib.bib130.2.2.m2.1.1.cmml" xref="bib.bib130.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib130.2.2.m2.1c">\}</annotation></semantics></math> Symposium on Networked Systems Design
and Implementation (<math id="bib.bib130.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib130.3.3.m3.1a"><mo stretchy="false" id="bib.bib130.3.3.m3.1.1" xref="bib.bib130.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib130.3.3.m3.1b"><ci id="bib.bib130.3.3.m3.1.1.cmml" xref="bib.bib130.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib130.3.3.m3.1c">\{</annotation></semantics></math>NSDI<math id="bib.bib130.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib130.4.4.m4.1a"><mo stretchy="false" id="bib.bib130.4.4.m4.1.1" xref="bib.bib130.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib130.4.4.m4.1b"><ci id="bib.bib130.4.4.m4.1.1.cmml" xref="bib.bib130.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib130.4.4.m4.1c">\}</annotation></semantics></math> 17)</em><span id="bib.bib130.13.6" class="ltx_text" style="font-size:90%;">, pages 259–282, 2017.
</span>
</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib131.4.4.1" class="ltx_text" style="font-size:90%;">Corrigan-Gibbs and
Kogan [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib131.6.1" class="ltx_text" style="font-size:90%;">
Henry Corrigan-Gibbs and Dmitry Kogan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib131.7.1" class="ltx_text" style="font-size:90%;">Private information retrieval with sublinear online time.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib131.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib131.9.2" class="ltx_text" style="font-size:90%;">, 2019:1075, 2019.
</span>
</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib132.4.4.1" class="ltx_text" style="font-size:90%;">Cortes and Mohri [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib132.6.1" class="ltx_text" style="font-size:90%;">
Corinna Cortes and Mehryar Mohri.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib132.7.1" class="ltx_text" style="font-size:90%;">Domain adaptation and sample bias correction theory and algorithm for
regression.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib132.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Theoretical Computer Science</em><span id="bib.bib132.9.2" class="ltx_text" style="font-size:90%;">, 519:103–126, 2014.
</span>
</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib133.5.5.1" class="ltx_text" style="font-size:90%;">Cortes et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib133.7.1" class="ltx_text" style="font-size:90%;">
Corinna Cortes, Mehryar Mohri, Ananda Theertha Suresh, and Ningshan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib133.8.1" class="ltx_text" style="font-size:90%;">Multiple-source adaptation with domain classifiers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib133.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.11036</em><span id="bib.bib133.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib134.4.4.1" class="ltx_text" style="font-size:90%;">Costan and Devadas [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib134.6.1" class="ltx_text" style="font-size:90%;">
Victor Costan and Srinivas Devadas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib134.7.1" class="ltx_text" style="font-size:90%;">Intel SGX explained.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib134.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib134.9.2" class="ltx_text" style="font-size:90%;">, 2016(086):1–118, 2016.
</span>
</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib135.9.5.1" class="ltx_text" style="font-size:90%;">Costan et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib135.11.1" class="ltx_text" style="font-size:90%;">
Victor Costan, Ilia Lebedev, and Srinivas Devadas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib135.12.1" class="ltx_text" style="font-size:90%;">Sanctum: Minimal hardware extensions for strong software isolation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib135.13.5" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib135.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">25th <math id="bib.bib135.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib135.1.1.m1.1a"><mo stretchy="false" id="bib.bib135.1.1.m1.1.1" xref="bib.bib135.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib135.1.1.m1.1b"><ci id="bib.bib135.1.1.m1.1.1.cmml" xref="bib.bib135.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib135.1.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib135.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib135.2.2.m2.1a"><mo stretchy="false" id="bib.bib135.2.2.m2.1.1" xref="bib.bib135.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib135.2.2.m2.1b"><ci id="bib.bib135.2.2.m2.1.1.cmml" xref="bib.bib135.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib135.2.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib135.3.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib135.3.3.m3.1a"><mo stretchy="false" id="bib.bib135.3.3.m3.1.1" xref="bib.bib135.3.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib135.3.3.m3.1b"><ci id="bib.bib135.3.3.m3.1.1.cmml" xref="bib.bib135.3.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib135.3.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib135.4.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib135.4.4.m4.1a"><mo stretchy="false" id="bib.bib135.4.4.m4.1.1" xref="bib.bib135.4.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib135.4.4.m4.1b"><ci id="bib.bib135.4.4.m4.1.1.cmml" xref="bib.bib135.4.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib135.4.4.m4.1c">\}</annotation></semantics></math>
Security 16)</em><span id="bib.bib135.14.6" class="ltx_text" style="font-size:90%;">, pages 857–874, 2016.
</span>
</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib136.5.5.1" class="ltx_text" style="font-size:90%;">Costello et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib136.7.1" class="ltx_text" style="font-size:90%;">
Craig Costello, Cédric Fournet, Jon Howell, Markulf Kohlweiss, Benjamin
Kreuter, Michael Naehrig, Bryan Parno, and Samee Zahur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib136.8.1" class="ltx_text" style="font-size:90%;">Geppetto: Versatile verifiable computation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib136.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib136.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security and Privacy</em><span id="bib.bib136.11.3" class="ltx_text" style="font-size:90%;">, pages 253–270.
IEEE Computer Society, 2015.
</span>
</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib137.5.5.1" class="ltx_text" style="font-size:90%;">Cotter et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib137.7.1" class="ltx_text" style="font-size:90%;">
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib137.8.1" class="ltx_text" style="font-size:90%;">Better mini-batch algorithms via accelerated gradient methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib137.9.1" class="ltx_text" style="font-size:90%;">In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K. Q.
Weinberger, editors, </span><em id="bib.bib137.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing
Systems</em><span id="bib.bib137.11.3" class="ltx_text" style="font-size:90%;">, volume 24, pages 1647–1655. Curran Associates, Inc., 2011.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib137.12.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://proceedings.neurips.cc/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://proceedings.neurips.cc/paper/2011/file/b55ec28c52d5f6205684a473a2193564-Paper.pdf</a><span id="bib.bib137.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib138.5.5.1" class="ltx_text" style="font-size:90%;">Courbariaux et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib138.7.1" class="ltx_text" style="font-size:90%;">
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib138.8.1" class="ltx_text" style="font-size:90%;">BinaryConnect: Training deep neural networks with binary weights
during propagations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib138.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib138.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib138.11.3" class="ltx_text" style="font-size:90%;">, pages
3123–3131, 2015.
</span>
</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib139.5.5.1" class="ltx_text" style="font-size:90%;">Courtiol et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib139.7.1" class="ltx_text" style="font-size:90%;">
Pierre Courtiol, Charles Maussion, Matahi Moarii, Elodie Pronier, Samuel
Pilcer, Meriem Sefta, Pierre Manceron, Sylvain Toldo, Mikhail Zaslavskiy,
Nolwenn Le Stang, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib139.8.1" class="ltx_text" style="font-size:90%;">Deep learning-based classification of mesothelioma improves
prediction of patient outcome.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib139.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Nature medicine</em><span id="bib.bib139.10.2" class="ltx_text" style="font-size:90%;">, pages 1–7, 2019.
</span>
</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib140.4.4.1" class="ltx_text" style="font-size:90%;">Cover and Thomas [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib140.6.1" class="ltx_text" style="font-size:90%;">
Thomas M Cover and Joy A Thomas.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib140.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Elements of information theory</em><span id="bib.bib140.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib140.9.1" class="ltx_text" style="font-size:90%;">John Wiley &amp; Sons, 2012.
</span>
</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib141.5.5.1" class="ltx_text" style="font-size:90%;">Cretu et al. [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib141.7.1" class="ltx_text" style="font-size:90%;">
Gabriela F Cretu, Angelos Stavrou, Michael E Locasto, Salvatore J Stolfo, and
Angelos D Keromytis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib141.8.1" class="ltx_text" style="font-size:90%;">Casting out demons: Sanitizing training data for anomaly sensors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib141.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib141.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2008 IEEE Symposium on Security and Privacy (sp 2008)</em><span id="bib.bib141.11.3" class="ltx_text" style="font-size:90%;">,
pages 81–95. IEEE, 2008.
</span>
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib142.5.5.1" class="ltx_text" style="font-size:90%;">Cummings et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib142.7.1" class="ltx_text" style="font-size:90%;">
Rachel Cummings, Sara Krehbiel, Kevin Lai, and Uthaipon Tantitongpipat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib142.8.1" class="ltx_text" style="font-size:90%;">Differential privacy for growing databases.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib142.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib142.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 31</em><span id="bib.bib142.11.3" class="ltx_text" style="font-size:90%;">,
NeurIPS ’18, pages 8864–8873, 2018a.
</span>
</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib143.5.5.1" class="ltx_text" style="font-size:90%;">Cummings et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib143.7.1" class="ltx_text" style="font-size:90%;">
Rachel Cummings, Sara Krehbiel, Yajun Mei, Rui Tuo, and Wanrong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib143.8.1" class="ltx_text" style="font-size:90%;">Differentially private change-point detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib143.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib143.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 31</em><span id="bib.bib143.11.3" class="ltx_text" style="font-size:90%;">,
NeurIPS ’18, pages 10825–10834, 2018b.
</span>
</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib144.5.5.1" class="ltx_text" style="font-size:90%;">Cummings et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib144.7.1" class="ltx_text" style="font-size:90%;">
Rachel Cummings, Inbal Dekel, Ori Heffetz, and Katrina Ligett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib144.8.1" class="ltx_text" style="font-size:90%;">Bringing differential privacy into the experimental economics lab:
Theory and an application to a public-good game.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib144.9.1" class="ltx_text" style="font-size:90%;">Working paper, 2019a.
</span>
</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib145.5.5.1" class="ltx_text" style="font-size:90%;">Cummings et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib145.7.1" class="ltx_text" style="font-size:90%;">
Rachel Cummings, Varun Gupta, Dhamma Kimpara, and Jamie Morgenstern.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib145.8.1" class="ltx_text" style="font-size:90%;">On the compatibility of privacy and fairness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib145.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib145.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Fairness in User Modeling, Adaptation and
Personalization</em><span id="bib.bib145.11.3" class="ltx_text" style="font-size:90%;">, FairUMAP, 2019b.
</span>
</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib146.4.4.1" class="ltx_text" style="font-size:90%;">Cyffers and Bellet [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib146.6.1" class="ltx_text" style="font-size:90%;">
Edwige Cyffers and Aurélien Bellet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib146.7.1" class="ltx_text" style="font-size:90%;">Privacy amplification by decentralization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib146.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.05326</em><span id="bib.bib146.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib147.5.4.1" class="ltx_text" style="font-size:90%;">Damgård [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib147.7.1" class="ltx_text" style="font-size:90%;">
Damgård.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib147.8.1" class="ltx_text" style="font-size:90%;">On </span><math id="bib.bib147.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="bib.bib147.1.m1.1a"><mi mathsize="90%" id="bib.bib147.1.m1.1.1" xref="bib.bib147.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="bib.bib147.1.m1.1b"><ci id="bib.bib147.1.m1.1.1.cmml" xref="bib.bib147.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib147.1.m1.1c">\sigma</annotation></semantics></math><span id="bib.bib147.9.2" class="ltx_text" style="font-size:90%;"> protocols.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="http://www.cs.au.dk/~ivan/Sigma.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://www.cs.au.dk/~ivan/Sigma.pdf</a><span id="bib.bib147.10.1" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib148.5.5.1" class="ltx_text" style="font-size:90%;">Data et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib148.7.1" class="ltx_text" style="font-size:90%;">
Deepesh Data, Linqi Song, and Suhas Diggavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib148.8.1" class="ltx_text" style="font-size:90%;">Data encoding for byzantine-resilient distributed optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib148.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib148.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib149.4.4.1" class="ltx_text" style="font-size:90%;">de Brouwer [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib149.6.1" class="ltx_text" style="font-size:90%;">
Walter de Brouwer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib149.7.1" class="ltx_text" style="font-size:90%;">The federated future is ready for shipping.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doc.ai/blog/federated-future-ready-shipping/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doc.ai/blog/federated-future-ready-shipping/</a><span id="bib.bib149.8.1" class="ltx_text" style="font-size:90%;">, March
2019.
</span>
</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib150.5.5.1" class="ltx_text" style="font-size:90%;">Dean et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib150.7.1" class="ltx_text" style="font-size:90%;">
Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V.
Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang,
and Andrew Y. Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib150.8.1" class="ltx_text" style="font-size:90%;">Large scale distributed deep networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib150.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib150.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the International Conference on Neural
Information Processing Systems</em><span id="bib.bib150.11.3" class="ltx_text" style="font-size:90%;">, pages 1223–1231, 2012.
</span>
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib151.5.5.1" class="ltx_text" style="font-size:90%;">Dekel et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib151.7.1" class="ltx_text" style="font-size:90%;">
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib151.8.1" class="ltx_text" style="font-size:90%;">Optimal distributed online prediction using mini-batches.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib151.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">J. Mach. Learn. Res.</em><span id="bib.bib151.10.2" class="ltx_text" style="font-size:90%;">, 13(1), January 2012.
</span>
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib152.5.5.1" class="ltx_text" style="font-size:90%;">Diakonikolas et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib152.7.1" class="ltx_text" style="font-size:90%;">
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Jacob Steinhardt, and
Alistair Stewart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib152.8.1" class="ltx_text" style="font-size:90%;">Sever: A robust meta-algorithm for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib152.9.1" class="ltx_text" style="font-size:90%;">In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
</span><em id="bib.bib152.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 36th International Conference on Machine Learning</em><span id="bib.bib152.11.3" class="ltx_text" style="font-size:90%;">,
volume 97 of </span><em id="bib.bib152.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning Research</em><span id="bib.bib152.13.5" class="ltx_text" style="font-size:90%;">, pages
1596–1606, Long Beach, California, USA, 09–15 Jun 2019. PMLR.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib152.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://proceedings.mlr.press/v97/diakonikolas19a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://proceedings.mlr.press/v97/diakonikolas19a.html</a><span id="bib.bib152.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib153.5.5.1" class="ltx_text" style="font-size:90%;">Diaz et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib153.7.1" class="ltx_text" style="font-size:90%;">
Mario Diaz, Peter Kairouz, Jiachun Liao, and Lalitha Sankar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib153.8.1" class="ltx_text" style="font-size:90%;">Theoretical guarantees for model auditing with finite adversaries.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib153.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.03405</em><span id="bib.bib153.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib154.4.4.1" class="ltx_text" style="font-size:90%;">Differential Privacy Team [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib154.6.1" class="ltx_text" style="font-size:90%;">
Differential Privacy Team.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib154.7.1" class="ltx_text" style="font-size:90%;">Learning with privacy at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib154.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Apple Machine Learning Journal</em><span id="bib.bib154.9.2" class="ltx_text" style="font-size:90%;">, 1(8), 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib154.10.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://machinelearning.apple.com/2017/12/06/learning-with-privacy-at-scale.html</a><span id="bib.bib154.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib155.5.5.1" class="ltx_text" style="font-size:90%;">Ding et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib155.7.1" class="ltx_text" style="font-size:90%;">
Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib155.8.1" class="ltx_text" style="font-size:90%;">Collecting telemetry data privately.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib155.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib155.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 30</em><span id="bib.bib155.11.3" class="ltx_text" style="font-size:90%;">,
December 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib155.12.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://www.microsoft.com/en-us/research/publication/collecting-telemetry-data-privately/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.microsoft.com/en-us/research/publication/collecting-telemetry-data-privately/</a><span id="bib.bib155.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib156.5.5.1" class="ltx_text" style="font-size:90%;">Ding et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib156.7.1" class="ltx_text" style="font-size:90%;">
Zeyu Ding, Yuxin Wang, Guanhong Wang, Danfeng Zhang, and Daniel Kifer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib156.8.1" class="ltx_text" style="font-size:90%;">Detecting violations of differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib156.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib156.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib156.11.3" class="ltx_text" style="font-size:90%;">, CCS ’18, pages 475–489, New York, NY, USA,
2018. ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib156.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-5693-0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib156.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/3243734.3243818</span><span id="bib.bib156.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib156.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/3243734.3243818" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/3243734.3243818</a><span id="bib.bib156.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib157.5.5.1" class="ltx_text" style="font-size:90%;">Dingledine et al. [2004]</span></span>
<span class="ltx_bibblock"><span id="bib.bib157.7.1" class="ltx_text" style="font-size:90%;">
Roger Dingledine, Nick Mathewson, and Paul Syverson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib157.8.1" class="ltx_text" style="font-size:90%;">Tor: The second-generation onion router.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib157.9.1" class="ltx_text" style="font-size:90%;">Technical report, Naval Research Lab Washington DC, 2004.
</span>
</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib158.5.5.1" class="ltx_text" style="font-size:90%;">Dinh et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib158.7.1" class="ltx_text" style="font-size:90%;">
Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib158.8.1" class="ltx_text" style="font-size:90%;">Personalized Federated Learning with Moreau Envelopes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib158.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib158.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib158.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib159.4.4.1" class="ltx_text" style="font-size:90%;">D’Oliveira and Rouayheb [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib159.6.1" class="ltx_text" style="font-size:90%;">
Rafael G. L. D’Oliveira and S. E. Rouayheb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib159.7.1" class="ltx_text" style="font-size:90%;">Lifting private information retrieval from two to any number of
messages.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib159.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib159.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE International Symposium on Information Theory
(ISIT)</em><span id="bib.bib159.10.3" class="ltx_text" style="font-size:90%;">, pages 1744–1748, June 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib159.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/ISIT.2018.8437805</span><span id="bib.bib159.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib160.4.4.1" class="ltx_text" style="font-size:90%;">Douceur [2002]</span></span>
<span class="ltx_bibblock"><span id="bib.bib160.6.1" class="ltx_text" style="font-size:90%;">
John R. Douceur.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib160.7.1" class="ltx_text" style="font-size:90%;">The sybil attack.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib160.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib160.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Revised Papers from the First International Workshop on
Peer-to-Peer Systems</em><span id="bib.bib160.10.3" class="ltx_text" style="font-size:90%;">, IPTPS ’01, pages 251–260, London, UK, UK, 2002.
Springer-Verlag.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib160.11.1" class="ltx_text" style="font-size:90%;">ISBN 3-540-44179-4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib160.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://dl.acm.org/citation.cfm?id=646334.687813" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://dl.acm.org/citation.cfm?id=646334.687813</a><span id="bib.bib160.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib161.5.5.1" class="ltx_text" style="font-size:90%;">Duchi et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib161.7.1" class="ltx_text" style="font-size:90%;">
John Duchi, Elad Hazan, and Yoram Singer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib161.8.1" class="ltx_text" style="font-size:90%;">Adaptive subgradient methods for online learning and stochastic
optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib161.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of machine learning research</em><span id="bib.bib161.10.2" class="ltx_text" style="font-size:90%;">, 12(7), 2011.
</span>
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib162.5.5.1" class="ltx_text" style="font-size:90%;">Duchi et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib162.7.1" class="ltx_text" style="font-size:90%;">
John C Duchi, Michael I Jordan, and Martin J Wainwright.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib162.8.1" class="ltx_text" style="font-size:90%;">Local privacy and statistical minimax rates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib162.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib162.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations of Computer Science (FOCS), 2013 IEEE 54th
Annual Symposium on</em><span id="bib.bib162.11.3" class="ltx_text" style="font-size:90%;">, pages 429–438. IEEE, 2013.
</span>
</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib163.5.5.1" class="ltx_text" style="font-size:90%;">Dutta et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib163.7.1" class="ltx_text" style="font-size:90%;">
Sanghamitra Dutta, Gauri Joshi, Soumyadip Ghosh, Parijat Dube, and Priya
Nagpurkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib163.8.1" class="ltx_text" style="font-size:90%;">Slow and Stale Gradients Can Win the Race: Error-Runtime Trade-offs
in Distributed SGD.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib163.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Artificial Intelligence and
Statistics (AISTATS)</em><span id="bib.bib163.10.2" class="ltx_text" style="font-size:90%;">, April 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib163.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1803.01113" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1803.01113</a><span id="bib.bib163.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib164.4.4.1" class="ltx_text" style="font-size:90%;">Dwork [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib164.6.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib164.7.1" class="ltx_text" style="font-size:90%;">Differential privacy: A survey of results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib164.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib164.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Theory and Applications of
Models of Computation</em><span id="bib.bib164.10.3" class="ltx_text" style="font-size:90%;">, pages 1–19. Springer, 2008.
</span>
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib165.4.4.1" class="ltx_text" style="font-size:90%;">Dwork and Roth [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib165.6.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork and Aaron Roth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib165.7.1" class="ltx_text" style="font-size:90%;">The algorithmic foundations of differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib165.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations and Trends in Theoretical Computer Science</em><span id="bib.bib165.9.2" class="ltx_text" style="font-size:90%;">,
9(3–4):211–407, 2014.
</span>
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib166.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. [2006a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib166.7.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni
Naor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib166.8.1" class="ltx_text" style="font-size:90%;">Our data, ourselves: Privacy via distributed noise generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib166.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib166.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual International Conference on the Theory and
Applications of Cryptographic Techniques</em><span id="bib.bib166.11.3" class="ltx_text" style="font-size:90%;">, pages 486–503. Springer,
2006a.
</span>
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib167.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. [2006b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib167.7.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib167.8.1" class="ltx_text" style="font-size:90%;">Calibrating noise to sensitivity in private data analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib167.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib167.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Theory of Cryptography Conference (TCC), New
York, New York</em><span id="bib.bib167.11.3" class="ltx_text" style="font-size:90%;">, volume 3876 of </span><em id="bib.bib167.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer Science</em><span id="bib.bib167.13.5" class="ltx_text" style="font-size:90%;">,
pages 265–284. Springer-Verlag, 2006b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib167.14.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1007/11681878˙14</span><span id="bib.bib167.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib168.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib168.7.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork, Guy N. Rothblum, and Salil Vadhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib168.8.1" class="ltx_text" style="font-size:90%;">Boosting and differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib168.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib168.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE 51st Annual Symposium on Foundations
of Computer Science</em><span id="bib.bib168.11.3" class="ltx_text" style="font-size:90%;">, FOCS ’10, pages 51–60, 2010.
</span>
</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib169.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib169.7.1" class="ltx_text" style="font-size:90%;">
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib169.8.1" class="ltx_text" style="font-size:90%;">Fairness through awareness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib169.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib169.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 3rd innovations in theoretical computer
science conference</em><span id="bib.bib169.11.3" class="ltx_text" style="font-size:90%;">, pages 214–226. ACM, 2012.
</span>
</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib170.5.5.1" class="ltx_text" style="font-size:90%;">Eckhouse et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib170.7.1" class="ltx_text" style="font-size:90%;">
Laurel Eckhouse, Kristian Lum, Cynthia Conti-Cook, and Julie Ciccolini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib170.8.1" class="ltx_text" style="font-size:90%;">Layers of bias: A unified approach for understanding problems with
risk assessment.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib170.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Criminal Justice and Behavior</em><span id="bib.bib170.10.2" class="ltx_text" style="font-size:90%;">, 46(2):185–209, 2019.
</span>
</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib171.5.5.1" class="ltx_text" style="font-size:90%;">Eichner et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib171.7.1" class="ltx_text" style="font-size:90%;">
Hubert Eichner, Tomer Koren, H. Brendan McMahan, Nathan Srebro, and Kunal
Talwar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib171.8.1" class="ltx_text" style="font-size:90%;">Semi-cyclic stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib171.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib171.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Accepted to ICML 2019.</em><span id="bib.bib171.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib171.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1904.10120" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1904.10120</a><span id="bib.bib171.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib172.5.5.1" class="ltx_text" style="font-size:90%;">Eldefrawy et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib172.7.1" class="ltx_text" style="font-size:90%;">
Karim Eldefrawy, Gene Tsudik, Aurélien Francillon, and Daniele Perito.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib172.8.1" class="ltx_text" style="font-size:90%;">SMART: secure and minimal architecture for (establishing dynamic)
root of trust.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib172.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib172.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NDSS</em><span id="bib.bib172.11.3" class="ltx_text" style="font-size:90%;">. The Internet Society, 2012.
</span>
</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib173.5.5.1" class="ltx_text" style="font-size:90%;">Elgabli et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib173.7.1" class="ltx_text" style="font-size:90%;">
Anis Elgabli, Jihong Park, Amrit S Bedi, Mehdi Bennis, and Vaneet Aggarwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib173.8.1" class="ltx_text" style="font-size:90%;">GADMM: Fast and communication efficient framework for distributed
machine learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib173.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.00047</em><span id="bib.bib173.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib174.5.5.1" class="ltx_text" style="font-size:90%;">Elgabli et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib174.7.1" class="ltx_text" style="font-size:90%;">
Anis Elgabli, Jihong Park, Chaouki Ben Issaid, and Mehdi Bennis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib174.8.1" class="ltx_text" style="font-size:90%;">Harnessing wireless channels for scalable and privacy-preserving
federated learning, 2020.
</span>
</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib175.5.5.1" class="ltx_text" style="font-size:90%;">Elsken et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib175.7.1" class="ltx_text" style="font-size:90%;">
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib175.8.1" class="ltx_text" style="font-size:90%;">Efficient multi-objective neural architecture search via Lamarckian
evolution.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib175.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.09081</em><span id="bib.bib175.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib176.5.5.1" class="ltx_text" style="font-size:90%;">Engstrom et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib176.7.1" class="ltx_text" style="font-size:90%;">
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander
Madry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib176.8.1" class="ltx_text" style="font-size:90%;">A rotation and a translation suffice: Fooling CNNs with simple
transformations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib176.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.02779</em><span id="bib.bib176.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib177.5.5.1" class="ltx_text" style="font-size:90%;">Erlingsson et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib177.7.1" class="ltx_text" style="font-size:90%;">
Úlfar Erlingsson, Vasyl Pihur, and Aleksandra Korolova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib177.8.1" class="ltx_text" style="font-size:90%;">RAPPOR: Randomized aggregatable privacy-preserving ordinal
response.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib177.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib177.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM CCS</em><span id="bib.bib177.11.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib177.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-2957-6.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib177.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/2660267.2660348</span><span id="bib.bib177.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib177.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/2660267.2660348" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/2660267.2660348</a><span id="bib.bib177.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib178.5.5.1" class="ltx_text" style="font-size:90%;">Erlingsson et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib178.7.1" class="ltx_text" style="font-size:90%;">
Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal
Talwar, and Abhradeep Thakurta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib178.8.1" class="ltx_text" style="font-size:90%;">Amplification by shuffling: From local to central differential
privacy via anonymity.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib178.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib178.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SODA</em><span id="bib.bib178.11.3" class="ltx_text" style="font-size:90%;">, pages 2468–2479, 2019.
</span>
</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib179.4.4.1" class="ltx_text" style="font-size:90%;">EU CORDIS [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib179.6.1" class="ltx_text" style="font-size:90%;">
EU CORDIS.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib179.7.1" class="ltx_text" style="font-size:90%;">Machine learning ledger orchestration for drug discovery, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib179.8.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://cordis.europa.eu/project/rcn/223634/factsheet/en?WT.mc_id=RSS-Feed&amp;WT.rss_f=project&amp;WT.rss_a=223634&amp;WT.rss_ev=a" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://cordis.europa.eu/project/rcn/223634/factsheet/en?WT.mc_id=RSS-Feed&amp;WT.rss_f=project&amp;WT.rss_a=223634&amp;WT.rss_ev=a</a><span id="bib.bib179.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib179.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib180.5.5.1" class="ltx_text" style="font-size:90%;">Falkner et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib180.7.1" class="ltx_text" style="font-size:90%;">
Stefan Falkner, Aaron Klein, and Frank Hutter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib180.8.1" class="ltx_text" style="font-size:90%;">BOHB: Robust and efficient hyperparameter optimization at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib180.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.01774</em><span id="bib.bib180.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib181.5.5.1" class="ltx_text" style="font-size:90%;">Fallah et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib181.7.1" class="ltx_text" style="font-size:90%;">
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib181.8.1" class="ltx_text" style="font-size:90%;">Personalized federated learning: A meta-learning approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib181.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.07948</em><span id="bib.bib181.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib182.4.4.1" class="ltx_text" style="font-size:90%;">Fan and Vercauteren [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib182.6.1" class="ltx_text" style="font-size:90%;">
Junfeng Fan and Frederik Vercauteren.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib182.7.1" class="ltx_text" style="font-size:90%;">Somewhat practical fully homomorphic encryption.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib182.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib182.9.2" class="ltx_text" style="font-size:90%;">, 2012:144, 2012.
</span>
</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib183.5.5.1" class="ltx_text" style="font-size:90%;">Fang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib183.7.1" class="ltx_text" style="font-size:90%;">
Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Zhenqiang Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib183.8.1" class="ltx_text" style="font-size:90%;">Local model poisoning attacks to Byzantine-robust federated
learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib183.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.11815</em><span id="bib.bib183.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib184.4.4.1" class="ltx_text" style="font-size:90%;">FeatureCloud [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib184.6.1" class="ltx_text" style="font-size:90%;">
FeatureCloud.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib184.7.1" class="ltx_text" style="font-size:90%;">FeatureCloud: Our vision, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib184.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://featurecloud.eu/about/our-vision/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://featurecloud.eu/about/our-vision/</a><span id="bib.bib184.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib184.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib185.5.5.1" class="ltx_text" style="font-size:90%;">Feldman et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib185.7.1" class="ltx_text" style="font-size:90%;">
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib185.8.1" class="ltx_text" style="font-size:90%;">Privacy amplification by iteration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib185.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib185.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE 59th Annual Symposium on Foundations of Computer
Science (FOCS)</em><span id="bib.bib185.11.3" class="ltx_text" style="font-size:90%;">, pages 521–532. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib186.5.5.1" class="ltx_text" style="font-size:90%;">Feutry et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib186.7.1" class="ltx_text" style="font-size:90%;">
Clément Feutry, Pablo Piantanida, Yoshua Bengio, and Pierre Duhamel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib186.8.1" class="ltx_text" style="font-size:90%;">Learning anonymized representations with adversarial neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib186.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib186.10.2" class="ltx_text" style="font-size:90%;">, abs/1802.09386, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib186.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1802.09386" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1802.09386</a><span id="bib.bib186.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib187.5.5.1" class="ltx_text" style="font-size:90%;">Finn et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib187.7.1" class="ltx_text" style="font-size:90%;">
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib187.8.1" class="ltx_text" style="font-size:90%;">Model-agnostic meta-learning for fast adaptation of deep networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib187.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib187.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning</em><span id="bib.bib187.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib188.5.5.1" class="ltx_text" style="font-size:90%;">Francillon et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib188.7.1" class="ltx_text" style="font-size:90%;">
Aurélien Francillon, Quan Nguyen, Kasper Bonne Rasmussen, and Gene
Tsudik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib188.8.1" class="ltx_text" style="font-size:90%;">A minimalist approach to remote attestation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib188.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib188.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">DATE</em><span id="bib.bib188.11.3" class="ltx_text" style="font-size:90%;">, pages 1–6. European Design and Automation
Association, 2014.
</span>
</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib189.5.5.1" class="ltx_text" style="font-size:90%;">Fredrikson et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib189.7.1" class="ltx_text" style="font-size:90%;">
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib189.8.1" class="ltx_text" style="font-size:90%;">Model inversion attacks that exploit confidence information and basic
countermeasures.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib189.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib189.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 22nd ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib189.11.3" class="ltx_text" style="font-size:90%;">, pages 1322–1333. ACM, 2015.
</span>
</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib190.5.5.1" class="ltx_text" style="font-size:90%;">Fung et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib190.7.1" class="ltx_text" style="font-size:90%;">
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib190.8.1" class="ltx_text" style="font-size:90%;">Mitigating sybils in federated learning poisoning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib190.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.04866</em><span id="bib.bib190.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib191.5.5.1" class="ltx_text" style="font-size:90%;">Furukawa et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib191.7.1" class="ltx_text" style="font-size:90%;">
Jun Furukawa, Yehuda Lindell, Ariel Nof, and Or Weinstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib191.8.1" class="ltx_text" style="font-size:90%;">High-throughput secure three-party computation for malicious
adversaries and an honest majority.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib191.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib191.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EUROCRYPT (2)</em><span id="bib.bib191.11.3" class="ltx_text" style="font-size:90%;">, volume 10211 of </span><em id="bib.bib191.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib191.13.5" class="ltx_text" style="font-size:90%;">, pages 225–255, 2017.
</span>
</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib192.4.4.1" class="ltx_text" style="font-size:90%;">Gaier and Ha [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib192.6.1" class="ltx_text" style="font-size:90%;">
Adam Gaier and David Ha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib192.7.1" class="ltx_text" style="font-size:90%;">Weight agnostic neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib192.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.04358</em><span id="bib.bib192.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib193.5.5.1" class="ltx_text" style="font-size:90%;">Gandikota et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib193.7.1" class="ltx_text" style="font-size:90%;">
Venkata Gandikota, Raj Kumar Maity, and Arya Mazumdar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib193.8.1" class="ltx_text" style="font-size:90%;">vqSGD: Vector quantized stochastic gradient descent.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib193.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.07971</em><span id="bib.bib193.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib194.5.5.1" class="ltx_text" style="font-size:90%;">Gascón et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib194.7.1" class="ltx_text" style="font-size:90%;">
Adrià Gascón, Phillipp Schoppmann, Borja Balle, Mariana Raykova,
Jack Doerner, Samee Zahur, and David Evans.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib194.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving distributed linear regression on high-dimensional
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib194.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">PoPETs</em><span id="bib.bib194.10.2" class="ltx_text" style="font-size:90%;">, 2017(4):345–364, 2017.
</span>
</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib195.5.5.1" class="ltx_text" style="font-size:90%;">Gennaro et al. [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib195.7.1" class="ltx_text" style="font-size:90%;">
Rosario Gennaro, Craig Gentry, and Bryan Parno.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib195.8.1" class="ltx_text" style="font-size:90%;">Non-interactive verifiable computing: Outsourcing computation to
untrusted workers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib195.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib195.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO</em><span id="bib.bib195.11.3" class="ltx_text" style="font-size:90%;">, volume 6223 of </span><em id="bib.bib195.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer
Science</em><span id="bib.bib195.13.5" class="ltx_text" style="font-size:90%;">, pages 465–482. Springer, 2010.
</span>
</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib196.5.5.1" class="ltx_text" style="font-size:90%;">Gennaro et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib196.7.1" class="ltx_text" style="font-size:90%;">
Rosario Gennaro, Craig Gentry, Bryan Parno, and Mariana Raykova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib196.8.1" class="ltx_text" style="font-size:90%;">Quadratic span programs and succinct NIZKs without PCPs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib196.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib196.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EUROCRYPT</em><span id="bib.bib196.11.3" class="ltx_text" style="font-size:90%;">, volume 7881 of </span><em id="bib.bib196.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer
Science</em><span id="bib.bib196.13.5" class="ltx_text" style="font-size:90%;">, pages 626–645. Springer, 2013.
</span>
</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib197.4.4.1" class="ltx_text" style="font-size:90%;">Gentry [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib197.6.1" class="ltx_text" style="font-size:90%;">
Craig Gentry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib197.7.1" class="ltx_text" style="font-size:90%;">Fully homomorphic encryption using ideal lattices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib197.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib197.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the forty-first annual ACM symposium on
Theory of computing</em><span id="bib.bib197.10.3" class="ltx_text" style="font-size:90%;">, pages 169–178, 2009.
</span>
</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib198.4.4.1" class="ltx_text" style="font-size:90%;">Gentry and Halevi [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib198.6.1" class="ltx_text" style="font-size:90%;">
Craig Gentry and Shai Halevi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib198.7.1" class="ltx_text" style="font-size:90%;">Compressible FHE with applications to PIR.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib198.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib198.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">TCC (2)</em><span id="bib.bib198.10.3" class="ltx_text" style="font-size:90%;">, volume 11892 of </span><em id="bib.bib198.11.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib198.12.5" class="ltx_text" style="font-size:90%;">, pages 438–464. Springer, 2019.
</span>
</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib199.5.5.1" class="ltx_text" style="font-size:90%;">Geyer et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib199.7.1" class="ltx_text" style="font-size:90%;">
Robin C. Geyer, Tassilo Klein, and Moin Nabi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib199.8.1" class="ltx_text" style="font-size:90%;">Differentially private federated learning: A client level
perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib199.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib199.10.2" class="ltx_text" style="font-size:90%;">, abs/1712.07557, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib199.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1712.07557" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1712.07557</a><span id="bib.bib199.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib200.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib200.7.1" class="ltx_text" style="font-size:90%;">
Badih Ghazi, Noah Golowich, Ravi Kumar, Rasmus Pagh, and Ameya Velingker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib200.8.1" class="ltx_text" style="font-size:90%;">On the power of multiple anonymous messages.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib200.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1908.11358</em><span id="bib.bib200.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib201.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib201.7.1" class="ltx_text" style="font-size:90%;">
Badih Ghazi, Rasmus Pagh, and Ameya Velingker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib201.8.1" class="ltx_text" style="font-size:90%;">Scalable and differentially private distributed aggregation in the
shuffled model.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib201.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.08320</em><span id="bib.bib201.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib202.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib202.7.1" class="ltx_text" style="font-size:90%;">
Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, Rasmus Pagh, and
Ameya Velingker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib202.8.1" class="ltx_text" style="font-size:90%;">Pure differentially private summation from anonymous messages.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib202.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib202.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ITC</em><span id="bib.bib202.11.3" class="ltx_text" style="font-size:90%;">, pages 15:1–15:23, 2020a.
</span>
</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib203.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib203.7.1" class="ltx_text" style="font-size:90%;">
Badih Ghazi, Ravi Kumar, Pasin Manurangsi, and Rasmus Pagh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib203.8.1" class="ltx_text" style="font-size:90%;">Private counting from anonymous messages: Near-optimal accuracy with
vanishing communication overhead.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib203.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib203.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib203.11.3" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib204.5.5.1" class="ltx_text" style="font-size:90%;">Ghazi et al. [2020c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib204.7.1" class="ltx_text" style="font-size:90%;">
Badih Ghazi, Pasin Manurangsi, Rasmus Pagh, and Ameya Velingker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib204.8.1" class="ltx_text" style="font-size:90%;">Private aggregation from fewer anonymous messages.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib204.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib204.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EUROCRYPT</em><span id="bib.bib204.11.3" class="ltx_text" style="font-size:90%;">, pages 798–827, 2020c.
</span>
</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib205.5.5.1" class="ltx_text" style="font-size:90%;">Ghosh et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib205.7.1" class="ltx_text" style="font-size:90%;">
Arpita Ghosh, Tim Roughgarden, and Mukund Sundararajan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib205.8.1" class="ltx_text" style="font-size:90%;">Universally utility-maximizing privacy mechanisms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib205.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib205.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Forty-first Annual ACM Symposium on
Theory of Computing</em><span id="bib.bib205.11.3" class="ltx_text" style="font-size:90%;">, STOC ’09, pages 351–360, New York, NY, USA, 2009. ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib205.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-60558-506-2.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib205.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/1536414.1536464</span><span id="bib.bib205.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib205.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/1536414.1536464" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/1536414.1536464</a><span id="bib.bib205.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib206.5.5.1" class="ltx_text" style="font-size:90%;">Gilad-Bachrach et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib206.7.1" class="ltx_text" style="font-size:90%;">
Ran Gilad-Bachrach, Nathan Dowlin, Kim Laine, Kristin E. Lauter, Michael
Naehrig, and John Wernsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib206.8.1" class="ltx_text" style="font-size:90%;">CryptoNets: Applying neural networks to encrypted data with high
throughput and accuracy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib206.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib206.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 33nd International Conference on Machine
Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016</em><span id="bib.bib206.11.3" class="ltx_text" style="font-size:90%;">, pages
201–210, 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib206.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://proceedings.mlr.press/v48/gilad-bachrach16.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://proceedings.mlr.press/v48/gilad-bachrach16.html</a><span id="bib.bib206.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib207.5.5.1" class="ltx_text" style="font-size:90%;">Girgis et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib207.7.1" class="ltx_text" style="font-size:90%;">
Antonious M Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and
Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib207.8.1" class="ltx_text" style="font-size:90%;">Shuffled model of federated learning: Privacy, communication and
accuracy trade-offs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib207.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.07180</em><span id="bib.bib207.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib208.5.5.1" class="ltx_text" style="font-size:90%;">Goldreich et al. [1987]</span></span>
<span class="ltx_bibblock"><span id="bib.bib208.7.1" class="ltx_text" style="font-size:90%;">
O. Goldreich, S. Micali, and A. Wigderson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib208.8.1" class="ltx_text" style="font-size:90%;">How to play any mental game.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib208.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib208.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Nineteenth Annual ACM Symposium on Theory
of Computing</em><span id="bib.bib208.11.3" class="ltx_text" style="font-size:90%;">, STOC ’87, pages 218–229, New York, NY, USA, 1987. ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib208.12.1" class="ltx_text" style="font-size:90%;">ISBN 0-89791-221-7.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib208.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/28395.28420</span><span id="bib.bib208.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib208.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/28395.28420" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/28395.28420</a><span id="bib.bib208.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib209" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib209.5.5.1" class="ltx_text" style="font-size:90%;">Goldwasser et al. [1989]</span></span>
<span class="ltx_bibblock"><span id="bib.bib209.7.1" class="ltx_text" style="font-size:90%;">
Shafi Goldwasser, Silvio Micali, and Charles Rackoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib209.8.1" class="ltx_text" style="font-size:90%;">The knowledge complexity of interactive proof systems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib209.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIAM J. Comput.</em><span id="bib.bib209.10.2" class="ltx_text" style="font-size:90%;">, 18(1):186–208, 1989.
</span>
</span>
</li>
<li id="bib.bib210" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib210.5.5.1" class="ltx_text" style="font-size:90%;">Goldwasser et al. [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib210.7.1" class="ltx_text" style="font-size:90%;">
Shafi Goldwasser, Yael Tauman Kalai, and Guy N. Rothblum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib210.8.1" class="ltx_text" style="font-size:90%;">Delegating computation: interactive proofs for muggles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib210.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib210.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">STOC</em><span id="bib.bib210.11.3" class="ltx_text" style="font-size:90%;">, pages 113–122. ACM, 2008.
</span>
</span>
</li>
<li id="bib.bib211" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib211.5.5.1" class="ltx_text" style="font-size:90%;">Goodfellow et al. [2015a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib211.7.1" class="ltx_text" style="font-size:90%;">
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib211.8.1" class="ltx_text" style="font-size:90%;">Explaining and harnessing adversarial examples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib211.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib211.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
Proceedings</em><span id="bib.bib211.11.3" class="ltx_text" style="font-size:90%;">, 2015a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib211.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1412.6572" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1412.6572</a><span id="bib.bib211.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib212" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib212.5.5.1" class="ltx_text" style="font-size:90%;">Goodfellow et al. [2015b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib212.7.1" class="ltx_text" style="font-size:90%;">
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib212.8.1" class="ltx_text" style="font-size:90%;">Explaining and harnessing adversarial examples.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib212.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib212.10.2" class="ltx_text" style="font-size:90%;">, 2015b.
</span>
</span>
</li>
<li id="bib.bib213" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib213.4.4.1" class="ltx_text" style="font-size:90%;">Goryczka and Xiong [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib213.6.1" class="ltx_text" style="font-size:90%;">
Slawomir Goryczka and Li Xiong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib213.7.1" class="ltx_text" style="font-size:90%;">A comprehensive comparison of multiparty secure additions with
differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib213.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Trans. Dependable Sec. Comput.</em><span id="bib.bib213.9.2" class="ltx_text" style="font-size:90%;">, 14(5):463–477, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib213.10.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/TDSC.2015.2484326</span><span id="bib.bib213.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib213.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1109/TDSC.2015.2484326" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1109/TDSC.2015.2484326</a><span id="bib.bib213.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib214" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib214.5.5.1" class="ltx_text" style="font-size:90%;">Gu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib214.7.1" class="ltx_text" style="font-size:90%;">
Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib214.8.1" class="ltx_text" style="font-size:90%;">BadNets: Identifying vulnerabilities in the machine learning model
supply chain.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib214.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1708.06733</em><span id="bib.bib214.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib215" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib215.4.4.1" class="ltx_text" style="font-size:90%;">Gupta and Raskar [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib215.6.1" class="ltx_text" style="font-size:90%;">
Otkrist Gupta and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib215.7.1" class="ltx_text" style="font-size:90%;">Distributed learning of deep neural network over multiple agents.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib215.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Network and Computer Applications</em><span id="bib.bib215.9.2" class="ltx_text" style="font-size:90%;">, 116:1–8, 2018.
</span>
</span>
</li>
<li id="bib.bib216" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib216.5.5.1" class="ltx_text" style="font-size:90%;">Haddadpour et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib216.7.1" class="ltx_text" style="font-size:90%;">
Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck R
Cadambe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib216.8.1" class="ltx_text" style="font-size:90%;">Local SGD with periodic averaging: Tighter analysis and adaptive
synchronization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib216.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.13598</em><span id="bib.bib216.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib217" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib217.5.5.1" class="ltx_text" style="font-size:90%;">Haeberlen et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib217.7.1" class="ltx_text" style="font-size:90%;">
Andreas Haeberlen, Benjamin C Pierce, and Arjun Narayan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib217.8.1" class="ltx_text" style="font-size:90%;">Differential privacy under fire.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib217.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib217.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">USENIX Security Symposium</em><span id="bib.bib217.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib218" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib218.5.5.1" class="ltx_text" style="font-size:90%;">Halevi et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib218.7.1" class="ltx_text" style="font-size:90%;">
Shai Halevi, Yehuda Lindell, and Benny Pinkas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib218.8.1" class="ltx_text" style="font-size:90%;">Secure computation on the web: Computing without simultaneous
interaction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib218.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib218.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Cryptology Conference</em><span id="bib.bib218.11.3" class="ltx_text" style="font-size:90%;">, pages 132–150. Springer,
2011.
</span>
</span>
</li>
<li id="bib.bib219" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib219.5.5.1" class="ltx_text" style="font-size:90%;">Hamer et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib219.7.1" class="ltx_text" style="font-size:90%;">
Jenny Hamer, Mehryar Mohri, and Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib219.8.1" class="ltx_text" style="font-size:90%;">Fedboost: A communication-efficient algorithm for federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib219.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib219.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib219.11.3" class="ltx_text" style="font-size:90%;">, pages
3973–3983. PMLR, 2020.
</span>
</span>
</li>
<li id="bib.bib220" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib220.5.5.1" class="ltx_text" style="font-size:90%;">Han et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib220.7.1" class="ltx_text" style="font-size:90%;">
Song Han, Huizi Mao, and William J Dally.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib220.8.1" class="ltx_text" style="font-size:90%;">Deep compression: Compressing deep neural networks with pruning,
trained quantization and huffman coding.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib220.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1510.00149</em><span id="bib.bib220.10.2" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib221" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib221.5.5.1" class="ltx_text" style="font-size:90%;">Han et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib221.7.1" class="ltx_text" style="font-size:90%;">
Yanjun Han, Ayfer Özgür, and Tsachy Weissman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib221.8.1" class="ltx_text" style="font-size:90%;">Geometric lower bounds for distributed parameter estimation under
communication constraints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib221.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib221.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning Research</em><span id="bib.bib221.11.3" class="ltx_text" style="font-size:90%;">, pages 1–26, 75,
2018.
</span>
</span>
</li>
<li id="bib.bib222" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib222.5.5.1" class="ltx_text" style="font-size:90%;">Hard et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib222.7.1" class="ltx_text" style="font-size:90%;">
Andrew Hard, Kanishka Rao, Rajiv Mathews, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib222.8.1" class="ltx_text" style="font-size:90%;">Federated learning for mobile keyboard prediction.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib222.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1811.03604</em><span id="bib.bib222.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib223" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib223.5.5.1" class="ltx_text" style="font-size:90%;">Hardt et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib223.7.1" class="ltx_text" style="font-size:90%;">
Moritz Hardt, Eric Price, and Nathan Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib223.8.1" class="ltx_text" style="font-size:90%;">Equality of opportunity in supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib223.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib223.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib223.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib224" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib224.5.5.1" class="ltx_text" style="font-size:90%;">Hardy et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib224.7.1" class="ltx_text" style="font-size:90%;">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib224.8.1" class="ltx_text" style="font-size:90%;">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib224.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.10677</em><span id="bib.bib224.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib225" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib225.5.5.1" class="ltx_text" style="font-size:90%;">Hashimoto et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib225.7.1" class="ltx_text" style="font-size:90%;">
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib225.8.1" class="ltx_text" style="font-size:90%;">Fairness without demographics in repeated loss minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib225.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib225.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib225.11.3" class="ltx_text" style="font-size:90%;">, pages
1934–1943, 2018.
</span>
</span>
</li>
<li id="bib.bib226" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib226.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib226.7.1" class="ltx_text" style="font-size:90%;">
Chaoyang He, Conghui Tan, Hanlin Tang, Shuang Qiu, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib226.8.1" class="ltx_text" style="font-size:90%;">Central server free federated learning over single-sided trust social
networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib226.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.04956</em><span id="bib.bib226.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib227" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib227.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib227.7.1" class="ltx_text" style="font-size:90%;">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib227.8.1" class="ltx_text" style="font-size:90%;">Group knowledge transfer: Federated learning of large cnns at the
edge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib227.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib227.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems 34</em><span id="bib.bib227.11.3" class="ltx_text" style="font-size:90%;">,
2020a.
</span>
</span>
</li>
<li id="bib.bib228" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib228.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib228.7.1" class="ltx_text" style="font-size:90%;">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib228.8.1" class="ltx_text" style="font-size:90%;">Fednas: Federated deep learning via neural architecture search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib228.9.1" class="ltx_text" style="font-size:90%;">2020b.
</span>
</span>
</li>
<li id="bib.bib229" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib229.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2020c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib229.7.1" class="ltx_text" style="font-size:90%;">
Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang
Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Xinghua Zhu, Jianzong
Wang, Li Shen, Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar, Qiang Yang,
Murali Annavaram, and Salman Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib229.8.1" class="ltx_text" style="font-size:90%;">Fedml: A research library and benchmark for federated machine
learning, 2020c.
</span>
</span>
</li>
<li id="bib.bib230" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib230.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2020d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib230.7.1" class="ltx_text" style="font-size:90%;">
Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib230.8.1" class="ltx_text" style="font-size:90%;">Milenas: Efficient neural architecture search via mixed-level
reformulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib230.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib230.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em><span id="bib.bib230.11.3" class="ltx_text" style="font-size:90%;">, 2020d.
</span>
</span>
</li>
<li id="bib.bib231" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib231.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib231.7.1" class="ltx_text" style="font-size:90%;">
Lie He, An Bian, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib231.8.1" class="ltx_text" style="font-size:90%;">COLA: Decentralized linear learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib231.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib231.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS 2018 - Advances in Neural Information Processing
Systems 31</em><span id="bib.bib231.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib232" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib232.5.5.1" class="ltx_text" style="font-size:90%;">Hébert-Johnson et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib232.7.1" class="ltx_text" style="font-size:90%;">
Úrsula Hébert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib232.8.1" class="ltx_text" style="font-size:90%;">Multicalibration: Calibration for the (computationally-identifiable)
masses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib232.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib232.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib232.11.3" class="ltx_text" style="font-size:90%;">, pages
1944–1953, 2018.
</span>
</span>
</li>
<li id="bib.bib233" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib233.3.3.1" class="ltx_text" style="font-size:90%;">[233]</span></span>
<span class="ltx_bibblock"><span id="bib.bib233.5.1" class="ltx_text" style="font-size:90%;">
HElib.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib233.6.1" class="ltx_text" style="font-size:90%;">HElib.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/homenc/HElib" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/homenc/HElib</a><span id="bib.bib233.7.1" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib234" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib234.5.5.1" class="ltx_text" style="font-size:90%;">Hoffman et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib234.7.1" class="ltx_text" style="font-size:90%;">
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib234.8.1" class="ltx_text" style="font-size:90%;">Algorithms and theory for multiple-source adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib234.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib234.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib234.11.3" class="ltx_text" style="font-size:90%;">, pages
8246–8256, 2018.
</span>
</span>
</li>
<li id="bib.bib235" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib235.5.5.1" class="ltx_text" style="font-size:90%;">Horvath et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib235.7.1" class="ltx_text" style="font-size:90%;">
Samuel Horvath, Chen-Yu Ho, Ludovit Horvath, Atal Narayan Sahu, Marco Canini,
and Peter Richtarik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib235.8.1" class="ltx_text" style="font-size:90%;">Natural compression for distributed deep learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib235.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10988</em><span id="bib.bib235.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib236" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib236.5.5.1" class="ltx_text" style="font-size:90%;">Hsieh et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib236.7.1" class="ltx_text" style="font-size:90%;">
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B. Gibbons.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib236.8.1" class="ltx_text" style="font-size:90%;">The non-IID data quagmire of decentralized machine learning, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib236.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1910.00189" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1910.00189</a><span id="bib.bib236.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib237" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib237.5.5.1" class="ltx_text" style="font-size:90%;">Hsu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib237.7.1" class="ltx_text" style="font-size:90%;">
Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib237.8.1" class="ltx_text" style="font-size:90%;">Measuring the effects of non-identical data distribution for
federated visual classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib237.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.06335</em><span id="bib.bib237.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib238" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib238.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib238.7.1" class="ltx_text" style="font-size:90%;">
Yaochen Hu, Peng Liu, Linglong Kong, and Di Niu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib238.8.1" class="ltx_text" style="font-size:90%;">Learning privately over distributed features: An admm sharing
approach, 2019.
</span>
</span>
</li>
<li id="bib.bib239" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib239.5.5.1" class="ltx_text" style="font-size:90%;">Huang et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib239.7.1" class="ltx_text" style="font-size:90%;">
Zhenqi Huang, Sayan Mitra, and Nitin Vaidya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib239.8.1" class="ltx_text" style="font-size:90%;">Differentially Private Distributed Optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib239.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib239.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICDCN</em><span id="bib.bib239.11.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib240" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib240.5.5.1" class="ltx_text" style="font-size:90%;">Huo et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib240.7.1" class="ltx_text" style="font-size:90%;">
Zhouyuan Huo, Bin Gu, and Heng Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib240.8.1" class="ltx_text" style="font-size:90%;">Training neural networks using features replay.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib240.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib240.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib240.11.3" class="ltx_text" style="font-size:90%;">, pages
6659–6668, 2018.
</span>
</span>
</li>
<li id="bib.bib241" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib241.4.4.1" class="ltx_text" style="font-size:90%;">Intel [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib241.6.1" class="ltx_text" style="font-size:90%;">
R Intel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib241.7.1" class="ltx_text" style="font-size:90%;">Architecture instruction set extensions programming reference.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib241.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Intel Corporation, Feb</em><span id="bib.bib241.9.2" class="ltx_text" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li id="bib.bib242" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib242.5.5.1" class="ltx_text" style="font-size:90%;">Ion et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib242.7.1" class="ltx_text" style="font-size:90%;">
Mihaela Ion, Ben Kreuter, Erhan Nergiz, Sarvar Patel, Shobhit Saxena, Karn
Seth, David Shanahan, and Moti Yung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib242.8.1" class="ltx_text" style="font-size:90%;">Private intersection-sum protocol with applications to attributing
aggregate ad conversions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib242.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib242.10.2" class="ltx_text" style="font-size:90%;">, 2017:738, 2017.
</span>
</span>
</li>
<li id="bib.bib243" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib243.5.5.1" class="ltx_text" style="font-size:90%;">Ion et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib243.7.1" class="ltx_text" style="font-size:90%;">
Mihaela Ion, Ben Kreuter, Ahmet Erhan Nergiz, Sarvar Patel, Mariana Raykova,
Shobhit Saxena, Karn Seth, David Shanahan, and Moti Yung.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib243.8.1" class="ltx_text" style="font-size:90%;">On deploying secure computing commercially: Private intersection-sum
protocols and their business applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib243.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib243.10.2" class="ltx_text" style="font-size:90%;">, 2019:723, 2019.
</span>
</span>
</li>
<li id="bib.bib244" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib244.5.5.1" class="ltx_text" style="font-size:90%;">Ishai et al. [2003]</span></span>
<span class="ltx_bibblock"><span id="bib.bib244.7.1" class="ltx_text" style="font-size:90%;">
Yuval Ishai, Joe Kilian, Kobbi Nissim, and Erez Petrank.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib244.8.1" class="ltx_text" style="font-size:90%;">Extending oblivious transfers efficiently.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib244.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib244.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO</em><span id="bib.bib244.11.3" class="ltx_text" style="font-size:90%;">, volume 2729 of </span><em id="bib.bib244.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in Computer
Science</em><span id="bib.bib244.13.5" class="ltx_text" style="font-size:90%;">, pages 145–161. Springer, 2003.
</span>
</span>
</li>
<li id="bib.bib245" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib245.5.5.1" class="ltx_text" style="font-size:90%;">Jaderberg et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib245.7.1" class="ltx_text" style="font-size:90%;">
Max Jaderberg, Wojciech Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
Graves, David Silver, and Koray Kavukcuoglu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib245.8.1" class="ltx_text" style="font-size:90%;">Decoupled neural interfaces using synthetic gradients.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib245.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib245.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em><span id="bib.bib245.11.3" class="ltx_text" style="font-size:90%;">, pages 1627–1635. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib246" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib246.5.5.1" class="ltx_text" style="font-size:90%;">Jagielski et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib246.7.1" class="ltx_text" style="font-size:90%;">
Matthew Jagielski, Michael J. Kearns, Jieming Mao, Alina Oprea, Aaron Roth,
Saeed Sharifi-Malvajerdi, and Jonathan Ullman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib246.8.1" class="ltx_text" style="font-size:90%;">Differentially private fair learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib246.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib246.10.2" class="ltx_text" style="font-size:90%;">, abs/1812.02696, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib246.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1812.02696" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1812.02696</a><span id="bib.bib246.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib247" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib247.5.5.1" class="ltx_text" style="font-size:90%;">Jagielski et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib247.7.1" class="ltx_text" style="font-size:90%;">
Matthew Jagielski, Jonathan Ullman, and Alina Oprea.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib247.8.1" class="ltx_text" style="font-size:90%;">Auditing differentially private machine learning: How private is
private sgd?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib247.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib247.10.2" class="ltx_text" style="font-size:90%;">, 33, 2020.
</span>
</span>
</li>
<li id="bib.bib248" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib248.5.5.1" class="ltx_text" style="font-size:90%;">Jeong et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib248.7.1" class="ltx_text" style="font-size:90%;">
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and
Seong-Lyun Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib248.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient on-device machine learning: Federated
distillation and augmentation under non-IID private data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib248.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib248.10.2" class="ltx_text" style="font-size:90%;">, abs/1811.11479, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib248.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1811.11479" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1811.11479</a><span id="bib.bib248.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib249" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib249.4.4.1" class="ltx_text" style="font-size:90%;">Jia and Jafar [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib249.6.1" class="ltx_text" style="font-size:90%;">
Zhuqing Jia and Syed Ali Jafar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib249.7.1" class="ltx_text" style="font-size:90%;">On the capacity of secure distributed matrix multiplication.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib249.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ArXiv</em><span id="bib.bib249.9.2" class="ltx_text" style="font-size:90%;">, abs/1908.06957, 2019.
</span>
</span>
</li>
<li id="bib.bib250" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib250.5.5.1" class="ltx_text" style="font-size:90%;">Jiang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib250.7.1" class="ltx_text" style="font-size:90%;">
Yihan Jiang, Jakub Konečný, Keith Rush, and Sreeram Kannan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib250.8.1" class="ltx_text" style="font-size:90%;">Improving federated learning personalization via model agnostic meta
learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib250.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.12488</em><span id="bib.bib250.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib251" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib251.5.5.1" class="ltx_text" style="font-size:90%;">Kadhe et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib251.7.1" class="ltx_text" style="font-size:90%;">
S. Kadhe, B. Garcia, A. Heidarzadeh, S. E. Rouayheb, and
A. Sprintson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib251.8.1" class="ltx_text" style="font-size:90%;">Private information retrieval with side information: The single
server case.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib251.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib251.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 55th Annual Allerton Conference on Communication,
Control, and Computing (Allerton)</em><span id="bib.bib251.11.3" class="ltx_text" style="font-size:90%;">, pages 1099–1106, Oct 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib251.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/ALLERTON.2017.8262860</span><span id="bib.bib251.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib252" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib252.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib252.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib252.8.1" class="ltx_text" style="font-size:90%;">Extremal mechanisms for local differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib252.9.1" class="ltx_text" style="font-size:90%;">In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q.
Weinberger, editors, </span><em id="bib.bib252.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems
27</em><span id="bib.bib252.11.3" class="ltx_text" style="font-size:90%;">, pages 2879–2887. Curran Associates, Inc., 2014.
</span>
</span>
</li>
<li id="bib.bib253" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib253.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib253.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, K. A. Bonawitz, and Daniel Ramage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib253.8.1" class="ltx_text" style="font-size:90%;">Discrete distribution estimation under local privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib253.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib253.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib253.11.3" class="ltx_text" style="font-size:90%;">, pages
2436–2444, 2016.
</span>
</span>
</li>
<li id="bib.bib254" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib254.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib254.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, Sewoong Oh, and Pramod Viswanath.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib254.8.1" class="ltx_text" style="font-size:90%;">The composition theorem for differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib254.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib254.10.2" class="ltx_text" style="font-size:90%;">, 63(6):4037–4049, 2017.
</span>
</span>
</li>
<li id="bib.bib255" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib255.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib255.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, Jiachun Liao, Chong Huang, and Lalitha Sankar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib255.8.1" class="ltx_text" style="font-size:90%;">Censored and fair universal representations using generative
adversarial models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib255.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.00411</em><span id="bib.bib255.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib256" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib256.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2021a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib256.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, Ziyu Liu, and Thomas Steinke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib256.8.1" class="ltx_text" style="font-size:90%;">The distributed discrete gaussian mechanism for federated learning
with secure aggregation, 2021a.
</span>
</span>
</li>
<li id="bib.bib257" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib257.5.5.1" class="ltx_text" style="font-size:90%;">Kairouz et al. [2021b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib257.7.1" class="ltx_text" style="font-size:90%;">
Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta,
and Zheng Xu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib257.8.1" class="ltx_text" style="font-size:90%;">Practical and private (deep) learning without sampling or shuffling,
2021b.
</span>
</span>
</li>
<li id="bib.bib258" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib258.5.5.1" class="ltx_text" style="font-size:90%;">Kamishima et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib258.7.1" class="ltx_text" style="font-size:90%;">
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib258.8.1" class="ltx_text" style="font-size:90%;">Fairness-aware learning through regularization approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib258.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib258.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2011 IEEE 11th International Conference on Data Mining
Workshops</em><span id="bib.bib258.11.3" class="ltx_text" style="font-size:90%;">, pages 643–650. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib259" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib259.5.5.1" class="ltx_text" style="font-size:90%;">Kang et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib259.7.1" class="ltx_text" style="font-size:90%;">
Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib259.8.1" class="ltx_text" style="font-size:90%;">Testing robustness against unforeseen adversaries.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib259.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.08016</em><span id="bib.bib259.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib260" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib260.5.5.1" class="ltx_text" style="font-size:90%;">Kang et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib260.7.1" class="ltx_text" style="font-size:90%;">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib260.8.1" class="ltx_text" style="font-size:90%;">Incentive mechanism for reliable federated learning: A joint
optimization approach to combining reputation and contract theory.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib260.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Internet of Things Journal</em><span id="bib.bib260.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib261" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib261.5.5.1" class="ltx_text" style="font-size:90%;">Kang et al. [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib261.7.1" class="ltx_text" style="font-size:90%;">
Jiawen Kang, Zehui Xiong, Dusit Niyato, Han Yu, Ying-Chang Liang, and Dong In
Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib261.8.1" class="ltx_text" style="font-size:90%;">Incentive design for efficient federated learning in mobile networks:
A contract theory approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib261.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib261.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE VTS Asia Pacific Wireless Communications Symposium,
APWCS 2019, Singapore, August 28-30, 2019</em><span id="bib.bib261.11.3" class="ltx_text" style="font-size:90%;">, pages 1–5, 2019c.
</span>
</span>
</li>
<li id="bib.bib262" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib262.5.5.1" class="ltx_text" style="font-size:90%;">Karimi et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib262.7.1" class="ltx_text" style="font-size:90%;">
Hamed Karimi, Julie Nutini, and Mark Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib262.8.1" class="ltx_text" style="font-size:90%;">Linear convergence of gradient and proximal-gradient methods under
the Polyak-łojasiewicz condition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib262.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib262.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Joint European Conference on Machine Learning and Knowledge
Discovery in Databases</em><span id="bib.bib262.11.3" class="ltx_text" style="font-size:90%;">, pages 795–811. Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib263" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib263.5.5.1" class="ltx_text" style="font-size:90%;">Karimireddy et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib263.7.1" class="ltx_text" style="font-size:90%;">
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib263.8.1" class="ltx_text" style="font-size:90%;">Error feedback fixes SignSGD and other gradient compression
schemes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib263.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib263.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib263.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib264" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib264.5.5.1" class="ltx_text" style="font-size:90%;">Karimireddy et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib264.7.1" class="ltx_text" style="font-size:90%;">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J
Reddi, Sebastian U Stich, and Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib264.8.1" class="ltx_text" style="font-size:90%;">Mime: Mimicking centralized stochastic algorithms in federated
learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib264.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.03606</em><span id="bib.bib264.10.2" class="ltx_text" style="font-size:90%;">, 2020a.
</span>
</span>
</li>
<li id="bib.bib265" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib265.5.5.1" class="ltx_text" style="font-size:90%;">Karimireddy et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib265.7.1" class="ltx_text" style="font-size:90%;">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib265.8.1" class="ltx_text" style="font-size:90%;">Scaffold: Stochastic controlled averaging for federated learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib265.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib265.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib265.11.3" class="ltx_text" style="font-size:90%;">, pages
5132–5143. PMLR, 2020b.
</span>
</span>
</li>
<li id="bib.bib266" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib266.5.5.1" class="ltx_text" style="font-size:90%;">Kasiviswanathan et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib266.7.1" class="ltx_text" style="font-size:90%;">
Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova,
and Adam D. Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib266.8.1" class="ltx_text" style="font-size:90%;">What can we learn privately?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib266.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIAM J. Comput.</em><span id="bib.bib266.10.2" class="ltx_text" style="font-size:90%;">, 40(3):793–826, 2011.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib266.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1137/090756090" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1137/090756090</a><span id="bib.bib266.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib267" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib267.5.5.1" class="ltx_text" style="font-size:90%;">Kearns et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib267.7.1" class="ltx_text" style="font-size:90%;">
Michael J. Kearns, Aaron Roth, Zhiwei Steven Wu, and Grigory Yaroslavtsev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib267.8.1" class="ltx_text" style="font-size:90%;">Privacy for the protected (only).
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib267.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib267.10.2" class="ltx_text" style="font-size:90%;">, abs/1506.00242, 2015.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib267.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1506.00242" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1506.00242</a><span id="bib.bib267.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib268" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib268.5.5.1" class="ltx_text" style="font-size:90%;">Khaled et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib268.7.1" class="ltx_text" style="font-size:90%;">
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib268.8.1" class="ltx_text" style="font-size:90%;">First analysis of local GD on heterogeneous data,
2019a.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib268.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1909.04715" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1909.04715</a><span id="bib.bib268.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib269" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib269.5.5.1" class="ltx_text" style="font-size:90%;">Khaled et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib269.7.1" class="ltx_text" style="font-size:90%;">
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib269.8.1" class="ltx_text" style="font-size:90%;">Better communication complexity for local SGD, 2019b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib269.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1909.04746" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1909.04746</a><span id="bib.bib269.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib270" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib270.5.5.1" class="ltx_text" style="font-size:90%;">Khodak et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib270.7.1" class="ltx_text" style="font-size:90%;">
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib270.8.1" class="ltx_text" style="font-size:90%;">Adaptive gradient-based meta-learning methods.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib270.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib270.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib270.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib271" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib271.4.4.1" class="ltx_text" style="font-size:90%;">Kifer and Machanavajjhala [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib271.6.1" class="ltx_text" style="font-size:90%;">
Daniel Kifer and Ashwin Machanavajjhala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib271.7.1" class="ltx_text" style="font-size:90%;">Pufferfish: A framework for mathematical privacy definitions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib271.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Database Systems</em><span id="bib.bib271.9.2" class="ltx_text" style="font-size:90%;">, 39(1):3:1–3:36, 2014.
</span>
</span>
</li>
<li id="bib.bib272" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib272.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib272.7.1" class="ltx_text" style="font-size:90%;">
Yejin Kim, Jimeng Sun, Hwanjo Yu, and Xiaoqian Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib272.8.1" class="ltx_text" style="font-size:90%;">Federated tensor factorization for computational phenotyping.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib272.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib272.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada,
August 13 - 17, 2017</em><span id="bib.bib272.11.3" class="ltx_text" style="font-size:90%;">, pages 887–895, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib272.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/3097983.3098118</span><span id="bib.bib272.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib272.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1145/3097983.3098118" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1145/3097983.3098118</a><span id="bib.bib272.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib273" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib273.5.5.1" class="ltx_text" style="font-size:90%;">King et al. [1995]</span></span>
<span class="ltx_bibblock"><span id="bib.bib273.7.1" class="ltx_text" style="font-size:90%;">
Ross D. King, Cao Feng, and Alistair Sutherland.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib273.8.1" class="ltx_text" style="font-size:90%;">StatLog: comparison of classification algorithms on large
real-world problems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib273.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Applied Artificial Intelligence an International Journal</em><span id="bib.bib273.10.2" class="ltx_text" style="font-size:90%;">,
9(3):289–333, 1995.
</span>
</span>
</li>
<li id="bib.bib274" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib274.5.5.1" class="ltx_text" style="font-size:90%;">Koeberl et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib274.7.1" class="ltx_text" style="font-size:90%;">
Patrick Koeberl, Steffen Schulz, Ahmad-Reza Sadeghi, and Vijay Varadharajan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib274.8.1" class="ltx_text" style="font-size:90%;">TrustLite: a security architecture for tiny embedded devices.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib274.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib274.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EuroSys</em><span id="bib.bib274.11.3" class="ltx_text" style="font-size:90%;">, pages 10:1–10:14. ACM, 2014.
</span>
</span>
</li>
<li id="bib.bib275" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib275.4.4.1" class="ltx_text" style="font-size:90%;">Koh and Liang [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib275.6.1" class="ltx_text" style="font-size:90%;">
Pang Wei Koh and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib275.7.1" class="ltx_text" style="font-size:90%;">Understanding black-box predictions via influence functions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib275.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib275.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em><span id="bib.bib275.10.3" class="ltx_text" style="font-size:90%;">, pages 1885–1894. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib276" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib276.5.5.1" class="ltx_text" style="font-size:90%;">Koh et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib276.7.1" class="ltx_text" style="font-size:90%;">
Pang Wei Koh, Jacob Steinhardt, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib276.8.1" class="ltx_text" style="font-size:90%;">Stronger data poisoning attacks break data sanitization defenses.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib276.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.00741</em><span id="bib.bib276.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib277" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib277.4.4.1" class="ltx_text" style="font-size:90%;">Kohavi and John [1995]</span></span>
<span class="ltx_bibblock"><span id="bib.bib277.6.1" class="ltx_text" style="font-size:90%;">
Ron Kohavi and George H John.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib277.7.1" class="ltx_text" style="font-size:90%;">Automatic parameter selection by minimizing estimated error.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib277.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib277.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Machine Learning Proceedings 1995</em><span id="bib.bib277.10.3" class="ltx_text" style="font-size:90%;">, pages 304–312.
Elsevier, 1995.
</span>
</span>
</li>
<li id="bib.bib278" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib278.5.5.1" class="ltx_text" style="font-size:90%;">Koloskova et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib278.7.1" class="ltx_text" style="font-size:90%;">
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib278.8.1" class="ltx_text" style="font-size:90%;">Decentralized Stochastic Optimization and Gossip Algorithms with
Compressed Communication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib278.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib278.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib278.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib279" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib279.5.5.1" class="ltx_text" style="font-size:90%;">Koloskova et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib279.7.1" class="ltx_text" style="font-size:90%;">
Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib279.8.1" class="ltx_text" style="font-size:90%;">Decentralized deep learning with arbitrary communication compression.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib279.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</em><span id="bib.bib279.10.2" class="ltx_text" style="font-size:90%;">,
2020a.
</span>
</span>
</li>
<li id="bib.bib280" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib280.5.5.1" class="ltx_text" style="font-size:90%;">Koloskova et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib280.7.1" class="ltx_text" style="font-size:90%;">
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
Sebastian U. Stich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib280.8.1" class="ltx_text" style="font-size:90%;">A Unified Theory of Decentralized SGD with Changing Topology and
Local Updates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib280.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib280.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib280.11.3" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib281" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib281.4.4.1" class="ltx_text" style="font-size:90%;">Konečný and Richtárik [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib281.6.1" class="ltx_text" style="font-size:90%;">
Jakub Konečný and Peter Richtárik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib281.7.1" class="ltx_text" style="font-size:90%;">Randomized distributed mean estimation: Accuracy vs communication.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib281.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Frontiers in Applied Mathematics and Statistics</em><span id="bib.bib281.9.2" class="ltx_text" style="font-size:90%;">, 4:62, 2018.
</span>
</span>
</li>
<li id="bib.bib282" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib282.5.5.1" class="ltx_text" style="font-size:90%;">Konečný et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib282.7.1" class="ltx_text" style="font-size:90%;">
Jakub Konečný, H Brendan McMahan, Felix X. Yu, Peter Richtárik,
Ananda Theertha Suresh, and Dave Bacon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib282.8.1" class="ltx_text" style="font-size:90%;">Federated learning: Strategies for improving communication
efficiency.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib282.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1610.05492</em><span id="bib.bib282.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib283" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib283.5.5.1" class="ltx_text" style="font-size:90%;">Kuppam et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib283.7.1" class="ltx_text" style="font-size:90%;">
Satya Kuppam, Ryan McKenna, David Pujol, Michael Hay, Ashwin Machanavajjhala,
and Gerome Miklau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib283.8.1" class="ltx_text" style="font-size:90%;">Fair decision making using privacy-protected data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib283.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib283.10.2" class="ltx_text" style="font-size:90%;">, abs/1905.12744, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib283.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1905.12744" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1905.12744</a><span id="bib.bib283.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib284" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib284.5.5.1" class="ltx_text" style="font-size:90%;">Kurakin et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib284.7.1" class="ltx_text" style="font-size:90%;">
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib284.8.1" class="ltx_text" style="font-size:90%;">Adversarial machine learning at scale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib284.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1611.01236</em><span id="bib.bib284.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib285" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib285.4.4.1" class="ltx_text" style="font-size:90%;">Kushilevitz and Nisan [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib285.6.1" class="ltx_text" style="font-size:90%;">
Eyal Kushilevitz and Noam Nisan.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib285.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Communication Complexity</em><span id="bib.bib285.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib285.9.1" class="ltx_text" style="font-size:90%;">Cambridge University Press, New York, NY, USA, 1997.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib285.10.1" class="ltx_text" style="font-size:90%;">ISBN 0-521-56067-5.
</span>
</span>
</li>
<li id="bib.bib286" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib286.4.4.1" class="ltx_text" style="font-size:90%;">Kushilevitz and Ostrovsky [1997]</span></span>
<span class="ltx_bibblock"><span id="bib.bib286.6.1" class="ltx_text" style="font-size:90%;">
Eyal Kushilevitz and Rafail Ostrovsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib286.7.1" class="ltx_text" style="font-size:90%;">Replication is not needed: Single database, computationally-private
information retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib286.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib286.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">In Proc. of the 38th Annu. IEEE Symp. on Foundations of
Computer Science</em><span id="bib.bib286.10.3" class="ltx_text" style="font-size:90%;">, pages 364–373, 1997.
</span>
</span>
</li>
<li id="bib.bib287" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib287.5.5.1" class="ltx_text" style="font-size:90%;">Kusner et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib287.7.1" class="ltx_text" style="font-size:90%;">
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib287.8.1" class="ltx_text" style="font-size:90%;">Counterfactual fairness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib287.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib287.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib287.11.3" class="ltx_text" style="font-size:90%;">, pages
4066–4076, 2017.
</span>
</span>
</li>
<li id="bib.bib288" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib288.5.5.1" class="ltx_text" style="font-size:90%;">Kwon et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib288.7.1" class="ltx_text" style="font-size:90%;">
Albert Kwon, David Lazar, Srinivas Devadas, and Bryan Ford.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib288.8.1" class="ltx_text" style="font-size:90%;">Riffle.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib288.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings on Privacy Enhancing Technologies</em><span id="bib.bib288.10.2" class="ltx_text" style="font-size:90%;">, 2016(2):115–134, 2016.
</span>
</span>
</li>
<li id="bib.bib289" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib289.5.5.1" class="ltx_text" style="font-size:90%;">Laguel et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib289.7.1" class="ltx_text" style="font-size:90%;">
Yassine Laguel, Krishna Pillutla, Jérôme Malick, and Zaid Harchaoui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib289.8.1" class="ltx_text" style="font-size:90%;">Device Heterogeneity in Federated Learning: A Superquantile
Approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib289.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.11223</em><span id="bib.bib289.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib290" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib290.5.5.1" class="ltx_text" style="font-size:90%;">Lake et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib290.7.1" class="ltx_text" style="font-size:90%;">
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib290.8.1" class="ltx_text" style="font-size:90%;">One shot learning of simple visual concepts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib290.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib290.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Conference of the Cognitive Science
Society (CogSci)</em><span id="bib.bib290.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib291" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib291.5.5.1" class="ltx_text" style="font-size:90%;">Lalitha et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib291.7.1" class="ltx_text" style="font-size:90%;">
Anusha Lalitha, Osman Cihan Kilinc, Tara Javidi, and Farinaz Koushanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib291.8.1" class="ltx_text" style="font-size:90%;">Peer-to-peer Federated Learning on Graphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib291.9.1" class="ltx_text" style="font-size:90%;">Technical report, arXiv:1901.11173, 2019a.
</span>
</span>
</li>
<li id="bib.bib292" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib292.5.5.1" class="ltx_text" style="font-size:90%;">Lalitha et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib292.7.1" class="ltx_text" style="font-size:90%;">
Anusha Lalitha, Xinghan Wang, Osman Kilinc, Yongxi Lu, Tara Javidi, and Farinaz
Koushanfar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib292.8.1" class="ltx_text" style="font-size:90%;">Decentralized Bayesian learning over graphs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib292.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint: 1905.10466</em><span id="bib.bib292.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib293" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib293.5.5.1" class="ltx_text" style="font-size:90%;">Lamport et al. [1982]</span></span>
<span class="ltx_bibblock"><span id="bib.bib293.7.1" class="ltx_text" style="font-size:90%;">
Leslie Lamport, Robert Shostak, and Marshall Pease.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib293.8.1" class="ltx_text" style="font-size:90%;">The Byzantine generals problem.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib293.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Programming Languages and Systems
(TOPLAS)</em><span id="bib.bib293.10.2" class="ltx_text" style="font-size:90%;">, 4(3):382–401, 1982.
</span>
</span>
</li>
<li id="bib.bib294" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib294.4.4.1" class="ltx_text" style="font-size:90%;">Lan [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib294.6.1" class="ltx_text" style="font-size:90%;">
Guanghui Lan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib294.7.1" class="ltx_text" style="font-size:90%;">An optimal method for stochastic composite optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib294.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Mathematical Programming</em><span id="bib.bib294.9.2" class="ltx_text" style="font-size:90%;">, 133(1):365–397,
Jun 2012.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib294.10.1" class="ltx_text" style="font-size:90%;">ISSN 1436-4646.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib294.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1007/s10107-010-0434-y</span><span id="bib.bib294.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib294.13.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1007/s10107-010-0434-y" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1007/s10107-010-0434-y</a><span id="bib.bib294.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib295" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib295.5.5.1" class="ltx_text" style="font-size:90%;">Lapets et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib295.7.1" class="ltx_text" style="font-size:90%;">
Andrei Lapets, Nikolaj Volgushev, Azer Bestavros, Frederick Jansen, and Mayank
Varia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib295.8.1" class="ltx_text" style="font-size:90%;">Secure MPC for analytics as a web application.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib295.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib295.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SecDev</em><span id="bib.bib295.11.3" class="ltx_text" style="font-size:90%;">, pages 73–74. IEEE Computer Society, 2016.
</span>
</span>
</li>
<li id="bib.bib296" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib296.5.5.1" class="ltx_text" style="font-size:90%;">Lécuyer et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib296.7.1" class="ltx_text" style="font-size:90%;">
Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and
Suman Jana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib296.8.1" class="ltx_text" style="font-size:90%;">Certified robustness to adversarial examples with differential
privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib296.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib296.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE Symposium on Security and Privacy, SP 2019,
San Francisco, CA, USA, May 19-23, 2019</em><span id="bib.bib296.11.3" class="ltx_text" style="font-size:90%;">, pages 656–672, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib296.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/SP.2019.00044</span><span id="bib.bib296.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib296.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1109/SP.2019.00044" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1109/SP.2019.00044</a><span id="bib.bib296.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib297" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib297.5.5.1" class="ltx_text" style="font-size:90%;">Lepoint et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib297.7.1" class="ltx_text" style="font-size:90%;">
Tancrède Lepoint, Sarvar Patel, Mariana Raykova, Karn Seth, and Ni Trieu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib297.8.1" class="ltx_text" style="font-size:90%;">Private join and compute from PIR with default.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib297.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptol. ePrint Arch.</em><span id="bib.bib297.10.2" class="ltx_text" style="font-size:90%;">, 2020:1011, 2020.
</span>
</span>
</li>
<li id="bib.bib298" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib298.5.5.1" class="ltx_text" style="font-size:90%;">Leroy et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib298.7.1" class="ltx_text" style="font-size:90%;">
David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht, and Joseph
Dureau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib298.8.1" class="ltx_text" style="font-size:90%;">Federated learning for keyword spotting.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib298.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.05512</em><span id="bib.bib298.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib299" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib299.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib299.7.1" class="ltx_text" style="font-size:90%;">
Jeffrey Li, Mikhail Khodak, Sebastian Caldas, and Ameet Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib299.8.1" class="ltx_text" style="font-size:90%;">Differentially private meta-learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib299.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.05830</em><span id="bib.bib299.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib300" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib300.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib300.7.1" class="ltx_text" style="font-size:90%;">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib300.8.1" class="ltx_text" style="font-size:90%;">Federated optimization in heterogeneous networks, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib300.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1812.06127" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1812.06127</a><span id="bib.bib300.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib301" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib301.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib301.7.1" class="ltx_text" style="font-size:90%;">
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib301.8.1" class="ltx_text" style="font-size:90%;">Federated learning: Challenges, methods, and future directions,
2019b.
</span>
</span>
</li>
<li id="bib.bib302" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib302.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib302.7.1" class="ltx_text" style="font-size:90%;">
Tian Li, Maziar Sanjabi, and Virginia Smith.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib302.8.1" class="ltx_text" style="font-size:90%;">Fair resource allocation in federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib302.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10497</em><span id="bib.bib302.10.2" class="ltx_text" style="font-size:90%;">, 2019c.
</span>
</span>
</li>
<li id="bib.bib303" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib303.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib303.7.1" class="ltx_text" style="font-size:90%;">
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib303.8.1" class="ltx_text" style="font-size:90%;">On the convergence of FedAvg on non-IID data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib303.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.02189</em><span id="bib.bib303.10.2" class="ltx_text" style="font-size:90%;">, 2019d.
</span>
</span>
</li>
<li id="bib.bib304" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib304.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2019e]</span></span>
<span class="ltx_bibblock"><span id="bib.bib304.7.1" class="ltx_text" style="font-size:90%;">
Xiang Li, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib304.8.1" class="ltx_text" style="font-size:90%;">Communication efficient decentralized training with multiple local
updates.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib304.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.09126</em><span id="bib.bib304.10.2" class="ltx_text" style="font-size:90%;">, 2019e.
</span>
</span>
</li>
<li id="bib.bib305" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib305.5.5.1" class="ltx_text" style="font-size:90%;">Lian et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib305.7.1" class="ltx_text" style="font-size:90%;">
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib305.8.1" class="ltx_text" style="font-size:90%;">Can Decentralized Algorithms Outperform Centralized Algorithms? A
Case Study for Decentralized Parallel Stochastic Gradient Descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib305.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib305.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib305.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib306" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib306.5.5.1" class="ltx_text" style="font-size:90%;">Lian et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib306.7.1" class="ltx_text" style="font-size:90%;">
Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib306.8.1" class="ltx_text" style="font-size:90%;">Asynchronous Decentralized Parallel Stochastic Gradient Descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib306.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib306.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib306.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib307" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib307.3.3.1" class="ltx_text" style="font-size:90%;">[307]</span></span>
<span class="ltx_bibblock"><span id="bib.bib307.5.1" class="ltx_text" style="font-size:90%;">
libsnark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib307.6.1" class="ltx_text" style="font-size:90%;">libsnark: a c++ library for zkSNARK proofs.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/scipr-lab/libsnark" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/scipr-lab/libsnark</a><span id="bib.bib307.7.1" class="ltx_text" style="font-size:90%;">, December 2019.
</span>
</span>
</li>
<li id="bib.bib308" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib308.4.4.1" class="ltx_text" style="font-size:90%;">Lie and Maniatis [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib308.6.1" class="ltx_text" style="font-size:90%;">
David Lie and Petros Maniatis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib308.7.1" class="ltx_text" style="font-size:90%;">Glimmers: Resolving the privacy/trust quagmire.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib308.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib308.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 16th Workshop on Hot Topics in Operating
Systems</em><span id="bib.bib308.10.3" class="ltx_text" style="font-size:90%;">, pages 94–99. ACM, 2017.
</span>
</span>
</li>
<li id="bib.bib309" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib309.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib309.7.1" class="ltx_text" style="font-size:90%;">
Darryl Lin, Sachin Talathi, and Sreekanth Annapureddy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib309.8.1" class="ltx_text" style="font-size:90%;">Fixed point quantization of deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib309.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib309.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib309.11.3" class="ltx_text" style="font-size:90%;">, pages
2849–2858, 2016.
</span>
</span>
</li>
<li id="bib.bib310" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib310.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib310.7.1" class="ltx_text" style="font-size:90%;">
Tao Lin, Sebastian U Stich, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib310.8.1" class="ltx_text" style="font-size:90%;">Don’t use large mini-batches, use local SGD.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib310.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations (ICLR)</em><span id="bib.bib310.10.2" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib311" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib311.5.5.1" class="ltx_text" style="font-size:90%;">Lin et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib311.7.1" class="ltx_text" style="font-size:90%;">
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib311.8.1" class="ltx_text" style="font-size:90%;">Deep gradient compression: Reducing the communication bandwidth for
distributed training.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib311.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1712.01887</em><span id="bib.bib311.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib312" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib312.4.4.1" class="ltx_text" style="font-size:90%;">Little [1993]</span></span>
<span class="ltx_bibblock"><span id="bib.bib312.6.1" class="ltx_text" style="font-size:90%;">
R. J. A. Little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib312.7.1" class="ltx_text" style="font-size:90%;">Post-stratification: A modeler’s perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib312.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of the American Statistical Association</em><span id="bib.bib312.9.2" class="ltx_text" style="font-size:90%;">, 88(423):1001–1012, 1993.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib312.10.1" class="ltx_text" style="font-size:90%;">ISSN 01621459.
</span>
</span>
</li>
<li id="bib.bib313" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib313.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib313.7.1" class="ltx_text" style="font-size:90%;">
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib313.8.1" class="ltx_text" style="font-size:90%;">DARTS: Differentiable architecture search.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib313.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1806.09055</em><span id="bib.bib313.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib314" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib314.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib314.7.1" class="ltx_text" style="font-size:90%;">
Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib314.8.1" class="ltx_text" style="font-size:90%;">Fine-pruning: Defending against backdooring attacks on deep neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib314.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib314.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Symposium on Research in Attacks, Intrusions,
and Defenses</em><span id="bib.bib314.11.3" class="ltx_text" style="font-size:90%;">, pages 273–294. Springer, 2018b.
</span>
</span>
</li>
<li id="bib.bib315" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib315.4.4.1" class="ltx_text" style="font-size:90%;">Liu and Oh [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib315.6.1" class="ltx_text" style="font-size:90%;">
Xiyang Liu and Sewoong Oh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib315.7.1" class="ltx_text" style="font-size:90%;">Minimax rates of estimating approximate differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib315.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.10335</em><span id="bib.bib315.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib316" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib316.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib316.7.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Yan Kang, Xinwei Zhang, Liping Li, Yong Cheng, Tianjian Chen, Mingyi
Hong, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib316.8.1" class="ltx_text" style="font-size:90%;">A communication efficient vertical federated learning framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib316.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib316.10.2" class="ltx_text" style="font-size:90%;">, abs/1912.11187, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib316.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1912.11187" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1912.11187</a><span id="bib.bib316.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib317" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib317.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib317.7.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib317.8.1" class="ltx_text" style="font-size:90%;">A secure federated transfer learning framework.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib317.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Intelligent Systems</em><span id="bib.bib317.10.2" class="ltx_text" style="font-size:90%;">, 35(4):70–82,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib317.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/MIS.2020.2988525</span><span id="bib.bib317.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib318" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib318.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib318.7.1" class="ltx_text" style="font-size:90%;">
Yang Liu, Zhihao Yi, and Tianjian Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib318.8.1" class="ltx_text" style="font-size:90%;">Backdoor attacks and defenses in feature-partitioned collaborative
learning, 2020a.
</span>
</span>
</li>
<li id="bib.bib319" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib319.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2018c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib319.7.1" class="ltx_text" style="font-size:90%;">
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang,
and Xiangyu Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib319.8.1" class="ltx_text" style="font-size:90%;">Trojaning attack on neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib319.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib319.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">25th Annual Network and Distributed System Security
Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018</em><span id="bib.bib319.11.3" class="ltx_text" style="font-size:90%;">,
2018c.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib319.12.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-5_Liu_paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://wp.internetsociety.org/ndss/wp-content/uploads/sites/25/2018/02/ndss2018_03A-5_Liu_paper.pdf</a><span id="bib.bib319.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib320" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib320.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib320.7.1" class="ltx_text" style="font-size:90%;">
Yuhan Liu, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Michael
Riley.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib320.8.1" class="ltx_text" style="font-size:90%;">Learning discrete distributions: user vs item-level privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib320.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib320.10.2" class="ltx_text" style="font-size:90%;">, 33,
2020b.
</span>
</span>
</li>
<li id="bib.bib321" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib321.5.5.1" class="ltx_text" style="font-size:90%;">Ludwig et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib321.7.1" class="ltx_text" style="font-size:90%;">
Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank
Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib321.8.1" class="ltx_text" style="font-size:90%;">IBM federated learning: An enterprise framework white paper V0.1.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib321.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.10987</em><span id="bib.bib321.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib322" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib322.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib322.7.1" class="ltx_text" style="font-size:90%;">
Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, and
Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib322.8.1" class="ltx_text" style="font-size:90%;">Real-world image datasets for federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib322.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.11089</em><span id="bib.bib322.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib323" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib323.5.5.1" class="ltx_text" style="font-size:90%;">Luo et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib323.7.1" class="ltx_text" style="font-size:90%;">
Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib323.8.1" class="ltx_text" style="font-size:90%;">Neural architecture optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib323.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib323.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib323.11.3" class="ltx_text" style="font-size:90%;">, pages
7816–7827, 2018.
</span>
</span>
</li>
<li id="bib.bib324" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib324.5.5.1" class="ltx_text" style="font-size:90%;">Lyu et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib324.7.1" class="ltx_text" style="font-size:90%;">
Lingjuan Lyu, Jiangshan Yu, Karthik Nandakumar, Yitong Li, Xingjun Ma, Jiong
Jin, Han Yu, and Kee Siong Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib324.8.1" class="ltx_text" style="font-size:90%;">Towards fair and privacy-preserving federated deep models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib324.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Parallel and Distributed Systems</em><span id="bib.bib324.10.2" class="ltx_text" style="font-size:90%;">,
31(11):2524–2541, 2020.
</span>
</span>
</li>
<li id="bib.bib325" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib325.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib325.7.1" class="ltx_text" style="font-size:90%;">
Jing Ma, Qiuchen Zhang, Jian Lou, Joyce Ho, Li Xiong, and Xiaoqian Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib325.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving tensor factorization for collaborative health data
analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib325.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib325.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM CIKM</em><span id="bib.bib325.11.3" class="ltx_text" style="font-size:90%;">, volume 2, 2019a.
</span>
</span>
</li>
<li id="bib.bib326" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib326.5.5.1" class="ltx_text" style="font-size:90%;">Ma et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib326.7.1" class="ltx_text" style="font-size:90%;">
Yuzhe Ma, Xiaojin Zhu, and Justin Hsu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib326.8.1" class="ltx_text" style="font-size:90%;">Data poisoning against differentially-private learners: Attacks and
defenses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib326.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib326.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Joint Conference on Artificial Intelligence
(IJCAI), Macao, China</em><span id="bib.bib326.11.3" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib326.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1903.09860" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1903.09860</a><span id="bib.bib326.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib327" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib327.5.5.1" class="ltx_text" style="font-size:90%;">Madras et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib327.7.1" class="ltx_text" style="font-size:90%;">
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib327.8.1" class="ltx_text" style="font-size:90%;">Learning adversarially fair and transferable representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib327.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib327.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib327.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib328" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib328.5.5.1" class="ltx_text" style="font-size:90%;">Madry et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib328.7.1" class="ltx_text" style="font-size:90%;">
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib328.8.1" class="ltx_text" style="font-size:90%;">Towards deep learning models resistant to adversarial attacks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib328.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib328.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib329" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib329.5.5.1" class="ltx_text" style="font-size:90%;">Mansour et al. [2009a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib329.7.1" class="ltx_text" style="font-size:90%;">
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib329.8.1" class="ltx_text" style="font-size:90%;">Domain adaptation: Learning bounds and algorithms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib329.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:0902.3430</em><span id="bib.bib329.10.2" class="ltx_text" style="font-size:90%;">, 2009a.
</span>
</span>
</li>
<li id="bib.bib330" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib330.5.5.1" class="ltx_text" style="font-size:90%;">Mansour et al. [2009b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib330.7.1" class="ltx_text" style="font-size:90%;">
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib330.8.1" class="ltx_text" style="font-size:90%;">Domain adaptation with multiple sources.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib330.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib330.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib330.11.3" class="ltx_text" style="font-size:90%;">, pages
1041–1048, 2009b.
</span>
</span>
</li>
<li id="bib.bib331" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib331.5.5.1" class="ltx_text" style="font-size:90%;">Mansour et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib331.7.1" class="ltx_text" style="font-size:90%;">
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib331.8.1" class="ltx_text" style="font-size:90%;">Three approaches for personalization with applications to federated
learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib331.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.10619</em><span id="bib.bib331.10.2" class="ltx_text" style="font-size:90%;">, 2020a.
</span>
</span>
</li>
<li id="bib.bib332" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib332.5.5.1" class="ltx_text" style="font-size:90%;">Mansour et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib332.7.1" class="ltx_text" style="font-size:90%;">
Yishay Mansour, Mehryar Mohri, Ananda Theertha Suresh, and Ke Wu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib332.8.1" class="ltx_text" style="font-size:90%;">A theory of multiple-source adaptation with limited target labeled
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib332.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.09762</em><span id="bib.bib332.10.2" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib333" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib333.5.5.1" class="ltx_text" style="font-size:90%;">Martin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib333.7.1" class="ltx_text" style="font-size:90%;">
Alicia R Martin, Masahiro Kanai, Yoichiro Kamatani, Yukinori Okada, Benjamin M
Neale, and Mark J Daly.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib333.8.1" class="ltx_text" style="font-size:90%;">Current clinical use of polygenic scores will risk exacerbating
health disparities.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib333.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">BioRxiv</em><span id="bib.bib333.10.2" class="ltx_text" style="font-size:90%;">, page 441261, 2019.
</span>
</span>
</li>
<li id="bib.bib334" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib334.4.4.1" class="ltx_text" style="font-size:90%;">McMahan and Ramage [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib334.6.1" class="ltx_text" style="font-size:90%;">
H Brendan McMahan and Daniel Ramage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib334.7.1" class="ltx_text" style="font-size:90%;">Federated learning: Collaborative machine learning without
centralized training data, April 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib334.8.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://ai.googleblog.com/2017/04/federated-learning-collaborative.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://ai.googleblog.com/2017/04/federated-learning-collaborative.html</a><span id="bib.bib334.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib334.10.1" class="ltx_text" style="font-size:90%;">Google AI Blog.
</span>
</span>
</li>
<li id="bib.bib335" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib335.4.4.1" class="ltx_text" style="font-size:90%;">McMahan and Streeter [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib335.6.1" class="ltx_text" style="font-size:90%;">
H Brendan McMahan and Matthew Streeter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib335.7.1" class="ltx_text" style="font-size:90%;">Adaptive bound optimization for online convex optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib335.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1002.4908</em><span id="bib.bib335.9.2" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib336" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib336.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. [1812]</span></span>
<span class="ltx_bibblock"><span id="bib.bib336.7.1" class="ltx_text" style="font-size:90%;">
H Brendan McMahan, Galen Andrew, Ulfar Erlingsson, Steve Chien, Ilya Mironov,
Nicolas Papernot, and Peter Kairouz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib336.8.1" class="ltx_text" style="font-size:90%;">A general approach to adding differential privacy to iterative
training procedures. dec 2018.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib336.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">URL https://arxiv. org/abs</em><span id="bib.bib336.10.2" class="ltx_text" style="font-size:90%;">, 1812.
</span>
</span>
</li>
<li id="bib.bib337" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib337.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. [2017 (original version on arxiv Feb. 2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib337.7.1" class="ltx_text" style="font-size:90%;">
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera
y Arcas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib337.8.1" class="ltx_text" style="font-size:90%;">Communication-efficient learning of deep networks from decentralized
data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib337.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib337.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em><span id="bib.bib337.11.3" class="ltx_text" style="font-size:90%;">, pages 1273–1282, 2017 (original
version on arxiv Feb. 2016).
</span>
</span>
</li>
<li id="bib.bib338" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib338.5.5.1" class="ltx_text" style="font-size:90%;">McMahan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib338.7.1" class="ltx_text" style="font-size:90%;">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib338.8.1" class="ltx_text" style="font-size:90%;">Learning differentially private recurrent language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib338.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib338.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations
(ICLR)</em><span id="bib.bib338.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib339" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib339.4.4.1" class="ltx_text" style="font-size:90%;">McSherry and Talwar [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib339.6.1" class="ltx_text" style="font-size:90%;">
Frank McSherry and Kunal Talwar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib339.7.1" class="ltx_text" style="font-size:90%;">Mechanism design via differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib339.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib339.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">FOCS</em><span id="bib.bib339.10.3" class="ltx_text" style="font-size:90%;">, pages 94–103, 2007.
</span>
</span>
</li>
<li id="bib.bib340" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib340.4.4.1" class="ltx_text" style="font-size:90%;">Mei and Zhu [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib340.6.1" class="ltx_text" style="font-size:90%;">
Shike Mei and Xiaojin Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib340.7.1" class="ltx_text" style="font-size:90%;">Using machine teaching to identify optimal training-set attacks on
machine learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib340.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib340.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Twenty-Ninth AAAI Conference on Artificial Intelligence</em><span id="bib.bib340.10.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
<li id="bib.bib341" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib341.5.5.1" class="ltx_text" style="font-size:90%;">Melis et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib341.7.1" class="ltx_text" style="font-size:90%;">
Luca Melis, Congzheng Song, Emiliano De Cristofaro, and Vitaly Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib341.8.1" class="ltx_text" style="font-size:90%;">Exploiting unintended feature leakage in collaborative learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib341.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1805.04049</em><span id="bib.bib341.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib342" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib342.5.5.1" class="ltx_text" style="font-size:90%;">Mhamdi et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib342.7.1" class="ltx_text" style="font-size:90%;">
El Mahdi El Mhamdi, Rachid Guerraoui, and Sébastien Rouault.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib342.8.1" class="ltx_text" style="font-size:90%;">The hidden vulnerability of distributed learning in Byzantium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib342.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib342.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib342.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib343" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib343.4.4.1" class="ltx_text" style="font-size:90%;">Micali [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib343.6.1" class="ltx_text" style="font-size:90%;">
Silvio Micali.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib343.7.1" class="ltx_text" style="font-size:90%;">Computationally sound proofs.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib343.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">SIAM J. Comput.</em><span id="bib.bib343.9.2" class="ltx_text" style="font-size:90%;">, 30(4):1253–1298, 2000.
</span>
</span>
</li>
<li id="bib.bib344" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib344.5.5.1" class="ltx_text" style="font-size:90%;">Mireshghallah et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib344.7.1" class="ltx_text" style="font-size:90%;">
Fatemehsadat Mireshghallah, Mohammadkazem Taram, , Praneeth Vepakomma, Abhishek
Singh, Ramesh Raskar, and Esmaeilzadeh Hadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib344.8.1" class="ltx_text" style="font-size:90%;">Privacy in deep learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib344.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.12254</em><span id="bib.bib344.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib345" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib345.4.4.1" class="ltx_text" style="font-size:90%;">Mironov [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib345.6.1" class="ltx_text" style="font-size:90%;">
Ilya Mironov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib345.7.1" class="ltx_text" style="font-size:90%;">On significance of the least significant bits for differential
privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib345.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib345.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2012 ACM conference on Computer and
communications security</em><span id="bib.bib345.10.3" class="ltx_text" style="font-size:90%;">, pages 650–661. ACM, 2012.
</span>
</span>
</li>
<li id="bib.bib346" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib346.4.4.1" class="ltx_text" style="font-size:90%;">Mironov [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib346.6.1" class="ltx_text" style="font-size:90%;">
Ilya Mironov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib346.7.1" class="ltx_text" style="font-size:90%;">Rényi differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib346.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib346.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE 30th Computer Security Foundations Symposium
(CSF)</em><span id="bib.bib346.10.3" class="ltx_text" style="font-size:90%;">, pages 263–275. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib347" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib347.5.5.1" class="ltx_text" style="font-size:90%;">Mironov et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib347.7.1" class="ltx_text" style="font-size:90%;">
Ilya Mironov, Omkant Pandey, Omer Reingold, and Salil Vadhan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib347.8.1" class="ltx_text" style="font-size:90%;">Computational differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib347.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib347.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Cryptology—CRYPTO</em><span id="bib.bib347.11.3" class="ltx_text" style="font-size:90%;">, pages 126–142, 2009.
</span>
</span>
</li>
<li id="bib.bib348" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib348.6.5.1" class="ltx_text" style="font-size:90%;">Mironov et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib348.8.1" class="ltx_text" style="font-size:90%;">
Ilya Mironov, Kunal Talwar, and Li Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib348.9.1" class="ltx_text" style="font-size:90%;">R</span><math id="bib.bib348.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib348.1.m1.1a"><mo mathsize="90%" id="bib.bib348.1.m1.1.1" xref="bib.bib348.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib348.1.m1.1b"><ci id="bib.bib348.1.m1.1.1.cmml" xref="bib.bib348.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib348.1.m1.1c">\backslash</annotation></semantics></math><span id="bib.bib348.10.2" class="ltx_text" style="font-size:90%;">’enyi differential privacy of the sampled Gaussian
mechanism.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib348.11.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.10530</em><span id="bib.bib348.12.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib349" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib349.5.5.1" class="ltx_text" style="font-size:90%;">Mitchell et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib349.7.1" class="ltx_text" style="font-size:90%;">
Shira Mitchell, Eric Potash, and Solon Barocas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib349.8.1" class="ltx_text" style="font-size:90%;">Prediction-based decisions and fairness: A catalogue of choices,
assumptions, and definitions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib349.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.07867</em><span id="bib.bib349.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib350" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib350.4.4.1" class="ltx_text" style="font-size:90%;">Mnih and Hinton [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib350.6.1" class="ltx_text" style="font-size:90%;">
Volodymyr Mnih and Geoffrey E Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib350.7.1" class="ltx_text" style="font-size:90%;">Learning to label aerial images from noisy data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib350.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib350.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 29th International conference on machine
learning (ICML-12)</em><span id="bib.bib350.10.3" class="ltx_text" style="font-size:90%;">, pages 567–574, 2012.
</span>
</span>
</li>
<li id="bib.bib351" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib351.4.4.1" class="ltx_text" style="font-size:90%;">Mohassel and Zhang [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib351.6.1" class="ltx_text" style="font-size:90%;">
Payman Mohassel and Yupeng Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib351.7.1" class="ltx_text" style="font-size:90%;">SecureML: A system for scalable privacy-preserving machine
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib351.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib351.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security and Privacy</em><span id="bib.bib351.10.3" class="ltx_text" style="font-size:90%;">, pages 19–38.
IEEE Computer Society, 2017.
</span>
</span>
</li>
<li id="bib.bib352" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib352.5.5.1" class="ltx_text" style="font-size:90%;">Mohri et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib352.7.1" class="ltx_text" style="font-size:90%;">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib352.8.1" class="ltx_text" style="font-size:90%;">Agnostic Federated Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib352.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib352.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib352.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib353" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib353.5.5.1" class="ltx_text" style="font-size:90%;">Moreno-Torres et al. [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib353.7.1" class="ltx_text" style="font-size:90%;">
Jose G. Moreno-Torres, Troy Raeder, RocíO Alaiz-RodríGuez, Nitesh V.
Chawla, and Francisco Herrera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib353.8.1" class="ltx_text" style="font-size:90%;">A unifying view on dataset shift in classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib353.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Pattern Recogn.</em><span id="bib.bib353.10.2" class="ltx_text" style="font-size:90%;">, 45(1), January 2012.
</span>
</span>
</li>
<li id="bib.bib354" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib354.4.4.1" class="ltx_text" style="font-size:90%;">Musketeer [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib354.6.1" class="ltx_text" style="font-size:90%;">
Musketeer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib354.7.1" class="ltx_text" style="font-size:90%;">Musketeer: About, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib354.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://musketeer.eu/project/" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://musketeer.eu/project/</a><span id="bib.bib354.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib354.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib355" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib355.5.5.1" class="ltx_text" style="font-size:90%;">Naim et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib355.7.1" class="ltx_text" style="font-size:90%;">
Carolina Naim, Fangwei Ye, and Salim El Rouayheb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib355.8.1" class="ltx_text" style="font-size:90%;">ON-OFF privacy with correlated requests.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib355.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib355.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE International Symposium on Information Theory
(ISIT)</em><span id="bib.bib355.11.3" class="ltx_text" style="font-size:90%;">, July 2019.
</span>
</span>
</li>
<li id="bib.bib356" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib356.5.5.1" class="ltx_text" style="font-size:90%;">Natarajan et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib356.7.1" class="ltx_text" style="font-size:90%;">
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib356.8.1" class="ltx_text" style="font-size:90%;">Learning with noisy labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib356.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib356.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib356.11.3" class="ltx_text" style="font-size:90%;">, pages
1196–1204, 2013.
</span>
</span>
</li>
<li id="bib.bib357" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib357.5.5.1" class="ltx_text" style="font-size:90%;">Neglia et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib357.7.1" class="ltx_text" style="font-size:90%;">
Giovanni Neglia, Chuan Xu, Don Towsley, and Gianmarco Calbi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib357.8.1" class="ltx_text" style="font-size:90%;">Decentralized gradient methods: does topology matter?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib357.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib357.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AISTATS</em><span id="bib.bib357.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib358" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib358.5.5.1" class="ltx_text" style="font-size:90%;">Nichol et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib358.7.1" class="ltx_text" style="font-size:90%;">
Alex Nichol, Joshua Achiam, and John Schulman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib358.8.1" class="ltx_text" style="font-size:90%;">On first-order meta-learning algorithms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib358.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.02999</em><span id="bib.bib358.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib359" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib359.5.5.1" class="ltx_text" style="font-size:90%;">Nikolaenko et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib359.7.1" class="ltx_text" style="font-size:90%;">
Valeria Nikolaenko, Udi Weinsberg, Stratis Ioannidis, Marc Joye, Dan Boneh, and
Nina Taft.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib359.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving ridge regression on hundreds of millions of
records.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib359.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib359.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Symposium on Security and Privacy</em><span id="bib.bib359.11.3" class="ltx_text" style="font-size:90%;">, pages 334–348.
IEEE Computer Society, 2013.
</span>
</span>
</li>
<li id="bib.bib360" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib360.5.5.1" class="ltx_text" style="font-size:90%;">Niu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib360.7.1" class="ltx_text" style="font-size:90%;">
Chaoyue Niu, Fan Wu, Shaojie Tang, Lifeng Hua, Rongfei Jia, Chengfei Lv, Zhihua
Wu, and Guihai Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib360.8.1" class="ltx_text" style="font-size:90%;">Secure federated submodel learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib360.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.02254</em><span id="bib.bib360.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib361" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib361.4.4.1" class="ltx_text" style="font-size:90%;">NSA [2012]</span></span>
<span class="ltx_bibblock"><span id="bib.bib361.6.1" class="ltx_text" style="font-size:90%;">
NSA.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib361.7.1" class="ltx_text" style="font-size:90%;">Defense in depth: A practical strategy for achieving Information
Assurance in today’s highly networked environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib361.8.1" class="ltx_text" style="font-size:90%;">Technical report, NSA, 2012.
</span>
</span>
</li>
<li id="bib.bib362" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib362.5.5.1" class="ltx_text" style="font-size:90%;">Oktay et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib362.7.1" class="ltx_text" style="font-size:90%;">
Deniz Oktay, Johannes Ballé, Saurabh Singh, and Abhinav Shrivastava.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib362.8.1" class="ltx_text" style="font-size:90%;">Model compression by entropy penalized reparameterization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib362.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1906.06624</em><span id="bib.bib362.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib363" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib363.4.4.1" class="ltx_text" style="font-size:90%;">Olumofin and Goldberg [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib363.6.1" class="ltx_text" style="font-size:90%;">
Femi Olumofin and Ian Goldberg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib363.7.1" class="ltx_text" style="font-size:90%;">Revisiting the computational practicality of private information
retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib363.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib363.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Financial Cryptography and Data
Security</em><span id="bib.bib363.10.3" class="ltx_text" style="font-size:90%;">, pages 158–172. Springer, 2011.
</span>
</span>
</li>
<li id="bib.bib364" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib364.3.3.1" class="ltx_text" style="font-size:90%;">[364]</span></span>
<span class="ltx_bibblock"><span id="bib.bib364.5.1" class="ltx_text" style="font-size:90%;">
Palisade.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib364.6.1" class="ltx_text" style="font-size:90%;">PALISADE lattice cryptography library.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://gitlab.com/palisade/palisade-release" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://gitlab.com/palisade/palisade-release</a><span id="bib.bib364.7.1" class="ltx_text" style="font-size:90%;">, October 2019.
</span>
</span>
</li>
<li id="bib.bib365" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib365.4.4.1" class="ltx_text" style="font-size:90%;">Pan and Yang [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib365.6.1" class="ltx_text" style="font-size:90%;">
Sinno Jialin Pan and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib365.7.1" class="ltx_text" style="font-size:90%;">A survey on transfer learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib365.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Knowledge and Data Engineering</em><span id="bib.bib365.9.2" class="ltx_text" style="font-size:90%;">,
22(10):1345–1359, 2010.
</span>
</span>
</li>
<li id="bib.bib366" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib366.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib366.7.1" class="ltx_text" style="font-size:90%;">
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik,
and Ananthram Swami.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib366.8.1" class="ltx_text" style="font-size:90%;">Practical black-box attacks against machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib366.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib366.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2017 ACM on Asia conference on computer
and communications security</em><span id="bib.bib366.11.3" class="ltx_text" style="font-size:90%;">, pages 506–519. ACM, 2017.
</span>
</span>
</li>
<li id="bib.bib367" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib367.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib367.7.1" class="ltx_text" style="font-size:90%;">
Nicolas Papernot, Abhradeep Thakurta, Shuang Song, Steve Chien, and Úlfar
Erlingsson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib367.8.1" class="ltx_text" style="font-size:90%;">Tempered sigmoid activations for deep learning with differential
privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib367.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2007.14191</em><span id="bib.bib367.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib368" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib368.5.5.1" class="ltx_text" style="font-size:90%;">Park et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib368.7.1" class="ltx_text" style="font-size:90%;">
Jihong Park, Sumudu Samarakoon, Mehdi Bennis, and Mérouane Debbah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib368.8.1" class="ltx_text" style="font-size:90%;">Wireless network intelligence at the edge.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib368.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib368.10.2" class="ltx_text" style="font-size:90%;">, abs/1812.02858, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib368.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1812.02858" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1812.02858</a><span id="bib.bib368.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib369" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib369.5.5.1" class="ltx_text" style="font-size:90%;">Parno et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib369.7.1" class="ltx_text" style="font-size:90%;">
Bryan Parno, Jon Howell, Craig Gentry, and Mariana Raykova.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib369.8.1" class="ltx_text" style="font-size:90%;">Pinocchio: nearly practical verifiable computation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib369.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Commun. ACM</em><span id="bib.bib369.10.2" class="ltx_text" style="font-size:90%;">, 59(2):103–112, 2016.
</span>
</span>
</li>
<li id="bib.bib370" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib370.5.5.1" class="ltx_text" style="font-size:90%;">Paszke et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib370.7.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib370.8.1" class="ltx_text" style="font-size:90%;">Pytorch: An imperative style, high-performance deep learning library.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib370.9.1" class="ltx_text" style="font-size:90%;">In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, </span><em id="bib.bib370.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural
Information Processing Systems 32</em><span id="bib.bib370.11.3" class="ltx_text" style="font-size:90%;">, pages 8024–8035. Curran Associates,
Inc., 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib370.12.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a><span id="bib.bib370.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib371" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib371.4.4.1" class="ltx_text" style="font-size:90%;">Patel and Dieuleveut [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib371.6.1" class="ltx_text" style="font-size:90%;">
Kumar Kshitij Patel and Aymeric Dieuleveut.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib371.7.1" class="ltx_text" style="font-size:90%;">Communication trade-offs for synchronized distributed SGD with
large step size.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib371.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib371.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib372" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib372.5.5.1" class="ltx_text" style="font-size:90%;">Patel et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib372.7.1" class="ltx_text" style="font-size:90%;">
Sarvar Patel, Giuseppe Persiano, and Kevin Yeo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib372.8.1" class="ltx_text" style="font-size:90%;">Private stateful information retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib372.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib372.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib372.11.3" class="ltx_text" style="font-size:90%;">, CCS ’18, pages 1002–1019, New York, NY, USA,
2018. ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib372.12.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-5693-0.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib372.13.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/3243734.3243821</span><span id="bib.bib372.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib372.15.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/3243734.3243821" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/3243734.3243821</a><span id="bib.bib372.16.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib373" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib373.5.5.1" class="ltx_text" style="font-size:90%;">Patrini et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib373.7.1" class="ltx_text" style="font-size:90%;">
Giorgio Patrini, Richard Nock, Stephen Hardy, and Tibério S. Caetano.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib373.8.1" class="ltx_text" style="font-size:90%;">Fast learning from distributed datasets without entity matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib373.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib373.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Twenty-Fifth International Joint
Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15
July 2016</em><span id="bib.bib373.11.3" class="ltx_text" style="font-size:90%;">, pages 1909–1917, 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib373.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://www.ijcai.org/Abstract/16/273" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://www.ijcai.org/Abstract/16/273</a><span id="bib.bib373.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib374" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib374.4.4.1" class="ltx_text" style="font-size:90%;">Pedregosa [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib374.6.1" class="ltx_text" style="font-size:90%;">
Fabian Pedregosa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib374.7.1" class="ltx_text" style="font-size:90%;">Hyperparameter optimization with approximate gradient.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib374.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1602.02355</em><span id="bib.bib374.9.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib375" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib375.5.5.1" class="ltx_text" style="font-size:90%;">Pham et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib375.7.1" class="ltx_text" style="font-size:90%;">
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib375.8.1" class="ltx_text" style="font-size:90%;">Efficient neural architecture search via parameter sharing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib375.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib375.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib375.11.3" class="ltx_text" style="font-size:90%;">, pages
4092–4101, 2018.
</span>
</span>
</li>
<li id="bib.bib376" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib376.4.4.1" class="ltx_text" style="font-size:90%;">Pichai [May 7, 2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib376.6.1" class="ltx_text" style="font-size:90%;">
Sundar Pichai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib376.7.1" class="ltx_text" style="font-size:90%;">Google’s Sundar Pichai: Privacy Should Not Be a Luxury Good.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib376.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">New York Times</em><span id="bib.bib376.9.2" class="ltx_text" style="font-size:90%;">, May 7, 2019.
</span>
</span>
</li>
<li id="bib.bib377" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib377.5.5.1" class="ltx_text" style="font-size:90%;">Pichapati et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib377.7.1" class="ltx_text" style="font-size:90%;">
Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X Yu, Sashank J Reddi,
and Sanjiv Kumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib377.8.1" class="ltx_text" style="font-size:90%;">AdaCliP: Adaptive clipping for private SGD.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib377.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.07643</em><span id="bib.bib377.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib378" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib378.5.5.1" class="ltx_text" style="font-size:90%;">Pihur et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib378.7.1" class="ltx_text" style="font-size:90%;">
Vasyl Pihur, Aleksandra Korolova, Frederick Liu, Subhash Sankuratripati, Moti
Yung, Dachuan Huang, and Ruogu Zeng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib378.8.1" class="ltx_text" style="font-size:90%;">Differentially-private “Draw and Discard” machine learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib378.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib378.10.2" class="ltx_text" style="font-size:90%;">, abs/1807.04369, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib378.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1807.04369" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1807.04369</a><span id="bib.bib378.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib379" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib379.5.5.1" class="ltx_text" style="font-size:90%;">Pillutla et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib379.7.1" class="ltx_text" style="font-size:90%;">
Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib379.8.1" class="ltx_text" style="font-size:90%;">Robust aggregation for federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib379.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1912.13445</em><span id="bib.bib379.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib380" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib380.5.5.1" class="ltx_text" style="font-size:90%;">Quionero-Candela et al. [2009]</span></span>
<span class="ltx_bibblock"><span id="bib.bib380.7.1" class="ltx_text" style="font-size:90%;">
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D.
Lawrence.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib380.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Dataset Shift in Machine Learning</em><span id="bib.bib380.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib380.10.1" class="ltx_text" style="font-size:90%;">The MIT Press, 2009.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib380.11.1" class="ltx_text" style="font-size:90%;">ISBN 0262170051, 9780262170055.
</span>
</span>
</li>
<li id="bib.bib381" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib381.5.5.1" class="ltx_text" style="font-size:90%;">Rajput et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib381.7.1" class="ltx_text" style="font-size:90%;">
Shashank Rajput, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib381.8.1" class="ltx_text" style="font-size:90%;">DETOX: A redundancy-based framework for faster and more robust
gradient aggregation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib381.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.12205</em><span id="bib.bib381.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib382" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib382.4.4.1" class="ltx_text" style="font-size:90%;">Ramage and Mazzocchi [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib382.6.1" class="ltx_text" style="font-size:90%;">
Daniel Ramage and Stefano Mazzocchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib382.7.1" class="ltx_text" style="font-size:90%;">Federated analytics: Collaborative data science without data
collection, May 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib382.8.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html</a><span id="bib.bib382.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib382.10.1" class="ltx_text" style="font-size:90%;">Google AI Blog.
</span>
</span>
</li>
<li id="bib.bib383" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib383.5.5.1" class="ltx_text" style="font-size:90%;">Ramaswamy et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib383.7.1" class="ltx_text" style="font-size:90%;">
Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and Françoise Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib383.8.1" class="ltx_text" style="font-size:90%;">Federated learning for emoji prediction in a mobile keyboard.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib383.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1906.04329</em><span id="bib.bib383.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib384" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib384.5.5.1" class="ltx_text" style="font-size:90%;">Ramaswamy et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib384.7.1" class="ltx_text" style="font-size:90%;">
Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan,
and Françoise Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib384.8.1" class="ltx_text" style="font-size:90%;">Training production language models without memorizing user data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib384.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2009.10031</em><span id="bib.bib384.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib385" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib385.4.4.1" class="ltx_text" style="font-size:90%;">Rastogi and Nath [2010]</span></span>
<span class="ltx_bibblock"><span id="bib.bib385.6.1" class="ltx_text" style="font-size:90%;">
Vibhor Rastogi and Suman Nath.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib385.7.1" class="ltx_text" style="font-size:90%;">Differentially private aggregation of distributed time-series with
transformation and encryption.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib385.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib385.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2010 ACM SIGMOD International Conference
on Management of Data</em><span id="bib.bib385.10.3" class="ltx_text" style="font-size:90%;">, SIGMOD ’10, pages 735–746, New York, NY, USA, 2010.
ACM.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib385.11.1" class="ltx_text" style="font-size:90%;">ISBN 978-1-4503-0032-2.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib385.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/1807167.1807247</span><span id="bib.bib385.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib385.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://doi.acm.org/10.1145/1807167.1807247" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://doi.acm.org/10.1145/1807167.1807247</a><span id="bib.bib385.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib386" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib386.4.4.1" class="ltx_text" style="font-size:90%;">Ravi and Larochelle [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib386.6.1" class="ltx_text" style="font-size:90%;">
Sachin Ravi and Hugo Larochelle.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib386.7.1" class="ltx_text" style="font-size:90%;">Optimization as a model for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib386.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib386.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 5th International Conference on Learning
Representations</em><span id="bib.bib386.10.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib387" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib387.5.5.1" class="ltx_text" style="font-size:90%;">Real et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib387.7.1" class="ltx_text" style="font-size:90%;">
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu,
Jie Tan, Quoc V Le, and Alexey Kurakin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib387.8.1" class="ltx_text" style="font-size:90%;">Large-scale evolution of image classifiers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib387.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib387.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em><span id="bib.bib387.11.3" class="ltx_text" style="font-size:90%;">, pages 2902–2911. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib388" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib388.5.5.1" class="ltx_text" style="font-size:90%;">Real et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib388.7.1" class="ltx_text" style="font-size:90%;">
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib388.8.1" class="ltx_text" style="font-size:90%;">Regularized evolution for image classifier architecture search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib388.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib388.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial
Intelligence</em><span id="bib.bib388.11.3" class="ltx_text" style="font-size:90%;">, volume 33, pages 4780–4789, 2019.
</span>
</span>
</li>
<li id="bib.bib389" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib389.5.5.1" class="ltx_text" style="font-size:90%;">Reddi et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib389.7.1" class="ltx_text" style="font-size:90%;">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib389.8.1" class="ltx_text" style="font-size:90%;">Adaptive federated optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib389.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2003.00295</em><span id="bib.bib389.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib390" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib390.5.5.1" class="ltx_text" style="font-size:90%;">Reisizadeh et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib390.7.1" class="ltx_text" style="font-size:90%;">
Amirhossein Reisizadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and
Ramtin Pedarsani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib390.8.1" class="ltx_text" style="font-size:90%;">Fedpaq: A communication-efficient federated learning method with
periodic averaging and quantization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib390.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.13014</em><span id="bib.bib390.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib391" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib391.5.5.1" class="ltx_text" style="font-size:90%;">Reisizadeh et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib391.7.1" class="ltx_text" style="font-size:90%;">
Amirhossein Reisizadeh, Hossein Taheri, Aryan Mokhtari, Hamed Hassani, and
Ramtin Pedarsani.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib391.8.1" class="ltx_text" style="font-size:90%;">Robust and communication-efficient collaborative learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib391.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1907.10595</em><span id="bib.bib391.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib392" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib392.5.5.1" class="ltx_text" style="font-size:90%;">Reyzin et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib392.7.1" class="ltx_text" style="font-size:90%;">
Leonid Reyzin, Adam D. Smith, and Sophia Yakoubov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib392.8.1" class="ltx_text" style="font-size:90%;">Turning HATE into LOVE: homomorphic ad hoc threshold encryption
for scalable MPC.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib392.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IACR Cryptology ePrint Archive</em><span id="bib.bib392.10.2" class="ltx_text" style="font-size:90%;">, 2018:997, 2018.
</span>
</span>
</li>
<li id="bib.bib393" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib393.5.5.1" class="ltx_text" style="font-size:90%;">Riazi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib393.7.1" class="ltx_text" style="font-size:90%;">
M Sadegh Riazi, Kim Laine, Blake Pelton, and Wei Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib393.8.1" class="ltx_text" style="font-size:90%;">HEAX: High-performance architecture for computation on
homomorphically encrypted data in the cloud.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib393.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.09731</em><span id="bib.bib393.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib394" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib394.5.5.1" class="ltx_text" style="font-size:90%;">Richardson et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib394.7.1" class="ltx_text" style="font-size:90%;">
Rashida Richardson, Jason Schultz, and Kate Crawford.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib394.8.1" class="ltx_text" style="font-size:90%;">Dirty data, bad predictions: How civil rights violations impact
police data, predictive policing systems, and justice.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib394.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">New York University Law Review Online, Forthcoming</em><span id="bib.bib394.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib395" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib395.4.4.1" class="ltx_text" style="font-size:90%;">Ripley [1993]</span></span>
<span class="ltx_bibblock"><span id="bib.bib395.6.1" class="ltx_text" style="font-size:90%;">
Brian D Ripley.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib395.7.1" class="ltx_text" style="font-size:90%;">Statistical aspects of neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib395.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Networks and chaos—statistical and probabilistic aspects</em><span id="bib.bib395.9.2" class="ltx_text" style="font-size:90%;">,
50:40–123, 1993.
</span>
</span>
</li>
<li id="bib.bib396" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib396.5.5.1" class="ltx_text" style="font-size:90%;">Rivest et al. [1978]</span></span>
<span class="ltx_bibblock"><span id="bib.bib396.7.1" class="ltx_text" style="font-size:90%;">
Ronald L Rivest, Len Adleman, and Michael L Dertouzos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib396.8.1" class="ltx_text" style="font-size:90%;">On data banks and privacy homomorphisms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib396.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations of Secure Computation, Academia Press</em><span id="bib.bib396.10.2" class="ltx_text" style="font-size:90%;">, pages
169–179, 1978.
</span>
</span>
</li>
<li id="bib.bib397" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib397.5.5.1" class="ltx_text" style="font-size:90%;">Rodríguez-Barroso et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib397.7.1" class="ltx_text" style="font-size:90%;">
Nuria Rodríguez-Barroso, Goran Stipcich, Daniel Jiménez-López,
José Antonio Ruiz-Millán, Eugenio Martínez-Cámara, Gerardo
González-Seco, M Victoria Luzón, Miguel Angel Veganzones, and
Francisco Herrera.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib397.8.1" class="ltx_text" style="font-size:90%;">Federated learning and differential privacy: Software tools analysis,
the sherpa. ai fl framework and methodological guidelines for preserving data
privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib397.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Fusion</em><span id="bib.bib397.10.2" class="ltx_text" style="font-size:90%;">, 64:270–292, 2020.
</span>
</span>
</li>
<li id="bib.bib398" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib398.5.5.1" class="ltx_text" style="font-size:90%;">Roth et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib398.7.1" class="ltx_text" style="font-size:90%;">
Edo Roth, Daniel Noble, Brett Hemenway Falk, and Andreas Haeberlen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib398.8.1" class="ltx_text" style="font-size:90%;">Honeycrisp: large-scale differentially private aggregation without a
trusted core.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib398.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib398.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">SOSP</em><span id="bib.bib398.11.3" class="ltx_text" style="font-size:90%;">, pages 196–210. ACM, 2019.
</span>
</span>
</li>
<li id="bib.bib399" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib399.5.5.1" class="ltx_text" style="font-size:90%;">Ryffel et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib399.7.1" class="ltx_text" style="font-size:90%;">
Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel
Rueckert, and Jonathan Passerat-Palmbach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib399.8.1" class="ltx_text" style="font-size:90%;">A generic framework for privacy preserving deep learning, 2018.
</span>
</span>
</li>
<li id="bib.bib400" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib400.5.5.1" class="ltx_text" style="font-size:90%;">Sabater et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib400.7.1" class="ltx_text" style="font-size:90%;">
César Sabater, Aurélien Bellet, and Jan Ramon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib400.8.1" class="ltx_text" style="font-size:90%;">Distributed Differentially Private Averaging with Improved Utility
and Robustness to Malicious Parties.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib400.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2006.07218</em><span id="bib.bib400.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib401" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib401.5.5.1" class="ltx_text" style="font-size:90%;">Salmon et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib401.7.1" class="ltx_text" style="font-size:90%;">
John K Salmon, Mark A Moraes, Ron O Dror, and David E Shaw.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib401.8.1" class="ltx_text" style="font-size:90%;">Parallel random numbers: As easy as 1, 2, 3.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib401.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib401.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of 2011 International Conference for High
Performance Computing, Networking, Storage and Analysis</em><span id="bib.bib401.11.3" class="ltx_text" style="font-size:90%;">, page 16. ACM, 2011.
</span>
</span>
</li>
<li id="bib.bib402" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib402.5.5.1" class="ltx_text" style="font-size:90%;">Samarakoon et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib402.7.1" class="ltx_text" style="font-size:90%;">
Sumudu Samarakoon, Mehdi Bennis, Walid Saad, and Mérouane Debbah.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib402.8.1" class="ltx_text" style="font-size:90%;">Federated learning for ultra-reliable low-latency V2V
communications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib402.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib402.10.2" class="ltx_text" style="font-size:90%;">, abs/1805.09253, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib402.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1805.09253" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1805.09253</a><span id="bib.bib402.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib403" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib403.7.5.1" class="ltx_text" style="font-size:90%;">Sambasivan et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib403.9.1" class="ltx_text" style="font-size:90%;">
Nithya Sambasivan, Garen Checkley, Amna Batool, Nova Ahmed, David Nemer,
Laura Sanely Gaytán-Lugo, Tara Matthews, Sunny Consolvo, and Elizabeth
Churchill.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib403.10.1" class="ltx_text" style="font-size:90%;">” privacy is not for me, it’s for those rich women”: Performative
privacy practices on mobile phones by women in south asia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib403.11.3" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib403.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Fourteenth Symposium on Usable Privacy and Security
(<math id="bib.bib403.1.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib403.1.1.m1.1a"><mo stretchy="false" id="bib.bib403.1.1.m1.1.1" xref="bib.bib403.1.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib403.1.1.m1.1b"><ci id="bib.bib403.1.1.m1.1.1.cmml" xref="bib.bib403.1.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib403.1.1.m1.1c">\{</annotation></semantics></math>SOUPS<math id="bib.bib403.2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib403.2.2.m2.1a"><mo stretchy="false" id="bib.bib403.2.2.m2.1.1" xref="bib.bib403.2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib403.2.2.m2.1b"><ci id="bib.bib403.2.2.m2.1.1.cmml" xref="bib.bib403.2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib403.2.2.m2.1c">\}</annotation></semantics></math> 2018)</em><span id="bib.bib403.12.4" class="ltx_text" style="font-size:90%;">, pages 127–142, 2018.
</span>
</span>
</li>
<li id="bib.bib404" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib404.5.5.1" class="ltx_text" style="font-size:90%;">Sathya et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib404.7.1" class="ltx_text" style="font-size:90%;">
Sai Sri Sathya, Praneeth Vepakomma, Ramesh Raskar, Ranjan Ramachandra, and
Santanu Bhattacharya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib404.8.1" class="ltx_text" style="font-size:90%;">A review of homomorphic encryption libraries for secure computation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib404.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.02428</em><span id="bib.bib404.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib405" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib405.5.5.1" class="ltx_text" style="font-size:90%;">Sattler et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib405.7.1" class="ltx_text" style="font-size:90%;">
Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib405.8.1" class="ltx_text" style="font-size:90%;">Robust and communication-efficient federated learning from non-IID
data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib405.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.02891</em><span id="bib.bib405.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib406" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib406.5.4.1" class="ltx_text" style="font-size:90%;">Schnell [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib406.7.1" class="ltx_text" style="font-size:90%;">
R. Schnell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib406.8.1" class="ltx_text" style="font-size:90%;">Efficient private record linkage of very large datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib406.9.2" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib406.1.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">59<sup id="bib.bib406.1.1.1" class="ltx_sup">th</sup> World Statistics Congress</em><span id="bib.bib406.10.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib407" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib407.5.5.1" class="ltx_text" style="font-size:90%;">Schnell et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib407.7.1" class="ltx_text" style="font-size:90%;">
R. Schnell, T. Bachteler, and J. Reiher.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib407.8.1" class="ltx_text" style="font-size:90%;">A novel error-tolerant anonymous linking code.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib407.9.1" class="ltx_text" style="font-size:90%;">Technical report, Paper No. WP-GRLC-2011-02, German Record Linkage
Center Working Paper Series, 2011.
</span>
</span>
</li>
<li id="bib.bib408" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib408.4.4.1" class="ltx_text" style="font-size:90%;">Schnorr [1990]</span></span>
<span class="ltx_bibblock"><span id="bib.bib408.6.1" class="ltx_text" style="font-size:90%;">
Claus P. Schnorr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib408.7.1" class="ltx_text" style="font-size:90%;">Efficient identification and signatures for smart cards.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib408.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib408.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Workshop on the Theory and Application of
Cryptographic Techniques on Advances in Cryptology</em><span id="bib.bib408.10.3" class="ltx_text" style="font-size:90%;">, EUROCRYPT ’89, 1990.
</span>
</span>
</li>
<li id="bib.bib409" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib409.3.3.1" class="ltx_text" style="font-size:90%;">[409]</span></span>
<span class="ltx_bibblock"><span id="bib.bib409.5.1" class="ltx_text" style="font-size:90%;">
SEAL.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib409.6.1" class="ltx_text" style="font-size:90%;">Microsoft SEAL (release 3.6).
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/Microsoft/SEAL" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/Microsoft/SEAL</a><span id="bib.bib409.7.1" class="ltx_text" style="font-size:90%;">, November 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib409.8.1" class="ltx_text" style="font-size:90%;">Microsoft Research, Redmond, WA.
</span>
</span>
</li>
<li id="bib.bib410" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib410.4.4.1" class="ltx_text" style="font-size:90%;">Seide and Agarwal [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib410.6.1" class="ltx_text" style="font-size:90%;">
Frank Seide and Amit Agarwal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib410.7.1" class="ltx_text" style="font-size:90%;">Cntk: Microsoft’s open-source deep-learning toolkit.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib410.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib410.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</em><span id="bib.bib410.10.3" class="ltx_text" style="font-size:90%;">, KDD ’16, page 2135, New York, NY,
USA, 2016. Association for Computing Machinery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib410.11.1" class="ltx_text" style="font-size:90%;">ISBN 9781450342322.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib410.12.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1145/2939672.2945397</span><span id="bib.bib410.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib410.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://doi.org/10.1145/2939672.2945397" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://doi.org/10.1145/2939672.2945397</a><span id="bib.bib410.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib411" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib411.5.5.1" class="ltx_text" style="font-size:90%;">Seshadri et al. [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib411.7.1" class="ltx_text" style="font-size:90%;">
Arvind Seshadri, Mark Luk, Adrian Perrig, Leendert van Doom, and Pradeep K.
Khosla.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib411.8.1" class="ltx_text" style="font-size:90%;">Pioneer: Verifying code integrity and enforcing untampered code
execution on legacy systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib411.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib411.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Malware Detection</em><span id="bib.bib411.11.3" class="ltx_text" style="font-size:90%;">, volume 27 of </span><em id="bib.bib411.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in
Information Security</em><span id="bib.bib411.13.5" class="ltx_text" style="font-size:90%;">, pages 253–289. Springer, 2007.
</span>
</span>
</li>
<li id="bib.bib412" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib412.5.5.1" class="ltx_text" style="font-size:90%;">Shafahi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib412.7.1" class="ltx_text" style="font-size:90%;">
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib412.8.1" class="ltx_text" style="font-size:90%;">Adversarial training for free.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib412.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib412.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib413" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib413.5.5.1" class="ltx_text" style="font-size:90%;">Sharma et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib413.7.1" class="ltx_text" style="font-size:90%;">
Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang, Jayashree
Kalpathy-Cramer, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib413.8.1" class="ltx_text" style="font-size:90%;">ExpertMatcher: Automating ML model selection for clients using
hidden representations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib413.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.03731</em><span id="bib.bib413.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib414" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib414.5.4.1" class="ltx_text" style="font-size:90%;">Sharma and Chen [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib414.7.1" class="ltx_text" style="font-size:90%;">
Yash Sharma and Pin-Yu Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib414.8.1" class="ltx_text" style="font-size:90%;">Attacking the Madry defense model with </span><math id="bib.bib414.1.m1.1" class="ltx_Math" alttext="l\_1" display="inline"><semantics id="bib.bib414.1.m1.1a"><mrow id="bib.bib414.1.m1.1.1" xref="bib.bib414.1.m1.1.1.cmml"><mi mathsize="90%" id="bib.bib414.1.m1.1.1.2" xref="bib.bib414.1.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="bib.bib414.1.m1.1.1.1" xref="bib.bib414.1.m1.1.1.1.cmml">​</mo><mi mathsize="90%" mathvariant="normal" id="bib.bib414.1.m1.1.1.3" xref="bib.bib414.1.m1.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="bib.bib414.1.m1.1.1.1a" xref="bib.bib414.1.m1.1.1.1.cmml">​</mo><mn mathsize="90%" id="bib.bib414.1.m1.1.1.4" xref="bib.bib414.1.m1.1.1.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="bib.bib414.1.m1.1b"><apply id="bib.bib414.1.m1.1.1.cmml" xref="bib.bib414.1.m1.1.1"><times id="bib.bib414.1.m1.1.1.1.cmml" xref="bib.bib414.1.m1.1.1.1"></times><ci id="bib.bib414.1.m1.1.1.2.cmml" xref="bib.bib414.1.m1.1.1.2">𝑙</ci><ci id="bib.bib414.1.m1.1.1.3.cmml" xref="bib.bib414.1.m1.1.1.3">_</ci><cn type="integer" id="bib.bib414.1.m1.1.1.4.cmml" xref="bib.bib414.1.m1.1.1.4">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib414.1.m1.1c">l\_1</annotation></semantics></math><span id="bib.bib414.9.2" class="ltx_text" style="font-size:90%;">-based adversarial
examples.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib414.10.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.10733</em><span id="bib.bib414.11.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib415" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib415.3.3.1" class="ltx_text" style="font-size:90%;">[415]</span></span>
<span class="ltx_bibblock"><span id="bib.bib415.5.1" class="ltx_text" style="font-size:90%;">
SHELL.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/google/shell-encryption" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/google/shell-encryption</a><span id="bib.bib415.6.1" class="ltx_text" style="font-size:90%;">, December 2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib415.7.1" class="ltx_text" style="font-size:90%;">Google.
</span>
</span>
</li>
<li id="bib.bib416" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib416.4.4.1" class="ltx_text" style="font-size:90%;">Shen and Sanghavi [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib416.6.1" class="ltx_text" style="font-size:90%;">
Yanyao Shen and Sujay Sanghavi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib416.7.1" class="ltx_text" style="font-size:90%;">Learning with bad training data via iterative trimmed loss
minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib416.8.1" class="ltx_text" style="font-size:90%;">In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
</span><em id="bib.bib416.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 36th International Conference on Machine Learning</em><span id="bib.bib416.10.3" class="ltx_text" style="font-size:90%;">,
volume 97 of </span><em id="bib.bib416.11.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning Research</em><span id="bib.bib416.12.5" class="ltx_text" style="font-size:90%;">, pages
5739–5748, Long Beach, California, USA, 09–15 Jun 2019. PMLR.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib416.13.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://proceedings.mlr.press/v97/shen19e.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://proceedings.mlr.press/v97/shen19e.html</a><span id="bib.bib416.14.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib417" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib417.5.5.1" class="ltx_text" style="font-size:90%;">Shi et al. [2011]</span></span>
<span class="ltx_bibblock"><span id="bib.bib417.7.1" class="ltx_text" style="font-size:90%;">
Elaine Shi, HTH Chan, Eleanor Rieffel, Richard Chow, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib417.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving aggregation of time-series data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib417.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib417.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Network &amp; Distributed System Security Symposium
(NDSS)</em><span id="bib.bib417.11.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib418" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib418.5.5.1" class="ltx_text" style="font-size:90%;">Shokri et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib418.7.1" class="ltx_text" style="font-size:90%;">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib418.8.1" class="ltx_text" style="font-size:90%;">Membership inference attacks against machine learning models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib418.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib418.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE Symposium on Security and Privacy (SP)</em><span id="bib.bib418.11.3" class="ltx_text" style="font-size:90%;">, pages
3–18. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib419" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib419.5.5.1" class="ltx_text" style="font-size:90%;">Shridhar et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib419.7.1" class="ltx_text" style="font-size:90%;">
Kumar Shridhar, Felix Laumann, and Marcus Liwicki.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib419.8.1" class="ltx_text" style="font-size:90%;">A comprehensive guide to Bayesian convolutional neural network with
variational inference.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib419.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint: 1901.02731</em><span id="bib.bib419.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib420" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib420.5.5.1" class="ltx_text" style="font-size:90%;">Silver et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib420.7.1" class="ltx_text" style="font-size:90%;">
Daniel L. Silver, Qiang Yang, and Lianghao Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib420.8.1" class="ltx_text" style="font-size:90%;">Lifelong machine learning systems: Beyond learning algorithms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib420.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib420.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI Spring Symposium Series</em><span id="bib.bib420.11.3" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib421" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib421.5.5.1" class="ltx_text" style="font-size:90%;">Singh et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib421.7.1" class="ltx_text" style="font-size:90%;">
Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib421.8.1" class="ltx_text" style="font-size:90%;">Detailed comparison of communication efficiency of split learning and
federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib421.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.09145</em><span id="bib.bib421.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib422" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib422.5.5.1" class="ltx_text" style="font-size:90%;">Singh et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib422.7.1" class="ltx_text" style="font-size:90%;">
Abhishek Singh, Ayush Chopra, Vivek Sharma, Ethan Garza, Emily Zhang, Praneeth
Vepakomma, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib422.8.1" class="ltx_text" style="font-size:90%;">DISCO: Dynamic and invariant sensitive channel obfuscation for deep
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib422.9.1" class="ltx_text" style="font-size:90%;">2020.
</span>
</span>
</li>
<li id="bib.bib423" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib423.4.4.1" class="ltx_text" style="font-size:90%;">Sion and Carbunar [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib423.6.1" class="ltx_text" style="font-size:90%;">
Radu Sion and Bogdan Carbunar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib423.7.1" class="ltx_text" style="font-size:90%;">On the computational practicality of private information retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib423.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib423.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the Network and Distributed Systems Security
Symposium</em><span id="bib.bib423.10.3" class="ltx_text" style="font-size:90%;">, pages 2006–06. Internet Society, 2007.
</span>
</span>
</li>
<li id="bib.bib424" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib424.5.5.1" class="ltx_text" style="font-size:90%;">Smith et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib424.7.1" class="ltx_text" style="font-size:90%;">
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S. Talwalkar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib424.8.1" class="ltx_text" style="font-size:90%;">Federated Multi-Task Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib424.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib424.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NIPS</em><span id="bib.bib424.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib425" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib425.5.5.1" class="ltx_text" style="font-size:90%;">Snell et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib425.7.1" class="ltx_text" style="font-size:90%;">
Jake Snell, Kevin Swersky, and Richard S. Zemel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib425.8.1" class="ltx_text" style="font-size:90%;">Prototypical networks for few-shot learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib425.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib425.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib425.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib426" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib426.5.5.1" class="ltx_text" style="font-size:90%;">Snoek et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib426.7.1" class="ltx_text" style="font-size:90%;">
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,
Narayanan Sundaram, Mostofa Patwary, Mr Prabhat, and Ryan Adams.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib426.8.1" class="ltx_text" style="font-size:90%;">Scalable Bayesian optimization using deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib426.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib426.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International conference on machine learning</em><span id="bib.bib426.11.3" class="ltx_text" style="font-size:90%;">, pages
2171–2180, 2015.
</span>
</span>
</li>
<li id="bib.bib427" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib427.5.5.1" class="ltx_text" style="font-size:90%;">So et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib427.7.1" class="ltx_text" style="font-size:90%;">
Jinhyun So, Basak Guler, and A. Salman Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib427.8.1" class="ltx_text" style="font-size:90%;">Byzantine-resilient secure federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib427.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Journal on Selected Areas in Communication, Series on
Machine Learning for Communications and Networks</em><span id="bib.bib427.10.2" class="ltx_text" style="font-size:90%;">, 2020a.
</span>
</span>
</li>
<li id="bib.bib428" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib428.5.5.1" class="ltx_text" style="font-size:90%;">So et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib428.7.1" class="ltx_text" style="font-size:90%;">
Jinhyun So, Basak Guler, and A Salman Avestimehr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib428.8.1" class="ltx_text" style="font-size:90%;">Turbo-aggregate: Breaking the quadratic aggregation barrier in secure
federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib428.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.04156</em><span id="bib.bib428.10.2" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib429" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib429.5.5.1" class="ltx_text" style="font-size:90%;">Song et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib429.7.1" class="ltx_text" style="font-size:90%;">
Liwei Song, Reza Shokri, and Prateek Mittal.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib429.8.1" class="ltx_text" style="font-size:90%;">Privacy risks of securing machine learning models against adversarial
examples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib429.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib429.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">In Proceedings of the ACM Conference on Computer and
Communication Security (CCS)</em><span id="bib.bib429.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib430" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib430.4.4.1" class="ltx_text" style="font-size:90%;">Srinathan and Rangan [2000]</span></span>
<span class="ltx_bibblock"><span id="bib.bib430.6.1" class="ltx_text" style="font-size:90%;">
K Srinathan and C Pandu Rangan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib430.7.1" class="ltx_text" style="font-size:90%;">Efficient asynchronous secure multiparty distributed computation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib430.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib430.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Cryptology in India</em><span id="bib.bib430.10.3" class="ltx_text" style="font-size:90%;">, pages
117–129. Springer, 2000.
</span>
</span>
</li>
<li id="bib.bib431" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib431.5.5.1" class="ltx_text" style="font-size:90%;">Srivastava et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib431.7.1" class="ltx_text" style="font-size:90%;">
Brij Mohan Lal Srivastava, Aurélien Bellet, Marc Tommasi, and Emmanuel
Vincent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib431.8.1" class="ltx_text" style="font-size:90%;">Privacy-Preserving Adversarial Representation Learning in
ASR: Reality or Illusion?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib431.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib431.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Annual Conference of the International Speech Communication
Association (Interspeech)</em><span id="bib.bib431.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib432" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib432.5.5.1" class="ltx_text" style="font-size:90%;">Steinhardt et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib432.7.1" class="ltx_text" style="font-size:90%;">
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib432.8.1" class="ltx_text" style="font-size:90%;">Certified defenses for data poisoning attacks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib432.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib432.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</em><span id="bib.bib432.11.3" class="ltx_text" style="font-size:90%;">, pages
3517–3529, 2017.
</span>
</span>
</li>
<li id="bib.bib433" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib433.4.4.1" class="ltx_text" style="font-size:90%;">Steinke and Ullman [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib433.6.1" class="ltx_text" style="font-size:90%;">
Thomas Steinke and Jonathan Ullman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib433.7.1" class="ltx_text" style="font-size:90%;">Tight lower bounds for differentially private selection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib433.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib433.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">FOCS</em><span id="bib.bib433.10.3" class="ltx_text" style="font-size:90%;">, pages 552–563, 2017.
</span>
</span>
</li>
<li id="bib.bib434" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib434.4.4.1" class="ltx_text" style="font-size:90%;">Stich [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib434.6.1" class="ltx_text" style="font-size:90%;">
Sebastian U Stich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib434.7.1" class="ltx_text" style="font-size:90%;">Local SGD converges fast and communicates little.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib434.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib434.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations
(ICLR)</em><span id="bib.bib434.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib435" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib435.4.4.1" class="ltx_text" style="font-size:90%;">Stich and Karimireddy [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib435.6.1" class="ltx_text" style="font-size:90%;">
Sebastian U Stich and Sai Praneeth Karimireddy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib435.7.1" class="ltx_text" style="font-size:90%;">The error-feedback framework: Better rates for SGD with delayed
gradients and compressed communication.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib435.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1909.05350</em><span id="bib.bib435.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib436" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib436.4.4.1" class="ltx_text" style="font-size:90%;">Su and Vaidya [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib436.6.1" class="ltx_text" style="font-size:90%;">
Lili Su and Nitin H. Vaidya.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib436.7.1" class="ltx_text" style="font-size:90%;">Fault-Tolerant Multi-Agent Optimization: Optimal Iterative
Distributed Algorithms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib436.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib436.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">PODC</em><span id="bib.bib436.10.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib437" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib437.5.5.1" class="ltx_text" style="font-size:90%;">Subramanyan et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib437.7.1" class="ltx_text" style="font-size:90%;">
Pramod Subramanyan, Rohit Sinha, Ilia Lebedev, Srinivas Devadas, and Sanjit A
Seshia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib437.8.1" class="ltx_text" style="font-size:90%;">A formal foundation for secure remote execution of enclaves.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib437.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib437.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security</em><span id="bib.bib437.11.3" class="ltx_text" style="font-size:90%;">, pages 2435–2450. ACM, 2017.
</span>
</span>
</li>
<li id="bib.bib438" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib438.5.5.1" class="ltx_text" style="font-size:90%;">Sun et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib438.7.1" class="ltx_text" style="font-size:90%;">
Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib438.8.1" class="ltx_text" style="font-size:90%;">Can you really backdoor federated learning?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib438.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.07963</em><span id="bib.bib438.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib439" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib439.4.4.1" class="ltx_text" style="font-size:90%;">support.google [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib439.6.1" class="ltx_text" style="font-size:90%;">
support.google.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib439.7.1" class="ltx_text" style="font-size:90%;">Your chats stay private while Messages improves suggestions, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib439.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://support.google.com/messages/answer/9327902" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://support.google.com/messages/answer/9327902</a><span id="bib.bib439.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib439.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib440" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib440.5.5.1" class="ltx_text" style="font-size:90%;">Suresh et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib440.7.1" class="ltx_text" style="font-size:90%;">
Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and H Brendan McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib440.8.1" class="ltx_text" style="font-size:90%;">Distributed mean estimation with limited communication.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib440.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib440.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 34th International Conference on Machine
Learning-Volume 70</em><span id="bib.bib440.11.3" class="ltx_text" style="font-size:90%;">, pages 3329–3337. JMLR. org, 2017.
</span>
</span>
</li>
<li id="bib.bib441" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib441.5.5.1" class="ltx_text" style="font-size:90%;">Szegedy et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib441.7.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
Ian Goodfellow, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib441.8.1" class="ltx_text" style="font-size:90%;">Intriguing properties of neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib441.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib441.10.2" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib442" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib442.5.5.1" class="ltx_text" style="font-size:90%;">Székely et al. [2007]</span></span>
<span class="ltx_bibblock"><span id="bib.bib442.7.1" class="ltx_text" style="font-size:90%;">
Gábor J Székely, Maria L Rizzo, Nail K Bakirov, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib442.8.1" class="ltx_text" style="font-size:90%;">Measuring and testing dependence by correlation of distances.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib442.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">The annals of statistics</em><span id="bib.bib442.10.2" class="ltx_text" style="font-size:90%;">, 35(6):2769–2794, 2007.
</span>
</span>
</li>
<li id="bib.bib443" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib443.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib443.7.1" class="ltx_text" style="font-size:90%;">
Hanlin Tang, Xiangru Lian, Ming Yan, Ce Zhang, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib443.8.1" class="ltx_text" style="font-size:90%;">D2: Decentralized training over decentralized data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib443.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib443.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib443.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib444" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib444.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib444.7.1" class="ltx_text" style="font-size:90%;">
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang, and
Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib444.8.1" class="ltx_text" style="font-size:90%;">DeepSqueeze: Parallel stochastic gradient descent with double-pass
error-compensated compression.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib444.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1907.07346</em><span id="bib.bib444.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib445" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib445.5.5.1" class="ltx_text" style="font-size:90%;">Tang et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib445.7.1" class="ltx_text" style="font-size:90%;">
Jun Tang, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and XiaoFeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib445.8.1" class="ltx_text" style="font-size:90%;">Privacy loss in Apple’s implementation of differential privacy on
MacOS 10.12.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib445.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib445.10.2" class="ltx_text" style="font-size:90%;">, abs/1709.02753, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib445.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1709.02753" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1709.02753</a><span id="bib.bib445.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib446" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib446.5.5.1" class="ltx_text" style="font-size:90%;">Thakkar et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib446.7.1" class="ltx_text" style="font-size:90%;">
Om Thakkar, Galen Andrew, and H Brendan McMahan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib446.8.1" class="ltx_text" style="font-size:90%;">Differentially private learning with adaptive clipping.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib446.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.03871</em><span id="bib.bib446.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib447" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib447.4.4.1" class="ltx_text" style="font-size:90%;">Tramèr and Boneh [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib447.6.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr and Dan Boneh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib447.7.1" class="ltx_text" style="font-size:90%;">Slalom: Fast, verifiable and private execution of neural networks in
trusted hardware.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib447.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib447.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib447.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib447.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://openreview.net/forum?id=rJVorjCcKQ" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://openreview.net/forum?id=rJVorjCcKQ</a><span id="bib.bib447.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib448" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib448.4.4.1" class="ltx_text" style="font-size:90%;">Tramèr and Boneh [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib448.6.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr and Dan Boneh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib448.7.1" class="ltx_text" style="font-size:90%;">Adversarial training and robustness for multiple perturbations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib448.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1904.13000</em><span id="bib.bib448.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib449" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib449.4.4.1" class="ltx_text" style="font-size:90%;">Tramèr and Boneh [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib449.6.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr and Dan Boneh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib449.7.1" class="ltx_text" style="font-size:90%;">Differentially private learning needs better features (or much more
data).
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib449.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2011.11660</em><span id="bib.bib449.9.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib450" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib450.5.5.1" class="ltx_text" style="font-size:90%;">Tramèr et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib450.7.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas
Ristenpart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib450.8.1" class="ltx_text" style="font-size:90%;">Stealing machine learning models via prediction APIs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib450.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib450.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">25th USENIX Security Symposium, USENIX Security 16,
Austin, TX, USA, August 10-12, 2016.</em><span id="bib.bib450.11.3" class="ltx_text" style="font-size:90%;">, pages 601–618, 2016.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib450.12.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/tramer</a><span id="bib.bib450.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib451" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib451.5.5.1" class="ltx_text" style="font-size:90%;">Tramèr et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib451.7.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr, Fan Zhang, Huang Lin, Jean-Pierre Hubaux, Ari Juels,
and Elaine Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib451.8.1" class="ltx_text" style="font-size:90%;">Sealed-glass proofs: Using transparent enclaves to prove and sell
knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib451.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib451.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2017 IEEE European Symposium on Security and Privacy,
EuroS&amp;P 2017, Paris, France, April 26-28, 2017</em><span id="bib.bib451.11.3" class="ltx_text" style="font-size:90%;">, pages 19–34, 2017.
</span>
</span>
</li>
<li id="bib.bib452" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib452.5.5.1" class="ltx_text" style="font-size:90%;">Tramèr et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib452.7.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan
Boneh, and Patrick D. McDaniel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib452.8.1" class="ltx_text" style="font-size:90%;">Ensemble adversarial training: Attacks and defenses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib452.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib452.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings</em><span id="bib.bib452.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib453" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib453.5.5.1" class="ltx_text" style="font-size:90%;">Tramèr et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib453.7.1" class="ltx_text" style="font-size:90%;">
Florian Tramèr, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and
Jörn-Henrik Jacobsen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib453.8.1" class="ltx_text" style="font-size:90%;">Fundamental tradeoffs between invariance and sensitivity to
adversarial perturbations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib453.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib453.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 37th International Conference on Machine
Learning, ICML 2020, 13-18 July 2020, Virtual Event</em><span id="bib.bib453.11.3" class="ltx_text" style="font-size:90%;">, volume 119 of
</span><em id="bib.bib453.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of Machine Learning Research</em><span id="bib.bib453.13.5" class="ltx_text" style="font-size:90%;">, pages 9561–9571. PMLR,
2020.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib453.14.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://proceedings.mlr.press/v119/tramer20a.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://proceedings.mlr.press/v119/tramer20a.html</a><span id="bib.bib453.15.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib454" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib454.5.5.1" class="ltx_text" style="font-size:90%;">Tran et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib454.7.1" class="ltx_text" style="font-size:90%;">
Brandon Tran, Jerry Li, and Aleksander Madry.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib454.8.1" class="ltx_text" style="font-size:90%;">Spectral signatures in backdoor attacks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib454.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib454.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib454.11.3" class="ltx_text" style="font-size:90%;">, pages
8000–8010, 2018.
</span>
</span>
</li>
<li id="bib.bib455" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib455.4.4.1" class="ltx_text" style="font-size:90%;">Ullman [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib455.6.1" class="ltx_text" style="font-size:90%;">
Jonathan Ullman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib455.7.1" class="ltx_text" style="font-size:90%;">Tight lower bounds for locally differentially private selection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib455.8.1" class="ltx_text" style="font-size:90%;">Technical Report abs/1802.02638, arXiv, 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib455.9.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1802.02638" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1802.02638</a><span id="bib.bib455.10.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib456" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib456.4.4.1" class="ltx_text" style="font-size:90%;">v2 Authors [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib456.6.1" class="ltx_text" style="font-size:90%;">
The Google-Landmark v2 Authors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib456.7.1" class="ltx_text" style="font-size:90%;">Google landmark dataset v2, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib456.8.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://github.com/cvdfoundation/google-landmark" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/cvdfoundation/google-landmark</a><span id="bib.bib456.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib457" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib457.5.5.1" class="ltx_text" style="font-size:90%;">Vaidya et al. [2008]</span></span>
<span class="ltx_bibblock"><span id="bib.bib457.7.1" class="ltx_text" style="font-size:90%;">
Jaideep Vaidya, Hwanjo Yu, and Xiaoqian Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib457.8.1" class="ltx_text" style="font-size:90%;">Privacy-preserving SVM classification.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib457.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Knowl. Inf. Syst.</em><span id="bib.bib457.10.2" class="ltx_text" style="font-size:90%;">, 14(2), January 2008.
</span>
</span>
</li>
<li id="bib.bib458" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib458.11.5.1" class="ltx_text" style="font-size:90%;">Van Bulck et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib458.13.1" class="ltx_text" style="font-size:90%;">
Jo Van Bulck, Marina Minkin, Ofir Weisse, Daniel Genkin, Baris Kasikci, Frank
Piessens, Mark Silberstein, Thomas F Wenisch, Yuval Yarom, and Raoul Strackx.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib458.14.1" class="ltx_text" style="font-size:90%;">Foreshadow: Extracting the keys to the intel </span><math id="bib.bib458.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib458.1.m1.1a"><mo maxsize="90%" minsize="90%" id="bib.bib458.1.m1.1.1" xref="bib.bib458.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.1.m1.1b"><ci id="bib.bib458.1.m1.1.1.cmml" xref="bib.bib458.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.1.m1.1c">\{</annotation></semantics></math><span id="bib.bib458.15.2" class="ltx_text" style="font-size:90%;">SGX</span><math id="bib.bib458.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib458.2.m2.1a"><mo maxsize="90%" minsize="90%" id="bib.bib458.2.m2.1.1" xref="bib.bib458.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.2.m2.1b"><ci id="bib.bib458.2.m2.1.1.cmml" xref="bib.bib458.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.2.m2.1c">\}</annotation></semantics></math><span id="bib.bib458.16.3" class="ltx_text" style="font-size:90%;"> kingdom with
transient out-of-order execution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib458.17.5" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib458.6.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">27th <math id="bib.bib458.3.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib458.3.1.m1.1a"><mo stretchy="false" id="bib.bib458.3.1.m1.1.1" xref="bib.bib458.3.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.3.1.m1.1b"><ci id="bib.bib458.3.1.m1.1.1.cmml" xref="bib.bib458.3.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.3.1.m1.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib458.4.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib458.4.2.m2.1a"><mo stretchy="false" id="bib.bib458.4.2.m2.1.1" xref="bib.bib458.4.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.4.2.m2.1b"><ci id="bib.bib458.4.2.m2.1.1.cmml" xref="bib.bib458.4.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.4.2.m2.1c">\}</annotation></semantics></math> Security Symposium (<math id="bib.bib458.5.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib458.5.3.m3.1a"><mo stretchy="false" id="bib.bib458.5.3.m3.1.1" xref="bib.bib458.5.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.5.3.m3.1b"><ci id="bib.bib458.5.3.m3.1.1.cmml" xref="bib.bib458.5.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.5.3.m3.1c">\{</annotation></semantics></math>USENIX<math id="bib.bib458.6.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib458.6.4.m4.1a"><mo stretchy="false" id="bib.bib458.6.4.m4.1.1" xref="bib.bib458.6.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib458.6.4.m4.1b"><ci id="bib.bib458.6.4.m4.1.1.cmml" xref="bib.bib458.6.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib458.6.4.m4.1c">\}</annotation></semantics></math>
Security 18)</em><span id="bib.bib458.18.6" class="ltx_text" style="font-size:90%;">, pages 991–1008, 2018.
</span>
</span>
</li>
<li id="bib.bib459" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib459.5.5.1" class="ltx_text" style="font-size:90%;">Vanhaesebrouck et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib459.7.1" class="ltx_text" style="font-size:90%;">
Paul Vanhaesebrouck, Aurélien Bellet, and Marc Tommasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib459.8.1" class="ltx_text" style="font-size:90%;">Decentralized collaborative learning of personalized models over
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib459.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib459.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">AISTATS</em><span id="bib.bib459.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib460" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib460.5.5.1" class="ltx_text" style="font-size:90%;">Vepakomma et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib460.7.1" class="ltx_text" style="font-size:90%;">
Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib460.8.1" class="ltx_text" style="font-size:90%;">Split learning for health: Distributed deep learning without sharing
raw patient data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib460.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.00564</em><span id="bib.bib460.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib461" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib461.5.5.1" class="ltx_text" style="font-size:90%;">Vepakomma et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib461.7.1" class="ltx_text" style="font-size:90%;">
Praneeth Vepakomma, Chetan Tonde, Ahmed Elgammal, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib461.8.1" class="ltx_text" style="font-size:90%;">Supervised dimensionality reduction via distance correlation
maximization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib461.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Electronic Journal of Statistics</em><span id="bib.bib461.10.2" class="ltx_text" style="font-size:90%;">, 12(1):960–984, 2018b.
</span>
</span>
</li>
<li id="bib.bib462" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib462.5.5.1" class="ltx_text" style="font-size:90%;">Vepakomma et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib462.7.1" class="ltx_text" style="font-size:90%;">
Praneeth Vepakomma, Otkrist Singh, Abhishek Gupta, and Ramesh Raskar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib462.8.1" class="ltx_text" style="font-size:90%;">Nopeek: Information leakage reduction to share activations in
distributed deep learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib462.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2008.09161</em><span id="bib.bib462.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib463" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib463.5.5.1" class="ltx_text" style="font-size:90%;">Vogels et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib463.7.1" class="ltx_text" style="font-size:90%;">
Thijs Vogels, Sai Praneeth Karimireddy, and Martin Jaggi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib463.8.1" class="ltx_text" style="font-size:90%;">PowerSGD: Practical low-rank gradient compression for distributed
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib463.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib463.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS 2019 - Advances in Neural Information Processing
Systems 32</em><span id="bib.bib463.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib464" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib464.5.5.1" class="ltx_text" style="font-size:90%;">Wahby et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib464.7.1" class="ltx_text" style="font-size:90%;">
Riad S. Wahby, Ioanna Tzialla, Abhi Shelat, Justin Thaler, and Michael Walfish.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib464.8.1" class="ltx_text" style="font-size:90%;">Doubly-efficient zksnarks without trusted setup.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib464.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib464.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE Symposium on Security and Privacy, SP 2018,
Proceedings, 21-23 May 2018, San Francisco, California, USA</em><span id="bib.bib464.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib465" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib465.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib465.7.1" class="ltx_text" style="font-size:90%;">
Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y Zhao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib465.8.1" class="ltx_text" style="font-size:90%;">Neural cleanse: Identifying and mitigating backdoor attacks in neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib465.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib465.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE Symposium on Security and Privacy</em><span id="bib.bib465.11.3" class="ltx_text" style="font-size:90%;">. IEEE,
2019a.
</span>
</span>
</li>
<li id="bib.bib466" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib466.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib466.7.1" class="ltx_text" style="font-size:90%;">
Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
Agarwal, Jy yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib466.8.1" class="ltx_text" style="font-size:90%;">Attack of the tails: Yes, you really can backdoor federated learning,
2020a.
</span>
</span>
</li>
<li id="bib.bib467" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib467.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Joshi [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib467.6.1" class="ltx_text" style="font-size:90%;">
Jianyu Wang and Gauri Joshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib467.7.1" class="ltx_text" style="font-size:90%;">Cooperative SGD: A unified framework for the design and analysis
of communication-efficient SGD algorithms.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib467.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">preprint</em><span id="bib.bib467.9.2" class="ltx_text" style="font-size:90%;">, August 2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib467.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1808.07576" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1808.07576</a><span id="bib.bib467.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib468" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib468.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Joshi [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib468.6.1" class="ltx_text" style="font-size:90%;">
Jianyu Wang and Gauri Joshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib468.7.1" class="ltx_text" style="font-size:90%;">Adaptive Communication Strategies for Best Error-Runtime Trade-offs
in Communication-Efficient Distributed SGD.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib468.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib468.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the SysML Conference</em><span id="bib.bib468.10.3" class="ltx_text" style="font-size:90%;">, April 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib468.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1810.08313" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1810.08313</a><span id="bib.bib468.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib469" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib469.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib469.7.1" class="ltx_text" style="font-size:90%;">
Jianyu Wang, Anit Sahu, Gauri Joshi, and Soummya Kar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib469.8.1" class="ltx_text" style="font-size:90%;">MATCHA: Speeding Up Decentralized SGD via Matching Decomposition
Sampling.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib469.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">preprint</em><span id="bib.bib469.10.2" class="ltx_text" style="font-size:90%;">, May 2019b.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib469.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1905.09435" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1905.09435</a><span id="bib.bib469.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib470" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib470.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib470.7.1" class="ltx_text" style="font-size:90%;">
Jianyu Wang, Vinayak Tantia, Nicolas Ballas, and Michael Rabbat.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib470.8.1" class="ltx_text" style="font-size:90%;">SlowMo: Improving communication-efficient distributed SGD with
slow momentum.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib470.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.00643</em><span id="bib.bib470.10.2" class="ltx_text" style="font-size:90%;">, 2019c.
</span>
</span>
</li>
<li id="bib.bib471" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib471.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib471.7.1" class="ltx_text" style="font-size:90%;">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib471.8.1" class="ltx_text" style="font-size:90%;">Tackling the objective inconsistency problem in heterogeneous
federated optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib471.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib471.10.2" class="ltx_text" style="font-size:90%;">, 33,
2020b.
</span>
</span>
</li>
<li id="bib.bib472" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib472.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2019d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib472.7.1" class="ltx_text" style="font-size:90%;">
Kangkang Wang, Rajiv Mathews, Chloé Kiddon, Hubert Eichner, Françoise
Beaufays, and Daniel Ramage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib472.8.1" class="ltx_text" style="font-size:90%;">Federated evaluation of on-device personalization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib472.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1910.10252</em><span id="bib.bib472.10.2" class="ltx_text" style="font-size:90%;">, 2019d.
</span>
</span>
</li>
<li id="bib.bib473" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib473.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib473.7.1" class="ltx_text" style="font-size:90%;">
Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib473.8.1" class="ltx_text" style="font-size:90%;">Dataset distillation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib473.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1811.10959</em><span id="bib.bib473.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib474" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib474.6.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib474.8.1" class="ltx_text" style="font-size:90%;">
Yu-Xiang Wang, Borja Balle, and Shiva Kasiviswanathan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib474.9.1" class="ltx_text" style="font-size:90%;">Subsampled R</span><math id="bib.bib474.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="bib.bib474.1.m1.1a"><mo mathsize="90%" id="bib.bib474.1.m1.1.1" xref="bib.bib474.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="bib.bib474.1.m1.1b"><ci id="bib.bib474.1.m1.1.1.cmml" xref="bib.bib474.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib474.1.m1.1c">\backslash</annotation></semantics></math><span id="bib.bib474.10.2" class="ltx_text" style="font-size:90%;">’enyi differential privacy and analytical
moments accountant.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib474.11.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1808.00087</em><span id="bib.bib474.12.2" class="ltx_text" style="font-size:90%;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib475" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib475.4.4.1" class="ltx_text" style="font-size:90%;">Warner [1965]</span></span>
<span class="ltx_bibblock"><span id="bib.bib475.6.1" class="ltx_text" style="font-size:90%;">
Stanley L. Warner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib475.7.1" class="ltx_text" style="font-size:90%;">Randomized response: A survey technique for eliminating evasive
answer bias.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib475.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of the American Statistical Association</em><span id="bib.bib475.9.2" class="ltx_text" style="font-size:90%;">, 60(309):63–69, 1965.
</span>
</span>
</li>
<li id="bib.bib476" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib476.4.4.1" class="ltx_text" style="font-size:90%;">WeBank [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib476.6.1" class="ltx_text" style="font-size:90%;">
WeBank.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib476.7.1" class="ltx_text" style="font-size:90%;">WeBank and Swiss re signed cooperation MOU, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib476.8.1" class="ltx_text" style="font-size:90%;">URL
</span><a target="_blank" href="https://finance.yahoo.com/news/webank-swiss-signed-cooperation-mou-112300218.html" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://finance.yahoo.com/news/webank-swiss-signed-cooperation-mou-112300218.html</a><span id="bib.bib476.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib476.10.1" class="ltx_text" style="font-size:90%;">Retrieved Aug 2019.
</span>
</span>
</li>
<li id="bib.bib477" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib477.5.5.1" class="ltx_text" style="font-size:90%;">Wong et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib477.7.1" class="ltx_text" style="font-size:90%;">
Eric Wong, Frank R Schmidt, and J Zico Kolter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib477.8.1" class="ltx_text" style="font-size:90%;">Wasserstein adversarial examples via projected sinkhorn iterations.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib477.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib477.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib478" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib478.4.4.1" class="ltx_text" style="font-size:90%;">Wood et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib478.6.1" class="ltx_text" style="font-size:90%;">
Gavin Wood et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib478.7.1" class="ltx_text" style="font-size:90%;">Ethereum: A secure decentralised generalised transaction ledger.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib478.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Ethereum project yellow paper</em><span id="bib.bib478.9.2" class="ltx_text" style="font-size:90%;">, 151(2014):1–32, 2014.
</span>
</span>
</li>
<li id="bib.bib479" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib479.4.4.1" class="ltx_text" style="font-size:90%;">Woodruff and Yekhanin [2005]</span></span>
<span class="ltx_bibblock"><span id="bib.bib479.6.1" class="ltx_text" style="font-size:90%;">
D. Woodruff and S. Yekhanin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib479.7.1" class="ltx_text" style="font-size:90%;">A geometric approach to information-theoretic private information
retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib479.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib479.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">20th Annual IEEE Conference on Computational Complexity
(CCC’05)</em><span id="bib.bib479.10.3" class="ltx_text" style="font-size:90%;">, pages 275–284, June 2005.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib479.11.1" class="ltx_text" style="font-size:90%;">doi: </span><span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self" style="font-size:90%;">10.1109/CCC.2005.2</span><span id="bib.bib479.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib480" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib480.5.5.1" class="ltx_text" style="font-size:90%;">Woodworth et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib480.7.1" class="ltx_text" style="font-size:90%;">
Blake Woodworth, Jialei Wang, H. Brendan McMahan, and Nathan Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib480.8.1" class="ltx_text" style="font-size:90%;">Graph oracle models, lower bounds, and gaps for parallel stochastic
optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib480.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib480.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems (NIPS)</em><span id="bib.bib480.11.3" class="ltx_text" style="font-size:90%;">,
2018.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib480.12.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="https://arxiv.org/abs/1805.10222" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://arxiv.org/abs/1805.10222</a><span id="bib.bib480.13.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib481" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib481.5.5.1" class="ltx_text" style="font-size:90%;">Woodworth et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib481.7.1" class="ltx_text" style="font-size:90%;">
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian
Bullins, H Brendan McMahan, Ohad Shamir, and Nathan Srebro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib481.8.1" class="ltx_text" style="font-size:90%;">Is local sgd better than minibatch sgd?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib481.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.07839</em><span id="bib.bib481.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib482" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib482.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib482.7.1" class="ltx_text" style="font-size:90%;">
Xiang Wu, Ruiqi Guo, Ananda Theertha Suresh, Sanjiv Kumar, Daniel N
Holtmann-Rice, David Simcha, and Felix X. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib482.8.1" class="ltx_text" style="font-size:90%;">Multiscale quantization for fast similarity search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib482.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib482.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib482.11.3" class="ltx_text" style="font-size:90%;">, pages
5745–5755, 2017.
</span>
</span>
</li>
<li id="bib.bib483" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib483.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2019a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib483.7.1" class="ltx_text" style="font-size:90%;">
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib483.8.1" class="ltx_text" style="font-size:90%;">Feature denoising for improving adversarial robustness.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib483.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CVPR</em><span id="bib.bib483.10.2" class="ltx_text" style="font-size:90%;">, 2019a.
</span>
</span>
</li>
<li id="bib.bib484" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib484.4.4.1" class="ltx_text" style="font-size:90%;">Xie [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib484.6.1" class="ltx_text" style="font-size:90%;">
Cong Xie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib484.7.1" class="ltx_text" style="font-size:90%;">Zeno++: robust asynchronous SGD with arbitrary number of
Byzantine workers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib484.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1903.07020</em><span id="bib.bib484.9.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib485" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib485.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2019b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib485.7.1" class="ltx_text" style="font-size:90%;">
Cong Xie, Oluwasanmi Koyejo, Indranil Gupta, and Haibin Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib485.8.1" class="ltx_text" style="font-size:90%;">Local adaalter: Communication-efficient stochastic gradient descent
with adaptive learning rates.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib485.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.09030</em><span id="bib.bib485.10.2" class="ltx_text" style="font-size:90%;">, 2019b.
</span>
</span>
</li>
<li id="bib.bib486" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib486.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2019c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib486.7.1" class="ltx_text" style="font-size:90%;">
Cong Xie, Sanmi Koyejo, and Indranil Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib486.8.1" class="ltx_text" style="font-size:90%;">Practical distributed learning: Secure machine learning with
communication-efficient local updates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib486.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib486.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">European Conference on Machine Learning and Principles and
Practice of Knowledge Discovery in Databases (ECML PKDD)</em><span id="bib.bib486.11.3" class="ltx_text" style="font-size:90%;">,
2019c.
</span>
</span>
</li>
<li id="bib.bib487" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib487.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2019d]</span></span>
<span class="ltx_bibblock"><span id="bib.bib487.7.1" class="ltx_text" style="font-size:90%;">
Cong Xie, Sanmi Koyejo, and Indranil Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib487.8.1" class="ltx_text" style="font-size:90%;">Zeno: Distributed stochastic gradient descent with suspicion-based
fault-tolerance.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib487.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib487.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Machine Learning</em><span id="bib.bib487.11.3" class="ltx_text" style="font-size:90%;">, pages
6893–6901, 2019d.
</span>
</span>
</li>
<li id="bib.bib488" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib488.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib488.7.1" class="ltx_text" style="font-size:90%;">
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib488.8.1" class="ltx_text" style="font-size:90%;">SNAS: stochastic neural architecture search.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib488.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1812.09926</em><span id="bib.bib488.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib489" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib489.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. [2019e]</span></span>
<span class="ltx_bibblock"><span id="bib.bib489.7.1" class="ltx_text" style="font-size:90%;">
Tiancheng Xie, Jiaheng Zhang, Yupeng Zhang, Charalampos Papamanthou, and Dawn
Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib489.8.1" class="ltx_text" style="font-size:90%;">Libra: Succinct zero-knowledge proofs with optimal prover
computation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib489.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib489.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">CRYPTO (3)</em><span id="bib.bib489.11.3" class="ltx_text" style="font-size:90%;">, volume 11694 of </span><em id="bib.bib489.12.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">Lecture Notes in
Computer Science</em><span id="bib.bib489.13.5" class="ltx_text" style="font-size:90%;">, pages 733–764. Springer, 2019e.
</span>
</span>
</li>
<li id="bib.bib490" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib490.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib490.7.1" class="ltx_text" style="font-size:90%;">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib490.8.1" class="ltx_text" style="font-size:90%;">Federated machine learning: Concept and applications.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib490.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib490.10.2" class="ltx_text" style="font-size:90%;">, abs/1902.04885, 2019.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib490.11.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1902.04885" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1902.04885</a><span id="bib.bib490.12.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib491" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib491.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib491.7.1" class="ltx_text" style="font-size:90%;">
Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas
Kong, Daniel Ramage, and Françoise Beaufays.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib491.8.1" class="ltx_text" style="font-size:90%;">Applied federated learning: Improving Google keyboard query
suggestions.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib491.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint 1812.02903</em><span id="bib.bib491.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib492" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib492.4.4.1" class="ltx_text" style="font-size:90%;">Yao [1982]</span></span>
<span class="ltx_bibblock"><span id="bib.bib492.6.1" class="ltx_text" style="font-size:90%;">
Andrew C Yao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib492.7.1" class="ltx_text" style="font-size:90%;">Protocols for secure computations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib492.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib492.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Symposium on Foundations of Computer Science</em><span id="bib.bib492.10.3" class="ltx_text" style="font-size:90%;">, 1982.
</span>
</span>
</li>
<li id="bib.bib493" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib493.4.4.1" class="ltx_text" style="font-size:90%;">Yao [1986]</span></span>
<span class="ltx_bibblock"><span id="bib.bib493.6.1" class="ltx_text" style="font-size:90%;">
Andrew Chi-Chih Yao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib493.7.1" class="ltx_text" style="font-size:90%;">How to generate and exchange secrets (extended abstract).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib493.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib493.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">FOCS</em><span id="bib.bib493.10.3" class="ltx_text" style="font-size:90%;">, pages 162–167. IEEE Computer Society, 1986.
</span>
</span>
</li>
<li id="bib.bib494" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib494.5.5.1" class="ltx_text" style="font-size:90%;">Ye et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib494.7.1" class="ltx_text" style="font-size:90%;">
Fangwei Ye, Carolina Naim, and Salim El Rouayheb.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib494.8.1" class="ltx_text" style="font-size:90%;">Preserving ON-OFF privacy for past and future requests.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib494.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib494.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2019 IEEE Information Theory Workshop (ITW)</em><span id="bib.bib494.11.3" class="ltx_text" style="font-size:90%;">, August 2019.
</span>
</span>
</li>
<li id="bib.bib495" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib495.4.4.1" class="ltx_text" style="font-size:90%;">Ye and Barg [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib495.6.1" class="ltx_text" style="font-size:90%;">
Min Ye and Alexander Barg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib495.7.1" class="ltx_text" style="font-size:90%;">Optimal schemes for discrete distribution estimation under locally
differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib495.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</em><span id="bib.bib495.9.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib496" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib496.5.5.1" class="ltx_text" style="font-size:90%;">Yeom et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib496.7.1" class="ltx_text" style="font-size:90%;">
Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib496.8.1" class="ltx_text" style="font-size:90%;">Privacy risk in machine learning: Analyzing the connection to
overfitting.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib496.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib496.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE 31st Computer Security Foundations Symposium
(CSF)</em><span id="bib.bib496.11.3" class="ltx_text" style="font-size:90%;">, pages 268–282. IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib497" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib497.5.5.1" class="ltx_text" style="font-size:90%;">Yin et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib497.7.1" class="ltx_text" style="font-size:90%;">
Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib497.8.1" class="ltx_text" style="font-size:90%;">Byzantine-robust distributed learning: Towards optimal statistical
rates.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib497.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib497.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib497.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib498" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib498.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2018a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib498.7.1" class="ltx_text" style="font-size:90%;">
Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan
Alistarh, Ce Zhang, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib498.8.1" class="ltx_text" style="font-size:90%;">Distributed learning over unreliable networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib498.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.07766</em><span id="bib.bib498.10.2" class="ltx_text" style="font-size:90%;">, 2018a.
</span>
</span>
</li>
<li id="bib.bib499" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib499.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2020a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib499.7.1" class="ltx_text" style="font-size:90%;">
Han Yu, Zelei Liu, Yang Liu, Tianjian Chen, Mingshu Cong, Xi Weng, Dusit
Niyato, and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib499.8.1" class="ltx_text" style="font-size:90%;">A sustainable incentive scheme for federated learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib499.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Intelligent Systems</em><span id="bib.bib499.10.2" class="ltx_text" style="font-size:90%;">, 35(4):58–69,
2020a.
</span>
</span>
</li>
<li id="bib.bib500" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib500.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2018b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib500.7.1" class="ltx_text" style="font-size:90%;">
Hao Yu, Sen Yang, and Shenghuo Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib500.8.1" class="ltx_text" style="font-size:90%;">Parallel restarted SGD for non-convex optimization with faster
convergence and less communication.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib500.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1807.06629</em><span id="bib.bib500.10.2" class="ltx_text" style="font-size:90%;">, 2018b.
</span>
</span>
</li>
<li id="bib.bib501" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib501.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib501.7.1" class="ltx_text" style="font-size:90%;">
Hao Yu, Rong Jin, and Sen Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib501.8.1" class="ltx_text" style="font-size:90%;">On the linear speedup analysis of communication efficient momentum
SGD for distributed non-convex optimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib501.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.03817</em><span id="bib.bib501.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib502" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib502.5.5.1" class="ltx_text" style="font-size:90%;">Yu et al. [2020b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib502.7.1" class="ltx_text" style="font-size:90%;">
Tao Yu, Eugene Bagdasaryan, and Vitaly Shmatikov.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib502.8.1" class="ltx_text" style="font-size:90%;">Salvaging federated learning by local adaptation.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib502.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2002.04758</em><span id="bib.bib502.10.2" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib503" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib503.5.5.1" class="ltx_text" style="font-size:90%;">Zafar et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib503.7.1" class="ltx_text" style="font-size:90%;">
Muhammad Bila Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P.
Gummadi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib503.8.1" class="ltx_text" style="font-size:90%;">Fairness constraints: Mechanisms for fair classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib503.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib503.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</em><span id="bib.bib503.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib504" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib504.5.5.1" class="ltx_text" style="font-size:90%;">Zantedeschi et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib504.7.1" class="ltx_text" style="font-size:90%;">
Valentina Zantedeschi, Aurélien Bellet, and Marc Tommasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib504.8.1" class="ltx_text" style="font-size:90%;">Fully Decentralized Joint Learning of Personalized Models and
Collaboration Graphs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib504.9.1" class="ltx_text" style="font-size:90%;">Technical report, arXiv:1901.08460, 2019.
</span>
</span>
</li>
<li id="bib.bib505" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib505.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib505.7.1" class="ltx_text" style="font-size:90%;">
Sixin Zhang, Anna E Choromanska, and Yann LeCun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib505.8.1" class="ltx_text" style="font-size:90%;">Deep learning with elastic averaging SGD.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib505.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib505.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib505.11.3" class="ltx_text" style="font-size:90%;">, pages
685–693, 2015.
</span>
</span>
</li>
<li id="bib.bib506" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib506.4.4.1" class="ltx_text" style="font-size:90%;">Zhang and Yang [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib506.6.1" class="ltx_text" style="font-size:90%;">
Yu Zhang and Qiang Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib506.7.1" class="ltx_text" style="font-size:90%;">A survey on multi-task learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib506.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">CoRR</em><span id="bib.bib506.9.2" class="ltx_text" style="font-size:90%;">, abs/1707.08114, 2017.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib506.10.1" class="ltx_text" style="font-size:90%;">URL </span><a target="_blank" href="http://arxiv.org/abs/1707.08114" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">http://arxiv.org/abs/1707.08114</a><span id="bib.bib506.11.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib507" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib507.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2013]</span></span>
<span class="ltx_bibblock"><span id="bib.bib507.7.1" class="ltx_text" style="font-size:90%;">
Yuchen Zhang, John Duchi, Micheal I. Jordan, and Martin J. Wainwright.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib507.8.1" class="ltx_text" style="font-size:90%;">Information-theoretic lower bounds for distributed statistical
estimation with communication constraints.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib507.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib507.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</em><span id="bib.bib507.11.3" class="ltx_text" style="font-size:90%;">, pages
2328–2336, 2013.
</span>
</span>
</li>
<li id="bib.bib508" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib508.5.5.1" class="ltx_text" style="font-size:90%;">Zhao et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib508.7.1" class="ltx_text" style="font-size:90%;">
Yawei Zhao, Chen Yu, Peilin Zhao, and Ji Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib508.8.1" class="ltx_text" style="font-size:90%;">Decentralized online learning: Take benefits from others’ data
without sharing your own to track global trend.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib508.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1901.10593</em><span id="bib.bib508.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib509" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib509.4.4.1" class="ltx_text" style="font-size:90%;">Zhu and Gupta [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib509.6.1" class="ltx_text" style="font-size:90%;">
Michael Zhu and Suyog Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib509.7.1" class="ltx_text" style="font-size:90%;">To prune, or not to prune: exploring the efficacy of pruning for
model compression.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib509.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1710.01878</em><span id="bib.bib509.9.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib510" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib510.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib510.7.1" class="ltx_text" style="font-size:90%;">
Wennan Zhu, Peter Kairouz, Haicheng Sun, Brendan McMahan, and Wei Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib510.8.1" class="ltx_text" style="font-size:90%;">Federated heavy hitters discovery with differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib510.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1902.08534</em><span id="bib.bib510.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib511" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib511.4.4.1" class="ltx_text" style="font-size:90%;">Zhu [2015]</span></span>
<span class="ltx_bibblock"><span id="bib.bib511.6.1" class="ltx_text" style="font-size:90%;">
Xiaojin Zhu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib511.7.1" class="ltx_text" style="font-size:90%;">Machine teaching: An inverse problem to machine learning and an
approach toward optimal education.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib511.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib511.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Twenty-Ninth AAAI Conference on Artificial Intelligence</em><span id="bib.bib511.10.3" class="ltx_text" style="font-size:90%;">,
2015.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Software and Datasets for Federated Learning</h2>

<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Software for simulation</h5>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">Simulations of federated learning require dealing with multiple issues that do not arise in datacenter ML research, for example, efficiently processing partitioned datasets, with computations running on different simulated devices, each with a variable amount of data. FL research also requires different metrics such as the number of bytes upload or downloaded by device, as well as the ability to simulate issues like time-varying arrival of different clients or client drop-out that is potentially correlated with the nature of the local dataset. With this in mind, the development of open software frameworks for federated learning research (simulation) has the potential to greatly accelerate research progress. Several platforms are available or in development, including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib404" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">404</span></a>]</cite>:</p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p">TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> specifically targets research use cases, providing large-scale simulation capabilities as well as flexible orchestration for the control of sampling.</p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p">FedML <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib229" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">229</span></a>]</cite> is a research-oriented library. It supports three platforms: on-device training for IoT and mobile devices, distributed computing, and single-machine simulation. For research diversity, FedML also supports various algorithms (e.g., decentralized learning, vertical FL, and split learning), models, and datasets.</p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p">PySyft <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib399" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">399</span></a>]</cite> is a Python library for secure, private Deep Learning. PySyft decouples private data from model training, using federated learning, differential privacy, and multi-party computation (MPC) within PyTorch.</p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p">Leaf <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite> provides multiple datasets (see below), as well as simulation and evaluation capabilities.</p>
</div>
</li>
<li id="A1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i5.p1" class="ltx_para">
<p id="A1.I1.i5.p1.1" class="ltx_p">Sherpa.ai Federated Learning and Differential Privacy Framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib397" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">397</span></a>]</cite> is an open source federated learning and differential privacy framework which provides methodologies, pipelines, and evaluation techniques for federated learning.</p>
</div>
</li>
<li id="A1.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i6.p1" class="ltx_para">
<p id="A1.I1.i6.p1.1" class="ltx_p">PyVertical <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>]</cite> is a project focusing on federated learning with data partitioned by features (also referred to as vertical partitioning) in the cross-silo setting; see <a href="#S2.SS2" title="2.2 Cross-Silo Federated Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Production-oriented software</h5>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">In addition to the above simulation platforms, several production-oriented federated learning platforms are being developed:</p>
<ul id="A1.I2" class="ltx_itemize">
<li id="A1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i1.p1" class="ltx_para">
<p id="A1.I2.i1.p1.1" class="ltx_p">FATE (Federated AI Technology Enabler) <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> is an open-source project intended to provide a secure computing framework to support the federated AI ecosystem.</p>
</div>
</li>
<li id="A1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i2.p1" class="ltx_para">
<p id="A1.I2.i2.p1.1" class="ltx_p">PaddleFL <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite> is an open source federated learning framework based on PaddlePaddle <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>. In PaddleFL, several federated learning strategies and training strategies are provided with application demonstrations.</p>
</div>
</li>
<li id="A1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i3.p1" class="ltx_para">
<p id="A1.I2.i3.p1.1" class="ltx_p">Clara Training Framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib125" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">125</span></a>]</cite> includes the support of cross-silo federated learning based on a server-client approach with data privacy protection.</p>
</div>
</li>
<li id="A1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i4.p1" class="ltx_para">
<p id="A1.I2.i4.p1.1" class="ltx_p">IBM Federated Learning <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib321" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">321</span></a>]</cite> is a Python-based federated learning framework for enterprise environments, which provides a basic fabric for adding advanced features.</p>
</div>
</li>
<li id="A1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i5.p1" class="ltx_para">
<p id="A1.I2.i5.p1.1" class="ltx_p">Flower framework <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib66" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">66</span></a>]</cite> supports implementation and experimentation of federated learning algorithms on mobile and embedded devices with a real-world system conditions simulation.</p>
</div>
</li>
<li id="A1.I2.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I2.i6.p1" class="ltx_para">
<p id="A1.I2.i6.p1.1" class="ltx_p">Fedlearner <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> is an open source federated learning framework that enables joint modeling of data distributed between institutions.</p>
</div>
</li>
</ul>
<p id="A1.SS0.SSS0.Px2.p1.2" class="ltx_p">Such production-oriented federated learning platforms must address problems that do not exist in simulation such as authentication, communication protocols, encryption and deployment to physical devices or silos. Note that while TensorFlow Federated is listed under “Software for simulation”, its design includes abstractions for aggregation and broadcast, and serialization of all TensorFlow computations for execution in non-Python environments, making it suitable for use as a component in a production system.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Datasets</h5>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p">Federated learning is adopted when the data is decentralized and typically unbalanced (different clients have different numbers of examples) and not identically distributed (each client’s data is drawn from a different distribution). The open source package TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> supports loading decentralized dataset in a simulated environment with each client id corresponding to a TensorFlow Dataset Object. These datasets can easily be converted to numpy arrays for use in other frameworks.<span id="footnote15" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span><a target="_blank" href="https://www.tensorflow.org/datasets/api_docs/python/tfds/as_numpy" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/datasets/api_docs/python/tfds/as_numpy</a>.</span></span></span> At the time of writing, three datasets are supported and we recommend researchers to benchmark on them.</p>
</div>
<div id="A1.SS0.SSS0.Px3.p2" class="ltx_para">
<ul id="A1.I3" class="ltx_itemize">
<li id="A1.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i1.p1" class="ltx_para">
<p id="A1.I3.i1.p1.1" class="ltx_p"><span id="A1.I3.i1.p1.1.1" class="ltx_text ltx_font_italic">EMNIST</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">126</span></a>]</cite> consists of 671,585 images of digits and upper and lower case English characters (62 classes).
The federated version splits the dataset into 3,400 unbalanced clients indexed by the original writer of the digits/characters.
The non-IID distribution comes from the unique writing style of each person.</p>
</div>
</li>
<li id="A1.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i2.p1" class="ltx_para">
<p id="A1.I3.i2.p1.1" class="ltx_p"><span id="A1.I3.i2.p1.1.1" class="ltx_text ltx_font_italic">Stackoverflow<span id="footnote16" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note"><span id="footnote16.1.1.1" class="ltx_text ltx_font_upright">16</span></span><a target="_blank" href="https://www.kaggle.com/stackoverflow/stackoverflow" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/stackoverflow/stackoverflow</a></span></span></span></span> dataset consists of question and answer from Stack Overflow with metadata like timestamps, scores, etc.
The training dataset has more than 342,477 unique users with 135,818,730 examples.
Note that the timestamp information can be helpful to simulate the pattern of incoming data.</p>
</div>
</li>
<li id="A1.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I3.i3.p1" class="ltx_para">
<p id="A1.I3.i3.p1.1" class="ltx_p"><span id="A1.I3.i3.p1.1.1" class="ltx_text ltx_font_italic">Shakespeare</span> is a language modeling dataset derived from <span id="A1.I3.i3.p1.1.2" class="ltx_text ltx_font_italic">The Complete Works of William Shakespeare</span>.
It consists of 715 characters whose contiguous lines are examples in the client dataset. The train set has 16,068 examples and test set has 2,356 examples.</p>
</div>
</li>
</ul>
<p id="A1.SS0.SSS0.Px3.p2.1" class="ltx_p">The preprocessing for <span id="A1.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_italic">EMNIST</span> and <span id="A1.SS0.SSS0.Px3.p2.1.2" class="ltx_text ltx_font_italic">Shakespeare</span> are provided by the Leaf project <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">96</span></a>]</cite>, which also provides federated versions of the sentiment140 and celebA datasets. These datasets have enough clients that they can be used to simulate cross-device FL scenarios, but for questions where scale is particularly important, they may be too small. In this respect <span id="A1.SS0.SSS0.Px3.p2.1.3" class="ltx_text ltx_font_italic">Stackoverflow</span> provides the most realistic example of a cross-device FL problem.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Cross-silo datasets</h5>

<div id="A1.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p1.1" class="ltx_p">One example is the iNaturalist dataset<span id="footnote17" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">17</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">17</sup><span class="ltx_tag ltx_tag_note">17</span><a target="_blank" href="https://www.inaturalist.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.inaturalist.org/</a></span></span></span> which consists of large numbers of observations of various organisms all over the world. One can partition it by the geolocation or the author of an observation. If we partition it by the group an organism belongs to, like kingdom, phylum, etc., then the clients have totally different labels and biological closeness between two clients is already known. This makes it a very suitable dataset to study federated transfer learning and multi-task learning in cross-silo settings.</p>
</div>
<div id="A1.SS0.SSS0.Px4.p2" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p2.1" class="ltx_p">Another example is the Google-Landmark-v2 <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib456" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">456</span></a>]</cite> that includes over 5 million images of more than 200 thousand different types of landmark. Similar to the iNaturalist dataset, one can split the dataset by authors, but due to the difference in scale with iNaturalist dataset, Google Landmark Dataset provides much more diversity and creates even greater challenges to large-scale federated learning.</p>
</div>
<div id="A1.SS0.SSS0.Px4.p3" class="ltx_para">
<p id="A1.SS0.SSS0.Px4.p3.3" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Luo et al.</span> [<a href="#bib.bib322" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">322</span></a>]</cite> has recently published a federated dataset for computer vision. The dataset contains more than <math id="A1.SS0.SSS0.Px4.p3.1.m1.1" class="ltx_Math" alttext="900" display="inline"><semantics id="A1.SS0.SSS0.Px4.p3.1.m1.1a"><mn id="A1.SS0.SSS0.Px4.p3.1.m1.1.1" xref="A1.SS0.SSS0.Px4.p3.1.m1.1.1.cmml">900</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px4.p3.1.m1.1b"><cn type="integer" id="A1.SS0.SSS0.Px4.p3.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px4.p3.1.m1.1.1">900</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px4.p3.1.m1.1c">900</annotation></semantics></math> annotated street images generated from <math id="A1.SS0.SSS0.Px4.p3.2.m2.1" class="ltx_Math" alttext="26" display="inline"><semantics id="A1.SS0.SSS0.Px4.p3.2.m2.1a"><mn id="A1.SS0.SSS0.Px4.p3.2.m2.1.1" xref="A1.SS0.SSS0.Px4.p3.2.m2.1.1.cmml">26</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px4.p3.2.m2.1b"><cn type="integer" id="A1.SS0.SSS0.Px4.p3.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px4.p3.2.m2.1.1">26</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px4.p3.2.m2.1c">26</annotation></semantics></math> street cameras and <math id="A1.SS0.SSS0.Px4.p3.3.m3.1" class="ltx_Math" alttext="7" display="inline"><semantics id="A1.SS0.SSS0.Px4.p3.3.m3.1a"><mn id="A1.SS0.SSS0.Px4.p3.3.m3.1.1" xref="A1.SS0.SSS0.Px4.p3.3.m3.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px4.p3.3.m3.1b"><cn type="integer" id="A1.SS0.SSS0.Px4.p3.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px4.p3.3.m3.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px4.p3.3.m3.1c">7</annotation></semantics></math> object categories annotated with detailed bounding box. Due to the relatively small number of examples in the dataset, it may not adequately reflect a challenging realistic scenario.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px5" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">The need for more datasets</h5>

<div id="A1.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px5.p1.1" class="ltx_p">Developing new federated learning datasets that are representative of real-world problems is an important question for the community to address. Platforms like TensorFlow Federated <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> welcome the contribution of new datasets and may be able to provide hosting support.</p>
</div>
<div id="A1.SS0.SSS0.Px5.p2" class="ltx_para">
<p id="A1.SS0.SSS0.Px5.p2.1" class="ltx_p">While completely new datasets are always interesting, in many cases it is possible to partition existing open datasets, treating each split as a client. Different partitioning strategies may be appropriate for different research questions, but often unbalanced and non-IID partitions will be most relevant. It is also interesting to maintain as much additional meta information (timestamp, geolocation, etc.) as possible.</p>
</div>
<div id="A1.SS0.SSS0.Px5.p3" class="ltx_para">
<p id="A1.SS0.SSS0.Px5.p3.1" class="ltx_p">In particular, there is a need for feature-partitioned datasets, as will be discussed in <a href="#S2.SS2" title="2.2 Cross-Silo Federated Learning ‣ 2 Relaxing the Core FL Assumptions: Applications to Emerging Settings and Scenarios ‣ Advances and Open Problems in Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>. For example, a patient may go to one medical institute for a pathology test and go to another for radiology picture archiving, in which case the features of one sample are partitioned over two institutes regulated by HIPAA. <cite class="ltx_cite ltx_citemacro_citep">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1912.04976" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1912.04977" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1912.04977">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1912.04977" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1912.04979" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 11:28:17 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
