<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.02601] Knowledge-Based Counterfactual Queries for Visual Question Answering</title><meta property="og:description" content="Visual Question Answering (VQA) has been a popular task that combines vision and language, with numerous relevant implementations in literature. Even though there are some attempts that approach explainability and robuâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Knowledge-Based Counterfactual Queries for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Knowledge-Based Counterfactual Queries for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.02601">

<!--Generated on Thu Feb 29 20:48:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\copyrightclause</span>
<p id="p1.2" class="ltx_p">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0).</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\conference</span>
<p id="p2.2" class="ltx_p">In A. Martin, K. Hinkelmann, H.-G. Fill, A. Gerber, D. Lenat, R. Stolle, F. van Harmelen (Eds.),
Proceedings of the AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering (AAAI-MAKE 2023), Hyatt Regency, San Francisco Airport, California, USA, March 27-29, 2023.</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">[orcid=0000-0002-3653-0041, email=dido.stoikou@gmail.com]
[orcid=0000-0001-9442-4186, email=marialymp@islab.ntua.gr]</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">[orcid= 0000-0003-1210-9874, email=gstam@cs.ntua.gr]</p>
</div>
<h1 class="ltx_title ltx_title_document">Knowledge-Based Counterfactual Queries for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Theodoti Stoikou
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maria Lymperaiou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">AILS Lab,
School of Electrical and Computer Engineering,
National Technical University of Athens
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giorgos Stamou
</span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) has been a popular task that combines vision and language, with numerous relevant implementations in literature. Even though there are some attempts that approach explainability and robustness issues in VQA models, very few of them employ counterfactuals as a means of probing such challenges in a model-agnostic way. In this work, we propose a systematic method for explaining the behavior and investigating the robustness of VQA models through counterfactual perturbations. For this reason, we exploit structured knowledge bases to perform deterministic, optimal and controllable word-level replacements targeting the linguistic modality, and we then evaluate the modelâ€™s response against such counterfactual inputs. Finally, we qualitatively extract local and global explanations based on counterfactual responses, which are ultimately proven insightful towards interpreting VQA model behaviors. By performing a variety of perturbation types, targeting different parts of speech of the input question, we gain insights to the reasoning of the model, through the comparison of its responses in different adversarial circumstances. Overall, we reveal possible biases in the decision-making process of the model, as well as expected and unexpected patterns, which impact its performance quantitatively and qualitatively, as indicated by our analysis.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Visual Question Answering <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>Knowledge Graphs <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>XAI <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>Counterfactual Explanations <span id="id5.id4" class="ltx_ERROR undefined">\sep</span>Robustness

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The indisputable rise in popularity of visiolinguistic (VL) learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> has offered a variety of impressive model implementations to the community in a short time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Visual Question Answering (VQA) is a VL task that has obtained a fundamental role in the evolution of various interactive VL AI systems, such as Visual Dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Text-Image Retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Visual Commonsense Reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
To this end, there is an extensive range of real-world applications that benefit significantly from the new advances around the VQA task, such as aiding systems for visually impaired individuals <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and self-driving cars <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA involves a textual question <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">q</span> from a pre-defined question set <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">Q</annotation></semantics></math> accompanied by an image <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">I</span>, the interaction of which yields a textual answer <span id="S1.p2.1.3" class="ltx_text ltx_font_italic">a</span>.
The race for continuously advancing VQA model performance unavoidably results in leaving open issues, especially attributed to the black-box nature of state-of-the-art implementations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. This limited access to the reasoning that such models follow to make decisions emphasizes the risk of an arbitrary behavior on their behalf. This peril lies mainly in the possibility of bias integration, decisions that lack the proper focus, as well as the absence of explainability and fairness of results. Especially when pivotal decisions are made based on this type of systems, their opacity renders them impractical, and at times hazardous, for most applications. This uncertainty indicates the need for new robustness evaluation methods, that prioritize the transparency of VQA models.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Different approaches to debiasing and explainability of VQA models focus on diverse aspects of the issue. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> examines VQA robustness and explainability by addressing transformations on the visual modality, as they attribute the problem mostly to the visual bias as occurring from unwanted correlations between image concepts. In general, existing works primarily focus on the effect of <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">visual bias</span> rather than the impact of <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">linguistic bias</span>, as a reason behind the lack of robustness in VQA models. Other works follow attention-based strategies that require extensive knowledge on the model architecture, thus they cannot handle efficiently the <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">black-box</span> nature of these systems. To this end, various model-specific approaches are proven to be fruitful in their strict framework, but lack the capability to be generalized to the evaluation of any other model, thus limiting their efficiency scope to just one specific case.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We argue that resolving explainability challenges in VQA models calls for a counterfactual approach, implemented as word-level perturbations on the questions <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">q</span>; thus, we diverge from the well-sought exploration of the visual modality, examining the role of the language on possible biases and spurious correlations hidden in VQA models, while tracing and interpreting their opaque decision-making process. Our proposed counterfactual perturbations are framed as: <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">"What is the response of the VQA model if we substitute word X with word Y in question q?"</span>
Specifically, by viewing words as concepts, we perform the minimum possible feasible transformation to stimulate a change in the modelâ€™s response; then, insightful comparisons are made by recording the modelâ€™s behavior to various such transformations. The counterfactual perturbations that we perform are fully guided by the deterministic assurance of <span id="S1.p4.1.3" class="ltx_text ltx_font_italic">hierarchical knowledge</span> structures. By deploying these knowledge sources, we provide transformations that are not only optimally targeted to each specific linguistic concept, but are also fully <span id="S1.p4.1.4" class="ltx_text ltx_font_italic">explainable</span> in terms of the strategy followed for their implementation.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Starting with the observation of <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">local model responses</span> in different linguistic perturbations for a single data sample, we further identify <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">global patterns</span> that refer to the overall behavior of the model when faced with a specific set of perturbed concepts. Following this, we propose global rules that characterize the response of a model and can underline its weaknesses, by indicating what concepts could harm its robustness and certainty. This process reveals possible biases that a model has integrated, and hence attributes explanations as to why a particular answer is generated in place of another one; thus, we are able to obtain insights to the reasoning process of a model, without the need of access to the modelâ€™s inner architecture.
Our method is generalizable to any VQA model and corresponding suitable dataset, as it approaches the issue in a totally model-agnostic strategy. To sum up, we contribute to the following:</p>
</div>
<div id="S1.p6" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We design counterfactual inputs applying a variety of structured <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">word-level replacements</span> on the questions <math id="S1.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S1.I1.i1.p1.1.m1.1a"><mrow id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml"><mi id="S1.I1.i1.p1.1.m1.1.1.2" xref="S1.I1.i1.p1.1.m1.1.1.2.cmml">q</mi><mo id="S1.I1.i1.p1.1.m1.1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S1.I1.i1.p1.1.m1.1.1.3" xref="S1.I1.i1.p1.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><apply id="S1.I1.i1.p1.1.m1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1"><in id="S1.I1.i1.p1.1.m1.1.1.1.cmml" xref="S1.I1.i1.p1.1.m1.1.1.1"></in><ci id="S1.I1.i1.p1.1.m1.1.1.2.cmml" xref="S1.I1.i1.p1.1.m1.1.1.2">ğ‘</ci><ci id="S1.I1.i1.p1.1.m1.1.1.3.cmml" xref="S1.I1.i1.p1.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">q\in Q</annotation></semantics></math>, as instructed by hierarchical knowledge sources. Our approach is model-agnostic, as we treat any VQA model as a black box.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We obtain <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">local explanations</span> derived from unexpected model responses to counterfactual <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">q</span> inputs.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">By summarizing local model behaviors for all <math id="S1.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S1.I1.i3.p1.1.m1.1a"><mrow id="S1.I1.i3.p1.1.m1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.cmml"><mi id="S1.I1.i3.p1.1.m1.1.1.2" xref="S1.I1.i3.p1.1.m1.1.1.2.cmml">q</mi><mo id="S1.I1.i3.p1.1.m1.1.1.1" xref="S1.I1.i3.p1.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S1.I1.i3.p1.1.m1.1.1.3" xref="S1.I1.i3.p1.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.I1.i3.p1.1.m1.1b"><apply id="S1.I1.i3.p1.1.m1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1"><in id="S1.I1.i3.p1.1.m1.1.1.1.cmml" xref="S1.I1.i3.p1.1.m1.1.1.1"></in><ci id="S1.I1.i3.p1.1.m1.1.1.2.cmml" xref="S1.I1.i3.p1.1.m1.1.1.2">ğ‘</ci><ci id="S1.I1.i3.p1.1.m1.1.1.3.cmml" xref="S1.I1.i3.p1.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.p1.1.m1.1c">q\in Q</annotation></semantics></math>, we extract some <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">global explanations</span> that reveal the overall model response to each of the designed counterfactual inputs.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Visual Question Answering (VQA)</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.15" class="ltx_p">Visual Question Answering (VQA), first introduced in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, lies in the category of multimodal learning tasks, as it receives both visual and linguistic modalities as input. Specifically, a VQA model <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">M</annotation></semantics></math> receives images <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.p1.2.m2.1a"><mi id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">i</annotation></semantics></math> from a set <span id="S2.p1.15.1" class="ltx_text ltx_font_italic">I</span> and relevant questions <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.3.m3.1a"><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">q</annotation></semantics></math> belonging to a predefined question set <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">Q</annotation></semantics></math>, and is expected to accurately answer those <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">q</annotation></semantics></math> by providing a natural language answer <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">a</annotation></semantics></math>. The aforementioned answers can be either open-ended (generated by <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p1.7.m7.1a"><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">M</annotation></semantics></math>) or belong to a set of pre-defined candidates <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S2.p1.8.m8.1a"><mi id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">ğ´</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">A</annotation></semantics></math>.
In general, questions <math id="S2.p1.9.m9.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.9.m9.1a"><mi id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><ci id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">q</annotation></semantics></math> have an arbitrary nature and they enclose different computer vision sub-problems, such as object recognition and detection, attribute and scene classification, as well as counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. Furthermore, more intricate questions concern more complex processes, such as spatial relationships among objects and commonsense reasoning.
According to each specific case, visual questions <math id="S2.p1.10.m10.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.10.m10.1a"><mi id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><ci id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">q</annotation></semantics></math> selectively target different areas of an image <math id="S2.p1.11.m11.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.p1.11.m11.1a"><mi id="S2.p1.11.m11.1.1" xref="S2.p1.11.m11.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.p1.11.m11.1b"><ci id="S2.p1.11.m11.1.1.cmml" xref="S2.p1.11.m11.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.11.m11.1c">I</annotation></semantics></math>, including background details and underlying context. In accordance, the focus regarding the linguistic input lies in different word concepts depending on each image-question pair. An example of the VQA task, including an image <math id="S2.p1.12.m12.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S2.p1.12.m12.1a"><mi id="S2.p1.12.m12.1.1" xref="S2.p1.12.m12.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.p1.12.m12.1b"><ci id="S2.p1.12.m12.1.1.cmml" xref="S2.p1.12.m12.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.12.m12.1c">I</annotation></semantics></math> and related questions <math id="S2.p1.13.m13.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p1.13.m13.1a"><mi id="S2.p1.13.m13.1.1" xref="S2.p1.13.m13.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p1.13.m13.1b"><ci id="S2.p1.13.m13.1.1.cmml" xref="S2.p1.13.m13.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.13.m13.1c">q</annotation></semantics></math>, as well as the answers <math id="S2.p1.14.m14.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.p1.14.m14.1a"><mi id="S2.p1.14.m14.1.1" xref="S2.p1.14.m14.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.p1.14.m14.1b"><ci id="S2.p1.14.m14.1.1.cmml" xref="S2.p1.14.m14.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.14.m14.1c">a</annotation></semantics></math> that a VQA model <math id="S2.p1.15.m15.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p1.15.m15.1a"><mi id="S2.p1.15.m15.1.1" xref="S2.p1.15.m15.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p1.15.m15.1b"><ci id="S2.p1.15.m15.1.1.cmml" xref="S2.p1.15.m15.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.15.m15.1c">M</annotation></semantics></math> returns to these questions, is demonstrated in Fig. <a href="#S2.F1" title="Figure 1 â€£ 2 Visual Question Answering (VQA) â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2303.02601/assets/images/horse.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="359" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Example of image and free-form questions retrieved from the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, targeted to the VQA task. The displayed answers were given as response by the ViLT model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</span></figcaption>
</figure>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.7" class="ltx_p">Our counterfactual approach regarding linguistic substitutions revolves around the following fundamental question: <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">"What is the response of <math id="S2.p2.1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p2.1.1.m1.1a"><mi id="S2.p2.1.1.m1.1.1" xref="S2.p2.1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.1.m1.1b"><ci id="S2.p2.1.1.m1.1.1.cmml" xref="S2.p2.1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.1.m1.1c">M</annotation></semantics></math> if we substitute word X with word Y in question q?"</span>. The implemented counterfactual <math id="S2.p2.2.m1.1" class="ltx_Math" alttext="X\rightarrow Y" display="inline"><semantics id="S2.p2.2.m1.1a"><mrow id="S2.p2.2.m1.1.1" xref="S2.p2.2.m1.1.1.cmml"><mi id="S2.p2.2.m1.1.1.2" xref="S2.p2.2.m1.1.1.2.cmml">X</mi><mo stretchy="false" id="S2.p2.2.m1.1.1.1" xref="S2.p2.2.m1.1.1.1.cmml">â†’</mo><mi id="S2.p2.2.m1.1.1.3" xref="S2.p2.2.m1.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.2.m1.1b"><apply id="S2.p2.2.m1.1.1.cmml" xref="S2.p2.2.m1.1.1"><ci id="S2.p2.2.m1.1.1.1.cmml" xref="S2.p2.2.m1.1.1.1">â†’</ci><ci id="S2.p2.2.m1.1.1.2.cmml" xref="S2.p2.2.m1.1.1.2">ğ‘‹</ci><ci id="S2.p2.2.m1.1.1.3.cmml" xref="S2.p2.2.m1.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m1.1c">X\rightarrow Y</annotation></semantics></math> substitution should be semantically <span id="S2.p2.7.2" class="ltx_text ltx_font_italic">minimal</span> and linguistically <span id="S2.p2.7.3" class="ltx_text ltx_font_italic">feasible</span>. <span id="S2.p2.7.4" class="ltx_text ltx_font_italic">Minimality</span> refers to substitutions that maintain a meaning close to the meaning of the original word <span id="S2.p2.7.5" class="ltx_text ltx_font_italic">X</span>. For example, synonym words preserve this minimality constraint. In order to ensure semantic minimality of substitutions, we leverage lexical knowledge sources (such as WordNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>), which can provide the minimum possible <math id="S2.p2.3.m2.1" class="ltx_Math" alttext="X\rightarrow Y" display="inline"><semantics id="S2.p2.3.m2.1a"><mrow id="S2.p2.3.m2.1.1" xref="S2.p2.3.m2.1.1.cmml"><mi id="S2.p2.3.m2.1.1.2" xref="S2.p2.3.m2.1.1.2.cmml">X</mi><mo stretchy="false" id="S2.p2.3.m2.1.1.1" xref="S2.p2.3.m2.1.1.1.cmml">â†’</mo><mi id="S2.p2.3.m2.1.1.3" xref="S2.p2.3.m2.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.3.m2.1b"><apply id="S2.p2.3.m2.1.1.cmml" xref="S2.p2.3.m2.1.1"><ci id="S2.p2.3.m2.1.1.1.cmml" xref="S2.p2.3.m2.1.1.1">â†’</ci><ci id="S2.p2.3.m2.1.1.2.cmml" xref="S2.p2.3.m2.1.1.2">ğ‘‹</ci><ci id="S2.p2.3.m2.1.1.3.cmml" xref="S2.p2.3.m2.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m2.1c">X\rightarrow Y</annotation></semantics></math> transitions by selecting the closest concept <math id="S2.p2.4.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S2.p2.4.m3.1a"><mi id="S2.p2.4.m3.1.1" xref="S2.p2.4.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m3.1b"><ci id="S2.p2.4.m3.1.1.cmml" xref="S2.p2.4.m3.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m3.1c">Y</annotation></semantics></math> to concept <math id="S2.p2.5.m4.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S2.p2.5.m4.1a"><mi id="S2.p2.5.m4.1.1" xref="S2.p2.5.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m4.1b"><ci id="S2.p2.5.m4.1.1.cmml" xref="S2.p2.5.m4.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m4.1c">X</annotation></semantics></math> that respects certain constrains. Linguistic <span id="S2.p2.7.6" class="ltx_text ltx_font_italic">feasibility</span> instructs meaningful substitutions which always involve the same part of speech (POS); for example, nouns can only be substituted by nouns but not by verbs. In total, such <math id="S2.p2.6.m5.1" class="ltx_Math" alttext="X\rightarrow Y" display="inline"><semantics id="S2.p2.6.m5.1a"><mrow id="S2.p2.6.m5.1.1" xref="S2.p2.6.m5.1.1.cmml"><mi id="S2.p2.6.m5.1.1.2" xref="S2.p2.6.m5.1.1.2.cmml">X</mi><mo stretchy="false" id="S2.p2.6.m5.1.1.1" xref="S2.p2.6.m5.1.1.1.cmml">â†’</mo><mi id="S2.p2.6.m5.1.1.3" xref="S2.p2.6.m5.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m5.1b"><apply id="S2.p2.6.m5.1.1.cmml" xref="S2.p2.6.m5.1.1"><ci id="S2.p2.6.m5.1.1.1.cmml" xref="S2.p2.6.m5.1.1.1">â†’</ci><ci id="S2.p2.6.m5.1.1.2.cmml" xref="S2.p2.6.m5.1.1.2">ğ‘‹</ci><ci id="S2.p2.6.m5.1.1.3.cmml" xref="S2.p2.6.m5.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m5.1c">X\rightarrow Y</annotation></semantics></math> substitutions are applied on the whole <math id="S2.p2.7.m6.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S2.p2.7.m6.1a"><mi id="S2.p2.7.m6.1.1" xref="S2.p2.7.m6.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m6.1b"><ci id="S2.p2.7.m6.1.1.cmml" xref="S2.p2.7.m6.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m6.1c">Q</annotation></semantics></math> set, targeting one POS at a time.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.5" class="ltx_p">Such counterfactual questions are able to trigger alternative model responses. Therefore, a <math id="S2.p3.1.m1.1" class="ltx_Math" alttext="X\rightarrow Y" display="inline"><semantics id="S2.p3.1.m1.1a"><mrow id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml"><mi id="S2.p3.1.m1.1.1.2" xref="S2.p3.1.m1.1.1.2.cmml">X</mi><mo stretchy="false" id="S2.p3.1.m1.1.1.1" xref="S2.p3.1.m1.1.1.1.cmml">â†’</mo><mi id="S2.p3.1.m1.1.1.3" xref="S2.p3.1.m1.1.1.3.cmml">Y</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><apply id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1"><ci id="S2.p3.1.m1.1.1.1.cmml" xref="S2.p3.1.m1.1.1.1">â†’</ci><ci id="S2.p3.1.m1.1.1.2.cmml" xref="S2.p3.1.m1.1.1.2">ğ‘‹</ci><ci id="S2.p3.1.m1.1.1.3.cmml" xref="S2.p3.1.m1.1.1.3">ğ‘Œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">X\rightarrow Y</annotation></semantics></math> concept substitution in the input may result in an alternative <math id="S2.p3.2.m2.1" class="ltx_Math" alttext="X^{\prime}\rightarrow Y^{\prime}" display="inline"><semantics id="S2.p3.2.m2.1a"><mrow id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml"><msup id="S2.p3.2.m2.1.1.2" xref="S2.p3.2.m2.1.1.2.cmml"><mi id="S2.p3.2.m2.1.1.2.2" xref="S2.p3.2.m2.1.1.2.2.cmml">X</mi><mo id="S2.p3.2.m2.1.1.2.3" xref="S2.p3.2.m2.1.1.2.3.cmml">â€²</mo></msup><mo stretchy="false" id="S2.p3.2.m2.1.1.1" xref="S2.p3.2.m2.1.1.1.cmml">â†’</mo><msup id="S2.p3.2.m2.1.1.3" xref="S2.p3.2.m2.1.1.3.cmml"><mi id="S2.p3.2.m2.1.1.3.2" xref="S2.p3.2.m2.1.1.3.2.cmml">Y</mi><mo id="S2.p3.2.m2.1.1.3.3" xref="S2.p3.2.m2.1.1.3.3.cmml">â€²</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><apply id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1"><ci id="S2.p3.2.m2.1.1.1.cmml" xref="S2.p3.2.m2.1.1.1">â†’</ci><apply id="S2.p3.2.m2.1.1.2.cmml" xref="S2.p3.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.2.1.cmml" xref="S2.p3.2.m2.1.1.2">superscript</csymbol><ci id="S2.p3.2.m2.1.1.2.2.cmml" xref="S2.p3.2.m2.1.1.2.2">ğ‘‹</ci><ci id="S2.p3.2.m2.1.1.2.3.cmml" xref="S2.p3.2.m2.1.1.2.3">â€²</ci></apply><apply id="S2.p3.2.m2.1.1.3.cmml" xref="S2.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.p3.2.m2.1.1.3.1.cmml" xref="S2.p3.2.m2.1.1.3">superscript</csymbol><ci id="S2.p3.2.m2.1.1.3.2.cmml" xref="S2.p3.2.m2.1.1.3.2">ğ‘Œ</ci><ci id="S2.p3.2.m2.1.1.3.3.cmml" xref="S2.p3.2.m2.1.1.3.3">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">X^{\prime}\rightarrow Y^{\prime}</annotation></semantics></math> response in the output, or not. Probing potential output changes is highly informative with respect to the reasoning process followed by the model <math id="S2.p3.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p3.3.m3.1a"><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.1b"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.1c">M</annotation></semantics></math>, highlighting concepts or concept families that are more or less influential to the decision-making process of <math id="S2.p3.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">M</annotation></semantics></math>. Hence, the counterfactual substitutions implemented on <math id="S2.p3.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.p3.5.m5.1a"><mi id="S2.p3.5.m5.1.1" xref="S2.p3.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.p3.5.m5.1b"><ci id="S2.p3.5.m5.1.1.cmml" xref="S2.p3.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.5.m5.1c">q</annotation></semantics></math> provide useful explanations for the modelâ€™s observed behavior and enhance its interpretability extent, while also handling it as a black-box structure.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Related work</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">VQA models</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">Since the introductory work on VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, several endeavors have extended this paradigm, either by suggesting advanced model architectures or by proposing more challenging datasets. State-of-the-art models addressing the VQA task are mainly based on VL transformer backbones; thus, models such as ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, VisualBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, FLAVA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, ALBEF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and others
have dominated the recent VQA literature demonstrating rapid improvements on relevant benchmark datasets.
Regarding datasets, improvements on the original VQA (VQA-v2) suggest adding similar image pairs corresponding to the same question <math id="S3.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">q</annotation></semantics></math>, but leading to diverging answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Visual Genome (VG) is another large scale dataset including numerous scene images, object, attribute and relationship annotations, as well as visual question-answer pairs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Our approach is tested on both VQA-v2 and VG datasets.
Other popular VQA datasets are Flickr30k-Entities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and others. For a detailed analysis on VQA and relevant topics we refer readers to recent specialized survey papers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Explainability in VQA</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">Regarding the research topic of explainability and robustness in Visual Question Answering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, multiple efforts have been proposed, including attention maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, and other model-specific approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. The strategy through counterfactuals is a rather new one, while already existing attempts focus on visual perturbations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, masking <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, introducing counterfactuals in the training stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> and relationship-driven approaches between original and counterfactual samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Linguistic perturbations</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">There is a variety of prior works that perform word-level linguistic perturbations, even though they target purely linguistic tasks, mostly text classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, but also semantic similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> and machine translation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Our perturbations regarding synonym replacement and random noun deletion are inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, guiding substitutions with the usage of WordNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Color perturbations are adapted from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, upon which we construct an appropriate hierarchy based on color distance. The rest of our implemented replacements involving noun and verb substitutions are completely novel ideas.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.2" class="ltx_p">The input of our framework consists of a dataset <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.p1.1.m1.1a"><mi id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><ci id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">D</annotation></semantics></math> that contains aligned images <span id="S4.p1.2.1" class="ltx_text ltx_font_italic">I</span>, textual questions forming a set <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S4.p1.2.m2.1a"><mi id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><ci id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">Q</annotation></semantics></math> and candidate textual answers <span id="S4.p1.2.2" class="ltx_text ltx_font_italic">A</span>. We will later present results on Visual Genome (VG) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and VQA-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, which satisfy these requirements.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.9" class="ltx_p">We select ViLT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> as a proof-of-concept pre-trained VQA model <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">M</annotation></semantics></math>. Nevertheless, our proposed method is not restricted to ViLT, as it only considers inputs (questions) and outputs (answers). ViLT receives a question <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">q</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">âˆˆ</mo><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><in id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></in><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ‘</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">q\in Q</annotation></semantics></math> and an image <math id="S4.p2.3.m3.1" class="ltx_Math" alttext="i\in I" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">i</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">âˆˆ</mo><mi id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">I</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><in id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></in><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">ğ‘–</ci><ci id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">ğ¼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">i\in I</annotation></semantics></math> from <math id="S4.p2.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.p2.4.m4.1a"><mi id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><ci id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">D</annotation></semantics></math> and then
generates an answer <math id="S4.p2.5.m5.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p2.5.m5.1a"><mi id="S4.p2.5.m5.1.1" xref="S4.p2.5.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p2.5.m5.1b"><ci id="S4.p2.5.m5.1.1.cmml" xref="S4.p2.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m5.1c">a</annotation></semantics></math>, rather than selecting one of the candidates <math id="S4.p2.6.m6.1" class="ltx_Math" alttext="a\in A" display="inline"><semantics id="S4.p2.6.m6.1a"><mrow id="S4.p2.6.m6.1.1" xref="S4.p2.6.m6.1.1.cmml"><mi id="S4.p2.6.m6.1.1.2" xref="S4.p2.6.m6.1.1.2.cmml">a</mi><mo id="S4.p2.6.m6.1.1.1" xref="S4.p2.6.m6.1.1.1.cmml">âˆˆ</mo><mi id="S4.p2.6.m6.1.1.3" xref="S4.p2.6.m6.1.1.3.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.6.m6.1b"><apply id="S4.p2.6.m6.1.1.cmml" xref="S4.p2.6.m6.1.1"><in id="S4.p2.6.m6.1.1.1.cmml" xref="S4.p2.6.m6.1.1.1"></in><ci id="S4.p2.6.m6.1.1.2.cmml" xref="S4.p2.6.m6.1.1.2">ğ‘</ci><ci id="S4.p2.6.m6.1.1.3.cmml" xref="S4.p2.6.m6.1.1.3">ğ´</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m6.1c">a\in A</annotation></semantics></math>; since this behavior is inherent to several VQA models, it is important to allow looser definitions of the accuracy metric. To be more precise, ViLT produces outputs <math id="S4.p2.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p2.7.m7.1a"><mi id="S4.p2.7.m7.1.1" xref="S4.p2.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m7.1b"><ci id="S4.p2.7.m7.1.1.cmml" xref="S4.p2.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m7.1c">a</annotation></semantics></math> in the form of natural language text, and there are many different ways of expressing the same answer in English.
In this case, heuristically comparing the generated answer with the ground truth answer from <span id="S4.p2.9.1" class="ltx_text ltx_font_italic">A</span> defines if the prediction of <math id="S4.p2.8.m8.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p2.8.m8.1a"><mi id="S4.p2.8.m8.1.1" xref="S4.p2.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p2.8.m8.1b"><ci id="S4.p2.8.m8.1.1.cmml" xref="S4.p2.8.m8.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m8.1c">M</annotation></semantics></math> is accurate or not. By repeating the same prediction process for all <span id="S4.p2.9.2" class="ltx_text ltx_font_italic">Q, I</span> pairs, and by obtaining successful or unsuccessful answers for them, we finally extract an accuracy score <math id="S4.p2.9.m9.1" class="ltx_Math" alttext="\textit{acc}_{Q}" display="inline"><semantics id="S4.p2.9.m9.1a"><msub id="S4.p2.9.m9.1.1" xref="S4.p2.9.m9.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.p2.9.m9.1.1.2" xref="S4.p2.9.m9.1.1.2a.cmml">acc</mtext><mi id="S4.p2.9.m9.1.1.3" xref="S4.p2.9.m9.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.9.m9.1b"><apply id="S4.p2.9.m9.1.1.cmml" xref="S4.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p2.9.m9.1.1.1.cmml" xref="S4.p2.9.m9.1.1">subscript</csymbol><ci id="S4.p2.9.m9.1.1.2a.cmml" xref="S4.p2.9.m9.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.p2.9.m9.1.1.2.cmml" xref="S4.p2.9.m9.1.1.2">acc</mtext></ci><ci id="S4.p2.9.m9.1.1.3.cmml" xref="S4.p2.9.m9.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m9.1c">\textit{acc}_{Q}</annotation></semantics></math>, reflecting the ratio of correct answers over all generated answers.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Our word substitutions are guided from external knowledge sources, targeting different parts of speech (nouns, verbs and adjectives) at a time. Specifically, WordNet knowledge graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> provides hierarchical relationships between an abundance of common words widely present in VG and VQA-v2 vocabularies. Therefore, substitution pairs are created by connecting specific words with their WordNet matches, respecting hierarchical relationships as described in Section <a href="#S4.SS1" title="4.1 Perturbations â€£ 4 Method â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
Furthermore, we extend the Matplotlib color<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://matplotlib.org/3.3.3/gallery/color/named_colors.html" title="" class="ltx_ref ltx_href">Matplotlib colors</a></span></span></span> relationships presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, forming a hierarchy of <span id="S4.p3.1.1" class="ltx_text ltx_font_italic">color relatedness</span>. This color hierarchy is based on color distances according to the RGB value of each Matplotlib color, with more details provided in Section <a href="#S4.SS1" title="4.1 Perturbations â€£ 4 Method â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.7</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2303.02601/assets/x1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="664" height="479" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">Overview of our proposed knowledge-based counterfactual VQA framework.</span></figcaption>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.9" class="ltx_p">We then proceed with applying our designed perturbations on dataset questions <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.p4.1.m1.1a"><mrow id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi id="S4.p4.1.m1.1.1.2" xref="S4.p4.1.m1.1.1.2.cmml">q</mi><mo id="S4.p4.1.m1.1.1.1" xref="S4.p4.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><in id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1.1"></in><ci id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1.2">ğ‘</ci><ci id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">q\in Q</annotation></semantics></math>, resulting in <span id="S4.p4.9.1" class="ltx_text ltx_font_italic">counterfactual questions</span> <math id="S4.p4.2.m2.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S4.p4.2.m2.1a"><mrow id="S4.p4.2.m2.1.1" xref="S4.p4.2.m2.1.1.cmml"><msup id="S4.p4.2.m2.1.1.2" xref="S4.p4.2.m2.1.1.2.cmml"><mi id="S4.p4.2.m2.1.1.2.2" xref="S4.p4.2.m2.1.1.2.2.cmml">q</mi><mo id="S4.p4.2.m2.1.1.2.3" xref="S4.p4.2.m2.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S4.p4.2.m2.1.1.1" xref="S4.p4.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S4.p4.2.m2.1.1.3" xref="S4.p4.2.m2.1.1.3.cmml"><mi id="S4.p4.2.m2.1.1.3.2" xref="S4.p4.2.m2.1.1.3.2.cmml">Q</mi><mo id="S4.p4.2.m2.1.1.3.3" xref="S4.p4.2.m2.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.2.m2.1b"><apply id="S4.p4.2.m2.1.1.cmml" xref="S4.p4.2.m2.1.1"><in id="S4.p4.2.m2.1.1.1.cmml" xref="S4.p4.2.m2.1.1.1"></in><apply id="S4.p4.2.m2.1.1.2.cmml" xref="S4.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.2.1.cmml" xref="S4.p4.2.m2.1.1.2">superscript</csymbol><ci id="S4.p4.2.m2.1.1.2.2.cmml" xref="S4.p4.2.m2.1.1.2.2">ğ‘</ci><times id="S4.p4.2.m2.1.1.2.3.cmml" xref="S4.p4.2.m2.1.1.2.3"></times></apply><apply id="S4.p4.2.m2.1.1.3.cmml" xref="S4.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p4.2.m2.1.1.3.1.cmml" xref="S4.p4.2.m2.1.1.3">superscript</csymbol><ci id="S4.p4.2.m2.1.1.3.2.cmml" xref="S4.p4.2.m2.1.1.3.2">ğ‘„</ci><times id="S4.p4.2.m2.1.1.3.3.cmml" xref="S4.p4.2.m2.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.2.m2.1c">q^{*}\in Q^{*}</annotation></semantics></math>.
For each substitution, we obtain the <span id="S4.p4.9.2" class="ltx_text ltx_font_italic">counterfactual question accuracy</span> <math id="S4.p4.3.m3.1" class="ltx_Math" alttext="\textit{acc}^{*}_{Q}" display="inline"><semantics id="S4.p4.3.m3.1a"><msubsup id="S4.p4.3.m3.1.1" xref="S4.p4.3.m3.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.p4.3.m3.1.1.2.2" xref="S4.p4.3.m3.1.1.2.2a.cmml">acc</mtext><mi id="S4.p4.3.m3.1.1.3" xref="S4.p4.3.m3.1.1.3.cmml">Q</mi><mo id="S4.p4.3.m3.1.1.2.3" xref="S4.p4.3.m3.1.1.2.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.p4.3.m3.1b"><apply id="S4.p4.3.m3.1.1.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.1.1.cmml" xref="S4.p4.3.m3.1.1">subscript</csymbol><apply id="S4.p4.3.m3.1.1.2.cmml" xref="S4.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p4.3.m3.1.1.2.1.cmml" xref="S4.p4.3.m3.1.1">superscript</csymbol><ci id="S4.p4.3.m3.1.1.2.2a.cmml" xref="S4.p4.3.m3.1.1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.p4.3.m3.1.1.2.2.cmml" xref="S4.p4.3.m3.1.1.2.2">acc</mtext></ci><times id="S4.p4.3.m3.1.1.2.3.cmml" xref="S4.p4.3.m3.1.1.2.3"></times></apply><ci id="S4.p4.3.m3.1.1.3.cmml" xref="S4.p4.3.m3.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.3.m3.1c">\textit{acc}^{*}_{Q}</annotation></semantics></math>, as the response of <math id="S4.p4.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p4.4.m4.1a"><mi id="S4.p4.4.m4.1.1" xref="S4.p4.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p4.4.m4.1b"><ci id="S4.p4.4.m4.1.1.cmml" xref="S4.p4.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.4.m4.1c">M</annotation></semantics></math> to the counterfactual questions <math id="S4.p4.5.m5.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S4.p4.5.m5.1a"><mrow id="S4.p4.5.m5.1.1" xref="S4.p4.5.m5.1.1.cmml"><msup id="S4.p4.5.m5.1.1.2" xref="S4.p4.5.m5.1.1.2.cmml"><mi id="S4.p4.5.m5.1.1.2.2" xref="S4.p4.5.m5.1.1.2.2.cmml">q</mi><mo id="S4.p4.5.m5.1.1.2.3" xref="S4.p4.5.m5.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S4.p4.5.m5.1.1.1" xref="S4.p4.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S4.p4.5.m5.1.1.3" xref="S4.p4.5.m5.1.1.3.cmml"><mi id="S4.p4.5.m5.1.1.3.2" xref="S4.p4.5.m5.1.1.3.2.cmml">Q</mi><mo id="S4.p4.5.m5.1.1.3.3" xref="S4.p4.5.m5.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p4.5.m5.1b"><apply id="S4.p4.5.m5.1.1.cmml" xref="S4.p4.5.m5.1.1"><in id="S4.p4.5.m5.1.1.1.cmml" xref="S4.p4.5.m5.1.1.1"></in><apply id="S4.p4.5.m5.1.1.2.cmml" xref="S4.p4.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.p4.5.m5.1.1.2.1.cmml" xref="S4.p4.5.m5.1.1.2">superscript</csymbol><ci id="S4.p4.5.m5.1.1.2.2.cmml" xref="S4.p4.5.m5.1.1.2.2">ğ‘</ci><times id="S4.p4.5.m5.1.1.2.3.cmml" xref="S4.p4.5.m5.1.1.2.3"></times></apply><apply id="S4.p4.5.m5.1.1.3.cmml" xref="S4.p4.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.p4.5.m5.1.1.3.1.cmml" xref="S4.p4.5.m5.1.1.3">superscript</csymbol><ci id="S4.p4.5.m5.1.1.3.2.cmml" xref="S4.p4.5.m5.1.1.3.2">ğ‘„</ci><times id="S4.p4.5.m5.1.1.3.3.cmml" xref="S4.p4.5.m5.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.5.m5.1c">q^{*}\in Q^{*}</annotation></semantics></math>, and we compare it to the ground truth accuracy scores <math id="S4.p4.6.m6.1" class="ltx_Math" alttext="\textit{acc}_{Q}" display="inline"><semantics id="S4.p4.6.m6.1a"><msub id="S4.p4.6.m6.1.1" xref="S4.p4.6.m6.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.p4.6.m6.1.1.2" xref="S4.p4.6.m6.1.1.2a.cmml">acc</mtext><mi id="S4.p4.6.m6.1.1.3" xref="S4.p4.6.m6.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.6.m6.1b"><apply id="S4.p4.6.m6.1.1.cmml" xref="S4.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S4.p4.6.m6.1.1.1.cmml" xref="S4.p4.6.m6.1.1">subscript</csymbol><ci id="S4.p4.6.m6.1.1.2a.cmml" xref="S4.p4.6.m6.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.p4.6.m6.1.1.2.cmml" xref="S4.p4.6.m6.1.1.2">acc</mtext></ci><ci id="S4.p4.6.m6.1.1.3.cmml" xref="S4.p4.6.m6.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.6.m6.1c">\textit{acc}_{Q}</annotation></semantics></math>.
Throughout this process, we evaluate whether and how the response of <math id="S4.p4.7.m7.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.p4.7.m7.1a"><mi id="S4.p4.7.m7.1.1" xref="S4.p4.7.m7.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.p4.7.m7.1b"><ci id="S4.p4.7.m7.1.1.cmml" xref="S4.p4.7.m7.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.7.m7.1c">M</annotation></semantics></math> changes by measuring the difference between <math id="S4.p4.8.m8.1" class="ltx_Math" alttext="\textit{acc}_{Q}" display="inline"><semantics id="S4.p4.8.m8.1a"><msub id="S4.p4.8.m8.1.1" xref="S4.p4.8.m8.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.p4.8.m8.1.1.2" xref="S4.p4.8.m8.1.1.2a.cmml">acc</mtext><mi id="S4.p4.8.m8.1.1.3" xref="S4.p4.8.m8.1.1.3.cmml">Q</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p4.8.m8.1b"><apply id="S4.p4.8.m8.1.1.cmml" xref="S4.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S4.p4.8.m8.1.1.1.cmml" xref="S4.p4.8.m8.1.1">subscript</csymbol><ci id="S4.p4.8.m8.1.1.2a.cmml" xref="S4.p4.8.m8.1.1.2"><mtext class="ltx_mathvariant_italic" id="S4.p4.8.m8.1.1.2.cmml" xref="S4.p4.8.m8.1.1.2">acc</mtext></ci><ci id="S4.p4.8.m8.1.1.3.cmml" xref="S4.p4.8.m8.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.8.m8.1c">\textit{acc}_{Q}</annotation></semantics></math> and <math id="S4.p4.9.m9.1" class="ltx_Math" alttext="\textit{acc}^{*}_{Q}" display="inline"><semantics id="S4.p4.9.m9.1a"><msubsup id="S4.p4.9.m9.1.1" xref="S4.p4.9.m9.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S4.p4.9.m9.1.1.2.2" xref="S4.p4.9.m9.1.1.2.2a.cmml">acc</mtext><mi id="S4.p4.9.m9.1.1.3" xref="S4.p4.9.m9.1.1.3.cmml">Q</mi><mo id="S4.p4.9.m9.1.1.2.3" xref="S4.p4.9.m9.1.1.2.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.p4.9.m9.1b"><apply id="S4.p4.9.m9.1.1.cmml" xref="S4.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p4.9.m9.1.1.1.cmml" xref="S4.p4.9.m9.1.1">subscript</csymbol><apply id="S4.p4.9.m9.1.1.2.cmml" xref="S4.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S4.p4.9.m9.1.1.2.1.cmml" xref="S4.p4.9.m9.1.1">superscript</csymbol><ci id="S4.p4.9.m9.1.1.2.2a.cmml" xref="S4.p4.9.m9.1.1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.p4.9.m9.1.1.2.2.cmml" xref="S4.p4.9.m9.1.1.2.2">acc</mtext></ci><times id="S4.p4.9.m9.1.1.2.3.cmml" xref="S4.p4.9.m9.1.1.2.3"></times></apply><ci id="S4.p4.9.m9.1.1.3.cmml" xref="S4.p4.9.m9.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.9.m9.1c">\textit{acc}^{*}_{Q}</annotation></semantics></math>, as an indicator of its robustness against replacements with semantically related concepts.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.4" class="ltx_p">Even though useful for benchmarking reasons, standalone accuracy scores are not informative enough to explain <span id="S4.p5.4.1" class="ltx_text ltx_font_italic">why</span> we observe such differences between original <math id="S4.p5.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.p5.1.m1.1a"><mi id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">q</annotation></semantics></math> and counterfactual inputs <math id="S4.p5.2.m2.1" class="ltx_Math" alttext="q^{*}" display="inline"><semantics id="S4.p5.2.m2.1a"><msup id="S4.p5.2.m2.1.1" xref="S4.p5.2.m2.1.1.cmml"><mi id="S4.p5.2.m2.1.1.2" xref="S4.p5.2.m2.1.1.2.cmml">q</mi><mo id="S4.p5.2.m2.1.1.3" xref="S4.p5.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p5.2.m2.1b"><apply id="S4.p5.2.m2.1.1.cmml" xref="S4.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p5.2.m2.1.1.1.cmml" xref="S4.p5.2.m2.1.1">superscript</csymbol><ci id="S4.p5.2.m2.1.1.2.cmml" xref="S4.p5.2.m2.1.1.2">ğ‘</ci><times id="S4.p5.2.m2.1.1.3.cmml" xref="S4.p5.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.2.m2.1c">q^{*}</annotation></semantics></math>. To this end, we separately examine samples where the generated <math id="S4.p5.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.p5.3.m3.1a"><mi id="S4.p5.3.m3.1.1" xref="S4.p5.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.p5.3.m3.1b"><ci id="S4.p5.3.m3.1.1.cmml" xref="S4.p5.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.3.m3.1c">a</annotation></semantics></math> changes under the presence of a perturbation, obtaining local explanation in the form <span id="S4.p5.4.2" class="ltx_text ltx_font_italic">if concept changes in q, then a -erroneously- changes</span>. The aggregation of such local rules leads to <span id="S4.p5.4.3" class="ltx_text ltx_font_italic">global explanations</span>, deriving <span id="S4.p5.4.4" class="ltx_text ltx_font_italic">if-then</span> relationships that apply to multiple samples of <math id="S4.p5.4.m4.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S4.p5.4.m4.1a"><mi id="S4.p5.4.m4.1.1" xref="S4.p5.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S4.p5.4.m4.1b"><ci id="S4.p5.4.m4.1.1.cmml" xref="S4.p5.4.m4.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.4.m4.1c">D</annotation></semantics></math>.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.2" class="ltx_p">We present a visual outline of our approach in Figure <a href="#S4.F2" title="Figure 2 â€£ 4 Method â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Words from <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.p6.1.m1.1a"><mrow id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi id="S4.p6.1.m1.1.1.2" xref="S4.p6.1.m1.1.1.2.cmml">q</mi><mo id="S4.p6.1.m1.1.1.1" xref="S4.p6.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><in id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1.1"></in><ci id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1.2">ğ‘</ci><ci id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">q\in Q</annotation></semantics></math> colored in <span id="S4.p6.2.1" class="ltx_text" style="color:#FF0000;">red</span> denote the concepts to be substituted, while words in <span id="S4.p6.2.2" class="ltx_text" style="color:#0000FF;">blue</span> indicate the knowledge-driven substitutions that lead to counterfactual questions <math id="S4.p6.2.m2.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S4.p6.2.m2.1a"><mrow id="S4.p6.2.m2.1.1" xref="S4.p6.2.m2.1.1.cmml"><msup id="S4.p6.2.m2.1.1.2" xref="S4.p6.2.m2.1.1.2.cmml"><mi id="S4.p6.2.m2.1.1.2.2" xref="S4.p6.2.m2.1.1.2.2.cmml">q</mi><mo id="S4.p6.2.m2.1.1.2.3" xref="S4.p6.2.m2.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S4.p6.2.m2.1.1.1" xref="S4.p6.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S4.p6.2.m2.1.1.3" xref="S4.p6.2.m2.1.1.3.cmml"><mi id="S4.p6.2.m2.1.1.3.2" xref="S4.p6.2.m2.1.1.3.2.cmml">Q</mi><mo id="S4.p6.2.m2.1.1.3.3" xref="S4.p6.2.m2.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.p6.2.m2.1b"><apply id="S4.p6.2.m2.1.1.cmml" xref="S4.p6.2.m2.1.1"><in id="S4.p6.2.m2.1.1.1.cmml" xref="S4.p6.2.m2.1.1.1"></in><apply id="S4.p6.2.m2.1.1.2.cmml" xref="S4.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.2.1.cmml" xref="S4.p6.2.m2.1.1.2">superscript</csymbol><ci id="S4.p6.2.m2.1.1.2.2.cmml" xref="S4.p6.2.m2.1.1.2.2">ğ‘</ci><times id="S4.p6.2.m2.1.1.2.3.cmml" xref="S4.p6.2.m2.1.1.2.3"></times></apply><apply id="S4.p6.2.m2.1.1.3.cmml" xref="S4.p6.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.p6.2.m2.1.1.3.1.cmml" xref="S4.p6.2.m2.1.1.3">superscript</csymbol><ci id="S4.p6.2.m2.1.1.3.2.cmml" xref="S4.p6.2.m2.1.1.3.2">ğ‘„</ci><times id="S4.p6.2.m2.1.1.3.3.cmml" xref="S4.p6.2.m2.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.2.m2.1c">q^{*}\in Q^{*}</annotation></semantics></math>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Perturbations</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.4" class="ltx_p">In our work, we perform a variety of substitutions or deletions in the linguistic representation of <span id="S4.SS1.p1.4.1" class="ltx_text ltx_font_italic">Q</span>. This counterfactual strategy exploits multiple and diverse morphological attributes of <span id="S4.SS1.p1.4.2" class="ltx_text ltx_font_italic">Q</span> and attempts to demonstrate the semantics that most affect the modelâ€™s response <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">a</annotation></semantics></math>. Our target is to stimulate an altered response of <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">M</annotation></semantics></math> through these counterfactual perturbations, in order to identify how the behavior of <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">M</annotation></semantics></math> changes when faced with different concepts. Thus, we can infer potential biases or points of weak robustness in <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mi id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><ci id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">M</annotation></semantics></math>. The aforementioned substitutions can be divided into the following categories, based on the knowledge source used and the targeted part of speech. A summary and representative examples of substitutions are provided in Table <a href="#S4.T1" title="Table 1 â€£ 4.1 Perturbations â€£ 4 Method â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">A. Wordnet hierarchy:</span> The knowledge-driven word substitutions involve replacing a noun word from questions <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">q</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><in id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></in><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ‘</ci><ci id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">q\in Q</annotation></semantics></math> with a hierarchically related word (<span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_italic">hyponym</span>, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_italic">hypernym</span>, <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_italic">sibling</span>), or verbs and adjectives with their <span id="S4.SS1.p2.1.5" class="ltx_text ltx_font_italic">synonyms</span>. The quality and relevance of our substitutions are reassured by the use of the deterministic structure of the Wordnet hierarchy, guaranteeing controllable and optimal word-level substitutions.</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><span id="S4.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Synonyms:</span> We employ synonym transformations on adjectives and verbs of the original questions <math id="S4.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.I1.i1.p1.1.m1.1a"><mrow id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml"><mi id="S4.I1.i1.p1.1.m1.1.1.2" xref="S4.I1.i1.p1.1.m1.1.1.2.cmml">q</mi><mo id="S4.I1.i1.p1.1.m1.1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S4.I1.i1.p1.1.m1.1.1.3" xref="S4.I1.i1.p1.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><apply id="S4.I1.i1.p1.1.m1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1"><in id="S4.I1.i1.p1.1.m1.1.1.1.cmml" xref="S4.I1.i1.p1.1.m1.1.1.1"></in><ci id="S4.I1.i1.p1.1.m1.1.1.2.cmml" xref="S4.I1.i1.p1.1.m1.1.1.2">ğ‘</ci><ci id="S4.I1.i1.p1.1.m1.1.1.3.cmml" xref="S4.I1.i1.p1.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">q\in Q</annotation></semantics></math>. For example, "talk" and "speak" are <span id="S4.I1.i1.p1.1.2" class="ltx_text ltx_font_italic">synonym verbs</span> according to WordNet, while "small" and "minuscule" are <span id="S4.I1.i1.p1.1.3" class="ltx_text ltx_font_italic">adjective synonyms</span>.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Hypernyms - Hyponyms:</span> More <span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">general</span>, as well as more <span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">specific</span> noun concepts are provided via WordNet in the form of <span id="S4.I1.i2.p1.1.4" class="ltx_text ltx_font_italic">hypernyms</span> and <span id="S4.I1.i2.p1.1.5" class="ltx_text ltx_font_italic">hyponyms</span> respectively.
For example, a given noun word (e.g. "dog") we can extract its immediate noun hypernyms (e.g. "canine"), or its immediate hyponyms (e.g. "labrador").</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Siblings:</span> We construct noun <span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">sibling</span> substitutions by traversing the Wordnet knowledge tree one step upwards and then one step downwards. Siblings are defined as noun entities that share the same immediate parent. For example, "carrot" and "radish" are siblings, because they both have "plant root" as their parent concept according to WordNet.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">B. Color relatedness hierarchy:</span>
Colors that are semantically similar, therefore presenting RGB values close to each other, will be also close within the color relatedness hierarchy. For example, "violet" and "orchid" Matplotlib colors lie close within the color hierarchy (their in-between color distance is 6.16), while "violet" and "deepskyblue" are placed far away from each other (their color distance is 207.88). Colors can be replaced with either distant or else similar colors from this color relatedness hierarchy, leading to the following <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_bold">Color Maximal</span> and <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_bold">Color Minimal</span> color substitutions. Both Maximal/Minimal substitutions may either involve <span id="S4.SS1.p3.1.4" class="ltx_text ltx_font_italic">common</span> colors, which already exist in the dataset or else <span id="S4.SS1.p3.1.5" class="ltx_text ltx_font_italic">uncommon</span> colors, which belong to the Matplotlib color list but not in VG/VQA-v2 vocabularies:</p>
<ul id="S4.I2" class="ltx_itemize">
<li id="S4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i1.p1" class="ltx_para">
<p id="S4.I2.i1.p1.8" class="ltx_p"><span id="S4.I2.i1.p1.8.1" class="ltx_text ltx_font_bold">Color Maximal:</span> On questions that mention some specific color we contradict the output <math id="S4.I2.i1.p1.1.m1.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S4.I2.i1.p1.1.m1.1a"><mi id="S4.I2.i1.p1.1.m1.1.1" xref="S4.I2.i1.p1.1.m1.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.1.m1.1b"><ci id="S4.I2.i1.p1.1.m1.1.1.cmml" xref="S4.I2.i1.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.1.m1.1c">a</annotation></semantics></math> of <math id="S4.I2.i1.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.I2.i1.p1.2.m2.1a"><mi id="S4.I2.i1.p1.2.m2.1.1" xref="S4.I2.i1.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.2.m2.1b"><ci id="S4.I2.i1.p1.2.m2.1.1.cmml" xref="S4.I2.i1.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.2.m2.1c">M</annotation></semantics></math> based on the input of the original question <math id="S4.I2.i1.p1.3.m3.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.I2.i1.p1.3.m3.1a"><mrow id="S4.I2.i1.p1.3.m3.1.1" xref="S4.I2.i1.p1.3.m3.1.1.cmml"><mi id="S4.I2.i1.p1.3.m3.1.1.2" xref="S4.I2.i1.p1.3.m3.1.1.2.cmml">q</mi><mo id="S4.I2.i1.p1.3.m3.1.1.1" xref="S4.I2.i1.p1.3.m3.1.1.1.cmml">âˆˆ</mo><mi id="S4.I2.i1.p1.3.m3.1.1.3" xref="S4.I2.i1.p1.3.m3.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.3.m3.1b"><apply id="S4.I2.i1.p1.3.m3.1.1.cmml" xref="S4.I2.i1.p1.3.m3.1.1"><in id="S4.I2.i1.p1.3.m3.1.1.1.cmml" xref="S4.I2.i1.p1.3.m3.1.1.1"></in><ci id="S4.I2.i1.p1.3.m3.1.1.2.cmml" xref="S4.I2.i1.p1.3.m3.1.1.2">ğ‘</ci><ci id="S4.I2.i1.p1.3.m3.1.1.3.cmml" xref="S4.I2.i1.p1.3.m3.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.3.m3.1c">q\in Q</annotation></semantics></math> vs the output <math id="S4.I2.i1.p1.4.m4.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S4.I2.i1.p1.4.m4.1a"><msup id="S4.I2.i1.p1.4.m4.1.1" xref="S4.I2.i1.p1.4.m4.1.1.cmml"><mi id="S4.I2.i1.p1.4.m4.1.1.2" xref="S4.I2.i1.p1.4.m4.1.1.2.cmml">a</mi><mo id="S4.I2.i1.p1.4.m4.1.1.3" xref="S4.I2.i1.p1.4.m4.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.4.m4.1b"><apply id="S4.I2.i1.p1.4.m4.1.1.cmml" xref="S4.I2.i1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.I2.i1.p1.4.m4.1.1.1.cmml" xref="S4.I2.i1.p1.4.m4.1.1">superscript</csymbol><ci id="S4.I2.i1.p1.4.m4.1.1.2.cmml" xref="S4.I2.i1.p1.4.m4.1.1.2">ğ‘</ci><times id="S4.I2.i1.p1.4.m4.1.1.3.cmml" xref="S4.I2.i1.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.4.m4.1c">a^{*}</annotation></semantics></math> of the perturbed question <math id="S4.I2.i1.p1.5.m5.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S4.I2.i1.p1.5.m5.1a"><mrow id="S4.I2.i1.p1.5.m5.1.1" xref="S4.I2.i1.p1.5.m5.1.1.cmml"><msup id="S4.I2.i1.p1.5.m5.1.1.2" xref="S4.I2.i1.p1.5.m5.1.1.2.cmml"><mi id="S4.I2.i1.p1.5.m5.1.1.2.2" xref="S4.I2.i1.p1.5.m5.1.1.2.2.cmml">q</mi><mo id="S4.I2.i1.p1.5.m5.1.1.2.3" xref="S4.I2.i1.p1.5.m5.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S4.I2.i1.p1.5.m5.1.1.1" xref="S4.I2.i1.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S4.I2.i1.p1.5.m5.1.1.3" xref="S4.I2.i1.p1.5.m5.1.1.3.cmml"><mi id="S4.I2.i1.p1.5.m5.1.1.3.2" xref="S4.I2.i1.p1.5.m5.1.1.3.2.cmml">Q</mi><mo id="S4.I2.i1.p1.5.m5.1.1.3.3" xref="S4.I2.i1.p1.5.m5.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.5.m5.1b"><apply id="S4.I2.i1.p1.5.m5.1.1.cmml" xref="S4.I2.i1.p1.5.m5.1.1"><in id="S4.I2.i1.p1.5.m5.1.1.1.cmml" xref="S4.I2.i1.p1.5.m5.1.1.1"></in><apply id="S4.I2.i1.p1.5.m5.1.1.2.cmml" xref="S4.I2.i1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.I2.i1.p1.5.m5.1.1.2.1.cmml" xref="S4.I2.i1.p1.5.m5.1.1.2">superscript</csymbol><ci id="S4.I2.i1.p1.5.m5.1.1.2.2.cmml" xref="S4.I2.i1.p1.5.m5.1.1.2.2">ğ‘</ci><times id="S4.I2.i1.p1.5.m5.1.1.2.3.cmml" xref="S4.I2.i1.p1.5.m5.1.1.2.3"></times></apply><apply id="S4.I2.i1.p1.5.m5.1.1.3.cmml" xref="S4.I2.i1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.I2.i1.p1.5.m5.1.1.3.1.cmml" xref="S4.I2.i1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S4.I2.i1.p1.5.m5.1.1.3.2.cmml" xref="S4.I2.i1.p1.5.m5.1.1.3.2">ğ‘„</ci><times id="S4.I2.i1.p1.5.m5.1.1.3.3.cmml" xref="S4.I2.i1.p1.5.m5.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.5.m5.1c">q^{*}\in Q^{*}</annotation></semantics></math>. In <math id="S4.I2.i1.p1.6.m6.1" class="ltx_Math" alttext="q^{*}" display="inline"><semantics id="S4.I2.i1.p1.6.m6.1a"><msup id="S4.I2.i1.p1.6.m6.1.1" xref="S4.I2.i1.p1.6.m6.1.1.cmml"><mi id="S4.I2.i1.p1.6.m6.1.1.2" xref="S4.I2.i1.p1.6.m6.1.1.2.cmml">q</mi><mo id="S4.I2.i1.p1.6.m6.1.1.3" xref="S4.I2.i1.p1.6.m6.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.6.m6.1b"><apply id="S4.I2.i1.p1.6.m6.1.1.cmml" xref="S4.I2.i1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.I2.i1.p1.6.m6.1.1.1.cmml" xref="S4.I2.i1.p1.6.m6.1.1">superscript</csymbol><ci id="S4.I2.i1.p1.6.m6.1.1.2.cmml" xref="S4.I2.i1.p1.6.m6.1.1.2">ğ‘</ci><times id="S4.I2.i1.p1.6.m6.1.1.3.cmml" xref="S4.I2.i1.p1.6.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.6.m6.1c">q^{*}</annotation></semantics></math> the original color is substituted with one that is greatly distant to it, such as "violet" <math id="S4.I2.i1.p1.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.I2.i1.p1.7.m7.1a"><mo stretchy="false" id="S4.I2.i1.p1.7.m7.1.1" xref="S4.I2.i1.p1.7.m7.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.7.m7.1b"><ci id="S4.I2.i1.p1.7.m7.1.1.cmml" xref="S4.I2.i1.p1.7.m7.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.7.m7.1c">\rightarrow</annotation></semantics></math> "deepskyblue". In this category, we also challenge the model using less frequent color instances (i.e. "azure", "turquoise", "salmon"). This substitution diverges from the initial counterfactual question requesting <span id="S4.I2.i1.p1.8.2" class="ltx_text ltx_font_italic">minimal</span> changes; nevertheless, the comparison with related <span id="S4.I2.i1.p1.8.3" class="ltx_text ltx_font_italic">minimal</span> changes will highlight the differences that varying color distances impose on the final <math id="S4.I2.i1.p1.8.m8.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S4.I2.i1.p1.8.m8.1a"><mrow id="S4.I2.i1.p1.8.m8.1.1" xref="S4.I2.i1.p1.8.m8.1.1.cmml"><mi id="S4.I2.i1.p1.8.m8.1.1.2" xref="S4.I2.i1.p1.8.m8.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.I2.i1.p1.8.m8.1.1.1" xref="S4.I2.i1.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S4.I2.i1.p1.8.m8.1.1.3" xref="S4.I2.i1.p1.8.m8.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.I2.i1.p1.8.m8.1.1.1a" xref="S4.I2.i1.p1.8.m8.1.1.1.cmml">â€‹</mo><msubsup id="S4.I2.i1.p1.8.m8.1.1.4" xref="S4.I2.i1.p1.8.m8.1.1.4.cmml"><mi id="S4.I2.i1.p1.8.m8.1.1.4.2.2" xref="S4.I2.i1.p1.8.m8.1.1.4.2.2.cmml">c</mi><mi id="S4.I2.i1.p1.8.m8.1.1.4.3" xref="S4.I2.i1.p1.8.m8.1.1.4.3.cmml">Q</mi><mo id="S4.I2.i1.p1.8.m8.1.1.4.2.3" xref="S4.I2.i1.p1.8.m8.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.I2.i1.p1.8.m8.1b"><apply id="S4.I2.i1.p1.8.m8.1.1.cmml" xref="S4.I2.i1.p1.8.m8.1.1"><times id="S4.I2.i1.p1.8.m8.1.1.1.cmml" xref="S4.I2.i1.p1.8.m8.1.1.1"></times><ci id="S4.I2.i1.p1.8.m8.1.1.2.cmml" xref="S4.I2.i1.p1.8.m8.1.1.2">ğ‘</ci><ci id="S4.I2.i1.p1.8.m8.1.1.3.cmml" xref="S4.I2.i1.p1.8.m8.1.1.3">ğ‘</ci><apply id="S4.I2.i1.p1.8.m8.1.1.4.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4"><csymbol cd="ambiguous" id="S4.I2.i1.p1.8.m8.1.1.4.1.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4">subscript</csymbol><apply id="S4.I2.i1.p1.8.m8.1.1.4.2.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4"><csymbol cd="ambiguous" id="S4.I2.i1.p1.8.m8.1.1.4.2.1.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4">superscript</csymbol><ci id="S4.I2.i1.p1.8.m8.1.1.4.2.2.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4.2.2">ğ‘</ci><times id="S4.I2.i1.p1.8.m8.1.1.4.2.3.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4.2.3"></times></apply><ci id="S4.I2.i1.p1.8.m8.1.1.4.3.cmml" xref="S4.I2.i1.p1.8.m8.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i1.p1.8.m8.1c">acc^{*}_{Q}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I2.i2.p1" class="ltx_para">
<p id="S4.I2.i2.p1.1" class="ltx_p"><span id="S4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Color Minimal:</span> In accordance with the above, we perform color substitutions with the least distant colors, such as "violet" <math id="S4.I2.i2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.I2.i2.p1.1.m1.1a"><mo stretchy="false" id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><ci id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">\rightarrow</annotation></semantics></math> "orchid". Again we also challenge the model with less frequent color substitutions.</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">C. Deletions:</span> We randomly select a noun in each question <math id="S4.SS1.p4.1.m1.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S4.SS1.p4.1.m1.1a"><mrow id="S4.SS1.p4.1.m1.1.1" xref="S4.SS1.p4.1.m1.1.1.cmml"><mi id="S4.SS1.p4.1.m1.1.1.2" xref="S4.SS1.p4.1.m1.1.1.2.cmml">q</mi><mo id="S4.SS1.p4.1.m1.1.1.1" xref="S4.SS1.p4.1.m1.1.1.1.cmml">âˆˆ</mo><mi id="S4.SS1.p4.1.m1.1.1.3" xref="S4.SS1.p4.1.m1.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p4.1.m1.1b"><apply id="S4.SS1.p4.1.m1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1"><in id="S4.SS1.p4.1.m1.1.1.1.cmml" xref="S4.SS1.p4.1.m1.1.1.1"></in><ci id="S4.SS1.p4.1.m1.1.1.2.cmml" xref="S4.SS1.p4.1.m1.1.1.2">ğ‘</ci><ci id="S4.SS1.p4.1.m1.1.1.3.cmml" xref="S4.SS1.p4.1.m1.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p4.1.m1.1c">q\in Q</annotation></semantics></math> and remove it.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.3.2" class="ltx_text" style="font-size:90%;">Question perturbations examples towards counterfactual queries.</span></figcaption>
<table id="S4.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.4.1" class="ltx_tr">
<td id="S4.T1.4.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Perturbation</td>
<td id="S4.T1.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">Question</td>
</tr>
<tr id="S4.T1.4.2" class="ltx_tr">
<td id="S4.T1.4.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.4.2.1.1" class="ltx_text ltx_font_bold">Original</span></td>
<td id="S4.T1.4.2.2" class="ltx_td ltx_align_center ltx_border_t">Do you see the white small dog?</td>
</tr>
<tr id="S4.T1.4.3" class="ltx_tr">
<td id="S4.T1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.3.1.1" class="ltx_text ltx_font_bold">Color Maximal</span></td>
<td id="S4.T1.4.3.2" class="ltx_td ltx_align_center">Do you see the <span id="S4.T1.4.3.2.1" class="ltx_text ltx_font_bold">black</span> small dog ?</td>
</tr>
<tr id="S4.T1.4.4" class="ltx_tr">
<td id="S4.T1.4.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.4.1.1" class="ltx_text ltx_font_bold">Color Minimal</span></td>
<td id="S4.T1.4.4.2" class="ltx_td ltx_align_center">Do you see the <span id="S4.T1.4.4.2.1" class="ltx_text ltx_font_bold">beige</span> small dog ?</td>
</tr>
<tr id="S4.T1.4.5" class="ltx_tr">
<td id="S4.T1.4.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.5.1.1" class="ltx_text ltx_font_bold">Synonym Adjectives</span></td>
<td id="S4.T1.4.5.2" class="ltx_td ltx_align_center">Do you see the white <span id="S4.T1.4.5.2.1" class="ltx_text ltx_font_bold">tiny</span> dog ?</td>
</tr>
<tr id="S4.T1.4.6" class="ltx_tr">
<td id="S4.T1.4.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.6.1.1" class="ltx_text ltx_font_bold">Synonym Verbs</span></td>
<td id="S4.T1.4.6.2" class="ltx_td ltx_align_center">Do you <span id="S4.T1.4.6.2.1" class="ltx_text ltx_font_bold">watch</span> the white small dog ?</td>
</tr>
<tr id="S4.T1.4.7" class="ltx_tr">
<td id="S4.T1.4.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.7.1.1" class="ltx_text ltx_font_bold">Hypernym Noun</span></td>
<td id="S4.T1.4.7.2" class="ltx_td ltx_align_center">Do you see the white small <span id="S4.T1.4.7.2.1" class="ltx_text ltx_font_bold">canine</span> ?</td>
</tr>
<tr id="S4.T1.4.8" class="ltx_tr">
<td id="S4.T1.4.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.8.1.1" class="ltx_text ltx_font_bold">Hyponym Noun</span></td>
<td id="S4.T1.4.8.2" class="ltx_td ltx_align_center">Do you see the white small <span id="S4.T1.4.8.2.1" class="ltx_text ltx_font_bold">labrador</span> ?</td>
</tr>
<tr id="S4.T1.4.9" class="ltx_tr">
<td id="S4.T1.4.9.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.4.9.1.1" class="ltx_text ltx_font_bold">Sibling Noun</span></td>
<td id="S4.T1.4.9.2" class="ltx_td ltx_align_center">Do you see the white small <span id="S4.T1.4.9.2.1" class="ltx_text ltx_font_bold">wolf</span> ?</td>
</tr>
<tr id="S4.T1.4.10" class="ltx_tr">
<td id="S4.T1.4.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S4.T1.4.10.1.1" class="ltx_text ltx_font_bold">Deletion Noun</span></td>
<td id="S4.T1.4.10.2" class="ltx_td ltx_align_center ltx_border_bb">Do you see the white small _ ?</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.9" class="ltx_p">Substitutions and deletions are an excellent way to quantify whether a VQA model <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mi id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><ci id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">M</annotation></semantics></math> understands a specific question-image pair, or if its output is greatly dependent on biased estimations. This way, we can reveal spurious correlations that are mistakenly integrated into <math id="S4.SS1.p5.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.2.m2.1a"><mi id="S4.SS1.p5.2.m2.1.1" xref="S4.SS1.p5.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.2.m2.1b"><ci id="S4.SS1.p5.2.m2.1.1.cmml" xref="S4.SS1.p5.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.2.m2.1c">M</annotation></semantics></math>. Color substitutions are motivated by the quantity of color-related questions that exist in our input datasets (VG and VQA-v2). By interrogating <math id="S4.SS1.p5.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.3.m3.1a"><mi id="S4.SS1.p5.3.m3.1.1" xref="S4.SS1.p5.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.3.m3.1b"><ci id="S4.SS1.p5.3.m3.1.1.cmml" xref="S4.SS1.p5.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.3.m3.1c">M</annotation></semantics></math> with color perturbations that are greatly distant to the original color (<span id="S4.SS1.p5.9.1" class="ltx_text ltx_font_bold">Color Maximal</span> substitutions experiment), we aim to detect whether <math id="S4.SS1.p5.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.4.m4.1a"><mi id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.1b"><ci id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.1c">M</annotation></semantics></math> will correctly and reasonably perceive this semantically massive change. We would expect <math id="S4.SS1.p5.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.5.m5.1a"><mi id="S4.SS1.p5.5.m5.1.1" xref="S4.SS1.p5.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.5.m5.1b"><ci id="S4.SS1.p5.5.m5.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.5.m5.1c">M</annotation></semantics></math> to change its response <math id="S4.SS1.p5.6.m6.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S4.SS1.p5.6.m6.1a"><msup id="S4.SS1.p5.6.m6.1.1" xref="S4.SS1.p5.6.m6.1.1.cmml"><mi id="S4.SS1.p5.6.m6.1.1.2" xref="S4.SS1.p5.6.m6.1.1.2.cmml">a</mi><mo id="S4.SS1.p5.6.m6.1.1.3" xref="S4.SS1.p5.6.m6.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.6.m6.1b"><apply id="S4.SS1.p5.6.m6.1.1.cmml" xref="S4.SS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.6.m6.1.1.1.cmml" xref="S4.SS1.p5.6.m6.1.1">superscript</csymbol><ci id="S4.SS1.p5.6.m6.1.1.2.cmml" xref="S4.SS1.p5.6.m6.1.1.2">ğ‘</ci><times id="S4.SS1.p5.6.m6.1.1.3.cmml" xref="S4.SS1.p5.6.m6.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.6.m6.1c">a^{*}</annotation></semantics></math> in most cases of <span id="S4.SS1.p5.9.2" class="ltx_text ltx_font_bold">Color Maximal</span> experiment; the opposite would indicate an underlying pattern of ignoring color attributes. Similarly, we perform the <span id="S4.SS1.p5.9.3" class="ltx_text ltx_font_bold">Color Minimal</span> substitution experiment in order to investigate the modelâ€™s behavior when faced with minor alterations in the color concept. We expect the substitutions from this experiment to have little to no influence on the modelâ€™s response <math id="S4.SS1.p5.7.m7.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S4.SS1.p5.7.m7.1a"><msup id="S4.SS1.p5.7.m7.1.1" xref="S4.SS1.p5.7.m7.1.1.cmml"><mi id="S4.SS1.p5.7.m7.1.1.2" xref="S4.SS1.p5.7.m7.1.1.2.cmml">a</mi><mo id="S4.SS1.p5.7.m7.1.1.3" xref="S4.SS1.p5.7.m7.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.7.m7.1b"><apply id="S4.SS1.p5.7.m7.1.1.cmml" xref="S4.SS1.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.7.m7.1.1.1.cmml" xref="S4.SS1.p5.7.m7.1.1">superscript</csymbol><ci id="S4.SS1.p5.7.m7.1.1.2.cmml" xref="S4.SS1.p5.7.m7.1.1.2">ğ‘</ci><times id="S4.SS1.p5.7.m7.1.1.3.cmml" xref="S4.SS1.p5.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.7.m7.1c">a^{*}</annotation></semantics></math>. An opposite behavior would reveal an existing bias regarding specific colors, which would lead to the conclusion that <math id="S4.SS1.p5.8.m8.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.8.m8.1a"><mi id="S4.SS1.p5.8.m8.1.1" xref="S4.SS1.p5.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.8.m8.1b"><ci id="S4.SS1.p5.8.m8.1.1.cmml" xref="S4.SS1.p5.8.m8.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.8.m8.1c">M</annotation></semantics></math> cannot properly and robustly adapt to minor color changes and generalize accordingly. Of course, <span id="S4.SS1.p5.9.4" class="ltx_text ltx_font_italic">uncommon</span> color substitutions in both Minimal/Maximal cases impose a more difficult problem, as <math id="S4.SS1.p5.9.m9.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p5.9.m9.1a"><mi id="S4.SS1.p5.9.m9.1.1" xref="S4.SS1.p5.9.m9.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.9.m9.1b"><ci id="S4.SS1.p5.9.m9.1.1.cmml" xref="S4.SS1.p5.9.m9.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.9.m9.1c">M</annotation></semantics></math> needs to adaptively respond to out-of-dataset color concepts.</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<p id="S4.SS1.p6.4" class="ltx_p">In relation to the <span id="S4.SS1.p6.4.1" class="ltx_text ltx_font_bold">Synonym</span> substitutions, we aim to investigate the modelâ€™s ability to efficiently handle mild morphological language alterations that maintain the same meaning. In this case, failing to properly respond (providing an alternative <math id="S4.SS1.p6.1.m1.1" class="ltx_Math" alttext="a^{*}=a" display="inline"><semantics id="S4.SS1.p6.1.m1.1a"><mrow id="S4.SS1.p6.1.m1.1.1" xref="S4.SS1.p6.1.m1.1.1.cmml"><msup id="S4.SS1.p6.1.m1.1.1.2" xref="S4.SS1.p6.1.m1.1.1.2.cmml"><mi id="S4.SS1.p6.1.m1.1.1.2.2" xref="S4.SS1.p6.1.m1.1.1.2.2.cmml">a</mi><mo id="S4.SS1.p6.1.m1.1.1.2.3" xref="S4.SS1.p6.1.m1.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S4.SS1.p6.1.m1.1.1.1" xref="S4.SS1.p6.1.m1.1.1.1.cmml">=</mo><mi id="S4.SS1.p6.1.m1.1.1.3" xref="S4.SS1.p6.1.m1.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.1.m1.1b"><apply id="S4.SS1.p6.1.m1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1"><eq id="S4.SS1.p6.1.m1.1.1.1.cmml" xref="S4.SS1.p6.1.m1.1.1.1"></eq><apply id="S4.SS1.p6.1.m1.1.1.2.cmml" xref="S4.SS1.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p6.1.m1.1.1.2.1.cmml" xref="S4.SS1.p6.1.m1.1.1.2">superscript</csymbol><ci id="S4.SS1.p6.1.m1.1.1.2.2.cmml" xref="S4.SS1.p6.1.m1.1.1.2.2">ğ‘</ci><times id="S4.SS1.p6.1.m1.1.1.2.3.cmml" xref="S4.SS1.p6.1.m1.1.1.2.3"></times></apply><ci id="S4.SS1.p6.1.m1.1.1.3.cmml" xref="S4.SS1.p6.1.m1.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.1.m1.1c">a^{*}=a</annotation></semantics></math>) would disclose overfitting to specific semantics, which renders the model lexically inflexible and thus non-robust to semantically negligible perturbations. The <span id="S4.SS1.p6.4.2" class="ltx_text ltx_font_bold">Hypernyms</span>-<span id="S4.SS1.p6.4.3" class="ltx_text ltx_font_bold">Hyponyms</span> perturbations are dedicated to depicting the modelâ€™s ability to generalize and specify correspondingly, while retaining a reliable level of robustness. Hypernym and hyponym relationships are notions profoundly understood in the real world and consequently embedded in large scale datasets, which are widely used for VQA models pre-training. Thus, <math id="S4.SS1.p6.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p6.2.m2.1a"><mi id="S4.SS1.p6.2.m2.1.1" xref="S4.SS1.p6.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.2.m2.1b"><ci id="S4.SS1.p6.2.m2.1.1.cmml" xref="S4.SS1.p6.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.2.m2.1c">M</annotation></semantics></math> should also be able to properly comprehend and reason over them. Ideally, we would expect <math id="S4.SS1.p6.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p6.3.m3.1a"><mi id="S4.SS1.p6.3.m3.1.1" xref="S4.SS1.p6.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.3.m3.1b"><ci id="S4.SS1.p6.3.m3.1.1.cmml" xref="S4.SS1.p6.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.3.m3.1c">M</annotation></semantics></math> to maintain the same response for hypernyms substitutions, whereas justifiably respond in specific ways for the hyponyms replacements, taking into account the specification of meaning. The commensurate amount of specifying skill is sought to be established through <span id="S4.SS1.p6.4.4" class="ltx_text ltx_font_bold">Sibling</span> substitution experiment. Depending on each particular case, we expect <math id="S4.SS1.p6.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p6.4.m4.1a"><mi id="S4.SS1.p6.4.m4.1.1" xref="S4.SS1.p6.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p6.4.m4.1b"><ci id="S4.SS1.p6.4.m4.1.1.cmml" xref="S4.SS1.p6.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p6.4.m4.1c">M</annotation></semantics></math> to modify or maintain its response appropriately, to confirm the level of understanding and distinguishment of different, but still related, meanings.</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<p id="S4.SS1.p7.3" class="ltx_p">Finally, we implemented the <span id="S4.SS1.p7.3.1" class="ltx_text ltx_font_bold">Deletion</span> experiment expecting ideally the performance of <math id="S4.SS1.p7.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p7.1.m1.1a"><mi id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><ci id="S4.SS1.p7.1.m1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">M</annotation></semantics></math> to degrade. The amount of the <math id="S4.SS1.p7.2.m2.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S4.SS1.p7.2.m2.1a"><mrow id="S4.SS1.p7.2.m2.1.1" xref="S4.SS1.p7.2.m2.1.1.cmml"><mi id="S4.SS1.p7.2.m2.1.1.2" xref="S4.SS1.p7.2.m2.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p7.2.m2.1.1.1" xref="S4.SS1.p7.2.m2.1.1.1.cmml">â€‹</mo><mi id="S4.SS1.p7.2.m2.1.1.3" xref="S4.SS1.p7.2.m2.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p7.2.m2.1.1.1a" xref="S4.SS1.p7.2.m2.1.1.1.cmml">â€‹</mo><msubsup id="S4.SS1.p7.2.m2.1.1.4" xref="S4.SS1.p7.2.m2.1.1.4.cmml"><mi id="S4.SS1.p7.2.m2.1.1.4.2.2" xref="S4.SS1.p7.2.m2.1.1.4.2.2.cmml">c</mi><mi id="S4.SS1.p7.2.m2.1.1.4.3" xref="S4.SS1.p7.2.m2.1.1.4.3.cmml">Q</mi><mo id="S4.SS1.p7.2.m2.1.1.4.2.3" xref="S4.SS1.p7.2.m2.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.2.m2.1b"><apply id="S4.SS1.p7.2.m2.1.1.cmml" xref="S4.SS1.p7.2.m2.1.1"><times id="S4.SS1.p7.2.m2.1.1.1.cmml" xref="S4.SS1.p7.2.m2.1.1.1"></times><ci id="S4.SS1.p7.2.m2.1.1.2.cmml" xref="S4.SS1.p7.2.m2.1.1.2">ğ‘</ci><ci id="S4.SS1.p7.2.m2.1.1.3.cmml" xref="S4.SS1.p7.2.m2.1.1.3">ğ‘</ci><apply id="S4.SS1.p7.2.m2.1.1.4.cmml" xref="S4.SS1.p7.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.p7.2.m2.1.1.4.1.cmml" xref="S4.SS1.p7.2.m2.1.1.4">subscript</csymbol><apply id="S4.SS1.p7.2.m2.1.1.4.2.cmml" xref="S4.SS1.p7.2.m2.1.1.4"><csymbol cd="ambiguous" id="S4.SS1.p7.2.m2.1.1.4.2.1.cmml" xref="S4.SS1.p7.2.m2.1.1.4">superscript</csymbol><ci id="S4.SS1.p7.2.m2.1.1.4.2.2.cmml" xref="S4.SS1.p7.2.m2.1.1.4.2.2">ğ‘</ci><times id="S4.SS1.p7.2.m2.1.1.4.2.3.cmml" xref="S4.SS1.p7.2.m2.1.1.4.2.3"></times></apply><ci id="S4.SS1.p7.2.m2.1.1.4.3.cmml" xref="S4.SS1.p7.2.m2.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.2.m2.1c">acc^{*}_{Q}</annotation></semantics></math> decline depends on the importance of the deleted noun for the meaning of the question. Consequently, an unbiased <math id="S4.SS1.p7.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS1.p7.3.m3.1a"><mi id="S4.SS1.p7.3.m3.1.1" xref="S4.SS1.p7.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.3.m3.1b"><ci id="S4.SS1.p7.3.m3.1.1.cmml" xref="S4.SS1.p7.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.3.m3.1c">M</annotation></semantics></math> should be able to determine this importance and act accordingly, without reaching unwarranted conclusions that are expressed through an indefensible response.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.13" class="ltx_p">We present results using the accuracy metric, which illustrates the extent of similarity of the modelâ€™s predicted answer to the ground truth answer, both for the original <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">ğ‘„</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">Q</annotation></semantics></math> of each dataset, as well as for the counterfactual question set <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="Q^{*}" display="inline"><semantics id="S5.p1.2.m2.1a"><msup id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><mi id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml">Q</mi><mo id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1">superscript</csymbol><ci id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2">ğ‘„</ci><times id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">Q^{*}</annotation></semantics></math>. In our analysis, accuracy is not profound enough to provide specific situational explanations and insights on the modelâ€™s behavior, when faced with particular concepts. However, accuracy still showcases a high-level approach on the modelâ€™s efficiency fluctuations under the implemented counterfactual perturbations. Since ViLT model is trained and optimized on the VQA-v2 dataset, it is somehow expected to perform better on it compared to VG (both datasets contain similar vocabularies). This observation is indeed validated by our results presented in Tables <a href="#S5.T2" title="Table 2 â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> &amp; <a href="#S5.T3" title="Table 3 â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which demonstrate a consistently higher <math id="S5.p1.3.m3.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><mi id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.3.m3.1.1.1a" xref="S5.p1.3.m3.1.1.1.cmml">â€‹</mo><msub id="S5.p1.3.m3.1.1.4" xref="S5.p1.3.m3.1.1.4.cmml"><mi id="S5.p1.3.m3.1.1.4.2" xref="S5.p1.3.m3.1.1.4.2.cmml">c</mi><mi id="S5.p1.3.m3.1.1.4.3" xref="S5.p1.3.m3.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><times id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></times><ci id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2">ğ‘</ci><ci id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3">ğ‘</ci><apply id="S5.p1.3.m3.1.1.4.cmml" xref="S5.p1.3.m3.1.1.4"><csymbol cd="ambiguous" id="S5.p1.3.m3.1.1.4.1.cmml" xref="S5.p1.3.m3.1.1.4">subscript</csymbol><ci id="S5.p1.3.m3.1.1.4.2.cmml" xref="S5.p1.3.m3.1.1.4.2">ğ‘</ci><ci id="S5.p1.3.m3.1.1.4.3.cmml" xref="S5.p1.3.m3.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">acc_{Q}</annotation></semantics></math> on the former versus the latter dataset, concerning all implemented experiments. We denote that <math id="S5.p1.4.m4.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><mi id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.4.m4.1.1.1" xref="S5.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.4.m4.1.1.1a" xref="S5.p1.4.m4.1.1.1.cmml">â€‹</mo><msub id="S5.p1.4.m4.1.1.4" xref="S5.p1.4.m4.1.1.4.cmml"><mi id="S5.p1.4.m4.1.1.4.2" xref="S5.p1.4.m4.1.1.4.2.cmml">c</mi><mi id="S5.p1.4.m4.1.1.4.3" xref="S5.p1.4.m4.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><times id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1"></times><ci id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2">ğ‘</ci><ci id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3">ğ‘</ci><apply id="S5.p1.4.m4.1.1.4.cmml" xref="S5.p1.4.m4.1.1.4"><csymbol cd="ambiguous" id="S5.p1.4.m4.1.1.4.1.cmml" xref="S5.p1.4.m4.1.1.4">subscript</csymbol><ci id="S5.p1.4.m4.1.1.4.2.cmml" xref="S5.p1.4.m4.1.1.4.2">ğ‘</ci><ci id="S5.p1.4.m4.1.1.4.3.cmml" xref="S5.p1.4.m4.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">acc_{Q}</annotation></semantics></math> scores for each experiment contain the corresponding questions only, e.g. color experiments only contain questions that mention colors. This contributes to the differences in original <math id="S5.p1.5.m5.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.p1.5.m5.1a"><mrow id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml"><mi id="S5.p1.5.m5.1.1.2" xref="S5.p1.5.m5.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.5.m5.1.1.1" xref="S5.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S5.p1.5.m5.1.1.3" xref="S5.p1.5.m5.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.5.m5.1.1.1a" xref="S5.p1.5.m5.1.1.1.cmml">â€‹</mo><msub id="S5.p1.5.m5.1.1.4" xref="S5.p1.5.m5.1.1.4.cmml"><mi id="S5.p1.5.m5.1.1.4.2" xref="S5.p1.5.m5.1.1.4.2.cmml">c</mi><mi id="S5.p1.5.m5.1.1.4.3" xref="S5.p1.5.m5.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><apply id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1"><times id="S5.p1.5.m5.1.1.1.cmml" xref="S5.p1.5.m5.1.1.1"></times><ci id="S5.p1.5.m5.1.1.2.cmml" xref="S5.p1.5.m5.1.1.2">ğ‘</ci><ci id="S5.p1.5.m5.1.1.3.cmml" xref="S5.p1.5.m5.1.1.3">ğ‘</ci><apply id="S5.p1.5.m5.1.1.4.cmml" xref="S5.p1.5.m5.1.1.4"><csymbol cd="ambiguous" id="S5.p1.5.m5.1.1.4.1.cmml" xref="S5.p1.5.m5.1.1.4">subscript</csymbol><ci id="S5.p1.5.m5.1.1.4.2.cmml" xref="S5.p1.5.m5.1.1.4.2">ğ‘</ci><ci id="S5.p1.5.m5.1.1.4.3.cmml" xref="S5.p1.5.m5.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">acc_{Q}</annotation></semantics></math> scores for each experiment.
Nevertheless, in both datasets, we notice an analogous difference between <math id="S5.p1.6.m6.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.p1.6.m6.1a"><mrow id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml"><mi id="S5.p1.6.m6.1.1.2" xref="S5.p1.6.m6.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.6.m6.1.1.1" xref="S5.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S5.p1.6.m6.1.1.3" xref="S5.p1.6.m6.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.6.m6.1.1.1a" xref="S5.p1.6.m6.1.1.1.cmml">â€‹</mo><msub id="S5.p1.6.m6.1.1.4" xref="S5.p1.6.m6.1.1.4.cmml"><mi id="S5.p1.6.m6.1.1.4.2" xref="S5.p1.6.m6.1.1.4.2.cmml">c</mi><mi id="S5.p1.6.m6.1.1.4.3" xref="S5.p1.6.m6.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.1b"><apply id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1"><times id="S5.p1.6.m6.1.1.1.cmml" xref="S5.p1.6.m6.1.1.1"></times><ci id="S5.p1.6.m6.1.1.2.cmml" xref="S5.p1.6.m6.1.1.2">ğ‘</ci><ci id="S5.p1.6.m6.1.1.3.cmml" xref="S5.p1.6.m6.1.1.3">ğ‘</ci><apply id="S5.p1.6.m6.1.1.4.cmml" xref="S5.p1.6.m6.1.1.4"><csymbol cd="ambiguous" id="S5.p1.6.m6.1.1.4.1.cmml" xref="S5.p1.6.m6.1.1.4">subscript</csymbol><ci id="S5.p1.6.m6.1.1.4.2.cmml" xref="S5.p1.6.m6.1.1.4.2">ğ‘</ci><ci id="S5.p1.6.m6.1.1.4.3.cmml" xref="S5.p1.6.m6.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.1c">acc_{Q}</annotation></semantics></math> and <math id="S5.p1.7.m7.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S5.p1.7.m7.1a"><mrow id="S5.p1.7.m7.1.1" xref="S5.p1.7.m7.1.1.cmml"><mi id="S5.p1.7.m7.1.1.2" xref="S5.p1.7.m7.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.7.m7.1.1.1" xref="S5.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S5.p1.7.m7.1.1.3" xref="S5.p1.7.m7.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.7.m7.1.1.1a" xref="S5.p1.7.m7.1.1.1.cmml">â€‹</mo><msubsup id="S5.p1.7.m7.1.1.4" xref="S5.p1.7.m7.1.1.4.cmml"><mi id="S5.p1.7.m7.1.1.4.2.2" xref="S5.p1.7.m7.1.1.4.2.2.cmml">c</mi><mi id="S5.p1.7.m7.1.1.4.3" xref="S5.p1.7.m7.1.1.4.3.cmml">Q</mi><mo id="S5.p1.7.m7.1.1.4.2.3" xref="S5.p1.7.m7.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.7.m7.1b"><apply id="S5.p1.7.m7.1.1.cmml" xref="S5.p1.7.m7.1.1"><times id="S5.p1.7.m7.1.1.1.cmml" xref="S5.p1.7.m7.1.1.1"></times><ci id="S5.p1.7.m7.1.1.2.cmml" xref="S5.p1.7.m7.1.1.2">ğ‘</ci><ci id="S5.p1.7.m7.1.1.3.cmml" xref="S5.p1.7.m7.1.1.3">ğ‘</ci><apply id="S5.p1.7.m7.1.1.4.cmml" xref="S5.p1.7.m7.1.1.4"><csymbol cd="ambiguous" id="S5.p1.7.m7.1.1.4.1.cmml" xref="S5.p1.7.m7.1.1.4">subscript</csymbol><apply id="S5.p1.7.m7.1.1.4.2.cmml" xref="S5.p1.7.m7.1.1.4"><csymbol cd="ambiguous" id="S5.p1.7.m7.1.1.4.2.1.cmml" xref="S5.p1.7.m7.1.1.4">superscript</csymbol><ci id="S5.p1.7.m7.1.1.4.2.2.cmml" xref="S5.p1.7.m7.1.1.4.2.2">ğ‘</ci><times id="S5.p1.7.m7.1.1.4.2.3.cmml" xref="S5.p1.7.m7.1.1.4.2.3"></times></apply><ci id="S5.p1.7.m7.1.1.4.3.cmml" xref="S5.p1.7.m7.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.7.m7.1c">acc^{*}_{Q}</annotation></semantics></math> per experiment when <math id="S5.p1.8.m8.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p1.8.m8.1a"><mi id="S5.p1.8.m8.1.1" xref="S5.p1.8.m8.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p1.8.m8.1b"><ci id="S5.p1.8.m8.1.1.cmml" xref="S5.p1.8.m8.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.8.m8.1c">M</annotation></semantics></math> is presented with counterfactual questions <math id="S5.p1.9.m9.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S5.p1.9.m9.1a"><mrow id="S5.p1.9.m9.1.1" xref="S5.p1.9.m9.1.1.cmml"><msup id="S5.p1.9.m9.1.1.2" xref="S5.p1.9.m9.1.1.2.cmml"><mi id="S5.p1.9.m9.1.1.2.2" xref="S5.p1.9.m9.1.1.2.2.cmml">q</mi><mo id="S5.p1.9.m9.1.1.2.3" xref="S5.p1.9.m9.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S5.p1.9.m9.1.1.1" xref="S5.p1.9.m9.1.1.1.cmml">âˆˆ</mo><msup id="S5.p1.9.m9.1.1.3" xref="S5.p1.9.m9.1.1.3.cmml"><mi id="S5.p1.9.m9.1.1.3.2" xref="S5.p1.9.m9.1.1.3.2.cmml">Q</mi><mo id="S5.p1.9.m9.1.1.3.3" xref="S5.p1.9.m9.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.9.m9.1b"><apply id="S5.p1.9.m9.1.1.cmml" xref="S5.p1.9.m9.1.1"><in id="S5.p1.9.m9.1.1.1.cmml" xref="S5.p1.9.m9.1.1.1"></in><apply id="S5.p1.9.m9.1.1.2.cmml" xref="S5.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S5.p1.9.m9.1.1.2.1.cmml" xref="S5.p1.9.m9.1.1.2">superscript</csymbol><ci id="S5.p1.9.m9.1.1.2.2.cmml" xref="S5.p1.9.m9.1.1.2.2">ğ‘</ci><times id="S5.p1.9.m9.1.1.2.3.cmml" xref="S5.p1.9.m9.1.1.2.3"></times></apply><apply id="S5.p1.9.m9.1.1.3.cmml" xref="S5.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S5.p1.9.m9.1.1.3.1.cmml" xref="S5.p1.9.m9.1.1.3">superscript</csymbol><ci id="S5.p1.9.m9.1.1.3.2.cmml" xref="S5.p1.9.m9.1.1.3.2">ğ‘„</ci><times id="S5.p1.9.m9.1.1.3.3.cmml" xref="S5.p1.9.m9.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.9.m9.1c">q^{*}\in Q^{*}</annotation></semantics></math>. This could generally indicate the existence of underlying biases: <math id="S5.p1.10.m10.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p1.10.m10.1a"><mi id="S5.p1.10.m10.1.1" xref="S5.p1.10.m10.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p1.10.m10.1b"><ci id="S5.p1.10.m10.1.1.cmml" xref="S5.p1.10.m10.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.10.m10.1c">M</annotation></semantics></math> presents a type of overfitting to the original <math id="S5.p1.11.m11.1" class="ltx_Math" alttext="q\in Q" display="inline"><semantics id="S5.p1.11.m11.1a"><mrow id="S5.p1.11.m11.1.1" xref="S5.p1.11.m11.1.1.cmml"><mi id="S5.p1.11.m11.1.1.2" xref="S5.p1.11.m11.1.1.2.cmml">q</mi><mo id="S5.p1.11.m11.1.1.1" xref="S5.p1.11.m11.1.1.1.cmml">âˆˆ</mo><mi id="S5.p1.11.m11.1.1.3" xref="S5.p1.11.m11.1.1.3.cmml">Q</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.11.m11.1b"><apply id="S5.p1.11.m11.1.1.cmml" xref="S5.p1.11.m11.1.1"><in id="S5.p1.11.m11.1.1.1.cmml" xref="S5.p1.11.m11.1.1.1"></in><ci id="S5.p1.11.m11.1.1.2.cmml" xref="S5.p1.11.m11.1.1.2">ğ‘</ci><ci id="S5.p1.11.m11.1.1.3.cmml" xref="S5.p1.11.m11.1.1.3">ğ‘„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.11.m11.1c">q\in Q</annotation></semantics></math>, which renders it less efficient when asked to handle minimally perturbed counterfactual questions. In all experiments, the accuracy reduction from the original <math id="S5.p1.12.m12.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.p1.12.m12.1a"><mrow id="S5.p1.12.m12.1.1" xref="S5.p1.12.m12.1.1.cmml"><mi id="S5.p1.12.m12.1.1.2" xref="S5.p1.12.m12.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.12.m12.1.1.1" xref="S5.p1.12.m12.1.1.1.cmml">â€‹</mo><mi id="S5.p1.12.m12.1.1.3" xref="S5.p1.12.m12.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.12.m12.1.1.1a" xref="S5.p1.12.m12.1.1.1.cmml">â€‹</mo><msub id="S5.p1.12.m12.1.1.4" xref="S5.p1.12.m12.1.1.4.cmml"><mi id="S5.p1.12.m12.1.1.4.2" xref="S5.p1.12.m12.1.1.4.2.cmml">c</mi><mi id="S5.p1.12.m12.1.1.4.3" xref="S5.p1.12.m12.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.12.m12.1b"><apply id="S5.p1.12.m12.1.1.cmml" xref="S5.p1.12.m12.1.1"><times id="S5.p1.12.m12.1.1.1.cmml" xref="S5.p1.12.m12.1.1.1"></times><ci id="S5.p1.12.m12.1.1.2.cmml" xref="S5.p1.12.m12.1.1.2">ğ‘</ci><ci id="S5.p1.12.m12.1.1.3.cmml" xref="S5.p1.12.m12.1.1.3">ğ‘</ci><apply id="S5.p1.12.m12.1.1.4.cmml" xref="S5.p1.12.m12.1.1.4"><csymbol cd="ambiguous" id="S5.p1.12.m12.1.1.4.1.cmml" xref="S5.p1.12.m12.1.1.4">subscript</csymbol><ci id="S5.p1.12.m12.1.1.4.2.cmml" xref="S5.p1.12.m12.1.1.4.2">ğ‘</ci><ci id="S5.p1.12.m12.1.1.4.3.cmml" xref="S5.p1.12.m12.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.12.m12.1c">acc_{Q}</annotation></semantics></math> to <math id="S5.p1.13.m13.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S5.p1.13.m13.1a"><mrow id="S5.p1.13.m13.1.1" xref="S5.p1.13.m13.1.1.cmml"><mi id="S5.p1.13.m13.1.1.2" xref="S5.p1.13.m13.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.p1.13.m13.1.1.1" xref="S5.p1.13.m13.1.1.1.cmml">â€‹</mo><mi id="S5.p1.13.m13.1.1.3" xref="S5.p1.13.m13.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.p1.13.m13.1.1.1a" xref="S5.p1.13.m13.1.1.1.cmml">â€‹</mo><msubsup id="S5.p1.13.m13.1.1.4" xref="S5.p1.13.m13.1.1.4.cmml"><mi id="S5.p1.13.m13.1.1.4.2.2" xref="S5.p1.13.m13.1.1.4.2.2.cmml">c</mi><mi id="S5.p1.13.m13.1.1.4.3" xref="S5.p1.13.m13.1.1.4.3.cmml">Q</mi><mo id="S5.p1.13.m13.1.1.4.2.3" xref="S5.p1.13.m13.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.13.m13.1b"><apply id="S5.p1.13.m13.1.1.cmml" xref="S5.p1.13.m13.1.1"><times id="S5.p1.13.m13.1.1.1.cmml" xref="S5.p1.13.m13.1.1.1"></times><ci id="S5.p1.13.m13.1.1.2.cmml" xref="S5.p1.13.m13.1.1.2">ğ‘</ci><ci id="S5.p1.13.m13.1.1.3.cmml" xref="S5.p1.13.m13.1.1.3">ğ‘</ci><apply id="S5.p1.13.m13.1.1.4.cmml" xref="S5.p1.13.m13.1.1.4"><csymbol cd="ambiguous" id="S5.p1.13.m13.1.1.4.1.cmml" xref="S5.p1.13.m13.1.1.4">subscript</csymbol><apply id="S5.p1.13.m13.1.1.4.2.cmml" xref="S5.p1.13.m13.1.1.4"><csymbol cd="ambiguous" id="S5.p1.13.m13.1.1.4.2.1.cmml" xref="S5.p1.13.m13.1.1.4">superscript</csymbol><ci id="S5.p1.13.m13.1.1.4.2.2.cmml" xref="S5.p1.13.m13.1.1.4.2.2">ğ‘</ci><times id="S5.p1.13.m13.1.1.4.2.3.cmml" xref="S5.p1.13.m13.1.1.4.2.3"></times></apply><ci id="S5.p1.13.m13.1.1.4.3.cmml" xref="S5.p1.13.m13.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.13.m13.1c">acc^{*}_{Q}</annotation></semantics></math> is approximately 15-20% or more.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">An extended depiction of the retrieved accuracies for both datasets is presented in Table <a href="#S5.T2" title="Table 2 â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (color-based substitutions) and Table <a href="#S5.T3" title="Table 3 â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (WordNet-based substitutions and noun deletions). Specifically, for <span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Color Maximal</span> in VQA-v2 we observe a decline of 34.2% for <span id="S5.p2.1.2" class="ltx_text ltx_font_italic">common</span> colors and a decline of 37.4% for <span id="S5.p2.1.3" class="ltx_text ltx_font_italic">uncommon</span> ones. Even for semantically minimal substitutions, (<span id="S5.p2.1.4" class="ltx_text ltx_font_bold">Color Minimal</span> experiment), the decline is 31% for both <span id="S5.p2.1.5" class="ltx_text ltx_font_italic">common</span> and <span id="S5.p2.1.6" class="ltx_text ltx_font_italic">uncommon</span> colors. As for VG, we observe a decline of 38.2% for <span id="S5.p2.1.7" class="ltx_text ltx_font_italic">common</span> colors and a decline of 52.2% for <span id="S5.p2.1.8" class="ltx_text ltx_font_italic">uncommon</span> colors when <span id="S5.p2.1.9" class="ltx_text ltx_font_bold">Color Maximal</span> substitutions are performed. Correspondingly, a decline of 35% for both <span id="S5.p2.1.10" class="ltx_text ltx_font_italic">common</span> and <span id="S5.p2.1.11" class="ltx_text ltx_font_italic">uncommon</span> colors is reported for <span id="S5.p2.1.12" class="ltx_text ltx_font_bold">Color Minimal</span> substitutions.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T2.5.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S5.T2.6.2" class="ltx_text" style="font-size:90%;">Accuracies for color perturbations on VQA-v2 and Visual Genome (VG). Common refers to substitutions with in-dataset colors, while uncommon refers to substitutions involving any Matplotlib color.</span></figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.3.3" class="ltx_tr">
<td id="S5.T2.3.3.4" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T2.1.1.1.m1.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.T2.1.1.1.m1.1a"><mrow id="S5.T2.1.1.1.m1.1.1" xref="S5.T2.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T2.1.1.1.m1.1.1.1" xref="S5.T2.1.1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T2.1.1.1.m1.1.1.3" xref="S5.T2.1.1.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T2.1.1.1.m1.1.1.1a" xref="S5.T2.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S5.T2.1.1.1.m1.1.1.4" xref="S5.T2.1.1.1.m1.1.1.4.cmml"><mi id="S5.T2.1.1.1.m1.1.1.4.2" xref="S5.T2.1.1.1.m1.1.1.4.2.cmml">c</mi><mi id="S5.T2.1.1.1.m1.1.1.4.3" xref="S5.T2.1.1.1.m1.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1"><times id="S5.T2.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.m1.1.1.1"></times><ci id="S5.T2.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.m1.1.1.2">ğ‘</ci><ci id="S5.T2.1.1.1.m1.1.1.3.cmml" xref="S5.T2.1.1.1.m1.1.1.3">ğ‘</ci><apply id="S5.T2.1.1.1.m1.1.1.4.cmml" xref="S5.T2.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.1.1.1.m1.1.1.4.1.cmml" xref="S5.T2.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S5.T2.1.1.1.m1.1.1.4.2.cmml" xref="S5.T2.1.1.1.m1.1.1.4.2">ğ‘</ci><ci id="S5.T2.1.1.1.m1.1.1.4.3.cmml" xref="S5.T2.1.1.1.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.m1.1c">acc_{Q}</annotation></semantics></math>%</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T2.2.2.2.m1.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S5.T2.2.2.2.m1.1a"><mrow id="S5.T2.2.2.2.m1.1.1" xref="S5.T2.2.2.2.m1.1.1.cmml"><mi id="S5.T2.2.2.2.m1.1.1.2" xref="S5.T2.2.2.2.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T2.2.2.2.m1.1.1.1" xref="S5.T2.2.2.2.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T2.2.2.2.m1.1.1.3" xref="S5.T2.2.2.2.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T2.2.2.2.m1.1.1.1a" xref="S5.T2.2.2.2.m1.1.1.1.cmml">â€‹</mo><msubsup id="S5.T2.2.2.2.m1.1.1.4" xref="S5.T2.2.2.2.m1.1.1.4.cmml"><mi id="S5.T2.2.2.2.m1.1.1.4.2.2" xref="S5.T2.2.2.2.m1.1.1.4.2.2.cmml">c</mi><mi id="S5.T2.2.2.2.m1.1.1.4.3" xref="S5.T2.2.2.2.m1.1.1.4.3.cmml">Q</mi><mo id="S5.T2.2.2.2.m1.1.1.4.2.3" xref="S5.T2.2.2.2.m1.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.m1.1b"><apply id="S5.T2.2.2.2.m1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1"><times id="S5.T2.2.2.2.m1.1.1.1.cmml" xref="S5.T2.2.2.2.m1.1.1.1"></times><ci id="S5.T2.2.2.2.m1.1.1.2.cmml" xref="S5.T2.2.2.2.m1.1.1.2">ğ‘</ci><ci id="S5.T2.2.2.2.m1.1.1.3.cmml" xref="S5.T2.2.2.2.m1.1.1.3">ğ‘</ci><apply id="S5.T2.2.2.2.m1.1.1.4.cmml" xref="S5.T2.2.2.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.2.2.2.m1.1.1.4.1.cmml" xref="S5.T2.2.2.2.m1.1.1.4">subscript</csymbol><apply id="S5.T2.2.2.2.m1.1.1.4.2.cmml" xref="S5.T2.2.2.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.2.2.2.m1.1.1.4.2.1.cmml" xref="S5.T2.2.2.2.m1.1.1.4">superscript</csymbol><ci id="S5.T2.2.2.2.m1.1.1.4.2.2.cmml" xref="S5.T2.2.2.2.m1.1.1.4.2.2">ğ‘</ci><times id="S5.T2.2.2.2.m1.1.1.4.2.3.cmml" xref="S5.T2.2.2.2.m1.1.1.4.2.3"></times></apply><ci id="S5.T2.2.2.2.m1.1.1.4.3.cmml" xref="S5.T2.2.2.2.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.m1.1c">acc^{*}_{Q}</annotation></semantics></math>% (common)</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T2.3.3.3.m1.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S5.T2.3.3.3.m1.1a"><mrow id="S5.T2.3.3.3.m1.1.1" xref="S5.T2.3.3.3.m1.1.1.cmml"><mi id="S5.T2.3.3.3.m1.1.1.2" xref="S5.T2.3.3.3.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T2.3.3.3.m1.1.1.1" xref="S5.T2.3.3.3.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T2.3.3.3.m1.1.1.3" xref="S5.T2.3.3.3.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T2.3.3.3.m1.1.1.1a" xref="S5.T2.3.3.3.m1.1.1.1.cmml">â€‹</mo><msubsup id="S5.T2.3.3.3.m1.1.1.4" xref="S5.T2.3.3.3.m1.1.1.4.cmml"><mi id="S5.T2.3.3.3.m1.1.1.4.2.2" xref="S5.T2.3.3.3.m1.1.1.4.2.2.cmml">c</mi><mi id="S5.T2.3.3.3.m1.1.1.4.3" xref="S5.T2.3.3.3.m1.1.1.4.3.cmml">Q</mi><mo id="S5.T2.3.3.3.m1.1.1.4.2.3" xref="S5.T2.3.3.3.m1.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.m1.1b"><apply id="S5.T2.3.3.3.m1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1"><times id="S5.T2.3.3.3.m1.1.1.1.cmml" xref="S5.T2.3.3.3.m1.1.1.1"></times><ci id="S5.T2.3.3.3.m1.1.1.2.cmml" xref="S5.T2.3.3.3.m1.1.1.2">ğ‘</ci><ci id="S5.T2.3.3.3.m1.1.1.3.cmml" xref="S5.T2.3.3.3.m1.1.1.3">ğ‘</ci><apply id="S5.T2.3.3.3.m1.1.1.4.cmml" xref="S5.T2.3.3.3.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.3.3.3.m1.1.1.4.1.cmml" xref="S5.T2.3.3.3.m1.1.1.4">subscript</csymbol><apply id="S5.T2.3.3.3.m1.1.1.4.2.cmml" xref="S5.T2.3.3.3.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T2.3.3.3.m1.1.1.4.2.1.cmml" xref="S5.T2.3.3.3.m1.1.1.4">superscript</csymbol><ci id="S5.T2.3.3.3.m1.1.1.4.2.2.cmml" xref="S5.T2.3.3.3.m1.1.1.4.2.2">ğ‘</ci><times id="S5.T2.3.3.3.m1.1.1.4.2.3.cmml" xref="S5.T2.3.3.3.m1.1.1.4.2.3"></times></apply><ci id="S5.T2.3.3.3.m1.1.1.4.3.cmml" xref="S5.T2.3.3.3.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.m1.1c">acc^{*}_{Q}</annotation></semantics></math>% (uncommon)</td>
</tr>
<tr id="S5.T2.3.4" class="ltx_tr">
<td id="S5.T2.3.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Perturbation</td>
<td id="S5.T2.3.4.2" class="ltx_td ltx_align_center ltx_border_tt">VQA-v2</td>
<td id="S5.T2.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VG</td>
<td id="S5.T2.3.4.4" class="ltx_td ltx_align_center ltx_border_tt">VQA-v2</td>
<td id="S5.T2.3.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VG</td>
<td id="S5.T2.3.4.6" class="ltx_td ltx_align_center ltx_border_tt">VQA-v2</td>
<td id="S5.T2.3.4.7" class="ltx_td ltx_align_center ltx_border_tt">VG</td>
</tr>
<tr id="S5.T2.3.5" class="ltx_tr">
<td id="S5.T2.3.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T2.3.5.1.1" class="ltx_text ltx_font_bold">Color Maximal</span></td>
<td id="S5.T2.3.5.2" class="ltx_td ltx_align_center ltx_border_t">69.6</td>
<td id="S5.T2.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">46.9</td>
<td id="S5.T2.3.5.4" class="ltx_td ltx_align_center ltx_border_t">45.8</td>
<td id="S5.T2.3.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.0</td>
<td id="S5.T2.3.5.6" class="ltx_td ltx_align_center ltx_border_t">43.6</td>
<td id="S5.T2.3.5.7" class="ltx_td ltx_align_center ltx_border_t">22.4</td>
</tr>
<tr id="S5.T2.3.6" class="ltx_tr">
<td id="S5.T2.3.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S5.T2.3.6.1.1" class="ltx_text ltx_font_bold">Color Minimal</span></td>
<td id="S5.T2.3.6.2" class="ltx_td ltx_align_center ltx_border_bb">70.0</td>
<td id="S5.T2.3.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">47.5</td>
<td id="S5.T2.3.6.4" class="ltx_td ltx_align_center ltx_border_bb">48.3</td>
<td id="S5.T2.3.6.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">30.9</td>
<td id="S5.T2.3.6.6" class="ltx_td ltx_align_center ltx_border_bb">48.3</td>
<td id="S5.T2.3.6.7" class="ltx_td ltx_align_center ltx_border_bb">30.9</td>
</tr>
</table>
</figure>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.5.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.6.2" class="ltx_text" style="font-size:90%;">Accuracies for WordNet-based perturbations
on VQA-v2 and Visual Genome (VG).</span></figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_align_middle">
<tr id="S5.T3.3.3" class="ltx_tr">
<td id="S5.T3.3.3.4" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mrow id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T3.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T3.1.1.1.m1.1.1.3" xref="S5.T3.1.1.1.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T3.1.1.1.m1.1.1.1a" xref="S5.T3.1.1.1.m1.1.1.1.cmml">â€‹</mo><msub id="S5.T3.1.1.1.m1.1.1.4" xref="S5.T3.1.1.1.m1.1.1.4.cmml"><mi id="S5.T3.1.1.1.m1.1.1.4.2" xref="S5.T3.1.1.1.m1.1.1.4.2.cmml">c</mi><mi id="S5.T3.1.1.1.m1.1.1.4.3" xref="S5.T3.1.1.1.m1.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1"><times id="S5.T3.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1.1"></times><ci id="S5.T3.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.m1.1.1.2">ğ‘</ci><ci id="S5.T3.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.m1.1.1.3">ğ‘</ci><apply id="S5.T3.1.1.1.m1.1.1.4.cmml" xref="S5.T3.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T3.1.1.1.m1.1.1.4.1.cmml" xref="S5.T3.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S5.T3.1.1.1.m1.1.1.4.2.cmml" xref="S5.T3.1.1.1.m1.1.1.4.2">ğ‘</ci><ci id="S5.T3.1.1.1.m1.1.1.4.3.cmml" xref="S5.T3.1.1.1.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">acc_{Q}</annotation></semantics></math>%</td>
<td id="S5.T3.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T3.2.2.2.m1.1" class="ltx_Math" alttext="acc^{*}_{Q}" display="inline"><semantics id="S5.T3.2.2.2.m1.1a"><mrow id="S5.T3.2.2.2.m1.1.1" xref="S5.T3.2.2.2.m1.1.1.cmml"><mi id="S5.T3.2.2.2.m1.1.1.2" xref="S5.T3.2.2.2.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T3.2.2.2.m1.1.1.1" xref="S5.T3.2.2.2.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T3.2.2.2.m1.1.1.3" xref="S5.T3.2.2.2.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T3.2.2.2.m1.1.1.1a" xref="S5.T3.2.2.2.m1.1.1.1.cmml">â€‹</mo><msubsup id="S5.T3.2.2.2.m1.1.1.4" xref="S5.T3.2.2.2.m1.1.1.4.cmml"><mi id="S5.T3.2.2.2.m1.1.1.4.2.2" xref="S5.T3.2.2.2.m1.1.1.4.2.2.cmml">c</mi><mi id="S5.T3.2.2.2.m1.1.1.4.3" xref="S5.T3.2.2.2.m1.1.1.4.3.cmml">Q</mi><mo id="S5.T3.2.2.2.m1.1.1.4.2.3" xref="S5.T3.2.2.2.m1.1.1.4.2.3.cmml">âˆ—</mo></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.m1.1b"><apply id="S5.T3.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1"><times id="S5.T3.2.2.2.m1.1.1.1.cmml" xref="S5.T3.2.2.2.m1.1.1.1"></times><ci id="S5.T3.2.2.2.m1.1.1.2.cmml" xref="S5.T3.2.2.2.m1.1.1.2">ğ‘</ci><ci id="S5.T3.2.2.2.m1.1.1.3.cmml" xref="S5.T3.2.2.2.m1.1.1.3">ğ‘</ci><apply id="S5.T3.2.2.2.m1.1.1.4.cmml" xref="S5.T3.2.2.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T3.2.2.2.m1.1.1.4.1.cmml" xref="S5.T3.2.2.2.m1.1.1.4">subscript</csymbol><apply id="S5.T3.2.2.2.m1.1.1.4.2.cmml" xref="S5.T3.2.2.2.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T3.2.2.2.m1.1.1.4.2.1.cmml" xref="S5.T3.2.2.2.m1.1.1.4">superscript</csymbol><ci id="S5.T3.2.2.2.m1.1.1.4.2.2.cmml" xref="S5.T3.2.2.2.m1.1.1.4.2.2">ğ‘</ci><times id="S5.T3.2.2.2.m1.1.1.4.2.3.cmml" xref="S5.T3.2.2.2.m1.1.1.4.2.3"></times></apply><ci id="S5.T3.2.2.2.m1.1.1.4.3.cmml" xref="S5.T3.2.2.2.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.m1.1c">acc^{*}_{Q}</annotation></semantics></math>%</td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<math id="S5.T3.3.3.3.m1.1" class="ltx_Math" alttext="acc_{Q}" display="inline"><semantics id="S5.T3.3.3.3.m1.1a"><mrow id="S5.T3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.m1.1.1.cmml"><mi id="S5.T3.3.3.3.m1.1.1.2" xref="S5.T3.3.3.3.m1.1.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.T3.3.3.3.m1.1.1.1" xref="S5.T3.3.3.3.m1.1.1.1.cmml">â€‹</mo><mi id="S5.T3.3.3.3.m1.1.1.3" xref="S5.T3.3.3.3.m1.1.1.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S5.T3.3.3.3.m1.1.1.1a" xref="S5.T3.3.3.3.m1.1.1.1.cmml">â€‹</mo><msub id="S5.T3.3.3.3.m1.1.1.4" xref="S5.T3.3.3.3.m1.1.1.4.cmml"><mi id="S5.T3.3.3.3.m1.1.1.4.2" xref="S5.T3.3.3.3.m1.1.1.4.2.cmml">c</mi><mi id="S5.T3.3.3.3.m1.1.1.4.3" xref="S5.T3.3.3.3.m1.1.1.4.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.m1.1b"><apply id="S5.T3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1"><times id="S5.T3.3.3.3.m1.1.1.1.cmml" xref="S5.T3.3.3.3.m1.1.1.1"></times><ci id="S5.T3.3.3.3.m1.1.1.2.cmml" xref="S5.T3.3.3.3.m1.1.1.2">ğ‘</ci><ci id="S5.T3.3.3.3.m1.1.1.3.cmml" xref="S5.T3.3.3.3.m1.1.1.3">ğ‘</ci><apply id="S5.T3.3.3.3.m1.1.1.4.cmml" xref="S5.T3.3.3.3.m1.1.1.4"><csymbol cd="ambiguous" id="S5.T3.3.3.3.m1.1.1.4.1.cmml" xref="S5.T3.3.3.3.m1.1.1.4">subscript</csymbol><ci id="S5.T3.3.3.3.m1.1.1.4.2.cmml" xref="S5.T3.3.3.3.m1.1.1.4.2">ğ‘</ci><ci id="S5.T3.3.3.3.m1.1.1.4.3.cmml" xref="S5.T3.3.3.3.m1.1.1.4.3">ğ‘„</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.m1.1c">acc_{Q}</annotation></semantics></math> reduction %</td>
</tr>
<tr id="S5.T3.3.4" class="ltx_tr">
<td id="S5.T3.3.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Perturbation</td>
<td id="S5.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_t">VQA-v2</td>
<td id="S5.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VG</td>
<td id="S5.T3.3.4.4" class="ltx_td ltx_align_center ltx_border_t">VQA-v2</td>
<td id="S5.T3.3.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">VG</td>
<td id="S5.T3.3.4.6" class="ltx_td ltx_align_center ltx_border_t">VQA-v2</td>
<td id="S5.T3.3.4.7" class="ltx_td ltx_align_center ltx_border_t">VG</td>
</tr>
<tr id="S5.T3.3.5" class="ltx_tr">
<td id="S5.T3.3.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S5.T3.3.5.1.1" class="ltx_text ltx_font_bold">Synonym Adjectives</span></td>
<td id="S5.T3.3.5.2" class="ltx_td ltx_align_center ltx_border_t">75.1</td>
<td id="S5.T3.3.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">47.0</td>
<td id="S5.T3.3.5.4" class="ltx_td ltx_align_center ltx_border_t">56.9</td>
<td id="S5.T3.3.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.4</td>
<td id="S5.T3.3.5.6" class="ltx_td ltx_align_center ltx_border_t">20.6</td>
<td id="S5.T3.3.5.7" class="ltx_td ltx_align_center ltx_border_t">20.4</td>
</tr>
<tr id="S5.T3.3.6" class="ltx_tr">
<td id="S5.T3.3.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.3.6.1.1" class="ltx_text ltx_font_bold">Synonym Verbs</span></td>
<td id="S5.T3.3.6.2" class="ltx_td ltx_align_center">76.8</td>
<td id="S5.T3.3.6.3" class="ltx_td ltx_align_center ltx_border_r">52.3</td>
<td id="S5.T3.3.6.4" class="ltx_td ltx_align_center">64.1</td>
<td id="S5.T3.3.6.5" class="ltx_td ltx_align_center ltx_border_r">44.4</td>
<td id="S5.T3.3.6.6" class="ltx_td ltx_align_center">16.5</td>
<td id="S5.T3.3.6.7" class="ltx_td ltx_align_center">15.1</td>
</tr>
<tr id="S5.T3.3.7" class="ltx_tr">
<td id="S5.T3.3.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.3.7.1.1" class="ltx_text ltx_font_bold">Hypernym Noun</span></td>
<td id="S5.T3.3.7.2" class="ltx_td ltx_align_center">75.2</td>
<td id="S5.T3.3.7.3" class="ltx_td ltx_align_center ltx_border_r">54.6</td>
<td id="S5.T3.3.7.4" class="ltx_td ltx_align_center">60.8</td>
<td id="S5.T3.3.7.5" class="ltx_td ltx_align_center ltx_border_r">41.5</td>
<td id="S5.T3.3.7.6" class="ltx_td ltx_align_center">19.1</td>
<td id="S5.T3.3.7.7" class="ltx_td ltx_align_center">24.0</td>
</tr>
<tr id="S5.T3.3.8" class="ltx_tr">
<td id="S5.T3.3.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.3.8.1.1" class="ltx_text ltx_font_bold">Hyponym Noun</span></td>
<td id="S5.T3.3.8.2" class="ltx_td ltx_align_center">75.1</td>
<td id="S5.T3.3.8.3" class="ltx_td ltx_align_center ltx_border_r">53.5</td>
<td id="S5.T3.3.8.4" class="ltx_td ltx_align_center">56.3</td>
<td id="S5.T3.3.8.5" class="ltx_td ltx_align_center ltx_border_r">36.1</td>
<td id="S5.T3.3.8.6" class="ltx_td ltx_align_center">25.0</td>
<td id="S5.T3.3.8.7" class="ltx_td ltx_align_center">32.5</td>
</tr>
<tr id="S5.T3.3.9" class="ltx_tr">
<td id="S5.T3.3.9.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.3.9.1.1" class="ltx_text ltx_font_bold">Sibling Noun</span></td>
<td id="S5.T3.3.9.2" class="ltx_td ltx_align_center">76.9</td>
<td id="S5.T3.3.9.3" class="ltx_td ltx_align_center ltx_border_r">54.0</td>
<td id="S5.T3.3.9.4" class="ltx_td ltx_align_center">54.0</td>
<td id="S5.T3.3.9.5" class="ltx_td ltx_align_center ltx_border_r">33.4</td>
<td id="S5.T3.3.9.6" class="ltx_td ltx_align_center">29.8</td>
<td id="S5.T3.3.9.7" class="ltx_td ltx_align_center">38.1</td>
</tr>
<tr id="S5.T3.3.10" class="ltx_tr">
<td id="S5.T3.3.10.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t"><span id="S5.T3.3.10.1.1" class="ltx_text ltx_font_bold">Deletion Noun</span></td>
<td id="S5.T3.3.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">76.9</td>
<td id="S5.T3.3.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">53.9</td>
<td id="S5.T3.3.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">59.1</td>
<td id="S5.T3.3.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">36.5</td>
<td id="S5.T3.3.10.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">23.1</td>
<td id="S5.T3.3.10.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">32.3</td>
</tr>
</table>
</figure>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.7" class="ltx_p">The discovery of global patterns provides a more profound and targeted view on robustness of the model <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p3.1.m1.1a"><mi id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">M</annotation></semantics></math>. To this end, we note that in all the cases we studied, we are not interested in the ground truth answer of a question <math id="S5.p3.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S5.p3.2.m2.1a"><mi id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><ci id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">q</annotation></semantics></math>, but rather in the differentiation between the answer <math id="S5.p3.3.m3.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.p3.3.m3.1a"><msup id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml"><mi id="S5.p3.3.m3.1.1.2" xref="S5.p3.3.m3.1.1.2.cmml">a</mi><mo id="S5.p3.3.m3.1.1.3" xref="S5.p3.3.m3.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.1b"><apply id="S5.p3.3.m3.1.1.cmml" xref="S5.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.cmml" xref="S5.p3.3.m3.1.1">superscript</csymbol><ci id="S5.p3.3.m3.1.1.2.cmml" xref="S5.p3.3.m3.1.1.2">ğ‘</ci><times id="S5.p3.3.m3.1.1.3.cmml" xref="S5.p3.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.1c">a^{*}</annotation></semantics></math> to the counterfactual question in relation to the original answer <math id="S5.p3.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S5.p3.4.m4.1a"><mi id="S5.p3.4.m4.1.1" xref="S5.p3.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.p3.4.m4.1b"><ci id="S5.p3.4.m4.1.1.cmml" xref="S5.p3.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.4.m4.1c">a</annotation></semantics></math> that <math id="S5.p3.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p3.5.m5.1a"><mi id="S5.p3.5.m5.1.1" xref="S5.p3.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p3.5.m5.1b"><ci id="S5.p3.5.m5.1.1.cmml" xref="S5.p3.5.m5.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.5.m5.1c">M</annotation></semantics></math> predicts, either if <math id="S5.p3.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S5.p3.6.m6.1a"><mi id="S5.p3.6.m6.1.1" xref="S5.p3.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.p3.6.m6.1b"><ci id="S5.p3.6.m6.1.1.cmml" xref="S5.p3.6.m6.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.6.m6.1c">a</annotation></semantics></math> is correct or not. We select this approach since we are interested in discovering the modelâ€™s change in decision-making under the presence of counterfactual inputs, which is more informative than measuring how much <math id="S5.p3.7.m7.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.p3.7.m7.1a"><msup id="S5.p3.7.m7.1.1" xref="S5.p3.7.m7.1.1.cmml"><mi id="S5.p3.7.m7.1.1.2" xref="S5.p3.7.m7.1.1.2.cmml">a</mi><mo id="S5.p3.7.m7.1.1.3" xref="S5.p3.7.m7.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.p3.7.m7.1b"><apply id="S5.p3.7.m7.1.1.cmml" xref="S5.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S5.p3.7.m7.1.1.1.cmml" xref="S5.p3.7.m7.1.1">superscript</csymbol><ci id="S5.p3.7.m7.1.1.2.cmml" xref="S5.p3.7.m7.1.1.2">ğ‘</ci><times id="S5.p3.7.m7.1.1.3.cmml" xref="S5.p3.7.m7.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.7.m7.1c">a^{*}</annotation></semantics></math> semantically deviates from the ground truth response.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.5" class="ltx_p">Based on the thorough investigation of our experimentsâ€™ results and the aggregation of the following <span id="S5.p4.5.1" class="ltx_text ltx_font_italic">local explanations</span>, as presented in the upcoming Figures, we have deduced some meaningful <span id="S5.p4.5.2" class="ltx_text ltx_font_italic">global rules</span> that both embody the robustness of <math id="S5.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p4.1.m1.1a"><mi id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">M</annotation></semantics></math> to our counterfactual questions <math id="S5.p4.2.m2.1" class="ltx_Math" alttext="q^{*}\in Q^{*}" display="inline"><semantics id="S5.p4.2.m2.1a"><mrow id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml"><msup id="S5.p4.2.m2.1.1.2" xref="S5.p4.2.m2.1.1.2.cmml"><mi id="S5.p4.2.m2.1.1.2.2" xref="S5.p4.2.m2.1.1.2.2.cmml">q</mi><mo id="S5.p4.2.m2.1.1.2.3" xref="S5.p4.2.m2.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S5.p4.2.m2.1.1.1" xref="S5.p4.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S5.p4.2.m2.1.1.3" xref="S5.p4.2.m2.1.1.3.cmml"><mi id="S5.p4.2.m2.1.1.3.2" xref="S5.p4.2.m2.1.1.3.2.cmml">Q</mi><mo id="S5.p4.2.m2.1.1.3.3" xref="S5.p4.2.m2.1.1.3.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><apply id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1"><in id="S5.p4.2.m2.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1"></in><apply id="S5.p4.2.m2.1.1.2.cmml" xref="S5.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.2.1.cmml" xref="S5.p4.2.m2.1.1.2">superscript</csymbol><ci id="S5.p4.2.m2.1.1.2.2.cmml" xref="S5.p4.2.m2.1.1.2.2">ğ‘</ci><times id="S5.p4.2.m2.1.1.2.3.cmml" xref="S5.p4.2.m2.1.1.2.3"></times></apply><apply id="S5.p4.2.m2.1.1.3.cmml" xref="S5.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.3.1.cmml" xref="S5.p4.2.m2.1.1.3">superscript</csymbol><ci id="S5.p4.2.m2.1.1.3.2.cmml" xref="S5.p4.2.m2.1.1.3.2">ğ‘„</ci><times id="S5.p4.2.m2.1.1.3.3.cmml" xref="S5.p4.2.m2.1.1.3.3"></times></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">q^{*}\in Q^{*}</annotation></semantics></math>, while providing reliable explanations that reveal the modelâ€™s reasoning behind its decision-making. Furthermore, we analyze underlying existing biases of <math id="S5.p4.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.p4.3.m3.1a"><mi id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><ci id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">M</annotation></semantics></math> that logically derive from these global rules.
In the following Figures, we highlight the original <math id="S5.p4.4.m4.2" class="ltx_Math" alttext="q,a" display="inline"><semantics id="S5.p4.4.m4.2a"><mrow id="S5.p4.4.m4.2.3.2" xref="S5.p4.4.m4.2.3.1.cmml"><mi id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml">q</mi><mo id="S5.p4.4.m4.2.3.2.1" xref="S5.p4.4.m4.2.3.1.cmml">,</mo><mi id="S5.p4.4.m4.2.2" xref="S5.p4.4.m4.2.2.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.2b"><list id="S5.p4.4.m4.2.3.1.cmml" xref="S5.p4.4.m4.2.3.2"><ci id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1">ğ‘</ci><ci id="S5.p4.4.m4.2.2.cmml" xref="S5.p4.4.m4.2.2">ğ‘</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.2c">q,a</annotation></semantics></math> with <span id="S5.p4.5.3" class="ltx_text" style="color:#FF0000;">red</span> and the counterfactual <math id="S5.p4.5.m5.2" class="ltx_Math" alttext="q^{*},a^{*}" display="inline"><semantics id="S5.p4.5.m5.2a"><mrow id="S5.p4.5.m5.2.2.2" xref="S5.p4.5.m5.2.2.3.cmml"><msup id="S5.p4.5.m5.1.1.1.1" xref="S5.p4.5.m5.1.1.1.1.cmml"><mi id="S5.p4.5.m5.1.1.1.1.2" xref="S5.p4.5.m5.1.1.1.1.2.cmml">q</mi><mo id="S5.p4.5.m5.1.1.1.1.3" xref="S5.p4.5.m5.1.1.1.1.3.cmml">âˆ—</mo></msup><mo id="S5.p4.5.m5.2.2.2.3" xref="S5.p4.5.m5.2.2.3.cmml">,</mo><msup id="S5.p4.5.m5.2.2.2.2" xref="S5.p4.5.m5.2.2.2.2.cmml"><mi id="S5.p4.5.m5.2.2.2.2.2" xref="S5.p4.5.m5.2.2.2.2.2.cmml">a</mi><mo id="S5.p4.5.m5.2.2.2.2.3" xref="S5.p4.5.m5.2.2.2.2.3.cmml">âˆ—</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.2b"><list id="S5.p4.5.m5.2.2.3.cmml" xref="S5.p4.5.m5.2.2.2"><apply id="S5.p4.5.m5.1.1.1.1.cmml" xref="S5.p4.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.5.m5.1.1.1.1.1.cmml" xref="S5.p4.5.m5.1.1.1.1">superscript</csymbol><ci id="S5.p4.5.m5.1.1.1.1.2.cmml" xref="S5.p4.5.m5.1.1.1.1.2">ğ‘</ci><times id="S5.p4.5.m5.1.1.1.1.3.cmml" xref="S5.p4.5.m5.1.1.1.1.3"></times></apply><apply id="S5.p4.5.m5.2.2.2.2.cmml" xref="S5.p4.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S5.p4.5.m5.2.2.2.2.1.cmml" xref="S5.p4.5.m5.2.2.2.2">superscript</csymbol><ci id="S5.p4.5.m5.2.2.2.2.2.cmml" xref="S5.p4.5.m5.2.2.2.2.2">ğ‘</ci><times id="S5.p4.5.m5.2.2.2.2.3.cmml" xref="S5.p4.5.m5.2.2.2.2.3"></times></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.2c">q^{*},a^{*}</annotation></semantics></math> with <span id="S5.p4.5.4" class="ltx_text" style="color:#0000FF;">blue</span>.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Color Maximal explanations</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">In <span id="S5.SS1.p1.2.1" class="ltx_text ltx_font_bold">Color Maximal</span> substitutions, we notice that <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">M</annotation></semantics></math> erroneously maintains the same answer <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="a^{*}=a" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><msup id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml"><mi id="S5.SS1.p1.2.m2.1.1.2.2" xref="S5.SS1.p1.2.m2.1.1.2.2.cmml">a</mi><mo id="S5.SS1.p1.2.m2.1.1.2.3" xref="S5.SS1.p1.2.m2.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">=</mo><mi id="S5.SS1.p1.2.m2.1.1.3" xref="S5.SS1.p1.2.m2.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><eq id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1"></eq><apply id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS1.p1.2.m2.1.1.2.1.cmml" xref="S5.SS1.p1.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS1.p1.2.m2.1.1.2.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2.2">ğ‘</ci><times id="S5.SS1.p1.2.m2.1.1.2.3.cmml" xref="S5.SS1.p1.2.m2.1.1.2.3"></times></apply><ci id="S5.SS1.p1.2.m2.1.1.3.cmml" xref="S5.SS1.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">a^{*}=a</annotation></semantics></math> when we replace the colors <span id="S5.SS1.p1.2.2" class="ltx_text ltx_font_italic">gray</span> and <span id="S5.SS1.p1.2.3" class="ltx_text ltx_font_italic">silver</span> with any other semantically maximal color, either common or uncommon (underlined). Therefore, we detect a bias in the model related to these two colors, as it does not make logical decisions after replacing them with others and does not properly reason over this substitution. A relevant example is presented in Figure <a href="#S5.F3.sf1" title="In Figure 3 â€£ 5.1 Color Maximal explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">However, contrary to the above, <math id="S5.SS1.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p2.1.m1.1a"><mi id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b"><ci id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">M</annotation></semantics></math> logically revises its answers when we replace the colors <span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_italic">green</span> and <span id="S5.SS1.p2.1.2" class="ltx_text ltx_font_italic">red</span> with any distant colors, either common or uncommon (underlined) as presented in Figure <a href="#S5.F3.sf2" title="In Figure 3 â€£ 5.1 Color Maximal explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(b)</span></a>. Therefore, the model recognizes and qualitatively understands these substitutions and has not incorporated any problematic attachment regarding these two colors.</p>
</div>
<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/11268.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F3.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F3.sf1.4.2.1" class="ltx_text ltx_font_medium">: What is surrounding the <span id="S5.F3.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">silver</span>/<span id="S5.F3.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">black</span>/<span id="S5.F3.sf1.4.2.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">navy</span> fire hydrant? 
<br class="ltx_break"></span>a<span id="S5.F3.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F3.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">posts</span>/<span id="S5.F3.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">posts</span>/<span id="S5.F3.sf1.4.2.2.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">posts</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/1765.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="157" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F3.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F3.sf2.4.2.1" class="ltx_text ltx_font_medium">: How many glasses have <span id="S5.F3.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">red</span>/<span id="S5.F3.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">gold</span>/<span id="S5.F3.sf2.4.2.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">darkturquoise</span> wine? 
<br class="ltx_break"></span>a<span id="S5.F3.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F3.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">6</span>/<span id="S5.F3.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">0</span>/<span id="S5.F3.sf2.4.2.2.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">0</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.3.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F3.4.2.1" class="ltx_text ltx_font_bold">Color Maximal</span> counterfactual perturbations.</span></figcaption>
</figure>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.3" class="ltx_p">This observation denotes that <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">M</annotation></semantics></math> is more sensitive towards intense and visually distinct colors and rather bypasses changes involving more neutral ones, focusing on object identities (e.g. "fire hydrant" and "posts" of Figure <a href="#S5.F3.sf1" title="In Figure 3 â€£ 5.1 Color Maximal explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3(a)</span></a>). A more uncertain <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><msup id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mi id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">a</mi><mo id="S5.SS1.p3.2.m2.1.1.3" xref="S5.SS1.p3.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">superscript</csymbol><ci id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">ğ‘</ci><times id="S5.SS1.p3.2.m2.1.1.3.cmml" xref="S5.SS1.p3.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">a^{*}</annotation></semantics></math> (e.g. the modelâ€™s answer <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><msup id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml"><mi id="S5.SS1.p3.3.m3.1.1.2" xref="S5.SS1.p3.3.m3.1.1.2.cmml">a</mi><mo id="S5.SS1.p3.3.m3.1.1.3" xref="S5.SS1.p3.3.m3.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><apply id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS1.p3.3.m3.1.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">superscript</csymbol><ci id="S5.SS1.p3.3.m3.1.1.2.cmml" xref="S5.SS1.p3.3.m3.1.1.2">ğ‘</ci><times id="S5.SS1.p3.3.m3.1.1.3.cmml" xref="S5.SS1.p3.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">a^{*}</annotation></semantics></math> could be "nothing") would be more suitable, if all question semantics were equally taken into account.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Color Minimal explanations</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.3" class="ltx_p">Based on our experiments on semantically minimal color substitutions, we derive the following global rule: When we replace the colors <span id="S5.SS2.p1.3.1" class="ltx_text ltx_font_italic">gray</span> and <span id="S5.SS2.p1.3.2" class="ltx_text ltx_font_italic">purple</span> with any other closely related color, <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">M</annotation></semantics></math> tends to give the same answer <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="a^{*}=a" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><msup id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2.2" xref="S5.SS2.p1.2.m2.1.1.2.2.cmml">a</mi><mo id="S5.SS2.p1.2.m2.1.1.2.3" xref="S5.SS2.p1.2.m2.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">=</mo><mi id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><eq id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></eq><apply id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.2.m2.1.1.2.1.cmml" xref="S5.SS2.p1.2.m2.1.1.2">superscript</csymbol><ci id="S5.SS2.p1.2.m2.1.1.2.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2.2">ğ‘</ci><times id="S5.SS2.p1.2.m2.1.1.2.3.cmml" xref="S5.SS2.p1.2.m2.1.1.2.3"></times></apply><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">a^{*}=a</annotation></semantics></math>. Therefore, <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mi id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><ci id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">M</annotation></semantics></math> does not give due importance to this change of colors, a fact that highlights a robust behavior related to the two aforementioned colors. The model maintains this invariant behavior equally when we perform replacements with <span id="S5.SS2.p1.3.3" class="ltx_text ltx_font_italic">common</span> colors or with <span id="S5.SS2.p1.3.4" class="ltx_text ltx_font_italic">uncommon</span> ones, as presented in Figure <a href="#S5.F4.sf1" title="In Figure 4 â€£ 5.2 Color Minimal explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(a)</span></a>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p">In contrast, <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">M</annotation></semantics></math> redefines its answers when we replace the <span id="S5.SS2.p2.2.1" class="ltx_text ltx_font_italic">green</span> color with any other, <span id="S5.SS2.p2.2.2" class="ltx_text ltx_font_italic">common</span> or <span id="S5.SS2.p2.2.3" class="ltx_text ltx_font_italic">uncommon</span>, semantically similar color. Consequently, it is being confused by such minimal changes, failing to provide a meaningful answer, as shown in Figure <a href="#S5.F4.sf2" title="In Figure 4 â€£ 5.2 Color Minimal explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4(b)</span></a>. Even a more uncertain answer (e.g. "nothing") to counterfactual questions would be more suitable compared to the semantically divergent ones returned ("bus" and "bag" instead of "light").
Likewise, <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">M</annotation></semantics></math> presents a similar change in behavior when we replace the <span id="S5.SS2.p2.2.4" class="ltx_text ltx_font_italic">pink</span> color with common minimal colors and the <span id="S5.SS2.p2.2.5" class="ltx_text ltx_font_italic">silver</span> color with uncommon ones.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/6536.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F4.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F4.sf1.4.2.1" class="ltx_text ltx_font_medium">: What organizationâ€™s logo is on the <span id="S5.F4.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">purple</span>/<span id="S5.F4.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">blue</span>/<span id="S5.F4.sf1.4.2.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">plum</span> banner? 
<br class="ltx_break"></span>a<span id="S5.F4.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F4.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">olympics</span>/<span id="S5.F4.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">olympics</span>/<span id="S5.F4.sf1.4.2.2.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">olympics</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/388.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F4.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F4.sf2.4.2.1" class="ltx_text ltx_font_medium">: What is being held <span id="S5.F4.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">green</span>/<span id="S5.F4.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">forestgreen</span>/<span id="S5.F4.sf2.4.2.1.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">olive</span>? 
<br class="ltx_break"></span>a<span id="S5.F4.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F4.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">light</span>/<span id="S5.F4.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">bus</span>/<span id="S5.F4.sf2.4.2.2.3" class="ltx_text ltx_framed ltx_framed_underline" style="color:#0000FF;">bag</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F4.4.2.1" class="ltx_text ltx_font_bold">Color Minimal</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Synonym Adjectives explanations</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.3" class="ltx_p">In general, adjective-noun pairs present in questions contain some joint special conceptual meaning which differs from the independent meaning of adjectives when they exist autonomously and separately in a sentence. In this case, we notice that the model <math id="S5.SS3.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p1.1.m1.1a"><mi id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><ci id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">M</annotation></semantics></math> varies its answer <math id="S5.SS3.p1.2.m2.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.SS3.p1.2.m2.1a"><msup id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">a</mi><mo id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">superscript</csymbol><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">ğ‘</ci><times id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">a^{*}</annotation></semantics></math> when we implement a synonym substitution of its question adjectives. This finding suggests that <math id="S5.SS3.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p1.3.m3.1a"><mi id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><ci id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">M</annotation></semantics></math> can qualitatively perceive the meaning of such adjective-noun pairs and differentiate its response accordingly, as presented in Figure <a href="#S5.F5.sf1" title="In Figure 5 â€£ 5.3 Synonym Adjectives explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(a)</span></a>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.2" class="ltx_p">Another finding is related to the ability of <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mi id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><ci id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">M</annotation></semantics></math> to correctly adjust its answer, when it is presented with a lexically correct synonym to an adjective, which, however, is not quite appropriate for the given linguistic environment of the question. Accordingly, we conclude that <math id="S5.SS3.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p2.2.m2.1a"><mi id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><ci id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">M</annotation></semantics></math> is capable of understanding the meaning of an adjective in relation to the context of the sentence ("typical food" is meaningful, but "distinctive food" is not), as presented in Figure <a href="#S5.F5.sf2" title="In Figure 5 â€£ 5.3 Synonym Adjectives explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(b)</span></a>.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.3" class="ltx_p">In addition, we derive a global rule that concerns the behavior of <math id="S5.SS3.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p3.1.m1.1a"><mi id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><ci id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">M</annotation></semantics></math> when we replace an adjective having multiple meanings with one of its synonyms, which, although it is optimal with respect to the aforementioned meanings, is however not suitable to the semantic context of the substituted adjectives (such as "delicious" vs "delightful"). We note that in this case, <math id="S5.SS3.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p3.2.m2.1a"><mi id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><ci id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">M</annotation></semantics></math> demonstrates a stable behavior against such substitutions, which proves that it is able to reason over adjectives in a contextualized manner, without being fooled by synonyms not suitable to the exact context of the question <math id="S5.SS3.p3.3.m3.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S5.SS3.p3.3.m3.1a"><mi id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><ci id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">q</annotation></semantics></math>.
An example of this observation is provided in Figure <a href="#S5.F5.sf3" title="In Figure 5 â€£ 5.3 Synonym Adjectives explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(c)</span></a>.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.2" class="ltx_p">Size-related adjective substitutions are demonstrated in Figure <a href="#S5.F5.sf4" title="In Figure 5 â€£ 5.3 Synonym Adjectives explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(d)</span></a>. We observe that <math id="S5.SS3.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p4.1.m1.1a"><mi id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><ci id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">M</annotation></semantics></math> is particularly robust to such substitutions, therefore correctly capturing the underlying meaning without being biased towards specific words. This global rule demonstrates the flexibility of <math id="S5.SS3.p4.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p4.2.m2.1a"><mi id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><ci id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">M</annotation></semantics></math> towards appropriately handling semantically and contextually equivalent adjective substitutions.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">Finally, <math id="S5.SS3.p5.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS3.p5.1.m1.1a"><mi id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><ci id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">M</annotation></semantics></math> is proven to be unstable when it has to handle rare or difficult synonyms of adjectives, as the ones shown in Figure <a href="#S5.F5.sf5" title="In Figure 5 â€£ 5.3 Synonym Adjectives explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5(e)</span></a>. Consequently, it presents a lexical weakness in handling such rare adjectives and possibly a bias in specific words that are more familiar to it.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/138.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F5.sf1.4.2.1" class="ltx_text ltx_font_medium">: Is this a <span id="S5.F5.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">hot</span>/<span id="S5.F5.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">raging</span> dog? 
<br class="ltx_break"></span>a<span id="S5.F5.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F5.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F5.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">no</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/274.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F5.sf2.4.2.1" class="ltx_text ltx_font_medium">: Of what meal is this kind of food <span id="S5.F5.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">typical</span>/<span id="S5.F5.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">distinctive</span>? 
<br class="ltx_break"></span>a<span id="S5.F5.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F5.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">lunch</span>/<span id="S5.F5.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">hot dog</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/285.png" id="S5.F5.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F5.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F5.sf3.4.2.1" class="ltx_text ltx_font_medium">: How <span id="S5.F5.sf3.4.2.1.1" class="ltx_text" style="color:#FF0000;">delicious</span>/<span id="S5.F5.sf3.4.2.1.2" class="ltx_text" style="color:#0000FF;">delightful</span> does this look? 
<br class="ltx_break"></span>a<span id="S5.F5.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F5.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">very</span>/<span id="S5.F5.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">not very</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/347.png" id="S5.F5.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F5.sf4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F5.sf4.4.2.1" class="ltx_text ltx_font_medium">: Is this a <span id="S5.F5.sf4.4.2.1.1" class="ltx_text" style="color:#FF0000;">small</span>/<span id="S5.F5.sf4.4.2.1.2" class="ltx_text" style="color:#0000FF;">little</span> town? 
<br class="ltx_break"></span>a<span id="S5.F5.sf4.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F5.sf4.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F5.sf4.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F5.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/20.png" id="S5.F5.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F5.sf5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F5.sf5.4.2.1" class="ltx_text ltx_font_medium">: Does the man look <span id="S5.F5.sf5.4.2.1.1" class="ltx_text" style="color:#FF0000;">happy</span>/<span id="S5.F5.sf5.4.2.1.2" class="ltx_text" style="color:#0000FF;">felicitous</span>? 
<br class="ltx_break"></span>a<span id="S5.F5.sf5.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F5.sf5.4.2.2.1" class="ltx_text" style="color:#FF0000;">no</span>/<span id="S5.F5.sf5.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F5.4.2.1" class="ltx_text ltx_font_bold">Synonym Adjectives</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Synonym Verbs explanations</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Regarding substitutions involving verb synonyms, <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mi id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><ci id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">M</annotation></semantics></math> is not particularly stable when dealing with substitutions of verbs that present multiple meanings, as presented in Figure <a href="#S5.F6.sf1" title="In Figure 6 â€£ 5.4 Synonym Verbs explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(a)</span></a>. This indicates a difficulty in distinguishing the correct and desired meaning among multiple ones.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.2" class="ltx_p">An even more specific rule we extract is that <math id="S5.SS4.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS4.p2.1.m1.1a"><mi id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><ci id="S5.SS4.p2.1.m1.1.1.cmml" xref="S5.SS4.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">M</annotation></semantics></math> falsely changes its original answer <math id="S5.SS4.p2.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S5.SS4.p2.2.m2.1a"><mi id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><ci id="S5.SS4.p2.2.m2.1.1.cmml" xref="S5.SS4.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">a</annotation></semantics></math> when we replace the verb "see" with a qualitative synonym of it. A relevant example is provided in Figure <a href="#S5.F6.sf2" title="In Figure 6 â€£ 5.4 Synonym Verbs explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(b)</span></a>. This finding indicates an unwanted attachment of the model to the word "see", which is interpreted as bias. This is an example of a more general situation, where optimal synonyms may not be the best choice for a synonym in a specific contextual setting. In these kinds of instances, the model tends to change its response, as observed in this particular case.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p">The model <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mi id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><ci id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">M</annotation></semantics></math> presents satisfactory robustness when it has to deal with easy or common verbs of the English vocabulary, which means that it has acquired a certain degree of versatility in simple vocabulary challenges, as shown in Figure <a href="#S5.F6.sf3" title="In Figure 6 â€£ 5.4 Synonym Verbs explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(c)</span></a>.</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/46.png" id="S5.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F6.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F6.sf1.4.2.1" class="ltx_text ltx_font_medium">: What <span id="S5.F6.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">says</span>/<span id="S5.F6.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">state</span> STAPLES? 
<br class="ltx_break"></span>a<span id="S5.F6.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F6.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">nothing</span>/<span id="S5.F6.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">New York</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/527.png" id="S5.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F6.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F6.sf2.4.2.1" class="ltx_text ltx_font_medium">: Do you <span id="S5.F6.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">see</span>/<span id="S5.F6.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">understand</span> any motorcycle helmets? 
<br class="ltx_break"></span>a<span id="S5.F6.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F6.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">no</span>/<span id="S5.F6.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/37.png" id="S5.F6.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F6.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F6.sf3.4.2.1" class="ltx_text ltx_font_medium">: Are the walls <span id="S5.F6.sf3.4.2.1.1" class="ltx_text" style="color:#FF0000;">done</span>/<span id="S5.F6.sf3.4.2.1.2" class="ltx_text" style="color:#0000FF;">made</span> in a summery color? 
<br class="ltx_break"></span>a<span id="S5.F6.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F6.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F6.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/2419.png" id="S5.F6.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="102" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F6.sf4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F6.sf4.4.2.1" class="ltx_text ltx_font_medium">: What kind of birds are <span id="S5.F6.sf4.4.2.1.1" class="ltx_text" style="color:#FF0000;">pictured</span>/<span id="S5.F6.sf4.4.2.1.2" class="ltx_text" style="color:#0000FF;">visualized</span>? 
<br class="ltx_break"></span>a<span id="S5.F6.sf4.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F6.sf4.4.2.2.1" class="ltx_text" style="color:#FF0000;">parrots</span>/<span id="S5.F6.sf4.4.2.2.2" class="ltx_text" style="color:#0000FF;">parrots</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F6.4.2.1" class="ltx_text ltx_font_bold">Synonym Verbs</span> counterfactual perturbations.</span></figcaption>
</figure>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.5" class="ltx_p">Finally, <math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mi id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><ci id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">M</annotation></semantics></math> is rather stable when the replaced verb
corresponds to a noun counterpart (e.g. picture -verb-, picture -noun-) or even adjective counterpart (e.g. pictured), such as the ones of Figure <a href="#S5.F6.sf4" title="In Figure 6 â€£ 5.4 Synonym Verbs explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6(d)</span></a>. Consequently, <math id="S5.SS4.p4.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS4.p4.2.m2.1a"><mi id="S5.SS4.p4.2.m2.1.1" xref="S5.SS4.p4.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.2.m2.1b"><ci id="S5.SS4.p4.2.m2.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.2.m2.1c">M</annotation></semantics></math> is capable of capturing the general sense of such verbs in the context of the question; equivalent substitution of corresponding nouns (picture<math id="S5.SS4.p4.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS4.p4.3.m3.1a"><mo stretchy="false" id="S5.SS4.p4.3.m3.1.1" xref="S5.SS4.p4.3.m3.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.3.m3.1b"><ci id="S5.SS4.p4.3.m3.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.3.m3.1c">\rightarrow</annotation></semantics></math>visualization) or adjectives (pictured<math id="S5.SS4.p4.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS4.p4.4.m4.1a"><mo stretchy="false" id="S5.SS4.p4.4.m4.1.1" xref="S5.SS4.p4.4.m4.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.4.m4.1b"><ci id="S5.SS4.p4.4.m4.1.1.cmml" xref="S5.SS4.p4.4.m4.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.4.m4.1c">\rightarrow</annotation></semantics></math>visualized) would mostly yield the same counterfactual response <math id="S5.SS4.p4.5.m5.1" class="ltx_Math" alttext="a^{*}" display="inline"><semantics id="S5.SS4.p4.5.m5.1a"><msup id="S5.SS4.p4.5.m5.1.1" xref="S5.SS4.p4.5.m5.1.1.cmml"><mi id="S5.SS4.p4.5.m5.1.1.2" xref="S5.SS4.p4.5.m5.1.1.2.cmml">a</mi><mo id="S5.SS4.p4.5.m5.1.1.3" xref="S5.SS4.p4.5.m5.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.5.m5.1b"><apply id="S5.SS4.p4.5.m5.1.1.cmml" xref="S5.SS4.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.p4.5.m5.1.1.1.cmml" xref="S5.SS4.p4.5.m5.1.1">superscript</csymbol><ci id="S5.SS4.p4.5.m5.1.1.2.cmml" xref="S5.SS4.p4.5.m5.1.1.2">ğ‘</ci><times id="S5.SS4.p4.5.m5.1.1.3.cmml" xref="S5.SS4.p4.5.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.5.m5.1c">a^{*}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Hypernym Noun explanations</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">Throughout our Hypernym Noun substitutions, we conclude that <math id="S5.SS5.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS5.p1.1.m1.1a"><mi id="S5.SS5.p1.1.m1.1.1" xref="S5.SS5.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p1.1.m1.1b"><ci id="S5.SS5.p1.1.m1.1.1.cmml" xref="S5.SS5.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p1.1.m1.1c">M</annotation></semantics></math> is particularly robust against substitutions involving living creatures, such as animals or humans, which shows that it can properly reason over hierarchical relationships governing such concepts, as in Figure <a href="#S5.F7.sf1" title="In Figure 7 â€£ 5.5 Hypernym Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(a)</span></a>.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.2" class="ltx_p">On the contrary, <math id="S5.SS5.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS5.p2.1.m1.1a"><mi id="S5.SS5.p2.1.m1.1.1" xref="S5.SS5.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.1.m1.1b"><ci id="S5.SS5.p2.1.m1.1.1.cmml" xref="S5.SS5.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.1.m1.1c">M</annotation></semantics></math> does not clearly distinguish between concepts related to types of clothing, i.e. it tends to erroneously change its answer when replaced with a broader concept. Consequently, <math id="S5.SS5.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS5.p2.2.m2.1a"><mi id="S5.SS5.p2.2.m2.1.1" xref="S5.SS5.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p2.2.m2.1b"><ci id="S5.SS5.p2.2.m2.1.1.cmml" xref="S5.SS5.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p2.2.m2.1c">M</annotation></semantics></math> does not generalize well on such entities and a bias towards more specific and clear types of clothing emerges. A relevant example is presented in Figure <a href="#S5.F7.sf2" title="In Figure 7 â€£ 5.5 Hypernym Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(b)</span></a>.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.2" class="ltx_p">As an extension of the above, <math id="S5.SS5.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS5.p3.1.m1.1a"><mi id="S5.SS5.p3.1.m1.1.1" xref="S5.SS5.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.1.m1.1b"><ci id="S5.SS5.p3.1.m1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.1.m1.1c">M</annotation></semantics></math> exhibits instability in hypernym substitutions that are very broad, inclusive, and polysemous. Therefore, when we replace a noun with an optimal hypernym that presents much greater conceptual generality, <math id="S5.SS5.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS5.p3.2.m2.1a"><mi id="S5.SS5.p3.2.m2.1.1" xref="S5.SS5.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.2.m2.1b"><ci id="S5.SS5.p3.2.m2.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.2.m2.1c">M</annotation></semantics></math> is unable to qualitatively perceive the hierarchical relation that governs them, outputting a wrong answer, as in Figure <a href="#S5.F7.sf3" title="In Figure 7 â€£ 5.5 Hypernym Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7(c)</span></a>.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/713.png" id="S5.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F7.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F7.sf1.4.2.1" class="ltx_text ltx_font_medium">: Where is the <span id="S5.F7.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">cat</span>/<span id="S5.F7.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">feline</span>? 
<br class="ltx_break"></span>a<span id="S5.F7.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F7.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">bed</span>/<span id="S5.F7.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">bed</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/784.png" id="S5.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F7.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F7.sf2.4.2.1" class="ltx_text ltx_font_medium">: Are all the players wearing black <span id="S5.F7.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">shirts</span>/<span id="S5.F7.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">garment</span>? 
<br class="ltx_break"></span>a<span id="S5.F7.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F7.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">no</span>/<span id="S5.F7.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/863.png" id="S5.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F7.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F7.sf3.4.2.1" class="ltx_text ltx_font_medium">: Are there multiple vegetables on the <span id="S5.F7.sf3.4.2.1.1" class="ltx_text" style="color:#FF0000;">plate</span>/<span id="S5.F7.sf3.4.2.1.2" class="ltx_text" style="color:#0000FF;">base</span>? 
<br class="ltx_break"></span>a<span id="S5.F7.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F7.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F7.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">no</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F7.4.2.1" class="ltx_text ltx_font_bold">Hypernym Noun</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>Hyponyms Noun explanations</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">Similar to the hypernyms substitution experiment, <math id="S5.SS6.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS6.p1.1.m1.1a"><mi id="S5.SS6.p1.1.m1.1.1" xref="S5.SS6.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p1.1.m1.1b"><ci id="S5.SS6.p1.1.m1.1.1.cmml" xref="S5.SS6.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p1.1.m1.1c">M</annotation></semantics></math> is able to appropriately respond in cases where the substitutions of hyponyms refer to living entities. Therefore, the specialization in more specific living entities concepts is properly perceived, as presented in Figure <a href="#S5.F8.sf1" title="In Figure 8 â€£ 5.6 Hyponyms Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a>.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">Correspondingly, <math id="S5.SS6.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS6.p2.1.m1.1a"><mi id="S5.SS6.p2.1.m1.1.1" xref="S5.SS6.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS6.p2.1.m1.1b"><ci id="S5.SS6.p2.1.m1.1.1.cmml" xref="S5.SS6.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS6.p2.1.m1.1c">M</annotation></semantics></math> also shows stability in the substitutions of hyponyms that represent articles of clothing. Therefore, it specializes skillfully in more specific cloth-related entities and according to the case, it appropriately changes its response by adapting to the change. A relevant example is presented in Figure <a href="#S5.F8.sf2" title="In Figure 8 â€£ 5.6 Hyponyms Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p">On the contrary, the model does not demonstrate robustness to hyponym substitutions referring to means of transport. Consequently, the model is biased toward such broader concepts and fails to adequately understand their specialization, as shown in Figure <a href="#S5.F8.sf3" title="In Figure 8 â€£ 5.6 Hyponyms Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(c)</span></a>.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/85.png" id="S5.F8.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F8.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F8.sf1.4.2.1" class="ltx_text ltx_font_medium">: Are the <span id="S5.F8.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">animals</span>/<span id="S5.F8.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">acrodont</span> eating? 
<br class="ltx_break"></span>a<span id="S5.F8.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F8.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F8.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/784.png" id="S5.F8.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F8.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F8.sf2.4.2.1" class="ltx_text ltx_font_medium">: Are all the players wearing black <span id="S5.F8.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">shirts</span>/<span id="S5.F8.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">camise</span>? 
<br class="ltx_break"></span>a<span id="S5.F8.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F8.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">no</span>/<span id="S5.F8.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/2820.png" id="S5.F8.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F8.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F8.sf3.4.2.1" class="ltx_text ltx_font_medium">: What are objects behind the <span id="S5.F8.sf3.4.2.1.1" class="ltx_text" style="color:#FF0000;">motorcycles</span>/<span id="S5.F8.sf3.4.2.1.2" class="ltx_text" style="color:#0000FF;">minibike</span>? 
<br class="ltx_break"></span>a<span id="S5.F8.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F8.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">sign</span>/<span id="S5.F8.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">sign</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F8.4.2.1" class="ltx_text ltx_font_bold">Hyponyms Noun</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
<section id="S5.SS7" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7 </span>Sibling Noun explanations</h3>

<div id="S5.SS7.p1" class="ltx_para">
<p id="S5.SS7.p1.6" class="ltx_p">With reference to Sibling Noun substitutions, we notice as a global pattern that <math id="S5.SS7.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p1.1.m1.1a"><mi id="S5.SS7.p1.1.m1.1.1" xref="S5.SS7.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.1.m1.1b"><ci id="S5.SS7.p1.1.m1.1.1.cmml" xref="S5.SS7.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.1.m1.1c">M</annotation></semantics></math> has insufficient separation ability when the sibling nouns refer to rooms of buildings or houses. As an example, in Figure <a href="#S5.F9.sf1" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(a)</span></a>, <math id="S5.SS7.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p1.2.m2.1a"><mi id="S5.SS7.p1.2.m2.1.1" xref="S5.SS7.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.2.m2.1b"><ci id="S5.SS7.p1.2.m2.1.1.cmml" xref="S5.SS7.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.2.m2.1c">M</annotation></semantics></math> cannot properly differentiate between the described interior spaces and can be easily fooled by substitutions involving places of different functionality. <math id="S5.SS7.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p1.3.m3.1a"><mi id="S5.SS7.p1.3.m3.1.1" xref="S5.SS7.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.3.m3.1b"><ci id="S5.SS7.p1.3.m3.1.1.cmml" xref="S5.SS7.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.3.m3.1c">M</annotation></semantics></math> is also confused in the case of Figure <a href="#S5.F9.sf4" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(d)</span></a>, when sibling means of transport are substituted. Specifically, <math id="S5.SS7.p1.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p1.4.m4.1a"><mi id="S5.SS7.p1.4.m4.1.1" xref="S5.SS7.p1.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.4.m4.1b"><ci id="S5.SS7.p1.4.m4.1.1.cmml" xref="S5.SS7.p1.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.4.m4.1c">M</annotation></semantics></math> insists on its answer even though a concept not existing in the image appears (bike<math id="S5.SS7.p1.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS7.p1.5.m5.1a"><mo stretchy="false" id="S5.SS7.p1.5.m5.1.1" xref="S5.SS7.p1.5.m5.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.5.m5.1b"><ci id="S5.SS7.p1.5.m5.1.1.cmml" xref="S5.SS7.p1.5.m5.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.5.m5.1c">\rightarrow</annotation></semantics></math>truck). This indicates that <math id="S5.SS7.p1.6.m6.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p1.6.m6.1a"><mi id="S5.SS7.p1.6.m6.1.1" xref="S5.SS7.p1.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p1.6.m6.1b"><ci id="S5.SS7.p1.6.m6.1.1.cmml" xref="S5.SS7.p1.6.m6.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p1.6.m6.1c">M</annotation></semantics></math> rather trespasses the linguistic modality context, providing an â€™easyâ€™ answer based on the visual modality, since the only <span id="S5.SS7.p1.6.1" class="ltx_text ltx_font_italic">bird</span> appearing in the image is a <span id="S5.SS7.p1.6.2" class="ltx_text ltx_font_italic">parrot</span>. In this case, the relevant position of the bird <span id="S5.SS7.p1.6.3" class="ltx_text ltx_font_italic">on the manâ€™s bike/truck</span> is ignored.</p>
</div>
<div id="S5.SS7.p2" class="ltx_para">
<p id="S5.SS7.p2.7" class="ltx_p">An interesting behavior is observed when sibling concepts involving animals are tested, as in Figure <a href="#S5.F9.sf5" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(e)</span></a>. In this case, <math id="S5.SS7.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p2.1.m1.1a"><mi id="S5.SS7.p2.1.m1.1.1" xref="S5.SS7.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.1.m1.1b"><ci id="S5.SS7.p2.1.m1.1.1.cmml" xref="S5.SS7.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.1.m1.1c">M</annotation></semantics></math> seems to circumvent reasoning over the image, providing an answer based on knowledge it has most possibly acquired during its pre-training phase (zebras are black and white in color). Nevertheless, <math id="S5.SS7.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p2.2.m2.1a"><mi id="S5.SS7.p2.2.m2.1.1" xref="S5.SS7.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.2.m2.1b"><ci id="S5.SS7.p2.2.m2.1.1.cmml" xref="S5.SS7.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.2.m2.1c">M</annotation></semantics></math> is not fooled by the horse<math id="S5.SS7.p2.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS7.p2.3.m3.1a"><mo stretchy="false" id="S5.SS7.p2.3.m3.1.1" xref="S5.SS7.p2.3.m3.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.3.m3.1b"><ci id="S5.SS7.p2.3.m3.1.1.cmml" xref="S5.SS7.p2.3.m3.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.3.m3.1c">\rightarrow</annotation></semantics></math>zebra substitution, in which case it would conclude that <span id="S5.SS7.p2.7.1" class="ltx_text ltx_font_italic">the zebra is brown</span>, which is a wrong factual statement.
Another case that <math id="S5.SS7.p2.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p2.4.m4.1a"><mi id="S5.SS7.p2.4.m4.1.1" xref="S5.SS7.p2.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.4.m4.1b"><ci id="S5.SS7.p2.4.m4.1.1.cmml" xref="S5.SS7.p2.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.4.m4.1c">M</annotation></semantics></math> is not being fooled is depicted in Figure <a href="#S5.F9.sf3" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(c)</span></a>. In this case, <math id="S5.SS7.p2.5.m5.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p2.5.m5.1a"><mi id="S5.SS7.p2.5.m5.1.1" xref="S5.SS7.p2.5.m5.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.5.m5.1b"><ci id="S5.SS7.p2.5.m5.1.1.cmml" xref="S5.SS7.p2.5.m5.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.5.m5.1c">M</annotation></semantics></math> is very consistent in sibling entities that declare human body parts, which means that it correctly perceives their differences and does not group them in an arbitrary way. Furthermore, <math id="S5.SS7.p2.6.m6.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p2.6.m6.1a"><mi id="S5.SS7.p2.6.m6.1.1" xref="S5.SS7.p2.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.6.m6.1b"><ci id="S5.SS7.p2.6.m6.1.1.cmml" xref="S5.SS7.p2.6.m6.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.6.m6.1c">M</annotation></semantics></math> presents a correct reasoning process by differentiating its answer when the sibling concepts present very different meanings between them, as the concepts air<math id="S5.SS7.p2.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.SS7.p2.7.m7.1a"><mo stretchy="false" id="S5.SS7.p2.7.m7.1.1" xref="S5.SS7.p2.7.m7.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S5.SS7.p2.7.m7.1b"><ci id="S5.SS7.p2.7.m7.1.1.cmml" xref="S5.SS7.p2.7.m7.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p2.7.m7.1c">\rightarrow</annotation></semantics></math>water in Figure <a href="#S5.F9.sf2" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(b)</span></a>.</p>
</div>
<div id="S5.SS7.p3" class="ltx_para">
<p id="S5.SS7.p3.3" class="ltx_p">Overall, Siblings Noun substitution provided a rich set of insights, unequally relying on either <math id="S5.SS7.p3.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S5.SS7.p3.1.m1.1a"><mi id="S5.SS7.p3.1.m1.1.1" xref="S5.SS7.p3.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p3.1.m1.1b"><ci id="S5.SS7.p3.1.m1.1.1.cmml" xref="S5.SS7.p3.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p3.1.m1.1c">q</annotation></semantics></math> or <math id="S5.SS7.p3.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S5.SS7.p3.2.m2.1a"><mi id="S5.SS7.p3.2.m2.1.1" xref="S5.SS7.p3.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p3.2.m2.1b"><ci id="S5.SS7.p3.2.m2.1.1.cmml" xref="S5.SS7.p3.2.m2.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p3.2.m2.1c">I</annotation></semantics></math> to derive an answer in many cases, rather than providing an uncertain outcome (such as answering "nothing" in the examples of Figures <a href="#S5.F9.sf4" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(d)</span></a>, <a href="#S5.F9.sf5" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(e)</span></a>, similarly to the correct reasoning of Figure <a href="#S5.F9.sf3" title="In Figure 9 â€£ 5.7 Sibling Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9(c)</span></a>). In total, this indicates an unstable behavior of <math id="S5.SS7.p3.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS7.p3.3.m3.1a"><mi id="S5.SS7.p3.3.m3.1.1" xref="S5.SS7.p3.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS7.p3.3.m3.1b"><ci id="S5.SS7.p3.3.m3.1.1.cmml" xref="S5.SS7.p3.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS7.p3.3.m3.1c">M</annotation></semantics></math> towards different sibling pairs, yielding unpredictable outcomes under different substitutions of the same conceptual distance.</p>
</div>
<figure id="S5.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/2605.png" id="S5.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F9.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F9.sf1.4.2.1" class="ltx_text ltx_font_medium">: Is the <span id="S5.F9.sf1.4.2.1.1" class="ltx_text" style="color:#FF0000;">bathroom</span>/<span id="S5.F9.sf1.4.2.1.2" class="ltx_text" style="color:#0000FF;">workroom</span> organized? 
<br class="ltx_break"></span>a<span id="S5.F9.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F9.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F9.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">yes</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/137.png" id="S5.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F9.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F9.sf2.4.2.1" class="ltx_text ltx_font_medium">: Are those kites in the <span id="S5.F9.sf2.4.2.1.1" class="ltx_text" style="color:#FF0000;">air</span>/<span id="S5.F9.sf2.4.2.1.2" class="ltx_text" style="color:#0000FF;">water</span>? 
<br class="ltx_break"></span>a<span id="S5.F9.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F9.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">yes</span>/<span id="S5.F9.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">no</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/1792.png" id="S5.F9.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F9.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F9.sf3.4.2.1" class="ltx_text ltx_font_medium">: What is she wearing on her <span id="S5.F9.sf3.4.2.1.1" class="ltx_text" style="color:#FF0000;">head</span>/<span id="S5.F9.sf3.4.2.1.2" class="ltx_text" style="color:#0000FF;">throat</span>? 
<br class="ltx_break"></span>a<span id="S5.F9.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F9.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">helmet</span>/<span id="S5.F9.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">nothing</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/800.png" id="S5.F9.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F9.sf4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F9.sf4.4.2.1" class="ltx_text ltx_font_medium">: What bird is on the manâ€™s <span id="S5.F9.sf4.4.2.1.1" class="ltx_text" style="color:#FF0000;">bike</span>/<span id="S5.F9.sf4.4.2.1.2" class="ltx_text" style="color:#0000FF;">truck</span>? 
<br class="ltx_break"></span>a<span id="S5.F9.sf4.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F9.sf4.4.2.2.1" class="ltx_text" style="color:#FF0000;">parrot</span>/<span id="S5.F9.sf4.4.2.2.2" class="ltx_text" style="color:#0000FF;">parrot</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_many">
<figure id="S5.F9.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/1468.png" id="S5.F9.sf5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="99" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.sf5.3.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F9.sf5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F9.sf5.4.2.1" class="ltx_text ltx_font_medium">: What color is the <span id="S5.F9.sf5.4.2.1.1" class="ltx_text" style="color:#FF0000;">horse</span>/<span id="S5.F9.sf5.4.2.1.2" class="ltx_text" style="color:#0000FF;">zebra</span>? 
<br class="ltx_break"></span>a<span id="S5.F9.sf5.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F9.sf5.4.2.2.1" class="ltx_text" style="color:#FF0000;">brown</span>/<span id="S5.F9.sf5.4.2.2.2" class="ltx_text" style="color:#0000FF;">black and white</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F9.4.2.1" class="ltx_text ltx_font_bold">Sibling Noun</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
<section id="S5.SS8" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8 </span>Deletion Noun explanations</h3>

<div id="S5.SS8.p1" class="ltx_para">
<p id="S5.SS8.p1.2" class="ltx_p">Regarding the counterfactual questions concerning deletions of nouns, we firstly observe the following pattern: When the deleted noun has a determining role in another noun already present in the question, <math id="S5.SS8.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p1.1.m1.1a"><mi id="S5.SS8.p1.1.m1.1.1" xref="S5.SS8.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p1.1.m1.1b"><ci id="S5.SS8.p1.1.m1.1.1.cmml" xref="S5.SS8.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p1.1.m1.1c">M</annotation></semantics></math> maintains its original answer even after the deletion. Therefore, we detect a tendency towards attaching to the determined noun, while at the same time not paying due attention to the determiner noun. Hence, <math id="S5.SS8.p1.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p1.2.m2.1a"><mi id="S5.SS8.p1.2.m2.1.1" xref="S5.SS8.p1.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p1.2.m2.1b"><ci id="S5.SS8.p1.2.m2.1.1.cmml" xref="S5.SS8.p1.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p1.2.m2.1c">M</annotation></semantics></math> answers such questions arbitrarily, even though its answer cannot be perceived as wrong; a human could have also answered the same, especially in yes/no questions. A related example is provided in Figure <a href="#S5.F10.sf1" title="In Figure 10 â€£ 5.8 Deletion Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(a)</span></a>.</p>
</div>
<div id="S5.SS8.p2" class="ltx_para">
<p id="S5.SS8.p2.2" class="ltx_p">Another pattern that we detect concerns questions that refer to the color of a noun, which has been deleted. In these cases, <math id="S5.SS8.p2.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p2.1.m1.1a"><mi id="S5.SS8.p2.1.m1.1.1" xref="S5.SS8.p2.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p2.1.m1.1b"><ci id="S5.SS8.p2.1.m1.1.1.cmml" xref="S5.SS8.p2.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p2.1.m1.1c">M</annotation></semantics></math> tends to respond with the most dominant color in the image, without taking into account the absence of the noun that this color should define, as in Figure <a href="#S5.F10.sf2" title="In Figure 10 â€£ 5.8 Deletion Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(b)</span></a>. Of course, we regard this behavior as justified, since a human would most probably answer such questions in the same way.
Similarly, in questions concerning the location of a noun, which has been deleted, <math id="S5.SS8.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p2.2.m2.1a"><mi id="S5.SS8.p2.2.m2.1.1" xref="S5.SS8.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p2.2.m2.1b"><ci id="S5.SS8.p2.2.m2.1.1.cmml" xref="S5.SS8.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p2.2.m2.1c">M</annotation></semantics></math> answers with the most dominant entity present in the given image. A relevant example is demonstrated in Figure <a href="#S5.F10.sf3" title="In Figure 10 â€£ 5.8 Deletion Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(c)</span></a>.</p>
</div>
<div id="S5.SS8.p3" class="ltx_para">
<p id="S5.SS8.p3.2" class="ltx_p">Finally, we list some nouns to which we notice that <math id="S5.SS8.p3.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p3.1.m1.1a"><mi id="S5.SS8.p3.1.m1.1.1" xref="S5.SS8.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p3.1.m1.1b"><ci id="S5.SS8.p3.1.m1.1.1.cmml" xref="S5.SS8.p3.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p3.1.m1.1c">M</annotation></semantics></math> does not pay due attention when asked to give an answer, as in Figure <a href="#S5.F10.sf4" title="In Figure 10 â€£ 5.8 Deletion Noun explanations â€£ 5 Experiments â€£ Knowledge-Based Counterfactual Queries for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(d)</span></a>. Specifically, even after deleting them, <math id="S5.SS8.p3.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS8.p3.2.m2.1a"><mi id="S5.SS8.p3.2.m2.1.1" xref="S5.SS8.p3.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS8.p3.2.m2.1b"><ci id="S5.SS8.p3.2.m2.1.1.cmml" xref="S5.SS8.p3.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS8.p3.2.m2.1c">M</annotation></semantics></math> tends to return the initial answer with great frequency. These words are: image, photographs, human, man, animal, room. In general, these are words that are encountered very often in questions and usually act in addition to other, more specific, entities. Once again, this behavior is justified.</p>
</div>
<div id="S5.SS8.p4" class="ltx_para">
<p id="S5.SS8.p4.1" class="ltx_p">All in all, we observe that the random deletion of a noun results in a rather expected model behavior, driven by dominant visual concepts present in the given image.</p>
</div>
<figure id="S5.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/3891.png" id="S5.F10.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F10.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F10.sf1.4.2.1" class="ltx_text ltx_font_medium">: Is the womanâ€™s <span id="S5.F10.sf1.4.2.1.1" class="ltx_text" style="text-decoration:line-through; text-decoration-color:;">hair</span> tied back? 
<br class="ltx_break"></span>a<span id="S5.F10.sf1.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F10.sf1.4.2.2.1" class="ltx_text" style="color:#FF0000;">no</span>/<span id="S5.F10.sf1.4.2.2.2" class="ltx_text" style="color:#0000FF;">no</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/331.png" id="S5.F10.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F10.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F10.sf2.4.2.1" class="ltx_text ltx_font_medium">: What color is the <span id="S5.F10.sf2.4.2.1.1" class="ltx_text" style="text-decoration:line-through; text-decoration-color:;">bathroom</span>? 
<br class="ltx_break"></span>a<span id="S5.F10.sf2.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F10.sf2.4.2.2.1" class="ltx_text" style="color:#FF0000;">yellow</span>/<span id="S5.F10.sf2.4.2.2.2" class="ltx_text" style="color:#0000FF;">white</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F10.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/2363.png" id="S5.F10.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F10.sf3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F10.sf3.4.2.1" class="ltx_text ltx_font_medium">: Where are the <span id="S5.F10.sf3.4.2.1.1" class="ltx_text" style="text-decoration:line-through; text-decoration-color:;">cakes</span>? 
<br class="ltx_break"></span>a<span id="S5.F10.sf3.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F10.sf3.4.2.2.1" class="ltx_text" style="color:#FF0000;">table</span>/<span id="S5.F10.sf3.4.2.2.2" class="ltx_text" style="color:#0000FF;">table</span>.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S5.F10.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2303.02601/assets/images/99.png" id="S5.F10.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F10.sf4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">q<span id="S5.F10.sf4.4.2.1" class="ltx_text ltx_font_medium">: How many animals are in this <span id="S5.F10.sf4.4.2.1.1" class="ltx_text" style="text-decoration:line-through; text-decoration-color:;">photo</span>? 
<br class="ltx_break"></span>a<span id="S5.F10.sf4.4.2.2" class="ltx_text ltx_font_medium">: <span id="S5.F10.sf4.4.2.2.1" class="ltx_text" style="color:#FF0000;">2</span>/<span id="S5.F10.sf4.4.2.2.2" class="ltx_text" style="color:#0000FF;">2</span>.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F10.3.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S5.F10.4.2" class="ltx_text" style="font-size:90%;">Local explanations for <span id="S5.F10.4.2.1" class="ltx_text ltx_font_bold">Deletion Noun</span> counterfactual perturbations.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Counterfactual perturbations in VQA models can provide novel and useful insights regarding model robustness and explainability of results. In our work, we
propose a knowledge-based counterfactual framework targeting substitutions on questions. Specifically, our framework suggests multiple types of word-level linguistic transformations in order to probe selected VQA models in a black-box fashion, and investigate whether the presence of counterfactual questions will lead to unexpected model responses. Through this process, underlying linguistic biases are revealed, while informative explanations regarding the modelâ€™s behavior are provided, by extracting global rules in a qualitative manner, ultimately depicting those existing biases.
Our results on Visual Genome and VQA-v2 datasets, using ViLT model as proof of concept, illustrate the
merits for our approach, highlighting concepts that incite model biases, in a model-agnostic manner. As an immediate extension of our method, we aim to apply the same linguistic perturbations on dataset answers, addressing VQA models that reason over multiple choice answers. As future work, we plan to expand our approach to other related visiolinguistic tasks, such as Text - Image Retrieval, Visual
Entailment, and Visual Commonsense Reasoning, while another direction involves crafting counterfactual perturbations targeting the visual modality.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
The research work was supported by the Hellenic Foundation for Research and Innovation (HFRI) under the 3rd Call for HFRI PhD Fellowships (Fellowship Number 5537).

</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mogadala etÂ al. [2021]</span>
<span class="ltx_bibblock">
A.Â Mogadala, M.Â Kalimuthu,
D.Â Klakow,

</span>
<span class="ltx_bibblock">Trends in integration of vision and language
research: A survey of tasks, datasets, and methods,

</span>
<span class="ltx_bibblock">Journal of Artificial Intelligence Research
71 (2021) 1183â€“1317.
URL: <a target="_blank" href="http://dx.doi.org/10.1613/jair.1.11688" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1613/jair.1.11688</a>.
doi:<a target="_blank" href="https:/doi.org/10.1613/jair.1.11688" title="" class="ltx_ref">10.1613/jair.1.11688</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. [2022]</span>
<span class="ltx_bibblock">
Y.Â Du, Z.Â Liu, J.Â Li,
W.Â Zhao,

</span>
<span class="ltx_bibblock">A survey of vision-language pre-trained models
(2022).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lymperaiou and Stamou [2022]</span>
<span class="ltx_bibblock">
M.Â Lymperaiou, G.Â Stamou,

</span>
<span class="ltx_bibblock">A survey on knowledge-enhanced multimodal learning,

</span>
<span class="ltx_bibblock">ArXiv abs/2211.12328
(2022).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2019]</span>
<span class="ltx_bibblock">
L.Â H. Li, M.Â Yatskar,
D.Â Yin, C.-J. Hsieh,
K.-W. Chang, Visualbert: A simple and
performant baseline for vision and language, 2019.
URL: <a target="_blank" href="https://arxiv.org/abs/1908.03557" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1908.03557</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1908.03557" title="" class="ltx_ref">10.48550/ARXIV.1908.03557</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. [2021]</span>
<span class="ltx_bibblock">
W.Â Kim, B.Â Son, I.Â Kim,
Vilt: Vision-and-language transformer without convolution or
region supervision, 2021. URL: <a target="_blank" href="https://arxiv.org/abs/2102.03334" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2102.03334</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2102.03334" title="" class="ltx_ref">10.48550/ARXIV.2102.03334</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. [2019]</span>
<span class="ltx_bibblock">
J.Â Lu, D.Â Batra,
D.Â Parikh, S.Â Lee,
Vilbert: Pretraining task-agnostic visiolinguistic
representations for vision-and-language tasks, 2019.
URL: <a target="_blank" href="https://arxiv.org/abs/1908.02265" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1908.02265</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1908.02265" title="" class="ltx_ref">10.48550/ARXIV.1908.02265</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kazemi and Elqursh [2017]</span>
<span class="ltx_bibblock">
V.Â Kazemi, A.Â Elqursh,
Show, ask, attend, and answer: A strong baseline for visual
question answering, 2017. URL: <a target="_blank" href="https://arxiv.org/abs/1704.03162" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1704.03162</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1704.03162" title="" class="ltx_ref">10.48550/ARXIV.1704.03162</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. [2021]</span>
<span class="ltx_bibblock">
A.Â Singh, R.Â Hu,
V.Â Goswami, G.Â Couairon,
W.Â Galuba, M.Â Rohrbach,
D.Â Kiela, Flava: A foundational language
and vision alignment model, 2021. URL: <a target="_blank" href="https://arxiv.org/abs/2112.04482" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2112.04482</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2112.04482" title="" class="ltx_ref">10.48550/ARXIV.2112.04482</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2021]</span>
<span class="ltx_bibblock">
J.Â Li, R.Â R. Selvaraju,
A.Â D. Gotmare, S.Â Joty,
C.Â Xiong, S.Â Hoi, Align
before fuse: Vision and language representation learning with momentum
distillation, 2021. URL: <a target="_blank" href="https://arxiv.org/abs/2107.07651" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2107.07651</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2107.07651" title="" class="ltx_ref">10.48550/ARXIV.2107.07651</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">El-Nouby etÂ al. [2019]</span>
<span class="ltx_bibblock">
A.Â El-Nouby, S.Â Sharma,
H.Â Schulz, D.Â Hjelm,
L.Â E. Asri, S.Â E. Kahou,
Y.Â Bengio, G.Â W. Taylor,
Tell, draw, and repeat: Generating and modifying images based
on continual linguistic instruction, 2019.
<a target="_blank" href="http://arxiv.org/abs/1811.09845" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1811.09845</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey [2021]</span>
<span class="ltx_bibblock">
S.Â R. Dubey,

</span>
<span class="ltx_bibblock">A decade survey of content based image retrieval
using deep learning,

</span>
<span class="ltx_bibblock">IEEE Transactions on Circuits and Systems for Video
Technology (2021) 1â€“1. URL: <a target="_blank" href="http://dx.doi.org/10.1109/TCSVT.2021.3080920" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://dx.doi.org/10.1109/TCSVT.2021.3080920</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/tcsvt.2021.3080920" title="" class="ltx_ref">10.1109/tcsvt.2021.3080920</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers etÂ al. [2019]</span>
<span class="ltx_bibblock">
R.Â Zellers, Y.Â Bisk,
A.Â Farhadi, Y.Â Choi, From
recognition to cognition: Visual commonsense reasoning,
2019. <a target="_blank" href="http://arxiv.org/abs/1811.10830" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1811.10830</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baker etÂ al. [2021]</span>
<span class="ltx_bibblock">
K.Â Baker, A.Â Parekh,
A.Â Fabre, A.Â Addlesee,
R.Â Kruiper, O.Â Lemon,

</span>
<span class="ltx_bibblock">The spoon is in the sink: Assisting visually impaired
people in the kitchen,

</span>
<span class="ltx_bibblock">in: Proceedings of the Reasoning and Interaction
Conference (ReInAct 2021), Association for Computational
Linguistics, Gothenburg, Sweden, 2021,
pp. 32â€“39. URL: <a target="_blank" href="https://aclanthology.org/2021.reinact-1.5" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.reinact-1.5</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2022]</span>
<span class="ltx_bibblock">
C.Â Chen, S.Â Anjum,
D.Â Gurari, Grounding answers for visual
questions asked by visually impaired people, 2022.
URL: <a target="_blank" href="https://arxiv.org/abs/2202.01993" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2202.01993</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2202.01993" title="" class="ltx_ref">10.48550/ARXIV.2202.01993</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben-Younes etÂ al. [2022]</span>
<span class="ltx_bibblock">
H.Â Ben-Younes, Ãƒâ€°loi Zablocki,
P.Â PÃƒÂ©rez, M.Â Cord,

</span>
<span class="ltx_bibblock">Driving behavior explanation with multi-level
fusion,

</span>
<span class="ltx_bibblock">Pattern Recognition 123
(2022) 108421. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0031320321005975" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0031320321005975</a>.
doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.patcog.2021.108421" title="" class="ltx_ref">https://doi.org/10.1016/j.patcog.2021.108421</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cadene etÂ al. [2019]</span>
<span class="ltx_bibblock">
R.Â Cadene, C.Â Dancette,
H.Â Ben-younes, M.Â Cord,
D.Â Parikh,

</span>
<span class="ltx_bibblock">Rubi: Reducing unimodal biases in visual question
answering (2019). URL: <a target="_blank" href="https://arxiv.org/abs/1906.10169" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1906.10169</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1906.10169" title="" class="ltx_ref">10.48550/ARXIV.1906.10169</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarwal etÂ al. [2019]</span>
<span class="ltx_bibblock">
V.Â Agarwal, R.Â Shetty,
M.Â Fritz, Towards causal vqa: Revealing and
reducing spurious correlations by invariant and covariant semantic editing,
2019. URL: <a target="_blank" href="https://arxiv.org/abs/1912.07538" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1912.07538</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1912.07538" title="" class="ltx_ref">10.48550/ARXIV.1912.07538</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2020]</span>
<span class="ltx_bibblock">
L.Â Chen, X.Â Yan, J.Â Xiao,
H.Â Zhang, S.Â Pu,
Y.Â Zhuang, Counterfactual samples
synthesizing for robust visual question answering, 2020.
URL: <a target="_blank" href="https://arxiv.org/abs/2003.06576" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.06576</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2003.06576" title="" class="ltx_ref">10.48550/ARXIV.2003.06576</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson etÂ al. [2017]</span>
<span class="ltx_bibblock">
P.Â Anderson, X.Â He,
C.Â Buehler, D.Â Teney,
M.Â Johnson, S.Â Gould,
L.Â Zhang, Bottom-up and top-down attention
for image captioning and visual question answering, 2017.
URL: <a target="_blank" href="https://arxiv.org/abs/1707.07998" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1707.07998</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1707.07998" title="" class="ltx_ref">10.48550/ARXIV.1707.07998</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. [2016]</span>
<span class="ltx_bibblock">
Y.Â Goyal, T.Â Khot,
D.Â Summers-Stay, D.Â Batra,
D.Â Parikh, Making the v in vqa matter:
Elevating the role of image understanding in visual question answering,
2016. URL: <a target="_blank" href="https://arxiv.org/abs/1612.00837" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1612.00837</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1612.00837" title="" class="ltx_ref">10.48550/ARXIV.1612.00837</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boukhers etÂ al. [2022]</span>
<span class="ltx_bibblock">
Z.Â Boukhers, T.Â Hartmann,
J.Â JÃ¼rjens, Coin: Counterfactual image
generation for vqa interpretation, 2022. URL: <a target="_blank" href="https://arxiv.org/abs/2201.03342" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.03342</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2201.03342" title="" class="ltx_ref">10.48550/ARXIV.2201.03342</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal etÂ al. [2015]</span>
<span class="ltx_bibblock">
A.Â Agrawal, J.Â Lu,
S.Â Antol, M.Â Mitchell,
C.Â L. Zitnick, D.Â Batra,
D.Â Parikh, Vqa: Visual question answering,
2015. URL: <a target="_blank" href="https://arxiv.org/abs/1505.00468" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1505.00468</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1505.00468" title="" class="ltx_ref">10.48550/ARXIV.1505.00468</a>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kafle and Kanan [2017]</span>
<span class="ltx_bibblock">
K.Â Kafle, C.Â Kanan,

</span>
<span class="ltx_bibblock">Visual question answering: Datasets, algorithms, and
future challenges,

</span>
<span class="ltx_bibblock">Computer Vision and Image Understanding
163 (2017) 3â€“20.
URL: <a target="_blank" href="https://doi.org/10.1016%2Fj.cviu.2017.06.005" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1016%2Fj.cviu.2017.06.005</a>.
doi:<a target="_blank" href="https:/doi.org/10.1016/j.cviu.2017.06.005" title="" class="ltx_ref">10.1016/j.cviu.2017.06.005</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna etÂ al. [2016]</span>
<span class="ltx_bibblock">
R.Â Krishna, Y.Â Zhu,
O.Â Groth, J.Â Johnson,
K.Â Hata, J.Â Kravitz,
S.Â Chen, Y.Â Kalantidis,
L.-J. Li, D.Â A. Shamma,
M.Â S. Bernstein, F.-F. Li,
Visual genome: Connecting language and vision using
crowdsourced dense image annotations, 2016. URL: <a target="_blank" href="https://arxiv.org/abs/1602.07332" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1602.07332</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1602.07332" title="" class="ltx_ref">10.48550/ARXIV.1602.07332</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fellbaum [1998]</span>
<span class="ltx_bibblock">
C.Â Fellbaum,

</span>
<span class="ltx_bibblock">Wordnet: An electronic lexical database
(1998).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plummer etÂ al. [2015]</span>
<span class="ltx_bibblock">
B.Â A. Plummer, L.Â Wang,
C.Â M. Cervantes, J.Â C. Caicedo,
J.Â Hockenmaier, S.Â Lazebnik,

</span>
<span class="ltx_bibblock">Flickr30k entities: Collecting region-to-phrase
correspondences for richer image-to-sentence models,

</span>
<span class="ltx_bibblock">in: 2015 IEEE International Conference on
Computer Vision (ICCV), 2015, pp.
2641â€“2649. doi:<a target="_blank" href="https:/doi.org/10.1109/ICCV.2015.303" title="" class="ltx_ref">10.1109/ICCV.2015.303</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. [2015]</span>
<span class="ltx_bibblock">
M.Â Ren, R.Â Kiros,
R.Â Zemel, Exploring models and data for
image question answering, 2015.
<a target="_blank" href="http://arxiv.org/abs/1505.02074" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1505.02074</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. [2016]</span>
<span class="ltx_bibblock">
Y.Â Zhu, O.Â Groth,
M.Â Bernstein, L.Â Fei-Fei,

</span>
<span class="ltx_bibblock">Visual7w: Grounded question answering in images,

</span>
<span class="ltx_bibblock">2016, pp. 4995â€“5004.
doi:<a target="_blank" href="https:/doi.org/10.1109/CVPR.2016.540" title="" class="ltx_ref">10.1109/CVPR.2016.540</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou and Xie [2020]</span>
<span class="ltx_bibblock">
Y.Â Zou, Q.Â Xie,

</span>
<span class="ltx_bibblock">A survey on VQA: Datasets and approaches,

</span>
<span class="ltx_bibblock">in: 2020 2nd International Conference on
Information Technology and Computer Application (ITCA),
IEEE, 2020. URL: <a target="_blank" href="https://doi.org/10.1109%2Fitca52113.2020.00069" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109%2Fitca52113.2020.00069</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/itca52113.2020.00069" title="" class="ltx_ref">10.1109/itca52113.2020.00069</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banchhor and Singh [2021]</span>
<span class="ltx_bibblock">
M.Â Banchhor, P.Â Singh,

</span>
<span class="ltx_bibblock">A survey on visual question answering,

</span>
<span class="ltx_bibblock">in: 2021 2nd Global Conference for Advancement in
Technology (GCAT), 2021, pp. 1â€“5.
doi:<a target="_blank" href="https:/doi.org/10.1109/GCAT52182.2021.9587797" title="" class="ltx_ref">10.1109/GCAT52182.2021.9587797</a>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma and Jalal [2021]</span>
<span class="ltx_bibblock">
H.Â Sharma, A.Â S. Jalal,

</span>
<span class="ltx_bibblock">A survey of methods, datasets and evaluation metrics
for visual question answering,

</span>
<span class="ltx_bibblock">Image and Vision Computing 116
(2021) 104327. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0262885621002328" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0262885621002328</a>.
doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.imavis.2021.104327" title="" class="ltx_ref">https://doi.org/10.1016/j.imavis.2021.104327</a>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panesar etÂ al. [2022]</span>
<span class="ltx_bibblock">
A.Â Panesar, F.Â I. DoÄŸan,
I.Â Leite,

</span>
<span class="ltx_bibblock">Improving visual question answering by leveraging
depth and adapting explainability,

</span>
<span class="ltx_bibblock">in: 2022 31st IEEE International Conference on
Robot and Human Interactive Communication (RO-MAN), 2022,
pp. 252â€“259.
doi:<a target="_blank" href="https:/doi.org/10.1109/RO-MAN53752.2022.9900586" title="" class="ltx_ref">10.1109/RO-MAN53752.2022.9900586</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alipour etÂ al. [2020]</span>
<span class="ltx_bibblock">
K.Â Alipour, J.Â P. Schulze,
Y.Â Yao, A.Â Ziskind,
G.Â Burachas,

</span>
<span class="ltx_bibblock">A study on multimodal and interactive explanations
for visual question answering (2020). URL: <a target="_blank" href="https://arxiv.org/abs/2003.00431" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2003.00431</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2003.00431" title="" class="ltx_ref">10.48550/ARXIV.2003.00431</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang etÂ al. [2019]</span>
<span class="ltx_bibblock">
J.-H. Huang, M.Â Alfadly,
B.Â Ghanem, M.Â Worring,
Assessing the robustness of visual question answering
models, 2019. URL: <a target="_blank" href="https://arxiv.org/abs/1912.01452" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1912.01452</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1912.01452" title="" class="ltx_ref">10.48550/ARXIV.1912.01452</a>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. [2016]</span>
<span class="ltx_bibblock">
J.Â Lu, J.Â Yang, D.Â Batra,
D.Â Parikh, Hierarchical question-image
co-attention for visual question answering, 2016. URL: <a target="_blank" href="https://arxiv.org/abs/1606.00061" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1606.00061</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1606.00061" title="" class="ltx_ref">10.48550/ARXIV.1606.00061</a>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. [2020]</span>
<span class="ltx_bibblock">
M.Â Jiang, S.Â Chen,
J.Â Yang, Q.Â Zhao,

</span>
<span class="ltx_bibblock">Fantastic answers and where to find them: Immersive
question-directed visual attention,

</span>
<span class="ltx_bibblock">in: 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2020, pp.
2977â€“2986. doi:<a target="_blank" href="https:/doi.org/10.1109/CVPR42600.2020.00305" title="" class="ltx_ref">10.1109/CVPR42600.2020.00305</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sammani etÂ al. [2022]</span>
<span class="ltx_bibblock">
F.Â Sammani, T.Â Mukherjee,
N.Â Deligiannis, Nlx-gpt: A model for
natural language explanations in vision and vision-language tasks,
2022. URL: <a target="_blank" href="https://arxiv.org/abs/2203.05081" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.05081</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2203.05081" title="" class="ltx_ref">10.48550/ARXIV.2203.05081</a>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2021]</span>
<span class="ltx_bibblock">
L.Â Chen, Y.Â Zheng,
Y.Â Niu, H.Â Zhang,
J.Â Xiao, Counterfactual samples
synthesizing and training for robust visual question answering,
2021. URL: <a target="_blank" href="https://arxiv.org/abs/2110.01013" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.01013</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2110.01013" title="" class="ltx_ref">10.48550/ARXIV.2110.01013</a>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbasnejad etÂ al. [2020]</span>
<span class="ltx_bibblock">
E.Â Abbasnejad, D.Â Teney,
A.Â Parvaneh, J.Â Shi,
A.Â vanÂ den Hengel,

</span>
<span class="ltx_bibblock">Counterfactual vision and language learning,

</span>
<span class="ltx_bibblock">in: 2020 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR), 2020, pp.
10041â€“10051. doi:<a target="_blank" href="https:/doi.org/10.1109/CVPR42600.2020.01006" title="" class="ltx_ref">10.1109/CVPR42600.2020.01006</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. [2020]</span>
<span class="ltx_bibblock">
Z.Â Liang, W.Â Jiang,
H.Â Hu, J.Â Zhu,

</span>
<span class="ltx_bibblock">Learning to contrast the counterfactual samples for
robust visual question answering,

</span>
<span class="ltx_bibblock">in: Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
Association for Computational Linguistics,
Online, 2020, pp.
3285â€“3292. URL: <a target="_blank" href="https://aclanthology.org/2020.emnlp-main.265" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.emnlp-main.265</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/2020.emnlp-main.265" title="" class="ltx_ref">10.18653/v1/2020.emnlp-main.265</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al. [2019]</span>
<span class="ltx_bibblock">
S.Â Ren, Y.Â Deng, K.Â He,
W.Â Che,

</span>
<span class="ltx_bibblock">Generating natural language adversarial examples
through probability weighted word saliency,

</span>
<span class="ltx_bibblock">in: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, Association
for Computational Linguistics, Florence, Italy,
2019, pp. 1085â€“1097. URL: <a target="_blank" href="https://aclanthology.org/P19-1103" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1103</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/P19-1103" title="" class="ltx_ref">10.18653/v1/P19-1103</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei and Zou [2019]</span>
<span class="ltx_bibblock">
J.Â Wei, K.Â Zou, Eda: Easy
data augmentation techniques for boosting performance on text classification
tasks, 2019. URL: <a target="_blank" href="https://arxiv.org/abs/1901.11196" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1901.11196</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.1901.11196" title="" class="ltx_ref">10.48550/ARXIV.1901.11196</a>.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Garg and Ramakrishnan [2020]</span>
<span class="ltx_bibblock">
S.Â Garg, G.Â Ramakrishnan,

</span>
<span class="ltx_bibblock">BAE: BERT-based adversarial examples for text
classification,

</span>
<span class="ltx_bibblock">in: Proceedings of the 2020 Conference on
Empirical Methods in Natural Language Processing (EMNLP),
Association for Computational Linguistics,
Online, 2020, pp.
6174â€“6181. URL: <a target="_blank" href="https://aclanthology.org/2020.emnlp-main.498" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.emnlp-main.498</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/2020.emnlp-main.498" title="" class="ltx_ref">10.18653/v1/2020.emnlp-main.498</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morris etÂ al. [2020]</span>
<span class="ltx_bibblock">
J.Â X. Morris, E.Â Lifland,
J.Â Y. Yoo, J.Â Grigsby,
D.Â Jin, Y.Â Qi,
Textattack: A framework for adversarial attacks, data
augmentation, and adversarial training in nlp, 2020.
URL: <a target="_blank" href="https://arxiv.org/abs/2005.05909" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2005.05909</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2005.05909" title="" class="ltx_ref">10.48550/ARXIV.2005.05909</a>.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimi etÂ al. [2021]</span>
<span class="ltx_bibblock">
A.Â Karimi, L.Â Rossi,
A.Â Prati,

</span>
<span class="ltx_bibblock">AEDA: An easier data augmentation technique for
text classification,

</span>
<span class="ltx_bibblock">in: Findings of the Association for Computational
Linguistics: EMNLP 2021, Association for Computational
Linguistics, Punta Cana, Dominican Republic,
2021, pp. 2748â€“2754. URL: <a target="_blank" href="https://aclanthology.org/2021.findings-emnlp.234" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.findings-emnlp.234</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/2021.findings-emnlp.234" title="" class="ltx_ref">10.18653/v1/2021.findings-emnlp.234</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lymperaiou etÂ al. [2022]</span>
<span class="ltx_bibblock">
M.Â Lymperaiou, G.Â Manoliadis,
O.Â M. Mastromichalakis, E.Â G. Dervakos,
G.Â Stamou, Towards explainable evaluation
of language models on the semantic similarity of visual concepts,
2022. URL: <a target="_blank" href="https://arxiv.org/abs/2209.03723" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2209.03723</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2209.03723" title="" class="ltx_ref">10.48550/ARXIV.2209.03723</a>.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan etÂ al. [2022]</span>
<span class="ltx_bibblock">
J.Â Wan, J.Â Yang, S.Â Ma,
D.Â Zhang, W.Â Zhang,
Y.Â Yu, Z.Â Li, Paeg:
Phrase-level adversarial example generation for neural machine translation,
2022. URL: <a target="_blank" href="https://arxiv.org/abs/2201.02009" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.02009</a>.
doi:<a target="_blank" href="https:/doi.org/10.48550/ARXIV.2201.02009" title="" class="ltx_ref">10.48550/ARXIV.2201.02009</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.02600" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.02601" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.02601">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.02601" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.02602" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 20:48:04 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
