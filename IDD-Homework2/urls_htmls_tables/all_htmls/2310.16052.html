<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.16052] Synthetic Data as Validation</title><meta property="og:description" content="This study leverages synthetic data as a validation set to reduce overfitting and ease the selection of the best model in AI development. While synthetic data have been used for augmenting the training set, we find tha…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data as Validation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data as Validation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.16052">

<!--Generated on Tue Feb 27 22:07:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Synthetic Data as Validation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Qixin Hu 
<br class="ltx_break">The Chinese University of Hong Kong 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">qixinhu@cuhk.edu.hk</span>
&amp;Alan Yuille 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_typewriter">ayuille1@jhu.edu</span>
&amp;Zongwei Zhou 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_text ltx_font_typewriter">zzhou82@jh.edu</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Correponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p"><span id="id1.1.1" class="ltx_text">This study leverages synthetic data as a validation set to reduce overfitting and ease the selection of the best model in AI development. While synthetic data have been used for augmenting the training set, we find that synthetic data can also significantly diversify the validation set, offering marked advantages in domains like healthcare, where data are typically limited, sensitive, and from out-domain sources (i.e., hospitals). In this study, we illustrate the effectiveness of synthetic data for early cancer detection in computed tomography (CT) volumes, where synthetic tumors are generated and superimposed onto healthy organs, thereby creating an extensive dataset for rigorous validation. Using synthetic data as validation can improve AI robustness in both in-domain and out-domain test sets. Furthermore, we establish a new continual learning framework that continuously trains AI models on a stream of out-domain data with synthetic tumors. The AI model trained and validated in dynamically expanding synthetic data can consistently outperform models trained and validated exclusively on real-world data. Specifically, the DSC score for liver tumor segmentation improves from 26.7% (95% CI: 22.6%–30.9%) to 34.5% (30.8%–38.2%) when evaluated on an in-domain dataset and from 31.1% (26.0%–36.2%) to 35.4% (32.1%–38.7%) on an out-domain dataset. Importantly, the performance gain is particularly significant in identifying very tiny liver tumors (radius <math id="id1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><lt id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">&lt;</annotation></semantics></math> 5mm) in CT volumes, with Sensitivity improving from 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain dataset, justifying the efficacy in early detection of cancer. The application of synthetic data, from both training and validation perspectives, underlines a promising avenue to enhance AI robustness when dealing with data from varying domains. As open science, we have released the codes and models at <a target="_blank" href="https://github.com/MrGiovanni/SyntheticValidation" title="" class="ltx_ref ltx_href">https://github.com/MrGiovanni/SyntheticValidation</a>.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Standard AI development divides the dataset into a training set and a test set; the former is used for model training and the latter for evaluation <cite class="ltx_cite ltx_citemacro_citep">(Russell, <a href="#bib.bib41" title="" class="ltx_ref">2010</a>; Gareth et al., <a href="#bib.bib12" title="" class="ltx_ref">2013</a>)</cite>. The AI model is updated every <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">training epochs</span>, resulting in a number of intermediate models during the training trajectory. The performance of these models tends to improve on the training set, but this does not mean that the performance on the test set also improves due to the over-fitting problem <cite class="ltx_cite ltx_citemacro_citep">(Kuhn et al., <a href="#bib.bib23" title="" class="ltx_ref">2013</a>)</cite>. A question then arises: <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">How do we identify the best model that performs well on the test set, especially when it is evaluated on test sets taken from different domains?</span> A prevalent strategy is to delineate a validation set from the training set <cite class="ltx_cite ltx_citemacro_citep">(Ripley, <a href="#bib.bib38" title="" class="ltx_ref">2007</a>)</cite>. This validation set neither contributes to training nor to evaluating the AI performance. Instead, it functions as an independent set to fix the training hyper-parameters and, more importantly, to estimate the performance of each model on different datasets, thus enabling the selection of the best model from the many intermediate models during the training trajectory.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The validation set is often kept small. Naturally, we would like to maximize the use of the training data. Annotating data for AI training is time-consuming and expensive, requiring specialized expertise, so the annotated datasets are limited in size in many fields <cite class="ltx_cite ltx_citemacro_citep">(Zhou, <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>. Allocating too many annotated data for validation would inevitably diminish the training set size and compromise the AI training. On the other hand, the validation set should be sufficiently representative to provide a reliable performance estimate on unseen data. An overly small validation set might risk the reliability of performance estimation and checkpoint selection. As a result, the calibration of the validation set remains largely empirical and lacks systematic investigation for better alternatives to select the best checkpoint. Fulfilling this knowledge gap is particularly important in scenarios where real-world data are scarce, sensitive, or costly to collect and annotate, as seen in the field of <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">AI for healthcare</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite>. Therefore, our study uses early detection of cancerous tumors in computed tomography (CT) volumes as a demonstration. While early detection of cancer holds immense clinical potential, it faces profound constraints like disease prevalence and annotation difficulty to collect examples of early-stage tumors <cite class="ltx_cite ltx_citemacro_citep">(Crosby et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. The scarcity of annotated early cancer not only constrains the data available for validation but also amplifies the overfitting problem inherent in a small, biased validation set, potentially causing underdiagnosis and overdiagnosis.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose using synthetic data as validation, a strategy that guarantees the full utilization of the training set while ensuring ample data diversity for validation. Data synthesis has held longstanding interest and presents numerous intriguing merits for augmenting training and test data <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Gao et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> as reviewed in §<a href="#S2" title="2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, but its use in validation has seldom been explored. We find that synthetic data can facilitate a more reliable performance estimate on unseen data and effectively address the constraints commonly associated with small, biased validation sets. Specifically, we synthesize tumors in the healthy liver, which gives us orders of magnitude larger datasets for training. To ensure the realism of the synthetic tumors, we employ a modeling-based strategy <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> to simulate cancerous tumors with controlled shape, size, texture, location, and intensity. The use of diverse, healthy CT volumes, supplemented with synthetic tumors, as validation has demonstrated efficacy in mitigating model overfitting and enhancing the selection of checkpoints. Furthermore, we relieve the pressing demand for human annotations to train AI models by utilizing CT volumes with synthetic tumors as the training set. We then assess the model’s performance using a substantial number of publicly available, fully-annotated CT volumes with real-world cancerous tumors, showing that our models generalize well to these volumes from different hospitals and accurately segment the tumors at their early stage.
Our findings can be summarized as follows:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The best model checkpoint, selected by standard AI development with an in-domain real-tumor validation set, may not necessarily be generalized to unseen data, especially for an out-domain test set. This limitation arises from the validation set failing to adequately represent corner cases.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The best model checkpoint, selected by our strategy with a diverse synthetic-tumor validation set, tends to be generalized well to unseen data. This is because the validation set can cover theoretically infinite examples of possible cancerous tumors across diverse conditions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We introduce a novel continual learning framework. This framework integrates a continuous stream of synthetic data, characterized by diverse data distribution, for both training and validation. Traditional validation sets, constrained by static and limited in-domain real tumors, fall short in such a setting, whereas our synthetic tumors can be dynamically tailored to align with emerging distributions. Importantly, our framework can continuously generate tumors spanning a spectrum of sizes—from small to large—enhancing the detection rate of tumors at their early stages.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Although our study focuses on AI in healthcare, the insight should be pertinent to various imaging applications within the field of computer vision. However, at the time this paper is written, very few studies in computer vision have provided evidence that training <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">exclusively</span> on generated synthetic data can match or surpass the performance achieved when trained on real data <cite class="ltx_cite ltx_citemacro_citep">(Black et al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>. In specific applications, integrating synthetic data with real data—essentially acting as data augmentation—has been found empirically to boost AI performance <cite class="ltx_cite ltx_citemacro_citep">(Mu et al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Luzi et al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Azizi et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Burg et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. In this regard, data synthesis—cancerous tumor synthesis in particular—in medical imaging is relatively more successful<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The greater success of data synthesis in medical imaging (reviewed in §<a href="#S2" title="2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), compared with computer vision, can be attributed to two factors from our perspective. Firstly, the focus is primarily on synthesizing tumors rather than other components of the human anatomy. Secondly, the synthesis of tumors in 3D medical images is less complex as it does not require considerations for intricate variables such as lighting conditions, pose, and occlusion, which are typical in computer vision tasks.</span></span></span> with specific applications benefiting more from training exclusively on synthetic data than real data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">The dilemma of validation.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the field of machine learning, it is customary to use finite, static datasets with a pre-defined data split. While this standard offers a fair benchmark for comparing different AI models, it does not accurately represent real-world learning conditions. Two more realistic scenarios often arise in practice.</p>
</div>
<div id="S2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">The first scenario is the <span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">small data regime</span>, commonly observed in medical applications due to constraints like disease prevalence and annotation difficulty <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. In such cases, curating an appropriate validation set poses a conundrum. A large validation set would compromise the size of the training set, whereas a small one may not sufficiently estimate the model’s performance. Despite its critical importance, this issue has yet to receive adequate attention in the field.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The second scenario involves dealing with <span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">a stream of data</span>, in a context of continual learning where the model encounters a continuous flow of new data <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam et al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>. A finite, static validation set proves unsuitable as it cannot accurately assess the model’s capability in processing an extensive and diverse data range. We argue that a validation set—made up of real-world data—might not be needed during the training stage in such situations. Given the vastness of the training data, overfitting can be naturally avoided. Consequently, selecting the last-epoch model checkpoint could be a judicious choice.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Progresses in data synthesis.</span> Real-world data often encounters challenges such as poor quality, limited quantity, and inaccessibility <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Qu et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Kang et al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite>. To tackle these obstacles, the notion of <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">synthetic data</span> has emerged as a practical alternative, allowing for the generation of samples as needed <cite class="ltx_cite ltx_citemacro_citep">(Jordon et al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>; Yoon et al., <a href="#bib.bib52" title="" class="ltx_ref">2019</a>; Chen et al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. This approach has proven valuable in addressing data limitations and facilitating machine learning processes, including computer vision <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Ramesh et al., <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, natural language processing <cite class="ltx_cite ltx_citemacro_citep">(Collobert and Weston, <a href="#bib.bib9" title="" class="ltx_ref">2008</a>; Brown et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, voice <cite class="ltx_cite ltx_citemacro_citep">(Oord et al., <a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>, and many other fields <cite class="ltx_cite ltx_citemacro_citep">(Wiese et al., <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Jin et al., <a href="#bib.bib18" title="" class="ltx_ref">2018</a>; Zheng et al., <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>. In the medical domain, the practice of data synthesis—<span id="S2.p4.1.3" class="ltx_text ltx_font_italic">tumor synthesis</span> in particular—endeavors to produce artificial tumors in the image, which can significantly diversify the data and annotations for AI training <cite class="ltx_cite ltx_citemacro_citep">(Xing et al., <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> and, arguably, can strengthen the AI robustness evaluation using corner cases generated by data synthesis. Successful works related to tumor synthesis include polyp detection from colonoscopy videos <cite class="ltx_cite ltx_citemacro_citep">(Shin et al., <a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite>, COVID-19 detection from Chest CT and X-ray <cite class="ltx_cite ltx_citemacro_citep">(Yao et al., <a href="#bib.bib51" title="" class="ltx_ref">2021</a>; Lyu et al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Gao et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, diabetic lesion detection from retinal images <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>, cancer detection from fluorescence microscopy images <cite class="ltx_cite ltx_citemacro_citep">(Horvath et al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, brain tumor detection from MRI <cite class="ltx_cite ltx_citemacro_citep">(Wyatt et al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, early pancreatic cancer localization from CT <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, and pneumonia detection from ultrasound <cite class="ltx_cite ltx_citemacro_citep">(Yu et al., <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>. A recent study <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib17" title="" class="ltx_ref">2022</a>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> indicated that AI trained <span id="S2.p4.1.4" class="ltx_text ltx_font_italic">exclusively</span> on synthetic tumors can segment liver tumors with accuracy comparable to that on real tumors.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To the best of our knowledge, data synthesis has been widely recognized for its contribution to enhancing training and test datasets <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Jordon et al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>; Liu et al., <a href="#bib.bib29" title="" class="ltx_ref">2023b</a>)</cite>, but its capacity for improving the validation set remains largely untapped. In this paper, we extend the application of synthetic data to the validation set, enabling the full use of the annotated data for AI training while ensuring diverse and comprehensive validation data in the framework of continual learning.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2310.16052/assets/fig_methods.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.6.1" class="ltx_text ltx_font_bold">Illustration of continual learning framework.</span> <span id="S2.F1.7.2" class="ltx_text ltx_font_bold">(a)</span> The setting for the static training, where the real-tumor dataset is partitioned into training and validation sets. The AI model is then developed using these datasets and subsequently tested with unseen data. <span id="S2.F1.8.3" class="ltx_text ltx_font_bold">(b)</span> Dynamic training setting integrated with synthetic data. <span id="S2.F1.9.4" class="ltx_text ltx_font_bold">(c)</span> Tumor generator pipeline. By leveraging an advanced tumor generator, we can create a dynamic training and validation set. Models developed using the continual learning framework exhibit superior performance compared to static training settings. <span id="S2.F1.10.5" class="ltx_text ltx_font_bold">(d)</span> Visualization of synthetic data.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method &amp; Material</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Continual Learning for Tumor Segmentation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">According to <cite class="ltx_cite ltx_citemacro_citet">Van de Ven and Tolias (<a href="#bib.bib45" title="" class="ltx_ref">2019</a>); Van de Ven et al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite>, continual learning can be categorized into three settings: class-incremental learning, task-incremental learning, and domain-incremental learning. In the domain-incremental setting, which is relevant to our situation, the task remains the same while the data distribution changes. More specifically, the model sequentially encounters data from a continuum of domains (datasets):</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\{X_{1},Y_{1}\},\{X_{2},Y_{2}\},...,\{X_{N},Y_{N}\}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1"><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.4.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">X</mi><mn id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.2.2.1.1.1.1.2.4" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml">Y</mi><mn id="S3.E1.m1.2.2.1.1.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.2.5" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">}</mo></mrow><mo id="S3.E1.m1.2.2.1.1.3.4" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.2.1.1.2.cmml">X</mi><mn id="S3.E1.m1.2.2.1.1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.2.1.1.3.cmml">2</mn></msub><mo id="S3.E1.m1.2.2.1.1.2.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml">Y</mi><mn id="S3.E1.m1.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.5" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">}</mo></mrow><mo id="S3.E1.m1.2.2.1.1.3.5" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">…</mi><mo id="S3.E1.m1.2.2.1.1.3.6" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.3" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.3.3.1.1" xref="S3.E1.m1.2.2.1.1.3.3.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.3.1.1.2" xref="S3.E1.m1.2.2.1.1.3.3.1.1.2.cmml">X</mi><mi id="S3.E1.m1.2.2.1.1.3.3.1.1.3" xref="S3.E1.m1.2.2.1.1.3.3.1.1.3.cmml">N</mi></msub><mo id="S3.E1.m1.2.2.1.1.3.3.2.4" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.3.3.2.2" xref="S3.E1.m1.2.2.1.1.3.3.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.3.3.2.2.2" xref="S3.E1.m1.2.2.1.1.3.3.2.2.2.cmml">Y</mi><mi id="S3.E1.m1.2.2.1.1.3.3.2.2.3" xref="S3.E1.m1.2.2.1.1.3.3.2.2.3.cmml">N</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.5" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">}</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><list id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.3"><set id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">𝑋</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2">𝑌</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3">1</cn></apply></set><set id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><apply id="S3.E1.m1.2.2.1.1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1.2">𝑋</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1.3">2</cn></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2">𝑌</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3">2</cn></apply></set><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">…</ci><set id="S3.E1.m1.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2"><apply id="S3.E1.m1.2.2.1.1.3.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.3.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.3.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1.2">𝑋</ci><ci id="S3.E1.m1.2.2.1.1.3.3.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1.3">𝑁</ci></apply><apply id="S3.E1.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2.2">𝑌</ci><ci id="S3.E1.m1.2.2.1.1.3.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2.3">𝑁</ci></apply></set></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\{X_{1},Y_{1}\},\{X_{2},Y_{2}\},...,\{X_{N},Y_{N}\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.3" class="ltx_p">The objective is to train a model <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathscr{F}:X\rightarrow Y" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathscript" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">ℱ</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">:</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">X</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">→</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">Y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><ci id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">:</ci><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ℱ</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><ci id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">→</ci><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">𝑋</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathscr{F}:X\rightarrow Y</annotation></semantics></math> that can be effectively queried at any given time, regardless of the data distribution. In liver tumor segmentation tasks, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">X</annotation></semantics></math> is the CT volume and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">Y</annotation></semantics></math> is the tumor mask. The continuum of domains refers to CT volumes taken from different medical centers.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the setting of <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">static</span> training, shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), the AI model is trained and validated on fixed subsets of a dataset. This setting presents three limitations: Firstly, the limited scales and acquisition sources of the data, coupled with unchanged data distribution, pose challenges in generalizing to out-domain data. Secondly, the task of data annotation, specifically tumor annotation, is exceptionally challenging as it often requires the use of corroborative pathology reports. This requirement adds to the difficulty of extending the dataset. Thirdly, there are specific cases, such as extremely small tumors, where obtaining real data becomes significantly challenging. As a result, static training will likely result in biased, sub-optimal performance on unseen data, especially for out-domain test sets.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In contrast, the setting of <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">dynamic</span> training achieved by synthetic data, shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b), can overcome the aforementioned limitations. In this setting, the AI model is trained and validated on a dynamically changing dataset. In our study, this dynamic dataset is a stream of normal CT volumes—over 40 million CT volumes in the United States each year. Generating synthetic tumors is advantageous because, firstly, acquiring healthy CT volumes is much easier than obtaining those with cancerous tumors. As a result, our continual learning framework can start from a diverse dataset comprising CT volumes of healthy subjects from multiple domains. Secondly, by controlling the parameters within our framework, we have the ability to generate synthetic data that fulfills specific requirements, including those of a tiny radius (<math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><lt id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">&lt;</annotation></semantics></math>5 mm, shown in Appendix Figure <a href="#A2.F9" title="Figure 9 ‣ Appendix B Shape Examples ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Consequently, our framework achieves a noteworthy level of diversity, encompassing a wide array of variations. Therefore, the AI model developed using this framework of synthetic data has the potential to improve its performance on out-domain data.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Modeling-Based Synthetic Tumor Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Following the standardized clinical guidance and statistical distribution of real tumors, as detailed in Appendix Figure <a href="#A1.F7" title="Figure 7 ‣ Appendix A Distribution of Real Tumors ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> <a href="#A1.F8" title="Figure 8 ‣ Appendix A Distribution of Real Tumors ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we develop a modeling-based strategy to generate synthetic tumors. For example, according to the Liver Imaging Reporting and Data System (LI-RADS) <cite class="ltx_cite ltx_citemacro_citep">(Chernyak et al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, the malignancy of hepatocellular carcinomas is determined by shape, size, location, and texture, enhancing capsule appearance. We use a sequence of morphological image-processing operations to model real tumors, as shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c). The tumor generator consists of four steps: (1) location selection, (2) shape generation, (3) texture generation, and (4) post-processing.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Location selection.</span> Liver tumors generally do not allow the passage of preexisting blood vessels from the host tissue through them. To address this concern, we initially perform voxel value thresholding for vessel segmentation <cite class="ltx_cite ltx_citemacro_citep">(Gonzalez, <a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>. Utilizing the vessel mask acquired from this step enables us to identify if a particular location can cause the tumor-blood collision.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.5" class="ltx_p"><span id="S3.I1.i2.p1.5.1" class="ltx_text ltx_font_bold">Shape generation.</span> Based on clinical knowledge, a tumor is initiated from a malignant cell and gradually proliferates and expands, resulting in a nearly spherical shape for small tumors (<math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mo id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><leq id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\leq</annotation></semantics></math>5mm). On the other hand, statistical distributions of real liver tumors indicate that larger tumors tend to exhibit an elliptical shape. This observation has inspired us to generate a tumor-like shape using an ellipsoid <math id="S3.I1.i2.p1.2.m2.3" class="ltx_Math" alttext="\textit{ellip}(a,b,c)" display="inline"><semantics id="S3.I1.i2.p1.2.m2.3a"><mrow id="S3.I1.i2.p1.2.m2.3.4" xref="S3.I1.i2.p1.2.m2.3.4.cmml"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.2.m2.3.4.2" xref="S3.I1.i2.p1.2.m2.3.4.2a.cmml">ellip</mtext><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.2.m2.3.4.1" xref="S3.I1.i2.p1.2.m2.3.4.1.cmml">​</mo><mrow id="S3.I1.i2.p1.2.m2.3.4.3.2" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.I1.i2.p1.2.m2.3.4.3.2.1" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">(</mo><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.2.m2.3.4.3.2.2" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.2.m2.2.2" xref="S3.I1.i2.p1.2.m2.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.2.m2.3.4.3.2.3" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.2.m2.3.3" xref="S3.I1.i2.p1.2.m2.3.3.cmml">c</mi><mo stretchy="false" id="S3.I1.i2.p1.2.m2.3.4.3.2.4" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.3b"><apply id="S3.I1.i2.p1.2.m2.3.4.cmml" xref="S3.I1.i2.p1.2.m2.3.4"><times id="S3.I1.i2.p1.2.m2.3.4.1.cmml" xref="S3.I1.i2.p1.2.m2.3.4.1"></times><ci id="S3.I1.i2.p1.2.m2.3.4.2a.cmml" xref="S3.I1.i2.p1.2.m2.3.4.2"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.2.m2.3.4.2.cmml" xref="S3.I1.i2.p1.2.m2.3.4.2">ellip</mtext></ci><vector id="S3.I1.i2.p1.2.m2.3.4.3.1.cmml" xref="S3.I1.i2.p1.2.m2.3.4.3.2"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">𝑎</ci><ci id="S3.I1.i2.p1.2.m2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2">𝑏</ci><ci id="S3.I1.i2.p1.2.m2.3.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3">𝑐</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.3c">\textit{ellip}(a,b,c)</annotation></semantics></math>, where <math id="S3.I1.i2.p1.3.m3.3" class="ltx_Math" alttext="a,b,c" display="inline"><semantics id="S3.I1.i2.p1.3.m3.3a"><mrow id="S3.I1.i2.p1.3.m3.3.4.2" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.3.m3.3.4.2.1" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml">,</mo><mi id="S3.I1.i2.p1.3.m3.2.2" xref="S3.I1.i2.p1.3.m3.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.3.m3.3.4.2.2" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml">,</mo><mi id="S3.I1.i2.p1.3.m3.3.3" xref="S3.I1.i2.p1.3.m3.3.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.3b"><list id="S3.I1.i2.p1.3.m3.3.4.1.cmml" xref="S3.I1.i2.p1.3.m3.3.4.2"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">𝑎</ci><ci id="S3.I1.i2.p1.3.m3.2.2.cmml" xref="S3.I1.i2.p1.3.m3.2.2">𝑏</ci><ci id="S3.I1.i2.p1.3.m3.3.3.cmml" xref="S3.I1.i2.p1.3.m3.3.3">𝑐</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.3c">a,b,c</annotation></semantics></math> are the lengths of the semi-axes. Additionally, we utilize elastic deformation <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite> to enhance the authenticity of the generated tumor shapes <math id="S3.I1.i2.p1.4.m4.5" class="ltx_Math" alttext="D(\textit{ellip}(a,b,c),\sigma_{d})" display="inline"><semantics id="S3.I1.i2.p1.4.m4.5a"><mrow id="S3.I1.i2.p1.4.m4.5.5" xref="S3.I1.i2.p1.4.m4.5.5.cmml"><mi id="S3.I1.i2.p1.4.m4.5.5.4" xref="S3.I1.i2.p1.4.m4.5.5.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.4.m4.5.5.3" xref="S3.I1.i2.p1.4.m4.5.5.3.cmml">​</mo><mrow id="S3.I1.i2.p1.4.m4.5.5.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml"><mo stretchy="false" id="S3.I1.i2.p1.4.m4.5.5.2.2.3" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">(</mo><mrow id="S3.I1.i2.p1.4.m4.4.4.1.1.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2a.cmml">ellip</mtext><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.1.cmml">​</mo><mrow id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">(</mo><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.4.m4.2.2" xref="S3.I1.i2.p1.4.m4.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.3" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.4.m4.3.3" xref="S3.I1.i2.p1.4.m4.3.3.cmml">c</mi><mo stretchy="false" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.4" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.I1.i2.p1.4.m4.5.5.2.2.4" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">,</mo><msub id="S3.I1.i2.p1.4.m4.5.5.2.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.cmml"><mi id="S3.I1.i2.p1.4.m4.5.5.2.2.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.2.cmml">σ</mi><mi id="S3.I1.i2.p1.4.m4.5.5.2.2.2.3" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.3.cmml">d</mi></msub><mo stretchy="false" id="S3.I1.i2.p1.4.m4.5.5.2.2.5" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.5b"><apply id="S3.I1.i2.p1.4.m4.5.5.cmml" xref="S3.I1.i2.p1.4.m4.5.5"><times id="S3.I1.i2.p1.4.m4.5.5.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.3"></times><ci id="S3.I1.i2.p1.4.m4.5.5.4.cmml" xref="S3.I1.i2.p1.4.m4.5.5.4">𝐷</ci><interval closure="open" id="S3.I1.i2.p1.4.m4.5.5.2.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2"><apply id="S3.I1.i2.p1.4.m4.4.4.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1"><times id="S3.I1.i2.p1.4.m4.4.4.1.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.1"></times><ci id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2a.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2">ellip</mtext></ci><vector id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2"><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">𝑎</ci><ci id="S3.I1.i2.p1.4.m4.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2">𝑏</ci><ci id="S3.I1.i2.p1.4.m4.3.3.cmml" xref="S3.I1.i2.p1.4.m4.3.3">𝑐</ci></vector></apply><apply id="S3.I1.i2.p1.4.m4.5.5.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.5.5.2.2.2.1.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.4.m4.5.5.2.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.2">𝜎</ci><ci id="S3.I1.i2.p1.4.m4.5.5.2.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.3">𝑑</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.5c">D(\textit{ellip}(a,b,c),\sigma_{d})</annotation></semantics></math>, where <math id="S3.I1.i2.p1.5.m5.1" class="ltx_Math" alttext="\sigma_{d}" display="inline"><semantics id="S3.I1.i2.p1.5.m5.1a"><msub id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml"><mi id="S3.I1.i2.p1.5.m5.1.1.2" xref="S3.I1.i2.p1.5.m5.1.1.2.cmml">σ</mi><mi id="S3.I1.i2.p1.5.m5.1.1.3" xref="S3.I1.i2.p1.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><apply id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.5.m5.1.1.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2">𝜎</ci><ci id="S3.I1.i2.p1.5.m5.1.1.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">\sigma_{d}</annotation></semantics></math> control the magnitude of displacements. We show examples of the generated tumor shapes in Appendix Figure <a href="#A2.F9" title="Figure 9 ‣ Appendix B Shape Examples ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Texture generation.</span> The generation of textures is a significant challenge due to the varied patterns found in tumors. Our current understanding of tumor textures is derived solely from clinical expertise, which considers factors such as the attenuation value and the distribution characteristics. To achieve the desired texture, we introduce Gaussian noise <math id="S3.I1.i3.p1.1.m1.2" class="ltx_Math" alttext="\mathcal{N}(\mu,\sigma_{g})" display="inline"><semantics id="S3.I1.i3.p1.1.m1.2a"><mrow id="S3.I1.i3.p1.1.m1.2.2" xref="S3.I1.i3.p1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.2.2.3" xref="S3.I1.i3.p1.1.m1.2.2.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.1.m1.2.2.2" xref="S3.I1.i3.p1.1.m1.2.2.2.cmml">​</mo><mrow id="S3.I1.i3.p1.1.m1.2.2.1.1" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.I1.i3.p1.1.m1.2.2.1.1.2" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">μ</mi><mo id="S3.I1.i3.p1.1.m1.2.2.1.1.3" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">,</mo><msub id="S3.I1.i3.p1.1.m1.2.2.1.1.1" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.2.2.1.1.1.2" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml">σ</mi><mi id="S3.I1.i3.p1.1.m1.2.2.1.1.1.3" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml">g</mi></msub><mo stretchy="false" id="S3.I1.i3.p1.1.m1.2.2.1.1.4" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><apply id="S3.I1.i3.p1.1.m1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2"><times id="S3.I1.i3.p1.1.m1.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2"></times><ci id="S3.I1.i3.p1.1.m1.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.3">𝒩</ci><interval closure="open" id="S3.I1.i3.p1.1.m1.2.2.1.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">𝜇</ci><apply id="S3.I1.i3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.2">𝜎</ci><ci id="S3.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.3">𝑔</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">\mathcal{N}(\mu,\sigma_{g})</annotation></semantics></math> with a predetermined mean attenuation value, matching the standard deviation of liver tumors. Subsequently, we use cubic interpolation to smooth the texture. Furthermore, to better replicate textures obtained from CT imaging, we use a final step of texture blurring. Examples of the generated texture can be found in Appendix Figure <a href="#A3.F10" title="Figure 10 ‣ Appendix C Texture Examples ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Post-processing.</span> The post-processing involves evaluating image characteristics through visual inspection and feedback from medical professionals. The purpose of these steps is to replicate the phenomena of mass effect and the appearance of a capsule <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib25" title="" class="ltx_ref">2004</a>)</cite>. Mass effect refers to the phenomenon wherein the tumor undergoes growth, resulting in the displacement and deformation of surrounding tissues. We utilize local scaling warping <cite class="ltx_cite ltx_citemacro_citep">(Glasbey and Mardia, <a href="#bib.bib13" title="" class="ltx_ref">1998</a>)</cite> to replicate this effect. Additionally, we brighten the edges of the tumor, thereby simulating the capsule appearance. Consequently, CT volumes with synthetic tumors can be used for the continual learning framework, where examples of the generated liver tumors can be found in Appendix Figure <a href="#A4.F11" title="Figure 11 ‣ Appendix D Synthetic tumor examples ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset &amp; Benchmark</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
<span id="S4.T1.2.1" class="ltx_text ltx_font_bold">Datasets description.</span> The LiTS dataset was used to train, validate, and evaluate AI models in segmenting liver tumors. The FLARE’23 dataset was used for external validation. An assembly of the CHAOS <cite class="ltx_cite ltx_citemacro_citep">(Valindria et al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, BTCV <cite class="ltx_cite ltx_citemacro_citep">(Landman et al., <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>, and Pancreas-CT <cite class="ltx_cite ltx_citemacro_citep">(Roth et al., <a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> datasets were used for generating synthetic training and validation sets, in which the liver in these datasets is confirmed to be healthy.
</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.3.1" class="ltx_tr">
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.1.1.1" class="ltx_p" style="width:121.4pt;">dataset</span>
</span>
</td>
<td id="S4.T1.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.2.1.1" class="ltx_p" style="width:52.0pt;">notation</span>
</span>
</td>
<td id="S4.T1.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.3.1.1" class="ltx_p" style="width:43.4pt;">split</span>
</span>
</td>
<td id="S4.T1.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.4.1.1" class="ltx_p" style="width:43.4pt;">annotation</span>
</span>
</td>
<td id="S4.T1.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.5.1.1" class="ltx_p" style="width:43.4pt;"># of CTs</span>
</span>
</td>
<td id="S4.T1.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.6.1.1" class="ltx_p" style="width:52.0pt;">tumor</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.2" class="ltx_tr">
<td id="S4.T1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S4.T1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.2.1.1.1.1" class="ltx_text">LiTS <cite class="ltx_cite ltx_citemacro_citep">(Bilic et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 1</span>
</span>
</td>
<td id="S4.T1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.3.1.1" class="ltx_p" style="width:43.4pt;">training</span>
</span>
</td>
<td id="S4.T1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.4.1.1" class="ltx_p" style="width:43.4pt;">✓</span>
</span>
</td>
<td id="S4.T1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.5.1.1" class="ltx_p" style="width:43.4pt;">25</span>
</span>
</td>
<td id="S4.T1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.6.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 2</span>
</span>
</td>
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.2.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.3.1.1" class="ltx_p" style="width:43.4pt;">✓</span>
</span>
</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.4.1.1" class="ltx_p" style="width:43.4pt;">5</span>
</span>
</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.4" class="ltx_tr">
<td id="S4.T1.3.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 3</span>
</span>
</td>
<td id="S4.T1.3.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.2.1.1" class="ltx_p" style="width:43.4pt;">testing</span>
</span>
</td>
<td id="S4.T1.3.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.3.1.1" class="ltx_p" style="width:43.4pt;">✓</span>
</span>
</td>
<td id="S4.T1.3.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.4.1.1" class="ltx_p" style="width:43.4pt;">70</span>
</span>
</td>
<td id="S4.T1.3.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.5" class="ltx_tr">
<td id="S4.T1.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S4.T1.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.5.1.1.1.1" class="ltx_text">Assembly <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 4</span>
</span>
</td>
<td id="S4.T1.3.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.3.1.1" class="ltx_p" style="width:43.4pt;">training</span>
</span>
</td>
<td id="S4.T1.3.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.4.1.1" class="ltx_p" style="width:43.4pt;">✗</span>
</span>
</td>
<td id="S4.T1.3.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.5.1.1" class="ltx_p" style="width:43.4pt;">25</span>
</span>
</td>
<td id="S4.T1.3.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.6.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.6" class="ltx_tr">
<td id="S4.T1.3.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 5</span>
</span>
</td>
<td id="S4.T1.3.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.2.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.3.1.1" class="ltx_p" style="width:43.4pt;">✗</span>
</span>
</td>
<td id="S4.T1.3.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.4.1.1" class="ltx_p" style="width:43.4pt;">50</span>
</span>
</td>
<td id="S4.T1.3.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.5.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.7" class="ltx_tr">
<td id="S4.T1.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" rowspan="2">
<span id="S4.T1.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.7.1.1.1.1" class="ltx_text">FLARE’23 <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 6</span>
</span>
</td>
<td id="S4.T1.3.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.3.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.4.1.1" class="ltx_p" style="width:43.4pt;">✗</span>
</span>
</td>
<td id="S4.T1.3.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.5.1.1" class="ltx_p" style="width:43.4pt;">50</span>
</span>
</td>
<td id="S4.T1.3.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.6.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.8" class="ltx_tr">
<td id="S4.T1.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 7</span>
</span>
</td>
<td id="S4.T1.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.2.1.1" class="ltx_p" style="width:43.4pt;">testing</span>
</span>
</td>
<td id="S4.T1.3.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.3.1.1" class="ltx_p" style="width:43.4pt;">✓</span>
</span>
</td>
<td id="S4.T1.3.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.4.1.1" class="ltx_p" style="width:43.4pt;">120</span>
</span>
</td>
<td id="S4.T1.3.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset &amp; Benchmark ‣ 4 Experiment ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes a total of five publicly available datasets used in this study. We group them into three classes.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.2" class="ltx_p"><span id="S4.I1.i1.p1.2.1" class="ltx_text ltx_font_bold">Real-tumor dataset.</span> We select the LiTS dataset <cite class="ltx_cite ltx_citemacro_citep">(Bilic et al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> for training and testing AI models. LiTS provides detailed per-voxel annotations of liver tumors. The tumor types include HCC and secondary liver tumors and metastasis derived from colorectal, breast, and lung cancer. The size of liver tumors ranges from 38mm<sup id="S4.I1.i1.p1.2.2" class="ltx_sup">3</sup> to 349 cm<sup id="S4.I1.i1.p1.2.3" class="ltx_sup">3</sup>, and the radius of tumors is approximately in the range of [2, 44] mm. LiTS is partitioned into a training set (<span id="S4.I1.i1.p1.2.4" class="ltx_text ltx_font_italic">cohort 1</span>; 25 CT volumes), validation set (<span id="S4.I1.i1.p1.2.5" class="ltx_text ltx_font_italic">cohort 2</span>; 5 CT volumes), and test set (<span id="S4.I1.i1.p1.2.6" class="ltx_text ltx_font_italic">cohort 3</span>; 70 CT volumes).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Healthy CT assembly.</span> We have collected a dataset of 75 CT volumes with healthy liver assembled from CHAOS <cite class="ltx_cite ltx_citemacro_citep">(Valindria et al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, Pancreas-CT <cite class="ltx_cite ltx_citemacro_citep">(Roth et al., <a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> and BTCV <cite class="ltx_cite ltx_citemacro_citep">(Landman et al., <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>. This assembled dataset is partitioned into a training set (<span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">cohort 4</span>; 25 CT volumes) and a validation set (<span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">cohort 5</span>; 50 CT volumes). As illustrated in Figure <a href="#S2.F1" title="Figure 1 ‣ 2 Related Work ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b) For the training set, tumors were dynamically generated within these volumes during training, resulting in a sequential collection of image-label pairs comprising synthetic tumors. For the validation set, we generated three different tumor sizes (small, medium, and large) for each healthy CT volume offline, giving a total of 150 CT volumes.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">External benchmark.</span> FLARE’23 <cite class="ltx_cite ltx_citemacro_citep">(Ma et al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> is used for an external benchmark because it provides out-domain CT volumes from the LiTS dataset. This dataset was specifically chosen due to its extensive coverage, encompassing over 4000 3D CT volumes obtained from more than 30 medical centers. The inclusion of such a diverse dataset ensures the generalizability of the benchmark. The FLARE’23 dataset contains partially labeled annotations. To ensure the suitability of the test set, specific criteria are applied to the annotations. These criteria require that the annotations include per-voxel labeling for both the liver and tumors, with the additional constraint that the connected component of the tumor must intersect with the liver. Adhering to these conditions, we chose the external test set (<span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">cohort 7</span>; 120 CT volumes). Additionally, same as the assembly dataset, we can use the healthy cases within the FLARE’23 to generate synthetic data to serve as <span id="S4.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">in-domain</span> validation set (<span id="S4.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">cohort 6</span>; 50 CT volumes), which will be used in §<a href="#S5.SS5" title="5.5 Continual Learning Framework is Enhanced by In-domain Synthetic-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We have implemented our codes utilizing the MONAI<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://monai.io/" title="" class="ltx_ref ltx_href">https://monai.io/</a></span></span></span> framework for the U-Net architecture <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger et al., <a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite>, a well-established network commonly employed in medical image segmentation tasks. During the pre-processing stage, input images undergo clipping with a window range of [-21,189]. Following this, they are normalized to achieve a zero mean and unit standard deviation <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>. For training purposes, random patches with dimensions of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="96\times 96\times 96" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">96</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">96</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">96\times 96\times 96</annotation></semantics></math> are cropped from the 3D image volumes. A base learning rate of 0.0002 is utilized in the training process, accompanied by a batch size of two per GPU. To further enhance the training process, we employ both the linear warmup strategy and the cosine annealing learning rate schedule. Our model is trained for 6,000 epochs, with a model checkpoint being saved every 100 epochs, and a total of 60 model checkpoints are saved throughout the entire training process. During the inference phase, a sliding window strategy with an overlapping area ratio of 0.75 is adopted. To ensure robustness and comprehensiveness in obtaining results, the experiment is conducted ten times each to perform statistical analysis. By averaging all runs, we obtain reliable results. The segmentation performance is evaluated using the Dice Similarity Coefficient (DSC) score, while Sensitivity is used to evaluate the performance of detecting very tiny liver tumors (radius <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><lt id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">&lt;</annotation></semantics></math> 5mm).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Result</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Summary.</span> Using synthetic data as validation can select the best model checkpoint and alleviate the overfitting problem. Furthermore, the AI model developed using our continual learning framework outperforms models trained and validated on a static dataset. The performance is particularly high for detecting small/tiny tumors because we can generate a vast number of examples of small/tiny tumors for both training and validation.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2310.16052/assets/fig_real_validation.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S5.F2.9.1" class="ltx_text ltx_font_bold">The overfitting is due to a small-scale, biased real-tumor validation set.</span>
Model checkpoints are saved at each training epoch, and their performance trajectories are evaluated. The <span id="S5.F2.10.2" class="ltx_text" style="color:#3CBA54;">green</span> curve plots the test set performance of each checkpoint. It serves as the gold standard for a specific dataset, though the test set performance is often apriori unknown. The <span id="S5.F2.11.3" class="ltx_text" style="color:#999999;">gray</span> curve plots the validation set performance of each checkpoint. This performance is accessible during the AI training and instrumental in selecting the best model checkpoint.
We train a model on the LiTS training set and subsequently test it on the LiTS test set as in-domain evaluation <span id="S5.F2.12.4" class="ltx_text ltx_font_bold">(a)</span> and the FLARE’23 dataset as out-domain evaluation <span id="S5.F2.13.5" class="ltx_text ltx_font_bold">(b)</span>. At the initial epochs, the model performs increasingly well, but its test set performance declines when trained for more epochs, highlighted by the green arrow. This decline is attributed to <span id="S5.F2.14.6" class="ltx_text ltx_font_italic">overfitting</span>, where the model becomes too specialized on the training set and loses its ability to generalize effectively to the test set. The purpose of a validation set is to select the best model checkpoint that is expected to perform well on unseen data. However, in practice, the size and diversity of the validation set may be limited, leading to potential inaccuracies in checkpoint selection. This is evidenced by both in-domain (a) and out-domain (b) evaluations. The dots on the curve represent the best checkpoint selected by the test set (<span id="S5.F2.15.7" class="ltx_text" style="color:#3CBA54;">green</span>) or the real-tumor validation (<span id="S5.F2.16.8" class="ltx_text" style="color:#999999;">gray</span>). A comparative analysis reveals that the checkpoints selected based on the real-tumor validation set might not be the most suitable for test sets.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overfitting is Attributed to Small-Scale, Real-Tumor Validation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To demonstrate the potential limitations of selecting the best model checkpoint based on a small-scale and biased real-tumor validation set, we evaluate all the model checkpoints in real-tumor validation set (cohort 2), in-domain LiTS test set (cohort 3) and out-domain FLARE’23 test set (cohort 7). In-domain test set (cohort 3) assesses the performance of each checkpoint and aids in determining the effectiveness of the selected best checkpoints using the validation set (cohort 2). Out-domain test set (cohort 7) serves as a robust benchmark, providing an enhanced evaluation of the performance of the model checkpoints on out-domain unseen data.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">As shown in Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, two significant observations can be made. Firstly, the best checkpoint identified by the small-scale real-tumor validation set exhibits considerable instability, with notable variations observed when different validation samples are chosen. This result indicates that the small-scale real-tumor validation is inherently biased and lacks the ability to adequately represent the broader range of cases. Secondly, the performance of the best checkpoint determined by the real validation set does not effectively generalize to unseen test data, particularly when confronted with out-domain data. These observations indicate that overfitting can be attributed to a small-scale, biased real-tumor validation set.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2310.16052/assets/fig_synt_validation.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S5.F3.6.1" class="ltx_text ltx_font_bold">The overfitting is alleviated by a large-scale, synthetic-tumor validation set.</span> Similar to Figure <a href="#S5.F2" title="Figure 2 ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="S5.F3.7.2" class="ltx_text ltx_font_bold">(a)</span> and <span id="S5.F3.8.3" class="ltx_text ltx_font_bold">(b)</span> denote in-domain and out-domain evaluations, and the <span id="S5.F3.9.4" class="ltx_text" style="color:#3CBA54;">green</span> curves are the test set performance on these two datasets—serving as the gold standard for checkpoint selection. The <span id="S5.F3.10.5" class="ltx_text" style="color:#E3242B;">red</span> curves are the validation performance using synthetic tumors (cohort 5). In theory, we can generate an unlimited number of tumors in varied conditions, e.g., size, location, shape, and texture, as needed, using the tumor generator described in §<a href="#S3.SS2" title="3.2 Modeling-Based Synthetic Tumor Generation ‣ 3 Method &amp; Material ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Such extensive coverage enhances the ability of the validation set to estimate how well the model can be generalized to previously unseen data distributions. As shown in both in-domain and out-domain evaluation, synthetic data as validation can accurately select the best model checkpoint that is almost identical to that selected by test sets.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The overfitting can be alleviated by a diverse, large-scale synthetic-tumor validation set. We conducted a similar experiment to § <a href="#S5.SS1" title="5.1 Overfitting is Attributed to Small-Scale, Real-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. This experiment involved evaluating all the model checkpoints in synthetic-tumor validation set (cohort 5), in-domain test set (cohort 3) and out-domain test set (cohort 7).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The evaluation trajectory can be observed in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Overfitting is Attributed to Small-Scale, Real-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with the synthetic-tumor validation set represented by the red line. It is clear that the best checkpoint selected using the synthetic-tumor validation set performs much better than the best checkpoint chosen using the real-tumor validation set when tested with unseen data. Notably, this improvement is especially remarkable when dealing with out-domain data (cohort 7), as the selected model checkpoint is identical to the one chosen by the out-domain test set. These findings emphasize the effectiveness of synthetic-tumor validation set, which serves as a superior alternative to mitigate overfitting issues.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.8.1" class="ltx_text ltx_font_bold">The overfitting can be addressed by continual learning on synthetic data.</span> We set up the continual learning framework for liver tumor segmentation, described in §<a href="#S3.SS1" title="3.1 Continual Learning for Tumor Segmentation ‣ 3 Method &amp; Material ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Its learning curves are presented in <span id="S5.F4.9.2" class="ltx_text" style="color:#E3242B;">red</span>, referred to as <span id="S5.F4.10.3" class="ltx_text ltx_font_italic">dynamic training</span>. In comparison, <span id="S5.F4.11.4" class="ltx_text" style="color:#999999;">gray</span> curves are the conventional framework training with a limited number of real data, referred to as <span id="S5.F4.12.5" class="ltx_text ltx_font_italic">static training</span>. Both in-domain <span id="S5.F4.13.6" class="ltx_text ltx_font_bold">(a)</span> and out-domain <span id="S5.F4.14.7" class="ltx_text ltx_font_bold">(b)</span> evaluations show that the AI model continuously trained on synthetic data outperforms the one trained on real data.</figcaption><img src="/html/2310.16052/assets/fig_real_synt_training.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.4" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.4.5" class="ltx_tr">
<td id="S5.T2.4.5.1" class="ltx_td ltx_align_top ltx_border_r ltx_border_tt"></td>
<td id="S5.T2.4.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt" colspan="2">evaluated on real data</td>
<td id="S5.T2.4.5.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2">evaluated on synthetic data</td>
</tr>
<tr id="S5.T2.4.4" class="ltx_tr">
<td id="S5.T2.4.4.5" class="ltx_td ltx_align_top ltx_border_r"></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.1.1.1.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">@</annotation></semantics></math> real</span>
</span>
</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.2.2.2.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.2.2.2.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.2.2.2.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.2.2.2.1.1.m1.1.1" xref="S5.T2.2.2.2.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.1.m1.1b"><ci id="S5.T2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.1.m1.1c">@</annotation></semantics></math> synt</span>
</span>
</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.3.3.3.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.3.3.3.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.3.3.3.1.1.m1.1.1" xref="S5.T2.3.3.3.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.1.m1.1b"><ci id="S5.T2.3.3.3.1.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.1.m1.1c">@</annotation></semantics></math> real</span>
</span>
</td>
<td id="S5.T2.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.4.4.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.4.4.4.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.4.4.4.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.4.4.4.1.1.m1.1.1" xref="S5.T2.4.4.4.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.1.m1.1b"><ci id="S5.T2.4.4.4.1.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.1.m1.1c">@</annotation></semantics></math> synt</span>
</span>
</td>
</tr>
<tr id="S5.T2.4.6" class="ltx_tr">
<td id="S5.T2.4.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.4.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 3</span>
</span>
</td>
<td id="S5.T2.4.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.2.1.1" class="ltx_p" style="width:78.0pt;">26.7 (22.6-30.9)</span>
</span>
</td>
<td id="S5.T2.4.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.4.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.3.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.6.3.1.1.1" class="ltx_text ltx_font_bold">33.4 (28.7-38.0)</span></span>
</span>
</td>
<td id="S5.T2.4.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.4.1.1" class="ltx_p" style="width:78.0pt;">27.0 (23.7-30.3)</span>
</span>
</td>
<td id="S5.T2.4.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.5.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.6.5.1.1.1" class="ltx_text ltx_font_bold">34.5 (30.8-38.2)</span></span>
</span>
</td>
</tr>
<tr id="S5.T2.4.7" class="ltx_tr">
<td id="S5.T2.4.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S5.T2.4.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 7</span>
</span>
</td>
<td id="S5.T2.4.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.2.1.1" class="ltx_p" style="width:78.0pt;">31.1 (26.0-36.2)</span>
</span>
</td>
<td id="S5.T2.4.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S5.T2.4.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.3.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.7.3.1.1.1" class="ltx_text ltx_font_bold">33.3 (30.6-36.0)</span></span>
</span>
</td>
<td id="S5.T2.4.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.4.1.1" class="ltx_p" style="width:78.0pt;">32.0 (28.5-35.5)</span>
</span>
</td>
<td id="S5.T2.4.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.5.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.7.5.1.1.1" class="ltx_text ltx_font_bold">35.4 (32.1-38.7)</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
<span id="S5.T2.11.1" class="ltx_text ltx_font_bold">
Synthetic data for both training and validation.</span> As shown in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, AI model can be trained on either static real data or dynamic synthetic data. The terms “train <math id="S5.T2.7.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T2.7.m1.1b"><mi mathvariant="normal" id="S5.T2.7.m1.1.1" xref="S5.T2.7.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.7.m1.1c"><ci id="S5.T2.7.m1.1.1.cmml" xref="S5.T2.7.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.m1.1d">@</annotation></semantics></math> real” and “trained <math id="S5.T2.8.m2.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T2.8.m2.1b"><mi mathvariant="normal" id="S5.T2.8.m2.1.1" xref="S5.T2.8.m2.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.8.m2.1c"><ci id="S5.T2.8.m2.1.1.cmml" xref="S5.T2.8.m2.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.m2.1d">@</annotation></semantics></math> synt” denote static training with real data and dynamic training with synthetic data, respectively. We save the model checkpoints at each training epoch and then use a validation set to select the <span id="S5.T2.12.2" class="ltx_text ltx_font_italic">best</span> model. These selected model checkpoints are tested on the in-domain LiTS test set (cohort 3) and out-domain FLARE’23 test set (cohort 7). We report the DSC score (%) and 95% confidence interval achieved on the test set. The result reveals that training and validating AI models with our continual learning framework can significantly improve liver tumor segmentation.
</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Overfitting Can Be Addressed by Continual Learning on Synthetic Data</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We have shown the effectiveness of large-scale synthetic-tumor validation set.
Now, we will shift our attention to synthetic data in handling the overfitting problem from a training perspective. For this purpose, we introduce a continual learning framework on synthetic data, detailed in § <a href="#S3.SS1" title="3.1 Continual Learning for Tumor Segmentation ‣ 3 Method &amp; Material ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The training trajectory of static training on real data and dynamic training on synthetic data is shown in Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and the liver tumor segmentation results are presented in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Specifically, the AI model trained on static real data demonstrates a DSC score of 26.7% for the in-domain test set (cohort 3) and 31.1% for the out-domain test set (cohort 7). In comparison, the AI model developed using our continual learning framework with synthetic data achieves notably higher DSC scores, reaching 34.5% on cohort 3 and 35.4% on cohort 7, respectively. These results indicate a notable improvement in the synthetic data. Based on these findings, we can confidently assert that incorporating our continual learning framework with synthetic data allows us to effectively address the issue of overfitting, encompassing both the training and validation perspectives.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2310.16052/assets/fig_small_tumor_detection.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S5.F5.16.1" class="ltx_text ltx_font_bold">Synthetic data can benefit early cancer detection.</span> We evaluate the efficacy of synthetic data in detecting tiny liver tumors (radius <math id="S5.F5.7.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.F5.7.m1.1b"><mo id="S5.F5.7.m1.1.1" xref="S5.F5.7.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.F5.7.m1.1c"><lt id="S5.F5.7.m1.1.1.cmml" xref="S5.F5.7.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.7.m1.1d">&lt;</annotation></semantics></math> 5mm). Specifically, the AI model developed with our continual learning framework – trained and evaluated with synthetic data – is evaluated on tumor detection. We compare it with AI model developed on static real data. The “trained<math id="S5.F5.8.m2.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.8.m2.1b"><mi mathvariant="normal" id="S5.F5.8.m2.1.1" xref="S5.F5.8.m2.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.8.m2.1c"><ci id="S5.F5.8.m2.1.1.cmml" xref="S5.F5.8.m2.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.8.m2.1d">@</annotation></semantics></math>real“ and “trained<math id="S5.F5.9.m3.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.9.m3.1b"><mi mathvariant="normal" id="S5.F5.9.m3.1.1" xref="S5.F5.9.m3.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.9.m3.1c"><ci id="S5.F5.9.m3.1.1.cmml" xref="S5.F5.9.m3.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.9.m3.1d">@</annotation></semantics></math>synt“ hold the same meaning as described in Table <a href="#S5.T2" title="Table 2 ‣ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The “best<math id="S5.F5.10.m4.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.10.m4.1b"><mi mathvariant="normal" id="S5.F5.10.m4.1.1" xref="S5.F5.10.m4.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.10.m4.1c"><ci id="S5.F5.10.m4.1.1.cmml" xref="S5.F5.10.m4.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.10.m4.1d">@</annotation></semantics></math>real“ and “best<math id="S5.F5.11.m5.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.11.m5.1b"><mi mathvariant="normal" id="S5.F5.11.m5.1.1" xref="S5.F5.11.m5.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.11.m5.1c"><ci id="S5.F5.11.m5.1.1.cmml" xref="S5.F5.11.m5.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.11.m5.1d">@</annotation></semantics></math>synt“ denote the selection of best checkpoints based on real- and synthetic- tumor validation sets, respectively. We report the sensitivities (%) achieved on the test set. As shown in both in-domain <span id="S5.F5.17.2" class="ltx_text ltx_font_bold">(a)</span> and out-domain <span id="S5.F5.18.3" class="ltx_text ltx_font_bold">(b)</span> evaluations, our continual learning framework with synthetic data proves to be effective in detecting tiny liver tumors (radius <math id="S5.F5.12.m6.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.F5.12.m6.1b"><mo id="S5.F5.12.m6.1.1" xref="S5.F5.12.m6.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.F5.12.m6.1c"><lt id="S5.F5.12.m6.1.1.cmml" xref="S5.F5.12.m6.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.12.m6.1d">&lt;</annotation></semantics></math> 5mm), thereby benefiting early cancer detection.</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Synthetic Data Can Benefit Early Cancer Detection</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Early tumor (radius <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mo id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><lt id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">&lt;</annotation></semantics></math> 5mm) detection plays a critical role in clinical applications, providing valuable information for early cancer diagnosis. Acquiring real data of such a small size is challenging, often posing difficulties or even making it impossible to acquire them. However, our strategy can dynamically generate numerous tiny tumors as required. As a result, the AI model developed within the continual learning framework yields a significant improvement in detecting tiny liver tumors.
The improvement can be found in Figure <a href="#S5.F5" title="Figure 5 ‣ 5.3 Overfitting Can Be Addressed by Continual Learning on Synthetic Data ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We assessed the sensitivity of the AI model under different settings. The performance of the AI model trained and validated on the static real data is 33.1% for the in-domain test set (cohort 3) and 33.9% for the out-domain test set (cohort 7). Comparatively, the AI model developed using our continual learning framework on synthetic data gives a sensitivity of 55.4% for cohort 3 and 52.3% for cohort 7. These results prove the effectiveness of our framework in early detection of cancer.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2310.16052/assets/fig_indomain_synt_validation.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S5.F6.5.1" class="ltx_text ltx_font_bold">The continual learning framework is enhanced by in-domain synthetic-tumor validation.</span> Our framework enables the utilization of healthy cases from various domains. Specifically, an interesting situation arises where we can directly create synthetic-tumor validation using healthy cases from the same domain as the test set. This in-domain synthetic-tumor validation set can benefit our continual learning framework. To illustrate this advantage, we utilize the FLARE’23 dataset, which contains both disease cases and healthy cases. We trained the model using dynamic synthetic data and saved model checkpoints at each training epoch. Subsequently, we evaluated them on three datasets, i.e., out-domain synthetic-tumor validation set from assembly dataset (cohort 5, <span id="S5.F6.6.2" class="ltx_text" style="color:#808080;">gray</span> curve), in-domain synthetic-tumor validation set from FLARE’23 dataset (cohort 6, <span id="S5.F6.7.3" class="ltx_text" style="color:#E3242B;">red</span> curve), and FLARE’23 test set served as the gold standard (cohort 7, <span id="S5.F6.8.4" class="ltx_text" style="color:#3CBA54;">green</span> curve). As shown, the in-domain synthetic-tumor validation set accurately identifies the best model, which aligns with the model selected by the FLARE’23 test set.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Continual Learning Framework is Enhanced by In-domain Synthetic-Tumor Validation</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">We have demonstrated the effectiveness of our continual learning framework. Moving forward, let’s consider the framework itself. Synthetic data offers a significant advantage as we can utilize healthy CT volumes from various domains. A notable scenario arises wherein we can directly generate synthetic-tumor validation using healthy cases from the same domain as the test set, providing valuable insights for our continual learning framework. As shown in Figure <a href="#S5.F6" title="Figure 6 ‣ 5.4 Synthetic Data Can Benefit Early Cancer Detection ‣ 5 Result ‣ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the in-domain synthetic-tumor validation set showcases its capability to accurately identify the best model, which aligns with the model selected by the test set. This result highlights that the continual learning framework yields more favorable outcomes when we can generate in-domain synthetic-tumor validation.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Data synthesis strategies continue to pique the interest of researchers and practitioners, propelling ongoing investigations within this field. This paper justifies the potential and stresses the necessity of leveraging synthetic data as validation to select the best model checkpoint along the training trajectory. Moreover, by employing a continual learning framework on synthetic data, we realize a marked improvement in liver tumor segmentation as well as in the early detection of cancerous tumors compared with the static training on real data, where procuring ample annotated examples can be cost-prohibitive. It is particularly valuable in scenarios characterized by limited annotated data. In the future, we plan to improve the generation of synthetic tumors and verify our findings across different organs, such as the pancreas, kidneys, and stomach.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research and the Patrick J. McGovern Foundation Award. We appreciate the effort of the MONAI Team to provide open-source code for the community. This work has partially utilized the GPUs provided by ASU Research Computing and NVIDIA. We thank Yu-Cheng Chou, Junfei Xiao, Xiaoxi Chen, and Bowen Li for their constructive suggestions at several stages of the project.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi et al. (2023)</span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J Fleet. 2023.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08466</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilic et al. (2019)</span>
<span class="ltx_bibblock">
Patrick Bilic, Patrick Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, Qi Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, Jürgen Hesser, et al. 2019.

</span>
<span class="ltx_bibblock">The liver tumor segmentation benchmark (lits).

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.04056</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black et al. (2023)</span>
<span class="ltx_bibblock">
Michael J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. 2023.

</span>
<span class="ltx_bibblock">Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 8726–8737.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burg et al. (2023)</span>
<span class="ltx_bibblock">
Max F Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. 2023.

</span>
<span class="ltx_bibblock">A data augmentation perspective on diffusion models and retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10253</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2021)</span>
<span class="ltx_bibblock">
Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew FK Williamson, and Faisal Mahmood. 2021.

</span>
<span class="ltx_bibblock">Synthetic data in machine learning for medicine and healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Nature Biomedical Engineering</em>, 5(6):493–497.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2019)</span>
<span class="ltx_bibblock">
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool. 2019.

</span>
<span class="ltx_bibblock">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 1841–1850.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chernyak et al. (2018)</span>
<span class="ltx_bibblock">
Victoria Chernyak, Kathryn J Fowler, Aya Kamaya, Ania Z Kielar, Khaled M Elsayes, Mustafa R Bashir, Yuko Kono, Richard K Do, Donald G Mitchell, Amit G Singal, An Tang, and Claude B Sirlin. 2018.

</span>
<span class="ltx_bibblock">Liver imaging reporting and data system (LI-RADS) version 2018: Imaging of hepatocellular carcinoma in at-risk patients.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Radiology</em>, 289(3):816–830.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collobert and Weston (2008)</span>
<span class="ltx_bibblock">
Ronan Collobert and Jason Weston. 2008.

</span>
<span class="ltx_bibblock">A unified architecture for natural language processing: Deep neural networks with multitask learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th international conference on Machine learning</em>, pages 160–167.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crosby et al. (2022)</span>
<span class="ltx_bibblock">
David Crosby, Sangeeta Bhatia, Kevin M Brindle, Lisa M Coussens, Caroline Dive, Mark Emberton, Sadik Esener, Rebecca C Fitzgerald, Sanjiv S Gambhir, Peter Kuhn, et al. 2022.

</span>
<span class="ltx_bibblock">Early detection of cancer.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Science</em>, 375(6586):eaay9040.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Cong Gao, Benjamin D Killeen, Yicheng Hu, Robert B Grupp, Russell H Taylor, Mehran Armand, and Mathias Unberath. 2023.

</span>
<span class="ltx_bibblock">Synthetic data accelerates the development of generalizable learning-based algorithms for x-ray image analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 5(3):294–308.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gareth et al. (2013)</span>
<span class="ltx_bibblock">
James Gareth, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">An introduction to statistical learning: with applications in R</em>.

</span>
<span class="ltx_bibblock">Spinger.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glasbey and Mardia (1998)</span>
<span class="ltx_bibblock">
Chris A Glasbey and Kantilal Vardichand Mardia. 1998.

</span>
<span class="ltx_bibblock">A review of image-warping methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of applied statistics</em>, 25(2):155–171.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonzalez (2009)</span>
<span class="ltx_bibblock">
Rafael C Gonzalez. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Digital image processing</em>.

</span>
<span class="ltx_bibblock">Pearson education india.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath et al. (2022)</span>
<span class="ltx_bibblock">
Izabela Horvath, Johannes Paetzold, Oliver Schoppe, Rami Al-Maskari, Ivan Ezhov, Suprosanna Shit, Hongwei Li, Ali Ertürk, and Bjoern Menze. 2022.

</span>
<span class="ltx_bibblock">Metgan: Generative tumour inpainting and modality synthesis in light sheet microscopy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pages 227–237.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, Alan L Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Label-free liver tumor segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 7422–7432.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Qixin Hu, Junfei Xiao, Yixiong Chen, Shuwen Sun, Jie-Neng Chen, Alan Yuille, and Zongwei Zhou. 2022.

</span>
<span class="ltx_bibblock">Synthetic tumors make ai segment tumors better.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">NeurIPS Workshop on Medical Imaging meets NeurIPS</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2018)</span>
<span class="ltx_bibblock">
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018.

</span>
<span class="ltx_bibblock">Junction tree variational autoencoder for molecular graph generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2323–2332. PMLR.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 2901–2910.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon et al. (2022)</span>
<span class="ltx_bibblock">
James Jordon, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli, Giovanni Cherubin, Carsten Maple, Samuel N Cohen, and Adrian Weller. 2022.

</span>
<span class="ltx_bibblock">Synthetic data–what, why and how?

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.03257</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon et al. (2018)</span>
<span class="ltx_bibblock">
James Jordon, Jinsung Yoon, and Mihaela Van Der Schaar. 2018.

</span>
<span class="ltx_bibblock">Pate-gan: Generating synthetic data with differential privacy guarantees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International conference on learning representations</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al. (2023)</span>
<span class="ltx_bibblock">
Mintong Kang, Bowen Li, Zengle Zhu, Yongyi Lu, Elliot K Fishman, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Label-assemble: Leveraging multiple datasets with partial labels.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)</em>, pages 1–5. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn et al. (2013)</span>
<span class="ltx_bibblock">
Max Kuhn, Kjell Johnson, et al. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Applied predictive modeling</em>, volume 26.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landman et al. (2015)</span>
<span class="ltx_bibblock">
B Landman, Z Xu, J Igelsias, M Styner, T Langerak, and A Klein. 2015.

</span>
<span class="ltx_bibblock">2015 miccai multi-atlas labeling beyond the cranial vault workshop and challenge.

</span>
<span class="ltx_bibblock"><a href="doi:10.7303/syn3193805" title="" class="ltx_ref ltx_url ltx_font_typewriter">doi:10.7303/syn3193805</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2004)</span>
<span class="ltx_bibblock">
KHY Lee, ME O’Malley, MA Haider, and A Hanbidge. 2004.

</span>
<span class="ltx_bibblock">Triple-phase mdct of hepatocellular carcinoma.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">American Journal of Roentgenology</em>, 182(3):643–649.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Early detection and localization of pancreatic cancer by label-free tumor synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">MICCAI Workshop on Big Task Small Data, 1001-AI</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Cong Liu, Casey N Ta, Jim M Havrilla, Jordan G Nestor, Matthew E Spotnitz, Andrew S Geneslaw, Yu Hu, Wendy K Chung, Kai Wang, and Chunhua Weng. 2022.

</span>
<span class="ltx_bibblock">Oard: Open annotations for rare diseases and their phenotypes based on real-world data.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">The American Journal of Human Genetics</em>, 109(9):1591–1604.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. 2023a.

</span>
<span class="ltx_bibblock">Clip-driven universal model for organ segmentation and tumor detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 21152–21164.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Qihao Liu, Adam Kortylewski, and Alan L Yuille. 2023b.

</span>
<span class="ltx_bibblock">Poseexaminer: Automated testing of out-of-distribution robustness in human pose and shape estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 672–681.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luzi et al. (2022)</span>
<span class="ltx_bibblock">
Lorenzo Luzi, Ali Siahkoohi, Paul M Mayer, Josue Casco-Rodriguez, and Richard Baraniuk. 2022.

</span>
<span class="ltx_bibblock">Boomerang: Local sampling on image manifolds using diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.12100</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu et al. (2022)</span>
<span class="ltx_bibblock">
Fei Lyu, Mang Ye, Jonathan Frederik Carlsen, Kenny Erleben, Sune Darkner, and Pong C Yuen. 2022.

</span>
<span class="ltx_bibblock">Pseudo-label guided image synthesis for semi-supervised covid-19 pneumonia infection segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2022)</span>
<span class="ltx_bibblock">
Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, Yu Wang, Yinan Xu, et al. 2022.

</span>
<span class="ltx_bibblock">Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 82:102616.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al. (2020)</span>
<span class="ltx_bibblock">
Jiteng Mu, Weichao Qiu, Gregory D Hager, and Alan L Yuille. 2020.

</span>
<span class="ltx_bibblock">Learning from synthetic animals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 12386–12395.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord et al. (2016)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016.

</span>
<span class="ltx_bibblock">Wavenet: A generative model for raw audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.03499</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purushwalkam et al. (2022)</span>
<span class="ltx_bibblock">
Senthil Purushwalkam, Pedro Morgado, and Abhinav Gupta. 2022.

</span>
<span class="ltx_bibblock">The challenges of continuous self-supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 702–721. Springer.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al. (2023)</span>
<span class="ltx_bibblock">
Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Abdomenatlas-8k: Annotating 8,000 abdominal ct volumes for multi-organ segmentation in three weeks.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al. (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.12092</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ripley (2007)</span>
<span class="ltx_bibblock">
Brian D Ripley. 2007.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Pattern recognition and neural networks</em>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger et al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, pages 234–241. Springer.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth et al. (2016)</span>
<span class="ltx_bibblock">
Holger Roth, Amal Farag, Evrim B. Turkbey, Le Lu, Jiamin Liu, and Ronald M. Summers. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.7937/K9/TCIA.2016.TNB1KQBU" title="" class="ltx_ref ltx_href">Data from pancreas-ct</a>.

</span>
<span class="ltx_bibblock">The Cancer Imaging Archive. <a target="_blank" href="https://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell (2010)</span>
<span class="ltx_bibblock">
Stuart J Russell. 2010.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence a modern approach</em>.

</span>
<span class="ltx_bibblock">Pearson Education, Inc.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al. (2018)</span>
<span class="ltx_bibblock">
Younghak Shin, Hemin Ali Qadir, and Ilangko Balasingham. 2018.

</span>
<span class="ltx_bibblock">Abnormal colon polyp image synthesis using conditional adversarial networks for improved detection performance.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 6:56007–56017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2022)</span>
<span class="ltx_bibblock">
Yucheng Tang, Dong Yang, Wenqi Li, Holger R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. 2022.

</span>
<span class="ltx_bibblock">Self-supervised pre-training of swin transformers for 3d medical image analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 20730–20740.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valindria et al. (2018)</span>
<span class="ltx_bibblock">
Vanya V Valindria, Nick Pawlowski, Martin Rajchl, Ioannis Lavdas, Eric O Aboagye, Andrea G Rockall, Daniel Rueckert, and Ben Glocker. 2018.

</span>
<span class="ltx_bibblock">Multi-modal learning from unpaired images: Application to multi-organ segmentation in ct and mri.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2018 IEEE winter conference on applications of computer vision (WACV)</em>, pages 547–556. IEEE.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van de Ven and Tolias (2019)</span>
<span class="ltx_bibblock">
Gido M Van de Ven and Andreas S Tolias. 2019.

</span>
<span class="ltx_bibblock">Three scenarios for continual learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.07734</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van de Ven et al. (2022)</span>
<span class="ltx_bibblock">
Gido M Van de Ven, Tinne Tuytelaars, and Andreas S Tolias. 2022.

</span>
<span class="ltx_bibblock">Three types of incremental learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 4(12):1185–1197.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Hualin Wang, Yuhong Zhou, Jiong Zhang, Jianqin Lei, Dongke Sun, Feng Xu, and Xiayu Xu. 2022.

</span>
<span class="ltx_bibblock">Anomaly segmentation in retinal images with poisson-blending data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, page 102534.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiese et al. (2020)</span>
<span class="ltx_bibblock">
Magnus Wiese, Robert Knobloch, Ralf Korn, and Peter Kretschmer. 2020.

</span>
<span class="ltx_bibblock">Quant gans: deep generation of financial time series.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Quantitative Finance</em>, 20(9):1419–1440.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wyatt et al. (2022)</span>
<span class="ltx_bibblock">
Julian Wyatt, Adam Leach, Sebastian M Schmon, and Chris G Willcocks. 2022.

</span>
<span class="ltx_bibblock">Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 650–656.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al. (2023)</span>
<span class="ltx_bibblock">
Xiaodan Xing, Giorgos Papanastasiou, Simon Walsh, and Guang Yang. 2023.

</span>
<span class="ltx_bibblock">Less is more: Unsupervised mask-guided annotated ct image synthesis with minimum manual segmentations.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2021)</span>
<span class="ltx_bibblock">
Qingsong Yao, Li Xiao, Peihang Liu, and S Kevin Zhou. 2021.

</span>
<span class="ltx_bibblock">Label-free segmentation of covid-19 lesions in lung ct.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon et al. (2019)</span>
<span class="ltx_bibblock">
Jinsung Yoon, Daniel Jarrett, and Mihaela Van der Schaar. 2019.

</span>
<span class="ltx_bibblock">Time-series generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Menghan Yu, Sourabh Kulhare, Courosh Mehanian, Charles B Delahunt, Daniel E Shea, Zohreh Laverriere, Ishan Shah, and Matthew P Horning. 2023.

</span>
<span class="ltx_bibblock">How good are synthetic medical images? an empirical study with lung ultrasound.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">International Workshop on Simulation and Synthesis in Medical Imaging</em>, pages 75–85. Springer.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Chenyu Zheng, Guoqiang Wu, and Chongxuan Li. 2023.

</span>
<span class="ltx_bibblock">Toward understanding generative data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.17476</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou (2021)</span>
<span class="ltx_bibblock">
Zongwei Zhou. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Towards Annotation-Efficient Deep Learning for Computer-Aided Diagnosis</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Arizona State University.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2022)</span>
<span class="ltx_bibblock">
Zongwei Zhou, Michael B Gotway, and Jianming Liang. 2022.

</span>
<span class="ltx_bibblock">Interpreting medical images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Intelligent Systems in Medicine and Health</em>, pages 343–371. Springer.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2022)</span>
<span class="ltx_bibblock">
Zengle Zhu, Mintong Kang, Alan Yuille, and Zongwei Zhou. 2022.

</span>
<span class="ltx_bibblock">Assembling and exploiting large-scale existing labels of common thorax diseases for improved covid-19 classification using chest radiographs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Radiological Society of North America (RSNA)</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Distribution of Real Tumors</h2>

<figure id="A1.F7" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_distribution.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A1.F7.2.1" class="ltx_text ltx_font_bold">Intensity distribution of liver tumors and their healthy counterparts.</span> Our tumor generator, which is based on modeling and medical knowledge, incorporates the distributional characteristics of real tumors. We present the intensity distributions of real tumors obtained from the LiTS dataset. </figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_size_dis.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="329" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="A1.F8.2.1" class="ltx_text ltx_font_bold">Size distribution of liver tumors.</span> We have calculated the size distribution of liver tumors from the LiTS dataset. This tumor size distribution will serve as a guide for determining the sizes of the synthetic tumors we generate.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Shape Examples</h2>

<figure id="A2.F9" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_shape_examples.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="389" height="619" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A2.F9.2.1" class="ltx_text ltx_font_bold">Examples of the generated shape.</span> The tumor generator pipeline enables us to control the size and deformation of the generated tumors. Here, we present some examples of generated shapes under different conditions.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Texture Examples</h2>

<figure id="A3.F10" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_texture_examples.png" id="A3.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="389" height="615" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="A3.F10.2.1" class="ltx_text ltx_font_bold">Examples of the generated texture.</span> Our data synthesis strategy also enables us to generate different textures, as illustrated here for visualization.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Synthetic tumor examples</h2>

<figure id="A4.F11" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appdix_examples.png" id="A4.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span id="A4.F11.2.1" class="ltx_text ltx_font_bold">Visualization of synthetic data.</span> By combining all the pipelines together, we can obtain a wide range of diverse synthetic data for validation and training.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.16051" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.16052" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.16052">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.16052" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.16053" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 22:07:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
