<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2310.16052] Synthetic Data as Validation</title><meta property="og:description" content="This study leverages synthetic data as a validation set to reduce overfitting and ease the selection of the best model in AI development. While synthetic data have been used for augmenting the training set, we find thaâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Synthetic Data as Validation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Synthetic Data as Validation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2310.16052">

<!--Generated on Tue Feb 27 22:07:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line" lang="en">
<h1 class="ltx_title ltx_title_document">Synthetic Data as Validation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Qixin Hu 
<br class="ltx_break">The Chinese University of Hong Kong 
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter">qixinhu@cuhk.edu.hk</span>
&amp;Alan Yuille 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break"><span id="id3.2.id2" class="ltx_text ltx_font_typewriter">ayuille1@jhu.edu</span>
&amp;Zongwei Zhou 
<br class="ltx_break">Johns Hopkins University 
<br class="ltx_break"><span id="id4.3.id3" class="ltx_text ltx_font_typewriter">zzhou82@jh.edu</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Correponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p"><span id="id1.1.1" class="ltx_text">This study leverages synthetic data as a validation set to reduce overfitting and ease the selection of the best model in AI development. While synthetic data have been used for augmenting the training set, we find that synthetic data can also significantly diversify the validation set, offering marked advantages in domains like healthcare, where data are typically limited, sensitive, and from out-domain sources (i.e., hospitals). In this study, we illustrate the effectiveness of synthetic data for early cancer detection in computed tomography (CT) volumes, where synthetic tumors are generated and superimposed onto healthy organs, thereby creating an extensive dataset for rigorous validation. Using synthetic data as validation can improve AI robustness in both in-domain and out-domain test sets. Furthermore, we establish a new continual learning framework that continuously trains AI models on a stream of out-domain data with synthetic tumors. The AI model trained and validated in dynamically expanding synthetic data can consistently outperform models trained and validated exclusively on real-world data. Specifically, the DSC score for liver tumor segmentation improves from 26.7% (95% CI: 22.6%â€“30.9%) to 34.5% (30.8%â€“38.2%) when evaluated on an in-domain dataset and from 31.1% (26.0%â€“36.2%) to 35.4% (32.1%â€“38.7%) on an out-domain dataset. Importantly, the performance gain is particularly significant in identifying very tiny liver tumors (radius <math id="id1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="id1.1.1.m1.1a"><mo id="id1.1.1.m1.1.1" xref="id1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="id1.1.1.m1.1b"><lt id="id1.1.1.m1.1.1.cmml" xref="id1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="id1.1.1.m1.1c">&lt;</annotation></semantics></math> 5mm) in CT volumes, with Sensitivity improving from 33.1% to 55.4% on an in-domain dataset and 33.9% to 52.3% on an out-domain dataset, justifying the efficacy in early detection of cancer. The application of synthetic data, from both training and validation perspectives, underlines a promising avenue to enhance AI robustness when dealing with data from varying domains. As open science, we have released the codes and models at <a target="_blank" href="https://github.com/MrGiovanni/SyntheticValidation" title="" class="ltx_ref ltx_href">https://github.com/MrGiovanni/SyntheticValidation</a>.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Standard AI development divides the dataset into a training set and a test set; the former is used for model training and the latter for evaluationÂ <cite class="ltx_cite ltx_citemacro_citep">(Russell, <a href="#bib.bib41" title="" class="ltx_ref">2010</a>; Gareth etÂ al., <a href="#bib.bib12" title="" class="ltx_ref">2013</a>)</cite>. The AI model is updated every <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">training epochs</span>, resulting in a number of intermediate models during the training trajectory. The performance of these models tends to improve on the training set, but this does not mean that the performance on the test set also improves due to the over-fitting problemÂ <cite class="ltx_cite ltx_citemacro_citep">(Kuhn etÂ al., <a href="#bib.bib23" title="" class="ltx_ref">2013</a>)</cite>. A question then arises: <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">How do we identify the best model that performs well on the test set, especially when it is evaluated on test sets taken from different domains?</span> A prevalent strategy is to delineate a validation set from the training setÂ <cite class="ltx_cite ltx_citemacro_citep">(Ripley, <a href="#bib.bib38" title="" class="ltx_ref">2007</a>)</cite>. This validation set neither contributes to training nor to evaluating the AI performance. Instead, it functions as an independent set to fix the training hyper-parameters and, more importantly, to estimate the performance of each model on different datasets, thus enabling the selection of the best model from the many intermediate models during the training trajectory.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The validation set is often kept small. Naturally, we would like to maximize the use of the training data. Annotating data for AI training is time-consuming and expensive, requiring specialized expertise, so the annotated datasets are limited in size in many fieldsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou, <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>. Allocating too many annotated data for validation would inevitably diminish the training set size and compromise the AI training. On the other hand, the validation set should be sufficiently representative to provide a reliable performance estimate on unseen data. An overly small validation set might risk the reliability of performance estimation and checkpoint selection. As a result, the calibration of the validation set remains largely empirical and lacks systematic investigation for better alternatives to select the best checkpoint. Fulfilling this knowledge gap is particularly important in scenarios where real-world data are scarce, sensitive, or costly to collect and annotate, as seen in the field of <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">AI for healthcare</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a href="#bib.bib56" title="" class="ltx_ref">2022</a>)</cite>. Therefore, our study uses early detection of cancerous tumors in computed tomography (CT) volumes as a demonstration. While early detection of cancer holds immense clinical potential, it faces profound constraints like disease prevalence and annotation difficulty to collect examples of early-stage tumorsÂ <cite class="ltx_cite ltx_citemacro_citep">(Crosby etÂ al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite>. The scarcity of annotated early cancer not only constrains the data available for validation but also amplifies the overfitting problem inherent in a small, biased validation set, potentially causing underdiagnosis and overdiagnosis.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We propose using synthetic data as validation, a strategy that guarantees the full utilization of the training set while ensuring ample data diversity for validation. Data synthesis has held longstanding interest and presents numerous intriguing merits for augmenting training and test dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Gao etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> as reviewed in Â§<a href="#S2" title="2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, but its use in validation has seldom been explored. We find that synthetic data can facilitate a more reliable performance estimate on unseen data and effectively address the constraints commonly associated with small, biased validation sets. Specifically, we synthesize tumors in the healthy liver, which gives us orders of magnitude larger datasets for training. To ensure the realism of the synthetic tumors, we employ a modeling-based strategyÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> to simulate cancerous tumors with controlled shape, size, texture, location, and intensity. The use of diverse, healthy CT volumes, supplemented with synthetic tumors, as validation has demonstrated efficacy in mitigating model overfitting and enhancing the selection of checkpoints. Furthermore, we relieve the pressing demand for human annotations to train AI models by utilizing CT volumes with synthetic tumors as the training set. We then assess the modelâ€™s performance using a substantial number of publicly available, fully-annotated CT volumes with real-world cancerous tumors, showing that our models generalize well to these volumes from different hospitals and accurately segment the tumors at their early stage.
Our findings can be summarized as follows:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The best model checkpoint, selected by standard AI development with an in-domain real-tumor validation set, may not necessarily be generalized to unseen data, especially for an out-domain test set. This limitation arises from the validation set failing to adequately represent corner cases.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The best model checkpoint, selected by our strategy with a diverse synthetic-tumor validation set, tends to be generalized well to unseen data. This is because the validation set can cover theoretically infinite examples of possible cancerous tumors across diverse conditions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We introduce a novel continual learning framework. This framework integrates a continuous stream of synthetic data, characterized by diverse data distribution, for both training and validation. Traditional validation sets, constrained by static and limited in-domain real tumors, fall short in such a setting, whereas our synthetic tumors can be dynamically tailored to align with emerging distributions. Importantly, our framework can continuously generate tumors spanning a spectrum of sizesâ€”from small to largeâ€”enhancing the detection rate of tumors at their early stages.</p>
</div>
</li>
</ol>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Although our study focuses on AI in healthcare, the insight should be pertinent to various imaging applications within the field of computer vision. However, at the time this paper is written, very few studies in computer vision have provided evidence that training <span id="S1.p5.1.1" class="ltx_text ltx_font_italic">exclusively</span> on generated synthetic data can match or surpass the performance achieved when trained on real dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Black etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>. In specific applications, integrating synthetic data with real dataâ€”essentially acting as data augmentationâ€”has been found empirically to boost AI performanceÂ <cite class="ltx_cite ltx_citemacro_citep">(Mu etÂ al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Luzi etÂ al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>; Azizi etÂ al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>; Burg etÂ al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>. In this regard, data synthesisâ€”cancerous tumor synthesis in particularâ€”in medical imaging is relatively more successful<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The greater success of data synthesis in medical imaging (reviewed in Â§<a href="#S2" title="2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), compared with computer vision, can be attributed to two factors from our perspective. Firstly, the focus is primarily on synthesizing tumors rather than other components of the human anatomy. Secondly, the synthesis of tumors in 3D medical images is less complex as it does not require considerations for intricate variables such as lighting conditions, pose, and occlusion, which are typical in computer vision tasks.</span></span></span> with specific applications benefiting more from training exclusively on synthetic data than real data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">The dilemma of validation.</span></p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the field of machine learning, it is customary to use finite, static datasets with a pre-defined data split. While this standard offers a fair benchmark for comparing different AI models, it does not accurately represent real-world learning conditions. Two more realistic scenarios often arise in practice.</p>
</div>
<div id="S2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">The first scenario is the <span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">small data regime</span>, commonly observed in medical applications due to constraints like disease prevalence and annotation difficultyÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>. In such cases, curating an appropriate validation set poses a conundrum. A large validation set would compromise the size of the training set, whereas a small one may not sufficiently estimate the modelâ€™s performance. Despite its critical importance, this issue has yet to receive adequate attention in the field.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The second scenario involves dealing with <span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">a stream of data</span>, in a context of continual learning where the model encounters a continuous flow of new dataÂ <cite class="ltx_cite ltx_citemacro_citep">(Purushwalkam etÂ al., <a href="#bib.bib35" title="" class="ltx_ref">2022</a>)</cite>. A finite, static validation set proves unsuitable as it cannot accurately assess the modelâ€™s capability in processing an extensive and diverse data range. We argue that a validation setâ€”made up of real-world dataâ€”might not be needed during the training stage in such situations. Given the vastness of the training data, overfitting can be naturally avoided. Consequently, selecting the last-epoch model checkpoint could be a judicious choice.</p>
</div>
</li>
</ul>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Progresses in data synthesis.</span> Real-world data often encounters challenges such as poor quality, limited quantity, and inaccessibilityÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al., <a href="#bib.bib57" title="" class="ltx_ref">2022</a>; Qu etÂ al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Kang etÂ al., <a href="#bib.bib22" title="" class="ltx_ref">2023</a>; Liu etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2023a</a>)</cite>. To tackle these obstacles, the notion of <span id="S2.p4.1.2" class="ltx_text ltx_font_italic">synthetic data</span> has emerged as a practical alternative, allowing for the generation of samples as neededÂ <cite class="ltx_cite ltx_citemacro_citep">(Jordon etÂ al., <a href="#bib.bib21" title="" class="ltx_ref">2018</a>; Yoon etÂ al., <a href="#bib.bib52" title="" class="ltx_ref">2019</a>; Chen etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>)</cite>. This approach has proven valuable in addressing data limitations and facilitating machine learning processes, including computer visionÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Ramesh etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2021</a>)</cite>, natural language processingÂ <cite class="ltx_cite ltx_citemacro_citep">(Collobert and Weston, <a href="#bib.bib9" title="" class="ltx_ref">2008</a>; Brown etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, voiceÂ <cite class="ltx_cite ltx_citemacro_citep">(Oord etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2016</a>)</cite>, and many other fieldsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wiese etÂ al., <a href="#bib.bib48" title="" class="ltx_ref">2020</a>; Jin etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2018</a>; Zheng etÂ al., <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>. In the medical domain, the practice of data synthesisâ€”<span id="S2.p4.1.3" class="ltx_text ltx_font_italic">tumor synthesis</span> in particularâ€”endeavors to produce artificial tumors in the image, which can significantly diversify the data and annotations for AI trainingÂ <cite class="ltx_cite ltx_citemacro_citep">(Xing etÂ al., <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite> and, arguably, can strengthen the AI robustness evaluation using corner cases generated by data synthesis. Successful works related to tumor synthesis include polyp detection from colonoscopy videosÂ <cite class="ltx_cite ltx_citemacro_citep">(Shin etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2018</a>)</cite>, COVID-19 detection from Chest CT and X-rayÂ <cite class="ltx_cite ltx_citemacro_citep">(Yao etÂ al., <a href="#bib.bib51" title="" class="ltx_ref">2021</a>; Lyu etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Gao etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, diabetic lesion detection from retinal imagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a href="#bib.bib47" title="" class="ltx_ref">2022</a>)</cite>, cancer detection from fluorescence microscopy imagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Horvath etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite>, brain tumor detection from MRIÂ <cite class="ltx_cite ltx_citemacro_citep">(Wyatt etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2022</a>)</cite>, early pancreatic cancer localization from CTÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite>, and pneumonia detection from ultrasoundÂ <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al., <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>. A recent studyÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2022</a>, <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite> indicated that AI trained <span id="S2.p4.1.4" class="ltx_text ltx_font_italic">exclusively</span> on synthetic tumors can segment liver tumors with accuracy comparable to that on real tumors.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">To the best of our knowledge, data synthesis has been widely recognized for its contribution to enhancing training and test datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Johnson etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2017</a>; Jordon etÂ al., <a href="#bib.bib20" title="" class="ltx_ref">2022</a>; Liu etÂ al., <a href="#bib.bib29" title="" class="ltx_ref">2023b</a>)</cite>, but its capacity for improving the validation set remains largely untapped. In this paper, we extend the application of synthetic data to the validation set, enabling the full use of the annotated data for AI training while ensuring diverse and comprehensive validation data in the framework of continual learning.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2310.16052/assets/fig_methods.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S2.F1.6.1" class="ltx_text ltx_font_bold">Illustration of continual learning framework.</span> <span id="S2.F1.7.2" class="ltx_text ltx_font_bold">(a)</span> The setting for the static training, where the real-tumor dataset is partitioned into training and validation sets. The AI model is then developed using these datasets and subsequently tested with unseen data. <span id="S2.F1.8.3" class="ltx_text ltx_font_bold">(b)</span> Dynamic training setting integrated with synthetic data. <span id="S2.F1.9.4" class="ltx_text ltx_font_bold">(c)</span> Tumor generator pipeline. By leveraging an advanced tumor generator, we can create a dynamic training and validation set. Models developed using the continual learning framework exhibit superior performance compared to static training settings. <span id="S2.F1.10.5" class="ltx_text ltx_font_bold">(d)</span> Visualization of synthetic data.</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method &amp; Material</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Continual Learning for Tumor Segmentation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.4" class="ltx_p">According toÂ <cite class="ltx_cite ltx_citemacro_citet">VanÂ de Ven and Tolias (<a href="#bib.bib45" title="" class="ltx_ref">2019</a>); VanÂ de Ven etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2022</a>)</cite>, continual learning can be categorized into three settings: class-incremental learning, task-incremental learning, and domain-incremental learning. In the domain-incremental setting, which is relevant to our situation, the task remains the same while the data distribution changes. More specifically, the model sequentially encounters data from a continuum of domains (datasets):</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\{X_{1},Y_{1}\},\{X_{2},Y_{2}\},...,\{X_{N},Y_{N}\}," display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2.1"><mrow id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.4.cmml"><mrow id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.2.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml">X</mi><mn id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E1.m1.2.2.1.1.1.1.2.4" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.1.1.2.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml">Y</mi><mn id="S3.E1.m1.2.2.1.1.1.1.2.2.3" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml">1</mn></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.2.5" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">}</mo></mrow><mo id="S3.E1.m1.2.2.1.1.3.4" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.2.1.1.2.cmml">X</mi><mn id="S3.E1.m1.2.2.1.1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.2.2.1.1.3.cmml">2</mn></msub><mo id="S3.E1.m1.2.2.1.1.2.2.2.4" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.2.2.2.2.2" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml">Y</mi><mn id="S3.E1.m1.2.2.1.1.2.2.2.2.3" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml">2</mn></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.2.2.2.5" xref="S3.E1.m1.2.2.1.1.2.2.3.cmml">}</mo></mrow><mo id="S3.E1.m1.2.2.1.1.3.5" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mi mathvariant="normal" id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">â€¦</mi><mo id="S3.E1.m1.2.2.1.1.3.6" xref="S3.E1.m1.2.2.1.1.4.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.3.3.2" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.3" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">{</mo><msub id="S3.E1.m1.2.2.1.1.3.3.1.1" xref="S3.E1.m1.2.2.1.1.3.3.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.3.3.1.1.2" xref="S3.E1.m1.2.2.1.1.3.3.1.1.2.cmml">X</mi><mi id="S3.E1.m1.2.2.1.1.3.3.1.1.3" xref="S3.E1.m1.2.2.1.1.3.3.1.1.3.cmml">N</mi></msub><mo id="S3.E1.m1.2.2.1.1.3.3.2.4" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">,</mo><msub id="S3.E1.m1.2.2.1.1.3.3.2.2" xref="S3.E1.m1.2.2.1.1.3.3.2.2.cmml"><mi id="S3.E1.m1.2.2.1.1.3.3.2.2.2" xref="S3.E1.m1.2.2.1.1.3.3.2.2.2.cmml">Y</mi><mi id="S3.E1.m1.2.2.1.1.3.3.2.2.3" xref="S3.E1.m1.2.2.1.1.3.3.2.2.3.cmml">N</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.3.3.2.5" xref="S3.E1.m1.2.2.1.1.3.3.3.cmml">}</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><list id="S3.E1.m1.2.2.1.1.4.cmml" xref="S3.E1.m1.2.2.1.1.3"><set id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.2">ğ‘‹</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.E1.m1.2.2.1.1.1.1.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.2">ğ‘Œ</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2.2.3">1</cn></apply></set><set id="S3.E1.m1.2.2.1.1.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2"><apply id="S3.E1.m1.2.2.1.1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1.2">ğ‘‹</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.1.1.3">2</cn></apply><apply id="S3.E1.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.2">ğ‘Œ</ci><cn type="integer" id="S3.E1.m1.2.2.1.1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.2.2.2.2.3">2</cn></apply></set><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">â€¦</ci><set id="S3.E1.m1.2.2.1.1.3.3.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2"><apply id="S3.E1.m1.2.2.1.1.3.3.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.3.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.3.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1.2">ğ‘‹</ci><ci id="S3.E1.m1.2.2.1.1.3.3.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.1.1.3">ğ‘</ci></apply><apply id="S3.E1.m1.2.2.1.1.3.3.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.3.3.2.2.2.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2.2">ğ‘Œ</ci><ci id="S3.E1.m1.2.2.1.1.3.3.2.2.3.cmml" xref="S3.E1.m1.2.2.1.1.3.3.2.2.3">ğ‘</ci></apply></set></list></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\{X_{1},Y_{1}\},\{X_{2},Y_{2}\},...,\{X_{N},Y_{N}\},</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.p1.3" class="ltx_p">The objective is to train a model <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathscr{F}:X\rightarrow Y" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathscript" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">â„±</mi><mo lspace="0.278em" rspace="0.278em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">:</mo><mrow id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">X</mi><mo stretchy="false" id="S3.SS1.p1.1.m1.1.1.3.1" xref="S3.SS1.p1.1.m1.1.1.3.1.cmml">â†’</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml">Y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><ci id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">:</ci><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">â„±</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><ci id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.1">â†’</ci><ci id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">ğ‘‹</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3">ğ‘Œ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathscr{F}:X\rightarrow Y</annotation></semantics></math> that can be effectively queried at any given time, regardless of the data distribution. In liver tumor segmentation tasks, <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">X</annotation></semantics></math> is the CT volume and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="Y" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">Y</annotation></semantics></math> is the tumor mask. The continuum of domains refers to CT volumes taken from different medical centers.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In the setting of <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">static</span> training, shown in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a), the AI model is trained and validated on fixed subsets of a dataset. This setting presents three limitations: Firstly, the limited scales and acquisition sources of the data, coupled with unchanged data distribution, pose challenges in generalizing to out-domain data. Secondly, the task of data annotation, specifically tumor annotation, is exceptionally challenging as it often requires the use of corroborative pathology reports. This requirement adds to the difficulty of extending the dataset. Thirdly, there are specific cases, such as extremely small tumors, where obtaining real data becomes significantly challenging. As a result, static training will likely result in biased, sub-optimal performance on unseen data, especially for out-domain test sets.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In contrast, the setting of <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_italic">dynamic</span> training achieved by synthetic data, shown in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b), can overcome the aforementioned limitations. In this setting, the AI model is trained and validated on a dynamically changing dataset. In our study, this dynamic dataset is a stream of normal CT volumesâ€”over 40 million CT volumes in the United States each year. Generating synthetic tumors is advantageous because, firstly, acquiring healthy CT volumes is much easier than obtaining those with cancerous tumors. As a result, our continual learning framework can start from a diverse dataset comprising CT volumes of healthy subjects from multiple domains. Secondly, by controlling the parameters within our framework, we have the ability to generate synthetic data that fulfills specific requirements, including those of a tiny radius (<math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mo id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><lt id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">&lt;</annotation></semantics></math>5 mm, shown in AppendixÂ FigureÂ <a href="#A2.F9" title="Figure 9 â€£ Appendix B Shape Examples â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>). Consequently, our framework achieves a noteworthy level of diversity, encompassing a wide array of variations. Therefore, the AI model developed using this framework of synthetic data has the potential to improve its performance on out-domain data.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Modeling-Based Synthetic Tumor Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Following the standardized clinical guidance and statistical distribution of real tumors, as detailed in AppendixÂ FigureÂ <a href="#A1.F7" title="Figure 7 â€£ Appendix A Distribution of Real Tumors â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>Â <a href="#A1.F8" title="Figure 8 â€£ Appendix A Distribution of Real Tumors â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we develop a modeling-based strategy to generate synthetic tumors. For example, according to the Liver Imaging Reporting and Data System (LI-RADS)Â <cite class="ltx_cite ltx_citemacro_citep">(Chernyak etÂ al., <a href="#bib.bib8" title="" class="ltx_ref">2018</a>)</cite>, the malignancy of hepatocellular carcinomas is determined by shape, size, location, and texture, enhancing capsule appearance. We use a sequence of morphological image-processing operations to model real tumors, as shown in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(c). The tumor generator consists of four steps: (1) location selection, (2) shape generation, (3) texture generation, and (4) post-processing.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Location selection.</span> Liver tumors generally do not allow the passage of preexisting blood vessels from the host tissue through them. To address this concern, we initially perform voxel value thresholding for vessel segmentationÂ <cite class="ltx_cite ltx_citemacro_citep">(Gonzalez, <a href="#bib.bib14" title="" class="ltx_ref">2009</a>)</cite>. Utilizing the vessel mask acquired from this step enables us to identify if a particular location can cause the tumor-blood collision.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.5" class="ltx_p"><span id="S3.I1.i2.p1.5.1" class="ltx_text ltx_font_bold">Shape generation.</span> Based on clinical knowledge, a tumor is initiated from a malignant cell and gradually proliferates and expands, resulting in a nearly spherical shape for small tumors (<math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mo id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><leq id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\leq</annotation></semantics></math>5mm). On the other hand, statistical distributions of real liver tumors indicate that larger tumors tend to exhibit an elliptical shape. This observation has inspired us to generate a tumor-like shape using an ellipsoid <math id="S3.I1.i2.p1.2.m2.3" class="ltx_Math" alttext="\textit{ellip}(a,b,c)" display="inline"><semantics id="S3.I1.i2.p1.2.m2.3a"><mrow id="S3.I1.i2.p1.2.m2.3.4" xref="S3.I1.i2.p1.2.m2.3.4.cmml"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.2.m2.3.4.2" xref="S3.I1.i2.p1.2.m2.3.4.2a.cmml">ellip</mtext><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.2.m2.3.4.1" xref="S3.I1.i2.p1.2.m2.3.4.1.cmml">â€‹</mo><mrow id="S3.I1.i2.p1.2.m2.3.4.3.2" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml"><mo stretchy="false" id="S3.I1.i2.p1.2.m2.3.4.3.2.1" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">(</mo><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.2.m2.3.4.3.2.2" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.2.m2.2.2" xref="S3.I1.i2.p1.2.m2.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.2.m2.3.4.3.2.3" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.2.m2.3.3" xref="S3.I1.i2.p1.2.m2.3.3.cmml">c</mi><mo stretchy="false" id="S3.I1.i2.p1.2.m2.3.4.3.2.4" xref="S3.I1.i2.p1.2.m2.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.3b"><apply id="S3.I1.i2.p1.2.m2.3.4.cmml" xref="S3.I1.i2.p1.2.m2.3.4"><times id="S3.I1.i2.p1.2.m2.3.4.1.cmml" xref="S3.I1.i2.p1.2.m2.3.4.1"></times><ci id="S3.I1.i2.p1.2.m2.3.4.2a.cmml" xref="S3.I1.i2.p1.2.m2.3.4.2"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.2.m2.3.4.2.cmml" xref="S3.I1.i2.p1.2.m2.3.4.2">ellip</mtext></ci><vector id="S3.I1.i2.p1.2.m2.3.4.3.1.cmml" xref="S3.I1.i2.p1.2.m2.3.4.3.2"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">ğ‘</ci><ci id="S3.I1.i2.p1.2.m2.2.2.cmml" xref="S3.I1.i2.p1.2.m2.2.2">ğ‘</ci><ci id="S3.I1.i2.p1.2.m2.3.3.cmml" xref="S3.I1.i2.p1.2.m2.3.3">ğ‘</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.3c">\textit{ellip}(a,b,c)</annotation></semantics></math>, where <math id="S3.I1.i2.p1.3.m3.3" class="ltx_Math" alttext="a,b,c" display="inline"><semantics id="S3.I1.i2.p1.3.m3.3a"><mrow id="S3.I1.i2.p1.3.m3.3.4.2" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml"><mi id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.3.m3.3.4.2.1" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml">,</mo><mi id="S3.I1.i2.p1.3.m3.2.2" xref="S3.I1.i2.p1.3.m3.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.3.m3.3.4.2.2" xref="S3.I1.i2.p1.3.m3.3.4.1.cmml">,</mo><mi id="S3.I1.i2.p1.3.m3.3.3" xref="S3.I1.i2.p1.3.m3.3.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.3b"><list id="S3.I1.i2.p1.3.m3.3.4.1.cmml" xref="S3.I1.i2.p1.3.m3.3.4.2"><ci id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">ğ‘</ci><ci id="S3.I1.i2.p1.3.m3.2.2.cmml" xref="S3.I1.i2.p1.3.m3.2.2">ğ‘</ci><ci id="S3.I1.i2.p1.3.m3.3.3.cmml" xref="S3.I1.i2.p1.3.m3.3.3">ğ‘</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.3c">a,b,c</annotation></semantics></math> are the lengths of the semi-axes. Additionally, we utilize elastic deformationÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite> to enhance the authenticity of the generated tumor shapes <math id="S3.I1.i2.p1.4.m4.5" class="ltx_Math" alttext="D(\textit{ellip}(a,b,c),\sigma_{d})" display="inline"><semantics id="S3.I1.i2.p1.4.m4.5a"><mrow id="S3.I1.i2.p1.4.m4.5.5" xref="S3.I1.i2.p1.4.m4.5.5.cmml"><mi id="S3.I1.i2.p1.4.m4.5.5.4" xref="S3.I1.i2.p1.4.m4.5.5.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.4.m4.5.5.3" xref="S3.I1.i2.p1.4.m4.5.5.3.cmml">â€‹</mo><mrow id="S3.I1.i2.p1.4.m4.5.5.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml"><mo stretchy="false" id="S3.I1.i2.p1.4.m4.5.5.2.2.3" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">(</mo><mrow id="S3.I1.i2.p1.4.m4.4.4.1.1.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.cmml"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2a.cmml">ellip</mtext><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.1.cmml">â€‹</mo><mrow id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml"><mo stretchy="false" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.1" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">(</mo><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">a</mi><mo id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.2" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.4.m4.2.2" xref="S3.I1.i2.p1.4.m4.2.2.cmml">b</mi><mo id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.3" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">,</mo><mi id="S3.I1.i2.p1.4.m4.3.3" xref="S3.I1.i2.p1.4.m4.3.3.cmml">c</mi><mo stretchy="false" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2.4" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml">)</mo></mrow></mrow><mo id="S3.I1.i2.p1.4.m4.5.5.2.2.4" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">,</mo><msub id="S3.I1.i2.p1.4.m4.5.5.2.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.cmml"><mi id="S3.I1.i2.p1.4.m4.5.5.2.2.2.2" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.2.cmml">Ïƒ</mi><mi id="S3.I1.i2.p1.4.m4.5.5.2.2.2.3" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.3.cmml">d</mi></msub><mo stretchy="false" id="S3.I1.i2.p1.4.m4.5.5.2.2.5" xref="S3.I1.i2.p1.4.m4.5.5.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.5b"><apply id="S3.I1.i2.p1.4.m4.5.5.cmml" xref="S3.I1.i2.p1.4.m4.5.5"><times id="S3.I1.i2.p1.4.m4.5.5.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.3"></times><ci id="S3.I1.i2.p1.4.m4.5.5.4.cmml" xref="S3.I1.i2.p1.4.m4.5.5.4">ğ·</ci><interval closure="open" id="S3.I1.i2.p1.4.m4.5.5.2.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2"><apply id="S3.I1.i2.p1.4.m4.4.4.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1"><times id="S3.I1.i2.p1.4.m4.4.4.1.1.1.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.1"></times><ci id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2a.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2"><mtext class="ltx_mathvariant_italic" id="S3.I1.i2.p1.4.m4.4.4.1.1.1.2.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.2">ellip</mtext></ci><vector id="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.1.cmml" xref="S3.I1.i2.p1.4.m4.4.4.1.1.1.3.2"><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">ğ‘</ci><ci id="S3.I1.i2.p1.4.m4.2.2.cmml" xref="S3.I1.i2.p1.4.m4.2.2">ğ‘</ci><ci id="S3.I1.i2.p1.4.m4.3.3.cmml" xref="S3.I1.i2.p1.4.m4.3.3">ğ‘</ci></vector></apply><apply id="S3.I1.i2.p1.4.m4.5.5.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.4.m4.5.5.2.2.2.1.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2">subscript</csymbol><ci id="S3.I1.i2.p1.4.m4.5.5.2.2.2.2.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.2">ğœ</ci><ci id="S3.I1.i2.p1.4.m4.5.5.2.2.2.3.cmml" xref="S3.I1.i2.p1.4.m4.5.5.2.2.2.3">ğ‘‘</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.5c">D(\textit{ellip}(a,b,c),\sigma_{d})</annotation></semantics></math>, where <math id="S3.I1.i2.p1.5.m5.1" class="ltx_Math" alttext="\sigma_{d}" display="inline"><semantics id="S3.I1.i2.p1.5.m5.1a"><msub id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml"><mi id="S3.I1.i2.p1.5.m5.1.1.2" xref="S3.I1.i2.p1.5.m5.1.1.2.cmml">Ïƒ</mi><mi id="S3.I1.i2.p1.5.m5.1.1.3" xref="S3.I1.i2.p1.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><apply id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.5.m5.1.1.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2">ğœ</ci><ci id="S3.I1.i2.p1.5.m5.1.1.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">\sigma_{d}</annotation></semantics></math> control the magnitude of displacements. We show examples of the generated tumor shapes in AppendixÂ FigureÂ <a href="#A2.F9" title="Figure 9 â€£ Appendix B Shape Examples â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Texture generation.</span> The generation of textures is a significant challenge due to the varied patterns found in tumors. Our current understanding of tumor textures is derived solely from clinical expertise, which considers factors such as the attenuation value and the distribution characteristics. To achieve the desired texture, we introduce Gaussian noise <math id="S3.I1.i3.p1.1.m1.2" class="ltx_Math" alttext="\mathcal{N}(\mu,\sigma_{g})" display="inline"><semantics id="S3.I1.i3.p1.1.m1.2a"><mrow id="S3.I1.i3.p1.1.m1.2.2" xref="S3.I1.i3.p1.1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.2.2.3" xref="S3.I1.i3.p1.1.m1.2.2.3.cmml">ğ’©</mi><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.1.m1.2.2.2" xref="S3.I1.i3.p1.1.m1.2.2.2.cmml">â€‹</mo><mrow id="S3.I1.i3.p1.1.m1.2.2.1.1" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.I1.i3.p1.1.m1.2.2.1.1.2" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">(</mo><mi id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml">Î¼</mi><mo id="S3.I1.i3.p1.1.m1.2.2.1.1.3" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">,</mo><msub id="S3.I1.i3.p1.1.m1.2.2.1.1.1" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.cmml"><mi id="S3.I1.i3.p1.1.m1.2.2.1.1.1.2" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml">Ïƒ</mi><mi id="S3.I1.i3.p1.1.m1.2.2.1.1.1.3" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml">g</mi></msub><mo stretchy="false" id="S3.I1.i3.p1.1.m1.2.2.1.1.4" xref="S3.I1.i3.p1.1.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.2b"><apply id="S3.I1.i3.p1.1.m1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2"><times id="S3.I1.i3.p1.1.m1.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.2"></times><ci id="S3.I1.i3.p1.1.m1.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.3">ğ’©</ci><interval closure="open" id="S3.I1.i3.p1.1.m1.2.2.1.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1"><ci id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">ğœ‡</ci><apply id="S3.I1.i3.p1.1.m1.2.2.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.2">ğœ</ci><ci id="S3.I1.i3.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.2.2.1.1.1.3">ğ‘”</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.2c">\mathcal{N}(\mu,\sigma_{g})</annotation></semantics></math> with a predetermined mean attenuation value, matching the standard deviation of liver tumors. Subsequently, we use cubic interpolation to smooth the texture. Furthermore, to better replicate textures obtained from CT imaging, we use a final step of texture blurring. Examples of the generated texture can be found in AppendixÂ FigureÂ <a href="#A3.F10" title="Figure 10 â€£ Appendix C Texture Examples â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Post-processing.</span> The post-processing involves evaluating image characteristics through visual inspection and feedback from medical professionals. The purpose of these steps is to replicate the phenomena of mass effect and the appearance of a capsuleÂ <cite class="ltx_cite ltx_citemacro_citep">(Lee etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2004</a>)</cite>. Mass effect refers to the phenomenon wherein the tumor undergoes growth, resulting in the displacement and deformation of surrounding tissues. We utilize local scaling warpingÂ <cite class="ltx_cite ltx_citemacro_citep">(Glasbey and Mardia, <a href="#bib.bib13" title="" class="ltx_ref">1998</a>)</cite> to replicate this effect. Additionally, we brighten the edges of the tumor, thereby simulating the capsule appearance. Consequently, CT volumes with synthetic tumors can be used for the continual learning framework, where examples of the generated liver tumors can be found in AppendixÂ FigureÂ <a href="#A4.F11" title="Figure 11 â€£ Appendix D Synthetic tumor examples â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset &amp; Benchmark</h3>

<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>
<span id="S4.T1.2.1" class="ltx_text ltx_font_bold">Datasets description.</span> The LiTS dataset was used to train, validate, and evaluate AI models in segmenting liver tumors. The FLAREâ€™23 dataset was used for external validation. An assembly of the CHAOSÂ <cite class="ltx_cite ltx_citemacro_citep">(Valindria etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, BTCVÂ <cite class="ltx_cite ltx_citemacro_citep">(Landman etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>, and Pancreas-CTÂ <cite class="ltx_cite ltx_citemacro_citep">(Roth etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> datasets were used for generating synthetic training and validation sets, in which the liver in these datasets is confirmed to be healthy.
</figcaption>
<table id="S4.T1.3" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.3.1" class="ltx_tr">
<td id="S4.T1.3.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.1.1.1" class="ltx_p" style="width:121.4pt;">dataset</span>
</span>
</td>
<td id="S4.T1.3.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.2.1.1" class="ltx_p" style="width:52.0pt;">notation</span>
</span>
</td>
<td id="S4.T1.3.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.3.1.1" class="ltx_p" style="width:43.4pt;">split</span>
</span>
</td>
<td id="S4.T1.3.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.4.1.1" class="ltx_p" style="width:43.4pt;">annotation</span>
</span>
</td>
<td id="S4.T1.3.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.5.1.1" class="ltx_p" style="width:43.4pt;"># of CTs</span>
</span>
</td>
<td id="S4.T1.3.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S4.T1.3.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.1.6.1.1" class="ltx_p" style="width:52.0pt;">tumor</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.2" class="ltx_tr">
<td id="S4.T1.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="3">
<span id="S4.T1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.2.1.1.1.1" class="ltx_text">LiTSÂ <cite class="ltx_cite ltx_citemacro_citep">(Bilic etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 1</span>
</span>
</td>
<td id="S4.T1.3.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.3.1.1" class="ltx_p" style="width:43.4pt;">training</span>
</span>
</td>
<td id="S4.T1.3.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.4.1.1" class="ltx_p" style="width:43.4pt;">âœ“</span>
</span>
</td>
<td id="S4.T1.3.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.5.1.1" class="ltx_p" style="width:43.4pt;">25</span>
</span>
</td>
<td id="S4.T1.3.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.2.6.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 2</span>
</span>
</td>
<td id="S4.T1.3.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.2.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.3.1.1" class="ltx_p" style="width:43.4pt;">âœ“</span>
</span>
</td>
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.4.1.1" class="ltx_p" style="width:43.4pt;">5</span>
</span>
</td>
<td id="S4.T1.3.3.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.3.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.4" class="ltx_tr">
<td id="S4.T1.3.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 3</span>
</span>
</td>
<td id="S4.T1.3.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.2.1.1" class="ltx_p" style="width:43.4pt;">testing</span>
</span>
</td>
<td id="S4.T1.3.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.3.1.1" class="ltx_p" style="width:43.4pt;">âœ“</span>
</span>
</td>
<td id="S4.T1.3.4.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.4.1.1" class="ltx_p" style="width:43.4pt;">70</span>
</span>
</td>
<td id="S4.T1.3.4.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.4.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.5" class="ltx_tr">
<td id="S4.T1.3.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" rowspan="2">
<span id="S4.T1.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.5.1.1.1.1" class="ltx_text">AssemblyÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 4</span>
</span>
</td>
<td id="S4.T1.3.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.3.1.1" class="ltx_p" style="width:43.4pt;">training</span>
</span>
</td>
<td id="S4.T1.3.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.4.1.1" class="ltx_p" style="width:43.4pt;">âœ—</span>
</span>
</td>
<td id="S4.T1.3.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.5.1.1" class="ltx_p" style="width:43.4pt;">25</span>
</span>
</td>
<td id="S4.T1.3.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.5.6.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.6" class="ltx_tr">
<td id="S4.T1.3.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 5</span>
</span>
</td>
<td id="S4.T1.3.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.2.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.3.1.1" class="ltx_p" style="width:43.4pt;">âœ—</span>
</span>
</td>
<td id="S4.T1.3.6.4" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.4.1.1" class="ltx_p" style="width:43.4pt;">50</span>
</span>
</td>
<td id="S4.T1.3.6.5" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T1.3.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.6.5.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.7" class="ltx_tr">
<td id="S4.T1.3.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" rowspan="2">
<span id="S4.T1.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.1.1.1" class="ltx_p" style="width:121.4pt;"><span id="S4.T1.3.7.1.1.1.1" class="ltx_text">FLAREâ€™23Â <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite></span></span>
</span>
</td>
<td id="S4.T1.3.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.2.1.1" class="ltx_p" style="width:52.0pt;">cohort 6</span>
</span>
</td>
<td id="S4.T1.3.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.3.1.1" class="ltx_p" style="width:43.4pt;">validation</span>
</span>
</td>
<td id="S4.T1.3.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.4.1.1" class="ltx_p" style="width:43.4pt;">âœ—</span>
</span>
</td>
<td id="S4.T1.3.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.5.1.1" class="ltx_p" style="width:43.4pt;">50</span>
</span>
</td>
<td id="S4.T1.3.7.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T1.3.7.6.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.7.6.1.1" class="ltx_p" style="width:52.0pt;">synthetic</span>
</span>
</td>
</tr>
<tr id="S4.T1.3.8" class="ltx_tr">
<td id="S4.T1.3.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 7</span>
</span>
</td>
<td id="S4.T1.3.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.2.1.1" class="ltx_p" style="width:43.4pt;">testing</span>
</span>
</td>
<td id="S4.T1.3.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.3.1.1" class="ltx_p" style="width:43.4pt;">âœ“</span>
</span>
</td>
<td id="S4.T1.3.8.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.4.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.4.1.1" class="ltx_p" style="width:43.4pt;">120</span>
</span>
</td>
<td id="S4.T1.3.8.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="S4.T1.3.8.5.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T1.3.8.5.1.1" class="ltx_p" style="width:52.0pt;">real</span>
</span>
</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">TableÂ <a href="#S4.T1" title="Table 1 â€£ 4.1 Dataset &amp; Benchmark â€£ 4 Experiment â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes a total of five publicly available datasets used in this study. We group them into three classes.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.2" class="ltx_p"><span id="S4.I1.i1.p1.2.1" class="ltx_text ltx_font_bold">Real-tumor dataset.</span> We select the LiTS datasetÂ <cite class="ltx_cite ltx_citemacro_citep">(Bilic etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> for training and testing AI models. LiTS provides detailed per-voxel annotations of liver tumors. The tumor types include HCC and secondary liver tumors and metastasis derived from colorectal, breast, and lung cancer. The size of liver tumors ranges from 38mm<sup id="S4.I1.i1.p1.2.2" class="ltx_sup">3</sup> to 349 cm<sup id="S4.I1.i1.p1.2.3" class="ltx_sup">3</sup>, and the radius of tumors is approximately in the range of [2, 44] mm. LiTS is partitioned into a training set (<span id="S4.I1.i1.p1.2.4" class="ltx_text ltx_font_italic">cohort 1</span>; 25 CT volumes), validation set (<span id="S4.I1.i1.p1.2.5" class="ltx_text ltx_font_italic">cohort 2</span>; 5 CT volumes), and test set (<span id="S4.I1.i1.p1.2.6" class="ltx_text ltx_font_italic">cohort 3</span>; 70 CT volumes).</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><span id="S4.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Healthy CT assembly.</span> We have collected a dataset of 75 CT volumes with healthy liver assembled from CHAOSÂ <cite class="ltx_cite ltx_citemacro_citep">(Valindria etÂ al., <a href="#bib.bib44" title="" class="ltx_ref">2018</a>)</cite>, Pancreas-CTÂ <cite class="ltx_cite ltx_citemacro_citep">(Roth etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite> and BTCVÂ <cite class="ltx_cite ltx_citemacro_citep">(Landman etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2015</a>)</cite>. This assembled dataset is partitioned into a training set (<span id="S4.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">cohort 4</span>; 25 CT volumes) and a validation set (<span id="S4.I1.i2.p1.1.3" class="ltx_text ltx_font_italic">cohort 5</span>; 50 CT volumes). As illustrated in FigureÂ <a href="#S2.F1" title="Figure 1 â€£ 2 Related Work â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b) For the training set, tumors were dynamically generated within these volumes during training, resulting in a sequential collection of image-label pairs comprising synthetic tumors. For the validation set, we generated three different tumor sizes (small, medium, and large) for each healthy CT volume offline, giving a total of 150 CT volumes.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><span id="S4.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">External benchmark.</span> FLAREâ€™23Â <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al., <a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite> is used for an external benchmark because it provides out-domain CT volumes from the LiTS dataset. This dataset was specifically chosen due to its extensive coverage, encompassing over 4000 3D CT volumes obtained from more than 30 medical centers. The inclusion of such a diverse dataset ensures the generalizability of the benchmark. The FLAREâ€™23 dataset contains partially labeled annotations. To ensure the suitability of the test set, specific criteria are applied to the annotations. These criteria require that the annotations include per-voxel labeling for both the liver and tumors, with the additional constraint that the connected component of the tumor must intersect with the liver. Adhering to these conditions, we chose the external test set (<span id="S4.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">cohort 7</span>; 120 CT volumes). Additionally, same as the assembly dataset, we can use the healthy cases within the FLAREâ€™23 to generate synthetic data to serve as <span id="S4.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">in-domain</span> validation set (<span id="S4.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">cohort 6</span>; 50 CT volumes), which will be used in Â§<a href="#S5.SS5" title="5.5 Continual Learning Framework is Enhanced by In-domain Synthetic-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.2" class="ltx_p">We have implemented our codes utilizing the MONAI<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://monai.io/" title="" class="ltx_ref ltx_href">https://monai.io/</a></span></span></span> framework for the U-Net architectureÂ <cite class="ltx_cite ltx_citemacro_citep">(Ronneberger etÂ al., <a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite>, a well-established network commonly employed in medical image segmentation tasks. During the pre-processing stage, input images undergo clipping with a window range of [-21,189]. Following this, they are normalized to achieve a zero mean and unit standard deviationÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a href="#bib.bib43" title="" class="ltx_ref">2022</a>)</cite>. For training purposes, random patches with dimensions of <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="96\times 96\times 96" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">96</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS2.p1.1.m1.1.1.1a" xref="S4.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS2.p1.1.m1.1.1.4" xref="S4.SS2.p1.1.m1.1.1.4.cmml">96</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><times id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">96</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">96</cn><cn type="integer" id="S4.SS2.p1.1.m1.1.1.4.cmml" xref="S4.SS2.p1.1.m1.1.1.4">96</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">96\times 96\times 96</annotation></semantics></math> are cropped from the 3D image volumes. A base learning rate of 0.0002 is utilized in the training process, accompanied by a batch size of two per GPU. To further enhance the training process, we employ both the linear warmup strategy and the cosine annealing learning rate schedule. Our model is trained for 6,000 epochs, with a model checkpoint being saved every 100 epochs, and a total of 60 model checkpoints are saved throughout the entire training process. During the inference phase, a sliding window strategy with an overlapping area ratio of 0.75 is adopted. To ensure robustness and comprehensiveness in obtaining results, the experiment is conducted ten times each to perform statistical analysis. By averaging all runs, we obtain reliable results. The segmentation performance is evaluated using the Dice Similarity Coefficient (DSC) score, while Sensitivity is used to evaluate the performance of detecting very tiny liver tumors (radius <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><lt id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">&lt;</annotation></semantics></math> 5mm).</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Result</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Summary.</span> Using synthetic data as validation can select the best model checkpoint and alleviate the overfitting problem. Furthermore, the AI model developed using our continual learning framework outperforms models trained and validated on a static dataset. The performance is particularly high for detecting small/tiny tumors because we can generate a vast number of examples of small/tiny tumors for both training and validation.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2310.16052/assets/fig_real_validation.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S5.F2.9.1" class="ltx_text ltx_font_bold">The overfitting is due to a small-scale, biased real-tumor validation set.</span>
Model checkpoints are saved at each training epoch, and their performance trajectories are evaluated. The <span id="S5.F2.10.2" class="ltx_text" style="color:#3CBA54;">green</span> curve plots the test set performance of each checkpoint. It serves as the gold standard for a specific dataset, though the test set performance is often apriori unknown. The <span id="S5.F2.11.3" class="ltx_text" style="color:#999999;">gray</span> curve plots the validation set performance of each checkpoint. This performance is accessible during the AI training and instrumental in selecting the best model checkpoint.
We train a model on the LiTS training set and subsequently test it on the LiTS test set as in-domain evaluation <span id="S5.F2.12.4" class="ltx_text ltx_font_bold">(a)</span> and the FLAREâ€™23 dataset as out-domain evaluation <span id="S5.F2.13.5" class="ltx_text ltx_font_bold">(b)</span>. At the initial epochs, the model performs increasingly well, but its test set performance declines when trained for more epochs, highlighted by the green arrow. This decline is attributed to <span id="S5.F2.14.6" class="ltx_text ltx_font_italic">overfitting</span>, where the model becomes too specialized on the training set and loses its ability to generalize effectively to the test set. The purpose of a validation set is to select the best model checkpoint that is expected to perform well on unseen data. However, in practice, the size and diversity of the validation set may be limited, leading to potential inaccuracies in checkpoint selection. This is evidenced by both in-domain (a) and out-domain (b) evaluations. The dots on the curve represent the best checkpoint selected by the test set (<span id="S5.F2.15.7" class="ltx_text" style="color:#3CBA54;">green</span>) or the real-tumor validation (<span id="S5.F2.16.8" class="ltx_text" style="color:#999999;">gray</span>). A comparative analysis reveals that the checkpoints selected based on the real-tumor validation set might not be the most suitable for test sets.</figcaption>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Overfitting is Attributed to Small-Scale, Real-Tumor Validation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">To demonstrate the potential limitations of selecting the best model checkpoint based on a small-scale and biased real-tumor validation set, we evaluate all the model checkpoints in real-tumor validation set (cohort 2), in-domain LiTS test set (cohort 3) and out-domain FLAREâ€™23 test set (cohort 7). In-domain test set (cohort 3) assesses the performance of each checkpoint and aids in determining the effectiveness of the selected best checkpoints using the validation set (cohort 2). Out-domain test set (cohort 7) serves as a robust benchmark, providing an enhanced evaluation of the performance of the model checkpoints on out-domain unseen data.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">As shown in FigureÂ <a href="#S5.F2" title="Figure 2 â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, two significant observations can be made. Firstly, the best checkpoint identified by the small-scale real-tumor validation set exhibits considerable instability, with notable variations observed when different validation samples are chosen. This result indicates that the small-scale real-tumor validation is inherently biased and lacks the ability to adequately represent the broader range of cases. Secondly, the performance of the best checkpoint determined by the real validation set does not effectively generalize to unseen test data, particularly when confronted with out-domain data. These observations indicate that overfitting can be attributed to a small-scale, biased real-tumor validation set.</p>
</div>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2310.16052/assets/fig_synt_validation.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S5.F3.6.1" class="ltx_text ltx_font_bold">The overfitting is alleviated by a large-scale, synthetic-tumor validation set.</span> Similar to FigureÂ <a href="#S5.F2" title="Figure 2 â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <span id="S5.F3.7.2" class="ltx_text ltx_font_bold">(a)</span> and <span id="S5.F3.8.3" class="ltx_text ltx_font_bold">(b)</span> denote in-domain and out-domain evaluations, and the <span id="S5.F3.9.4" class="ltx_text" style="color:#3CBA54;">green</span> curves are the test set performance on these two datasetsâ€”serving as the gold standard for checkpoint selection. The <span id="S5.F3.10.5" class="ltx_text" style="color:#E3242B;">red</span> curves are the validation performance using synthetic tumors (cohort 5). In theory, we can generate an unlimited number of tumors in varied conditions, e.g., size, location, shape, and texture, as needed, using the tumor generator described in Â§<a href="#S3.SS2" title="3.2 Modeling-Based Synthetic Tumor Generation â€£ 3 Method &amp; Material â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. Such extensive coverage enhances the ability of the validation set to estimate how well the model can be generalized to previously unseen data distributions. As shown in both in-domain and out-domain evaluation, synthetic data as validation can accurately select the best model checkpoint that is almost identical to that selected by test sets.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The overfitting can be alleviated by a diverse, large-scale synthetic-tumor validation set. We conducted a similar experiment to Â§Â <a href="#S5.SS1" title="5.1 Overfitting is Attributed to Small-Scale, Real-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.1</span></a>. This experiment involved evaluating all the model checkpoints in synthetic-tumor validation set (cohort 5), in-domain test set (cohort 3) and out-domain test set (cohort 7).</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">The evaluation trajectory can be observed in FigureÂ <a href="#S5.F3" title="Figure 3 â€£ 5.1 Overfitting is Attributed to Small-Scale, Real-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, with the synthetic-tumor validation set represented by the red line. It is clear that the best checkpoint selected using the synthetic-tumor validation set performs much better than the best checkpoint chosen using the real-tumor validation set when tested with unseen data. Notably, this improvement is especially remarkable when dealing with out-domain data (cohort 7), as the selected model checkpoint is identical to the one chosen by the out-domain test set. These findings emphasize the effectiveness of synthetic-tumor validation set, which serves as a superior alternative to mitigate overfitting issues.</p>
</div>
<figure id="S5.F4" class="ltx_figure">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.8.1" class="ltx_text ltx_font_bold">The overfitting can be addressed by continual learning on synthetic data.</span> We set up the continual learning framework for liver tumor segmentation, described in Â§<a href="#S3.SS1" title="3.1 Continual Learning for Tumor Segmentation â€£ 3 Method &amp; Material â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Its learning curves are presented in <span id="S5.F4.9.2" class="ltx_text" style="color:#E3242B;">red</span>, referred to as <span id="S5.F4.10.3" class="ltx_text ltx_font_italic">dynamic training</span>. In comparison, <span id="S5.F4.11.4" class="ltx_text" style="color:#999999;">gray</span> curves are the conventional framework training with a limited number of real data, referred to as <span id="S5.F4.12.5" class="ltx_text ltx_font_italic">static training</span>. Both in-domain <span id="S5.F4.13.6" class="ltx_text ltx_font_bold">(a)</span> and out-domain <span id="S5.F4.14.7" class="ltx_text ltx_font_bold">(b)</span> evaluations show that the AI model continuously trained on synthetic data outperforms the one trained on real data.</figcaption><img src="/html/2310.16052/assets/fig_real_synt_training.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="167" alt="Refer to caption">
</figure>
<figure id="S5.T2" class="ltx_table">
<table id="S5.T2.4" class="ltx_tabular ltx_align_middle">
<tr id="S5.T2.4.5" class="ltx_tr">
<td id="S5.T2.4.5.1" class="ltx_td ltx_align_top ltx_border_r ltx_border_tt"></td>
<td id="S5.T2.4.5.2" class="ltx_td ltx_align_center ltx_align_top ltx_border_r ltx_border_tt" colspan="2">evaluated on real data</td>
<td id="S5.T2.4.5.3" class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2">evaluated on synthetic data</td>
</tr>
<tr id="S5.T2.4.4" class="ltx_tr">
<td id="S5.T2.4.4.5" class="ltx_td ltx_align_top ltx_border_r"></td>
<td id="S5.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.1.1.1.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.1.1.1.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.1.1.1.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.m1.1c">@</annotation></semantics></math> real</span>
</span>
</td>
<td id="S5.T2.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.2.2.2.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.2.2.2.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.2.2.2.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.2.2.2.1.1.m1.1.1" xref="S5.T2.2.2.2.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.1.1.m1.1b"><ci id="S5.T2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T2.2.2.2.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.1.1.m1.1c">@</annotation></semantics></math> synt</span>
</span>
</td>
<td id="S5.T2.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.3.3.3.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.3.3.3.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.3.3.3.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.3.3.3.1.1.m1.1.1" xref="S5.T2.3.3.3.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.1.1.m1.1b"><ci id="S5.T2.3.3.3.1.1.m1.1.1.cmml" xref="S5.T2.3.3.3.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.1.1.m1.1c">@</annotation></semantics></math> real</span>
</span>
</td>
<td id="S5.T2.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.4.4.1.1" class="ltx_p" style="width:78.0pt;">train <math id="S5.T2.4.4.4.1.1.m1.1" class="ltx_centering" alttext="@" display="inline"><semantics id="S5.T2.4.4.4.1.1.m1.1a"><mi mathvariant="normal" id="S5.T2.4.4.4.1.1.m1.1.1" xref="S5.T2.4.4.4.1.1.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.1.1.m1.1b"><ci id="S5.T2.4.4.4.1.1.m1.1.1.cmml" xref="S5.T2.4.4.4.1.1.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.1.1.m1.1c">@</annotation></semantics></math> synt</span>
</span>
</td>
</tr>
<tr id="S5.T2.4.6" class="ltx_tr">
<td id="S5.T2.4.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.4.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 3</span>
</span>
</td>
<td id="S5.T2.4.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.2.1.1" class="ltx_p" style="width:78.0pt;">26.7 (22.6-30.9)</span>
</span>
</td>
<td id="S5.T2.4.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S5.T2.4.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.3.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.6.3.1.1.1" class="ltx_text ltx_font_bold">33.4 (28.7-38.0)</span></span>
</span>
</td>
<td id="S5.T2.4.6.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.4.1.1" class="ltx_p" style="width:78.0pt;">27.0 (23.7-30.3)</span>
</span>
</td>
<td id="S5.T2.4.6.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S5.T2.4.6.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.6.5.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.6.5.1.1.1" class="ltx_text ltx_font_bold">34.5 (30.8-38.2)</span></span>
</span>
</td>
</tr>
<tr id="S5.T2.4.7" class="ltx_tr">
<td id="S5.T2.4.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S5.T2.4.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.1.1.1" class="ltx_p" style="width:52.0pt;">cohort 7</span>
</span>
</td>
<td id="S5.T2.4.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.2.1.1" class="ltx_p" style="width:78.0pt;">31.1 (26.0-36.2)</span>
</span>
</td>
<td id="S5.T2.4.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r ltx_border_t">
<span id="S5.T2.4.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.3.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.7.3.1.1.1" class="ltx_text ltx_font_bold">33.3 (30.6-36.0)</span></span>
</span>
</td>
<td id="S5.T2.4.7.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.4.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.4.1.1" class="ltx_p" style="width:78.0pt;">32.0 (28.5-35.5)</span>
</span>
</td>
<td id="S5.T2.4.7.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S5.T2.4.7.5.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T2.4.7.5.1.1" class="ltx_p" style="width:78.0pt;"><span id="S5.T2.4.7.5.1.1.1" class="ltx_text ltx_font_bold">35.4 (32.1-38.7)</span></span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>
<span id="S5.T2.11.1" class="ltx_text ltx_font_bold">
Synthetic data for both training and validation.</span> As shown in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, AI model can be trained on either static real data or dynamic synthetic data. The terms â€œtrain <math id="S5.T2.7.m1.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T2.7.m1.1b"><mi mathvariant="normal" id="S5.T2.7.m1.1.1" xref="S5.T2.7.m1.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.7.m1.1c"><ci id="S5.T2.7.m1.1.1.cmml" xref="S5.T2.7.m1.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.7.m1.1d">@</annotation></semantics></math> realâ€ and â€œtrained <math id="S5.T2.8.m2.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.T2.8.m2.1b"><mi mathvariant="normal" id="S5.T2.8.m2.1.1" xref="S5.T2.8.m2.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.T2.8.m2.1c"><ci id="S5.T2.8.m2.1.1.cmml" xref="S5.T2.8.m2.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.8.m2.1d">@</annotation></semantics></math> syntâ€ denote static training with real data and dynamic training with synthetic data, respectively. We save the model checkpoints at each training epoch and then use a validation set to select the <span id="S5.T2.12.2" class="ltx_text ltx_font_italic">best</span> model. These selected model checkpoints are tested on the in-domain LiTS test set (cohort 3) and out-domain FLAREâ€™23 test set (cohort 7). We report the DSC score (%) and 95% confidence interval achieved on the test set. The result reveals that training and validating AI models with our continual learning framework can significantly improve liver tumor segmentation.
</figcaption>
</figure>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Overfitting Can Be Addressed by Continual Learning on Synthetic Data</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">We have shown the effectiveness of large-scale synthetic-tumor validation set.
Now, we will shift our attention to synthetic data in handling the overfitting problem from a training perspective. For this purpose, we introduce a continual learning framework on synthetic data, detailed in Â§Â <a href="#S3.SS1" title="3.1 Continual Learning for Tumor Segmentation â€£ 3 Method &amp; Material â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The training trajectory of static training on real data and dynamic training on synthetic data is shown in FigureÂ <a href="#S5.F4" title="Figure 4 â€£ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and the liver tumor segmentation results are presented in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Specifically, the AI model trained on static real data demonstrates a DSC score of 26.7% for the in-domain test set (cohort 3) and 31.1% for the out-domain test set (cohort 7). In comparison, the AI model developed using our continual learning framework with synthetic data achieves notably higher DSC scores, reaching 34.5% on cohort 3 and 35.4% on cohort 7, respectively. These results indicate a notable improvement in the synthetic data. Based on these findings, we can confidently assert that incorporating our continual learning framework with synthetic data allows us to effectively address the issue of overfitting, encompassing both the training and validation perspectives.</p>
</div>
<figure id="S5.F5" class="ltx_figure"><img src="/html/2310.16052/assets/fig_small_tumor_detection.png" id="S5.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="178" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span><span id="S5.F5.16.1" class="ltx_text ltx_font_bold">Synthetic data can benefit early cancer detection.</span> We evaluate the efficacy of synthetic data in detecting tiny liver tumors (radius <math id="S5.F5.7.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.F5.7.m1.1b"><mo id="S5.F5.7.m1.1.1" xref="S5.F5.7.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.F5.7.m1.1c"><lt id="S5.F5.7.m1.1.1.cmml" xref="S5.F5.7.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.7.m1.1d">&lt;</annotation></semantics></math> 5mm). Specifically, the AI model developed with our continual learning framework â€“ trained and evaluated with synthetic data â€“ is evaluated on tumor detection. We compare it with AI model developed on static real data. The â€œtrained<math id="S5.F5.8.m2.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.8.m2.1b"><mi mathvariant="normal" id="S5.F5.8.m2.1.1" xref="S5.F5.8.m2.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.8.m2.1c"><ci id="S5.F5.8.m2.1.1.cmml" xref="S5.F5.8.m2.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.8.m2.1d">@</annotation></semantics></math>realâ€œ and â€œtrained<math id="S5.F5.9.m3.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.9.m3.1b"><mi mathvariant="normal" id="S5.F5.9.m3.1.1" xref="S5.F5.9.m3.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.9.m3.1c"><ci id="S5.F5.9.m3.1.1.cmml" xref="S5.F5.9.m3.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.9.m3.1d">@</annotation></semantics></math>syntâ€œ hold the same meaning as described in TableÂ <a href="#S5.T2" title="Table 2 â€£ 5.2 Overfitting is Alleviated by Large-Scale, Synthetic-Tumor Validation â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The â€œbest<math id="S5.F5.10.m4.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.10.m4.1b"><mi mathvariant="normal" id="S5.F5.10.m4.1.1" xref="S5.F5.10.m4.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.10.m4.1c"><ci id="S5.F5.10.m4.1.1.cmml" xref="S5.F5.10.m4.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.10.m4.1d">@</annotation></semantics></math>realâ€œ and â€œbest<math id="S5.F5.11.m5.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S5.F5.11.m5.1b"><mi mathvariant="normal" id="S5.F5.11.m5.1.1" xref="S5.F5.11.m5.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S5.F5.11.m5.1c"><ci id="S5.F5.11.m5.1.1.cmml" xref="S5.F5.11.m5.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.11.m5.1d">@</annotation></semantics></math>syntâ€œ denote the selection of best checkpoints based on real- and synthetic- tumor validation sets, respectively. We report the sensitivities (%) achieved on the test set. As shown in both in-domain <span id="S5.F5.17.2" class="ltx_text ltx_font_bold">(a)</span> and out-domain <span id="S5.F5.18.3" class="ltx_text ltx_font_bold">(b)</span> evaluations, our continual learning framework with synthetic data proves to be effective in detecting tiny liver tumors (radius <math id="S5.F5.12.m6.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.F5.12.m6.1b"><mo id="S5.F5.12.m6.1.1" xref="S5.F5.12.m6.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.F5.12.m6.1c"><lt id="S5.F5.12.m6.1.1.cmml" xref="S5.F5.12.m6.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.F5.12.m6.1d">&lt;</annotation></semantics></math> 5mm), thereby benefiting early cancer detection.</figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Synthetic Data Can Benefit Early Cancer Detection</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">Early tumor (radius <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mo id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><lt id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">&lt;</annotation></semantics></math> 5mm) detection plays a critical role in clinical applications, providing valuable information for early cancer diagnosis. Acquiring real data of such a small size is challenging, often posing difficulties or even making it impossible to acquire them. However, our strategy can dynamically generate numerous tiny tumors as required. As a result, the AI model developed within the continual learning framework yields a significant improvement in detecting tiny liver tumors.
The improvement can be found in FigureÂ <a href="#S5.F5" title="Figure 5 â€£ 5.3 Overfitting Can Be Addressed by Continual Learning on Synthetic Data â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We assessed the sensitivity of the AI model under different settings. The performance of the AI model trained and validated on the static real data is 33.1% for the in-domain test set (cohort 3) and 33.9% for the out-domain test set (cohort 7). Comparatively, the AI model developed using our continual learning framework on synthetic data gives a sensitivity of 55.4% for cohort 3 and 52.3% for cohort 7. These results prove the effectiveness of our framework in early detection of cancer.</p>
</div>
<figure id="S5.F6" class="ltx_figure"><img src="/html/2310.16052/assets/fig_indomain_synt_validation.png" id="S5.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="123" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span><span id="S5.F6.5.1" class="ltx_text ltx_font_bold">The continual learning framework is enhanced by in-domain synthetic-tumor validation.</span> Our framework enables the utilization of healthy cases from various domains. Specifically, an interesting situation arises where we can directly create synthetic-tumor validation using healthy cases from the same domain as the test set. This in-domain synthetic-tumor validation set can benefit our continual learning framework. To illustrate this advantage, we utilize the FLAREâ€™23 dataset, which contains both disease cases and healthy cases. We trained the model using dynamic synthetic data and saved model checkpoints at each training epoch. Subsequently, we evaluated them on three datasets, i.e., out-domain synthetic-tumor validation set from assembly dataset (cohort 5, <span id="S5.F6.6.2" class="ltx_text" style="color:#808080;">gray</span> curve), in-domain synthetic-tumor validation set from FLAREâ€™23 dataset (cohort 6, <span id="S5.F6.7.3" class="ltx_text" style="color:#E3242B;">red</span> curve), and FLAREâ€™23 test set served as the gold standard (cohort 7, <span id="S5.F6.8.4" class="ltx_text" style="color:#3CBA54;">green</span> curve). As shown, the in-domain synthetic-tumor validation set accurately identifies the best model, which aligns with the model selected by the FLAREâ€™23 test set.</figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Continual Learning Framework is Enhanced by In-domain Synthetic-Tumor Validation</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">We have demonstrated the effectiveness of our continual learning framework. Moving forward, letâ€™s consider the framework itself. Synthetic data offers a significant advantage as we can utilize healthy CT volumes from various domains. A notable scenario arises wherein we can directly generate synthetic-tumor validation using healthy cases from the same domain as the test set, providing valuable insights for our continual learning framework. As shown in FigureÂ <a href="#S5.F6" title="Figure 6 â€£ 5.4 Synthetic Data Can Benefit Early Cancer Detection â€£ 5 Result â€£ Synthetic Data as Validation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the in-domain synthetic-tumor validation set showcases its capability to accurately identify the best model, which aligns with the model selected by the test set. This result highlights that the continual learning framework yields more favorable outcomes when we can generate in-domain synthetic-tumor validation.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Data synthesis strategies continue to pique the interest of researchers and practitioners, propelling ongoing investigations within this field. This paper justifies the potential and stresses the necessity of leveraging synthetic data as validation to select the best model checkpoint along the training trajectory. Moreover, by employing a continual learning framework on synthetic data, we realize a marked improvement in liver tumor segmentation as well as in the early detection of cancerous tumors compared with the static training on real data, where procuring ample annotated examples can be cost-prohibitive. It is particularly valuable in scenarios characterized by limited annotated data. In the future, we plan to improve the generation of synthetic tumors and verify our findings across different organs, such as the pancreas, kidneys, and stomach.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">This work was supported by the Lustgarten Foundation for Pancreatic Cancer Research and the Patrick J. McGovern Foundation Award. We appreciate the effort of the MONAI Team to provide open-source code for the community. This work has partially utilized the GPUs provided by ASU Research Computing and NVIDIA. We thank Yu-Cheng Chou, Junfei Xiao, Xiaoxi Chen, and Bowen Li for their constructive suggestions at several stages of the project.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azizi etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and DavidÂ J Fleet. 2023.

</span>
<span class="ltx_bibblock">Synthetic data from diffusion models improves imagenet classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.08466</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilic etÂ al. (2019)</span>
<span class="ltx_bibblock">
Patrick Bilic, PatrickÂ Ferdinand Christ, Eugene Vorontsov, Grzegorz Chlebus, Hao Chen, QiÂ Dou, Chi-Wing Fu, Xiao Han, Pheng-Ann Heng, JÃ¼rgen Hesser, etÂ al. 2019.

</span>
<span class="ltx_bibblock">The liver tumor segmentation benchmark (lits).

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.04056</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black etÂ al. (2023)</span>
<span class="ltx_bibblock">
MichaelÂ J Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. 2023.

</span>
<span class="ltx_bibblock">Bedlam: A synthetic dataset of bodies exhibiting detailed lifelike animated motion.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 8726â€“8737.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 33:1877â€“1901.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Burg etÂ al. (2023)</span>
<span class="ltx_bibblock">
MaxÂ F Burg, Florian Wenzel, Dominik Zietlow, Max Horn, Osama Makansi, Francesco Locatello, and Chris Russell. 2023.

</span>
<span class="ltx_bibblock">A data augmentation perspective on diffusion models and retrieval.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10253</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
RichardÂ J Chen, MingÂ Y Lu, TiffanyÂ Y Chen, DrewÂ FK Williamson, and Faisal Mahmood. 2021.

</span>
<span class="ltx_bibblock">Synthetic data in machine learning for medicine and healthcare.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Nature Biomedical Engineering</em>, 5(6):493â€“497.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yuhua Chen, Wen Li, Xiaoran Chen, and LucÂ Van Gool. 2019.

</span>
<span class="ltx_bibblock">Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 1841â€“1850.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chernyak etÂ al. (2018)</span>
<span class="ltx_bibblock">
Victoria Chernyak, KathrynÂ J Fowler, Aya Kamaya, AniaÂ Z Kielar, KhaledÂ M Elsayes, MustafaÂ R Bashir, Yuko Kono, RichardÂ K Do, DonaldÂ G Mitchell, AmitÂ G Singal, AnÂ Tang, and ClaudeÂ B Sirlin. 2018.

</span>
<span class="ltx_bibblock">Liver imaging reporting and data system (LI-RADS) version 2018: Imaging of hepatocellular carcinoma in at-risk patients.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Radiology</em>, 289(3):816â€“830.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Collobert and Weston (2008)</span>
<span class="ltx_bibblock">
Ronan Collobert and Jason Weston. 2008.

</span>
<span class="ltx_bibblock">A unified architecture for natural language processing: Deep neural networks with multitask learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 25th international conference on Machine learning</em>, pages 160â€“167.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crosby etÂ al. (2022)</span>
<span class="ltx_bibblock">
David Crosby, Sangeeta Bhatia, KevinÂ M Brindle, LisaÂ M Coussens, Caroline Dive, Mark Emberton, Sadik Esener, RebeccaÂ C Fitzgerald, SanjivÂ S Gambhir, Peter Kuhn, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Early detection of cancer.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Science</em>, 375(6586):eaay9040.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Cong Gao, BenjaminÂ D Killeen, Yicheng Hu, RobertÂ B Grupp, RussellÂ H Taylor, Mehran Armand, and Mathias Unberath. 2023.

</span>
<span class="ltx_bibblock">Synthetic data accelerates the development of generalizable learning-based algorithms for x-ray image analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 5(3):294â€“308.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gareth etÂ al. (2013)</span>
<span class="ltx_bibblock">
James Gareth, Witten Daniela, Hastie Trevor, and Tibshirani Robert. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">An introduction to statistical learning: with applications in R</em>.

</span>
<span class="ltx_bibblock">Spinger.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glasbey and Mardia (1998)</span>
<span class="ltx_bibblock">
ChrisÂ A Glasbey and KantilalÂ Vardichand Mardia. 1998.

</span>
<span class="ltx_bibblock">A review of image-warping methods.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Journal of applied statistics</em>, 25(2):155â€“171.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonzalez (2009)</span>
<span class="ltx_bibblock">
RafaelÂ C Gonzalez. 2009.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Digital image processing</em>.

</span>
<span class="ltx_bibblock">Pearson education india.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvath etÂ al. (2022)</span>
<span class="ltx_bibblock">
Izabela Horvath, Johannes Paetzold, Oliver Schoppe, Rami Al-Maskari, Ivan Ezhov, Suprosanna Shit, Hongwei Li, Ali ErtÃ¼rk, and Bjoern Menze. 2022.

</span>
<span class="ltx_bibblock">Metgan: Generative tumour inpainting and modality synthesis in light sheet microscopy.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, pages 227â€“237.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Qixin Hu, Yixiong Chen, Junfei Xiao, Shuwen Sun, Jieneng Chen, AlanÂ L Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Label-free liver tumor segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 7422â€“7432.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Qixin Hu, Junfei Xiao, Yixiong Chen, Shuwen Sun, Jie-Neng Chen, Alan Yuille, and Zongwei Zhou. 2022.

</span>
<span class="ltx_bibblock">Synthetic tumors make ai segment tumors better.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">NeurIPS Workshop on Medical Imaging meets NeurIPS</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. 2018.

</span>
<span class="ltx_bibblock">Junction tree variational autoencoder for molecular graph generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages 2323â€“2332. PMLR.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens Van DerÂ Maaten, LiÂ Fei-Fei, CÂ LawrenceÂ Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, pages 2901â€“2910.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon etÂ al. (2022)</span>
<span class="ltx_bibblock">
James Jordon, Lukasz Szpruch, Florimond Houssiau, Mirko Bottarelli, Giovanni Cherubin, Carsten Maple, SamuelÂ N Cohen, and Adrian Weller. 2022.

</span>
<span class="ltx_bibblock">Synthetic dataâ€“what, why and how?

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.03257</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jordon etÂ al. (2018)</span>
<span class="ltx_bibblock">
James Jordon, Jinsung Yoon, and Mihaela Van DerÂ Schaar. 2018.

</span>
<span class="ltx_bibblock">Pate-gan: Generating synthetic data with differential privacy guarantees.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">International conference on learning representations</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Mintong Kang, Bowen Li, Zengle Zhu, Yongyi Lu, ElliotÂ K Fishman, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Label-assemble: Leveraging multiple datasets with partial labels.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)</em>, pages 1â€“5. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuhn etÂ al. (2013)</span>
<span class="ltx_bibblock">
Max Kuhn, Kjell Johnson, etÂ al. 2013.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Applied predictive modeling</em>, volumeÂ 26.

</span>
<span class="ltx_bibblock">Springer.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Landman etÂ al. (2015)</span>
<span class="ltx_bibblock">
BÂ Landman, ZÂ Xu, JÂ Igelsias, MÂ Styner, TÂ Langerak, and AÂ Klein. 2015.

</span>
<span class="ltx_bibblock">2015 miccai multi-atlas labeling beyond the cranial vault workshop and challenge.

</span>
<span class="ltx_bibblock"><a href="doi:10.7303/syn3193805" title="" class="ltx_ref ltx_url ltx_font_typewriter">doi:10.7303/syn3193805</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2004)</span>
<span class="ltx_bibblock">
KHY Lee, MEÂ Oâ€™Malley, MAÂ Haider, and AÂ Hanbidge. 2004.

</span>
<span class="ltx_bibblock">Triple-phase mdct of hepatocellular carcinoma.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">American Journal of Roentgenology</em>, 182(3):643â€“649.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bowen Li, Yu-Cheng Chou, Shuwen Sun, Hualin Qiao, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Early detection and localization of pancreatic cancer by label-free tumor synthesis.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">MICCAI Workshop on Big Task Small Data, 1001-AI</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Cong Liu, CaseyÂ N Ta, JimÂ M Havrilla, JordanÂ G Nestor, MatthewÂ E Spotnitz, AndrewÂ S Geneslaw, YuÂ Hu, WendyÂ K Chung, Kai Wang, and Chunhua Weng. 2022.

</span>
<span class="ltx_bibblock">Oard: Open annotations for rare diseases and their phenotypes based on real-world data.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">The American Journal of Human Genetics</em>, 109(9):1591â€“1604.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett AÂ Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou. 2023a.

</span>
<span class="ltx_bibblock">Clip-driven universal model for organ segmentation and tumor detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, pages 21152â€“21164.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Qihao Liu, Adam Kortylewski, and AlanÂ L Yuille. 2023b.

</span>
<span class="ltx_bibblock">Poseexaminer: Automated testing of out-of-distribution robustness in human pose and shape estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 672â€“681.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luzi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Lorenzo Luzi, Ali Siahkoohi, PaulÂ M Mayer, Josue Casco-Rodriguez, and Richard Baraniuk. 2022.

</span>
<span class="ltx_bibblock">Boomerang: Local sampling on image manifolds using diffusion models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.12100</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Fei Lyu, Mang Ye, JonathanÂ Frederik Carlsen, Kenny Erleben, Sune Darkner, and PongÂ C Yuen. 2022.

</span>
<span class="ltx_bibblock">Pseudo-label guided image synthesis for semi-supervised covid-19 pneumonia infection segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jun Ma, Yao Zhang, Song Gu, Xingle An, Zhihe Wang, Cheng Ge, Congcong Wang, Fan Zhang, YuÂ Wang, Yinan Xu, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Fast and low-gpu-memory abdomen ct organ segmentation: the flare challenge.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 82:102616.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jiteng Mu, Weichao Qiu, GregoryÂ D Hager, and AlanÂ L Yuille. 2020.

</span>
<span class="ltx_bibblock">Learning from synthetic animals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 12386â€“12395.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord etÂ al. (2016)</span>
<span class="ltx_bibblock">
Aaron vanÂ den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016.

</span>
<span class="ltx_bibblock">Wavenet: A generative model for raw audio.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.03499</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Purushwalkam etÂ al. (2022)</span>
<span class="ltx_bibblock">
Senthil Purushwalkam, Pedro Morgado, and Abhinav Gupta. 2022.

</span>
<span class="ltx_bibblock">The challenges of continuous self-supervised learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>, pages 702â€“721. Springer.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Jie Liu, Yucheng Tang, Alan Yuille, and Zongwei Zhou. 2023.

</span>
<span class="ltx_bibblock">Abdomenatlas-8k: Annotating 8,000 abdominal ct volumes for multi-organ segmentation in three weeks.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh etÂ al. (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2102.12092</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ripley (2007)</span>
<span class="ltx_bibblock">
BrianÂ D Ripley. 2007.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Pattern recognition and neural networks</em>.

</span>
<span class="ltx_bibblock">Cambridge university press.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ronneberger etÂ al. (2015)</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical Image Computing and Computer-Assisted Intervention</em>, pages 234â€“241. Springer.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth etÂ al. (2016)</span>
<span class="ltx_bibblock">
Holger Roth, Amal Farag, EvrimÂ B. Turkbey, LeÂ Lu, Jiamin Liu, and RonaldÂ M. Summers. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.7937/K9/TCIA.2016.TNB1KQBU" title="" class="ltx_ref ltx_href">Data from pancreas-ct</a>.

</span>
<span class="ltx_bibblock">The Cancer Imaging Archive. <a target="_blank" href="https://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.7937/K9/TCIA.2016.tNB1kqBU</a>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Russell (2010)</span>
<span class="ltx_bibblock">
StuartÂ J Russell. 2010.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence a modern approach</em>.

</span>
<span class="ltx_bibblock">Pearson Education, Inc.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin etÂ al. (2018)</span>
<span class="ltx_bibblock">
Younghak Shin, HeminÂ Ali Qadir, and Ilangko Balasingham. 2018.

</span>
<span class="ltx_bibblock">Abnormal colon polyp image synthesis using conditional adversarial networks for improved detection performance.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 6:56007â€“56017.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yucheng Tang, Dong Yang, Wenqi Li, HolgerÂ R Roth, Bennett Landman, Daguang Xu, Vishwesh Nath, and Ali Hatamizadeh. 2022.

</span>
<span class="ltx_bibblock">Self-supervised pre-training of swin transformers for 3d medical image analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 20730â€“20740.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valindria etÂ al. (2018)</span>
<span class="ltx_bibblock">
VanyaÂ V Valindria, Nick Pawlowski, Martin Rajchl, Ioannis Lavdas, EricÂ O Aboagye, AndreaÂ G Rockall, Daniel Rueckert, and Ben Glocker. 2018.

</span>
<span class="ltx_bibblock">Multi-modal learning from unpaired images: Application to multi-organ segmentation in ct and mri.

</span>
<span class="ltx_bibblock">In <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">2018 IEEE winter conference on applications of computer vision (WACV)</em>, pages 547â€“556. IEEE.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ de Ven and Tolias (2019)</span>
<span class="ltx_bibblock">
GidoÂ M VanÂ de Ven and AndreasÂ S Tolias. 2019.

</span>
<span class="ltx_bibblock">Three scenarios for continual learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.07734</em>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">VanÂ de Ven etÂ al. (2022)</span>
<span class="ltx_bibblock">
GidoÂ M VanÂ de Ven, Tinne Tuytelaars, and AndreasÂ S Tolias. 2022.

</span>
<span class="ltx_bibblock">Three types of incremental learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Nature Machine Intelligence</em>, 4(12):1185â€“1197.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Hualin Wang, Yuhong Zhou, Jiong Zhang, Jianqin Lei, Dongke Sun, Feng Xu, and Xiayu Xu. 2022.

</span>
<span class="ltx_bibblock">Anomaly segmentation in retinal images with poisson-blending data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, page 102534.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiese etÂ al. (2020)</span>
<span class="ltx_bibblock">
Magnus Wiese, Robert Knobloch, Ralf Korn, and Peter Kretschmer. 2020.

</span>
<span class="ltx_bibblock">Quant gans: deep generation of financial time series.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">Quantitative Finance</em>, 20(9):1419â€“1440.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wyatt etÂ al. (2022)</span>
<span class="ltx_bibblock">
Julian Wyatt, Adam Leach, SebastianÂ M Schmon, and ChrisÂ G Willcocks. 2022.

</span>
<span class="ltx_bibblock">Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, pages 650â€“656.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiaodan Xing, Giorgos Papanastasiou, Simon Walsh, and Guang Yang. 2023.

</span>
<span class="ltx_bibblock">Less is more: Unsupervised mask-guided annotated ct image synthesis with minimum manual segmentations.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2021)</span>
<span class="ltx_bibblock">
Qingsong Yao, LiÂ Xiao, Peihang Liu, and SÂ Kevin Zhou. 2021.

</span>
<span class="ltx_bibblock">Label-free segmentation of covid-19 lesions in lung ct.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Medical Imaging</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jinsung Yoon, Daniel Jarrett, and Mihaela VanÂ der Schaar. 2019.

</span>
<span class="ltx_bibblock">Time-series generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Menghan Yu, Sourabh Kulhare, Courosh Mehanian, CharlesÂ B Delahunt, DanielÂ E Shea, Zohreh Laverriere, Ishan Shah, and MatthewÂ P Horning. 2023.

</span>
<span class="ltx_bibblock">How good are synthetic medical images? an empirical study with lung ultrasound.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">International Workshop on Simulation and Synthesis in Medical Imaging</em>, pages 75â€“85. Springer.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chenyu Zheng, Guoqiang Wu, and Chongxuan Li. 2023.

</span>
<span class="ltx_bibblock">Toward understanding generative data augmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.17476</em>.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou (2021)</span>
<span class="ltx_bibblock">
Zongwei Zhou. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">Towards Annotation-Efficient Deep Learning for Computer-Aided Diagnosis</em>.

</span>
<span class="ltx_bibblock">Ph.D. thesis, Arizona State University.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zongwei Zhou, MichaelÂ B Gotway, and Jianming Liang. 2022.

</span>
<span class="ltx_bibblock">Interpreting medical images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">Intelligent Systems in Medicine and Health</em>, pages 343â€“371. Springer.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zengle Zhu, Mintong Kang, Alan Yuille, and Zongwei Zhou. 2022.

</span>
<span class="ltx_bibblock">Assembling and exploiting large-scale existing labels of common thorax diseases for improved covid-19 classification using chest radiographs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Radiological Society of North America (RSNA)</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Distribution of Real Tumors</h2>

<figure id="A1.F7" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_distribution.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="329" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span id="A1.F7.2.1" class="ltx_text ltx_font_bold">Intensity distribution of liver tumors and their healthy counterparts.</span> Our tumor generator, which is based on modeling and medical knowledge, incorporates the distributional characteristics of real tumors. We present the intensity distributions of real tumors obtained from the LiTS dataset. </figcaption>
</figure>
<figure id="A1.F8" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_size_dis.png" id="A1.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="329" height="242" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span><span id="A1.F8.2.1" class="ltx_text ltx_font_bold">Size distribution of liver tumors.</span> We have calculated the size distribution of liver tumors from the LiTS dataset. This tumor size distribution will serve as a guide for determining the sizes of the synthetic tumors we generate.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Shape Examples</h2>

<figure id="A2.F9" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_shape_examples.png" id="A2.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="389" height="619" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span><span id="A2.F9.2.1" class="ltx_text ltx_font_bold">Examples of the generated shape.</span> The tumor generator pipeline enables us to control the size and deformation of the generated tumors. Here, we present some examples of generated shapes under different conditions.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Texture Examples</h2>

<figure id="A3.F10" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appendix_texture_examples.png" id="A3.F10.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="389" height="615" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="A3.F10.2.1" class="ltx_text ltx_font_bold">Examples of the generated texture.</span> Our data synthesis strategy also enables us to generate different textures, as illustrated here for visualization.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Synthetic tumor examples</h2>

<figure id="A4.F11" class="ltx_figure"><img src="/html/2310.16052/assets/fig_appdix_examples.png" id="A4.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span id="A4.F11.2.1" class="ltx_text ltx_font_bold">Visualization of synthetic data.</span> By combining all the pipelines together, we can obtain a wide range of diverse synthetic data for validation and training.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2310.16051" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2310.16052" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2310.16052">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2310.16052" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2310.16053" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 22:07:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
