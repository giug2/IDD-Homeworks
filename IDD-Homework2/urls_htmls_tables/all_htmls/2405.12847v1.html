<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A dataset and Baselines for Measuring and Predicting the Music Piece Memorability</title>
<!--Generated on Fri May 24 17:32:37 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.12847v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S1" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S2" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset construction for Music Memorability</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.SS1" title="In 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Audio Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.SS2" title="In 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>The Music Memory Game</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.SS3" title="In 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Labels and Consistency Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S4" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Music Memorability Prediction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S4.SS1" title="In 4 Music Memorability Prediction ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Learning with Handcrafted Features</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S4.SS2" title="In 4 Music Memorability Prediction ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>End-to-End Deep Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiment Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.SS1" title="In 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Prediction Results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.SS2" title="In 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Ablation Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.SS3" title="In 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Interpretability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S6" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S7" title="In A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgement</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A dataset and Baselines for Measuring and Predicting the Music Piece Memorability</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Nowadays, humans are constantly exposed to music, whether through voluntary streaming services or incidental encounters during commercial breaks. Despite the abundance of music, certain pieces remain more memorable and often gain greater popularity. Inspired by this phenomenon, we focus on measuring and predicting music memorability. To achieve this, we collect a new music piece dataset with reliable memorability labels using a novel interactive experimental procedure. We then train baselines to predict and analyze music memorability, leveraging both interpretable features and audio mel-spectrograms as inputs. To the best of our knowledge, we are the first to explore music memorability using data-driven deep learning-based methods. Through a series of experiments and ablation studies, we demonstrate that while there is room for improvement, predicting music memorability with limited data is possible. Certain intrinsic elements, such as higher valence, arousal, and faster tempo, contribute to memorable music. As prediction techniques continue to evolve, real-life applications like music recommendation systems and music style transfer will undoubtedly benefit from this new area of research.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Music memorability is essential and has a wide range of commercial applications. For instance, content creators and marketing teams can use unique visual aids or audio components to captivate target audiences and distinguish themselves from other information sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib2" title="">2</a>]</cite>. Sound logos, such as Netflix’s iconic “ta-dum,” are designed to engage listeners and promote brand recognition. In the realm of cognition literature, numerous studies have sought to understand the factors that contribute to music memorability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib6" title="">6</a>]</cite>. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib6" title="">6</a>]</cite> bridged the gap between cognitive science and MIR by examining whether implicit or explicit memory for a single tune is impacted by the type of encoding task and variations in timbre, tempo and structure.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, music memorability remains a relatively unexplored area, particularly from a data-driven standpoint. Research related to music memorability includes the study of involuntary musical imagery (INMI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib8" title="">8</a>]</cite>, also known as “earworms,” which refers to the phenomenon where fragments of music become mentally lodged on repeat. For instance, Jakubowski et al. proposed a model that can determine whether a piece of music may induce the INMI effect by using statistical analysis and a random forest model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib8" title="">8</a>]</cite>. However, the mechanism of INMI differs from music memorability since the former is a passive process while the latter can be active, e.g., everyone remembers how to sing “Happy Birthday,” but the song may not qualify as an earworm. Another line of prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib12" title="">12</a>]</cite> investigating the intrinsic memorability of multimedia content have predominantly focused on computer vision, with their findings suggesting that data-driven approaches can effectively determine memorability levels. Motivated by these studies, we break new ground in exploring music memorability from a data-driven perspective by compiling a novel dataset and employing machine learning techniques.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Specifically, to expand the scope of memorability detection and recognition in music information retrieval (MIR), we establish a new research domain called music memorability regression (MMR), which aims to predict a memorability score for a given music piece. We create an experimental procedure as shown in <span class="ltx_text" id="S1.p3.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">1</span></a></span> to collect a new dataset, the YouTube Music Memorability (YTMM) dataset, where memorability scores are determined by the percentage of participants who can recall the music piece after a certain period. This dataset provides reliable and consistent music memorability scores across all participants, paving the way for further research in the field. We also propose several baseline approaches for predicting music memorability, including feature engineering using hand-crafted music-related features and transfer learning techniques. These baselines not only demonstrate the potential of machine learning in addressing music memorability but also serve as a foundation for future work.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<p class="ltx_p ltx_align_center" id="S1.F1.1"><span class="ltx_text" id="S1.F1.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="241" id="S1.F1.1.1.g1" src="extracted/2405.12847v1/procedure.png" width="1196"/></span></p>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S1.F1.3.1.1">Figure 1</span>: </span>The music memory game, which allows data annotators to label music memorability scores reliably. The experiment is divided into three stages, each with a 3-minute long break in between. Each 18-minute stage is composed of multiple 5-second music pieces and short breaks.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite the promise of machine learning in tackling music memorability by predicting memorability scores, its “black box” characteristics hinder the interpretation of machine decisions in MIR tasks. A straightforward approach would be to compute correlations without relying on black-box prediction models to glean insights about the relationship between memorability and musical features. However, given the complexity of analyzing music memorability, using a single feature results in an extremely low correlation with memorability, leading to inconclusive findings. One alternative would be to explore all possible feature combinations when calculating correlations, but the sheer number of combinations, e.g., <math alttext="2^{20}-1" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><msup id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml"><mn id="S1.p4.1.m1.1.1.2.2" xref="S1.p4.1.m1.1.1.2.2.cmml">2</mn><mn id="S1.p4.1.m1.1.1.2.3" xref="S1.p4.1.m1.1.1.2.3.cmml">20</mn></msup><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">−</mo><mn id="S1.p4.1.m1.1.1.3" xref="S1.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><minus id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1"></minus><apply id="S1.p4.1.m1.1.1.2.cmml" xref="S1.p4.1.m1.1.1.2"><csymbol cd="ambiguous" id="S1.p4.1.m1.1.1.2.1.cmml" xref="S1.p4.1.m1.1.1.2">superscript</csymbol><cn id="S1.p4.1.m1.1.1.2.2.cmml" type="integer" xref="S1.p4.1.m1.1.1.2.2">2</cn><cn id="S1.p4.1.m1.1.1.2.3.cmml" type="integer" xref="S1.p4.1.m1.1.1.2.3">20</cn></apply><cn id="S1.p4.1.m1.1.1.3.cmml" type="integer" xref="S1.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">2^{20}-1</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">2 start_POSTSUPERSCRIPT 20 end_POSTSUPERSCRIPT - 1</annotation></semantics></math> for just 20 features, renders this approach impractical. A/B testing could be used to determine which type of music is more memorable, but it suffers from similar drawbacks, such as being time-consuming and unable to account for all variables that may impact the experiment’s outcome. To make machine learning models reveal their “black box” characteristics, researchers are increasingly adopting explainable artificial intelligence (XAI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib13" title="">13</a>]</cite> for deeper insights. Building on previous interpretability analyses in audio processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib15" title="">15</a>]</cite>, we utilize Shapley Additive Explanations (SHAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib16" title="">16</a>]</cite>, a game-theoretic approach that clarifies the output of machine learning models, to identify the key components of memorable music.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our main contributions are as follows: first, we present the new YTMM dataset with objective annotations of memorability scores, which will be publicly available for future research; second, we propose several deep learning baseline models for MMR; and finally, we explore the potential characteristics of memorable music pieces while providing interpretability for these deep learning-based methods.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In addition to the cognition literature on music memorability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib6" title="">6</a>]</cite>, there are several related yet distinct terms, such as Involuntary Musical Imagery (INMI) or "earworms"—fragments of music that involuntarily come to mind <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib7" title="">7</a>]</cite>. Studies have examined earworms through interviews, environmental and psychological conditions leading to INMI, and the impact of melodic features and song popularity on spontaneous musical imagination <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib8" title="">8</a>]</cite>. Crucial differences between INMI and music memorability include: 1) INMI involves uncontrollable mental repetition, while memorability requires conscious recall; and 2) the stimuli in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib8" title="">8</a>]</cite> are highly familiar to participants, whereas our study selects audios unfamiliar to most annotators to mitigate the influence of individual listening histories on memorability. Another related concept is hook catchiness <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib20" title="">20</a>]</cite>, which refers to the most easily recalled fragment of a musical piece. However, our focus lies in predicting the memorability of different music pieces rather than assessing the impact of various segments within the same tune on catchiness prediction and recognition. Furthermore, we ensure our stimuli consist solely of pure instrumental music clips to prevent any textual information from lyrics influencing music memorability.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Moreover, while deep learning has achieved significant success in supervised MIR tasks, it often demands large-scale annotated data. However, collecting useful annotations for MIR tasks can be costly, as it typically requires expertise and domain knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib21" title="">21</a>]</cite>. To tackle this challenge, various data augmentation and training strategies have been proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib24" title="">24</a>]</cite>. For instance, McFee et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib21" title="">21</a>]</cite> apply transformations such as pitch shifting, time-stretching, and adding background noise to the original waveform. Cubuk et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib22" title="">22</a>]</cite> mask both time and frequency content to expand the input space in automatic speech recognition (ASR) and MIR tasks. To enhance learning robustness with limited data, Wu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib23" title="">23</a>]</cite> extract general music representations using a multi-task pre-trained encoder, inspired by speech processing research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib26" title="">26</a>]</cite>. Similarly, Castellon et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib24" title="">24</a>]</cite> employ transfer learning from existing music generation architectures. However, not all the aforementioned methods are simultaneously open-source, computationally inexpensive, and interpretable. Therefore, in this paper, we focus on applying signal processing approaches like masking, with further details provided in Section 4.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.1">Task Type</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.2"># of Audios</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.3"># of Repetition (min)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.4"># of Repetition (max)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.5"># of Repetition (avg)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S2.T1.2.1.1.6"># of Repetition (std)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.1">Filler</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.2">65</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.3">-</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.2.2.1.6">-</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.1">Vigilance</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.2">21</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.3">5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.4">10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.5">6.5</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.3.2.6">1.08</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.1">Short-Term Target</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.2">88</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.3">10</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.4">49</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.5">25.23</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.4.3.6">10.81</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.1">Medium-Term Target</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.2">41</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.3">61</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.4">131</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.5">110.33</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.2.5.4.6">16.32</td>
</tr>
<tr class="ltx_tr" id="S2.T1.2.6.5">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.1">Long-Term Target</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.2">20</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.3">155</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.4">276</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.5">222.05</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S2.T1.2.6.5.6">36.64</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S2.T1.3.1.1">Table 1</span>: </span>Details of different audio tasks in the music memory game.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset construction for Music Memorability</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we discuss the details of our dataset collection process and how music memorability is measured.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Audio Collection</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To construct a dataset with objective music memorability scores, we first ensure that the audio samples are unbiased. We randomly select music by querying music-related videos using the YouTube API with random query keys, avoiding any specific music genre preference. Additionally, we manually filter the music to confirm that the queried videos contain pure music content, excluding instrument tutorials or gadget unboxings. Next, we conduct a pilot study to verify that the selected audios are unfamiliar to most of the annotators in our target user group. Considering the annotators’ nationalities might not be as varied as the music collection’s, and language can be a memorable yet non-music-related element, we only use the intro part of each song. This approach helps eliminate other potential variables affecting music memorability. Also, the volume across all audio clips is normalized to minimize any memorable attributes unrelated to the music itself. Loudness normalization ensures the music is remembered based on its inherent qualities rather than its loudness.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We use only a segment of each audio for two reasons: i) to better eliminate confounding factors, such as vocal timbre, and ii) to shorten the period of annotations and prevent fatigue. We achieve this by truncating audios into structurally meaningful segments and applying proper time-stretching to alter the duration of an audio signal to a fixed length without distorting the audio. The segmentation process is supervised by an expert with a professional music education background. Note that time-stretching not only reduces modeling complexity but also prevents annotators from memorizing the audio based on its duration.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Ultimately, we collect 235 structurally meaningful 5-second audios with labeled music memorability scores. Our goal is to determine which types of music pieces are more likely to be memorized, rather than focusing on entire music clips, which are more complex and involve additional factors. This research can facilitate various applications, such as Netflix’s iconic “ta-dum” sound. The collected data can be found in the supplementary materials. <span class="ltx_text" id="S3.SS1.p3.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.F2" title="Figure 2 ‣ 3.1 Audio Collection ‣ 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">2</span></a></span> illustrates the distribution of the collected audios concerning their published geographical locations and total views on YouTube, with view counts ranging from 10K to 100M.</p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center" id="S3.F2.1"><span class="ltx_text" id="S3.F2.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="188" id="S3.F2.1.1.g1" src="extracted/2405.12847v1/dist1.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F2.3.1.1">Figure 2</span>: </span>Distributions of the audio published location and the distributions of the audio views in the final dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The Music Memory Game</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.6">To annotate the memorability of the collected musical data, we follow the setting of image memory game <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib9" title="">9</a>]</cite> to design a novel music listening experiment. During the experiment, the recruited data annotators are asked to listen to 235 music pieces in total and answer whether the audio is repeated in the experiment or not. From a cognitive view, we define music memorability as long-term musical salience and the extent to which a musical piece continues to be remembered over time. In the music memory game, music memorability is measured as the tendency to correctly recognize a music piece when encountering it again in the experiment among all users. Specifically, let <math alttext="x_{j}^{(i)}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><msubsup id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.2.2.2" xref="S3.SS2.p1.1.m1.1.2.2.2.cmml">x</mi><mi id="S3.SS2.p1.1.m1.1.2.2.3" xref="S3.SS2.p1.1.m1.1.2.2.3.cmml">j</mi><mrow id="S3.SS2.p1.1.m1.1.1.1.3" xref="S3.SS2.p1.1.m1.1.2.cmml"><mo id="S3.SS2.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2">superscript</csymbol><apply id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.2.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.2.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2.2">𝑥</ci><ci id="S3.SS2.p1.1.m1.1.2.2.3.cmml" xref="S3.SS2.p1.1.m1.1.2.2.3">𝑗</ci></apply><ci id="S3.SS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">x_{j}^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math> denote whether the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_i</annotation></semantics></math>-th music piece can be recalled by the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mi id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_j</annotation></semantics></math>-th data annotator, <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.6.1">i.e.</span>, 1 if the annotator recognized the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_i</annotation></semantics></math>-th music piece.
The memorability score of music <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mi id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><ci id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_i</annotation></semantics></math>, denoted by <math alttext="m^{(i)}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msup id="S3.SS2.p1.6.m6.1.2" xref="S3.SS2.p1.6.m6.1.2.cmml"><mi id="S3.SS2.p1.6.m6.1.2.2" xref="S3.SS2.p1.6.m6.1.2.2.cmml">m</mi><mrow id="S3.SS2.p1.6.m6.1.1.1.3" xref="S3.SS2.p1.6.m6.1.2.cmml"><mo id="S3.SS2.p1.6.m6.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.6.m6.1.2.cmml">(</mo><mi id="S3.SS2.p1.6.m6.1.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS2.p1.6.m6.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.6.m6.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.2.cmml" xref="S3.SS2.p1.6.m6.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.2.1.cmml" xref="S3.SS2.p1.6.m6.1.2">superscript</csymbol><ci id="S3.SS2.p1.6.m6.1.2.2.cmml" xref="S3.SS2.p1.6.m6.1.2.2">𝑚</ci><ci id="S3.SS2.p1.6.m6.1.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1.1">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">m^{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_m start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT</annotation></semantics></math>, is then calculated by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="m^{(i)}=\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}x_{j}^{(i)},\;x_{j}^{(i)}\in\{0,1\}" class="ltx_Math" display="block" id="S3.E1.m1.7"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.2" xref="S3.E1.m1.7.7.3.cmml"><mrow id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml"><msup id="S3.E1.m1.6.6.1.1.2" xref="S3.E1.m1.6.6.1.1.2.cmml"><mi id="S3.E1.m1.6.6.1.1.2.2" xref="S3.E1.m1.6.6.1.1.2.2.cmml">m</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.6.6.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.6.6.1.1.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.2.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.6.6.1.1.1" xref="S3.E1.m1.6.6.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.6.6.1.1.3" xref="S3.E1.m1.6.6.1.1.3.cmml"><mfrac id="S3.E1.m1.6.6.1.1.3.2" xref="S3.E1.m1.6.6.1.1.3.2.cmml"><mn id="S3.E1.m1.6.6.1.1.3.2.2" xref="S3.E1.m1.6.6.1.1.3.2.2.cmml">1</mn><msub id="S3.E1.m1.6.6.1.1.3.2.3" xref="S3.E1.m1.6.6.1.1.3.2.3.cmml"><mi id="S3.E1.m1.6.6.1.1.3.2.3.2" xref="S3.E1.m1.6.6.1.1.3.2.3.2.cmml">n</mi><mi id="S3.E1.m1.6.6.1.1.3.2.3.3" xref="S3.E1.m1.6.6.1.1.3.2.3.3.cmml">i</mi></msub></mfrac><mo id="S3.E1.m1.6.6.1.1.3.1" xref="S3.E1.m1.6.6.1.1.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.6.6.1.1.3.3" xref="S3.E1.m1.6.6.1.1.3.3.cmml"><munderover id="S3.E1.m1.6.6.1.1.3.3.1" xref="S3.E1.m1.6.6.1.1.3.3.1.cmml"><mo id="S3.E1.m1.6.6.1.1.3.3.1.2.2" movablelimits="false" xref="S3.E1.m1.6.6.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.6.6.1.1.3.3.1.2.3" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.cmml"><mi id="S3.E1.m1.6.6.1.1.3.3.1.2.3.2" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.2.cmml">j</mi><mo id="S3.E1.m1.6.6.1.1.3.3.1.2.3.1" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.6.6.1.1.3.3.1.2.3.3" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><msub id="S3.E1.m1.6.6.1.1.3.3.1.3" xref="S3.E1.m1.6.6.1.1.3.3.1.3.cmml"><mi id="S3.E1.m1.6.6.1.1.3.3.1.3.2" xref="S3.E1.m1.6.6.1.1.3.3.1.3.2.cmml">n</mi><mi id="S3.E1.m1.6.6.1.1.3.3.1.3.3" xref="S3.E1.m1.6.6.1.1.3.3.1.3.3.cmml">i</mi></msub></munderover><msubsup id="S3.E1.m1.6.6.1.1.3.3.2" xref="S3.E1.m1.6.6.1.1.3.3.2.cmml"><mi id="S3.E1.m1.6.6.1.1.3.3.2.2.2" xref="S3.E1.m1.6.6.1.1.3.3.2.2.2.cmml">x</mi><mi id="S3.E1.m1.6.6.1.1.3.3.2.2.3" xref="S3.E1.m1.6.6.1.1.3.3.2.2.3.cmml">j</mi><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.6.6.1.1.3.3.2.cmml"><mo id="S3.E1.m1.2.2.1.3.1" stretchy="false" xref="S3.E1.m1.6.6.1.1.3.3.2.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.1.3.2" stretchy="false" xref="S3.E1.m1.6.6.1.1.3.3.2.cmml">)</mo></mrow></msubsup></mrow></mrow></mrow><mo id="S3.E1.m1.7.7.2.3" rspace="0.447em" xref="S3.E1.m1.7.7.3a.cmml">,</mo><mrow id="S3.E1.m1.7.7.2.2" xref="S3.E1.m1.7.7.2.2.cmml"><msubsup id="S3.E1.m1.7.7.2.2.2" xref="S3.E1.m1.7.7.2.2.2.cmml"><mi id="S3.E1.m1.7.7.2.2.2.2.2" xref="S3.E1.m1.7.7.2.2.2.2.2.cmml">x</mi><mi id="S3.E1.m1.7.7.2.2.2.2.3" xref="S3.E1.m1.7.7.2.2.2.2.3.cmml">j</mi><mrow id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.7.7.2.2.2.cmml"><mo id="S3.E1.m1.3.3.1.3.1" stretchy="false" xref="S3.E1.m1.7.7.2.2.2.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo id="S3.E1.m1.3.3.1.3.2" stretchy="false" xref="S3.E1.m1.7.7.2.2.2.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.7.7.2.2.1" xref="S3.E1.m1.7.7.2.2.1.cmml">∈</mo><mrow id="S3.E1.m1.7.7.2.2.3.2" xref="S3.E1.m1.7.7.2.2.3.1.cmml"><mo id="S3.E1.m1.7.7.2.2.3.2.1" stretchy="false" xref="S3.E1.m1.7.7.2.2.3.1.cmml">{</mo><mn id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml">0</mn><mo id="S3.E1.m1.7.7.2.2.3.2.2" xref="S3.E1.m1.7.7.2.2.3.1.cmml">,</mo><mn id="S3.E1.m1.5.5" xref="S3.E1.m1.5.5.cmml">1</mn><mo id="S3.E1.m1.7.7.2.2.3.2.3" stretchy="false" xref="S3.E1.m1.7.7.2.2.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.3.cmml" xref="S3.E1.m1.7.7.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.3a.cmml" xref="S3.E1.m1.7.7.2.3">formulae-sequence</csymbol><apply id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1.1"><eq id="S3.E1.m1.6.6.1.1.1.cmml" xref="S3.E1.m1.6.6.1.1.1"></eq><apply id="S3.E1.m1.6.6.1.1.2.cmml" xref="S3.E1.m1.6.6.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.2">superscript</csymbol><ci id="S3.E1.m1.6.6.1.1.2.2.cmml" xref="S3.E1.m1.6.6.1.1.2.2">𝑚</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci></apply><apply id="S3.E1.m1.6.6.1.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3"><times id="S3.E1.m1.6.6.1.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.1"></times><apply id="S3.E1.m1.6.6.1.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2"><divide id="S3.E1.m1.6.6.1.1.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2"></divide><cn id="S3.E1.m1.6.6.1.1.3.2.2.cmml" type="integer" xref="S3.E1.m1.6.6.1.1.3.2.2">1</cn><apply id="S3.E1.m1.6.6.1.1.3.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.2.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.2.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3.2">𝑛</ci><ci id="S3.E1.m1.6.6.1.1.3.2.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.2.3.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.6.6.1.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3"><apply id="S3.E1.m1.6.6.1.1.3.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.1.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1">superscript</csymbol><apply id="S3.E1.m1.6.6.1.1.3.3.1.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.1.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1">subscript</csymbol><sum id="S3.E1.m1.6.6.1.1.3.3.1.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.2.2"></sum><apply id="S3.E1.m1.6.6.1.1.3.3.1.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3"><eq id="S3.E1.m1.6.6.1.1.3.3.1.2.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.6.6.1.1.3.3.1.2.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.2">𝑗</ci><cn id="S3.E1.m1.6.6.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.E1.m1.6.6.1.1.3.3.1.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.6.6.1.1.3.3.1.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.1.3.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.3">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.3.1.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.3.2">𝑛</ci><ci id="S3.E1.m1.6.6.1.1.3.3.1.3.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.1.3.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.6.6.1.1.3.3.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2">superscript</csymbol><apply id="S3.E1.m1.6.6.1.1.3.3.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.6.6.1.1.3.3.2.2.1.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.6.6.1.1.3.3.2.2.2.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2.2.2">𝑥</ci><ci id="S3.E1.m1.6.6.1.1.3.3.2.2.3.cmml" xref="S3.E1.m1.6.6.1.1.3.3.2.2.3">𝑗</ci></apply><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">𝑖</ci></apply></apply></apply></apply><apply id="S3.E1.m1.7.7.2.2.cmml" xref="S3.E1.m1.7.7.2.2"><in id="S3.E1.m1.7.7.2.2.1.cmml" xref="S3.E1.m1.7.7.2.2.1"></in><apply id="S3.E1.m1.7.7.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.2.2.2.1.cmml" xref="S3.E1.m1.7.7.2.2.2">superscript</csymbol><apply id="S3.E1.m1.7.7.2.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.2.2.2.2.1.cmml" xref="S3.E1.m1.7.7.2.2.2">subscript</csymbol><ci id="S3.E1.m1.7.7.2.2.2.2.2.cmml" xref="S3.E1.m1.7.7.2.2.2.2.2">𝑥</ci><ci id="S3.E1.m1.7.7.2.2.2.2.3.cmml" xref="S3.E1.m1.7.7.2.2.2.2.3">𝑗</ci></apply><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">𝑖</ci></apply><set id="S3.E1.m1.7.7.2.2.3.1.cmml" xref="S3.E1.m1.7.7.2.2.3.2"><cn id="S3.E1.m1.4.4.cmml" type="integer" xref="S3.E1.m1.4.4">0</cn><cn id="S3.E1.m1.5.5.cmml" type="integer" xref="S3.E1.m1.5.5">1</cn></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">m^{(i)}=\frac{1}{n_{i}}\sum_{j=1}^{n_{i}}x_{j}^{(i)},\;x_{j}^{(i)}\in\{0,1\}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.7d">italic_m start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT , italic_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT ∈ { 0 , 1 }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.8">where <math alttext="n_{i}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m1.1"><semantics id="S3.SS2.p1.7.m1.1a"><msub id="S3.SS2.p1.7.m1.1.1" xref="S3.SS2.p1.7.m1.1.1.cmml"><mi id="S3.SS2.p1.7.m1.1.1.2" xref="S3.SS2.p1.7.m1.1.1.2.cmml">n</mi><mi id="S3.SS2.p1.7.m1.1.1.3" xref="S3.SS2.p1.7.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m1.1b"><apply id="S3.SS2.p1.7.m1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m1.1.1.1.cmml" xref="S3.SS2.p1.7.m1.1.1">subscript</csymbol><ci id="S3.SS2.p1.7.m1.1.1.2.cmml" xref="S3.SS2.p1.7.m1.1.1.2">𝑛</ci><ci id="S3.SS2.p1.7.m1.1.1.3.cmml" xref="S3.SS2.p1.7.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m1.1c">n_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m1.1d">italic_n start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the total number of data annotators for the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m2.1"><semantics id="S3.SS2.p1.8.m2.1a"><mi id="S3.SS2.p1.8.m2.1.1" xref="S3.SS2.p1.8.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m2.1b"><ci id="S3.SS2.p1.8.m2.1.1.cmml" xref="S3.SS2.p1.8.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m2.1d">italic_i</annotation></semantics></math>-th music.
<br class="ltx_break"/></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">To make the ground truth unbeknownst to all participants, music excerpts are split into three task categories: “vigilance”, “target”, and “filler”. Targets and vigilance targets are both repeated in the experiment, while the former are collected to be the true labels and the latter is used to make sure participants are attentive when labeling data. Moreover, fillers are used to stuff the spacing between the first and second repetition of a target and therefore is only presented once. The overview of the music memory game experimenting procedure is shown in <span class="ltx_text" id="S3.SS2.p2.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">1</span></a></span>. The target-vigilance-filler split details can be found in <span class="ltx_text" id="S3.SS2.p2.1.2">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S2.T1" title="Table 1 ‣ 2 Related Work ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">1</span></a></span>. Rigorous criteria are enforced to monitor the performances of data annotators and preserve the quality of memorability labels. Specifically, annotations from users who detect vigilance repetition with an accuracy lower than 60% are automatically discarded. Furthermore, to prevent gathering biased memorability, all annotators only engage in labeling once. We recruited a total of 218 users from campus, with 163 clearing the vigilance accuracy level, 17% of passed annotators having professional music education backgrounds, and over 98% being between the ages of 20 and 29.</p>
</div>
<figure class="ltx_figure" id="S3.F3">
<p class="ltx_p ltx_align_center" id="S3.F3.1"><span class="ltx_text" id="S3.F3.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="S3.F3.1.1.g1" src="extracted/2405.12847v1/exp_stage_corr.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F3.5.1.1">Figure 3</span>: </span>Memorability scores at various stages. The color symbolizes the rank of short-term memorability, while the lines represent stage relationships. The plot also shows Spearman’s rank correlations <math alttext="\rho" class="ltx_Math" display="inline" id="S3.F3.3.m1.1"><semantics id="S3.F3.3.m1.1b"><mi id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><ci id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S3.F3.3.m1.1e">italic_ρ</annotation></semantics></math> between memorabilities measured at each stage.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Differing from previous works on image memorability, our experiment is composed of three similar stages with breaks inserted in between. The reasons for using stages and breaks are two-fold. First, audios are sequential, therefore it is more exhausting to label the memorability score to audios as compared to static images. Second, it usually takes some time for the earworm phenomenon to happen when listening to music. Hence, we assume memorability should be invariant even after encountering breaks that probably would reset the memory. The results of relations between repeat interval and memorability score are shown in <span class="ltx_text" id="S3.SS2.p3.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.F3" title="Figure 3 ‣ 3.2 The Music Memory Game ‣ 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">3</span></a></span>, where the lines exhibiting memorability scores across short-term, medium-term, and long-term repeats. The results manifest that the memorability score is indeed independent of the sequential context. Therefore, it is easier to memorize truly memorable pieces of music even after long breaks. The fact that Spearman’s rank correlation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib27" title="">27</a>]</cite> between short-term, medium-term, and long-term are all greater than 0.64 also proves that the rank of memorability score is preserved across variant repeat intervals.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Labels and Consistency Analysis</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">To assure that collected labels are universal across all data annotators, we evaluate the human consistency according to previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib9" title="">9</a>]</cite> by randomly splitting all participants into 2 groups and examining how well the memorability scores measured in the first groups matched the ones measured in the second group by averaging Spearman’s rank correlation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib27" title="">27</a>]</cite> between randomly separated two halves of the participants 25 times. The average Spearman’s rank correlation coefficient <math alttext="\rho" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_ρ</annotation></semantics></math> is <math alttext="0.83" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">0.83</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn id="S3.SS3.p1.2.m2.1.1.cmml" type="float" xref="S3.SS3.p1.2.m2.1.1">0.83</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">0.83</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">0.83</annotation></semantics></math>, indicating the consistency of the collected data.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<p class="ltx_p ltx_align_center" id="S3.F4.1"><span class="ltx_text" id="S3.F4.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S3.F4.1.1.g1" src="extracted/2405.12847v1/macro_score_log_repeat_with_fatigue_plot.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F4.3.1.1">Figure 4</span>: </span>Relations between memorability score and target repeat interval in log scale. The hue represents the level of fatigue.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text" id="S3.SS3.p2.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S3.F4" title="Figure 4 ‣ 3.3 Labels and Consistency Analysis ‣ 3 Dataset construction for Music Memorability ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">4</span></a></span> shows the scattering plot of music memorability and repeat interval. The graph demonstrates that music possesses a linear relation between memorability score and log-scaled repeat interval. Please note that the fatigue level is another factor in the plot that also contributes to the memorability score of audio.
The fatigue level, defined as the amount of audios listened without a 3-minute break, is a direct result caused by staging experiment and participating in taking a break in the middle since listening to more music at one time without resting reduces participants’ ability to identify repeated music pieces. The setting of inserting audio to random positions in the experiment procedure adds more context diversity to the process of memorizing music, thus making the labeled memorability scores more robust.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Music Memorability Prediction</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Learning with Handcrafted Features</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Although feature extractions for deep learning models can be data-driven without being handcrafted, leading to a better result given sufficient training data, handcrafted features provide interpretable information for more insights. Therefore, we propose handcrafted features that can more accurately depict the low-level acoustic features or high-level semantic features of musical clips as shown in <span class="ltx_text" id="S4.SS1.p1.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S4.T2" title="Table 2 ‣ 4.1 Learning with Handcrafted Features ‣ 4 Music Memorability Prediction ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">2</span></a></span>. For the low-level acoustic features that can be directly derived from the audio signal of music segments, we leverage the harmony, rhythm and timbre since they are most easily recognizable fragments of a piece of music <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib17" title="">17</a>]</cite> and describe the fundamental elements of a tune. Moreover, zero crossings and zero crossing rate are also extracted since they give the impressions into the frequency content of a signal. On the other hand, high-level semantic features are more abstract descriptions. Since the previous works in Psychology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib29" title="">29</a>]</cite> mention the link between music emotion and memory, we introduce valence and arousal, which represent the mood of music pieces as features.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Another high-level feature is genre, which describes how likely a clip is belong to a certain type of music. Specifically, due to the unstable performance of existing algorithms for detecting sequences of chord labels, we employ chromagram (chroma) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib30" title="">30</a>]</cite> as a representation of harmony patterns. To extract timbre information, the Mel-Frequency Cepstral Coefficient (MFCCs) is widely utilized. Although MFCCs is representative for timbre, its components are difficult to grasp intuitively. As a result, we treat MFCCs as a raw feature and find an alternative solution by first separating source audios into four components using source separation software Spleeter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib31" title="">31</a>]</cite>, and calculating their respective amplitudes to represent the characteristics of different instruments and frequency ranges. For the rhythmic pattern, although Tempogram <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib32" title="">32</a>]</cite> captures the underlying rhythmic pattern of raw audios, it is unable to provide precise insights to concretely measure the audio’s groove. Therefore, we instead utilize beat per minute (bpm) to represent general rhythm characteristics. We also use static valence and arousal values to describe perceived music moods, which are predicted by using Support Vector Regression (SVR) with a linear kernel trained on the PMEmo dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib33" title="">33</a>]</cite>. For genre features, we use the predicted music tagging and instruments from the downstream task of PANN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib34" title="">34</a>]</cite>. Finally, SVR and Multilayer Perceptron (MLP) are employed as predictors to link audio features to memorability scores.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.2.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.1.1">
<span class="ltx_p" id="S4.T2.2.1.1.1.1.1" style="width:39.2pt;">Level</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.2.1">
<span class="ltx_p" id="S4.T2.2.1.1.2.1.1" style="width:53.8pt;">Category</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S4.T2.2.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.1.1.3.1">
<span class="ltx_p" id="S4.T2.2.1.1.3.1.1" style="width:107.7pt;">Feature Implementation</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.2.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.2.1.1" rowspan="4">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.1.1">
<span class="ltx_p" id="S4.T2.2.2.1.1.1.1" style="width:39.2pt;"><span class="ltx_text" id="S4.T2.2.2.1.1.1.1.1">Low-level</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.2.1">
<span class="ltx_p" id="S4.T2.2.2.1.2.1.1" style="width:53.8pt;">Harmony</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T2.2.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.2.1.3.1">
<span class="ltx_p" id="S4.T2.2.2.1.3.1.1" style="width:107.7pt;">mean, std of 12 pitch class</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.2.1.1">
<span class="ltx_p" id="S4.T2.2.3.2.1.1.1" style="width:53.8pt;">Rhythm</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.3.2.2.1">
<span class="ltx_p" id="S4.T2.2.3.2.2.1.1" style="width:107.7pt;">beat per minute (bpm)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.3.1.1">
<span class="ltx_p" id="S4.T2.2.4.3.1.1.1" style="width:53.8pt;">Timbre</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.4.3.2.1">
<span class="ltx_p" id="S4.T2.2.4.3.2.1.1" style="width:107.7pt;">mean, std of 4-tracks (Vocals, Bass, Drums, Others)</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.4.1.1">
<span class="ltx_p" id="S4.T2.2.5.4.1.1.1" style="width:53.8pt;">Zero Crossing</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T2.2.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.5.4.2.1">
<span class="ltx_p" id="S4.T2.2.5.4.2.1.1" style="width:107.7pt;"># of zero crossings &amp; avg, median of zero crossings rate</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S4.T2.2.6.5.1" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.1.1">
<span class="ltx_p" id="S4.T2.2.6.5.1.1.1" style="width:39.2pt;"><span class="ltx_text" id="S4.T2.2.6.5.1.1.1.1">High-level</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.2.1">
<span class="ltx_p" id="S4.T2.2.6.5.2.1.1" style="width:53.8pt;">Mood</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.2.6.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.6.5.3.1">
<span class="ltx_p" id="S4.T2.2.6.5.3.1.1" style="width:107.7pt;">valence, arousal</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.2.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S4.T2.2.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.6.1.1">
<span class="ltx_p" id="S4.T2.2.7.6.1.1.1" style="width:53.8pt;">Genre</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S4.T2.2.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.2.7.6.2.1">
<span class="ltx_p" id="S4.T2.2.7.6.2.1.1" style="width:107.7pt;">Music, Musical Instrument</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S4.T2.3.1.1">Table 2</span>: </span>Explainable handcraft features.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>End-to-End Deep Learning</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Deep learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib35" title="">35</a>]</cite> is featured by its ability to directly learn meaningful information from raw data, as opposed to using hand-crafted features. As a result, we also test end-to-end models to find if their feature-learning process improves performance. Our model uses spectrograms in Mel-scale as inputs, similar to previous end-to-end MIR tasks. Moreover, transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib36" title="">36</a>]</cite>, which applies previously learned knowledge to new data, has been found to significantly increase learning performance by skipping costly data-labeling procedures. Here, we use the self-supervised pre-trained Audio Spectrogram Transformer (SSAST) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib37" title="">37</a>]</cite> since SSAST has been proved to achieve state-of-the-art results on numerous audio tasks, including audio event classification, keyword spotting, mood recognition, and speaker identification, after being trained on a vast amount of unlabeled data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiment Results</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Evaluation Metrics.</span> Spearman’s rank correlation and mean squared error (MSE) loss are used as the metrics to evaluate the performance of music memorability prediction. The former indicates the ability to rank the relative memorability of different audios, while the latter indicates the absolute error of the predicted results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Different Baselines.</span> Here, we leverage Chroma and MFCCs along with their respective derivatives as two hand-crafted feature representations and fit the ground truth by Multilayer Perceptron (MLP) as two simple baselines. Moreover, we also use the convnet model as a baseline since it is the most referenced and available work in general music representation. The convnet model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib38" title="">38</a>]</cite> utilizes CNNs for music tagging in the pre-training stage, and the extracted features serve as the representation for downstream tasks. Finally, Self-Supervised Audio Spectrogram Transformer (SSAST) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib37" title="">37</a>]</cite> is also used as the baseline, which is a Transformer-based model with more parameters as compared to CNNs. SSAST pretrains the model with joint discriminative and generative masked spectrogram patch modeling.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Implementation Details.</span> All the feature classifiers are pretrained without finetuning on the self-collected dataset. To handle the instability stemming from the limited labeled data, we normalize labels by subtracting the mean value, <span class="ltx_text ltx_font_italic" id="S5.p3.1.2">i.e.</span>, predicting a relative value instead of an absolute value. For MLP and SSAST models, the learning rates are respectively set to 5e-5 and 5e-6 with the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib39" title="">39</a>]</cite>. We also conduct additional feature selection on the handcrafted features to improve the convergence of the MLP/SVR model (only select 25 features) due the small number of data samples. In addition, techniques including frequency masking, band stop filtering, and reverberation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib26" title="">26</a>]</cite> are used to augment data, together with the pitch shifting augmentation. The results are reported by the average of the 10-fold outputs.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.1.1.1">Method</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.1.1.2">Corr.</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.1.1.3">MSE</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.2.1.1.4">MSE STD</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.2.2.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.2.1.1">chroma + MLP</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.2.1.2">0.1740</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.2.1.3">0.0326</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.2.2.1.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.3.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.2.1">MFCCs + MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.2.2">0.1179</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.2.3">0.0353</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.3.2.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.4.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.4.3.1">convnet features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib38" title="">38</a>]</cite> + MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.4.3.2">0.1889</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.4.3.3">0.0314</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.4.3.4">-</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.5.4.1">EHC features + SVR</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.5.4.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.5.4.2.1">0.2988</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.5.4.3">0.0339</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.5.4.4">0.0128</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.6.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.6.5.1">EHC features + SVR + PS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.6.5.2">0.2084</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.6.5.3">0.0391</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.6.5.4">0.0129</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.7.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.7.6.1">EHC features + MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.7.6.2">0.2656</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.7.6.3">0.0263</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.7.6.4">0.0058</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.8.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.7.1">EHC features + MLP + PS</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.7.2">0.2388</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.7.3">0.0275</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.8.7.4">0.0059</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.9.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.9.8.1">mel-spectrograms + SSAST</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.9.8.2">0.0124</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.9.8.3">0.0298</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.9.8.4">0.0061</td>
</tr>
<tr class="ltx_tr" id="S5.T3.2.10.9">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T3.2.10.9.1">mel-spectrograms + SSAST + PS</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T3.2.10.9.2">0.2658</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T3.2.10.9.3">0.0265</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T3.2.10.9.4">0.0074</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1">Table 3</span>: </span>Spearman’s rank correlation and MSE loss between predicted and ground truth music memorability score using different models. Note that EHC features stand for explainable handcrafted features, PS stands for pitch shift data augmentation, and Corr. represents Spearman’s rank correlation.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Prediction Results.</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1"><span class="ltx_text" id="S5.SS1.p1.1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.T3" title="Table 3 ‣ 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">3</span></a></span> compares the results of different prediction models, where SVR and MLP take explainable handcrafted features as inputs, and SSAST takes mel-spectrograms as inputs. The results indicate that chroma and MFCCs produce the worst results due to the ineffective feature extraction. For convnet features, the performance is better than chroma and MFCCs due to the pretraining. However, the amount of training data is too small to finetune the model on the music memorability regression task. SSAST outperforms other baselines since it incorporates the prior knowledge of spectrograms pre-trained by using advanced methods. Finally, Explainable Handcrafted Features (EHC) method produces the best correlation results by combining both low- and high-level features that help improve music memorability. These quantitative findings manifest that data-driven MIR tasks are notably reliant on huge data quantities to be resilient and general.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.T4" title="Table 4 ‣ 5.2 Ablation Study ‣ 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">4</span></a> shows an ablation study on feature selection for handcrafted features, indicating that selecting top-25 features leads to the best overall correlation results. Moreover, Table <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.T3" title="Table 3 ‣ 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">3</span></a> also shows an ablation study on extra pitch shifting for data augmentations. Small pitch shifts (less than 5 semitones) make the altered audio seem natural to the human ear according to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib40" title="">40</a>]</cite>. Therefore, we add semitone shifts of -5 to 5 to our data. The results manifest that pitch shifting is effective for the models that take sequence information into account because applying mean pooling across time on harmony features in non-sequential models like SVR and MLP just forces the model to forecast the same value using multiple static chroma information. This may confuse the model on harmony characteristics. On the other hand, models with sequential information, such as SSAST, learn pitch-invariance after pitch shifting. The performance of SSAST notably decreases without pitch shift data augmentation, possibly due to its data-hungry nature as a Transformer-based model, <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">i.e.</span> requiring more data for optimal parameter tuning.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.2.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.1.1.1">Model</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.1.1.2">Top-k feature selection</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.1.1.3">Corr.</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.1.1.4">MSE</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.2.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.2.2.1">MLP</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.2.2.2">k = 40 (no feature selection)</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.2.2.3">0.2160</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.2.2.4">0.0272</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.3.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.3.3.1">MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.3.3.2">k = 35</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.3.3.3">0.2324</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.3.3.4">0.0270</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.4.4.1">MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.4.4.2">k = 25</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.4.4.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.4.4.3.1">0.2656</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T4.2.4.4.4.1">0.0263</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.5.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.5.5.1">MLP</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.5.5.2">k = 20</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.5.5.3">0.2018</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.5.5.4">0.0271</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.6.6">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.6.6.1">SVR</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.6.6.2">k = 40 (no feature selection)</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.6.6.3">0.2168</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T4.2.6.6.4"><span class="ltx_text ltx_font_bold" id="S5.T4.2.6.6.4.1">0.0324</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.7.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.7.7.1">SVR</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.7.7.2">k = 35</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.7.7.3">0.2291</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.7.7.4">0.0340</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.8.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.8.8.1">SVR</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.8.8.2">k = 25</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.8.8.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.8.8.3.1">0.2988</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.2.8.8.4">0.0339</td>
</tr>
<tr class="ltx_tr" id="S5.T4.2.9.9">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.2.9.9.1">SVR</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.2.9.9.2">k = 20</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.2.9.9.3">0.2630</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T4.2.9.9.4">0.0354</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S5.T4.3.1.1">Table 4</span>: </span>Spearman’s rank correlation and MSE loss for MLP/SVR models with different top-k feature selection.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Interpretability</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We attempt to gain insight into the intrinsic memory utilizing XAI methodologies. One post-hoc strategy for expressing black box models in a human-interpretable manner is SHAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib16" title="">16</a>]</cite>. Specifically, SHAP explanations are obtained by perturbing a specific instance in the data and observing the impact of these perturbations on the black-box model’s output. As such, SHAP allows us to explore the factors that the model considers when determining memorability. <span class="ltx_text" id="S5.SS3.p1.1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#S5.F5" title="Figure 5 ‣ 5.3 Interpretability ‣ 5 Experiment Results ‣ A dataset and Baselines for Measuring and Predicting the Music Piece Memorability"><span class="ltx_text ltx_ref_tag">5</span></a></span> visualizes the directionality impact of the top-5 features in SHAP, where the x-axis stands for SHAP value and each point is a SHAP value of a sample for a feature. Red color and blue colors respectively indicate higher and lower values of a feature. As such, we can observe the feature directionality impact based on the distribution. For example, the first row shows that a higher arousal value leads to high memorability scores, while a lower arousal value can lead to both high and low memorability scores. The important factors for the predictor among the EHC features include arousal, bpm, harmony (the feature "D mean") and the timbre features extracted from the source other than vocals, drums, and bass (the feature "other db mean"). According to Psychology research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib28" title="">28</a>]</cite>, normal individuals without brain damage find it easier to recognize musical excerpts with high arousal. The melodies are the main constituent elements of the source ”others” after applying 4-stem Spleeter separation. This finding supports our understanding that we often focus on the main melody in music, and thus the chorus or hook of the song with outstanding melody usually represents the entire song.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<p class="ltx_p ltx_align_center" id="S5.F5.1"><span class="ltx_text" id="S5.F5.1.1">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="268" id="S5.F5.1.1.g1" src="extracted/2405.12847v1/shap_summary_5.png" width="598"/></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S5.F5.3.1.1">Figure 5</span>: </span>SHAP summary of the SVR model with RBF kernel <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.12847v1#bib.bib41" title="">41</a>]</cite>. The most important features are listed in decreasing order and the fact that feature value rises after the SHAP value shows a positive relationship between the two.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we explore the novel task of music memorability regression (MMR) using a data-driven approach. The consistency of our newly proposed YouTube Music Memorability (YTMM) dataset supports our hypothesis that music memorability indeed exists and can be predicted. Furthermore, we investigate the use of feature engineering and self-supervised learning for predicting music memorability, highlighting the importance of prior knowledge and other training approaches, such as label normalization, for improving results with limited data. We make the dataset available online to encourage further research and development in the field of MMR. In the future, we plan to: 1) scale the dataset to better represent the memorability of full music structures, 2) investigate the potential of transfer learning trained on music-oriented datasets to enhance our current baselines, and 3) study the personalization issue since music memorability can be strongly related to the past musical experience of individuals.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgement</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was supported in part by the National Science and Technology Council of Taiwan under Grant NSTC-109-2221-E-009-114-MY3.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Alexomanolaki, C. Loveday, and C. Kennett, “Music and memory in
advertising: Music as a device of implicit learning and recall,”
<em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Music, Sound, and the Moving Image</em>, vol. 1, no. 1, pp. 51–71, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Hecker, “Music for advertising effect,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Psychology &amp; Marketing</em>,
vol. 1, no. 3-4, pp. 3–8, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Snyder and R. Snyder, <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Music and memory: An introduction</em>.   MIT press, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. B. Ramsay, I. Ananthabhotla, and J. A. Paradiso, “The intrinsic
memorability of everyday sounds,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">AES International Conference on
Immersive and Interactive Audio</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. R. Halpern and D. Müllensiefen, “Effects of timbre and tempo change on
memory for music,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Quarterly Journal of Experimental Psychology</em>,
vol. 61, no. 9, pp. 1371–1384, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
D. Müllensiefen and A. R. Halpern, “The role of features and context in
recognition of novel melodies,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Music Perception: An Interdisciplinary
Journal</em>, vol. 31, no. 5, pp. 418–435, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. McCullough Campbell and E. H. Margulis, “Catching an earworm through
movement,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of New Music Research</em>, vol. 44, no. 4, pp.
347–358, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
K. Jakubowski, S. Finkel, L. Stewart, and D. Müllensiefen, “Dissecting an
earworm: Melodic features and song popularity predict involuntary musical
imagery.” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Psychology of Aesthetics, Creativity, and the Arts</em>,
vol. 11, no. 2, p. 122, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Isola, J. Xiao, D. Parikh, A. Torralba, and A. Oliva, “What makes a
photograph memorable?” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE transactions on pattern analysis and
machine intelligence</em>, vol. 36, no. 7, pp. 1469–1482, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
A. Khosla, A. S. Raju, A. Torralba, and A. Oliva, “Understanding and
predicting image memorability at a large scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the
IEEE International Conference on Computer Vision (ICCV)</em>, December 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
R. Dubey, J. Peterson, A. Khosla, M.-H. Yang, and B. Ghanem, “What makes an
object memorable?” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the ieee international conference
on computer vision</em>, 2015, pp. 1089–1097.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Shekhar, D. Singal, H. Singh, M. Kedia, and A. Shetty, “Show and recall:
Learning what makes videos memorable,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the IEEE
International Conference on Computer Vision Workshops</em>, 2017, pp. 2730–2739.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Adadi and M. Berrada, “Peeking inside the black-box: a survey on
explainable artificial intelligence (xai),” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE access</em>, vol. 6, pp.
52 138–52 160, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C.-Y. Li, P.-C. Yuan, and H.-Y. Lee, “What does a network layer hear?
analyzing hidden representations of end-to-end asr through speech
synthesis,” in <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 6434–6438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. V. Jeyakumar, J. Noor, Y.-H. Cheng, L. Garcia, and M. Srivastava, “How can
i explain this to you? an empirical study of deep neural network explanation
methods,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, vol. 33,
pp. 4211–4222, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
predictions,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems
30</em>, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett, Eds.   Curran Associates, Inc., 2017, pp. 4765–4774. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. A. Burgoyne, D. Bountouridis, J. van Balen, H. Honing <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">et al.</em>,
“Hooked: a game for discovering what makes music catchy,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">Proceedings of the 14th Society of Music Information Retrieval
Conference (ISMIR)</em>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. V. Balen, J. A. Burgoyne, D. Bountouridis, D. Müllensiefen, and R. C.
Veltkamp, “Corpus analysis tools for computational hook discovery,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ISMIR</em>, 2015, pp. 227–233.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Van Balen <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">et al.</em>, “Audio description and corpus analysis of popular
music,” Ph.D. dissertation, Utrecht University, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
I. R. Korsmit, J. A. Burgoyne, and H. Honing, “If you wanna be my lover… a
hook discovery game to uncover individual differences in long-term musical
memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 25th Anniversary Conference of the
European Society for the Cognitive Sciences of Music</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B. McFee, E. J. Humphrey, and J. P. Bello, “A software framework for musical
data augmentation.” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ISMIR</em>, vol. 2015, 2015, pp. 248–254.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical
automated data augmentation with a reduced search space,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops</em>, 2020, pp. 702–703.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H.-H. Wu, C.-C. Kao, Q. Tang, M. Sun, B. McFee, J. P. Bello, and C. Wang,
“Multi-task self-supervised pre-training for music classification,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>.   IEEE,
2021, pp. 556–560.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Castellon, C. Donahue, and P. Liang, “Codified audio language modeling
learns useful representations for music information retrieval,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">ISMIR</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. Pascual, M. Ravanelli, J. Serrà, A. Bonafonte, and Y. Bengio, “Learning
Problem-Agnostic Speech Representations from Multiple Self-Supervised
Tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proc. of the Conf. of the Int. Speech Communication
Association (INTERSPEECH)</em>, 2019, pp. 161–165. [Online]. Available:
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://dx.doi.org/10.21437/Interspeech.2019-2605</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro, J. Trmal, and
Y. Bengio, “Multi-task self-supervised learning for robust speech
recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ICASSP 2020-2020 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2020, pp. 6989–6993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
C. Spearman, “The proof and measurement of association between two things,”
<em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The American journal of psychology</em>, vol. 100, no. 3/4, pp. 441–471,
1987.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Samson, D. Dellacherie, and H. Platel, “Emotional power of music in
patients with memory disorders: Clinical implications of cognitive
neuroscience,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Annals of the New York Academy of Sciences</em>, vol. 1169,
no. 1, pp. 245–255, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P. Vuilleumier and W. Trost, “Music and emotions: from enchantment to
entrainment,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Annals of the New York Academy of Sciences</em>, vol. 1337,
no. 1, pp. 212–222, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
C. Harte and M. Sandler, “Automatic chord identifcation using a quantised
chromagram,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Audio Engineering Society Convention 118</em>.   Audio Engineering Society, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
R. Hennequin, A. Khlif, F. Voituret, and M. Moussallam, “Spleeter: a fast and
efficient music source separation tool with pre-trained models,”
<em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Journal of Open Source Software</em>, vol. 5, no. 50, p. 2154, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
A. T. Cemgil, B. Kappen, P. Desain, and H. Honing, “On tempo tracking:
Tempogram representation and kalman filtering,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Journal of New Music
Research</em>, vol. 29, no. 4, pp. 259–273, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
K. Zhang, H. Zhang, S. Li, C. Yang, and L. Sun, “The pmemo dataset for music
emotion recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2018 acm on international
conference on multimedia retrieval</em>, 2018, pp. 135–142.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, “Panns:
Large-scale pretrained audio neural networks for audio pattern recognition,”
<em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>,
vol. 28, pp. 2880–2894, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">nature</em>, vol. 521,
no. 7553, pp. 436–444, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
S. J. Pan and Q. Yang, “A survey on transfer learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">IEEE
Transactions on knowledge and data engineering</em>, vol. 22, no. 10, pp.
1345–1359, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Gong, C.-I. J. Lai, Y.-A. Chung, and J. Glass, “Ssast: Self-supervised
audio spectrogram transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">AAAI Conference on Artificial
Intelligence</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
K. Choi, G. Fazekas, M. Sandler, and K. Cho, “Transfer learning for music
classification and regression tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">The 18th International
Society of Music Information Retrieval (ISMIR) Conference 2017, Suzhou,
China</em>.   International Society of Music
Information Retrieval, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ICLR (Poster)</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Thickstun, Z. Harchaoui, D. P. Foster, and S. M. Kakade, “Invariances and
data augmentation for supervised music transcription,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">2018 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.   IEEE, 2018, pp. 2241–2245.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
I. Steinwart, D. Hush, and C. Scovel, “An explicit description of the
reproducing kernel hilbert spaces of gaussian rbf kernels,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">IEEE
Transactions on Information Theory</em>, vol. 52, no. 10, pp. 4635–4643, 2006.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 24 17:32:37 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
