<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.14527] Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs</title><meta property="og:description" content="Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarke…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.14527">

<!--Generated on Tue Mar  5 15:07:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning Cell Type Classification Disease Prognosis">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Dept. of Computer Science, Leipzig University </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Center for Scalable Data Analytics and Artificial Intelligence (ScaDS.AI) Dresden/Leipzig, Leipzig University, Germany
<span id="id2.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">email: </span>surname@cs.uni-leipzig.de</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anika Hannemann
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Ewald
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Leo Seeger
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Erik Buchmann
</span><span class="ltx_author_notes">1122</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Machine learning on large-scale genomic or transcriptomic data is important for many novel health applications. For example, precision medicine tailors medical treatments to patients on the basis of individual biomarkers, cellular and molecular states, etc.
However, the data required is sensitive, voluminous, heterogeneous, and typically distributed across locations where dedicated machine learning hardware is not available. Due to privacy and regulatory reasons, it is also problematic to aggregate all data at a trusted third party.
Federated learning is a promising solution to this dilemma, because it enables decentralized, collaborative machine learning without exchanging raw data.</p>
<p id="id2.id2" class="ltx_p">In this paper, we perform comparative experiments with the federated learning frameworks TensorFlow Federated and Flower. Our test case is the training of disease prognosis and cell type classification models. We train the models with distributed transcriptomic data, considering both data heterogeneity and architectural heterogeneity. We measure model quality, robustness against privacy-enhancing noise, computational performance and resource overhead. Each of the federated learning frameworks has different strengths. However, our experiments confirm that both frameworks can readily build models on transcriptomic data, without transferring personal raw data to a third party with abundant computational resources.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Federated Learning Cell Type Classification Disease Prognosis
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Machine learning has the potential for a paradigm shift in healthcare, towards medical treatments based on individual patient characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. For example, precision medicine uses biomarkers, genome, cellular and molecular data, and considers the environment and the lifestyle of patients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Machine learning on large scale genomic and transcriptomic data enables
the identification of disease subtypes, prediction of disease progression and selection of targeted therapies. Therefore, models need to be trained on large, diverse patient cohorts (sample size) with high-resolution genetic characterization (number of features). This is challenging: The data is commonly distributed across multiple healthcare institutions that may not possess the high-performance computing resources needed to build large deep-learning models. The sensitive nature of genomic and transcriptomic data presents privacy challenges. Genomic mutations and markers could even allow to re-identify individuals and their relatives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. This disallows to freely share such data with centralized aggregators.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated learning (FL), as shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, allows for decentralized model training across multiple data sets without requiring to transfer sensitive raw data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Only model parameters are exchanged between the aggregator and participating clients, and the overall computational burden is effectively shared. Adding noise <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to shared parameters could further increase privacy.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2402.14527/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="110" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Key challenges of Federated Learning.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we investigate the technical and conceptual challenges that arise when implementing FL on transcriptomic data.
Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates our four key challenges:
<span id="S1.p3.1.1" class="ltx_text ltx_font_italic">Architectural Heterogeneity</span> refers to different numbers of clients with varying computational capabilities.
<span id="S1.p3.1.2" class="ltx_text ltx_font_italic">Statistical Heterogeneity</span> relates to data distributions and sizes.
<span id="S1.p3.1.3" class="ltx_text ltx_font_italic">Gaussian Noise</span> addresses the impact of applying noise to the data, e.g., to achieve Differential Privacy, with different models (Logistic Regression and Sequential Deep Learning) and problem types (Binary and Multi Label).
Finally, <span id="S1.p3.1.4" class="ltx_text ltx_font_italic">Resource Consumption</span> addresses storage, communication overhead and training times.
To explore these challenges, we have conducted comparative experiments with two state-of-the-art FL frameworks – <span id="S1.p3.1.5" class="ltx_text ltx_font_italic">TensorFlow Federated</span> (TFF) and <span id="S1.p3.1.6" class="ltx_text ltx_font_italic">Flower</span> (FLWR) and transcriptomic data. In particular, we make the following contributions:</p>
</div>
<div id="S1.p4" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We train disease prognosis and cell type classification models with TFF and FLWR using hyperparameter tuning, and we measure the model quality.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We analyze the effects of the number of clients, the amount of training data and data distribution on the global model quality.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We measure the impact of Gaussian noise, locally applied to the weights, on the global model quality.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We compare memory consumption, run-times and network traffic of TFF and FLWR from both the client’s and the aggregator’s perspective.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We have demonstrated that FL frameworks can be readily applied to precision medicine applications.
Even more, we obtained an excellent global model with an AUC of up to 0.98 for disease prognosis and cell type classification with transcriptomic data. This performance is robust in the presence of diminishing data quality, increasing clients and diverse data distributions, and it reduces the necessary computational resources for the individual medical institution.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Paper structure</span>: Section <a href="#S2" title="2 Related Work ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> introduces related work. Section <a href="#S3" title="3 Methodology ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes our methodology. Section <a href="#S4" title="4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents our experimental results. Section <a href="#S5" title="5 Conclusions ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> concludes.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section reviews related work in the fields of FL and its applications in precision medicine, and discusses technical challenges and FL frameworks.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Federated Learning and Applications</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.5" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> allows distributed clients <math id="S2.SS0.SSS0.Px1.p1.1.m1.3" class="ltx_Math" alttext="\mathcal{N}_{1},...,\mathcal{N}_{n}" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.3a"><mrow id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.3.cmml"><msub id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.2.cmml">𝒩</mi><mn id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.3.cmml">,</mo><mi mathvariant="normal" id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.4" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml">𝒩</mi><mi id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.3b"><list id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2"><apply id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.2">𝒩</ci><cn type="integer" id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.1.1.3">1</cn></apply><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">…</ci><apply id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.2">𝒩</ci><ci id="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.3c">\mathcal{N}_{1},...,\mathcal{N}_{n}</annotation></semantics></math> to collaboratively train a model <math id="S2.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{M}_{glob}" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="S2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">ℳ</mi><mrow id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1a" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.4" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1b" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.5" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.5.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.2">ℳ</ci><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3"><times id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.1"></times><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.2">𝑔</ci><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.3">𝑙</ci><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.4.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.4">𝑜</ci><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.5.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.3.5">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">\mathcal{M}_{glob}</annotation></semantics></math> without sharing raw data. Instead of centralizing the data sets like in traditional machine learning, each client <math id="S2.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{N}_{i}" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.3.m3.1a"><msub id="S2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">𝒩</mi><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.2">𝒩</ci><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.3.m3.1c">\mathcal{N}_{i}</annotation></semantics></math> trains a local model <math id="S2.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{M}_{i}" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.4.m4.1a"><msub id="S2.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">ℳ</mi><mi id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.2">ℳ</ci><ci id="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.4.m4.1c">\mathcal{M}_{i}</annotation></semantics></math> on its own data set. The model parameters are then shared with a central aggregator, which aggregates them to create a global model <math id="S2.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{M}_{glob}" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.5.m5.1a"><msub id="S2.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.2" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml">ℳ</mi><mrow id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1a" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.4" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1b" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1.cmml">​</mo><mi id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.5" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.5.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.2">ℳ</ci><apply id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3"><times id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1"></times><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2">𝑔</ci><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3">𝑙</ci><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.4.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.4">𝑜</ci><ci id="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.5.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m5.1.1.3.5">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.5.m5.1c">\mathcal{M}_{glob}</annotation></semantics></math>. This process is repeated as more data is collected, with clients continuously updating their local models and forwarding the updated parameters to the aggregator. Raw data remains with the respective clients and is never transmitted.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">FL is used for collaboration across medical institutions, hospitals, health care insurers or other entities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Some problems relevant to medicine were addressed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> who trained a federated model to model Alzheimer’s and Parkinson’s disease. Beguier <span id="S2.SS0.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> present a differentially private and federated cancer occurrence prediction based on genomic data. For practical implications and benchmarking of frameworks for FL, however, there is less literature available. The multi-class data set we use in experiments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, to our knowledge, has not been modeled by any FL system. The authors of the binary data set propose a collaborative learning method named swarm learning without an aggregator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. This method achieves very good model quality, but does not take into account many challenges that arise when bringing FL in to production. We identified four major challenges with heterogeneity in data distribution, participating clients, consumption of computational resources and privacy:</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Technical and Conceptual Challenges</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">The issue of <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_bold">data heterogeneity</span> in FL encompasses both the distribution and size of the data. This challenge involves dealing with the non-IID (non-independent and identically distributed) nature of distributed data, which can lead to skewed or biased model training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Additionally, the size of data sets of each client can vary significantly, where smaller data sets may not adequately represent the population, impacting model quality and generalizability. Fu <span id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">et al.<cite class="ltx_cite ltx_citemacro_cite"><span id="S2.SS0.SSS0.Px2.p1.1.2.1.1" class="ltx_text ltx_font_upright">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="S2.SS0.SSS0.Px2.p1.1.2.2.2" class="ltx_text ltx_font_upright">]</span></cite></span> showed that not only data heterogeneity can influence the model’s quality, but also the varying number of clients which we call <span id="S2.SS0.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_bold">architectural heterogeneity</span>. Navigating these aspects of statistical heterogeneity is crucial for ensuring the robustness and efficacy of FL models. In the context of transcriptomic data and health-care institutions both issues are very common since hospitals vary greatly in their sizes and specialization or different laboratories introduce bias due to small differences in sequencing protocols and machinery <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">Furthermore, while FL is designed to enhance <span id="S2.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_bold">privacy</span> by training models locally and sharing only model updates, ensuring the privacy and security of these updates against potential inference attacks remains a critical concern. Multiple works showed, that there is no formal privacy guarantee for FL without additional privacy-enhancing techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Recent publications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> showed that baseline FL is vulnerable to reconstruction attacks, while others <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> successfully performed Membership Inference Attacks (MIA). Multiple works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> explored Differential Privacy in a FL scenario for medical data. Differential Privacy protects the privacy of individual data points in a training data set while allowing ML models to benefit from the overall information. It adds controlled noise to data or model parameters, making it difficult to infer specifics about individual entries. However, the application of noise usually comes with information loss. For transcriptomic data in combination with clinical patient data, this is a dilemma since biomarker signals are often weak for multi-factorial diseases. Hence, privacy levels need to be carefully chosen to find a trade-off between model quality, which is highly critical in medical applications, and privacy.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p3.1" class="ltx_p">Finally, <span id="S2.SS0.SSS0.Px2.p3.1.1" class="ltx_text ltx_font_bold">resource consumption</span> presents another challenge for FL. The diverse and potentially resource-limited nature of participating clients in a FL network can lead to inefficiency and delay <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Furthermore, the communication required for model updates and synchronization in FL adds to network bandwidth demands, which can be a bottleneck in resource-limited environments.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">FL Frameworks</h4>

<div id="S2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p1.1" class="ltx_p">The field of federated learning is rapdily evolving, and there
are many existing open-source FL frameworks, such as TensorFlow Federated (TFF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, Flower (FLWR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, PySyft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, FATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
These frameworks vary in terms of their features, ease of use, and specific use cases. The choice of a federated learning framework typically depends on the specific requirements and constraints of the application.</p>
</div>
<div id="S2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px3.p2.1" class="ltx_p">Both PySyft and TFF are well established and benefit from a large community support. While TFF is based on the TensorFlow ecosystem, PySyft works primarily with PyTorch. Both PySyft and FATE provide multiple optional privacy-enhancing methods such as Differential Privacy and Secure Multi Party Computation.
FLWR is designed to be framework-agnostic and can work with various machine learning frameworks, including TensorFlow, PyTorch, and others. In terms of <span id="S2.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_italic">abstraction level</span>, Flower’s API is more low-level and is, therefore, supposed to be more user friendly than TFF and PySyft. While FLWR and FATE only allow simulation and cluster deployment, FedML provides a flexible and generic API and allows on-device training. Also, FedML can be used for various network topologies such as Split Learning, Meta FL and Transfer-Learning.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section explains our experimental concept, the data sets we used, and the architectures of the machine learning models for our experiments.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Concept</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To quantify the impact of our four key challenges on FL with transcriptomic data, we measure the quality of a global model obtained by FL on centralized data first. With this <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">baseline</span>, we conduct <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">comparative experiments</span> to explore the effects of <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">Architectural</span> and <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">Statistical Heterogeneity</span> and <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">Gaussian Noise</span>
on the model quality. Furthermore, we measure the <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_italic">Resource Consumption</span> at the aggregator and the clients.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To draw robust conclusions, we vary <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_italic">problem type</span> and <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_italic">model architecture</span>. In particular, we conduct experiments not only on a binary-labeled data set but also on a multi-class data set. This aligns with clinical research, which typically covers a variety of diseases and research questions rather than a single condition. In multi-class problems, the complexity increases as the model must differentiate between multiple, often overlapping conditions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">We decided to use the FL frameworks <span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">TensorFlow Federated</span> (TFF) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and <span id="S3.SS1.p3.1.2" class="ltx_text ltx_font_bold">Flower</span> (FLWR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Our choice was driven by multiple factors: We prioritize documentation and usability.
Furthermore, we are interested in exploring horizontal FL with frameworks that have programming interfaces at different levels of abstraction.
Finally, frameworks with the same communication protocol allow to compare the network performance.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Sets</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We use two data sets.
The Acute Myeloid Leukemia data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> was previously obtained from 105 studies, resulting in 12,029 samples with binary labels.
We call it the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">binary-labeled data set</span>. It consists of gene expressions by microarray and <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">RNA-Seq</span> technologies from peripheral blood mononuclear cells (PBMC) of patients with either a healthy condition or acute myeloid leukemia (AML).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">The <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">multi-class data set</span> includes expression profiles generated by <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">single-cell RNA-Seq</span> for cell types of the human brain, in particular the middle temporal gyrus (MTG). The data set was published by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, who isolated sample nuclei from eight donors and generated gene expression profiles by <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">single-cell RNA-Seq</span> for a total of 15928 cells (samples) describing 75 distinct cell subtypes. We reduced the number of classes (cell types) to make the data set more suitable to experiment with class imbalance. For that we selected only the the five most abundant cell types (classes) leading to 6931 cells (samples) for training. We preprocess both data sets as in previous analyses and benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Model Architectures</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We experiment with a <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">logistic regression model</span> and a <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_bold">deep-learning model</span>.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the deep-learning model uses a sequential neural network architecture. It consists of a series of dense layers, each with 256, 512, 128, 64, and 32 units, all activated using the ’relu’ activation function. Dropout layers with dropout rates of 0.4 and 0.15 prevent overfitting. The configuration of the output layer is based on the number of classes in the data set.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.2.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.3.2" class="ltx_text" style="font-size:90%;">Optimized Hyperparameters</span></figcaption>
<table id="S3.T1.4" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.4.1.1" class="ltx_tr">
<th id="S3.T1.4.1.1.1" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.1.1.1" class="ltx_p"><span id="S3.T1.4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Data Set</span></span>
</span>
</th>
<th id="S3.T1.4.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.2.1.1" class="ltx_p"><span id="S3.T1.4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Model</span></span>
</span>
</th>
<th id="S3.T1.4.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T1.4.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.1.1.3.1.1" class="ltx_p"><span id="S3.T1.4.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Hyperparameters</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.4.2.1" class="ltx_tr">
<td id="S3.T1.4.2.1.1" class="ltx_td ltx_align_justify ltx_border_t" rowspan="2">
<span id="S3.T1.4.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.1.1.1" class="ltx_p">Binary</span>
</span>
</td>
<td id="S3.T1.4.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.2.1.1" class="ltx_p">Deep Learning</span>
</span>
</td>
<td id="S3.T1.4.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.2.1.3.1.1" class="ltx_p">Adam, L2: 0.005, Epochs: 70</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.3.2" class="ltx_tr">
<td id="S3.T1.4.3.2.1" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.1.1.1" class="ltx_p">Logistic Regression</span>
</span>
</td>
<td id="S3.T1.4.3.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.3.2.2.1.1" class="ltx_p">SGD, L2: 0.001, Epochs: 8</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.4.3" class="ltx_tr">
<td id="S3.T1.4.4.3.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t" rowspan="2">
<span id="S3.T1.4.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.1.1.1" class="ltx_p">Multi Class</span>
</span>
</td>
<td id="S3.T1.4.4.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.2.1.1" class="ltx_p">Deep Learning</span>
</span>
</td>
<td id="S3.T1.4.4.3.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S3.T1.4.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.3.3.1.1" class="ltx_p">Adam, L2: 0.005, Epochs: 30</span>
</span>
</td>
</tr>
<tr id="S3.T1.4.5.4" class="ltx_tr">
<td id="S3.T1.4.5.4.1" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S3.T1.4.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.4.1.1.1" class="ltx_p">Logistic Regression</span>
</span>
</td>
<td id="S3.T1.4.5.4.2" class="ltx_td ltx_align_justify ltx_border_bb ltx_border_t">
<span id="S3.T1.4.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.5.4.2.1.1" class="ltx_p">SGD, L2: 1.0, Epochs: 10</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">For our baseline and for <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">hyperparameter tuning</span>, we train both models on centralized data.
In particular, the hyperparameter space was randomly searched to find Cross Entropy as optimal loss function with a batch size of 512. Hyperparameters that differ in the respective combinations of model and data are summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3.3 Model Architectures ‣ 3 Methodology ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
We denote the <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_bold">rounds of training</span> based on the local epochs, so that the total number of epochs remains a constant. Assume one round of training and two clients using 100 local training epochs. With two rounds of training, this would be 50 local epochs for both clients.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we describe our experimental setup and our analysis results regarding model quality, data quality and resource consumption.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We perform all experiments using one CPU core from an AMD(R) EPYC(R) 7551P@ 2.0GHz - Turbo 3.0GHz processor and 31 Gigabyte RAM for each client. The network is a 100 Gbit/s Infiniband. We measure the network traffic with tshark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. No GPU is used during the experiments. To ensure resource parity among different frameworks and the central model, each training process is bound exclusively to one CPU core. Our experiments are implemented in Python.
For preprocessing and data loading, we used the libraries Pandas <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and Scikit-learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.
Both the logistic regression model and the deep-learning model were implemented using Keras, with default settings and federated algorithms from FLWR and TFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
The default of FLWR is a federated averaging strategy in a client-aggregator setup. Additionally, we compute the average score of each metric for every client. The default building function in TFF uses a robust aggregator without zeroing and clipping of values as model aggregator. The clients of TFF all use the eager executor of TFF and are loading the data with a custom implementation of the data interface of TFF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In this configuration, FLWR and TFF implement the federated averaging algorithm with a learning rate set to 1.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. The code to our experiments can be found at <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">For our experiments, we tested combinations of Logistic Regression (LogReg) and Sequential Deep Learning (DL) models together with binary problems (Binary) and multi-label problems (Multi). In the figures, we abbreviate the combinations of models and problem types as <span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Binary LogReg</span>, <span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">Multi LogReg</span>, <span id="S4.SS1.p2.1.3" class="ltx_text ltx_font_bold">Binary DL</span> and <span id="S4.SS1.p2.1.4" class="ltx_text ltx_font_bold">Multi DL</span>. For each combination, we tested 3, 5, 10, 50 clients and 1, 2, 5, 10 rounds of training, and we measured model quality and computational resources used. To analyze the effect under investigation, we iterated over the respective other parameter and reported the averaged result, i.e., when varying the number of clients, we conducted tests for each training round configuration and presented the cumulative effect observed across all training rounds.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Model Quality</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To explore the impact of heterogeneity on the global model, we use a 5-fold cross validation and compute the Area Under the Curve (AUC). A higher AUC value (closer to 1) indicates a better model, that distinguishes between the classes more effectively.
We compare the AUC of the centralized baseline with the AUC obtained with FL and varying numbers of clients and training rounds.
In the following, we explain our key findings.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Finding 1: Boosting training rounds does not always enhance model quality.</span>
We assumed from previous work (cf. Sec. <a href="#S2" title="2 Related Work ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) that an increasing number of training rounds improves the quality of the global model.
To examine the impact of frequency of weight updates among clients during training, we varied the numbers of training rounds and kept the total number of epochs constant.
This approach is consistent with our round configurations optimized with hyperparameter tuning, as described in Section <a href="#S3" title="3 Methodology ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Consider Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). The left two columns of diagrams show the results of our experiments with varying numbers of rounds, the right ones varying numbers of clients over the respective other variable. The top line of diagrams were obtained with deep learning models, the bottom line with logistic regression.
We find that the AUC of the global model does not improve significantly with the number of rounds for logistic regression. Deep learning benefits slightly with more rounds of weight updates (see Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). For non-balanced data sets, matching most real-world FL scenarios,
updating rounds prove to be more effective to mitigate class-imbalance across different clients (see Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>).
Thus, healthcare institutions should carefully chose the number of rounds in FL applications, based on the data distribution and machine learning algorithms used.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2402.14527/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">AUC with respect to an increasing training rounds and clients</span></figcaption>
</figure>
<div id="S4.SS2.p4" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text ltx_font_bold">Finding 2: Fewer clients and more data increases model quality.</span>
To analyze the effect of the number of clients, we split the training data in disjoint subsets and distributed them to clients. Each subset has the same size and class distribution as the whole data set.
The data on each client is then split into training and test data with a ratio of 80:20. The test data of all clients is combined and the aggregated FL model is evaluated on this test data.
Then, both FL frameworks were used to train a global model.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p">For a small number of clients, Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports that the AUC of FL is similar to our baseline (AUC 0.98), proving that FL can reach centralized model quality in real-world scenarios. A slight decrease for 10 clients can be discovered which is followed by a drop in AUC in the extreme case of 50 clients. This results from the reduced size of local training data: As the number of clients increases, the data set is divided among them, resulting in a reduction in the training data available for local training. The limited data size does indeed reflect a possible scenario where small healthcare institutions want to attend to a FL scenario.
We conclude that clients should only allowed
when they provide a substantial number of samples on which local model training can be performed. </p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_bold">Finding 3: Model quality is driven by models, not by frameworks.</span>
Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows that the logistic regression model has an overall lower model quality than the deep learning model for the multi-class problem, as its highest AUC value is 0.90 for both frameworks. In contrast, the deep learning model has an AUC of 0.99 for TFF and 0.98 for FLWR. This emphasizes the superiority of deep learning for this data set, and underlines that the model (and their hyperparameterisation) must fit to the problem.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">Benchmarks for a direct comparison of model quality are rare, especially for transcriptomic data. We wanted to learn if there is a difference between FLWR and TFF across the tested scenarios. As Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates, if all parameters and aggregation algorithms for FLRW and TFF are configured in the same way, both frameworks deliver a similar model quality. This observation holds for various configuration. Healthcare institutions are therefore free to select FL frameworks based on functionality (e.g. privacy-support), usability and computational resource demand, instead of concerning model quality.
</p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text ltx_font_bold">Finding 4: Class imbalance impairs federated learning.</span>
A challenge for FL is that data points are usually not independent and identically distributed (IID), leading to statistical heterogeneity. With transcriptomic data, a balanced class distribution among all clients seems unlikely. Therefore, we explored the effect of data imbalance on the global model’s quality. We investigate IID- and non-IID-data by methodically increasing the imbalance to find a sweet spot of imbalance and model quality.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2402.14527/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="249" height="121" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">AUC with respect to an increasing class imbalance and training rounds over all imbalance configurations</span></figcaption>
</figure>
<div id="S4.SS2.p9" class="ltx_para">
<p id="S4.SS2.p9.1" class="ltx_p">We start our experiments with a number of clients equal to the number of classes, and all classes are equally distributed among the clients. Subsequent experiments increase the number of samples from one class while reducing the number of samples from another class. This process is repeated independently for each client and class, until each client contains samples from a single class. Due to lack of space, we present a visualization of only a subset of the conducted experiments (Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Detailed visualizations and results can be found at <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S4.SS2.p10" class="ltx_para">
<p id="S4.SS2.p10.1" class="ltx_p">We compare the resulting AUC with an equally-distributed data set. As Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows, deep learning can indeed fight class imbalance. However, if the imbalance exceeds a certain threshold, a sudden drop in AUC can be expected. The threshold depends on the data set and machine learning algorithm.
Our results indicate that a non-IID distribution not necessarily results in poor model quality. But, the model quality in the presence of non-IID is strongly dependent on the problem type and data set. This can be further increased with the appropriate model selection. As Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows, deep learning is robust with a drop in AUC at 90% imbalance. Whereas more training rounds does not improve the robustness for logistic regression, it does for deep learning. We also investigated the effect of multiple rounds to a non-IID setting. Again, we increased the training rounds over all configurations in data distributions and report the average.
The increase in a number of training rounds leads to an improvement of model quality for deep learning with non-IID data (see Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), but does not affect the quality of logistic regression, regardless of the problem type <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Thus, healthcare institutions need to consider that the model quality depends on minimal number of samples per class. This number depends on aspects like the training algorithm.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Data Quality and Privacy</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">There are many anonymization approaches such as Differential Privacy, which apply noise to the data. Because TFF and FLWR have different levels of support for Differential Privacy, we have chosen a general approach to investigate the impact of anonymization on model quality:
We add Gaussian noise to the local parameters before aggregation.
</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2402.14527/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="199" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.3.2" class="ltx_text" style="font-size:90%;">AUC with respect to an increasing local Gaussian noise</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Finding 5: Adding noise to the weights has a strong impact on model quality.</span>
Our experiments use five clients and varying noise parameters 0.01, 0.03, 0.05, 0.07, 0.085, 0.1.
We observe that all model and problem types deal with some noise. However, at some point AUC drops to approx. 0.07 – 0.085. TFF copes with noise better than FLWR, possibly because TFF uses regularization techniques that mitigate the impact of noisy updates. Increasing the training rounds does not improve the model quality, but slightly decreases AUC.
Thus, if healthcare institutions want to apply differential privacy, sophisticated approaches are required to ensure model quality.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Computational Resources</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">When analyzing the computational resources of a federated system, both the local and the global perspective are relevant.
We investigate the aggregated training time, memory consumption and network traffic for the clients and the aggregator.
Again, we present our main findings and refer to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for detailed results.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2402.14527/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Local training time with respect to increasing training rounds and clients</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2402.14527/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Local memory usage with respect to increasing training rounds and clients</span></figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Finding 6: FL does not increase individual training times.</span>
We measure the training time for each client individually over multiple training rounds.
Recall that we keep the total number of training epochs and the total number of samples constant, i.e., the more clients, the fewer local training rounds and the smaller the sample sizes per individual client. Thus, we assume that more clients result in smaller training times per client. Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4 Computational Resources ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> confirms this.
The figure is organized in the same way as Figure <a href="#S4.F2" title="Figure 2 ‣ 4.2 Model Quality ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, i.e., models in rows and problem types in columns.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">FLWR provides a much faster local training compared to TFF, because of it’s parallelism. TFF launches clients with less or no parallelism.
The difference between centralized training and federated training is less distinct for logistic regression than for training deep learning models.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Thus, healthcare institutions
benefit more from federated training for complex machine learning approaches with long training times such as deep learning models.
From a global perspective, the increased network traffic (see Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4 Computational Resources ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>) might slightly increase the total training times. This depends on the number of round and clients, and the network capacity and latency of the coordinator.</p>
</div>
<div id="S4.SS4.p5" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS4.p5.1" class="ltx_p"><span id="S4.SS4.p5.1.1" class="ltx_text ltx_font_bold">Finding 7: Memory consumption is effectively shared.</span>
We measured the aggregated memory consumption for both the clients and the overall system.
As the number of clients or rounds increases, the overall resource consumption increases due to the increase in coordination effort. However, the client-wise resource consumption decreases, which is an advantage for healthcare institutions.</p>
</div>
<div id="S4.SS4.p6" class="ltx_para">
<p id="S4.SS4.p6.1" class="ltx_p">We observed that both the clients and the global FL system as a whole require more memory with deep learning than with logistic regression.
This was expected, because deep learning uses much more parameters than logistic regression. Second, TFF uses much more memory than FLWR (Figure <a href="#S4.F6" title="Figure 6 ‣ 4.4 Computational Resources ‣ 4 Experiments ‣ Federated Learning on Transcriptomic Data: Model Quality and Performance Trade-Offs" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, again with models in rows and problem types in columns).
We conclude that FL saves healthcare institutions a significant amount of memory, at the cost of a slight increase in global training time and global memory requirements.
Further, FL frameworks show distinct differences in their training times and memory consumption revealing potential for optimization of FL tools.</p>
</div>
<div id="S4.SS4.p7" class="ltx_para">
<br class="ltx_break">
<p id="S4.SS4.p7.1" class="ltx_p"><span id="S4.SS4.p7.1.1" class="ltx_text ltx_font_bold">Finding 8: The network load is not a bottleneck.</span>
To assess the network load, we assume that the amount of data transmitted and received is determined by the data serialization method of the framework, and is not influenced by hardware or interference from other clients. Therefore, we experiment with a fixed number of 10 clients. In accordance to the increased memory usage of TFF, TFF comes with higher network traffic as well. Additionally, since deep learning needs to share more parameters than logistic regression, the network traffic rises from 4 MB (peak for LogReg) up to 30 MB (peak for DL).</p>
</div>
<div id="S4.SS4.p8" class="ltx_para">
<p id="S4.SS4.p8.1" class="ltx_p">For comparison, an average household has a bandwidth of 209 Mbps and can easily handle the network demands <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We conclude that the network traffic is acceptable for healthcare institutions, and may pose a problem only for the training of very large neural networks such as foundation models. The frameworks have different demands on computing resources, but this is not a basis for selection in most medical scenarios, and it does not restrict the applicability of FL on transcriptomic data.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2402.14527/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="199" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Network traffic with respect to an increasing number of rounds</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We have analyzed the challenges of applying Federated Learning to transcriptomic data regarding architectural hegerogeneity, statistical heterogeneity, Gaussian noise and resource consumption. This is important for application areas such as precision medicine, where sensitive patient data is distributed among clients, which do not possess the computational resources for traditional machine learning approaches. In particular, we tested two real-world data sets and use-cases with varying numbers of training rounds and clients, and we compared a centralized baseline with two FL frameworks.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our analysis shows, that for multi-factorial problems and high number of features, deep learning models outperform logistic regression models in terms of model quality.
Increasing the number of training rounds does not greatly improve the global model quality, showing the high effectiveness of weigh aggregation in FL. However, hyperparameter tuning has a large impact.
Transcriptomic data is robust to some class imbalance, especially using deep learning models. Problem type and data set are key factors for robustness, also with respect to the amount of training data.
Privacy-preserving Gaussian noise can lead to a drastic loss in model quality.
FL saves memory and training time for individual clients: The more clients, the lower the individual load.
Flower consumes less computational resources than TensorFlow Federated, requires less knowledge about FL, but is also less customizable. The network traffic we measured seems acceptable for typical health institutions.
Finally, our findings confirm that FL is applicable and beneficial for disease prognosis and cell type classification using transcriptomic data.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
tshark: Command line network protocol analyzer. <a target="_blank" href="https://www.wireshark.org/docs/man-pages/tshark.html" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.wireshark.org/docs/man-pages/tshark.html</a> (2024), accessed on: 20.01.2024

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Abadi, M., et al.: <math id="bib.bib2.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib2.1.m1.1a"><mo stretchy="false" id="bib.bib2.1.m1.1.1" xref="bib.bib2.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.1.m1.1b"><ci id="bib.bib2.1.m1.1.1.cmml" xref="bib.bib2.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.1.m1.1c">\{</annotation></semantics></math>TensorFlow<math id="bib.bib2.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib2.2.m2.1a"><mo stretchy="false" id="bib.bib2.2.m2.1.1" xref="bib.bib2.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.2.m2.1b"><ci id="bib.bib2.2.m2.1.1.cmml" xref="bib.bib2.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.2.m2.1c">\}</annotation></semantics></math>: a system for <math id="bib.bib2.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib2.3.m3.1a"><mo stretchy="false" id="bib.bib2.3.m3.1.1" xref="bib.bib2.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.3.m3.1b"><ci id="bib.bib2.3.m3.1.1.cmml" xref="bib.bib2.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.3.m3.1c">\{</annotation></semantics></math>Large-Scale<math id="bib.bib2.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib2.4.m4.1a"><mo stretchy="false" id="bib.bib2.4.m4.1.1" xref="bib.bib2.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib2.4.m4.1b"><ci id="bib.bib2.4.m4.1.1.cmml" xref="bib.bib2.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib2.4.m4.1c">\}</annotation></semantics></math> machine learning. In: 12th USENIX symposium on operating systems design and implementation (OSDI 16). pp. 265–283 (2016)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Adnan, M., Kalra, S., Cresswell, J.C., Taylor, G.W., Tizhoosh, H.R.: Federated learning and differential privacy for medical image analysis. Scientific reports <span id="bib.bib3.1.1" class="ltx_text ltx_font_bold">12</span>(1),  1953 (2022)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Antunes, R.S., André da Costa, C., Küderle, A., Yari, I.A., Eskofier, B.: Federated learning for healthcare: Systematic review and architecture proposal. ACM Transactions on Intelligent Systems and Technology (TIST) <span id="bib.bib4.1.1" class="ltx_text ltx_font_bold">13</span>(4), 1–23 (2022)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Beguier, C., Terrail, J.O.d., Meah, I., Andreux, M., Tramel, E.W.: Differentially private federated learning for cancer prediction. arXiv preprint arXiv:2101.02997 (2021)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Beutel, D.J., Topal, T., Mathur, A., Qiu, X., Fernandez-Marques, J., Gao, Y., Sani, L., Li, K.H., Parcollet, T., de Gusmão, P.P.B., et al.: Flower: A friendly federated learning research framework. arXiv preprint arXiv:2007.14390 (2020)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Boenisch, F., Dziedzic, A., Schuster, R., Shamsabadi, A.S., Shumailov, I., Papernot, N.: When the curious abandon honesty: Federated learning is not private. In: 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&amp;P). pp. 175–199. IEEE (2023)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Choudhury, O., Gkoulalas-Divanis, A., Salonidis, T., Sylla, I., Park, Y., Hsu, G., Das, A.: Differential privacy-enabled federated learning for sensitive health data. arXiv preprint arXiv:1910.02578 (2019)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Danek, B.P., Makarious, M.B., Dadu, A., Vitale, D., Nalls, M.A., Sun, J., Faghri, F., Lee, P.S.: Federated learning for multi-omics: a performance evaluation in parkinson’s disease. bioRxiv pp. 2023–10 (2023)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Dayan, I., Roth, H.R., Zhong, A., Harouni, A., Gentili, A., Abidin, A.Z., Liu, A., Costa, A.B., Wood, B.J., Tsai, C.S., et al.: Federated learning for predicting clinical outcomes in patients with covid-19. Nature medicine <span id="bib.bib10.1.1" class="ltx_text ltx_font_bold">27</span>(10), 1735–1743 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Fair Internet Report: Internet in the usa - stats and figures. URL: <a target="_blank" href="https://fairinternetreport.com/United-States" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://fairinternetreport.com/United-States</a>, accessed on: 19.12.2023

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Fu, L., Zhang, H., Gao, G., Zhang, M., Liu, X.: Client selection in federated learning: Principles, challenges, and opportunities. IEEE Internet of Things Journal (2023)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Ganju, K., Wang, Q., Yang, W., Gunter, C.A., Borisov, N.: Property inference attacks on fully connected neural networks using permutation invariant representations. In: Proceedings of the 2018 ACM SIGSAC conference on computer and communications security. pp. 619–633 (2018)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Google: Tensorflow federated: Machine learning on decentralized data. <a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.tensorflow.org/federated</a> (25-11-2023)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
He, C., et al.: Fedml: A research library and benchmark for federated machine learning. arXiv preprint arXiv:2007.13518 (2020)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Hodge, R.D., et al.: Conserved cell types with divergent features in human versus mouse cortex. Nature <span id="bib.bib16.1.1" class="ltx_text ltx_font_bold">573</span>(7772), 61–68 (2019)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hodson, R.: Precision medicine. Nature <span id="bib.bib17.1.1" class="ltx_text ltx_font_bold">537</span>(7619), S49–S49 (2016)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N., Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open problems in federated learning. Foundations and Trends® in Machine Learning <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">14</span>(1–2), 1–210 (2021)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Kosorok, M.R., Laber, E.B.: Precision medicine. Annual review of statistics and its application <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">6</span>, 263–286 (2019)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lee, G.H., Shin, S.Y.: Federated learning on clinical benchmark data: performance assessment. Journal of medical Internet research <span id="bib.bib20.1.1" class="ltx_text ltx_font_bold">22</span>(10), e20891 (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Leo Seeger: Benchmarkingfederated. URL: <a target="_blank" href="https://github.com/leoseg/BenchmarkingFederated" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/leoseg/BenchmarkingFederated</a> (2024), accessed on: 20.01.2024

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Liu, Y., Fan, T., Chen, T., Xu, Q., Yang, Q.: Fate: An industrial grade platform for collaborative learning with data protection. The Journal of Machine Learning Research <span id="bib.bib22.1.1" class="ltx_text ltx_font_bold">22</span>(1), 10320–10325 (2021)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
McKinney, W., et al.: Data structures for statistical computing in python. In: Proceedings of the 9th Python in Science Conference. vol. 445, pp. 51–56. Austin, TX (2010)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.: Communication-efficient learning of deep networks from decentralized data. In: Artificial intelligence and statistics. pp. 1273–1282. PMLR (2017)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Melis, L., Song, C., De Cristofaro, E., Shmatikov, V.: Exploiting unintended feature leakage in collaborative learning. In: 2019 IEEE symposium on security and privacy (SP). pp. 691–706. IEEE (2019)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mendieta, M., Yang, T., Wang, P., Lee, M., Ding, Z., Chen, C.: Local learning matters: Rethinking data heterogeneity in federated learning. In: Proceedings of the Conference on Computer Vision and Pattern Recognition. pp. 8397–8406 (2022)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Oestreich, M., Chen, D., Schultze, J.L., Fritz, M., Becker, M.: Privacy considerations for sharing genomics data. EXCLI journal <span id="bib.bib27.1.1" class="ltx_text ltx_font_bold">20</span>,  1243 (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., et al.: Scikit-learn: Machine learning in python. the Journal of machine Learning research <span id="bib.bib28.1.1" class="ltx_text ltx_font_bold">12</span>, 2825–2830 (2011)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Pfitzner, B., Steckhan, N., Arnrich, B.: Federated learning in a medical context: a systematic literature review. ACM Transactions on Internet Technology (TOIT) <span id="bib.bib29.1.1" class="ltx_text ltx_font_bold">21</span>(2), 1–31 (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Warnat-Herresthal, S., Schultze, H., Shastry, K.L., Manamohan, S., Mukherjee, S., Garg, V., Sarveswara, R., Händler, K., Pickkers, P., Aziz, N.A., et al.: Swarm learning for decentralized and confidential clinical machine learning. Nature <span id="bib.bib30.1.1" class="ltx_text ltx_font_bold">594</span>(7862), 265–270 (2021)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Warnat-Herresthal, S., et al.: Scalable prediction of acute myeloid leukemia using high-dimensional machine learning and blood transcriptomics. Iscience <span id="bib.bib31.1.1" class="ltx_text ltx_font_bold">23</span>(1) (2020)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Wu, J., Chen, Y., Wang, P., Caselli, R.J., Thompson, P.M., Wang, J., Wang, Y.: Integrating transcriptomics, genomics, and imaging in alzheimer’s disease: A federated model. Frontiers in Radiology <span id="bib.bib32.1.1" class="ltx_text ltx_font_bold">1</span>, 777030 (2022)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Zhao, J.C., Sharma, A., Elkordy, A.R., Ezzeldin, Y.H., Avestimehr, S., Bagchi, S.: Secure aggregation in federated learning is not private: Leaking user data at large scale through model modification. arXiv preprint arXiv:2303.12233 (2023)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhou, J., Chen, S., Wu, Y., Li, H., Zhang, B., Zhou, L., Hu, Y., Xiang, Z., Li, Z., Chen, N., et al.: Ppml-omics: a privacy-preserving federated machine learning method protects patients’ privacy in omic data. bioRxiv pp. 2022–03 (2022)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Ziller, A., Trask, A., Lopardo, A., Szymkow, B., Wagner, B., Bluemke, E., Nounahon, J.M., Passerat-Palmbach, J., Prakash, K., Rose, N., et al.: Pysyft: A library for easy federated learning. Federated Learning Systems: Towards Next-Generation AI pp. 111–139 (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.14526" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.14527" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.14527">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.14527" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.14528" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 15:07:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
