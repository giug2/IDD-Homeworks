<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder</title>
<!--Generated on Thu Sep 12 00:20:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.13747v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S1" title="In Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S2" title="In Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3" title="In Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Proposed Methodology and Experimental Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS1" title="In 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Incontext Learning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS1.SSS1" title="In 3.1 Incontext Learning ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Dataset Used</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS1.SSS2" title="In 3.1 Incontext Learning ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>ICL Experimental Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS2" title="In 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Finetuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS2.SSS1" title="In 3.2 Finetuning ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Finetuning Experimental Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS3" title="In 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Baseline Model Development</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.SS3.SSS1" title="In 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Baseline Experimental Results</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S4" title="In Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion and Future Work</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhinav P.M.
<br class="ltx_break"/>Calicut University 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id1.1.id1">\And</span>SujayKumar Reddy M 
<br class="ltx_break"/>VIT University 
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id2.2.id2">\And</span>Dr. Oswald Christopher 
<br class="ltx_break"/>Assistant Professor 
<br class="ltx_break"/>National Institute of Technology, Trichy 
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.id1">This project, titled "Machine Translation with Large Language Models: Decoder-only vs. Encoder-Decoder," aims to develop a multilingual machine translation (MT) model. Focused on Indian regional languages, especially Telugu, Tamil, and Malayalam, the model seeks to enable accurate and contextually appropriate translations across diverse language pairs. By comparing Decoder-only and Encoder-Decoder architectures, the project aims to optimize translation quality and efficiency, advancing cross-linguistic communication tools.The primary objective is to develop a model capable of delivering high-quality translations that are accurate and contextually appropriate. By leveraging large language models, specifically comparing the effectiveness of Decoder-only and Encoder-Decoder architectures, the project seeks to optimize translation performance and efficiency across multilingual contexts. Through rigorous experimentation and analysis, this project aims to advance the field of machine translation, contributing valuable insights into the effectiveness of different model architectures and paving the way for enhanced cross-linguistic communication tools.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1">Keywords : </span> Machine Translation, Decoder-only, Encoder-Decoder</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine Translation (MT) has witnessed significant advancements with the advent of Large Language Models (LLMs), which have revolutionized the field by offering robust capabilities in processing and translating natural language. These models, such as mT5 and LLaMA 2, vary in architecture from decoder-only designs to more complex encoder-decoder frameworks, each tailored to address specific challenges in multilingual translation tasks.
This work delves into the comparative analysis of LLMs across different architectural paradigms: decoder-only (1-1 and 1-many) and encoder-decoder (1-1, many-1, 1-many, and many-many). Our primary objectives are twofold: first, to explore how these models perform in bi-lingual and multilingual language translation scenarios, and second, to evaluate the effectiveness of encoder-decoder transformer models in Neural Machine Translation (NMT) compared to smaller, decoder-only models when trained under similar conditions.
Key considerations include assessing the impact of context length—measured in the number of tokens—on translation quality and efficiency for both architectural setups. By conducting comprehensive experiments and performance evaluations on datasets like FLORES-101 and TED Talks, we aim to provide insights into the optimal use cases and trade-offs associated with different LLM configurations in the realm of MT.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The code available on GitHub for In-context-learning (ICL), Baseline Model Development, and sample notebooks for finetuning at the following link <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sujaykumarmag/iasnlp" title="">https://github.com/sujaykumarmag/iasnlp</a></span></span></span>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><cite class="ltx_cite ltx_citemacro_cite">Aharoni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib15" title="">2019</a>)</cite> investigated the development of a universal NMT system capable of translating between 103 languages using over 25 billion training examples. The study emphasized the effectiveness of transfer learning for low-resource languages while maintaining high-quality translation for high-resource languages. By exploring complexities such as diverse scripting systems, data imbalance, and model capacity, the research compared multilingual NMT with bilingual baselines. The findings highlighted challenges in scaling models, balancing data distribution, and mitigating domain noise, offering insights for future research in universal machine translation. <cite class="ltx_cite ltx_citemacro_cite">Arivazhagan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib16" title="">2019</a>)</cite> explored the limits of multilingual NMT by training models to translate between 102 languages and English. Extensive experiments were conducted using the TED Talks multilingual corpus, revealing that massively multilingual models outperform previous state-of-the-art methods in low-resource settings while supporting up to 59 languages. The study analyzed different training setups and highlighted the trade-offs between translation quality and modeling decisions. Results demonstrated that the multilingual models exceeded strong bilingual baselines, indicating promising directions for future research in massively multilingual NMT.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The study by <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib17" title="">2023</a>)</cite> systematically investigated the performance and factors affecting LLMs in multilingual machine translation, finding that models like GPT-4, despite outperforming the strong supervised baseline NLLB in 40.91% of translation directions, still lag behind commercial systems like Google Translate, particularly for low-resource languages . They used the FLORES-101 dataset to benchmark translation quality and evaluated eight popular LLMs, including ChatGPT and GPT-4 . Their findings also highlighted that cross-lingual exemplars provided better guidance for low-resource translations than same-language exemplars. <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib18" title="">2018</a>)</cite> introduced BERT, a Bidirectional Encoder Representations from Transformers, which pretrains deep bidirectional representations from unlabeled text, conditioning on both left and right context across all layers. This design allows BERT to achieve state-of-the-art results on various natural language processing tasks with minimal task-specific architecture changes. BERT significantly improved performance on tasks such as GLUE, MultiNLI, and SQuAD v1.1, demonstrating absolute improvements in accuracy and F1 scores across different benchmarks <cite class="ltx_cite ltx_citemacro_cite">Devlin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib18" title="">2018</a>)</cite>. The Transformer architecture introduced by <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib19" title="">2017</a>)</cite> in 2017 revolutionized natural language processing (NLP) by employing self-attention mechanisms. This discovery transformed NLP and established a foundation for subsequently developed language translation
models.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Methodology and Experimental Results</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The proposed methodology aims to evaluate and compare the performance of Encoder-Decoder and Decoder-only models in natural language processing tasks. This methodology is structured into several key phases, including data preparation, model design, training, and evaluation. Each phase is described in detail below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Incontext Learning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our first approach in the Machine Translation is In-Context Learning using Few Shot Learning. A brief description about In-Context Learning and its working is given below.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.5">In-Context Learning allows language models to learn tasks using only a few examples <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib11" title="">Stanford </a></cite>. It is often seen as a prompt engineering task for Few-Shot Learning. In this method, Machine Translation pairs (<math alttext="&lt;X&gt;=&lt;Y&gt;" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.2"><semantics id="S3.SS1.p2.1.m1.2a"><mrow id="S3.SS1.p2.1.m1.2.3" xref="S3.SS1.p2.1.m1.2.3.cmml"><mrow id="S3.SS1.p2.1.m1.2.3.2.2" xref="S3.SS1.p2.1.m1.2.3.2.1.cmml"><mo fence="true" id="S3.SS1.p2.1.m1.2.3.2.2.1" rspace="0em" xref="S3.SS1.p2.1.m1.2.3.2.1.1.cmml">&lt;</mo><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">X</mi><mo fence="true" id="S3.SS1.p2.1.m1.2.3.2.2.2" lspace="0em" rspace="0.1389em" xref="S3.SS1.p2.1.m1.2.3.2.1.1.cmml">&gt;</mo></mrow><mo id="S3.SS1.p2.1.m1.2.3.1" lspace="0.1389em" rspace="0.1389em" xref="S3.SS1.p2.1.m1.2.3.1.cmml">=</mo><mrow id="S3.SS1.p2.1.m1.2.3.3.2" xref="S3.SS1.p2.1.m1.2.3.3.1.cmml"><mo fence="true" id="S3.SS1.p2.1.m1.2.3.3.2.1" lspace="0.1389em" rspace="0em" xref="S3.SS1.p2.1.m1.2.3.3.1.1.cmml">&lt;</mo><mi id="S3.SS1.p2.1.m1.2.2" xref="S3.SS1.p2.1.m1.2.2.cmml">Y</mi><mo fence="true" id="S3.SS1.p2.1.m1.2.3.3.2.2" lspace="0em" xref="S3.SS1.p2.1.m1.2.3.3.1.1.cmml">&gt;</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.2b"><apply id="S3.SS1.p2.1.m1.2.3.cmml" xref="S3.SS1.p2.1.m1.2.3"><eq id="S3.SS1.p2.1.m1.2.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.1"></eq><apply id="S3.SS1.p2.1.m1.2.3.2.1.cmml" xref="S3.SS1.p2.1.m1.2.3.2.2"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.2.3.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.3.2.2.1">expectation</csymbol><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">𝑋</ci></apply><apply id="S3.SS1.p2.1.m1.2.3.3.1.cmml" xref="S3.SS1.p2.1.m1.2.3.3.2"><csymbol cd="latexml" id="S3.SS1.p2.1.m1.2.3.3.1.1.cmml" xref="S3.SS1.p2.1.m1.2.3.3.2.1">expectation</csymbol><ci id="S3.SS1.p2.1.m1.2.2.cmml" xref="S3.SS1.p2.1.m1.2.2">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.2c">&lt;X&gt;=&lt;Y&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.2d">&lt; italic_X &gt; = &lt; italic_Y &gt;</annotation></semantics></math>), where <math alttext="X" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_X</annotation></semantics></math> is the source sentence and <math alttext="Y" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_Y</annotation></semantics></math> is the target sentence, are provided using a template <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_T</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib12" title="">2023</a>)</cite>. The In-Context Exemplars, which include <math alttext="&lt;X&gt;=&lt;Y&gt;" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.2"><semantics id="S3.SS1.p2.5.m5.2a"><mrow id="S3.SS1.p2.5.m5.2.3" xref="S3.SS1.p2.5.m5.2.3.cmml"><mrow id="S3.SS1.p2.5.m5.2.3.2.2" xref="S3.SS1.p2.5.m5.2.3.2.1.cmml"><mo fence="true" id="S3.SS1.p2.5.m5.2.3.2.2.1" rspace="0em" xref="S3.SS1.p2.5.m5.2.3.2.1.1.cmml">&lt;</mo><mi id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">X</mi><mo fence="true" id="S3.SS1.p2.5.m5.2.3.2.2.2" lspace="0em" rspace="0.1389em" xref="S3.SS1.p2.5.m5.2.3.2.1.1.cmml">&gt;</mo></mrow><mo id="S3.SS1.p2.5.m5.2.3.1" lspace="0.1389em" rspace="0.1389em" xref="S3.SS1.p2.5.m5.2.3.1.cmml">=</mo><mrow id="S3.SS1.p2.5.m5.2.3.3.2" xref="S3.SS1.p2.5.m5.2.3.3.1.cmml"><mo fence="true" id="S3.SS1.p2.5.m5.2.3.3.2.1" lspace="0.1389em" rspace="0em" xref="S3.SS1.p2.5.m5.2.3.3.1.1.cmml">&lt;</mo><mi id="S3.SS1.p2.5.m5.2.2" xref="S3.SS1.p2.5.m5.2.2.cmml">Y</mi><mo fence="true" id="S3.SS1.p2.5.m5.2.3.3.2.2" lspace="0em" xref="S3.SS1.p2.5.m5.2.3.3.1.1.cmml">&gt;</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.2b"><apply id="S3.SS1.p2.5.m5.2.3.cmml" xref="S3.SS1.p2.5.m5.2.3"><eq id="S3.SS1.p2.5.m5.2.3.1.cmml" xref="S3.SS1.p2.5.m5.2.3.1"></eq><apply id="S3.SS1.p2.5.m5.2.3.2.1.cmml" xref="S3.SS1.p2.5.m5.2.3.2.2"><csymbol cd="latexml" id="S3.SS1.p2.5.m5.2.3.2.1.1.cmml" xref="S3.SS1.p2.5.m5.2.3.2.2.1">expectation</csymbol><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">𝑋</ci></apply><apply id="S3.SS1.p2.5.m5.2.3.3.1.cmml" xref="S3.SS1.p2.5.m5.2.3.3.2"><csymbol cd="latexml" id="S3.SS1.p2.5.m5.2.3.3.1.1.cmml" xref="S3.SS1.p2.5.m5.2.3.3.2.1">expectation</csymbol><ci id="S3.SS1.p2.5.m5.2.2.cmml" xref="S3.SS1.p2.5.m5.2.2">𝑌</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.2c">&lt;X&gt;=&lt;Y&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.2d">&lt; italic_X &gt; = &lt; italic_Y &gt;</annotation></semantics></math> pairs, serve as a strong recipe for generating the best outputs from the model, as noted by Wu et al. <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib13" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4">A prompt <math alttext="P" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><mi id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><ci id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_P</annotation></semantics></math> is defined as <math alttext="T(X_{1},Y_{1})\oplus T(X_{2},Y_{2})\oplus\cdots\oplus T(X_{n},Y_{n})" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.6"><semantics id="S3.SS1.p3.2.m2.6a"><mrow id="S3.SS1.p3.2.m2.6.6" xref="S3.SS1.p3.2.m2.6.6.cmml"><mrow id="S3.SS1.p3.2.m2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.2.2.2.4" xref="S3.SS1.p3.2.m2.2.2.2.4.cmml">T</mi><mo id="S3.SS1.p3.2.m2.2.2.2.3" xref="S3.SS1.p3.2.m2.2.2.2.3.cmml">⁢</mo><mrow id="S3.SS1.p3.2.m2.2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.3.cmml"><mo id="S3.SS1.p3.2.m2.2.2.2.2.2.3" stretchy="false" xref="S3.SS1.p3.2.m2.2.2.2.2.3.cmml">(</mo><msub id="S3.SS1.p3.2.m2.1.1.1.1.1.1" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.1.1.1.1.2" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.2.cmml">X</mi><mn id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p3.2.m2.2.2.2.2.2.4" xref="S3.SS1.p3.2.m2.2.2.2.2.3.cmml">,</mo><msub id="S3.SS1.p3.2.m2.2.2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.2.2.2.2.2.2.2" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.2.cmml">Y</mi><mn id="S3.SS1.p3.2.m2.2.2.2.2.2.2.3" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.3.cmml">1</mn></msub><mo id="S3.SS1.p3.2.m2.2.2.2.2.2.5" stretchy="false" xref="S3.SS1.p3.2.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.2.m2.6.6.7" xref="S3.SS1.p3.2.m2.6.6.7.cmml">⊕</mo><mrow id="S3.SS1.p3.2.m2.4.4.4" xref="S3.SS1.p3.2.m2.4.4.4.cmml"><mi id="S3.SS1.p3.2.m2.4.4.4.4" xref="S3.SS1.p3.2.m2.4.4.4.4.cmml">T</mi><mo id="S3.SS1.p3.2.m2.4.4.4.3" xref="S3.SS1.p3.2.m2.4.4.4.3.cmml">⁢</mo><mrow id="S3.SS1.p3.2.m2.4.4.4.2.2" xref="S3.SS1.p3.2.m2.4.4.4.2.3.cmml"><mo id="S3.SS1.p3.2.m2.4.4.4.2.2.3" stretchy="false" xref="S3.SS1.p3.2.m2.4.4.4.2.3.cmml">(</mo><msub id="S3.SS1.p3.2.m2.3.3.3.1.1.1" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.3.3.3.1.1.1.2" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1.2.cmml">X</mi><mn id="S3.SS1.p3.2.m2.3.3.3.1.1.1.3" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1.3.cmml">2</mn></msub><mo id="S3.SS1.p3.2.m2.4.4.4.2.2.4" xref="S3.SS1.p3.2.m2.4.4.4.2.3.cmml">,</mo><msub id="S3.SS1.p3.2.m2.4.4.4.2.2.2" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.4.4.4.2.2.2.2" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2.2.cmml">Y</mi><mn id="S3.SS1.p3.2.m2.4.4.4.2.2.2.3" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p3.2.m2.4.4.4.2.2.5" stretchy="false" xref="S3.SS1.p3.2.m2.4.4.4.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p3.2.m2.6.6.7a" xref="S3.SS1.p3.2.m2.6.6.7.cmml">⊕</mo><mi id="S3.SS1.p3.2.m2.6.6.8" mathvariant="normal" xref="S3.SS1.p3.2.m2.6.6.8.cmml">⋯</mi><mo id="S3.SS1.p3.2.m2.6.6.7b" xref="S3.SS1.p3.2.m2.6.6.7.cmml">⊕</mo><mrow id="S3.SS1.p3.2.m2.6.6.6" xref="S3.SS1.p3.2.m2.6.6.6.cmml"><mi id="S3.SS1.p3.2.m2.6.6.6.4" xref="S3.SS1.p3.2.m2.6.6.6.4.cmml">T</mi><mo id="S3.SS1.p3.2.m2.6.6.6.3" xref="S3.SS1.p3.2.m2.6.6.6.3.cmml">⁢</mo><mrow id="S3.SS1.p3.2.m2.6.6.6.2.2" xref="S3.SS1.p3.2.m2.6.6.6.2.3.cmml"><mo id="S3.SS1.p3.2.m2.6.6.6.2.2.3" stretchy="false" xref="S3.SS1.p3.2.m2.6.6.6.2.3.cmml">(</mo><msub id="S3.SS1.p3.2.m2.5.5.5.1.1.1" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1.cmml"><mi id="S3.SS1.p3.2.m2.5.5.5.1.1.1.2" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1.2.cmml">X</mi><mi id="S3.SS1.p3.2.m2.5.5.5.1.1.1.3" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1.3.cmml">n</mi></msub><mo id="S3.SS1.p3.2.m2.6.6.6.2.2.4" xref="S3.SS1.p3.2.m2.6.6.6.2.3.cmml">,</mo><msub id="S3.SS1.p3.2.m2.6.6.6.2.2.2" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2.cmml"><mi id="S3.SS1.p3.2.m2.6.6.6.2.2.2.2" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2.2.cmml">Y</mi><mi id="S3.SS1.p3.2.m2.6.6.6.2.2.2.3" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2.3.cmml">n</mi></msub><mo id="S3.SS1.p3.2.m2.6.6.6.2.2.5" stretchy="false" xref="S3.SS1.p3.2.m2.6.6.6.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.6b"><apply id="S3.SS1.p3.2.m2.6.6.cmml" xref="S3.SS1.p3.2.m2.6.6"><csymbol cd="latexml" id="S3.SS1.p3.2.m2.6.6.7.cmml" xref="S3.SS1.p3.2.m2.6.6.7">direct-sum</csymbol><apply id="S3.SS1.p3.2.m2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2"><times id="S3.SS1.p3.2.m2.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.2.2.2.3"></times><ci id="S3.SS1.p3.2.m2.2.2.2.4.cmml" xref="S3.SS1.p3.2.m2.2.2.2.4">𝑇</ci><interval closure="open" id="S3.SS1.p3.2.m2.2.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2"><apply id="S3.SS1.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.2">𝑋</ci><cn id="S3.SS1.p3.2.m2.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.SS1.p3.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.2.2.2.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.2.2.2.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.2">𝑌</ci><cn id="S3.SS1.p3.2.m2.2.2.2.2.2.2.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.2.2.2.2.2.2.3">1</cn></apply></interval></apply><apply id="S3.SS1.p3.2.m2.4.4.4.cmml" xref="S3.SS1.p3.2.m2.4.4.4"><times id="S3.SS1.p3.2.m2.4.4.4.3.cmml" xref="S3.SS1.p3.2.m2.4.4.4.3"></times><ci id="S3.SS1.p3.2.m2.4.4.4.4.cmml" xref="S3.SS1.p3.2.m2.4.4.4.4">𝑇</ci><interval closure="open" id="S3.SS1.p3.2.m2.4.4.4.2.3.cmml" xref="S3.SS1.p3.2.m2.4.4.4.2.2"><apply id="S3.SS1.p3.2.m2.3.3.3.1.1.1.cmml" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.3.3.3.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.3.3.3.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1.2">𝑋</ci><cn id="S3.SS1.p3.2.m2.3.3.3.1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.3.3.3.1.1.1.3">2</cn></apply><apply id="S3.SS1.p3.2.m2.4.4.4.2.2.2.cmml" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.4.4.4.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.4.4.4.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2.2">𝑌</ci><cn id="S3.SS1.p3.2.m2.4.4.4.2.2.2.3.cmml" type="integer" xref="S3.SS1.p3.2.m2.4.4.4.2.2.2.3">2</cn></apply></interval></apply><ci id="S3.SS1.p3.2.m2.6.6.8.cmml" xref="S3.SS1.p3.2.m2.6.6.8">⋯</ci><apply id="S3.SS1.p3.2.m2.6.6.6.cmml" xref="S3.SS1.p3.2.m2.6.6.6"><times id="S3.SS1.p3.2.m2.6.6.6.3.cmml" xref="S3.SS1.p3.2.m2.6.6.6.3"></times><ci id="S3.SS1.p3.2.m2.6.6.6.4.cmml" xref="S3.SS1.p3.2.m2.6.6.6.4">𝑇</ci><interval closure="open" id="S3.SS1.p3.2.m2.6.6.6.2.3.cmml" xref="S3.SS1.p3.2.m2.6.6.6.2.2"><apply id="S3.SS1.p3.2.m2.5.5.5.1.1.1.cmml" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.5.5.5.1.1.1.1.cmml" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.5.5.5.1.1.1.2.cmml" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1.2">𝑋</ci><ci id="S3.SS1.p3.2.m2.5.5.5.1.1.1.3.cmml" xref="S3.SS1.p3.2.m2.5.5.5.1.1.1.3">𝑛</ci></apply><apply id="S3.SS1.p3.2.m2.6.6.6.2.2.2.cmml" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.6.6.6.2.2.2.1.cmml" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2">subscript</csymbol><ci id="S3.SS1.p3.2.m2.6.6.6.2.2.2.2.cmml" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2.2">𝑌</ci><ci id="S3.SS1.p3.2.m2.6.6.6.2.2.2.3.cmml" xref="S3.SS1.p3.2.m2.6.6.6.2.2.2.3">𝑛</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.6c">T(X_{1},Y_{1})\oplus T(X_{2},Y_{2})\oplus\cdots\oplus T(X_{n},Y_{n})</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.6d">italic_T ( italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ⊕ italic_T ( italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ⊕ ⋯ ⊕ italic_T ( italic_X start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_Y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="\oplus" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><mo id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">⊕</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><csymbol cd="latexml" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">direct-sum</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">\oplus</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">⊕</annotation></semantics></math> refers to concatenation, and <math alttext="n" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><mi id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><ci id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_n</annotation></semantics></math> represents the number of samples <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib12" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Our approach to In-Context Learning utilizes 3-shot learning, where the prompt to the model is structured as illustrated in Figure 1.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="29" id="S3.F1.g1" src="x1.jpg" width="149"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A sample prompt to the In-Context Learning</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">For the evaluation of the In-context Learning, we used Hindi, Malayalam, Telugu, Tamil and Marathi languages. A 3-short learning using XGLM and mT5 is used with BLEU metric.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Dataset Used</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">We used BPCC Wiki MT Dataset<cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib14" title="">AI4Bharat </a></cite> which had 16k-50k translation samples. It has English to 22 other Indian Language Pairs with a context length of each sentence pair being 40-200 characters long.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>ICL Experimental Results</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The Architectures that we used are 1. Decoder Only - XGLM. The reason for XGLM is it generates moderate translation with 500 million parameters and builds bi-lingual mapping between non-English and English <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib12" title="">2023</a>)</cite>. 2. Encoder-Decoder - mT5 because of its capability for multilingual translation (mT5-base) and contains 300 million parameters. A sample reference and its prediction in mT5 and XGLM is given in Figure 2. The experimental results of In-Context Learning for Decoder-only and Encoder-Decoder Architectures are given in Figure 3.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="46" id="S3.F2.g1" src="x2.jpg" width="149"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A sample reference results for the Predicted XGLM and mT5</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="97" id="S3.F3.g1" src="x3.jpg" width="143"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Experimental Results of In-Context Learning - Language Pairs and their BLEU Scores</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Finetuning</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="46" id="S3.F4.g1" src="extracted/5849279/images/Enc_Dec_Workflow.png" width="280"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Workflow of mT5 Fine-tuning (Decoder only model)</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="155" id="S3.F5.g1" src="extracted/5849279/images/Dec_Workflow.png" width="419"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Workflow of Llama2 Fine-tuning (Decoder only model)</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we describe the fine-tuning processes of the mT5 and LLaMA 2 models for multilingual and bilingual machine translation tasks. These models were selected because of their superior performance in natural language processing tasks. We meticulously detailed the workflow and evaluation metrics used to gauge their effectiveness.
We began with the mT5 model, an encoder-decoder model fine-tuned for translation tasks involving English to Hindi (en-hi) and English to Hindi-Bengali (en-hi-bg) pairs. The process started with data loading, where we prepared datasets for the specified language pairs. The mT5 model was selected for its robust multilingual capability. We configured the training parameters, including hyperparameters, learning rate, batch size, and optimization algorithms, to ensure an optimal setup for fine-tuning. The model was fine-tuned on the datasets, allowing it to adapt to the translation tasks. The evaluation metrics used for the fine-tuning task were BLEU, chrF, and TER.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Next, we focused on the LLaMA 2 model, a decoder-only model fine-tuned for English-to-Hindi (en-hi) and English-to-Hindi-Malayalam (en-hi-ml) translation tasks. The process began with data loading for the specified language pairs. The LLaMA-2-7b-hf model was loaded with quantization (QLoRA) to enhance the performance and reduce computational demands. Quantization with QLoRA helps in reducing the model size and computational requirements without significantly compromising the model’s performance. We set up LoRA configurations to facilitate efficient fine-tuning, followed by establishing training configurations, including hyperparameters and optimization settings. The model was then fine-tuned on the datasets with evaluation metrics, such as BLEU scores, used to assess performance. The LLaMA 2 model exhibited superior performance in one-to-one (1-1) translation tasks compared with one-to-many (1-many) tasks. Visualization of the loss over time indicated a steady decrease, confirming effective fine-tuning for both the bilingual and multilingual mT5 models. To feed data into the models, we used specific prompt templates: source text #hi#&gt; target text for English-to-Hindi translations and source text #ml#&gt; target text for English-to-Malayalam translations, where #hi#&gt; and #ml#&gt; signify the target language.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Finetuning Experimental Results</h4>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S3.F6.sf1.g1" src="extracted/5849279/images/mT5_multilingual.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>mT5 Multilingual eng, hi, bg (all pairs)</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="464" id="S3.F6.sf2.g1" src="extracted/5849279/images/mT5_bi-lingual_en-hi_.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>mT5 Bi-lingual eng-hi one-to-one</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Loss Convergence plots for Fine-tuning</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">For the fine-tuning task, the mT5 model demonstrated significant performance with a BLEU score of 14.1444 and a chrF score of 33.8278 for the bilingual translation task between English and Hindi. This indicates the model’s strong capability in handling the en-hi translation pair. Additionally, we extended our experiments to include six different combinations involving English, Hindi, and Bengali for the multilingual mT5 model, exploring various language pairings to assess the model’s robustness across different translation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">In the case of the LLaMA 2 model, our evaluations revealed that the 1-1 configuration outperformed the 1-many model. This suggests that the one-to-one translation setup is more effective for maintaining high translation quality, outperforming the many-to-one setup in our experiments.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="656" id="S3.F7.g1" src="extracted/5849279/images/BLEU_pic.png" width="1003"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Results for Finetuning the models</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Baseline Model Development</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The baseline models for this study are built upon pre-trained models that have been trained on extensive datasets. Specifically, we utilize the mT5 model, which is pre-trained to understand multiple languages. This inherent multi-lingual capability of mT5 is leveraged in our fine-tuning process to adapt the model to the specific tasks at hand.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">However, the problem statement for our study remains incomplete. Our primary objective is to compare the performance of Encoder-Decoder and Decoder-only models under similar training conditions. This comparison aims to evaluate the effectiveness of these models in multi-task learning scenarios, particularly in multi-lingual machine translation (MT). To ensure a comprehensive comparison, we will examine not only the overall performance metrics of the models but also their ability to handle different context lengths. Quantitative metrics will be employed to measure and interpret the models’ performance with varying context lengths, providing deeper insights into their strengths and limitations. This thorough evaluation will help us understand which model architecture is better suited for multi-lingual and multi-task learning applications.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="177" id="S3.F8.g1" src="extracted/5849279/images/Proposed_Methodology.drawio-7.png" width="260"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Proposed Methodology for Baseline Model Comparison</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The proposed methodology of our multi-lingual Encoder-Decoder based architecture can be viewed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F8" title="Figure 8 ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">8</span></a>. We used the same datasets from the IndicTrans2 <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib8" title="">2023</a>)</cite> which is in-tune with our Indian baselines study. We go to the next step which is data-curation where we curate the dataset for better translation.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">We wanted to write a model from scratch which we able to provide some construction, as using the pretrained model is more black boxed and less interpretable.
As creating and experimenting with the model comes out with their own challenges, we took some stable baseline models and we equated the parameters.
We use XLNet as a base model for implementing the MT task based learning as a Decoder-only model (Wu et al, 2021) and for Encoder-Decoder only model we use the IndicBART as a base model (Dabre et al, 2021) with the shared tokenizer.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1">Model Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1">Trainable Parameters</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.2.1.1">XLNet Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.1.2.1.2">147,490,318</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.3.2.1">Indic-BART Baseline</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.3.2.2">145,339,392</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Trainable parameters for XLNet and Indic-BART baseline models</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">We utilized the datasets from IndicTrans2 <cite class="ltx_cite ltx_citemacro_cite">Gala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib8" title="">2023</a>)</cite>, aligning with our focus on Indian language baselines. Following dataset selection, we curated the data to enhance translation quality. Our initial intention was to develop a model from scratch to ensure transparency and interpretability, as pre-trained models often function as black boxes. However, building and experimenting with custom models introduces significant challenges. To address these, we employed stable baseline models and ensured parameter equivalence for fair comparison. For the Decoder-only model, we chose XLNet as the base, implementing it for multi-task learning in machine translation, as demonstrated by <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib9" title="">2021</a>)</cite> For the Encoder-Decoder model, we used IndicBART as the foundation, based on the work of <cite class="ltx_cite ltx_citemacro_cite">Dabre et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#bib.bib10" title="">2021</a>)</cite>, Both models shared a common tokenizer to maintain consistency in data processing. This approach allowed us to systematically compare the performance and interpretability of Decoder-only and Encoder-Decoder models under similar conditions, providing insights into their respective strengths and weaknesses in multi-lingual and multi-task learning scenarios. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.T1" title="Table 1 ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">1</span></a>, the XLNet Baseline model has 147,490,318 trainable parameters, while the Indic-BART Baseline model has 145,339,392 parameters.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Baseline Experimental Results</h4>
<figure class="ltx_figure" id="S3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F9.sf1.g1" src="extracted/5849279/images/enc_dec_one2one.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Encoder-Decoder One-to-One Eng to Hindi Results</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F9.sf2.g1" src="extracted/5849279/images/dec_only_one2one.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Decoder only One-to-One Eng to Hindi Results</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Loss Convergence, BLEU, chrF, TER</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F10.sf1.g1" src="extracted/5849279/images/dec_only_one2many.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Decoder only One-to-Many English to (Hindi, Marathi) Translation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F10.sf2.g1" src="extracted/5849279/images/dec_only_many2one.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Decoder only Many-to-One (Hindi, Marathi) to English Translation</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F10.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F10.sf3.g1" src="extracted/5849279/images/enc_dec_one2many.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Encoder-Decoder One-to-Many English to (Hindi, Marathi) Translation</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F10.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="S3.F10.sf4.g1" src="extracted/5849279/images/enc_dec_one2many.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Many-to-One (Hindi, Marathi) to English Translation</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Loss Convergence, BLEU, chrF, TER</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">In this section, we present the experimental results of our study, focusing on comparisons between one-to-one, one-to-many, many-to-one Encoder-Decoder, and Decoder-only models. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F10" title="Figure 10 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">10</span></a> illustrates the performance comparison between one-to-one and one-to-many Encoder-Decoder models. The BLEU scores for one-to-many model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F10" title="Figure 10 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">10</span></a>(a)) indicate a slight improvement in translation quality over the one-to-one model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F10" title="Figure 10 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">10</span></a>(b)), particularly in handling multiple outputs from a single input. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F9" title="Figure 9 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">9</span></a> presents the results for many-to-one and Decoder-only models. The many-to-one model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F9" title="Figure 9 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">9</span></a>(a)) shows robust performance in aggregating multiple inputs into a single output, while the Decoder-only model (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.13747v1#S3.F9" title="Figure 9 ‣ 3.3.1 Baseline Experimental Results ‣ 3.3 Baseline Model Development ‣ 3 Proposed Methodology and Experimental Results ‣ Machine Translation with Large Language Models: Decoder Only vs. Encoder-Decoder"><span class="ltx_text ltx_ref_tag">9</span></a>(b)) excels in generating fluent translations with reduced computational complexity.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The Encoder-Decoder model has demonstrated reliable performance in our experiments, providing trustworthy results. However, the training paradigms for Decoder-only models differ significantly, as they are typically trained on next-word or next-character prediction tasks. The inherent differences in learning paradigms raise the question of how to achieve convergence in multilingual machine translation. Decoder-only models handle the starting positions of the source and target texts separately, posing unique challenges.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Additionally, the development of novel methods such as Streaming Self-Attention (SSA) marks a significant advancement. SSA enables the model to determine when it has sufficient context from the original text to begin translating accurately. This technique addresses some of the inherent challenges in translating long texts and could be crucial for improving the performance of Decoder-only models in multilingual settings. Future work should focus on refining these models and exploring ways to harmonize the learning paradigms of Encoder-Decoder and Decoder-only architectures. Further research is needed to fully exploit the potential of SSA and other innovative techniques to enhance translation accuracy and efficiency. This research will contribute to the broader goal of advancing machine translation technologies for multilingual applications.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We extend our heartfelt gratitude to the IIITH Administration for providing excellent resources and amenities, which have been instrumental in the successful completion of this work. We are deeply appreciative of the guidance and support from the IASNLP Coordinators, Dr. Parameswari and Dr. Rahul Mishra, whose expertise and encouragement have greatly contributed to our research. Our sincere thanks go to our mentor, Mr. Yash Bhaskar, for his invaluable advice and mentorship throughout this project. We would also like to acknowledge the contributions of our pre-school speakers: Prashanth Kodali, Aparajitha, Priyanka Dasari, Aadya Ranjan, and Sankalp Bahad, whose insights and feedback have been immensely helpful.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aho and Ullman (1972)</span>
<span class="ltx_bibblock">
Alfred V. Aho and Jeffrey D. Ullman. 1972.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">The Theory of Parsing, Translation and Compiling</span>, volume 1.

</span>
<span class="ltx_bibblock">Prentice-Hall, Englewood Cliffs, NJ.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">American Psychological Association (1983)</span>
<span class="ltx_bibblock">
American Psychological Association. 1983.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Publications Manual</span>.

</span>
<span class="ltx_bibblock">American Psychological Association, Washington, DC.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandra et al. (1981)</span>
<span class="ltx_bibblock">
Ashok K. Chandra, Dexter C. Kozen, and Larry J. Stockmeyer. 1981.

</span>
<span class="ltx_bibblock">Alternation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">Journal of the Association for Computing Machinery</span>, 28(1):114–133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andrew and Gao (2007)</span>
<span class="ltx_bibblock">
Galen Andrew and Jianfeng Gao. 2007.

</span>
<span class="ltx_bibblock">Scalable training of <math alttext="L_{1}" class="ltx_Math" display="inline" id="bib.bib4.1.m1.1"><semantics id="bib.bib4.1.m1.1a"><msub id="bib.bib4.1.m1.1.1" xref="bib.bib4.1.m1.1.1.cmml"><mi id="bib.bib4.1.m1.1.1.2" xref="bib.bib4.1.m1.1.1.2.cmml">L</mi><mn id="bib.bib4.1.m1.1.1.3" xref="bib.bib4.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="bib.bib4.1.m1.1b"><apply id="bib.bib4.1.m1.1.1.cmml" xref="bib.bib4.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib4.1.m1.1.1.1.cmml" xref="bib.bib4.1.m1.1.1">subscript</csymbol><ci id="bib.bib4.1.m1.1.1.2.cmml" xref="bib.bib4.1.m1.1.1.2">𝐿</ci><cn id="bib.bib4.1.m1.1.1.3.cmml" type="integer" xref="bib.bib4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib4.1.m1.1c">L_{1}</annotation><annotation encoding="application/x-llamapun" id="bib.bib4.1.m1.1d">italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>-regularized log-linear models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.2.1">Proceedings of the 24th International Conference on Machine Learning</span>, pages 33–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gusfield (1997)</span>
<span class="ltx_bibblock">
Dan Gusfield. 1997.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Algorithms on Strings, Trees and Sequences</span>.

</span>
<span class="ltx_bibblock">Cambridge University Press, Cambridge, UK.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasooli and Tetreault (2015)</span>
<span class="ltx_bibblock">
Mohammad Sadegh Rasooli and Joel R. Tetreault. 2015.

</span>
<span class="ltx_bibblock">Yara Parser: A fast and accurate dependency parser.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Computing Research Repository</span>, arXiv:1503.06733.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ando and Zhang (2005)</span>
<span class="ltx_bibblock">
Rie Kubota Ando and Tong Zhang. 2005.

</span>
<span class="ltx_bibblock">A framework for learning predictive structures from multiple tasks and unlabeled data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Journal of Machine Learning Research</span>, 6:1817–1853.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gala et al. (2023)</span>
<span class="ltx_bibblock">
Jay Gala, Pranjal A Chitale, A K Raghavan, Varun Gumma, Sumanth Doddapaneni, Aswanth Kumar M, Janki Atul Nawale, Anupama Sujatha, Ratish Puduppully, Vivek Raghavan, Pratyush Kumar, Mitesh M Khapra, Raj Dabre, and Anoop Kunchukuttan. 2023.

</span>
<span class="ltx_bibblock">IndicTrans2: Towards high-quality and accessible machine translation models for all 22 scheduled Indian languages.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Transactions on Machine Learning Research</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2021)</span>
<span class="ltx_bibblock">
Nier Wu, Hongxu Hou, Ziyue Guo, and Wei Zheng. 2021.

</span>
<span class="ltx_bibblock">Low-resource neural machine translation using XLNet pre-training model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Artificial Neural Networks and Machine Learning–ICANN 2021: 30th International Conference on Artificial Neural Networks</span>, pages 503–514. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dabre et al. (2021)</span>
<span class="ltx_bibblock">
Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M Khapra, and Pratyush Kumar. 2021.

</span>
<span class="ltx_bibblock">IndicBART: A pre-trained model for indic natural language generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2109.02903</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Stanford.

</span>
<span class="ltx_bibblock">Understanding In-Context Learning.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.stanford.edu/blog/understanding-incontext/" title="">https://ai.stanford.edu/blog/understanding-incontext/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023.

</span>
<span class="ltx_bibblock">Multilingual machine translation with large language models: Empirical results and analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2304.04675</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Zhenyu Wu, YaoXiang Wang, Jiacheng Ye, Jiangtao Feng, Jingjing Xu, Yu Qiao, and Zhiyong Wu. 2023.

</span>
<span class="ltx_bibblock">Openicl: An open-source framework for in-context learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2303.02913</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(14)</span>
<span class="ltx_bibblock">
AI4Bharat.

</span>
<span class="ltx_bibblock">BPCC.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai4bharat.iitm.ac.in/bpcc/" title="">https://ai4bharat.iitm.ac.in/bpcc/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2019)</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1903.00089</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arivazhagan et al. (2019)</span>
<span class="ltx_bibblock">
Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, and others. 2019.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation in the wild: Findings and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:1907.05019</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2023.

</span>
<span class="ltx_bibblock">Multilingual machine translation with large language models: Empirical results and analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2304.04675</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:1810.04805</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</span>, 30.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 00:20:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
