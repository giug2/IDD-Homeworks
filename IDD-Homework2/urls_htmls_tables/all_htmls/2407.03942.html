<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.03942] Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data</title><meta property="og:description" content="Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilit…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.03942">

<!--Generated on Mon Aug  5 13:38:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zihui Gu<sup id="id1.1.id1" class="ltx_sup">1,2</sup><span id="id2.2.id2" class="ltx_ERROR undefined">\equalcontrib</span>, Xingwu Sun<sup id="id3.3.id3" class="ltx_sup">2,3</sup><span id="id4.4.id4" class="ltx_ERROR undefined">\equalcontrib</span>, Fengzong Lian<sup id="id5.5.id5" class="ltx_sup">2</sup>, Zhanhui Kang<sup id="id6.6.id6" class="ltx_sup">2</sup>, Cheng-Zhong Xu<sup id="id7.7.id7" class="ltx_sup">3</sup>, Ju Fan<sup id="id8.8.id8" class="ltx_sup">1</sup>
<br class="ltx_break">
</span><span class="ltx_author_notes">Ju Fan is the corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Instruction-following is particularly crucial for large language models (<span id="id9.id1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s) to support diverse user requests. While existing work has made progress in aligning <span id="id9.id1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s with human preferences, evaluating their capabilities on instruction-following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, <span id="id9.id1.3" class="ltx_text ltx_font_italic">i.e.,</span> lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces <span id="id9.id1.4" class="ltx_text ltx_font_smallcaps">DINGO</span>, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) <span id="id9.id1.5" class="ltx_text ltx_font_smallcaps">DINGO</span> is based on a manual-annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) <span id="id9.id1.6" class="ltx_text ltx_font_smallcaps">DINGO</span> includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that <span id="id9.id1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> can not only provide more challenging and comprehensive evaluation for <span id="id9.id1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, but also provide task-level fine-grained directions to further improve <span id="id9.id1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, Large language models (<span id="S1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s) exhibit surprising capabilities not previously seen in smaller models, which are often referred to as <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">emergent abilities</span> <cite class="ltx_cite ltx_citemacro_citep">(Wei et al. <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, including <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">in-context learning</span>, <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">chain-of-thought</span>, and <span id="S1.p1.1.5" class="ltx_text ltx_font_italic">instruction-following</span> abilities. Among them, the <span id="S1.p1.1.6" class="ltx_text ltx_font_italic">instruction-following</span> ability is crucial to the interaction between humans and <span id="S1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S1.p1.1.8" class="ltx_text ltx_font_italic">e.g.,</span> ChatGPT). Existing studies <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Wang et al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Longpre et al. <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> align <span id="S1.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s with human instructions using supervised instruction-tuning or reinforcement learning from human feedback (RLHF), which enables <span id="S1.p1.1.10" class="ltx_text ltx_font_smallcaps">LLM</span>s to understand human instructions and make high-quality responses. Nonetheless, due to the complexity and diversity of human instructions, it remains a challenge to comprehensively evaluate the <span id="S1.p1.1.11" class="ltx_text ltx_font_italic">instruction-following</span> ability of <span id="S1.p1.1.12" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.03942/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="507" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Different user request examples extracted from ShareGPT.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing studies evaluate the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">instruction-following</span> ability from the perspective of general skills. For example, InstructEval <cite class="ltx_cite ltx_citemacro_citep">(Chia et al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> assesses <span id="S1.p2.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>’s instruction-following ability based on three general abilities: problem-solving, writing, and alignment to human values.
Flask <cite class="ltx_cite ltx_citemacro_citep">(Ye et al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> shifts the original coarse-grained scoring process to instance-wise skill scoring setup, and defines 4 primary abilities, divided into 12 specific skills, to assess the performance of <span id="S1.p2.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. However, there are still two shortcomings in existing evaluation methods:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">lack of fine-grained task-level evaluation</span> poses challenges in improving the instruction-following ability of <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s. For example, the <span id="S1.I1.i1.p1.1.3" class="ltx_text ltx_font_typewriter">Factuality</span> skill used in FLASK <cite class="ltx_cite ltx_citemacro_citep">(Ye et al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> includes many sub-tasks such as “History Knowledge QA” and “Chemical Knowledge QA”.
Consequently, even if we recognize that a particular <span id="S1.I1.i1.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span> is deficient in this skill, it is challenging to pinpoint the exact aspects of the instruction-following ability that the <span id="S1.I1.i1.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span> needs to be improved.
Specifically, if the performance of the <span id="S1.I1.i1.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span> is not satisfactory in “Chemical Knowledge QA”, it is not clear whether this is because the <span id="S1.I1.i1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>’s response contains non-standard chemical formulas.
Similarly, if the <span id="S1.I1.i1.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span> cannot perform well in “History Knowledge QA”, it could potentially be because the key points are not clearly outlined in the <span id="S1.I1.i1.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>’s response.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">expression of instructions tends to be singular</span>, resulting in a gap between real-world user instructions and existing evaluation datasets. Existing datasets <cite class="ltx_cite ltx_citemacro_citep">(Chia et al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> often use previous NLP datasets as evaluation data for specific skills, such as employing DROP <cite class="ltx_cite ltx_citemacro_citep">(Dua et al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> to evaluate the <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">Comprehension</span> ability, and design a specific instruction template for the dataset. However, in real-world scenarios, users express their requests in various ways. Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows several examples extracted from the ShareGPT website, a platform where users voluntarily share their interaction records with <span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. As can be seen, the styles and attitudes of user instructions are very diverse: users may ask questions directly (<span id="S1.I1.i2.p1.1.4" class="ltx_text ltx_font_italic">i.e.,</span> Concise) or set specific roles to ask questions (<span id="S1.I1.i2.p1.1.5" class="ltx_text ltx_font_italic">i.e.,</span> Role-play). Therefore, it could be very beneficial to evaluate the <span id="S1.I1.i2.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>’s instruction-following ability on these diverse instruction expressions.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address the aforementioned shortcomings, in this paper, we present <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, a <span id="S1.p4.1.2" class="ltx_text ltx_framed ltx_framed_underline">D</span>iverse and F<span id="S1.p4.1.3" class="ltx_text ltx_framed ltx_framed_underline">i</span>ne-grained I<span id="S1.p4.1.4" class="ltx_text ltx_framed ltx_framed_underline">n</span>struction-Followin<span id="S1.p4.1.5" class="ltx_text ltx_framed ltx_framed_underline">g</span> evaluation dataset. First, to support fine-grained instruction-following evaluation, we manually annotate a multi-level category tree with 130 nodes and 4 levels, based on the user instructions extracted from ShareGPT. This category tree encompasses tasks that users would want <span id="S1.p4.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s to complete in real-world scenarios, making it highly practical. Equipped with its multi-level structure, the category tree supports analyzing instruction-following ability at different granularities, and thus can address the shortcomings of <span id="S1.p4.1.7" class="ltx_text ltx_font_smallcaps">LLM</span> at task-level. Second, we prepare diverse instruction data for each category to comprehensively examine the instruction-following ability. Considering that user requests on ShareGPT have been used for instruction-tuning in many <span id="S1.p4.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> and TÜLU <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, we avoid data leakage by not directly using data from ShareGPT for evaluation. Instead, we employ GPT-4 to simulate various instruction styles, attitudes, and languages derived from ShareGPT, and generate diverse instruction data for each category. In addition, considering the weaknesses of <span id="S1.p4.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s in mathematics and logical reasoning, we utilize existing human-annotated datasets (<span id="S1.p4.1.10" class="ltx_text ltx_font_italic">e.g.,</span> GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite>) as basic questions and guide GPT-4 to generate diverse instructions from the basic questions to ensure the instruction quality. For example, a math question from GSM8K, <span id="S1.p4.1.11" class="ltx_text ltx_font_italic">“Ronnie was given 5 while Rissa was given thrice as much …”</span> would be transformed by GPT-4 into a role-playing instruction form: <span id="S1.p4.1.12" class="ltx_text ltx_font_italic">“Act as a patient math teacher to answer this question step by step: Ronnie was given 5 while Rissa was given thrice as much …”</span>. Based on the above methods, we, in total, collect 5026 diverse samples in <span id="S1.p4.1.13" class="ltx_text ltx_font_smallcaps">DINGO</span> to comprehensively evaluate the instruction-following ability of <span id="S1.p4.1.14" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Based on <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, we conduct extensive experiments to evaluate instruction-following of 10 different <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, and obtain the following findings.
(1) Even if an instruction-tuned <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">LLM</span> performs well on coarse-grained categories, its performance on fine-grained categories may be diversified and, sometimes, it could even be worse than the base <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">LLM</span> without instruction fine-tuning. (2) Our dataset with diverse instructions presents more significant challenges to <span id="S1.p5.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s to generate responses that align with human preferences.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We publicly release a multi-level task category tree consisting of 130 nodes, designed to support instruction-following evaluations at various granularities.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We collect 5026 diverse and high-quality instructions based on real-world user instructions, presenting more significant challenges for <span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s in generating responses that align with human preferences.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We conduct a comprehensive evaluation on 10 representative <span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s, and the experimental results demonstrate that <span id="S1.I2.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> can support more extensive and challenging evaluation on the instruction-following ability, as well as provide fine-grained guidance to further improve <span id="S1.I2.i3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. We release the <span id="S1.I2.i3.p1.1.4" class="ltx_text ltx_font_smallcaps">DINGO</span> dataset at Github<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/ruc-datalab/DINGO</span></span></span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background: Instruction-Following Ability of Large Language Models</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Language models (LMs) are designed to comprehend and produce text that resembles human language (<span id="S2.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al. <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, GPT2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al. <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>).
Recently, researchers have discovered that scaling LMs to
large LMs (<span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s) (<span id="S2.p1.1.3" class="ltx_text ltx_font_italic">e.g.,</span> ChatGPT, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a href="#bib.bib21" title="" class="ltx_ref">2023a</a>)</cite>) by increasing the model size or amount of training data can significantly enhance their downstream task abilities. Moreover, the existing studies also show that <span id="S2.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s demonstrate surprising abilities that have not been seen in previous smaller LMs <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et al. <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Rae et al. <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Brown et al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, such as <span id="S2.p1.1.5" class="ltx_text ltx_font_italic">in-context learning</span> and <span id="S2.p1.1.6" class="ltx_text ltx_font_italic">instruction-following</span>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Instruction-following</span> is an important ability for <span id="S2.p2.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s to interact with real-world users. This means that the model can complete various tasks based on a wide range of natural language instructions provided by humans, including polishing articles (<span id="S2.p2.1.3" class="ltx_text ltx_font_italic">e.g.,Polish this email above in very urgent tone: </span>{<span id="S2.p2.1.4" class="ltx_text ltx_font_italic">Email</span>}<span id="S2.p2.1.5" class="ltx_text ltx_font_italic">.</span>), solving math problems (<span id="S2.p2.1.6" class="ltx_text ltx_font_italic">e.g.,I need to calculate how long my gas canister will last for 360 burener.</span>), providing travel plans (<span id="S2.p2.1.7" class="ltx_text ltx_font_italic">e.g.,Plan a trip to Jindo for 2 nights and 3 days.</span>), etc. <span id="S2.p2.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s can obtain the instruction-following ability in the following two ways: (1) supervised learning using instruction-following datasets (<span id="S2.p2.1.9" class="ltx_text ltx_font_italic">e.g.,</span> vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>), and (2) reinforcement learning from Human Feedback(<span id="S2.p2.1.10" class="ltx_text ltx_font_italic">e.g.,</span> Llama2-chat <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this work, we aim to evaluate the capabilities of existing <span id="S2.p3.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on instruction-following across a variety of tasks and various instruction expressions, and provide a comprehensive benchmark <span id="S2.p3.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> to promote in-depth analysis of the instruction-following ability of <span id="S2.p3.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span id="S3.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> Dataset</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.03942/assets/figs/pipeline.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A high-level pipeline of Instruction Data Collection.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to generate a fine-grained <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">category tree</span> and diverse <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">instructions</span>. To achieve this goal, we first collect real-world user instructions as seed data. Then, we manually classify the seed data to obtain a fine-grained <span id="S3.p1.1.3" class="ltx_text ltx_font_typewriter">category tree</span>. Finally, based on seed data and <span id="S3.p1.1.4" class="ltx_text ltx_font_typewriter">category tree</span>, we collect diverse <span id="S3.p1.1.5" class="ltx_text ltx_font_typewriter">instructions</span> for each category by guiding GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> to simulate various instruction styles, attitudes, and languages.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Seed Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To obtain real-world instruction-following data, we utilize public data from ShareGPT (https://sharegpt.com/), which is a platform for users to share their interactions with <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> GPT-4). Following previous work <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Wang et al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, we use the ‘html_cleaned‘ version<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/datasets/anon8231489123/
<br class="ltx_break">ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset</span></span></span> and truncate conversations with more than 2048 tokens. Based on this, we obtain 7265 seed samples from ShareGPT.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Category Tree Annotation</h3>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">First-level</span></span>
</span>
</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p" style="width:156.5pt;"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Second-level</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1" class="ltx_p" style="width:56.9pt;">Language Understanding</span>
</span>
</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1" class="ltx_p" style="width:156.5pt;">Relationship Judgement; Classification; Sorting; Error Correction; Joke Explanation; Information Extraction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.1.1.1" class="ltx_p" style="width:56.9pt;">Code</span>
</span>
</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_p" style="width:156.5pt;">Text2Code; Code2Text; Code2Code</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.1.1.1" class="ltx_p" style="width:56.9pt;">Knowledge Unitilization</span>
</span>
</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.2.1.1" class="ltx_p" style="width:156.5pt;">Open-book Questions; Close-book Questions</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.1.1.1" class="ltx_p" style="width:56.9pt;">Creation</span>
</span>
</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.2.1.1" class="ltx_p" style="width:156.5pt;">Thematic creation; Specialized writing; Plan; Non-verbal creation; Simulation creation</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.1.1.1" class="ltx_p" style="width:56.9pt;">Language Generation</span>
</span>
</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.2.1.1" class="ltx_p" style="width:156.5pt;">Question Generation; Rewriting/Paraphrasing; Summary/Abstract/Title; Translation</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.1.1.1" class="ltx_p" style="width:56.9pt;">Mathematics and Reasoning</span>
</span>
</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.2.1.1" class="ltx_p" style="width:156.5pt;">Word Problems; Mathematical theorem; Combinatorics; Mathematical Calculations; Common Sense Reasoning; Logical Reasoning</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The first and second level categories of <span id="S3.T1.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.1.1.1" class="ltx_p" style="width:227.6pt;">/* Task description */</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.5" class="ltx_tr">
<td id="S3.T2.3.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.5.1.1.1" class="ltx_p" style="width:227.6pt;">I need you to simulate the conversation between “human” and “AI”. I will specify some constraints, including …</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.6" class="ltx_tr">
<td id="S3.T2.3.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.6.1.1.1" class="ltx_p" style="width:227.6pt;">/* Demos from seed data */</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Applied Problems; <span id="S3.T2.1.1.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.1.1.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Concise; <span id="S3.T2.1.1.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Command;</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.7" class="ltx_tr">
<td id="S3.T2.3.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.7.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.7.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: Calculate how long my gas …? AI: …</span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.2.2.1.1.1.m1.1.1" xref="S3.T2.2.2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.1.1.m1.1b"><ci id="S3.T2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Combinatorics; <span id="S3.T2.2.2.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.2.2.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Roly-play; <span id="S3.T2.2.2.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Polite;</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.8" class="ltx_tr">
<td id="S3.T2.3.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.8.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.8.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: You are a math teacher, please explain this question step by step: There are two rows in a classroom …? AI: …</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.9" class="ltx_tr">
<td id="S3.T2.3.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.9.1.1.1" class="ltx_p" style="width:227.6pt;">/* Constraints for GPT-4 */</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.3.3.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.3.3.1.1.1.m1.1.1" xref="S3.T2.3.3.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.1.1.m1.1b"><ci id="S3.T2.3.3.1.1.1.m1.1.1.cmml" xref="S3.T2.3.3.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Word Problems; <span id="S3.T2.3.3.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.3.3.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Roly-play; <span id="S3.T2.3.3.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Command; <span id="S3.T2.3.3.1.1.1.5" class="ltx_text ltx_font_bold">Basic Question</span>: Ronnie was given 5 while Rissa was given thrice as much…</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.10" class="ltx_tr">
<td id="S3.T2.3.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T2.3.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.10.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.10.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: Act as a helpful math assistant to answer this question: Ronnie …</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of prompt for GPT-4 for instruction data generation via in-context learning. The content generated by GPT-4 has been highlighted.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Unlike previous work, we focus only on tasks that may appear in real-world user instructions, as this represents what users genuinely want the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s to achieve, providing a more practical evaluation of the instruction-following ability. Thus, we manually annotated the fine-grained task categories of the extracted instruction data from ShareGPT, primarily adhering to the traditional NLP task types commonly defined in previous research <cite class="ltx_cite ltx_citemacro_citep">(Longpre et al. <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; bench authors <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Zhao et al. <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>.
For the convenience of conducting evaluations at different granularities, we design the categories as a multi-level tree structure, which facilitates efficient and in-depth analysis of the capabilities of <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s. Statistically, our category tree comprises 4 levels, with the first level containing <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">6</span> categories, the second level containing <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_bold">25</span> categories, the third level containing <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_bold">65</span> categories, and the fourth level containing <span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_bold">34</span> categories. We present the first and second level categories in Table <a href="#S3.T1" title="Table 1 ‣ 3.2 Category Tree Annotation ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As the goal of this work is to evaluate the performance of <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on various instruction expressions, we also annotate the instruction style, attitude, and language for each instruction sample in the seed data, which are described as follows.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For instruction style, we specify the following five types:</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Inquisitive</span> represents asking multiple questions on the same topic, or delving deeper into a particular question (See the first example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Reflective</span> represents asking multiple questions with the user’s own thoughts and ideas (See the second example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Challenge</span> represents asking multiple questions, which are increasingly difficult (See the third example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Role-play</span> represents setting roles for both <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s and users, and conducting questioning under this setting. (See the fourth example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Concise</span> represents asking a question directly and clearly. (See the fifth example in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">For instruction attitude, we specify three types:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Polite</span> represents asking questions using gentle words, such as “<em id="S3.I2.i1.p1.1.2" class="ltx_emph ltx_font_italic">Could you answer the question…</em>”.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Command</span> represents asking questions in a strong and imperative tone, such as “<em id="S3.I2.i2.p1.1.2" class="ltx_emph ltx_font_italic">Summarize this passage: …</em>”.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Impatient</span> represents urging the <span id="S3.I2.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span> to respond to a certain aspect during the questioning process, such as “<em id="S3.I2.i3.p1.1.3" class="ltx_emph ltx_font_italic">Answer this question directly: …, hurry up!</em>”</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Moreover, for languages, we list all the languages included in the conversation, as users often switch between languages during the conversation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Instruction Data Collection</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Generating diverse instructions is very challenging for human annotators, as it requires (1) the ability to transition between various instruction styles, attitudes, and languages, and (2) the capacity to produce a range of samples within a single category (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> “Grammar-based Rewriting”). Consequently, we propose employing the highly capable <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>, GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, to simulate a variety of user types and generate diverse, high-quality instructions for each category. Please note that we do not directly incorporate instruction data from ShareGPT into our benchmark, because numerous <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">e.g.,</span> Vicuna <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, TÜLU <cite class="ltx_cite ltx_citemacro_citep">(Wang et al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>) have already utilized data from shareGPT for supervised instruction-tuning. Therefore, we only use ShareGPT data as seed data to guide GPT-4. The data collection pipeline is depicted in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">For any leaf category (<span id="S3.SS3.p2.2.1" class="ltx_text ltx_font_italic">e.g.,</span> “Mathematics and Reasoning ”<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo stretchy="false" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\rightarrow</annotation></semantics></math>“Mathematical Calculations”<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mo stretchy="false" id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\rightarrow</annotation></semantics></math> “Algebraic Equation Problems”), we consider the following three steps to collect the instruction-following data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.11" class="ltx_p">In the first step, the goal is to generate constraints to guide GPT-4 to simulate specific user types, thereby preventing generation of unrealistic instructions. To achieve this, we treat the seed data as a sample pool and randomly select two samples as demos of in-context learning, each associated with a particular instruction style (<math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">𝒮</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝒮</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathcal{S}</annotation></semantics></math>), attitude (<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\mathcal{A}</annotation></semantics></math>), and language (<math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">ℒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">ℒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathcal{L}</annotation></semantics></math>). We randomly sample target style, attitude, and language from these two demos to form constraints, compelling GPT-4 to learn from different instruction demos rather than excessively imitating one. For example, the constraints in Figure <a href="#S3.F2" title="Figure 2 ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is <math id="S3.SS3.p3.4.m4.1" class="ltx_math_unparsed" alttext="\{\mathcal{S}=" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1b"><mo stretchy="false" id="S3.SS3.p3.4.m4.1.1">{</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.4.m4.1.2">𝒮</mi><mo id="S3.SS3.p3.4.m4.1.3">=</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\{\mathcal{S}=</annotation></semantics></math>“<math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="Reflective" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mrow id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1a" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.4" xref="S3.SS3.p3.5.m5.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1b" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.5" xref="S3.SS3.p3.5.m5.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1c" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.6" xref="S3.SS3.p3.5.m5.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1d" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.7" xref="S3.SS3.p3.5.m5.1.1.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1e" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.8" xref="S3.SS3.p3.5.m5.1.1.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1f" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.9" xref="S3.SS3.p3.5.m5.1.1.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1g" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.10" xref="S3.SS3.p3.5.m5.1.1.10.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1h" xref="S3.SS3.p3.5.m5.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.5.m5.1.1.11" xref="S3.SS3.p3.5.m5.1.1.11.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><times id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1.1"></times><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">𝑅</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">𝑒</ci><ci id="S3.SS3.p3.5.m5.1.1.4.cmml" xref="S3.SS3.p3.5.m5.1.1.4">𝑓</ci><ci id="S3.SS3.p3.5.m5.1.1.5.cmml" xref="S3.SS3.p3.5.m5.1.1.5">𝑙</ci><ci id="S3.SS3.p3.5.m5.1.1.6.cmml" xref="S3.SS3.p3.5.m5.1.1.6">𝑒</ci><ci id="S3.SS3.p3.5.m5.1.1.7.cmml" xref="S3.SS3.p3.5.m5.1.1.7">𝑐</ci><ci id="S3.SS3.p3.5.m5.1.1.8.cmml" xref="S3.SS3.p3.5.m5.1.1.8">𝑡</ci><ci id="S3.SS3.p3.5.m5.1.1.9.cmml" xref="S3.SS3.p3.5.m5.1.1.9">𝑖</ci><ci id="S3.SS3.p3.5.m5.1.1.10.cmml" xref="S3.SS3.p3.5.m5.1.1.10">𝑣</ci><ci id="S3.SS3.p3.5.m5.1.1.11.cmml" xref="S3.SS3.p3.5.m5.1.1.11">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">Reflective</annotation></semantics></math>”, <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{A}=" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><mrow id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">𝒜</mi><mo id="S3.SS3.p3.6.m6.1.1.1" xref="S3.SS3.p3.6.m6.1.1.1.cmml">=</mo><mi id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><eq id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1"></eq><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">𝒜</ci><csymbol cd="latexml" id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\mathcal{A}=</annotation></semantics></math>“<math id="S3.SS3.p3.7.m7.1" class="ltx_Math" alttext="Polite" display="inline"><semantics id="S3.SS3.p3.7.m7.1a"><mrow id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1" xref="S3.SS3.p3.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1a" xref="S3.SS3.p3.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.7.m7.1.1.4" xref="S3.SS3.p3.7.m7.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1b" xref="S3.SS3.p3.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.7.m7.1.1.5" xref="S3.SS3.p3.7.m7.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1c" xref="S3.SS3.p3.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.7.m7.1.1.6" xref="S3.SS3.p3.7.m7.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1d" xref="S3.SS3.p3.7.m7.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.7.m7.1.1.7" xref="S3.SS3.p3.7.m7.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><times id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1.1"></times><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">𝑃</ci><ci id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">𝑜</ci><ci id="S3.SS3.p3.7.m7.1.1.4.cmml" xref="S3.SS3.p3.7.m7.1.1.4">𝑙</ci><ci id="S3.SS3.p3.7.m7.1.1.5.cmml" xref="S3.SS3.p3.7.m7.1.1.5">𝑖</ci><ci id="S3.SS3.p3.7.m7.1.1.6.cmml" xref="S3.SS3.p3.7.m7.1.1.6">𝑡</ci><ci id="S3.SS3.p3.7.m7.1.1.7.cmml" xref="S3.SS3.p3.7.m7.1.1.7">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">Polite</annotation></semantics></math>”, <math id="S3.SS3.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{L}=" display="inline"><semantics id="S3.SS3.p3.8.m8.1a"><mrow id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml">ℒ</mi><mo id="S3.SS3.p3.8.m8.1.1.1" xref="S3.SS3.p3.8.m8.1.1.1.cmml">=</mo><mi id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><eq id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1.1"></eq><ci id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2">ℒ</ci><csymbol cd="latexml" id="S3.SS3.p3.8.m8.1.1.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">\mathcal{L}=</annotation></semantics></math>“<math id="S3.SS3.p3.9.m9.1" class="ltx_Math" alttext="English" display="inline"><semantics id="S3.SS3.p3.9.m9.1a"><mrow id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml"><mi id="S3.SS3.p3.9.m9.1.1.2" xref="S3.SS3.p3.9.m9.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.3" xref="S3.SS3.p3.9.m9.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1a" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.4" xref="S3.SS3.p3.9.m9.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1b" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.5" xref="S3.SS3.p3.9.m9.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1c" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.6" xref="S3.SS3.p3.9.m9.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1d" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.7" xref="S3.SS3.p3.9.m9.1.1.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1e" xref="S3.SS3.p3.9.m9.1.1.1.cmml">​</mo><mi id="S3.SS3.p3.9.m9.1.1.8" xref="S3.SS3.p3.9.m9.1.1.8.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><apply id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1"><times id="S3.SS3.p3.9.m9.1.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1.1"></times><ci id="S3.SS3.p3.9.m9.1.1.2.cmml" xref="S3.SS3.p3.9.m9.1.1.2">𝐸</ci><ci id="S3.SS3.p3.9.m9.1.1.3.cmml" xref="S3.SS3.p3.9.m9.1.1.3">𝑛</ci><ci id="S3.SS3.p3.9.m9.1.1.4.cmml" xref="S3.SS3.p3.9.m9.1.1.4">𝑔</ci><ci id="S3.SS3.p3.9.m9.1.1.5.cmml" xref="S3.SS3.p3.9.m9.1.1.5">𝑙</ci><ci id="S3.SS3.p3.9.m9.1.1.6.cmml" xref="S3.SS3.p3.9.m9.1.1.6">𝑖</ci><ci id="S3.SS3.p3.9.m9.1.1.7.cmml" xref="S3.SS3.p3.9.m9.1.1.7">𝑠</ci><ci id="S3.SS3.p3.9.m9.1.1.8.cmml" xref="S3.SS3.p3.9.m9.1.1.8">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">English</annotation></semantics></math>”<math id="S3.SS3.p3.10.m10.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S3.SS3.p3.10.m10.1a"><mo stretchy="false" id="S3.SS3.p3.10.m10.1.1" xref="S3.SS3.p3.10.m10.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.10.m10.1b"><ci id="S3.SS3.p3.10.m10.1.1.cmml" xref="S3.SS3.p3.10.m10.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.10.m10.1c">\}</annotation></semantics></math>. Additionally, given that GPT-4 may struggle to generate high-quality mathematical or logical reasoning questions, we gather data from previous task-specific benchmarks as basic questions, which are then incorporated as part of the constraints. For example, we use the GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite> dataset as a basic question source for the “Mathematics and Reasoning” <math id="S3.SS3.p3.11.m11.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.11.m11.1a"><mo stretchy="false" id="S3.SS3.p3.11.m11.1.1" xref="S3.SS3.p3.11.m11.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.11.m11.1b"><ci id="S3.SS3.p3.11.m11.1.1.cmml" xref="S3.SS3.p3.11.m11.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.11.m11.1c">\rightarrow</annotation></semantics></math>“Word Problems” category. More details of the existing dataset resources included in <span id="S3.SS3.p3.11.1" class="ltx_text ltx_font_smallcaps">DINGO</span> are listed in Table <a href="#S3.T3" title="Table 3 ‣ 3.3 Instruction Data Collection ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.1.2.1" class="ltx_text ltx_font_bold">Basic Question Source</span></td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Word Problems</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Mathematical Theorem</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center">ProofNet <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev et al. <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Combinatorics</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center">Math <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al. <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Numerical Calculation</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center">Math <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al. <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Common Sense Reasoning</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center">StrategyQA <cite class="ltx_cite ltx_citemacro_citep">(Geva et al. <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_align_center ltx_border_r">Logical Reasoning</td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_align_center">LogiQA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al. <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.8" class="ltx_tr">
<td id="S3.T3.1.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Text2Code</td>
<td id="S3.T3.1.8.2" class="ltx_td ltx_align_center ltx_border_b">MBPP <cite class="ltx_cite ltx_citemacro_citep">(Austin et al. <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The basic question source of <span id="S3.T3.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The goal of the second step is for GPT-4 to simulate a real-world user and generate high-quality instructions by adhering to the constraints. We use in-context learning to achieve this goal. As illustrated in Table <a href="#S3.T2" title="Table 2 ‣ 3.2 Category Tree Annotation ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we combine the task description, the two demos obtained from the first step, and the target constraints as input context for GPT-4. As demonstrated, GPT-4 learns different expressions from two demos and transforms the basic question into specific instruction “<em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">Act as a helpful math assistant to …</em>” based on the <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_bold">Role-play</span> and <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_bold">Command</span> constraints. Following previous work <cite class="ltx_cite ltx_citemacro_citep">(Wiegreffe et al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Yuan et al. <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, we adopt the <span id="S3.SS3.p4.1.4" class="ltx_text ltx_font_italic">over-generate-then-filter</span> approach to obtain higher quality instructions. Thus, in this step, we prompt GPT-4 to make two predictions based on the same input, generating two instruction candidates.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">In the third step, the objective is to select faithful and diverse instructions. We consider two selection methods, constraint-based pair-wise selection and similarity-based selection. Specifically, we first use GPT-4 to determine which of the two candidates adheres more closely to the constraints. We require GPT-4 to choose from three options, <math id="S3.SS3.p5.1.m1.3" class="ltx_Math" alttext="\{first,second,tie\}" display="inline"><semantics id="S3.SS3.p5.1.m1.3a"><mrow id="S3.SS3.p5.1.m1.3.3.3" xref="S3.SS3.p5.1.m1.3.3.4.cmml"><mo stretchy="false" id="S3.SS3.p5.1.m1.3.3.3.4" xref="S3.SS3.p5.1.m1.3.3.4.cmml">{</mo><mrow id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1a" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.4" xref="S3.SS3.p5.1.m1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1b" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.5" xref="S3.SS3.p5.1.m1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1c" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.6" xref="S3.SS3.p5.1.m1.1.1.1.1.6.cmml">t</mi></mrow><mo id="S3.SS3.p5.1.m1.3.3.3.5" xref="S3.SS3.p5.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p5.1.m1.2.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.3" xref="S3.SS3.p5.1.m1.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1a" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1b" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.5" xref="S3.SS3.p5.1.m1.2.2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1c" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.6" xref="S3.SS3.p5.1.m1.2.2.2.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1d" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.7" xref="S3.SS3.p5.1.m1.2.2.2.2.7.cmml">d</mi></mrow><mo id="S3.SS3.p5.1.m1.3.3.3.6" xref="S3.SS3.p5.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.1.m1.3.3.3.3" xref="S3.SS3.p5.1.m1.3.3.3.3.cmml"><mi id="S3.SS3.p5.1.m1.3.3.3.3.2" xref="S3.SS3.p5.1.m1.3.3.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.3.3.3.3.1" xref="S3.SS3.p5.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.3.3.3.3.3" xref="S3.SS3.p5.1.m1.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.3.3.3.3.1a" xref="S3.SS3.p5.1.m1.3.3.3.3.1.cmml">​</mo><mi id="S3.SS3.p5.1.m1.3.3.3.3.4" xref="S3.SS3.p5.1.m1.3.3.3.3.4.cmml">e</mi></mrow><mo stretchy="false" id="S3.SS3.p5.1.m1.3.3.3.7" xref="S3.SS3.p5.1.m1.3.3.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.3b"><set id="S3.SS3.p5.1.m1.3.3.4.cmml" xref="S3.SS3.p5.1.m1.3.3.3"><apply id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1"><times id="S3.SS3.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.1"></times><ci id="S3.SS3.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.2">𝑓</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.3">𝑖</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.4.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.4">𝑟</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.5.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.5">𝑠</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.6.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.6">𝑡</ci></apply><apply id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2"><times id="S3.SS3.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.1"></times><ci id="S3.SS3.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.2">𝑠</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.3">𝑒</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.4.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.4">𝑐</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.5.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.5">𝑜</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.6.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.6">𝑛</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.7.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.7">𝑑</ci></apply><apply id="S3.SS3.p5.1.m1.3.3.3.3.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3"><times id="S3.SS3.p5.1.m1.3.3.3.3.1.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.1"></times><ci id="S3.SS3.p5.1.m1.3.3.3.3.2.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.2">𝑡</ci><ci id="S3.SS3.p5.1.m1.3.3.3.3.3.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.3">𝑖</ci><ci id="S3.SS3.p5.1.m1.3.3.3.3.4.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.4">𝑒</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.3c">\{first,second,tie\}</annotation></semantics></math>, and provide a rationale. Next, to ensure diversity, we calculate the similarity between the best candidate and the collected data in the dataset. Then, we only add the candidate to the dataset if the maximum ROUGE-L similarity is less than 0.6.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset Analysis</h3>

<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.2.1" class="ltx_text ltx_font_bold">#Tasks</span></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.3.1" class="ltx_text ltx_font_bold">#Samples</span></td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.1" class="ltx_text ltx_font_bold">#Turns</span></td>
<td id="S3.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.1.1.5.1" class="ltx_text ltx_font_bold">#Input length</span></td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mathematics and Reasoning</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">432</td>
<td id="S3.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6</td>
<td id="S3.T4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">83.4</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Language Understanding</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_r">11</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_r">530</td>
<td id="S3.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_r">2.0</td>
<td id="S3.T4.1.3.5" class="ltx_td ltx_align_center">125.7</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Language Generation</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_center ltx_border_r">14</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_r">730</td>
<td id="S3.T4.1.4.4" class="ltx_td ltx_align_center ltx_border_r">1.9</td>
<td id="S3.T4.1.4.5" class="ltx_td ltx_align_center">157.1</td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Knowledge Utilization</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_r">34</td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_r">1596</td>
<td id="S3.T4.1.5.4" class="ltx_td ltx_align_center ltx_border_r">2.6</td>
<td id="S3.T4.1.5.5" class="ltx_td ltx_align_center">72.7</td>
</tr>
<tr id="S3.T4.1.6" class="ltx_tr">
<td id="S3.T4.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Creation</td>
<td id="S3.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_r">31</td>
<td id="S3.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_r">1498</td>
<td id="S3.T4.1.6.4" class="ltx_td ltx_align_center ltx_border_r">1.9</td>
<td id="S3.T4.1.6.5" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="S3.T4.1.7" class="ltx_tr">
<td id="S3.T4.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Code</td>
<td id="S3.T4.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">5</td>
<td id="S3.T4.1.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">240</td>
<td id="S3.T4.1.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2.1</td>
<td id="S3.T4.1.7.5" class="ltx_td ltx_align_center ltx_border_b">105.5</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The statistics of the <span id="S3.T4.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span> dataset. ‘Category’ represents the first-level category in the category tree. ‘#Tasks’ represents the number of tasks belonging to each first-level category. ‘#Samples’ represents the number of samples contained in each first-level category. ‘#Turns’ represents the average number of conversation turns included in each sample. ‘#Input length’ represents the average length of user input in each sample.
</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Table <a href="#S3.T4" title="Table 4 ‣ 3.4 Dataset Analysis ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents statistics of <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, which exhibits two main characteristics: (1) More fine-grained tasks are divided under each first-level category, such as “Biology” and “Chemistry” within the “Knowledge Utilization” category. (2) Each sample may comprise multiple turns of questions, simulating the process of human interaction with <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To validate diversity within each category, we calculate the overlap degree of instructions in each category. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.4 Dataset Analysis ‣ 3 The DINGO Dataset ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the similarity distribution of instructions.
For each instruction, we compute its highest ROUGE-L score with regard to other instructions in the same category. The results illustrate the diversity of instructions in <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.03942/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of the ROUGE-L scores between instructions within a category.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Baseline Models</h4>

<div id="S4.SS1.SSSx1.p1" class="ltx_para">
<p id="S4.SS1.SSSx1.p1.1" class="ltx_p">We select two representative types of <span id="S4.SS1.SSSx1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s: (1) Pre-trained only <span id="S4.SS1.SSSx1.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, including Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a href="#bib.bib21" title="" class="ltx_ref">2023a</a>)</cite> and Llama2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>; and (2) Instruction-tuned <span id="S4.SS1.SSSx1.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s, including vicuna-v1.3 <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, vicuna-v1.5 <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, Llama2-chat <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>. Considering that vicuna-v1.3 is instruction-tuned from Llama and vicuna-v1.5 is instruction-tuned from Llama2, we refer to vicuna-v1.3 as vicuna and vicuna-v1.5 as vicuna2 in this paper to make the notations consistent with Llama and Llama2.</p>
</div>
</section>
<section id="S4.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Evaluation Method</h4>

<div id="S4.SS1.SSSx2.p1" class="ltx_para">
<p id="S4.SS1.SSSx2.p1.1" class="ltx_p">We employ the <span id="S4.SS1.SSSx2.p1.1.1" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> method to comprehensively evaluate <span id="S4.SS1.SSSx2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>’s responses <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al. <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. <span id="S4.SS1.SSSx2.p1.1.3" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> is a technique to score the performance of <span id="S4.SS1.SSSx2.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s by utilizing GPT-4. Researchers have discovered that GPT-4 can generate consistent scores and provide detailed justifications, which exhibit a high level of agreement with human experts. However, considering that GPT-4 has difficulty in accurately scoring math/code problems <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al. <a href="#bib.bib9" title="" class="ltx_ref">2021b</a>)</cite>, we include the standard answers for basic questions as a reference in the prompt given to GPT-4. Regarding the grading method, <span id="S4.SS1.SSSx2.p1.1.5" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> considers two types, pair-wise comparison and single-answer grading. However, considering that we need to compare the performance of multiple <span id="S4.SS1.SSSx2.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s, we choose to use single-answer grading for more efficient evaluation. For different categories, we have manually annotated different scoring criteria to assist GPT-4 in generating scores that align with human preferences. For instance, in “Mathematics and Reasoning” tasks, the primary considerations include the clarity of steps, the correctness of reasoning, and the appropriateness of natural language explanations. Meanwhile, for “Knowledge Unilization” tasks, the primary considerations is on the adequacy of key points and whether the answers contain hallucination.</p>
</div>
<div id="S4.SS1.SSSx2.p2" class="ltx_para">
<p id="S4.SS1.SSSx2.p2.1" class="ltx_p">We explore the agreement between these two grading methods and human experts in Section <a href="#S4.SS2" title="4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.03942/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="217" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Evaluation results of different <span id="S4.F4.2.1" class="ltx_text ltx_font_smallcaps">LLM</span>s under different category granularity.</figcaption>
</figure>
<section id="S4.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How do the existing <span id="S4.SS2.SSSx1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s perform on <span id="S4.SS2.SSSx1.2.2" class="ltx_text ltx_font_smallcaps">DINGO</span>?</h4>

<div id="S4.SS2.SSSx1.p1" class="ltx_para">
<p id="S4.SS2.SSSx1.p1.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(a) shows the overall performance of ten <span id="S4.SS2.SSSx1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on the first-level categories of <span id="S4.SS2.SSSx1.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span>. First, comparing pre-trained <span id="S4.SS2.SSSx1.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s with instruction-tuned <span id="S4.SS2.SSSx1.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as <span id="S4.SS2.SSSx1.p1.1.5" class="ltx_text ltx_font_typewriter">Llama-13B</span> and <span id="S4.SS2.SSSx1.p1.1.6" class="ltx_text ltx_font_typewriter">vicuna-13B</span>, we can see that instruction-tuning significantly impacts alignment with human preferences. Second, comparing different instruction-tuned <span id="S4.SS2.SSSx1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>s based on the same pre-trained <span id="S4.SS2.SSSx1.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as <span id="S4.SS2.SSSx1.p1.1.9" class="ltx_text ltx_font_typewriter">vicuna2-7B</span> and <span id="S4.SS2.SSSx1.p1.1.10" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span>, we find that <span id="S4.SS2.SSSx1.p1.1.11" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> has better instruction-following ability than <span id="S4.SS2.SSSx1.p1.1.12" class="ltx_text ltx_font_typewriter">Vicuna2-7B</span>. This is mainly because <span id="S4.SS2.SSSx1.p1.1.13" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> utilizes an RLHF (reinforcement learning from human feedback) framework with two reward models for usefulness and safety to align with human preferences, enabling it to outperform the base <span id="S4.SS2.SSSx1.p1.1.14" class="ltx_text ltx_font_smallcaps">LLM</span> (<span id="S4.SS2.SSSx1.p1.1.15" class="ltx_text ltx_font_italic">i.e.,</span><span id="S4.SS2.SSSx1.p1.1.16" class="ltx_text ltx_font_typewriter">Llama2-7B</span>) under various user instructions. Finally, comparing <span id="S4.SS2.SSSx1.p1.1.17" class="ltx_text ltx_font_smallcaps">LLM</span>s of different sizes indicates that increasing the model size significantly improves the instruction-following ability of the pre-trained <span id="S4.SS2.SSSx1.p1.1.18" class="ltx_text ltx_font_smallcaps">LLM</span>s (such as <span id="S4.SS2.SSSx1.p1.1.19" class="ltx_text ltx_font_typewriter">Llama2-7B</span> and <span id="S4.SS2.SSSx1.p1.1.20" class="ltx_text ltx_font_typewriter">Llama2-13B</span>), but the impact on instruction-tuned <span id="S4.SS2.SSSx1.p1.1.21" class="ltx_text ltx_font_smallcaps">LLM</span>s (such as <span id="S4.SS2.SSSx1.p1.1.22" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> and <span id="S4.SS2.SSSx1.p1.1.23" class="ltx_text ltx_font_typewriter">Llama2-chat-13B</span>) is comparatively weaker.</p>
</div>
</section>
<section id="S4.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Can instruction-tuning consistently achieve stable improvements in more fine-grained categories?</h4>

<div id="S4.SS2.SSSx2.p1" class="ltx_para">
<p id="S4.SS2.SSSx2.p1.2" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(b) illustrates the performance across all sub-categories under “Knowledge Utilization”<math id="S4.SS2.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSSx2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS2.SSSx2.p1.1.m1.1.1" xref="S4.SS2.SSSx2.p1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSSx2.p1.1.m1.1b"><ci id="S4.SS2.SSSx2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSSx2.p1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSSx2.p1.1.m1.1c">\rightarrow</annotation></semantics></math>“Open-Book Questions”<math id="S4.SS2.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSSx2.p1.2.m2.1a"><mo stretchy="false" id="S4.SS2.SSSx2.p1.2.m2.1.1" xref="S4.SS2.SSSx2.p1.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSSx2.p1.2.m2.1b"><ci id="S4.SS2.SSSx2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSSx2.p1.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSSx2.p1.2.m2.1c">\rightarrow</annotation></semantics></math>“Knowledge-Intensive Questions”. It can be seen that under a more fine-grained evaluation, the improvement brought by instruction-tuning is not consistent. For example, the instruction-following performance of <span id="S4.SS2.SSSx2.p1.2.1" class="ltx_text ltx_font_typewriter">vicuna2-7B</span> after instruction-tuning does not improve compared to its base <span id="S4.SS2.SSSx2.p1.2.2" class="ltx_text ltx_font_smallcaps">LLM</span> <span id="S4.SS2.SSSx2.p1.2.3" class="ltx_text ltx_font_typewriter">Llama2-7B</span> in the two sub-categories: “Biology” and “Medicine”.
This suggests that conducting a more fine-grained evaluation of <span id="S4.SS2.SSSx2.p1.2.4" class="ltx_text ltx_font_smallcaps">LLM</span>s’ instruction-following ability is necessary, as high scores in coarse categories (<span id="S4.SS2.SSSx2.p1.2.5" class="ltx_text ltx_font_italic">e.g.,</span> “Knowledge Utilization”) do not necessarily indicate stable performance in all finer sub-categories (<span id="S4.SS2.SSSx2.p1.2.6" class="ltx_text ltx_font_italic">e.g.,</span> “Biology”). Therefore, <span id="S4.SS2.SSSx2.p1.2.7" class="ltx_text ltx_font_smallcaps">DINGO</span> helps guide instruction-tuned <span id="S4.SS2.SSSx2.p1.2.8" class="ltx_text ltx_font_smallcaps">LLM</span>s towards a more comprehensive direction for improvement, thus enhancing the interactive experiences for users with diverse requests.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold">From</span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">DINGO</span></span>
</span>
</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Basic Question</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Example-1</td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.2.1.1" class="ltx_p" style="width:213.4pt;">User: Find out the number of ways Kim can place 10 identical lamps on 3 identical tables. Directly output the answer without any explanation.</span>
</span>
</td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.3.1.1" class="ltx_p" style="width:213.4pt;">User: Kim has 10 identical lamps and 3 identical tables. How many ways are therefor her to put all the lamps on the tables?</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.3.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: There are several ways to calculate the number of ways Kim can place 10 identical lamps on 3 identical tables …</span>
</span>
</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T5.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.3.3.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: First, let’s count the ways to put 2 identical lamps on a single table. There are 2! ways to arrange …</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Example-2</td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.2.1.1" class="ltx_p" style="width:213.4pt;">User: For listed companies, … If the above statement is true, which statement cannot be true? A.A listed company needs financing and has no way to pay dividends. B … C … D …</span>
</span>
</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.3.1.1" class="ltx_p" style="width:213.4pt;">User: For listed companies, … If the above statement is true, which statement cannot be true? A. A listed company needs financing and has no way to pay dividends. B … C … D …</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.5.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: Let’s analyze each statement one by one: … Therefore, the final answer is D.</span>
</span>
</td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T5.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.5.3.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: Let’s analyze each statement one by one: … Therefore, the final answer is D.</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.2.1.1" class="ltx_p" style="width:213.4pt;">User: Please check each condition carefully!</span>
</span>
</td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr">
<td id="S4.T5.1.7.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S4.T5.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S4.T5.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.7.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.7.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: I apologize for any confusion … Therefore, the final answer is None of the above.</span>
</span>
</td>
<td id="S4.T5.1.7.3" class="ltx_td ltx_align_top ltx_border_b"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Two illustrative examples demonstrate why the <span id="S4.T5.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span> is more challenging than basic questions in instruction following. We use ellipses to omit lengthy, less significant sentences.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Do diverse instruction types pose a challenge to <span id="S4.SS2.SSSx3.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s?</h4>

<div id="S4.SS2.SSSx3.p1" class="ltx_para">
<p id="S4.SS2.SSSx3.p1.1" class="ltx_p">To investigate whether the diverse instruction types in <span id="S4.SS2.SSSx3.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> present a significant challenge to <span id="S4.SS2.SSSx3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, we conduct an analysis on the categories containing basic questions. Specifically, we use four <span id="S4.SS2.SSSx3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s to respond to basic questions and instructions in <span id="S4.SS2.SSSx3.p1.1.4" class="ltx_text ltx_font_smallcaps">DINGO</span> across four subcategories. The experimental results are shown in Figure <a href="#S4.F5" title="Figure 5 ‣ Do diverse instruction types pose a challenge to LLMs? ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It can be observed that the instruction following scores of the four <span id="S4.SS2.SSSx3.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s on <span id="S4.SS2.SSSx3.p1.1.6" class="ltx_text ltx_font_smallcaps">DINGO</span> are lower than those on basic questions, indicating that the diverse instructions in <span id="S4.SS2.SSSx3.p1.1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> are more challenging compared to standard questions. This also suggests that it is necessary to evaluate the <span id="S4.SS2.SSSx3.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s’ instruction following ability using more diverse instructions, as an <span id="S4.SS2.SSSx3.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span> may perform well in one mode of expression but not in others, implying that the <span id="S4.SS2.SSSx3.p1.1.10" class="ltx_text ltx_font_smallcaps">LLM</span>’s robustness to diverse instructions in real-world scenarios might be insufficient.</p>
</div>
<div id="S4.SS2.SSSx3.p2" class="ltx_para">
<p id="S4.SS2.SSSx3.p2.1" class="ltx_p">Additionally, to intuitively understand why the <span id="S4.SS2.SSSx3.p2.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s perform poorly on diverse instructions, we present two examples in Table <a href="#S4.T5" title="Table 5 ‣ Can instruction-tuning consistently achieve stable improvements in more fine-grained categories? ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Example-1 indicates that when user instructions become more concise and require a concise output (<span id="S4.SS2.SSSx3.p2.1.2" class="ltx_text ltx_font_italic">i.e.,</span> “<em id="S4.SS2.SSSx3.p2.1.3" class="ltx_emph ltx_font_italic">Directly output the answer without any explanation.</em>”), <span id="S4.SS2.SSSx3.p2.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s still generate lengthy explanations that do not align with user instructions. Example-2 shows that when the instruction is in <span id="S4.SS2.SSSx3.p2.1.5" class="ltx_text ltx_font_bold">Challenge</span> style (<span id="S4.SS2.SSSx3.p2.1.6" class="ltx_text ltx_font_italic">i.e.,</span> “<em id="S4.SS2.SSSx3.p2.1.7" class="ltx_emph ltx_font_italic">Please check each condition carefully!</em>”), the <span id="S4.SS2.SSSx3.p2.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s may go against the original correct answer in order to cater to human users, <span id="S4.SS2.SSSx3.p2.1.9" class="ltx_text ltx_font_italic">i.e.,</span> “<em id="S4.SS2.SSSx3.p2.1.10" class="ltx_emph ltx_font_italic">Therefore, the final answer is None of the above.</em>”.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2407.03942/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="277" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of instruction following performance of <span id="S4.F5.3.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on <span id="S4.F5.4.2" class="ltx_text ltx_font_smallcaps">DINGO</span> and on basic questions.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2407.03942/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Average win rate of four <span id="S4.F6.2.1" class="ltx_text ltx_font_smallcaps">LLM</span>s under different judge methods.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">What is the agreement between human judge scores and GPT-4 judge scores?</h4>

<div id="S4.SS2.SSSx4.p1" class="ltx_para">
<p id="S4.SS2.SSSx4.p1.1" class="ltx_p">To evaluate the agreement between GPT-4 and human experts, we choose 100 examples from <span id="S4.SS2.SSSx4.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> and employ six human experts. Given a judge (<span id="S4.SS2.SSSx4.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> either GPT-4 or human expert), we ask the judge to score the responses of the <span id="S4.SS2.SSSx4.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s using two methods, (1) pairwise comparison and (2) single-answer grading. Pairwise comparison provides the judge a question and two potential answers, and asks the judge to decide which answer is more appropriate. Single-answer grading asks a juedge to assign a score to a specific answer. Figure <a href="#S4.F6" title="Figure 6 ‣ Do diverse instruction types pose a challenge to LLMs? ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the agreement between GPT-4 and humans under the two scoring methods. With pairwise comparison, GPT-4 has higher agreement with human. However, pairwise comparison would incur high cost. On the other hand, single-answer grading is more efficient. Thus, we recommend single-answer grading for rough identification of model issues, and pairwise comparison for more detailed evaluations.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work: Evaluation of <span id="S5.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">For benchmarking the effectiveness of <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s, various evaluation frameworks have emerged.
Frameworks such as HELM <cite class="ltx_cite ltx_citemacro_citep">(Liang et al. <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> and BIG-BENCH <cite class="ltx_cite ltx_citemacro_citep">(bench authors <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> focus on the effectiveness of <span id="S5.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s on a wide range of NLP tasks, mainly evaluating the problem solving ability of the model, without paying attention to the <span id="S5.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>’s instruction-following ability. Recently, some work has started to focus on the instruction-following ability of <span id="S5.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s. For example, InstructEval <cite class="ltx_cite ltx_citemacro_citep">(Chia et al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> focuses on evaluating the ability of Instruction-Tuned <span id="S5.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s on three aspects, including problem solving, writing, and alignment. Alpaca Farm <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al. <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and Chatbot Arena <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al. <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> focus on evaluating the open-ended instruction-following ability of <span id="S5.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s. However, there are two main differences between <span id="S5.p1.1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> and the above studies: (1) a diverse set of instructions based on real-world scenarios, which can comprehensively evaluate the model’s instruction-following performance. (2) a fine-grained task category tree, which can deeply analyze <span id="S5.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>’s instruction-following ability on fine-grained task types and pinpoint the deficiencies for further improvement.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we have presented a diverse and fine-grained instruction-following evaluation dataset <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>. Based on a multi-level category tree with 130 nodes derived from real-world user requests, <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> includes 5026 diverse instructions. Our experiments demonstrate that (1) while an instruction-tuned <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span> may excel in broad categories, its performance can vary in fine-grained categories; (2) diverse instructions pose greater challenges for <span id="S6.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s to generate responses that match human preferences.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This paper is supported by the Science and Technology Development Fund of Macau SAR (File no. 0081/2022/A2, 0123/2022/AFJ, and 0015/2019/AKP), and GuangDong Basic and Applied Basic Research Foundation (No. 2020B1515130004). This paper is also partly supported by the NSF of China (62122090, 62072461 and 62072458), the Fund for Building World-Class Universities (Disciplines) of Renmin University of China, the Beijing Natural Science Foundation (L222006), and the Research Funds of Renmin University of China.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin et al. (2021)</span>
<span class="ltx_bibblock">
Austin, J.; Odena, A.; Nye, M. I.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang, E.; Cai, C. J.; Terry, M.; Le, Q. V.; and Sutton, C. 2021.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2108.07732.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev et al. (2023)</span>
<span class="ltx_bibblock">
Azerbayev, Z.; Piotrowski, B.; Schoelkopf, H.; Ayers, E. W.; Radev, D.; and Avigad, J. 2023.

</span>
<span class="ltx_bibblock">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bench authors (2023)</span>
<span class="ltx_bibblock">
bench authors, B. 2023.

</span>
<span class="ltx_bibblock">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S. M.; Nori, H.; Palangi, H.; Ribeiro, M. T.; and Zhang, Y. 2023.

</span>
<span class="ltx_bibblock">Sparks of Artificial General Intelligence: Early experiments with GPT-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.12712.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chia et al. (2023)</span>
<span class="ltx_bibblock">
Chia, Y. K.; Hong, P.; Bing, L.; and Poria, S. 2023.

</span>
<span class="ltx_bibblock">INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04757</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021a)</span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021a.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021b)</span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; et al. 2021b.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock">In Burstein, J.; Doran, C.; and Solorio, T., eds., <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, 4171–4186. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua et al. (2019)</span>
<span class="ltx_bibblock">
Dua, D.; Wang, Y.; Dasigi, P.; Stanovsky, G.; Singh, S.; and Gardner, M. 2019.

</span>
<span class="ltx_bibblock">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1903.00161.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al. (2023)</span>
<span class="ltx_bibblock">
Dubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14387</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2021)</span>
<span class="ltx_bibblock">
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Berant, J. 2021.

</span>
<span class="ltx_bibblock">Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 9: 346–361.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021.

</span>
<span class="ltx_bibblock">Measuring Mathematical Problem Solving With the MATH Dataset.

</span>
<span class="ltx_bibblock">In Vanschoren, J.; and Yeung, S., eds., <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022)</span>
<span class="ltx_bibblock">
Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; et al. 2022.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09110</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Liu, J.; Cui, L.; Liu, H.; Huang, D.; Wang, Y.; and Zhang, Y. 2020.

</span>
<span class="ltx_bibblock">LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning.

</span>
<span class="ltx_bibblock">In Bessiere, C., ed., <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</em>, 3622–3628. ijcai.org.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre et al. (2023)</span>
<span class="ltx_bibblock">
Longpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H. W.; Tay, Y.; Zhou, D.; Le, Q. V.; Zoph, B.; Wei, J.; et al. 2023.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.13688</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8): 9.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae et al. (2021)</span>
<span class="ltx_bibblock">
Rae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; et al. 2021.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training gopher.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11446</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023a)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023b)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Wang, Y.; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu, K. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy, I.; and Hajishirzi, H. 2023.

</span>
<span class="ltx_bibblock">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.04751.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; Chi, E. H.; Hashimoto, T.; Vinyals, O.; Liang, P.; Dean, J.; and Fedus, W. 2022.

</span>
<span class="ltx_bibblock">Emergent Abilities of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiegreffe et al. (2021)</span>
<span class="ltx_bibblock">
Wiegreffe, S.; Hessel, J.; Swayamdipta, S.; Riedl, M.; and Choi, Y. 2021.

</span>
<span class="ltx_bibblock">Reframing human-AI collaboration for generating free-text explanations.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.08674</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Ye, S.; Kim, D.; Kim, S.; Hwang, H.; Kim, S.; Jo, Y.; Thorne, J.; Kim, J.; and Seo, M. 2023.

</span>
<span class="ltx_bibblock">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.10928.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2023)</span>
<span class="ltx_bibblock">
Yuan, S.; Chen, J.; Fu, Z.; Ge, X.; Shah, S.; Jankowski, C. R.; Yang, D.; and Xiao, Y. 2023.

</span>
<span class="ltx_bibblock">Distilling Script Knowledge from Large Language Models for Constrained Language Planning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.05252</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Zhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.; and Wen, J. 2023.

</span>
<span class="ltx_bibblock">A Survey of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.18223.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05685</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.03941" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.03942" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.03942">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.03942" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.03943" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 13:38:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
