<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.03942] Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data</title><meta property="og:description" content="Instruction-following is particularly crucial for large language models (LLMs) to support diverse user requests. While existing work has made progress in aligning LLMs with human preferences, evaluating their capabilitâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.03942">

<!--Generated on Mon Aug  5 13:38:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Zihui Gu<sup id="id1.1.id1" class="ltx_sup">1,2</sup><span id="id2.2.id2" class="ltx_ERROR undefined">\equalcontrib</span>, Xingwu Sun<sup id="id3.3.id3" class="ltx_sup">2,3</sup><span id="id4.4.id4" class="ltx_ERROR undefined">\equalcontrib</span>, Fengzong Lian<sup id="id5.5.id5" class="ltx_sup">2</sup>, Zhanhui Kang<sup id="id6.6.id6" class="ltx_sup">2</sup>, Cheng-Zhong Xu<sup id="id7.7.id7" class="ltx_sup">3</sup>, Ju Fan<sup id="id8.8.id8" class="ltx_sup">1</sup>
<br class="ltx_break">
</span><span class="ltx_author_notes">Ju Fan is the corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Instruction-following is particularly crucial for large language models (<span id="id9.id1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s) to support diverse user requests. While existing work has made progress in aligning <span id="id9.id1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s with human preferences, evaluating their capabilities on instruction-following remains a challenge due to complexity and diversity of real-world user instructions. While existing evaluation methods focus on general skills, they suffer from two main shortcomings, <span id="id9.id1.3" class="ltx_text ltx_font_italic">i.e.,</span> lack of fine-grained task-level evaluation and reliance on singular instruction expression. To address these problems, this paper introduces <span id="id9.id1.4" class="ltx_text ltx_font_smallcaps">DINGO</span>, a fine-grained and diverse instruction-following evaluation dataset that has two main advantages: (1) <span id="id9.id1.5" class="ltx_text ltx_font_smallcaps">DINGO</span> is based on a manual-annotated, fine-grained and multi-level category tree with 130 nodes derived from real-world user requests; (2) <span id="id9.id1.6" class="ltx_text ltx_font_smallcaps">DINGO</span> includes diverse instructions, generated by both GPT-4 and human experts. Through extensive experiments, we demonstrate that <span id="id9.id1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> can not only provide more challenging and comprehensive evaluation for <span id="id9.id1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, but also provide task-level fine-grained directions to further improve <span id="id9.id1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recently, Large language models (<span id="S1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s) exhibit surprising capabilities not previously seen in smaller models, which are often referred to as <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">emergent abilities</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al. <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite>, including <span id="S1.p1.1.3" class="ltx_text ltx_font_italic">in-context learning</span>, <span id="S1.p1.1.4" class="ltx_text ltx_font_italic">chain-of-thought</span>, and <span id="S1.p1.1.5" class="ltx_text ltx_font_italic">instruction-following</span> abilities. Among them, the <span id="S1.p1.1.6" class="ltx_text ltx_font_italic">instruction-following</span> ability is crucial to the interaction between humans and <span id="S1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S1.p1.1.8" class="ltx_text ltx_font_italic">e.g.,</span> ChatGPT). Existing studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>; Longpre etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite> align <span id="S1.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s with human instructions using supervised instruction-tuning or reinforcement learning from human feedback (RLHF), which enables <span id="S1.p1.1.10" class="ltx_text ltx_font_smallcaps">LLM</span>s to understand human instructions and make high-quality responses. Nonetheless, due to the complexity and diversity of human instructions, it remains a challenge to comprehensively evaluate the <span id="S1.p1.1.11" class="ltx_text ltx_font_italic">instruction-following</span> ability of <span id="S1.p1.1.12" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2407.03942/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="507" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Different user request examples extracted from ShareGPT.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing studies evaluate the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">instruction-following</span> ability from the perspective of general skills. For example, InstructEvalÂ <cite class="ltx_cite ltx_citemacro_citep">(Chia etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> assesses <span id="S1.p2.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s instruction-following ability based on three general abilities: problem-solving, writing, and alignment to human values.
FlaskÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> shifts the original coarse-grained scoring process to instance-wise skill scoring setup, and defines 4 primary abilities, divided into 12 specific skills, to assess the performance of <span id="S1.p2.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. However, there are still two shortcomings in existing evaluation methods:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">lack of fine-grained task-level evaluation</span> poses challenges in improving the instruction-following ability of <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s. For example, the <span id="S1.I1.i1.p1.1.3" class="ltx_text ltx_font_typewriter">Factuality</span> skill used in FLASKÂ <cite class="ltx_cite ltx_citemacro_citep">(Ye etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2023</a>)</cite> includes many sub-tasks such as â€œHistory Knowledge QAâ€ and â€œChemical Knowledge QAâ€.
Consequently, even if we recognize that a particular <span id="S1.I1.i1.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span> is deficient in this skill, it is challenging to pinpoint the exact aspects of the instruction-following ability that the <span id="S1.I1.i1.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span> needs to be improved.
Specifically, if the performance of the <span id="S1.I1.i1.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span> is not satisfactory in â€œChemical Knowledge QAâ€, it is not clear whether this is because the <span id="S1.I1.i1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s response contains non-standard chemical formulas.
Similarly, if the <span id="S1.I1.i1.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span> cannot perform well in â€œHistory Knowledge QAâ€, it could potentially be because the key points are not clearly outlined in the <span id="S1.I1.i1.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s response.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">expression of instructions tends to be singular</span>, resulting in a gap between real-world user instructions and existing evaluation datasets. Existing datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Chia etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> often use previous NLP datasets as evaluation data for specific skills, such as employing DROPÂ <cite class="ltx_cite ltx_citemacro_citep">(Dua etÂ al. <a href="#bib.bib11" title="" class="ltx_ref">2019</a>)</cite> to evaluate the <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">Comprehension</span> ability, and design a specific instruction template for the dataset. However, in real-world scenarios, users express their requests in various ways. FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows several examples extracted from the ShareGPT website, a platform where users voluntarily share their interaction records with <span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. As can be seen, the styles and attitudes of user instructions are very diverse: users may ask questions directly (<span id="S1.I1.i2.p1.1.4" class="ltx_text ltx_font_italic">i.e.,</span> Concise) or set specific roles to ask questions (<span id="S1.I1.i2.p1.1.5" class="ltx_text ltx_font_italic">i.e.,</span> Role-play). Therefore, it could be very beneficial to evaluate the <span id="S1.I1.i2.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s instruction-following ability on these diverse instruction expressions.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To address the aforementioned shortcomings, in this paper, we present <span id="S1.p4.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, a <span id="S1.p4.1.2" class="ltx_text ltx_framed ltx_framed_underline">D</span>iverse and F<span id="S1.p4.1.3" class="ltx_text ltx_framed ltx_framed_underline">i</span>ne-grained I<span id="S1.p4.1.4" class="ltx_text ltx_framed ltx_framed_underline">n</span>struction-Followin<span id="S1.p4.1.5" class="ltx_text ltx_framed ltx_framed_underline">g</span> evaluation dataset. First, to support fine-grained instruction-following evaluation, we manually annotate a multi-level category tree with 130 nodes and 4 levels, based on the user instructions extracted from ShareGPT. This category tree encompasses tasks that users would want <span id="S1.p4.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s to complete in real-world scenarios, making it highly practical. Equipped with its multi-level structure, the category tree supports analyzing instruction-following ability at different granularities, and thus can address the shortcomings of <span id="S1.p4.1.7" class="ltx_text ltx_font_smallcaps">LLM</span> at task-level. Second, we prepare diverse instruction data for each category to comprehensively examine the instruction-following ability. Considering that user requests on ShareGPT have been used for instruction-tuning in many <span id="S1.p4.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as vicunaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite> and TÃœLUÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, we avoid data leakage by not directly using data from ShareGPT for evaluation. Instead, we employ GPT-4 to simulate various instruction styles, attitudes, and languages derived from ShareGPT, and generate diverse instruction data for each category. In addition, considering the weaknesses of <span id="S1.p4.1.9" class="ltx_text ltx_font_smallcaps">LLM</span>s in mathematics and logical reasoning, we utilize existing human-annotated datasets (<span id="S1.p4.1.10" class="ltx_text ltx_font_italic">e.g.,</span> GSM8KÂ <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite>) as basic questions and guide GPT-4 to generate diverse instructions from the basic questions to ensure the instruction quality. For example, a math question from GSM8K, <span id="S1.p4.1.11" class="ltx_text ltx_font_italic">â€œRonnie was given 5 while Rissa was given thrice as much â€¦â€</span> would be transformed by GPT-4 into a role-playing instruction form: <span id="S1.p4.1.12" class="ltx_text ltx_font_italic">â€œAct as a patient math teacher to answer this question step by step: Ronnie was given 5 while Rissa was given thrice as much â€¦â€</span>. Based on the above methods, we, in total, collect 5026 diverse samples in <span id="S1.p4.1.13" class="ltx_text ltx_font_smallcaps">DINGO</span> to comprehensively evaluate the instruction-following ability of <span id="S1.p4.1.14" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Based on <span id="S1.p5.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, we conduct extensive experiments to evaluate instruction-following of 10 different <span id="S1.p5.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, and obtain the following findings.
(1) Even if an instruction-tuned <span id="S1.p5.1.3" class="ltx_text ltx_font_smallcaps">LLM</span> performs well on coarse-grained categories, its performance on fine-grained categories may be diversified and, sometimes, it could even be worse than the base <span id="S1.p5.1.4" class="ltx_text ltx_font_smallcaps">LLM</span> without instruction fine-tuning. (2) Our dataset with diverse instructions presents more significant challenges to <span id="S1.p5.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s to generate responses that align with human preferences.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our contributions can be summarized as follows:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We publicly release a multi-level task category tree consisting of 130 nodes, designed to support instruction-following evaluations at various granularities.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We collect 5026 diverse and high-quality instructions based on real-world user instructions, presenting more significant challenges for <span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s in generating responses that align with human preferences.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">We conduct a comprehensive evaluation on 10 representative <span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s, and the experimental results demonstrate that <span id="S1.I2.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> can support more extensive and challenging evaluation on the instruction-following ability, as well as provide fine-grained guidance to further improve <span id="S1.I2.i3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s. We release the <span id="S1.I2.i3.p1.1.4" class="ltx_text ltx_font_smallcaps">DINGO</span> dataset at Github<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/ruc-datalab/DINGO</span></span></span>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background: Instruction-Following Ability of Large Language Models</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Language models (LMs) are designed to comprehend and produce text that resembles human language (<span id="S2.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al. <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, GPT2Â <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al. <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite>).
Recently, researchers have discovered that scaling LMs to
large LMs (<span id="S2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s) (<span id="S2.p1.1.3" class="ltx_text ltx_font_italic">e.g.,</span> ChatGPT, GPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, LLaMAÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2023a</a>)</cite>) by increasing the model size or amount of training data can significantly enhance their downstream task abilities. Moreover, the existing studies also show that <span id="S2.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s demonstrate surprising abilities that have not been seen in previous smaller LMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Bubeck etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Rae etÂ al. <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Brown etÂ al. <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, such as <span id="S2.p1.1.5" class="ltx_text ltx_font_italic">in-context learning</span> and <span id="S2.p1.1.6" class="ltx_text ltx_font_italic">instruction-following</span>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_italic">Instruction-following</span> is an important ability for <span id="S2.p2.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s to interact with real-world users. This means that the model can complete various tasks based on a wide range of natural language instructions provided by humans, including polishing articles (<span id="S2.p2.1.3" class="ltx_text ltx_font_italic">e.g.,Polish this email above in very urgent tone: </span>{<span id="S2.p2.1.4" class="ltx_text ltx_font_italic">Email</span>}<span id="S2.p2.1.5" class="ltx_text ltx_font_italic">.</span>), solving math problems (<span id="S2.p2.1.6" class="ltx_text ltx_font_italic">e.g.,I need to calculate how long my gas canister will last for 360 burener.</span>), providing travel plans (<span id="S2.p2.1.7" class="ltx_text ltx_font_italic">e.g.,Plan a trip to Jindo for 2 nights and 3 days.</span>), etc. <span id="S2.p2.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s can obtain the instruction-following ability in the following two ways: (1) supervised learning using instruction-following datasets (<span id="S2.p2.1.9" class="ltx_text ltx_font_italic">e.g.,</span> vicunaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>), and (2) reinforcement learning from Human Feedback(<span id="S2.p2.1.10" class="ltx_text ltx_font_italic">e.g.,</span> Llama2-chatÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>).</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In this work, we aim to evaluate the capabilities of existing <span id="S2.p3.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on instruction-following across a variety of tasks and various instruction expressions, and provide a comprehensive benchmark <span id="S2.p3.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> to promote in-depth analysis of the instruction-following ability of <span id="S2.p3.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span id="S3.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> Dataset</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2407.03942/assets/figs/pipeline.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="268" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A high-level pipeline of Instruction Data Collection.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to generate a fine-grained <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">category tree</span> and diverse <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">instructions</span>. To achieve this goal, we first collect real-world user instructions as seed data. Then, we manually classify the seed data to obtain a fine-grained <span id="S3.p1.1.3" class="ltx_text ltx_font_typewriter">category tree</span>. Finally, based on seed data and <span id="S3.p1.1.4" class="ltx_text ltx_font_typewriter">category tree</span>, we collect diverse <span id="S3.p1.1.5" class="ltx_text ltx_font_typewriter">instructions</span> for each category by guiding GPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite> to simulate various instruction styles, attitudes, and languages.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Seed Data Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To obtain real-world instruction-following data, we utilize public data from ShareGPT (https://sharegpt.com/), which is a platform for users to share their interactions with <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.,</span> GPT-4). Following previous workÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>; Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>, we use the â€˜html_cleanedâ€˜ version<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://huggingface.co/datasets/anon8231489123/
<br class="ltx_break">ShareGPT_Vicuna_unfiltered/tree/main/HTML_cleaned_raw_dataset</span></span></span> and truncate conversations with more than 2048 tokens. Based on this, we obtain 7265 seed samples from ShareGPT.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Category Tree Annotation</h3>

<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">First-level</span></span>
</span>
</td>
<td id="S3.T1.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.2.1.1" class="ltx_p" style="width:156.5pt;"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Second-level</span></span>
</span>
</td>
</tr>
<tr id="S3.T1.1.2" class="ltx_tr">
<td id="S3.T1.1.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.1.1.1" class="ltx_p" style="width:56.9pt;">Language Understanding</span>
</span>
</td>
<td id="S3.T1.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.2.2.1.1" class="ltx_p" style="width:156.5pt;">Relationship Judgement; Classification; Sorting; Error Correction; Joke Explanation; Information Extraction</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.3" class="ltx_tr">
<td id="S3.T1.1.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.1.1.1" class="ltx_p" style="width:56.9pt;">Code</span>
</span>
</td>
<td id="S3.T1.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.3.2.1.1" class="ltx_p" style="width:156.5pt;">Text2Code; Code2Text; Code2Code</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.4" class="ltx_tr">
<td id="S3.T1.1.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.1.1.1" class="ltx_p" style="width:56.9pt;">Knowledge Unitilization</span>
</span>
</td>
<td id="S3.T1.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.4.2.1.1" class="ltx_p" style="width:156.5pt;">Open-book Questions; Close-book Questions</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.5" class="ltx_tr">
<td id="S3.T1.1.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.1.1.1" class="ltx_p" style="width:56.9pt;">Creation</span>
</span>
</td>
<td id="S3.T1.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.5.2.1.1" class="ltx_p" style="width:156.5pt;">Thematic creation; Specialized writing; Plan; Non-verbal creation; Simulation creation</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.6" class="ltx_tr">
<td id="S3.T1.1.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.1.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.1.1.1" class="ltx_p" style="width:56.9pt;">Language Generation</span>
</span>
</td>
<td id="S3.T1.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T1.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.6.2.1.1" class="ltx_p" style="width:156.5pt;">Question Generation; Rewriting/Paraphrasing; Summary/Abstract/Title; Translation</span>
</span>
</td>
</tr>
<tr id="S3.T1.1.7" class="ltx_tr">
<td id="S3.T1.1.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t">
<span id="S3.T1.1.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.1.1.1" class="ltx_p" style="width:56.9pt;">Mathematics and Reasoning</span>
</span>
</td>
<td id="S3.T1.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t">
<span id="S3.T1.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.7.2.1.1" class="ltx_p" style="width:156.5pt;">Word Problems; Mathematical theorem; Combinatorics; Mathematical Calculations; Common Sense Reasoning; Logical Reasoning</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The first and second level categories of <span id="S3.T1.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T2.3.4" class="ltx_tr">
<td id="S3.T2.3.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S3.T2.3.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.4.1.1.1" class="ltx_p" style="width:227.6pt;">/* Task description */</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.5" class="ltx_tr">
<td id="S3.T2.3.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.5.1.1.1" class="ltx_p" style="width:227.6pt;">I need you to simulate the conversation between â€œhumanâ€ and â€œAIâ€. I will specify some constraints, including â€¦</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.6" class="ltx_tr">
<td id="S3.T2.3.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.6.1.1.1" class="ltx_p" style="width:227.6pt;">/* Demos from seed data */</span>
</span>
</td>
</tr>
<tr id="S3.T2.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.1.1.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Applied Problems; <span id="S3.T2.1.1.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.1.1.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Concise; <span id="S3.T2.1.1.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Command;</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.7" class="ltx_tr">
<td id="S3.T2.3.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.7.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.7.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: Calculate how long my gas â€¦? AI: â€¦</span>
</span>
</td>
</tr>
<tr id="S3.T2.2.2" class="ltx_tr">
<td id="S3.T2.2.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.2.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.2.2.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.2.2.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.2.2.1.1.1.m1.1.1" xref="S3.T2.2.2.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.1.1.m1.1b"><ci id="S3.T2.2.2.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Combinatorics; <span id="S3.T2.2.2.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.2.2.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Roly-play; <span id="S3.T2.2.2.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Polite;</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.8" class="ltx_tr">
<td id="S3.T2.3.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.8.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.8.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: You are a math teacher, please explain this question step by step: There are two rows in a classroom â€¦? AI: â€¦</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.9" class="ltx_tr">
<td id="S3.T2.3.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.9.1.1.1" class="ltx_p" style="width:227.6pt;">/* Constraints for GPT-4 */</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.3" class="ltx_tr">
<td id="S3.T2.3.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S3.T2.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.3.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.3.1.1.1.1" class="ltx_text ltx_font_bold">Category</span>: Mathematics and Reasoning <math id="S3.T2.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.T2.3.3.1.1.1.m1.1a"><mo stretchy="false" id="S3.T2.3.3.1.1.1.m1.1.1" xref="S3.T2.3.3.1.1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.1.1.1.m1.1b"><ci id="S3.T2.3.3.1.1.1.m1.1.1.cmml" xref="S3.T2.3.3.1.1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.1.1.1.m1.1c">\rightarrow</annotation></semantics></math> Word Problems; <span id="S3.T2.3.3.1.1.1.2" class="ltx_text ltx_font_bold">Language</span>: English; <span id="S3.T2.3.3.1.1.1.3" class="ltx_text ltx_font_bold">Style</span>: Roly-play; <span id="S3.T2.3.3.1.1.1.4" class="ltx_text ltx_font_bold">Attitude</span>: Command; <span id="S3.T2.3.3.1.1.1.5" class="ltx_text ltx_font_bold">Basic Question</span>: Ronnie was given 5 while Rissa was given thrice as muchâ€¦</span>
</span>
</td>
</tr>
<tr id="S3.T2.3.10" class="ltx_tr">
<td id="S3.T2.3.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b">
<span id="S3.T2.3.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.3.10.1.1.1" class="ltx_p" style="width:227.6pt;"><span id="S3.T2.3.10.1.1.1.1" class="ltx_text ltx_font_bold">Conversation</span>: Human: Act as a helpful math assistant to answer this question: Ronnie â€¦</span>
</span>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Examples of prompt for GPT-4 for instruction data generation via in-context learning. The content generated by GPT-4 has been highlighted.</figcaption>
</figure>
<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Unlike previous work, we focus only on tasks that may appear in real-world user instructions, as this represents what users genuinely want the <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s to achieve, providing a more practical evaluation of the instruction-following ability. Thus, we manually annotated the fine-grained task categories of the extracted instruction data from ShareGPT, primarily adhering to the traditional NLP task types commonly defined in previous researchÂ <cite class="ltx_cite ltx_citemacro_citep">(Longpre etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; bench authors <a href="#bib.bib3" title="" class="ltx_ref">2023</a>; Zhao etÂ al. <a href="#bib.bib28" title="" class="ltx_ref">2023</a>)</cite>.
For the convenience of conducting evaluations at different granularities, we design the categories as a multi-level tree structure, which facilitates efficient and in-depth analysis of the capabilities of <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s. Statistically, our category tree comprises 4 levels, with the first level containing <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_bold">6</span> categories, the second level containing <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_bold">25</span> categories, the third level containing <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_bold">65</span> categories, and the fourth level containing <span id="S3.SS2.p1.1.6" class="ltx_text ltx_font_bold">34</span> categories. We present the first and second level categories in TableÂ <a href="#S3.T1" title="Table 1 â€£ 3.2 Category Tree Annotation â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">As the goal of this work is to evaluate the performance of <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on various instruction expressions, we also annotate the instruction style, attitude, and language for each instruction sample in the seed data, which are described as follows.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">For instruction style, we specify the following five types:</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Inquisitive</span> represents asking multiple questions on the same topic, or delving deeper into a particular question (See the first example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Reflective</span> represents asking multiple questions with the userâ€™s own thoughts and ideas (See the second example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Challenge</span> represents asking multiple questions, which are increasingly difficult (See the third example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Role-play</span> represents setting roles for both <span id="S3.I1.i4.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s and users, and conducting questioning under this setting. (See the fourth example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p"><span id="S3.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Concise</span> represents asking a question directly and clearly. (See the fifth example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">For instruction attitude, we specify three types:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Polite</span> represents asking questions using gentle words, such as â€œ<em id="S3.I2.i1.p1.1.2" class="ltx_emph ltx_font_italic">Could you answer the questionâ€¦</em>â€.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Command</span> represents asking questions in a strong and imperative tone, such as â€œ<em id="S3.I2.i2.p1.1.2" class="ltx_emph ltx_font_italic">Summarize this passage: â€¦</em>â€.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Impatient</span> represents urging the <span id="S3.I2.i3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span> to respond to a certain aspect during the questioning process, such as â€œ<em id="S3.I2.i3.p1.1.3" class="ltx_emph ltx_font_italic">Answer this question directly: â€¦, hurry up!</em>â€</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">Moreover, for languages, we list all the languages included in the conversation, as users often switch between languages during the conversation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Instruction Data Collection</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Generating diverse instructions is very challenging for human annotators, as it requires (1) the ability to transition between various instruction styles, attitudes, and languages, and (2) the capacity to produce a range of samples within a single category (<span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> â€œGrammar-based Rewritingâ€). Consequently, we propose employing the highly capable <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>, GPT-4Â <cite class="ltx_cite ltx_citemacro_citep">(OpenAI <a href="#bib.bib18" title="" class="ltx_ref">2023</a>)</cite>, to simulate a variety of user types and generate diverse, high-quality instructions for each category. Please note that we do not directly incorporate instruction data from ShareGPT into our benchmark, because numerous <span id="S3.SS3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s (<span id="S3.SS3.p1.1.4" class="ltx_text ltx_font_italic">e.g.,</span> VicunaÂ <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, TÃœLUÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al. <a href="#bib.bib23" title="" class="ltx_ref">2023</a>)</cite>) have already utilized data from shareGPT for supervised instruction-tuning. Therefore, we only use ShareGPT data as seed data to guide GPT-4. The data collection pipeline is depicted in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.2" class="ltx_p">For any leaf category (<span id="S3.SS3.p2.2.1" class="ltx_text ltx_font_italic">e.g.,</span> â€œMathematics and Reasoning â€<math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mo stretchy="false" id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\rightarrow</annotation></semantics></math>â€œMathematical Calculationsâ€<math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mo stretchy="false" id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\rightarrow</annotation></semantics></math> â€œAlgebraic Equation Problemsâ€), we consider the following three steps to collect the instruction-following data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.11" class="ltx_p">In the first step, the goal is to generate constraints to guide GPT-4 to simulate specific user types, thereby preventing generation of unrealistic instructions. To achieve this, we treat the seed data as a sample pool and randomly select two samples as demos of in-context learning, each associated with a particular instruction style (<math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\mathcal{S}" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">ğ’®</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">ğ’®</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathcal{S}</annotation></semantics></math>), attitude (<math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\mathcal{A}</annotation></semantics></math>), and language (<math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{L}" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">â„’</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><ci id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">â„’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathcal{L}</annotation></semantics></math>). We randomly sample target style, attitude, and language from these two demos to form constraints, compelling GPT-4 to learn from different instruction demos rather than excessively imitating one. For example, the constraints in FigureÂ <a href="#S3.F2" title="Figure 2 â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> is <math id="S3.SS3.p3.4.m4.1" class="ltx_math_unparsed" alttext="\{\mathcal{S}=" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mrow id="S3.SS3.p3.4.m4.1b"><mo stretchy="false" id="S3.SS3.p3.4.m4.1.1">{</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.4.m4.1.2">ğ’®</mi><mo id="S3.SS3.p3.4.m4.1.3">=</mo></mrow><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">\{\mathcal{S}=</annotation></semantics></math>â€œ<math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="Reflective" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mrow id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml"><mi id="S3.SS3.p3.5.m5.1.1.2" xref="S3.SS3.p3.5.m5.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.3" xref="S3.SS3.p3.5.m5.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1a" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.4" xref="S3.SS3.p3.5.m5.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1b" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.5" xref="S3.SS3.p3.5.m5.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1c" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.6" xref="S3.SS3.p3.5.m5.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1d" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.7" xref="S3.SS3.p3.5.m5.1.1.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1e" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.8" xref="S3.SS3.p3.5.m5.1.1.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1f" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.9" xref="S3.SS3.p3.5.m5.1.1.9.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1g" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.10" xref="S3.SS3.p3.5.m5.1.1.10.cmml">v</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.5.m5.1.1.1h" xref="S3.SS3.p3.5.m5.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.5.m5.1.1.11" xref="S3.SS3.p3.5.m5.1.1.11.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><apply id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1"><times id="S3.SS3.p3.5.m5.1.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1.1"></times><ci id="S3.SS3.p3.5.m5.1.1.2.cmml" xref="S3.SS3.p3.5.m5.1.1.2">ğ‘…</ci><ci id="S3.SS3.p3.5.m5.1.1.3.cmml" xref="S3.SS3.p3.5.m5.1.1.3">ğ‘’</ci><ci id="S3.SS3.p3.5.m5.1.1.4.cmml" xref="S3.SS3.p3.5.m5.1.1.4">ğ‘“</ci><ci id="S3.SS3.p3.5.m5.1.1.5.cmml" xref="S3.SS3.p3.5.m5.1.1.5">ğ‘™</ci><ci id="S3.SS3.p3.5.m5.1.1.6.cmml" xref="S3.SS3.p3.5.m5.1.1.6">ğ‘’</ci><ci id="S3.SS3.p3.5.m5.1.1.7.cmml" xref="S3.SS3.p3.5.m5.1.1.7">ğ‘</ci><ci id="S3.SS3.p3.5.m5.1.1.8.cmml" xref="S3.SS3.p3.5.m5.1.1.8">ğ‘¡</ci><ci id="S3.SS3.p3.5.m5.1.1.9.cmml" xref="S3.SS3.p3.5.m5.1.1.9">ğ‘–</ci><ci id="S3.SS3.p3.5.m5.1.1.10.cmml" xref="S3.SS3.p3.5.m5.1.1.10">ğ‘£</ci><ci id="S3.SS3.p3.5.m5.1.1.11.cmml" xref="S3.SS3.p3.5.m5.1.1.11">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">Reflective</annotation></semantics></math>â€, <math id="S3.SS3.p3.6.m6.1" class="ltx_Math" alttext="\mathcal{A}=" display="inline"><semantics id="S3.SS3.p3.6.m6.1a"><mrow id="S3.SS3.p3.6.m6.1.1" xref="S3.SS3.p3.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.6.m6.1.1.2" xref="S3.SS3.p3.6.m6.1.1.2.cmml">ğ’œ</mi><mo id="S3.SS3.p3.6.m6.1.1.1" xref="S3.SS3.p3.6.m6.1.1.1.cmml">=</mo><mi id="S3.SS3.p3.6.m6.1.1.3" xref="S3.SS3.p3.6.m6.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.1b"><apply id="S3.SS3.p3.6.m6.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1"><eq id="S3.SS3.p3.6.m6.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1"></eq><ci id="S3.SS3.p3.6.m6.1.1.2.cmml" xref="S3.SS3.p3.6.m6.1.1.2">ğ’œ</ci><csymbol cd="latexml" id="S3.SS3.p3.6.m6.1.1.3.cmml" xref="S3.SS3.p3.6.m6.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.1c">\mathcal{A}=</annotation></semantics></math>â€œ<math id="S3.SS3.p3.7.m7.1" class="ltx_Math" alttext="Polite" display="inline"><semantics id="S3.SS3.p3.7.m7.1a"><mrow id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml"><mi id="S3.SS3.p3.7.m7.1.1.2" xref="S3.SS3.p3.7.m7.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1" xref="S3.SS3.p3.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.7.m7.1.1.3" xref="S3.SS3.p3.7.m7.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1a" xref="S3.SS3.p3.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.7.m7.1.1.4" xref="S3.SS3.p3.7.m7.1.1.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1b" xref="S3.SS3.p3.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.7.m7.1.1.5" xref="S3.SS3.p3.7.m7.1.1.5.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1c" xref="S3.SS3.p3.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.7.m7.1.1.6" xref="S3.SS3.p3.7.m7.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.7.m7.1.1.1d" xref="S3.SS3.p3.7.m7.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.7.m7.1.1.7" xref="S3.SS3.p3.7.m7.1.1.7.cmml">e</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><apply id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1"><times id="S3.SS3.p3.7.m7.1.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1.1"></times><ci id="S3.SS3.p3.7.m7.1.1.2.cmml" xref="S3.SS3.p3.7.m7.1.1.2">ğ‘ƒ</ci><ci id="S3.SS3.p3.7.m7.1.1.3.cmml" xref="S3.SS3.p3.7.m7.1.1.3">ğ‘œ</ci><ci id="S3.SS3.p3.7.m7.1.1.4.cmml" xref="S3.SS3.p3.7.m7.1.1.4">ğ‘™</ci><ci id="S3.SS3.p3.7.m7.1.1.5.cmml" xref="S3.SS3.p3.7.m7.1.1.5">ğ‘–</ci><ci id="S3.SS3.p3.7.m7.1.1.6.cmml" xref="S3.SS3.p3.7.m7.1.1.6">ğ‘¡</ci><ci id="S3.SS3.p3.7.m7.1.1.7.cmml" xref="S3.SS3.p3.7.m7.1.1.7">ğ‘’</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">Polite</annotation></semantics></math>â€, <math id="S3.SS3.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{L}=" display="inline"><semantics id="S3.SS3.p3.8.m8.1a"><mrow id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.8.m8.1.1.2" xref="S3.SS3.p3.8.m8.1.1.2.cmml">â„’</mi><mo id="S3.SS3.p3.8.m8.1.1.1" xref="S3.SS3.p3.8.m8.1.1.1.cmml">=</mo><mi id="S3.SS3.p3.8.m8.1.1.3" xref="S3.SS3.p3.8.m8.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><apply id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1"><eq id="S3.SS3.p3.8.m8.1.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1.1"></eq><ci id="S3.SS3.p3.8.m8.1.1.2.cmml" xref="S3.SS3.p3.8.m8.1.1.2">â„’</ci><csymbol cd="latexml" id="S3.SS3.p3.8.m8.1.1.3.cmml" xref="S3.SS3.p3.8.m8.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">\mathcal{L}=</annotation></semantics></math>â€œ<math id="S3.SS3.p3.9.m9.1" class="ltx_Math" alttext="English" display="inline"><semantics id="S3.SS3.p3.9.m9.1a"><mrow id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml"><mi id="S3.SS3.p3.9.m9.1.1.2" xref="S3.SS3.p3.9.m9.1.1.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.3" xref="S3.SS3.p3.9.m9.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1a" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.4" xref="S3.SS3.p3.9.m9.1.1.4.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1b" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.5" xref="S3.SS3.p3.9.m9.1.1.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1c" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.6" xref="S3.SS3.p3.9.m9.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1d" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.7" xref="S3.SS3.p3.9.m9.1.1.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p3.9.m9.1.1.1e" xref="S3.SS3.p3.9.m9.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p3.9.m9.1.1.8" xref="S3.SS3.p3.9.m9.1.1.8.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><apply id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1"><times id="S3.SS3.p3.9.m9.1.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1.1"></times><ci id="S3.SS3.p3.9.m9.1.1.2.cmml" xref="S3.SS3.p3.9.m9.1.1.2">ğ¸</ci><ci id="S3.SS3.p3.9.m9.1.1.3.cmml" xref="S3.SS3.p3.9.m9.1.1.3">ğ‘›</ci><ci id="S3.SS3.p3.9.m9.1.1.4.cmml" xref="S3.SS3.p3.9.m9.1.1.4">ğ‘”</ci><ci id="S3.SS3.p3.9.m9.1.1.5.cmml" xref="S3.SS3.p3.9.m9.1.1.5">ğ‘™</ci><ci id="S3.SS3.p3.9.m9.1.1.6.cmml" xref="S3.SS3.p3.9.m9.1.1.6">ğ‘–</ci><ci id="S3.SS3.p3.9.m9.1.1.7.cmml" xref="S3.SS3.p3.9.m9.1.1.7">ğ‘ </ci><ci id="S3.SS3.p3.9.m9.1.1.8.cmml" xref="S3.SS3.p3.9.m9.1.1.8">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">English</annotation></semantics></math>â€<math id="S3.SS3.p3.10.m10.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S3.SS3.p3.10.m10.1a"><mo stretchy="false" id="S3.SS3.p3.10.m10.1.1" xref="S3.SS3.p3.10.m10.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.10.m10.1b"><ci id="S3.SS3.p3.10.m10.1.1.cmml" xref="S3.SS3.p3.10.m10.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.10.m10.1c">\}</annotation></semantics></math>. Additionally, given that GPT-4 may struggle to generate high-quality mathematical or logical reasoning questions, we gather data from previous task-specific benchmarks as basic questions, which are then incorporated as part of the constraints. For example, we use the GSM8KÂ <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite> dataset as a basic question source for the â€œMathematics and Reasoningâ€ <math id="S3.SS3.p3.11.m11.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S3.SS3.p3.11.m11.1a"><mo stretchy="false" id="S3.SS3.p3.11.m11.1.1" xref="S3.SS3.p3.11.m11.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.11.m11.1b"><ci id="S3.SS3.p3.11.m11.1.1.cmml" xref="S3.SS3.p3.11.m11.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.11.m11.1c">\rightarrow</annotation></semantics></math>â€œWord Problemsâ€ category. More details of the existing dataset resources included in <span id="S3.SS3.p3.11.1" class="ltx_text ltx_font_smallcaps">DINGO</span> are listed in TableÂ <a href="#S3.T3" title="Table 3 â€£ 3.3 Instruction Data Collection â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T3.1.1" class="ltx_tr">
<td id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.1.2.1" class="ltx_text ltx_font_bold">Basic Question Source</span></td>
</tr>
<tr id="S3.T3.1.2" class="ltx_tr">
<td id="S3.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Word Problems</td>
<td id="S3.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t">GSM8KÂ <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2021a</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.3" class="ltx_tr">
<td id="S3.T3.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Mathematical Theorem</td>
<td id="S3.T3.1.3.2" class="ltx_td ltx_align_center">ProofNetÂ <cite class="ltx_cite ltx_citemacro_citep">(Azerbayev etÂ al. <a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.4" class="ltx_tr">
<td id="S3.T3.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Combinatorics</td>
<td id="S3.T3.1.4.2" class="ltx_td ltx_align_center">MathÂ <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.5" class="ltx_tr">
<td id="S3.T3.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Numerical Calculation</td>
<td id="S3.T3.1.5.2" class="ltx_td ltx_align_center">MathÂ <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks etÂ al. <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.6" class="ltx_tr">
<td id="S3.T3.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Common Sense Reasoning</td>
<td id="S3.T3.1.6.2" class="ltx_td ltx_align_center">StrategyQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Geva etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.7" class="ltx_tr">
<td id="S3.T3.1.7.1" class="ltx_td ltx_align_center ltx_border_r">Logical Reasoning</td>
<td id="S3.T3.1.7.2" class="ltx_td ltx_align_center">LogiQAÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al. <a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite>
</td>
</tr>
<tr id="S3.T3.1.8" class="ltx_tr">
<td id="S3.T3.1.8.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Text2Code</td>
<td id="S3.T3.1.8.2" class="ltx_td ltx_align_center ltx_border_b">MBPPÂ <cite class="ltx_cite ltx_citemacro_citep">(Austin etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>
</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The basic question source of <span id="S3.T3.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</figcaption>
</figure>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The goal of the second step is for GPT-4 to simulate a real-world user and generate high-quality instructions by adhering to the constraints. We use in-context learning to achieve this goal. As illustrated in TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2 Category Tree Annotation â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we combine the task description, the two demos obtained from the first step, and the target constraints as input context for GPT-4. As demonstrated, GPT-4 learns different expressions from two demos and transforms the basic question into specific instruction â€œ<em id="S3.SS3.p4.1.1" class="ltx_emph ltx_font_italic">Act as a helpful math assistant to â€¦</em>â€ based on the <span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_bold">Role-play</span> and <span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_bold">Command</span> constraints. Following previous workÂ <cite class="ltx_cite ltx_citemacro_citep">(Wiegreffe etÂ al. <a href="#bib.bib25" title="" class="ltx_ref">2021</a>; Yuan etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite>, we adopt the <span id="S3.SS3.p4.1.4" class="ltx_text ltx_font_italic">over-generate-then-filter</span> approach to obtain higher quality instructions. Thus, in this step, we prompt GPT-4 to make two predictions based on the same input, generating two instruction candidates.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">In the third step, the objective is to select faithful and diverse instructions. We consider two selection methods, constraint-based pair-wise selection and similarity-based selection. Specifically, we first use GPT-4 to determine which of the two candidates adheres more closely to the constraints. We require GPT-4 to choose from three options, <math id="S3.SS3.p5.1.m1.3" class="ltx_Math" alttext="\{first,second,tie\}" display="inline"><semantics id="S3.SS3.p5.1.m1.3a"><mrow id="S3.SS3.p5.1.m1.3.3.3" xref="S3.SS3.p5.1.m1.3.3.4.cmml"><mo stretchy="false" id="S3.SS3.p5.1.m1.3.3.3.4" xref="S3.SS3.p5.1.m1.3.3.4.cmml">{</mo><mrow id="S3.SS3.p5.1.m1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.1.1.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.1.1.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1a" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.4" xref="S3.SS3.p5.1.m1.1.1.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1b" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.5" xref="S3.SS3.p5.1.m1.1.1.1.1.5.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.1.1.1.1.1c" xref="S3.SS3.p5.1.m1.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.1.1.1.1.6" xref="S3.SS3.p5.1.m1.1.1.1.1.6.cmml">t</mi></mrow><mo id="S3.SS3.p5.1.m1.3.3.3.5" xref="S3.SS3.p5.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.1.m1.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.cmml"><mi id="S3.SS3.p5.1.m1.2.2.2.2.2" xref="S3.SS3.p5.1.m1.2.2.2.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.3" xref="S3.SS3.p5.1.m1.2.2.2.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1a" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.4" xref="S3.SS3.p5.1.m1.2.2.2.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1b" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.5" xref="S3.SS3.p5.1.m1.2.2.2.2.5.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1c" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.6" xref="S3.SS3.p5.1.m1.2.2.2.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.2.2.2.2.1d" xref="S3.SS3.p5.1.m1.2.2.2.2.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.2.2.2.2.7" xref="S3.SS3.p5.1.m1.2.2.2.2.7.cmml">d</mi></mrow><mo id="S3.SS3.p5.1.m1.3.3.3.6" xref="S3.SS3.p5.1.m1.3.3.4.cmml">,</mo><mrow id="S3.SS3.p5.1.m1.3.3.3.3" xref="S3.SS3.p5.1.m1.3.3.3.3.cmml"><mi id="S3.SS3.p5.1.m1.3.3.3.3.2" xref="S3.SS3.p5.1.m1.3.3.3.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.3.3.3.3.1" xref="S3.SS3.p5.1.m1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.3.3.3.3.3" xref="S3.SS3.p5.1.m1.3.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p5.1.m1.3.3.3.3.1a" xref="S3.SS3.p5.1.m1.3.3.3.3.1.cmml">â€‹</mo><mi id="S3.SS3.p5.1.m1.3.3.3.3.4" xref="S3.SS3.p5.1.m1.3.3.3.3.4.cmml">e</mi></mrow><mo stretchy="false" id="S3.SS3.p5.1.m1.3.3.3.7" xref="S3.SS3.p5.1.m1.3.3.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.3b"><set id="S3.SS3.p5.1.m1.3.3.4.cmml" xref="S3.SS3.p5.1.m1.3.3.3"><apply id="S3.SS3.p5.1.m1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1"><times id="S3.SS3.p5.1.m1.1.1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.1"></times><ci id="S3.SS3.p5.1.m1.1.1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.2">ğ‘“</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.3">ğ‘–</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.4.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.4">ğ‘Ÿ</ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.5.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.5">ğ‘ </ci><ci id="S3.SS3.p5.1.m1.1.1.1.1.6.cmml" xref="S3.SS3.p5.1.m1.1.1.1.1.6">ğ‘¡</ci></apply><apply id="S3.SS3.p5.1.m1.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2"><times id="S3.SS3.p5.1.m1.2.2.2.2.1.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.1"></times><ci id="S3.SS3.p5.1.m1.2.2.2.2.2.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.2">ğ‘ </ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.3.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.3">ğ‘’</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.4.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.4">ğ‘</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.5.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.5">ğ‘œ</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.6.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.6">ğ‘›</ci><ci id="S3.SS3.p5.1.m1.2.2.2.2.7.cmml" xref="S3.SS3.p5.1.m1.2.2.2.2.7">ğ‘‘</ci></apply><apply id="S3.SS3.p5.1.m1.3.3.3.3.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3"><times id="S3.SS3.p5.1.m1.3.3.3.3.1.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.1"></times><ci id="S3.SS3.p5.1.m1.3.3.3.3.2.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.2">ğ‘¡</ci><ci id="S3.SS3.p5.1.m1.3.3.3.3.3.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.3">ğ‘–</ci><ci id="S3.SS3.p5.1.m1.3.3.3.3.4.cmml" xref="S3.SS3.p5.1.m1.3.3.3.3.4">ğ‘’</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.3c">\{first,second,tie\}</annotation></semantics></math>, and provide a rationale. Next, to ensure diversity, we calculate the similarity between the best candidate and the collected data in the dataset. Then, we only add the candidate to the dataset if the maximum ROUGE-L similarity is less than 0.6.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Dataset Analysis</h3>

<figure id="S3.T4" class="ltx_table">
<table id="S3.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S3.T4.1.1" class="ltx_tr">
<td id="S3.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S3.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.2.1" class="ltx_text ltx_font_bold">#Tasks</span></td>
<td id="S3.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.3.1" class="ltx_text ltx_font_bold">#Samples</span></td>
<td id="S3.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T4.1.1.4.1" class="ltx_text ltx_font_bold">#Turns</span></td>
<td id="S3.T4.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.1.1.5.1" class="ltx_text ltx_font_bold">#Input length</span></td>
</tr>
<tr id="S3.T4.1.2" class="ltx_tr">
<td id="S3.T4.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Mathematics and Reasoning</td>
<td id="S3.T4.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">9</td>
<td id="S3.T4.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">432</td>
<td id="S3.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.6</td>
<td id="S3.T4.1.2.5" class="ltx_td ltx_align_center ltx_border_t">83.4</td>
</tr>
<tr id="S3.T4.1.3" class="ltx_tr">
<td id="S3.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_r">Language Understanding</td>
<td id="S3.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_r">11</td>
<td id="S3.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_r">530</td>
<td id="S3.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_r">2.0</td>
<td id="S3.T4.1.3.5" class="ltx_td ltx_align_center">125.7</td>
</tr>
<tr id="S3.T4.1.4" class="ltx_tr">
<td id="S3.T4.1.4.1" class="ltx_td ltx_align_center ltx_border_r">Language Generation</td>
<td id="S3.T4.1.4.2" class="ltx_td ltx_align_center ltx_border_r">14</td>
<td id="S3.T4.1.4.3" class="ltx_td ltx_align_center ltx_border_r">730</td>
<td id="S3.T4.1.4.4" class="ltx_td ltx_align_center ltx_border_r">1.9</td>
<td id="S3.T4.1.4.5" class="ltx_td ltx_align_center">157.1</td>
</tr>
<tr id="S3.T4.1.5" class="ltx_tr">
<td id="S3.T4.1.5.1" class="ltx_td ltx_align_center ltx_border_r">Knowledge Utilization</td>
<td id="S3.T4.1.5.2" class="ltx_td ltx_align_center ltx_border_r">34</td>
<td id="S3.T4.1.5.3" class="ltx_td ltx_align_center ltx_border_r">1596</td>
<td id="S3.T4.1.5.4" class="ltx_td ltx_align_center ltx_border_r">2.6</td>
<td id="S3.T4.1.5.5" class="ltx_td ltx_align_center">72.7</td>
</tr>
<tr id="S3.T4.1.6" class="ltx_tr">
<td id="S3.T4.1.6.1" class="ltx_td ltx_align_center ltx_border_r">Creation</td>
<td id="S3.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_r">31</td>
<td id="S3.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_r">1498</td>
<td id="S3.T4.1.6.4" class="ltx_td ltx_align_center ltx_border_r">1.9</td>
<td id="S3.T4.1.6.5" class="ltx_td ltx_align_center">63.4</td>
</tr>
<tr id="S3.T4.1.7" class="ltx_tr">
<td id="S3.T4.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Code</td>
<td id="S3.T4.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">5</td>
<td id="S3.T4.1.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">240</td>
<td id="S3.T4.1.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">2.1</td>
<td id="S3.T4.1.7.5" class="ltx_td ltx_align_center ltx_border_b">105.5</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>The statistics of the <span id="S3.T4.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span> dataset. â€˜Categoryâ€™ represents the first-level category in the category tree. â€˜#Tasksâ€™ represents the number of tasks belonging to each first-level category. â€˜#Samplesâ€™ represents the number of samples contained in each first-level category. â€˜#Turnsâ€™ represents the average number of conversation turns included in each sample. â€˜#Input lengthâ€™ represents the average length of user input in each sample.
</figcaption>
</figure>
<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">TableÂ <a href="#S3.T4" title="Table 4 â€£ 3.4 Dataset Analysis â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents statistics of <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>, which exhibits two main characteristics: (1) More fine-grained tasks are divided under each first-level category, such as â€œBiologyâ€ and â€œChemistryâ€ within the â€œKnowledge Utilizationâ€ category. (2) Each sample may comprise multiple turns of questions, simulating the process of human interaction with <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To validate diversity within each category, we calculate the overlap degree of instructions in each category. FigureÂ <a href="#S3.F3" title="Figure 3 â€£ 3.4 Dataset Analysis â€£ 3 The DINGO Dataset â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the similarity distribution of instructions.
For each instruction, we compute its highest ROUGE-L score with regard to other instructions in the same category. The results illustrate the diversity of instructions in <span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.03942/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="207" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Distribution of the ROUGE-L scores between instructions within a category.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental Setup</h3>

<section id="S4.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Baseline Models</h4>

<div id="S4.SS1.SSSx1.p1" class="ltx_para">
<p id="S4.SS1.SSSx1.p1.1" class="ltx_p">We select two representative types of <span id="S4.SS1.SSSx1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s: (1) Pre-trained only <span id="S4.SS1.SSSx1.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, including LlamaÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2023a</a>)</cite> and Llama2Â <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>; and (2) Instruction-tuned <span id="S4.SS1.SSSx1.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s, including vicuna-v1.3Â <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, vicuna-v1.5Â <cite class="ltx_cite ltx_citemacro_citep">(Chiang etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>, Llama2-chatÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2023b</a>)</cite>. Considering that vicuna-v1.3 is instruction-tuned from Llama and vicuna-v1.5 is instruction-tuned from Llama2, we refer to vicuna-v1.3 as vicuna and vicuna-v1.5 as vicuna2 in this paper to make the notations consistent with Llama and Llama2.</p>
</div>
</section>
<section id="S4.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Evaluation Method</h4>

<div id="S4.SS1.SSSx2.p1" class="ltx_para">
<p id="S4.SS1.SSSx2.p1.1" class="ltx_p">We employ the <span id="S4.SS1.SSSx2.p1.1.1" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> method to comprehensively evaluate <span id="S4.SS1.SSSx2.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s responsesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zheng etÂ al. <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite>. <span id="S4.SS1.SSSx2.p1.1.3" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> is a technique to score the performance of <span id="S4.SS1.SSSx2.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s by utilizing GPT-4. Researchers have discovered that GPT-4 can generate consistent scores and provide detailed justifications, which exhibit a high level of agreement with human experts. However, considering that GPT-4 has difficulty in accurately scoring math/code problemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Cobbe etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2021b</a>)</cite>, we include the standard answers for basic questions as a reference in the prompt given to GPT-4. Regarding the grading method, <span id="S4.SS1.SSSx2.p1.1.5" class="ltx_text ltx_font_typewriter">LLM-as-a-judge</span> considers two types, pair-wise comparison and single-answer grading. However, considering that we need to compare the performance of multiple <span id="S4.SS1.SSSx2.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s, we choose to use single-answer grading for more efficient evaluation. For different categories, we have manually annotated different scoring criteria to assist GPT-4 in generating scores that align with human preferences. For instance, in â€œMathematics and Reasoningâ€ tasks, the primary considerations include the clarity of steps, the correctness of reasoning, and the appropriateness of natural language explanations. Meanwhile, for â€œKnowledge Unilizationâ€ tasks, the primary considerations is on the adequacy of key points and whether the answers contain hallucination.</p>
</div>
<div id="S4.SS1.SSSx2.p2" class="ltx_para">
<p id="S4.SS1.SSSx2.p2.1" class="ltx_p">We explore the agreement between these two grading methods and human experts in SectionÂ <a href="#S4.SS2" title="4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental Results</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2407.03942/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="217" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Evaluation results of different <span id="S4.F4.2.1" class="ltx_text ltx_font_smallcaps">LLM</span>s under different category granularity.</figcaption>
</figure>
<section id="S4.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">How do the existing <span id="S4.SS2.SSSx1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s perform on <span id="S4.SS2.SSSx1.2.2" class="ltx_text ltx_font_smallcaps">DINGO</span>?</h4>

<div id="S4.SS2.SSSx1.p1" class="ltx_para">
<p id="S4.SS2.SSSx1.p1.1" class="ltx_p">FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(a) shows the overall performance of ten <span id="S4.SS2.SSSx1.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on the first-level categories of <span id="S4.SS2.SSSx1.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span>. First, comparing pre-trained <span id="S4.SS2.SSSx1.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s with instruction-tuned <span id="S4.SS2.SSSx1.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as <span id="S4.SS2.SSSx1.p1.1.5" class="ltx_text ltx_font_typewriter">Llama-13B</span> and <span id="S4.SS2.SSSx1.p1.1.6" class="ltx_text ltx_font_typewriter">vicuna-13B</span>, we can see that instruction-tuning significantly impacts alignment with human preferences. Second, comparing different instruction-tuned <span id="S4.SS2.SSSx1.p1.1.7" class="ltx_text ltx_font_smallcaps">LLM</span>s based on the same pre-trained <span id="S4.SS2.SSSx1.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s, such as <span id="S4.SS2.SSSx1.p1.1.9" class="ltx_text ltx_font_typewriter">vicuna2-7B</span> and <span id="S4.SS2.SSSx1.p1.1.10" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span>, we find that <span id="S4.SS2.SSSx1.p1.1.11" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> has better instruction-following ability than <span id="S4.SS2.SSSx1.p1.1.12" class="ltx_text ltx_font_typewriter">Vicuna2-7B</span>. This is mainly because <span id="S4.SS2.SSSx1.p1.1.13" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> utilizes an RLHF (reinforcement learning from human feedback) framework with two reward models for usefulness and safety to align with human preferences, enabling it to outperform the base <span id="S4.SS2.SSSx1.p1.1.14" class="ltx_text ltx_font_smallcaps">LLM</span> (<span id="S4.SS2.SSSx1.p1.1.15" class="ltx_text ltx_font_italic">i.e.,</span><span id="S4.SS2.SSSx1.p1.1.16" class="ltx_text ltx_font_typewriter">Llama2-7B</span>) under various user instructions. Finally, comparing <span id="S4.SS2.SSSx1.p1.1.17" class="ltx_text ltx_font_smallcaps">LLM</span>s of different sizes indicates that increasing the model size significantly improves the instruction-following ability of the pre-trained <span id="S4.SS2.SSSx1.p1.1.18" class="ltx_text ltx_font_smallcaps">LLM</span>s (such as <span id="S4.SS2.SSSx1.p1.1.19" class="ltx_text ltx_font_typewriter">Llama2-7B</span> and <span id="S4.SS2.SSSx1.p1.1.20" class="ltx_text ltx_font_typewriter">Llama2-13B</span>), but the impact on instruction-tuned <span id="S4.SS2.SSSx1.p1.1.21" class="ltx_text ltx_font_smallcaps">LLM</span>s (such as <span id="S4.SS2.SSSx1.p1.1.22" class="ltx_text ltx_font_typewriter">Llama2-chat-7B</span> and <span id="S4.SS2.SSSx1.p1.1.23" class="ltx_text ltx_font_typewriter">Llama2-chat-13B</span>) is comparatively weaker.</p>
</div>
</section>
<section id="S4.SS2.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Can instruction-tuning consistently achieve stable improvements in more fine-grained categories?</h4>

<div id="S4.SS2.SSSx2.p1" class="ltx_para">
<p id="S4.SS2.SSSx2.p1.2" class="ltx_p">FigureÂ <a href="#S4.F4" title="Figure 4 â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>-(b) illustrates the performance across all sub-categories under â€œKnowledge Utilizationâ€<math id="S4.SS2.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSSx2.p1.1.m1.1a"><mo stretchy="false" id="S4.SS2.SSSx2.p1.1.m1.1.1" xref="S4.SS2.SSSx2.p1.1.m1.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSSx2.p1.1.m1.1b"><ci id="S4.SS2.SSSx2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSSx2.p1.1.m1.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSSx2.p1.1.m1.1c">\rightarrow</annotation></semantics></math>â€œOpen-Book Questionsâ€<math id="S4.SS2.SSSx2.p1.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.SS2.SSSx2.p1.2.m2.1a"><mo stretchy="false" id="S4.SS2.SSSx2.p1.2.m2.1.1" xref="S4.SS2.SSSx2.p1.2.m2.1.1.cmml">â†’</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSSx2.p1.2.m2.1b"><ci id="S4.SS2.SSSx2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSSx2.p1.2.m2.1.1">â†’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSSx2.p1.2.m2.1c">\rightarrow</annotation></semantics></math>â€œKnowledge-Intensive Questionsâ€. It can be seen that under a more fine-grained evaluation, the improvement brought by instruction-tuning is not consistent. For example, the instruction-following performance of <span id="S4.SS2.SSSx2.p1.2.1" class="ltx_text ltx_font_typewriter">vicuna2-7B</span> after instruction-tuning does not improve compared to its base <span id="S4.SS2.SSSx2.p1.2.2" class="ltx_text ltx_font_smallcaps">LLM</span>Â <span id="S4.SS2.SSSx2.p1.2.3" class="ltx_text ltx_font_typewriter">Llama2-7B</span> in the two sub-categories: â€œBiologyâ€ and â€œMedicineâ€.
This suggests that conducting a more fine-grained evaluation of <span id="S4.SS2.SSSx2.p1.2.4" class="ltx_text ltx_font_smallcaps">LLM</span>sâ€™ instruction-following ability is necessary, as high scores in coarse categories (<span id="S4.SS2.SSSx2.p1.2.5" class="ltx_text ltx_font_italic">e.g.,</span> â€œKnowledge Utilizationâ€) do not necessarily indicate stable performance in all finer sub-categories (<span id="S4.SS2.SSSx2.p1.2.6" class="ltx_text ltx_font_italic">e.g.,</span> â€œBiologyâ€). Therefore, <span id="S4.SS2.SSSx2.p1.2.7" class="ltx_text ltx_font_smallcaps">DINGO</span> helps guide instruction-tuned <span id="S4.SS2.SSSx2.p1.2.8" class="ltx_text ltx_font_smallcaps">LLM</span>s towards a more comprehensive direction for improvement, thus enhancing the interactive experiences for users with diverse requests.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold">From</span></td>
<td id="S4.T5.1.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.1.2.1.1.1" class="ltx_text ltx_font_bold">DINGO</span></span>
</span>
</td>
<td id="S4.T5.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.1.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Basic Question</span></span>
</span>
</td>
</tr>
<tr id="S4.T5.1.2" class="ltx_tr">
<td id="S4.T5.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Example-1</td>
<td id="S4.T5.1.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.2.1.1" class="ltx_p" style="width:213.4pt;">User: Find out the number of ways Kim can place 10 identical lamps on 3 identical tables. Directly output the answer without any explanation.</span>
</span>
</td>
<td id="S4.T5.1.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.2.3.1.1" class="ltx_p" style="width:213.4pt;">User: Kim has 10 identical lamps and 3 identical tables. How many ways are therefor her to put all the lamps on the tables?</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.3" class="ltx_tr">
<td id="S4.T5.1.3.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.3.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: There are several ways to calculate the number of ways Kim can place 10 identical lamps on 3 identical tables â€¦</span>
</span>
</td>
<td id="S4.T5.1.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T5.1.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.3.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.3.3.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: First, letâ€™s count the ways to put 2 identical lamps on a single table. There are 2! ways to arrange â€¦</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.4" class="ltx_tr">
<td id="S4.T5.1.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Example-2</td>
<td id="S4.T5.1.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S4.T5.1.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.2.1.1" class="ltx_p" style="width:213.4pt;">User: For listed companies, â€¦ If the above statement is true, which statement cannot be true? A.A listed company needs financing and has no way to pay dividends. B â€¦ C â€¦ D â€¦</span>
</span>
</td>
<td id="S4.T5.1.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S4.T5.1.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.4.3.1.1" class="ltx_p" style="width:213.4pt;">User: For listed companies, â€¦ If the above statement is true, which statement cannot be true? A. A listed company needs financing and has no way to pay dividends. B â€¦ C â€¦ D â€¦</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.5" class="ltx_tr">
<td id="S4.T5.1.5.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.5.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: Letâ€™s analyze each statement one by one: â€¦ Therefore, the final answer is D.</span>
</span>
</td>
<td id="S4.T5.1.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S4.T5.1.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.5.3.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.5.3.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: Letâ€™s analyze each statement one by one: â€¦ Therefore, the final answer is D.</span>
</span>
</td>
</tr>
<tr id="S4.T5.1.6" class="ltx_tr">
<td id="S4.T5.1.6.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T5.1.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S4.T5.1.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.6.2.1.1" class="ltx_p" style="width:213.4pt;">User: Please check each condition carefully!</span>
</span>
</td>
<td id="S4.T5.1.6.3" class="ltx_td ltx_align_top"></td>
</tr>
<tr id="S4.T5.1.7" class="ltx_tr">
<td id="S4.T5.1.7.1" class="ltx_td ltx_border_b ltx_border_r"></td>
<td id="S4.T5.1.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S4.T5.1.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T5.1.7.2.1.1" class="ltx_p" style="width:213.4pt;"><span id="S4.T5.1.7.2.1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>: I apologize for any confusion â€¦ Therefore, the final answer is None of the above.</span>
</span>
</td>
<td id="S4.T5.1.7.3" class="ltx_td ltx_align_top ltx_border_b"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Two illustrative examples demonstrate why the <span id="S4.T5.3.1" class="ltx_text ltx_font_smallcaps">DINGO</span> is more challenging than basic questions in instruction following. We use ellipses to omit lengthy, less significant sentences.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Do diverse instruction types pose a challenge to <span id="S4.SS2.SSSx3.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s?</h4>

<div id="S4.SS2.SSSx3.p1" class="ltx_para">
<p id="S4.SS2.SSSx3.p1.1" class="ltx_p">To investigate whether the diverse instruction types in <span id="S4.SS2.SSSx3.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> present a significant challenge to <span id="S4.SS2.SSSx3.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s, we conduct an analysis on the categories containing basic questions. Specifically, we use four <span id="S4.SS2.SSSx3.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s to respond to basic questions and instructions in <span id="S4.SS2.SSSx3.p1.1.4" class="ltx_text ltx_font_smallcaps">DINGO</span> across four subcategories. The experimental results are shown in FigureÂ <a href="#S4.F5" title="Figure 5 â€£ Do diverse instruction types pose a challenge to LLMs? â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. It can be observed that the instruction following scores of the four <span id="S4.SS2.SSSx3.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s on <span id="S4.SS2.SSSx3.p1.1.6" class="ltx_text ltx_font_smallcaps">DINGO</span> are lower than those on basic questions, indicating that the diverse instructions in <span id="S4.SS2.SSSx3.p1.1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> are more challenging compared to standard questions. This also suggests that it is necessary to evaluate the <span id="S4.SS2.SSSx3.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>sâ€™ instruction following ability using more diverse instructions, as an <span id="S4.SS2.SSSx3.p1.1.9" class="ltx_text ltx_font_smallcaps">LLM</span> may perform well in one mode of expression but not in others, implying that the <span id="S4.SS2.SSSx3.p1.1.10" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s robustness to diverse instructions in real-world scenarios might be insufficient.</p>
</div>
<div id="S4.SS2.SSSx3.p2" class="ltx_para">
<p id="S4.SS2.SSSx3.p2.1" class="ltx_p">Additionally, to intuitively understand why the <span id="S4.SS2.SSSx3.p2.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s perform poorly on diverse instructions, we present two examples in TableÂ <a href="#S4.T5" title="Table 5 â€£ Can instruction-tuning consistently achieve stable improvements in more fine-grained categories? â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Example-1 indicates that when user instructions become more concise and require a concise output (<span id="S4.SS2.SSSx3.p2.1.2" class="ltx_text ltx_font_italic">i.e.,</span> â€œ<em id="S4.SS2.SSSx3.p2.1.3" class="ltx_emph ltx_font_italic">Directly output the answer without any explanation.</em>â€), <span id="S4.SS2.SSSx3.p2.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s still generate lengthy explanations that do not align with user instructions. Example-2 shows that when the instruction is in <span id="S4.SS2.SSSx3.p2.1.5" class="ltx_text ltx_font_bold">Challenge</span> style (<span id="S4.SS2.SSSx3.p2.1.6" class="ltx_text ltx_font_italic">i.e.,</span> â€œ<em id="S4.SS2.SSSx3.p2.1.7" class="ltx_emph ltx_font_italic">Please check each condition carefully!</em>â€), the <span id="S4.SS2.SSSx3.p2.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>s may go against the original correct answer in order to cater to human users, <span id="S4.SS2.SSSx3.p2.1.9" class="ltx_text ltx_font_italic">i.e.,</span> â€œ<em id="S4.SS2.SSSx3.p2.1.10" class="ltx_emph ltx_font_italic">Therefore, the final answer is None of the above.</em>â€.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2407.03942/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="277" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of instruction following performance of <span id="S4.F5.3.1" class="ltx_text ltx_font_smallcaps">LLM</span>s on <span id="S4.F5.4.2" class="ltx_text ltx_font_smallcaps">DINGO</span> and on basic questions.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2407.03942/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Average win rate of four <span id="S4.F6.2.1" class="ltx_text ltx_font_smallcaps">LLM</span>s under different judge methods.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSSx4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">What is the agreement between human judge scores and GPT-4 judge scores?</h4>

<div id="S4.SS2.SSSx4.p1" class="ltx_para">
<p id="S4.SS2.SSSx4.p1.1" class="ltx_p">To evaluate the agreement between GPT-4 and human experts, we choose 100 examples from <span id="S4.SS2.SSSx4.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span> and employ six human experts. Given a judge (<span id="S4.SS2.SSSx4.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> either GPT-4 or human expert), we ask the judge to score the responses of the <span id="S4.SS2.SSSx4.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>s using two methods, (1) pairwise comparison and (2) single-answer grading. Pairwise comparison provides the judge a question and two potential answers, and asks the judge to decide which answer is more appropriate. Single-answer grading asks a juedge to assign a score to a specific answer. FigureÂ <a href="#S4.F6" title="Figure 6 â€£ Do diverse instruction types pose a challenge to LLMs? â€£ 4.2 Experimental Results â€£ 4 Experiments â€£ Diverse and Fine-Grained Instruction-Following Ability Exploration with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the agreement between GPT-4 and humans under the two scoring methods. With pairwise comparison, GPT-4 has higher agreement with human. However, pairwise comparison would incur high cost. On the other hand, single-answer grading is more efficient. Thus, we recommend single-answer grading for rough identification of model issues, and pairwise comparison for more detailed evaluations.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work: Evaluation of <span id="S5.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">For benchmarking the effectiveness of <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">LLM</span>s, various evaluation frameworks have emerged.
Frameworks such as HELMÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> and BIG-BENCHÂ <cite class="ltx_cite ltx_citemacro_citep">(bench authors <a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite> focus on the effectiveness of <span id="S5.p1.1.2" class="ltx_text ltx_font_smallcaps">LLM</span>s on a wide range of NLP tasks, mainly evaluating the problem solving ability of the model, without paying attention to the <span id="S5.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s instruction-following ability. Recently, some work has started to focus on the instruction-following ability of <span id="S5.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s. For example, InstructEvalÂ <cite class="ltx_cite ltx_citemacro_citep">(Chia etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2023</a>)</cite> focuses on evaluating the ability of Instruction-Tuned <span id="S5.p1.1.5" class="ltx_text ltx_font_smallcaps">LLM</span>s on three aspects, including problem solving, writing, and alignment. Alpaca FarmÂ <cite class="ltx_cite ltx_citemacro_citep">(Dubois etÂ al. <a href="#bib.bib12" title="" class="ltx_ref">2023</a>)</cite> and Chatbot ArenaÂ <cite class="ltx_cite ltx_citemacro_citep">(Zheng etÂ al. <a href="#bib.bib29" title="" class="ltx_ref">2023</a>)</cite> focus on evaluating the open-ended instruction-following ability of <span id="S5.p1.1.6" class="ltx_text ltx_font_smallcaps">LLM</span>s. However, there are two main differences between <span id="S5.p1.1.7" class="ltx_text ltx_font_smallcaps">DINGO</span> and the above studies: (1) a diverse set of instructions based on real-world scenarios, which can comprehensively evaluate the modelâ€™s instruction-following performance. (2) a fine-grained task category tree, which can deeply analyze <span id="S5.p1.1.8" class="ltx_text ltx_font_smallcaps">LLM</span>â€™s instruction-following ability on fine-grained task types and pinpoint the deficiencies for further improvement.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we have presented a diverse and fine-grained instruction-following evaluation dataset <span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">DINGO</span>. Based on a multi-level category tree with 130 nodes derived from real-world user requests, <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">DINGO</span> includes 5026 diverse instructions. Our experiments demonstrate that (1) while an instruction-tuned <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">LLM</span> may excel in broad categories, its performance can vary in fine-grained categories; (2) diverse instructions pose greater challenges for <span id="S6.p1.1.4" class="ltx_text ltx_font_smallcaps">LLM</span>s to generate responses that match human preferences.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This paper is supported by the Science and Technology Development Fund of Macau SAR (File no. 0081/2022/A2, 0123/2022/AFJ, and 0015/2019/AKP), and GuangDong Basic and Applied Basic Research Foundation (No. 2020B1515130004). This paper is also partly supported by the NSF of China (62122090, 62072461 and 62072458), the Fund for Building World-Class Universities (Disciplines) of Renmin University of China, the Beijing Natural Science Foundation (L222006), and the Research Funds of Renmin University of China.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Austin etÂ al. (2021)</span>
<span class="ltx_bibblock">
Austin, J.; Odena, A.; Nye, M.Â I.; Bosma, M.; Michalewski, H.; Dohan, D.; Jiang, E.; Cai, C.Â J.; Terry, M.; Le, Q.Â V.; and Sutton, C. 2021.

</span>
<span class="ltx_bibblock">Program Synthesis with Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2108.07732.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azerbayev etÂ al. (2023)</span>
<span class="ltx_bibblock">
Azerbayev, Z.; Piotrowski, B.; Schoelkopf, H.; Ayers, E.Â W.; Radev, D.; and Avigad, J. 2023.

</span>
<span class="ltx_bibblock">ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.12433.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">bench authors (2023)</span>
<span class="ltx_bibblock">
bench authors, B. 2023.

</span>
<span class="ltx_bibblock">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions on Machine Learning Research</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Brown, T.Â B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D.Â M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; and Amodei, D. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">In Larochelle, H.; Ranzato, M.; Hadsell, R.; Balcan, M.; and Lin, H., eds., <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y.Â T.; Li, Y.; Lundberg, S.Â M.; Nori, H.; Palangi, H.; Ribeiro, M.Â T.; and Zhang, Y. 2023.

</span>
<span class="ltx_bibblock">Sparks of Artificial General Intelligence: Early experiments with GPT-4.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.12712.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chia etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chia, Y.Â K.; Hong, P.; Bing, L.; and Poria, S. 2023.

</span>
<span class="ltx_bibblock">INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.04757</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J.Â E.; Stoica, I.; and Xing, E.Â P. 2023.

</span>
<span class="ltx_bibblock">Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; etÂ al. 2021a.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.; Kaiser, L.; Plappert, M.; Tworek, J.; Hilton, J.; Nakano, R.; etÂ al. 2021b.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.

</span>
<span class="ltx_bibblock">In Burstein, J.; Doran, C.; and Solorio, T., eds., <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, 4171â€“4186. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua etÂ al. (2019)</span>
<span class="ltx_bibblock">
Dua, D.; Wang, Y.; Dasigi, P.; Stanovsky, G.; Singh, S.; and Gardner, M. 2019.

</span>
<span class="ltx_bibblock">DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1903.00161.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois etÂ al. (2023)</span>
<span class="ltx_bibblock">
Dubois, Y.; Li, X.; Taori, R.; Zhang, T.; Gulrajani, I.; Ba, J.; Guestrin, C.; Liang, P.; and Hashimoto, T.Â B. 2023.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.14387</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva etÂ al. (2021)</span>
<span class="ltx_bibblock">
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Berant, J. 2021.

</span>
<span class="ltx_bibblock">Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Trans. Assoc. Comput. Linguistics</em>, 9: 346â€“361.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hendrycks, D.; Burns, C.; Kadavath, S.; Arora, A.; Basart, S.; Tang, E.; Song, D.; and Steinhardt, J. 2021.

</span>
<span class="ltx_bibblock">Measuring Mathematical Problem Solving With the MATH Dataset.

</span>
<span class="ltx_bibblock">In Vanschoren, J.; and Yeung, S., eds., <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Liang, P.; Bommasani, R.; Lee, T.; Tsipras, D.; Soylu, D.; Yasunaga, M.; Zhang, Y.; Narayanan, D.; Wu, Y.; Kumar, A.; etÂ al. 2022.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2211.09110</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Liu, J.; Cui, L.; Liu, H.; Huang, D.; Wang, Y.; and Zhang, Y. 2020.

</span>
<span class="ltx_bibblock">LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning.

</span>
<span class="ltx_bibblock">In Bessiere, C., ed., <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</em>, 3622â€“3628. ijcai.org.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre etÂ al. (2023)</span>
<span class="ltx_bibblock">
Longpre, S.; Hou, L.; Vu, T.; Webson, A.; Chung, H.Â W.; Tay, Y.; Zhou, D.; Le, Q.Â V.; Zoph, B.; Wei, J.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">The flan collection: Designing data and methods for effective instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.13688</em>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.08774.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I.; etÂ al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">OpenAI blog</em>, 1(8): 9.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rae etÂ al. (2021)</span>
<span class="ltx_bibblock">
Rae, J.Â W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.; Song, F.; Aslanides, J.; Henderson, S.; Ring, R.; Young, S.; etÂ al. 2021.

</span>
<span class="ltx_bibblock">Scaling language models: Methods, analysis &amp; insights from training gopher.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.11446</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.-A.; Lacroix, T.; RoziÃ¨re, B.; Goyal, N.; Hambro, E.; Azhar, F.; etÂ al. 2023a.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; etÂ al. 2023b.

</span>
<span class="ltx_bibblock">Llama 2: Open Foundation and Fine-Tuned Chat Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wang, Y.; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu, K.Â R.; Wadden, D.; MacMillan, K.; Smith, N.Â A.; Beltagy, I.; and Hajishirzi, H. 2023.

</span>
<span class="ltx_bibblock">How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2306.04751.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Wei, J.; Tay, Y.; Bommasani, R.; Raffel, C.; Zoph, B.; Borgeaud, S.; Yogatama, D.; Bosma, M.; Zhou, D.; Metzler, D.; Chi, E.Â H.; Hashimoto, T.; Vinyals, O.; Liang, P.; Dean, J.; and Fedus, W. 2022.

</span>
<span class="ltx_bibblock">Emergent Abilities of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Trans. Mach. Learn. Res.</em>, 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiegreffe etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wiegreffe, S.; Hessel, J.; Swayamdipta, S.; Riedl, M.; and Choi, Y. 2021.

</span>
<span class="ltx_bibblock">Reframing human-AI collaboration for generating free-text explanations.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.08674</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ye, S.; Kim, D.; Kim, S.; Hwang, H.; Kim, S.; Jo, Y.; Thorne, J.; Kim, J.; and Seo, M. 2023.

</span>
<span class="ltx_bibblock">FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2307.10928.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yuan, S.; Chen, J.; Fu, Z.; Ge, X.; Shah, S.; Jankowski, C.Â R.; Yang, D.; and Xiao, Y. 2023.

</span>
<span class="ltx_bibblock">Distilling Script Knowledge from Large Language Models for Constrained Language Planning.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.05252</em>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhao, W.Â X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y.; Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.; Chen, Y.; Chen, Z.; Jiang, J.; Ren, R.; Li, Y.; Tang, X.; Liu, Z.; Liu, P.; Nie, J.; and Wen, J. 2023.

</span>
<span class="ltx_bibblock">A Survey of Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.18223.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; etÂ al. 2023.

</span>
<span class="ltx_bibblock">Judging LLM-as-a-judge with MT-Bench and Chatbot Arena.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2306.05685</em>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.03941" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.03942" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.03942">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.03942" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.03943" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 13:38:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
