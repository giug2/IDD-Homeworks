<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.12252] Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles</title><meta property="og:description" content="Acquiring data to train deep learning-based object detectors on Unmanned Aerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited by law in specific environments. On the other hand, synthetic data…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.12252">

<!--Generated on Fri Mar  1 15:36:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Leveraging Synthetic Data in Object Detection
on Unmanned Aerial Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Kiefer1


</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Tuebingen
<br class="ltx_break">benjamin.kiefer@uni-tuebingen.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Ott1
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Tuebingen
<br class="ltx_break">david.ott@uni-tuebingen.de
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andreas Zell
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Tuebingen
<br class="ltx_break">andreas.zell@uni-tuebingen.de
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Acquiring data to train deep learning-based object detectors on Unmanned Aerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited by law in specific environments. On the other hand, synthetic data is fast and cheap to access.
In this work, we explore the potential use of synthetic data in object detection from UAVs across various application environments. For that, we extend the open-source framework DeepGTAV to work for UAV scenarios. We capture various large-scale high-resolution synthetic data sets in several domains to demonstrate their use in real-world object detection from UAVs by analyzing multiple training strategies across several models. Furthermore, we analyze several different data generation and sampling parameters to provide actionable engineering advice for further scientific research. The DeepGTAV framework is available at <a target="_blank" href="https://git.io/Jyf5j" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://git.io/Jyf5j</a>.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution. This work has been supported by the German Ministry for Economic Affairs and Energy, Project Avalon, FKZ: 03SX481B.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Unmanned Aerial Vehicles (UAVs) equipped with cameras are increasingly used as autonomous vision systems in a wide range of applications, such as traffic surveillance, search and rescue, agriculture and smart cities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In all of these scenarios, these systems rely on the robust detection of objects of interest. Although object detection on natural images taken from hand-held or car-mounted cameras has been studied intensively, object detection from UAVs trails behind in performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This is partly due to the limited amount of annotated publicly available data sets. In turn, this is partly caused by the highly complex data generating missions, which are subject to permissions, UAV flying restrictions and environmental factors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Furthermore, there are more degrees of freedom in the UAV domain (camera angles, position), which account for objects from unnatural perspectives, e.g. small objects from above. These difficulties are on top of other common obstacles, such as high and enduring labeling costs.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">With more publicly available data sets, object detection on UAVs could be improved. However, data collection and labeling are expensive and time-consuming. Furthermore, data set collection raises serious privacy (e.g. GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> in Europe) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and security concerns because specific locations demand a long and complicated approval procedure.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Moreover, currently published data sets suffer from large class and domain imbalances <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Both problems are inherently caused by a problematic capturing procedure, where many variables cannot be controlled.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">On the other hand, synthetically generated data for computer vision problems can help train data demanding visual perception systems because it is comparably fast and inexpensive to acquire. Furthermore, this data can easily be tailored to specific requirements. Several works address synthetic data generation in computer vision. However, most of them focus on driving, simulating constrained traffic situations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Few works consider the generation of synthetic data captured from UAVs. However, these only focus on the capturing process of the sensors and are lacking in world, object and physics details, or do not feature them at all <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<table id="S1.F1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.F1.1.1" class="ltx_tr">
<td id="S1.F1.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2112.12252/assets/x1.png" id="S1.F1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="325" height="293" alt="Refer to caption"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Real (left) and synthetic (right) image samples in different applications scenarios with ground truth annotations. Representative objects are magnified.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">While the use of synthetic data in the context of autonomous driving has been investigated thoroughly, it is not clear whether these findings can be applied in the UAV setting. As mentioned in the beginning, the problems for general UAV object detection also apply to synthetic data generation. Most simulation engines focus on autonomous driving, and therefore rendering is aimed at looking realistic for these scenarios. Even if these simulation engines could technically be adapted to the UAV setting, light conditions, shadows, visibility range, rendered resolution, and more may significantly affect the quality of the rendered footage. In turn, the generated data may be less valuable for transfer to the real world.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In this work, we consider the video game Grand Theft Auto V (GTAV) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as a simulation platform. It offers numerous detailed object models that interact in a large world with realistic graphics and physics simulations. Building on previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we extend the DeepGTAV data extraction tool to work for airborne scenarios by extending its functionalities regarding in-game agent and camera position and rotation, environment manipulation, object spawning and metadata extraction.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Using this simulation engine, we create three large-scale high-resolution (4K) synthetic object detection data sets in different application scenarios. Using these, we evaluate different training strategies and their transfer performances on three corresponding real-world data sets. We provide analytical insights and actionable advice on which settings to choose by doing extensive experiments and ablations.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">In particular, our contributions are as follows:</p>
</div>
<div id="S1.p9" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We modify and extend the DeepGTAV tool, in particular, to allow the production of airborne data. We discuss the specific improvements in Section <a href="#S2.SS1.SSS1" title="II-A1 Tool Description and Improvements ‣ II-A DeepGTA-UAV ‣ II Experimental Setup ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span>1</span></a>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We provide three large-scale high-resolution metadata annotated data sets in different UAV application scenarios and make them publicly available.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We evaluate the applicability of these data sets to improve real-world object detection and analyze the from-scratch performance.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We analyze the influence of different parameters of the data generation, e.g. the graphics quality and the alignment of metadata.</p>
</div>
</li>
</ul>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S1.SS1.5.1.1" class="ltx_text">I-A</span> </span><span id="S1.SS1.6.2" class="ltx_text ltx_font_italic">Related Work</span>
</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">This section reviews the literature regarding synthetic data generation engines, synthetic data taken from UAVs and the role of meta/environmental data in this context.</p>
</div>
<section id="S1.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS1.5.1.1" class="ltx_text">I-A</span>1 </span>Synthetic Data Generation Engines</h4>

<div id="S1.SS1.SSS1.p1" class="ltx_para">
<p id="S1.SS1.SSS1.p1.1" class="ltx_p">Many data generation engines focus on autonomous driving. AirSim <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and Carla <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> leverage the Unreal Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to create a simulation engine suitable for autonomous driving scenarios. While AirSim also provides support for UAV scenarios, it lacks a world to simulate objects and its physics implementation, which must be created and modeled first.
Instead of laboriously creating simulation engines, some researchers use the computer game GTAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to generate data. With DeepGTAV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, researchers leverage GTAV to gather data for autonomous driving. PreSIL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> builds upon this approach to refine the data acquisition process with their tool DeepGTAV-PreSIL. However, both systems lack the feature to modify the in-game agent and camera position and rotation, essentially limiting it to a single autonomous driving scenario. Further manipulations, such as spawning objects, manipulating the environment and extracting the metadata, are not possible either. The work at hand builds upon these two works, improving and extending them to open it for UAV research.</p>
</div>
<div id="S1.SS1.SSS1.p2" class="ltx_para">
<p id="S1.SS1.SSS1.p2.1" class="ltx_p">There are many more synthetic, simulated environments. Please see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for an overview.</p>
</div>
</section>
<section id="S1.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS2.5.1.1" class="ltx_text">I-A</span>2 </span>Data Sets Taken on UAVs</h4>

<div id="S1.SS1.SSS2.p1" class="ltx_para">
<p id="S1.SS1.SSS2.p1.1" class="ltx_p">The first large-scale real-world object detection data sets taken on UAVs were VisDrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and UAVDT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Other data sets for object detection and tracking emerged with many different foci <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.</p>
</div>
<div id="S1.SS1.SSS2.p2" class="ltx_para">
<p id="S1.SS1.SSS2.p2.1" class="ltx_p">The need for data sets caused many researchers to focus on synthetic data. Mid-Air <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> presents a synthetic data set for unstructured environments captured with the Unreal Engine <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> in combination with AirSim <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. They laboriously built a world with landscape and streets for drone navigation. However, their world is lifeless and does not feature any objects of interest. The Synthinel-1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> data set features synthetic data showing building footprints with segmentation masks.</p>
</div>
<div id="S1.SS1.SSS2.p3" class="ltx_para">
<p id="S1.SS1.SSS2.p3.1" class="ltx_p">An overview comparing the most important data sets is shown in Table <a href="#S1.T1" title="TABLE I ‣ I-A3 Narrowing the Sim-to-Real Domain Gap ‣ I-A Related Work ‣ I Introduction ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
<section id="S1.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S1.SS1.SSS3.5.1.1" class="ltx_text">I-A</span>3 </span>Narrowing the Sim-to-Real Domain Gap</h4>

<div id="S1.SS1.SSS3.p1" class="ltx_para">
<p id="S1.SS1.SSS3.p1.1" class="ltx_p">On the model level, many approaches apply synthetic-to-real domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. By disentangling the features, these techniques aim to learn domain invariant features that lead to better transfer capabilities. A subset of these methods performs image stylization to make the synthetic images look more similar to their real-world counterparts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, referred to as narrowing the appearance gap.
Another domain gap arises from differences in content, called content gap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. It depicts the layout and types of objects featured in the synthetic world instead of in the real world.</p>
</div>
<div id="S1.SS1.SSS3.p2" class="ltx_para">
<p id="S1.SS1.SSS3.p2.1" class="ltx_p">Orthogonal to these gaps, we focus on another gap called meta gap. Meta gap depicts the imaging conditions at the time of capture, such as altitude, viewing angle and time. It can be seen as a useful abstraction of the content gap. These metadata are freely available in the synthetic engine and on an actual UAV. We leverage these metadata to align the distributions of the synthetic and real-world data set, leading to better performance and data efficiency.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison with the most prominent annotated aerial object detection data sets in three domains.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">Data Set</td>
<td id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">Domain</td>
<td id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">Data Type</td>
<td id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r"># Images</td>
<td id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r">Platform</td>
<td id="S1.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r">Image Widths</td>
<td id="S1.T1.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r">Altitude</td>
<td id="S1.T1.1.1.1.8" class="ltx_td ltx_align_center ltx_border_r">Angle</td>
<td id="S1.T1.1.1.1.9" class="ltx_td ltx_align_center">Other meta</td>
</tr>
<tr id="S1.T1.1.2.2" class="ltx_tr">
<td id="S1.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VisDrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S1.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">traffic</td>
<td id="S1.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">real</td>
<td id="S1.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10,209</td>
<td id="S1.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">UAV</td>
<td id="S1.T1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">960-2,000</td>
<td id="S1.T1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✕</td>
<td id="S1.T1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✕</td>
<td id="S1.T1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_tt">✕</td>
</tr>
<tr id="S1.T1.1.3.3" class="ltx_tr">
<td id="S1.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r">AU-AIR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</td>
<td id="S1.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">traffic</td>
<td id="S1.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">real</td>
<td id="S1.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">32,823</td>
<td id="S1.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r">UAV</td>
<td id="S1.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r">1,920</td>
<td id="S1.T1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.3.3.9" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.1.4.4" class="ltx_tr">
<td id="S1.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r">PreSIL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S1.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">traffic</td>
<td id="S1.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">synthetic</td>
<td id="S1.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">40,000</td>
<td id="S1.T1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r">car</td>
<td id="S1.T1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r">1,920</td>
<td id="S1.T1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S1.T1.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r">–</td>
<td id="S1.T1.1.4.4.9" class="ltx_td ltx_align_center">✕</td>
</tr>
<tr id="S1.T1.1.5.5" class="ltx_tr">
<td id="S1.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.5.5.1.1" class="ltx_text ltx_font_bold">DGTA-VisDrone</span></td>
<td id="S1.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">traffic</td>
<td id="S1.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r">synthetic</td>
<td id="S1.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r">50,000</td>
<td id="S1.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r">UAV</td>
<td id="S1.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r">3840</td>
<td id="S1.T1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.5.5.9" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.1.6.6" class="ltx_tr">
<td id="S1.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Airbus Ship <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S1.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">maritime</td>
<td id="S1.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">real</td>
<td id="S1.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">40,000</td>
<td id="S1.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">satellite</td>
<td id="S1.T1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">768</td>
<td id="S1.T1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S1.T1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S1.T1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_tt">✕</td>
</tr>
<tr id="S1.T1.1.7.7" class="ltx_tr">
<td id="S1.T1.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r">SeaDronesSee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S1.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">maritime</td>
<td id="S1.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r">real</td>
<td id="S1.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">5,630</td>
<td id="S1.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">UAV</td>
<td id="S1.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r">3,840-5,456</td>
<td id="S1.T1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.7.7.9" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.1.8.8" class="ltx_tr">
<td id="S1.T1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.8.8.1.1" class="ltx_text ltx_font_bold">DGTA-SeaDronesSee</span></td>
<td id="S1.T1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">maritime</td>
<td id="S1.T1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">synthetic</td>
<td id="S1.T1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">100,000</td>
<td id="S1.T1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r">UAV</td>
<td id="S1.T1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">3,840</td>
<td id="S1.T1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.8.8.9" class="ltx_td ltx_align_center">✓</td>
</tr>
<tr id="S1.T1.1.9.9" class="ltx_tr">
<td id="S1.T1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Cattle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>
</td>
<td id="S1.T1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">agriculture</td>
<td id="S1.T1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">real</td>
<td id="S1.T1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">670</td>
<td id="S1.T1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">UAV</td>
<td id="S1.T1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">4,000</td>
<td id="S1.T1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✕</td>
<td id="S1.T1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✕</td>
<td id="S1.T1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_tt">✕</td>
</tr>
<tr id="S1.T1.1.10.10" class="ltx_tr">
<td id="S1.T1.1.10.10.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.10.10.1.1" class="ltx_text ltx_font_bold">DGTA-Cattle</span></td>
<td id="S1.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">agriculture</td>
<td id="S1.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">synthetic</td>
<td id="S1.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">50,000</td>
<td id="S1.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r">UAV</td>
<td id="S1.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_r">3,840</td>
<td id="S1.T1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S1.T1.1.10.10.9" class="ltx_td ltx_align_center">✓</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Experimental Setup</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">DeepGTA-UAV</span>
</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.5.1.1" class="ltx_text">II-A</span>1 </span>Tool Description and Improvements</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">The DeepGTAV framework as used in this work builds upon the DeepGTAV-PreSIL framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, built upon the DeepGTAV framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>.
Originally, DeepGTAV was built as a reinforcement learning environment for self-driving cars, providing functionality to interact with GTAV through a TCP-server and building upon the functionality of SkriptHookV <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.
The Python library VPilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>
interacts with DeepGTAV, which runs in GTAV.</p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p">DeepGTA-PreSIL integrates DeepGTAV and GTAVisionExport <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>,
the technique presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, to extract depth and stencil buffers from the rendering pipeline. With those and the world coordinates of objects extracted in GTAV, pixel-wise object segmentation data can be extracted. From those, object bounding boxes can be calculated. The DeepGTAV framework also allows extracting voxel-wise LiDAR segmentation data, which was not used in this work. The most notable improvements made in this work include:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Increasing the ease of use and the multitude of scenarios of which synthetic data can be generated by improving the VPilot interface of
DeepGTAV. In particular, it is now possible to freely modify the in-game position, camera position and rotation, manipulate the environment
(time of day, weather), spawn objects (e.g. pedestrians, cars) and specify their animations.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Improving the speed and reliability of the DeepGTAV framework (to obtain almost no overhead, compared to running GTAV natively).
</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">Allowing the extraction of metadata of the generated data (like time of day, height, camera angle, …).</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">Providing multiple easily comprehensible and modifiable data generation scripts for several airborne scenarios and different strategies of metadata distribution.</p>
</div>
</li>
<li id="S2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S2.I1.i5.p1" class="ltx_para">
<p id="S2.I1.i5.p1.1" class="ltx_p">Modifications to capture 4k image data (although we did not analyze the influence of high resolution data).</p>
</div>
</li>
</ol>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p">In this work, those adaptations were used to capture object detection data from a UAV perspective (in comparison to an autonomous car scenario in
previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>).</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.5.1.1" class="ltx_text">II-A</span>2 </span>Modifications of GTAV</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">For optimal use as a simulation environment, some modifications were made to the GTAV game files by installing the modifications <span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_italic">Simple Increase Traffic (and Pedestrian)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, <span id="S2.SS1.SSS2.p1.1.2" class="ltx_text ltx_font_italic">No chromatic aberration lens distortion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> and <span id="S2.SS1.SSS2.p1.1.3" class="ltx_text ltx_font_italic">Heap Limit Adjuster</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>.
Additionally, the automatic spawning of objects was modified to match the class distribution in VisDrone.</p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.1" class="ltx_p">Furthermore, the bounding box quality was improved by setting the game resolution to 7680x4320DSR in NVIDIA GeForce Experience <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>. This results in an upscaling of the rendering buffers to 4k, which is needed to obtain pixel-perfect object segmentation data for 4k images.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Data Set Generation</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">To assess the usefulness of synthetic data as training data for real-world scenarios, particularly to assess the usefulness of DeepGTAV and its adaptability in this context, we examined three different object detection scenarios.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In choosing those three scenarios, we are bound to existing real-world data sets to assess the real-world performance. We find that the data sets VisDrone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, SeaDronesSee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and Cattle <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> are very popular in their respective application domain and therefore choose these. VisDrone aims for traffic surveillance in Asian cities with very crowded scenes. SeaDronesSee’s application domain is search and rescue in open water featuring swimmers and boats, with the main challenges being reflective regions, shadows and waves or seafoam. Finally, Cattle aims to bring autonomous vision systems to agriculture (cattle detection).</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Using the DeepGTAV framework, we generate synthetic training data for these scenarios by specifying VPilot data generation scripts for each scenario. In the following, we briefly describe the different capturing procedures employed in the VPilot scripts used to generate the synthetic data sets.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">For the generation of all synthetic data sets, the game world of GTAV is systematically
traversed. New images are captured with one frame per second to obtain mainly distinct images. We export the 4k images but discard the segmentation and depth maps to focus on pure object detection. Along with every frame, we export the corresponding ground truth bounding boxes and the meta labels, i.e. altitude, principal axes (yaw, pitch, roll of camera rotation), time of the day and weather state.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.2" class="ltx_p">For the random traversals of the game world, the camera height and angles were varied.
Additional in-game objects were spawned (e.g. vehicles, pedestrians). See Table <a href="#S2.T2" title="TABLE II ‣ II-B Data Set Generation ‣ II Experimental Setup ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> for details. For example, in Cattle, every two seconds, four cows are spawned <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="50-250" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mn id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml">50</mn><mo id="S2.SS2.p5.1.m1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.cmml">−</mo><mn id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><minus id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1"></minus><cn type="integer" id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2">50</cn><cn type="integer" id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">250</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">50-250</annotation></semantics></math>m in front of the camera with a left-right offset of <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="-160-160" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><mrow id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml"><mo id="S2.SS2.p5.2.m2.1.1.2a" xref="S2.SS2.p5.2.m2.1.1.2.cmml">−</mo><mn id="S2.SS2.p5.2.m2.1.1.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.cmml">160</mn></mrow><mo id="S2.SS2.p5.2.m2.1.1.1" xref="S2.SS2.p5.2.m2.1.1.1.cmml">−</mo><mn id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><minus id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1.1"></minus><apply id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2"><minus id="S2.SS2.p5.2.m2.1.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2"></minus><cn type="integer" id="S2.SS2.p5.2.m2.1.1.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2">160</cn></apply><cn type="integer" id="S2.SS2.p5.2.m2.1.1.3.cmml" xref="S2.SS2.p5.2.m2.1.1.3">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">-160-160</annotation></semantics></math>m.
The objects were spawned in the in-game traffic and pathfinding and were despawned after 200 seconds.
Additionally, a new random travel location in this area was chosen every 60 seconds to prevent the in-game navigation from getting stuck.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p">Concise descriptions of the data sets are given in Tables <a href="#S1.T1" title="TABLE I ‣ I-A3 Narrowing the Sim-to-Real Domain Gap ‣ I-A Related Work ‣ I Introduction ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> and <a href="#S3.T4" title="TABLE IV ‣ III-A General Benefit of Synthetic Data in UAV Object Detection ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Data set generation settings.</figcaption>
<table id="S2.T2.36" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.36.37.1" class="ltx_tr">
<th id="S2.T2.36.37.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">DGTA-</th>
<th id="S2.T2.36.37.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Cattle</th>
<th id="S2.T2.36.37.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">SeaDronesSee</th>
<th id="S2.T2.36.37.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">VisDrone</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.7.7" class="ltx_tr">
<td id="S2.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.1.1.1.1.2" class="ltx_tr">
<td id="S2.T2.1.1.1.1.2.1" class="ltx_td ltx_align_center">GPS</td>
</tr>
<tr id="S2.T2.1.1.1.1.1" class="ltx_tr">
<td id="S2.T2.1.1.1.1.1.1" class="ltx_td ltx_align_center"><math id="S2.T2.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\times 10^{3}" display="inline"><semantics id="S2.T2.1.1.1.1.1.1.m1.1a"><mrow id="S2.T2.1.1.1.1.1.1.m1.1.1" xref="S2.T2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T2.1.1.1.1.1.1.m1.1.1.2" xref="S2.T2.1.1.1.1.1.1.m1.1.1.2.cmml"></mi><mo lspace="0.222em" rspace="0.222em" id="S2.T2.1.1.1.1.1.1.m1.1.1.1" xref="S2.T2.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><msup id="S2.T2.1.1.1.1.1.1.m1.1.1.3" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3.cmml"><mn id="S2.T2.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3.2.cmml">10</mn><mn id="S2.T2.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3.3.cmml">3</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.1.1.m1.1b"><apply id="S2.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1"><times id="S2.T2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.1"></times><csymbol cd="latexml" id="S2.T2.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.2">absent</csymbol><apply id="S2.T2.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T2.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S2.T2.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3.2">10</cn><cn type="integer" id="S2.T2.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1.3.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.1.1.m1.1c">\times 10^{3}</annotation></semantics></math></td>
</tr>
</table>
</td>
<td id="S2.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.3.3.3.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.2.2.2.1.1" class="ltx_tr">
<td id="S2.T2.2.2.2.1.1.1" class="ltx_td ltx_align_center"><math id="S2.T2.2.2.2.1.1.1.m1.2" class="ltx_math_unparsed" alttext="[0,17]\times" display="inline"><semantics id="S2.T2.2.2.2.1.1.1.m1.2a"><mrow id="S2.T2.2.2.2.1.1.1.m1.2b"><mrow id="S2.T2.2.2.2.1.1.1.m1.2.3"><mo stretchy="false" id="S2.T2.2.2.2.1.1.1.m1.2.3.1">[</mo><mn id="S2.T2.2.2.2.1.1.1.m1.1.1">0</mn><mo id="S2.T2.2.2.2.1.1.1.m1.2.3.2">,</mo><mn id="S2.T2.2.2.2.1.1.1.m1.2.2">17</mn><mo rspace="0.055em" stretchy="false" id="S2.T2.2.2.2.1.1.1.m1.2.3.3">]</mo></mrow><mo id="S2.T2.2.2.2.1.1.1.m1.2.4">×</mo></mrow><annotation encoding="application/x-tex" id="S2.T2.2.2.2.1.1.1.m1.2c">[0,17]\times</annotation></semantics></math></td>
</tr>
<tr id="S2.T2.3.3.3.2.2" class="ltx_tr">
<td id="S2.T2.3.3.3.2.2.1" class="ltx_td ltx_align_center"><math id="S2.T2.3.3.3.2.2.1.m1.2" class="ltx_Math" alttext="[13,22]" display="inline"><semantics id="S2.T2.3.3.3.2.2.1.m1.2a"><mrow id="S2.T2.3.3.3.2.2.1.m1.2.3.2" xref="S2.T2.3.3.3.2.2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S2.T2.3.3.3.2.2.1.m1.2.3.2.1" xref="S2.T2.3.3.3.2.2.1.m1.2.3.1.cmml">[</mo><mn id="S2.T2.3.3.3.2.2.1.m1.1.1" xref="S2.T2.3.3.3.2.2.1.m1.1.1.cmml">13</mn><mo id="S2.T2.3.3.3.2.2.1.m1.2.3.2.2" xref="S2.T2.3.3.3.2.2.1.m1.2.3.1.cmml">,</mo><mn id="S2.T2.3.3.3.2.2.1.m1.2.2" xref="S2.T2.3.3.3.2.2.1.m1.2.2.cmml">22</mn><mo stretchy="false" id="S2.T2.3.3.3.2.2.1.m1.2.3.2.3" xref="S2.T2.3.3.3.2.2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.3.3.3.2.2.1.m1.2b"><interval closure="closed" id="S2.T2.3.3.3.2.2.1.m1.2.3.1.cmml" xref="S2.T2.3.3.3.2.2.1.m1.2.3.2"><cn type="integer" id="S2.T2.3.3.3.2.2.1.m1.1.1.cmml" xref="S2.T2.3.3.3.2.2.1.m1.1.1">13</cn><cn type="integer" id="S2.T2.3.3.3.2.2.1.m1.2.2.cmml" xref="S2.T2.3.3.3.2.2.1.m1.2.2">22</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.3.3.3.2.2.1.m1.2c">[13,22]</annotation></semantics></math></td>
</tr>
</table>
</td>
<td id="S2.T2.5.5.5" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.5.5.5.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.4.4.4.1.1" class="ltx_tr">
<td id="S2.T2.4.4.4.1.1.1" class="ltx_td ltx_align_center"><math id="S2.T2.4.4.4.1.1.1.m1.1" class="ltx_math_unparsed" alttext="[-28,-18]\times" display="inline"><semantics id="S2.T2.4.4.4.1.1.1.m1.1a"><mrow id="S2.T2.4.4.4.1.1.1.m1.1b"><mrow id="S2.T2.4.4.4.1.1.1.m1.1.1"><mo stretchy="false" id="S2.T2.4.4.4.1.1.1.m1.1.1.1">[</mo><mo lspace="0em" id="S2.T2.4.4.4.1.1.1.m1.1.1.2">−</mo><mn id="S2.T2.4.4.4.1.1.1.m1.1.1.3">28</mn><mo rspace="0em" id="S2.T2.4.4.4.1.1.1.m1.1.1.4">,</mo><mo lspace="0em" id="S2.T2.4.4.4.1.1.1.m1.1.1.5">−</mo><mn id="S2.T2.4.4.4.1.1.1.m1.1.1.6">18</mn><mo rspace="0.055em" stretchy="false" id="S2.T2.4.4.4.1.1.1.m1.1.1.7">]</mo></mrow><mo id="S2.T2.4.4.4.1.1.1.m1.1.2">×</mo></mrow><annotation encoding="application/x-tex" id="S2.T2.4.4.4.1.1.1.m1.1c">[-28,-18]\times</annotation></semantics></math></td>
</tr>
<tr id="S2.T2.5.5.5.2.2" class="ltx_tr">
<td id="S2.T2.5.5.5.2.2.1" class="ltx_td ltx_align_center"><math id="S2.T2.5.5.5.2.2.1.m1.2" class="ltx_Math" alttext="[-25,-13]" display="inline"><semantics id="S2.T2.5.5.5.2.2.1.m1.2a"><mrow id="S2.T2.5.5.5.2.2.1.m1.2.2.2" xref="S2.T2.5.5.5.2.2.1.m1.2.2.3.cmml"><mo stretchy="false" id="S2.T2.5.5.5.2.2.1.m1.2.2.2.3" xref="S2.T2.5.5.5.2.2.1.m1.2.2.3.cmml">[</mo><mrow id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.cmml"><mo id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1a" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.cmml">−</mo><mn id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.2" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.2.cmml">25</mn></mrow><mo id="S2.T2.5.5.5.2.2.1.m1.2.2.2.4" xref="S2.T2.5.5.5.2.2.1.m1.2.2.3.cmml">,</mo><mrow id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.cmml"><mo id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2a" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.cmml">−</mo><mn id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.2" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.2.cmml">13</mn></mrow><mo stretchy="false" id="S2.T2.5.5.5.2.2.1.m1.2.2.2.5" xref="S2.T2.5.5.5.2.2.1.m1.2.2.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.5.5.5.2.2.1.m1.2b"><interval closure="closed" id="S2.T2.5.5.5.2.2.1.m1.2.2.3.cmml" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2"><apply id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.cmml" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1"><minus id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.1.cmml" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1"></minus><cn type="integer" id="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.2.cmml" xref="S2.T2.5.5.5.2.2.1.m1.1.1.1.1.2">25</cn></apply><apply id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.cmml" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2"><minus id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.1.cmml" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2"></minus><cn type="integer" id="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.2.cmml" xref="S2.T2.5.5.5.2.2.1.m1.2.2.2.2.2">13</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.5.5.5.2.2.1.m1.2c">[-25,-13]</annotation></semantics></math></td>
</tr>
</table>
</td>
<td id="S2.T2.7.7.7" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.7.7.7.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.6.6.6.1.1" class="ltx_tr">
<td id="S2.T2.6.6.6.1.1.1" class="ltx_td ltx_align_center"><math id="S2.T2.6.6.6.1.1.1.m1.1" class="ltx_math_unparsed" alttext="[-12,14]\times" display="inline"><semantics id="S2.T2.6.6.6.1.1.1.m1.1a"><mrow id="S2.T2.6.6.6.1.1.1.m1.1b"><mrow id="S2.T2.6.6.6.1.1.1.m1.1.2"><mo stretchy="false" id="S2.T2.6.6.6.1.1.1.m1.1.2.1">[</mo><mo lspace="0em" id="S2.T2.6.6.6.1.1.1.m1.1.2.2">−</mo><mn id="S2.T2.6.6.6.1.1.1.m1.1.2.3">12</mn><mo id="S2.T2.6.6.6.1.1.1.m1.1.2.4">,</mo><mn id="S2.T2.6.6.6.1.1.1.m1.1.1">14</mn><mo rspace="0.055em" stretchy="false" id="S2.T2.6.6.6.1.1.1.m1.1.2.5">]</mo></mrow><mo id="S2.T2.6.6.6.1.1.1.m1.1.3">×</mo></mrow><annotation encoding="application/x-tex" id="S2.T2.6.6.6.1.1.1.m1.1c">[-12,14]\times</annotation></semantics></math></td>
</tr>
<tr id="S2.T2.7.7.7.2.2" class="ltx_tr">
<td id="S2.T2.7.7.7.2.2.1" class="ltx_td ltx_align_center"><math id="S2.T2.7.7.7.2.2.1.m1.2" class="ltx_Math" alttext="[-22,13]" display="inline"><semantics id="S2.T2.7.7.7.2.2.1.m1.2a"><mrow id="S2.T2.7.7.7.2.2.1.m1.2.2.1" xref="S2.T2.7.7.7.2.2.1.m1.2.2.2.cmml"><mo stretchy="false" id="S2.T2.7.7.7.2.2.1.m1.2.2.1.2" xref="S2.T2.7.7.7.2.2.1.m1.2.2.2.cmml">[</mo><mrow id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.cmml"><mo id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1a" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.cmml">−</mo><mn id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.2" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.2.cmml">22</mn></mrow><mo id="S2.T2.7.7.7.2.2.1.m1.2.2.1.3" xref="S2.T2.7.7.7.2.2.1.m1.2.2.2.cmml">,</mo><mn id="S2.T2.7.7.7.2.2.1.m1.1.1" xref="S2.T2.7.7.7.2.2.1.m1.1.1.cmml">13</mn><mo stretchy="false" id="S2.T2.7.7.7.2.2.1.m1.2.2.1.4" xref="S2.T2.7.7.7.2.2.1.m1.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.7.7.7.2.2.1.m1.2b"><interval closure="closed" id="S2.T2.7.7.7.2.2.1.m1.2.2.2.cmml" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1"><apply id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.cmml" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1"><minus id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.1.cmml" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1"></minus><cn type="integer" id="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.2.cmml" xref="S2.T2.7.7.7.2.2.1.m1.2.2.1.1.2">22</cn></apply><cn type="integer" id="S2.T2.7.7.7.2.2.1.m1.1.1.cmml" xref="S2.T2.7.7.7.2.2.1.m1.1.1">13</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.7.7.7.2.2.1.m1.2c">[-22,13]</annotation></semantics></math></td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.13.13" class="ltx_tr">
<td id="S2.T2.13.13.7" class="ltx_td ltx_align_center ltx_border_t">Altitude</td>
<td id="S2.T2.9.9.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.8.8.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S2.T2.8.8.1.m1.1a"><mn id="S2.T2.8.8.1.m1.1.1" xref="S2.T2.8.8.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S2.T2.8.8.1.m1.1b"><cn type="integer" id="S2.T2.8.8.1.m1.1.1.cmml" xref="S2.T2.8.8.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.8.8.1.m1.1c">10</annotation></semantics></math>-<math id="S2.T2.9.9.2.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S2.T2.9.9.2.m2.1a"><mn id="S2.T2.9.9.2.m2.1.1" xref="S2.T2.9.9.2.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S2.T2.9.9.2.m2.1b"><cn type="integer" id="S2.T2.9.9.2.m2.1.1.cmml" xref="S2.T2.9.9.2.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.9.9.2.m2.1c">80</annotation></semantics></math>m</td>
<td id="S2.T2.11.11.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.10.10.3.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.T2.10.10.3.m1.1a"><mn id="S2.T2.10.10.3.m1.1.1" xref="S2.T2.10.10.3.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.T2.10.10.3.m1.1b"><cn type="integer" id="S2.T2.10.10.3.m1.1.1.cmml" xref="S2.T2.10.10.3.m1.1.1">0</cn></annotation-xml></semantics></math>-<math id="S2.T2.11.11.4.m2.1" class="ltx_Math" alttext="80" display="inline"><semantics id="S2.T2.11.11.4.m2.1a"><mn id="S2.T2.11.11.4.m2.1.1" xref="S2.T2.11.11.4.m2.1.1.cmml">80</mn><annotation-xml encoding="MathML-Content" id="S2.T2.11.11.4.m2.1b"><cn type="integer" id="S2.T2.11.11.4.m2.1.1.cmml" xref="S2.T2.11.11.4.m2.1.1">80</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.11.11.4.m2.1c">80</annotation></semantics></math>m</td>
<td id="S2.T2.13.13.6" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.12.12.5.m1.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S2.T2.12.12.5.m1.1a"><mn id="S2.T2.12.12.5.m1.1.1" xref="S2.T2.12.12.5.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.T2.12.12.5.m1.1b"><cn type="integer" id="S2.T2.12.12.5.m1.1.1.cmml" xref="S2.T2.12.12.5.m1.1.1">0</cn></annotation-xml></semantics></math>-<math id="S2.T2.13.13.6.m2.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S2.T2.13.13.6.m2.1a"><mn id="S2.T2.13.13.6.m2.1.1" xref="S2.T2.13.13.6.m2.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S2.T2.13.13.6.m2.1b"><cn type="integer" id="S2.T2.13.13.6.m2.1.1.cmml" xref="S2.T2.13.13.6.m2.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.13.13.6.m2.1c">40</annotation></semantics></math>m</td>
</tr>
<tr id="S2.T2.19.19" class="ltx_tr">
<td id="S2.T2.19.19.7" class="ltx_td ltx_align_center ltx_border_t">Cam Pitch</td>
<td id="S2.T2.15.15.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.14.14.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S2.T2.14.14.1.m1.1a"><mn id="S2.T2.14.14.1.m1.1.1" xref="S2.T2.14.14.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S2.T2.14.14.1.m1.1b"><cn type="integer" id="S2.T2.14.14.1.m1.1.1.cmml" xref="S2.T2.14.14.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.14.14.1.m1.1c">20</annotation></semantics></math>-<math id="S2.T2.15.15.2.m2.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S2.T2.15.15.2.m2.1a"><mn id="S2.T2.15.15.2.m2.1.1" xref="S2.T2.15.15.2.m2.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S2.T2.15.15.2.m2.1b"><cn type="integer" id="S2.T2.15.15.2.m2.1.1.cmml" xref="S2.T2.15.15.2.m2.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.15.15.2.m2.1c">90</annotation></semantics></math>°</td>
<td id="S2.T2.17.17.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.16.16.3.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S2.T2.16.16.3.m1.1a"><mn id="S2.T2.16.16.3.m1.1.1" xref="S2.T2.16.16.3.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S2.T2.16.16.3.m1.1b"><cn type="integer" id="S2.T2.16.16.3.m1.1.1.cmml" xref="S2.T2.16.16.3.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.16.16.3.m1.1c">20</annotation></semantics></math>-<math id="S2.T2.17.17.4.m2.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S2.T2.17.17.4.m2.1a"><mn id="S2.T2.17.17.4.m2.1.1" xref="S2.T2.17.17.4.m2.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S2.T2.17.17.4.m2.1b"><cn type="integer" id="S2.T2.17.17.4.m2.1.1.cmml" xref="S2.T2.17.17.4.m2.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.17.17.4.m2.1c">90</annotation></semantics></math>°</td>
<td id="S2.T2.19.19.6" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.18.18.5.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S2.T2.18.18.5.m1.1a"><mn id="S2.T2.18.18.5.m1.1.1" xref="S2.T2.18.18.5.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S2.T2.18.18.5.m1.1b"><cn type="integer" id="S2.T2.18.18.5.m1.1.1.cmml" xref="S2.T2.18.18.5.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.18.18.5.m1.1c">20</annotation></semantics></math>-<math id="S2.T2.19.19.6.m2.1" class="ltx_Math" alttext="90" display="inline"><semantics id="S2.T2.19.19.6.m2.1a"><mn id="S2.T2.19.19.6.m2.1.1" xref="S2.T2.19.19.6.m2.1.1.cmml">90</mn><annotation-xml encoding="MathML-Content" id="S2.T2.19.19.6.m2.1b"><cn type="integer" id="S2.T2.19.19.6.m2.1.1.cmml" xref="S2.T2.19.19.6.m2.1.1">90</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.19.19.6.m2.1c">90</annotation></semantics></math>°</td>
</tr>
<tr id="S2.T2.24.24" class="ltx_tr">
<td id="S2.T2.24.24.6" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.24.24.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.24.24.6.1.1" class="ltx_tr">
<td id="S2.T2.24.24.6.1.1.1" class="ltx_td ltx_align_center">Manual</td>
</tr>
<tr id="S2.T2.24.24.6.1.2" class="ltx_tr">
<td id="S2.T2.24.24.6.1.2.1" class="ltx_td ltx_align_center">spawn</td>
</tr>
</table>
</td>
<td id="S2.T2.20.20.1" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.20.20.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.20.20.1.1.1" class="ltx_tr">
<td id="S2.T2.20.20.1.1.1.1" class="ltx_td ltx_align_center">4<math id="S2.T2.20.20.1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.T2.20.20.1.1.1.1.m1.1a"><mo id="S2.T2.20.20.1.1.1.1.m1.1.1" xref="S2.T2.20.20.1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.20.20.1.1.1.1.m1.1b"><times id="S2.T2.20.20.1.1.1.1.m1.1.1.cmml" xref="S2.T2.20.20.1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.20.20.1.1.1.1.m1.1c">\times</annotation></semantics></math>cow@2s</td>
</tr>
</table>
</td>
<td id="S2.T2.22.22.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.22.22.3.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.21.21.2.1.1" class="ltx_tr">
<td id="S2.T2.21.21.2.1.1.1" class="ltx_td ltx_align_center">4<math id="S2.T2.21.21.2.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.T2.21.21.2.1.1.1.m1.1a"><mo id="S2.T2.21.21.2.1.1.1.m1.1.1" xref="S2.T2.21.21.2.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.21.21.2.1.1.1.m1.1b"><times id="S2.T2.21.21.2.1.1.1.m1.1.1.cmml" xref="S2.T2.21.21.2.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.21.21.2.1.1.1.m1.1c">\times</annotation></semantics></math>ppl@2s</td>
</tr>
<tr id="S2.T2.22.22.3.2.2" class="ltx_tr">
<td id="S2.T2.22.22.3.2.2.1" class="ltx_td ltx_align_center">4<math id="S2.T2.22.22.3.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.T2.22.22.3.2.2.1.m1.1a"><mo id="S2.T2.22.22.3.2.2.1.m1.1.1" xref="S2.T2.22.22.3.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.22.22.3.2.2.1.m1.1b"><times id="S2.T2.22.22.3.2.2.1.m1.1.1.cmml" xref="S2.T2.22.22.3.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.22.22.3.2.2.1.m1.1c">\times</annotation></semantics></math>boat@2</td>
</tr>
</table>
</td>
<td id="S2.T2.24.24.5" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.24.24.5.2" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.23.23.4.1.1" class="ltx_tr">
<td id="S2.T2.23.23.4.1.1.1" class="ltx_td ltx_align_center">3<math id="S2.T2.23.23.4.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.T2.23.23.4.1.1.1.m1.1a"><mo id="S2.T2.23.23.4.1.1.1.m1.1.1" xref="S2.T2.23.23.4.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.23.23.4.1.1.1.m1.1b"><times id="S2.T2.23.23.4.1.1.1.m1.1.1.cmml" xref="S2.T2.23.23.4.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.23.23.4.1.1.1.m1.1c">\times</annotation></semantics></math>bike@8</td>
</tr>
<tr id="S2.T2.24.24.5.2.2" class="ltx_tr">
<td id="S2.T2.24.24.5.2.2.1" class="ltx_td ltx_align_center">1<math id="S2.T2.24.24.5.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.T2.24.24.5.2.2.1.m1.1a"><mo id="S2.T2.24.24.5.2.2.1.m1.1.1" xref="S2.T2.24.24.5.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.T2.24.24.5.2.2.1.m1.1b"><times id="S2.T2.24.24.5.2.2.1.m1.1.1.cmml" xref="S2.T2.24.24.5.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.24.24.5.2.2.1.m1.1c">\times</annotation></semantics></math>motor@8</td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T2.30.30" class="ltx_tr">
<td id="S2.T2.30.30.7" class="ltx_td ltx_align_center ltx_border_t">
<table id="S2.T2.30.30.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.30.30.7.1.1" class="ltx_tr">
<td id="S2.T2.30.30.7.1.1.1" class="ltx_td ltx_align_center">Spawn y</td>
</tr>
</table>
</td>
<td id="S2.T2.26.26.2" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.25.25.1.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.T2.25.25.1.m1.1a"><mn id="S2.T2.25.25.1.m1.1.1" xref="S2.T2.25.25.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.T2.25.25.1.m1.1b"><cn type="integer" id="S2.T2.25.25.1.m1.1.1.cmml" xref="S2.T2.25.25.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.25.25.1.m1.1c">50</annotation></semantics></math>-<math id="S2.T2.26.26.2.m2.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S2.T2.26.26.2.m2.1a"><mn id="S2.T2.26.26.2.m2.1.1" xref="S2.T2.26.26.2.m2.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S2.T2.26.26.2.m2.1b"><cn type="integer" id="S2.T2.26.26.2.m2.1.1.cmml" xref="S2.T2.26.26.2.m2.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.26.26.2.m2.1c">250</annotation></semantics></math>m</td>
<td id="S2.T2.28.28.4" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.27.27.3.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.T2.27.27.3.m1.1a"><mn id="S2.T2.27.27.3.m1.1.1" xref="S2.T2.27.27.3.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.T2.27.27.3.m1.1b"><cn type="integer" id="S2.T2.27.27.3.m1.1.1.cmml" xref="S2.T2.27.27.3.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.27.27.3.m1.1c">50</annotation></semantics></math>-<math id="S2.T2.28.28.4.m2.1" class="ltx_Math" alttext="250" display="inline"><semantics id="S2.T2.28.28.4.m2.1a"><mn id="S2.T2.28.28.4.m2.1.1" xref="S2.T2.28.28.4.m2.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S2.T2.28.28.4.m2.1b"><cn type="integer" id="S2.T2.28.28.4.m2.1.1.cmml" xref="S2.T2.28.28.4.m2.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.28.28.4.m2.1c">250</annotation></semantics></math>m</td>
<td id="S2.T2.30.30.6" class="ltx_td ltx_align_center ltx_border_t">
<math id="S2.T2.29.29.5.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.T2.29.29.5.m1.1a"><mn id="S2.T2.29.29.5.m1.1.1" xref="S2.T2.29.29.5.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.T2.29.29.5.m1.1b"><cn type="integer" id="S2.T2.29.29.5.m1.1.1.cmml" xref="S2.T2.29.29.5.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.29.29.5.m1.1c">50</annotation></semantics></math>-<math id="S2.T2.30.30.6.m2.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S2.T2.30.30.6.m2.1a"><mn id="S2.T2.30.30.6.m2.1.1" xref="S2.T2.30.30.6.m2.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S2.T2.30.30.6.m2.1b"><cn type="integer" id="S2.T2.30.30.6.m2.1.1.cmml" xref="S2.T2.30.30.6.m2.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.30.30.6.m2.1c">150</annotation></semantics></math>m</td>
</tr>
<tr id="S2.T2.36.36" class="ltx_tr">
<td id="S2.T2.36.36.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S2.T2.36.36.7.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T2.36.36.7.1.1" class="ltx_tr">
<td id="S2.T2.36.36.7.1.1.1" class="ltx_td ltx_align_center">Spawn x</td>
</tr>
</table>
</td>
<td id="S2.T2.32.32.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S2.T2.31.31.1.m1.1" class="ltx_Math" alttext="-160" display="inline"><semantics id="S2.T2.31.31.1.m1.1a"><mrow id="S2.T2.31.31.1.m1.1.1" xref="S2.T2.31.31.1.m1.1.1.cmml"><mo id="S2.T2.31.31.1.m1.1.1a" xref="S2.T2.31.31.1.m1.1.1.cmml">−</mo><mn id="S2.T2.31.31.1.m1.1.1.2" xref="S2.T2.31.31.1.m1.1.1.2.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.31.31.1.m1.1b"><apply id="S2.T2.31.31.1.m1.1.1.cmml" xref="S2.T2.31.31.1.m1.1.1"><minus id="S2.T2.31.31.1.m1.1.1.1.cmml" xref="S2.T2.31.31.1.m1.1.1"></minus><cn type="integer" id="S2.T2.31.31.1.m1.1.1.2.cmml" xref="S2.T2.31.31.1.m1.1.1.2">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.31.31.1.m1.1c">-160</annotation></semantics></math>-<math id="S2.T2.32.32.2.m2.1" class="ltx_Math" alttext="160" display="inline"><semantics id="S2.T2.32.32.2.m2.1a"><mn id="S2.T2.32.32.2.m2.1.1" xref="S2.T2.32.32.2.m2.1.1.cmml">160</mn><annotation-xml encoding="MathML-Content" id="S2.T2.32.32.2.m2.1b"><cn type="integer" id="S2.T2.32.32.2.m2.1.1.cmml" xref="S2.T2.32.32.2.m2.1.1">160</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.32.32.2.m2.1c">160</annotation></semantics></math>m</td>
<td id="S2.T2.34.34.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S2.T2.33.33.3.m1.1" class="ltx_Math" alttext="-160" display="inline"><semantics id="S2.T2.33.33.3.m1.1a"><mrow id="S2.T2.33.33.3.m1.1.1" xref="S2.T2.33.33.3.m1.1.1.cmml"><mo id="S2.T2.33.33.3.m1.1.1a" xref="S2.T2.33.33.3.m1.1.1.cmml">−</mo><mn id="S2.T2.33.33.3.m1.1.1.2" xref="S2.T2.33.33.3.m1.1.1.2.cmml">160</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T2.33.33.3.m1.1b"><apply id="S2.T2.33.33.3.m1.1.1.cmml" xref="S2.T2.33.33.3.m1.1.1"><minus id="S2.T2.33.33.3.m1.1.1.1.cmml" xref="S2.T2.33.33.3.m1.1.1"></minus><cn type="integer" id="S2.T2.33.33.3.m1.1.1.2.cmml" xref="S2.T2.33.33.3.m1.1.1.2">160</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.33.33.3.m1.1c">-160</annotation></semantics></math>-<math id="S2.T2.34.34.4.m2.1" class="ltx_Math" alttext="160" display="inline"><semantics id="S2.T2.34.34.4.m2.1a"><mn id="S2.T2.34.34.4.m2.1.1" xref="S2.T2.34.34.4.m2.1.1.cmml">160</mn><annotation-xml encoding="MathML-Content" id="S2.T2.34.34.4.m2.1b"><cn type="integer" id="S2.T2.34.34.4.m2.1.1.cmml" xref="S2.T2.34.34.4.m2.1.1">160</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.34.34.4.m2.1c">160</annotation></semantics></math>m</td>
<td id="S2.T2.36.36.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<math id="S2.T2.35.35.5.m1.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.T2.35.35.5.m1.1a"><mn id="S2.T2.35.35.5.m1.1.1" xref="S2.T2.35.35.5.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.T2.35.35.5.m1.1b"><cn type="integer" id="S2.T2.35.35.5.m1.1.1.cmml" xref="S2.T2.35.35.5.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.35.35.5.m1.1c">50</annotation></semantics></math>-<math id="S2.T2.36.36.6.m2.1" class="ltx_Math" alttext="150" display="inline"><semantics id="S2.T2.36.36.6.m2.1a"><mn id="S2.T2.36.36.6.m2.1.1" xref="S2.T2.36.36.6.m2.1.1.cmml">150</mn><annotation-xml encoding="MathML-Content" id="S2.T2.36.36.6.m2.1b"><cn type="integer" id="S2.T2.36.36.6.m2.1.1.cmml" xref="S2.T2.36.36.6.m2.1.1">150</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.36.36.6.m2.1c">150</annotation></semantics></math>m</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Models and Training setup</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.3" class="ltx_p">As object detection models, we take two one-stage real-time detectors, EfficientDet-<math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.cmml">​</mo><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">D0</annotation></semantics></math> (E.-<math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mrow id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.2.m2.1.1.1" xref="S2.SS3.p1.2.m2.1.1.1.cmml">​</mo><mn id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><times id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1.1"></times><ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">𝐷</ci><cn type="integer" id="S2.SS3.p1.2.m2.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">D0</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>, both being on the forefront of real-time object detectors as measured by their performances on the COCO test set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>. In particular, EfficientDet-<math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mrow id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml"><mi id="S2.SS3.p1.3.m3.1.1.2" xref="S2.SS3.p1.3.m3.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.3.m3.1.1.1" xref="S2.SS3.p1.3.m3.1.1.1.cmml">​</mo><mn id="S2.SS3.p1.3.m3.1.1.3" xref="S2.SS3.p1.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><apply id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1"><times id="S2.SS3.p1.3.m3.1.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1.1"></times><ci id="S2.SS3.p1.3.m3.1.1.2.cmml" xref="S2.SS3.p1.3.m3.1.1.2">𝐷</ci><cn type="integer" id="S2.SS3.p1.3.m3.1.1.3.cmml" xref="S2.SS3.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">D0</annotation></semantics></math> is the state-of-the-art model for real-time detectors on large-scale UAVDT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> traffic surveillance data set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For that, we use the implementation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> with an image size of 2176px width and anchor scales of (0.3 0.5 0.7).</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Yolov5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> is a state of the art implementation of the Yolo object detection model implemented with multiple improvements to the Yolo framework that have been found in recent years. In this work, we used the unmodified YOLOv5m6 implementation of Yolov5 in release v5.0 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> with an image size of 1280x1280px and a batchsize of 48. Unless otherwise specified, we used the provided weights pre-trained on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Furthermore, as a two-stage detector we take the best performing single-model (no ensemble) on VisDrone from the workshop report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (DE-FPN), i.e. a Faster R-CNN (F.R.) with a ResNeXt-101 64-4d <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> backbone (removing P6), which is trained using color jitter and random image cropping. The anchor sizes and strides are decreased to (16, 32, 64, 128, 256) and (4, 8, 16, 32, 64).</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">We measure the models performances on the popular mean average precision metric with overlap 0.5, i.e. mAP@0.5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>. As we are interested in real-world performance, we test on the test set of the real-world data set unless indicated otherwise.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experimental Evaluation</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">First, we conducted different experiments to show that synthetic training data could yield good real-world performance or improve the performance of a real-world object detector. Then we conducted further ablation studies to examine different factors that could influence or modulate the positive effect of synthetic training data.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">On a general level, we wanted to obtain actionable advice for an engineer using synthetic data to train an object detector, which factors should be examined with emphasis and which factors could be ignored. Such factors could be the graphics quality of the simulation environment or the alignment of synthetic and real height distributions.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In the following, those experimental conditions and their results will be described. For better clarity, we discuss the design of each ablation study and its result individually.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Performance of object detectors for different training strategies given by mAP@50. </figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<th id="S3.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Data set</th>
<th id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Synthetic</th>
<th id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Real</th>
<th id="S3.T3.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">SyntheticToReal</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" rowspan="3">
<span id="S3.T3.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S3.T3.1.1.1.1.1" class="ltx_p">
<span id="S3.T3.1.1.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:26.5pt;vertical-align:-9.8pt;"><span class="ltx_transformed_inner" style="width:26.5pt;transform:translate(-9.82pt,0pt) rotate(-90deg) ;">
<span id="S3.T3.1.1.1.1.1.1.1" class="ltx_p">E.-<math id="S3.T3.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S3.T3.1.1.1.1.1.1.1.m1.1a"><mrow id="S3.T3.1.1.1.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.T3.1.1.1.1.1.1.1.m1.1.1.1" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.1.cmml">​</mo><mn id="S3.T3.1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1"><times id="S3.T3.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.1"></times><ci id="S3.T3.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.T3.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.1.1.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.1.1.1.m1.1c">D0</annotation></semantics></math></span>
</span></span></span>
</span>
</th>
<td id="S3.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Cattle</td>
<td id="S3.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">29.2</td>
<td id="S3.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">78.4</td>
<td id="S3.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.1.1.5.1" class="ltx_text ltx_font_bold">85.8</span></td>
</tr>
<tr id="S3.T3.1.3.1" class="ltx_tr">
<td id="S3.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_r">SeaDronesSee</td>
<td id="S3.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r">10.3</td>
<td id="S3.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r">36.3</td>
<td id="S3.T3.1.3.1.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.3.1.4.1" class="ltx_text ltx_font_bold">38.8</span></td>
</tr>
<tr id="S3.T3.1.4.2" class="ltx_tr">
<td id="S3.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_r">VisDrone</td>
<td id="S3.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">1.2</td>
<td id="S3.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">24.6</td>
<td id="S3.T3.1.4.2.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.4.2.4.1" class="ltx_text ltx_font_bold">27.2</span></td>
</tr>
<tr id="S3.T3.1.5.3" class="ltx_tr">
<th id="S3.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3">
<span id="S3.T3.1.5.3.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S3.T3.1.5.3.1.1.1" class="ltx_p">
<span id="S3.T3.1.5.3.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:19.4pt;vertical-align:-6.3pt;"><span class="ltx_transformed_inner" style="width:19.4pt;transform:translate(-6.31pt,0pt) rotate(-90deg) ;">
<span id="S3.T3.1.5.3.1.1.1.1.1" class="ltx_p">F.R.</span>
</span></span></span>
</span>
</th>
<td id="S3.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cattle</td>
<td id="S3.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">38.8</td>
<td id="S3.T3.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90.5</td>
<td id="S3.T3.1.5.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T3.1.5.3.5.1" class="ltx_text ltx_font_bold">91.5</span></td>
</tr>
<tr id="S3.T3.1.6.4" class="ltx_tr">
<td id="S3.T3.1.6.4.1" class="ltx_td ltx_align_center ltx_border_r">SeaDronesSee</td>
<td id="S3.T3.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">14.6</td>
<td id="S3.T3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">54.7</td>
<td id="S3.T3.1.6.4.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.6.4.4.1" class="ltx_text ltx_font_bold">59.0</span></td>
</tr>
<tr id="S3.T3.1.7.5" class="ltx_tr">
<td id="S3.T3.1.7.5.1" class="ltx_td ltx_align_center ltx_border_r">VisDrone</td>
<td id="S3.T3.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r">2.4</td>
<td id="S3.T3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r">48.6</td>
<td id="S3.T3.1.7.5.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.7.5.4.1" class="ltx_text ltx_font_bold">51.2</span></td>
</tr>
<tr id="S3.T3.1.8.6" class="ltx_tr">
<th id="S3.T3.1.8.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" rowspan="3">
<span id="S3.T3.1.8.6.1.1" class="ltx_inline-block ltx_parbox ltx_align_top" style="width:5.7pt;">
<span id="S3.T3.1.8.6.1.1.1" class="ltx_p">
<span id="S3.T3.1.8.6.1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.8pt;height:29.3pt;vertical-align:-11.2pt;"><span class="ltx_transformed_inner" style="width:29.3pt;transform:translate(-11.24pt,0pt) rotate(-90deg) ;">
<span id="S3.T3.1.8.6.1.1.1.1.1" class="ltx_p">YOLO</span>
</span></span></span>
</span>
</th>
<td id="S3.T3.1.8.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cattle</td>
<td id="S3.T3.1.8.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">64.2</td>
<td id="S3.T3.1.8.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T3.1.8.6.4.1" class="ltx_text ltx_font_bold">88.8</span></td>
<td id="S3.T3.1.8.6.5" class="ltx_td ltx_align_center ltx_border_t">86.9</td>
</tr>
<tr id="S3.T3.1.9.7" class="ltx_tr">
<td id="S3.T3.1.9.7.1" class="ltx_td ltx_align_center ltx_border_r">SeaDronesSee</td>
<td id="S3.T3.1.9.7.2" class="ltx_td ltx_align_center ltx_border_r">10.5</td>
<td id="S3.T3.1.9.7.3" class="ltx_td ltx_align_center ltx_border_r">55.8</td>
<td id="S3.T3.1.9.7.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.9.7.4.1" class="ltx_text ltx_font_bold">60.3</span></td>
</tr>
<tr id="S3.T3.1.10.8" class="ltx_tr">
<td id="S3.T3.1.10.8.1" class="ltx_td ltx_align_center ltx_border_r">VisDrone</td>
<td id="S3.T3.1.10.8.2" class="ltx_td ltx_align_center ltx_border_r">10.2</td>
<td id="S3.T3.1.10.8.3" class="ltx_td ltx_align_center ltx_border_r">43.9</td>
<td id="S3.T3.1.10.8.4" class="ltx_td ltx_align_center"><span id="S3.T3.1.10.8.4.1" class="ltx_text ltx_font_bold">45.0</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">General Benefit of Synthetic Data in UAV Object Detection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The main goal of this work is to examine the usefulness of synthetic training data to train object detectors from scratch or improve the performance of object detectors trained with real-world data.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">From this goal, there naturally arise three conditions which we want to compare. First, as a baseline, we observe the performance of the object detector on the real-world data set. Second, we observe the performance of the object detector trained only on an entirely synthetically generated data set. Finally, we observe the performance of an object detector which is first pre-trained on a synthetic data set and then transfer-trained on the real-world data set. From preliminary experiments, we found these strategies to be superior to alternative strategies, such as combined training, similar to previous literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For all three application scenarios, the corresponding real-world training set was split into a training, validation and test set. For fully synthetic training and synthetic pre-training the data set was split into a training and a validation set, as no testing is conducted on the synthetic data. See the sizes of the different complete sets in Table <a href="#S3.T4" title="TABLE IV ‣ III-A General Benefit of Synthetic Data in UAV Object Detection ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span> Number of images and classes. Note that (awning-)tricycle is abbreviated as (a.-)tri., life jacket as LF and people as ppl.</figcaption>
<table id="S3.T4.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T4.4.5.1" class="ltx_tr">
<td id="S3.T4.4.5.1.1" class="ltx_td ltx_border_r"></td>
<td id="S3.T4.4.5.1.2" class="ltx_td ltx_align_center" colspan="3">Number of Images</td>
<td id="S3.T4.4.5.1.3" class="ltx_td"></td>
</tr>
<tr id="S3.T4.4.6.2" class="ltx_tr">
<td id="S3.T4.4.6.2.1" class="ltx_td ltx_align_center ltx_border_r">Data Set</td>
<td id="S3.T4.4.6.2.2" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T4.4.6.2.3" class="ltx_td ltx_align_center ltx_border_r">Val</td>
<td id="S3.T4.4.6.2.4" class="ltx_td ltx_align_center ltx_border_r">Test</td>
<td id="S3.T4.4.6.2.5" class="ltx_td ltx_align_center">Classes</td>
</tr>
<tr id="S3.T4.4.7.3" class="ltx_tr">
<td id="S3.T4.4.7.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">VisDrone</td>
<td id="S3.T4.4.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">6471</td>
<td id="S3.T4.4.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">548</td>
<td id="S3.T4.4.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1610</td>
<td id="S3.T4.4.7.3.5" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S3.T4.4.7.3.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.7.3.5.1.1" class="ltx_tr">
<td id="S3.T4.4.7.3.5.1.1.1" class="ltx_td ltx_align_center">ppl.,bike,car,truck,van</td>
</tr>
<tr id="S3.T4.4.7.3.5.1.2" class="ltx_tr">
<td id="S3.T4.4.7.3.5.1.2.1" class="ltx_td ltx_align_center">motor,tri.,a.-tri.,bus</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.2.2" class="ltx_tr">
<td id="S3.T4.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T4.2.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.2.2.3.1.1" class="ltx_tr">
<td id="S3.T4.2.2.3.1.1.1" class="ltx_td ltx_align_center">Sea-</td>
</tr>
<tr id="S3.T4.2.2.3.1.2" class="ltx_tr">
<td id="S3.T4.2.2.3.1.2.1" class="ltx_td ltx_align_center">DronesSee</td>
</tr>
</table>
</td>
<td id="S3.T4.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2975</td>
<td id="S3.T4.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">859</td>
<td id="S3.T4.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1796</td>
<td id="S3.T4.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T4.2.2.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.2.2.2.2.3" class="ltx_tr">
<td id="S3.T4.2.2.2.2.3.1" class="ltx_td ltx_align_center">swimmer,floater,boat</td>
</tr>
<tr id="S3.T4.2.2.2.2.2" class="ltx_tr">
<td id="S3.T4.2.2.2.2.2.2" class="ltx_td ltx_align_center">swimmer<sup id="S3.T4.2.2.2.2.2.2.1" class="ltx_sup">†</sup>,floater<sup id="S3.T4.2.2.2.2.2.2.2" class="ltx_sup">†</sup>,LJ</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.4.8.4" class="ltx_tr">
<td id="S3.T4.4.8.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Cattle</td>
<td id="S3.T4.4.8.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">402</td>
<td id="S3.T4.4.8.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">134</td>
<td id="S3.T4.4.8.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">134</td>
<td id="S3.T4.4.8.4.5" class="ltx_td ltx_align_center ltx_border_t">Cow</td>
</tr>
<tr id="S3.T4.4.9.5" class="ltx_tr">
<td id="S3.T4.4.9.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">
<table id="S3.T4.4.9.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.9.5.1.1.1" class="ltx_tr">
<td id="S3.T4.4.9.5.1.1.1.1" class="ltx_td ltx_align_center">DGTA-</td>
</tr>
<tr id="S3.T4.4.9.5.1.1.2" class="ltx_tr">
<td id="S3.T4.4.9.5.1.1.2.1" class="ltx_td ltx_align_center">VisDrone</td>
</tr>
</table>
</td>
<td id="S3.T4.4.9.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">40000</td>
<td id="S3.T4.4.9.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">10000</td>
<td id="S3.T4.4.9.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">–</td>
<td id="S3.T4.4.9.5.5" class="ltx_td ltx_align_center ltx_border_tt">
<table id="S3.T4.4.9.5.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.9.5.5.1.1" class="ltx_tr">
<td id="S3.T4.4.9.5.5.1.1.1" class="ltx_td ltx_align_center">ppl.,bike,truck</td>
</tr>
<tr id="S3.T4.4.9.5.5.1.2" class="ltx_tr">
<td id="S3.T4.4.9.5.5.1.2.1" class="ltx_td ltx_align_center">car,motor,bus,van</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.4.4" class="ltx_tr">
<td id="S3.T4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S3.T4.4.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.4.3.1.1" class="ltx_tr">
<td id="S3.T4.4.4.3.1.1.1" class="ltx_td ltx_align_center">DGTA-Sea-</td>
</tr>
<tr id="S3.T4.4.4.3.1.2" class="ltx_tr">
<td id="S3.T4.4.4.3.1.2.1" class="ltx_td ltx_align_center">DronesSee</td>
</tr>
</table>
</td>
<td id="S3.T4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">90000</td>
<td id="S3.T4.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10000</td>
<td id="S3.T4.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">–</td>
<td id="S3.T4.4.4.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T4.4.4.2.2" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.4.2.2.3" class="ltx_tr">
<td id="S3.T4.4.4.2.2.3.1" class="ltx_td ltx_align_center">swimmer,floater,boat</td>
</tr>
<tr id="S3.T4.4.4.2.2.2" class="ltx_tr">
<td id="S3.T4.4.4.2.2.2.2" class="ltx_td ltx_align_center">swimmer<sup id="S3.T4.4.4.2.2.2.2.1" class="ltx_sup">†</sup>,floater<sup id="S3.T4.4.4.2.2.2.2.2" class="ltx_sup">†</sup>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S3.T4.4.10.6" class="ltx_tr">
<td id="S3.T4.4.10.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<table id="S3.T4.4.10.6.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T4.4.10.6.1.1.1" class="ltx_tr">
<td id="S3.T4.4.10.6.1.1.1.1" class="ltx_td ltx_align_center">DGTA-</td>
</tr>
<tr id="S3.T4.4.10.6.1.1.2" class="ltx_tr">
<td id="S3.T4.4.10.6.1.1.2.1" class="ltx_td ltx_align_center">Cattle</td>
</tr>
</table>
</td>
<td id="S3.T4.4.10.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">40000</td>
<td id="S3.T4.4.10.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10000</td>
<td id="S3.T4.4.10.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">–</td>
<td id="S3.T4.4.10.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">Cow</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS1.SSSx1.p1" class="ltx_para">
<p id="S3.SS1.SSSx1.p1.1" class="ltx_p">Table <a href="#S3.T3" title="TABLE III ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> shows that purely training on synthetic data can already provide minimal working solutions. While all object detectors perform well on the simple data set Cattle (29.2-64.2 mAP@50), the accuracies on VisDrone and SeaDronesSee are far lower. This is partly due to missing classes in the corresponding synthetic data sets ((awning-)tricycle in VisDrone and life jacket in SeaDronesSee).
However, another apparent factor is the difference in the appearance of certain classes from the synthetic to the real data set. For example, see Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to compare the same classes in the synthetic and real data set. The default appearances of classes vary in some cases significantly. This appearance gap may be narrowed by manually editing the appearance of classes to resemble real-world objects.
Despite these challenges, synthetic pre-training with subsequent transfer training boosts performance on VisDrone and SeaDronesSee significantly across all models. On the SeaDronesSee evaluation benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, we can even achieve state-of-the-art performance by surpassing the best model by +5.6 mAP@50. While the performance improvement is not as apparent on Cattle, pre-training on synthetic data helps for EfficientDet-<math id="S3.SS1.SSSx1.p1.1.m1.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S3.SS1.SSSx1.p1.1.m1.1a"><mrow id="S3.SS1.SSSx1.p1.1.m1.1.1" xref="S3.SS1.SSSx1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSSx1.p1.1.m1.1.1.2" xref="S3.SS1.SSSx1.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS1.SSSx1.p1.1.m1.1.1.1" xref="S3.SS1.SSSx1.p1.1.m1.1.1.1.cmml">​</mo><mn id="S3.SS1.SSSx1.p1.1.m1.1.1.3" xref="S3.SS1.SSSx1.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSSx1.p1.1.m1.1b"><apply id="S3.SS1.SSSx1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSSx1.p1.1.m1.1.1"><times id="S3.SS1.SSSx1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSSx1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.SSSx1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSSx1.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS1.SSSx1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSSx1.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSSx1.p1.1.m1.1c">D0</annotation></semantics></math> and Faster R-CNN.</p>
</div>
<div id="S3.SS1.SSSx1.p2" class="ltx_para">
<p id="S3.SS1.SSSx1.p2.1" class="ltx_p">These experiments show that although synthetic data sets can not replace corresponding real-world data sets, they enhance the detection performance. Even the models from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> can be beaten just by synthetic pre-training.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Effect of Data Set Sizes</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We wanted to test the influence that the data set sizes had on the performance in this context. The intuitive hypothesis was that we would observe a curve of diminishing returns for larger data sets, which is typical in machine learning. We hypothesized that we would observe such diminishing returns for the size of the real-world data set, as well as for the size of the synthetic (pre-training) data set. Furthermore, we hypothesized that the effect of synthetic pre-training would be more emphasized when using a smaller real-world data set.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">To conduct this ablation, we varied the size of the real-world data set and of the synthetic data set for VisDrone and SeaDronesSee training.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2112.12252/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="322" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The effect of the real and synthetic data set size on the model performance for VisDrone and SeaDronesSee. The color shading specifies the size of the synthetic data set that was used for pre-training.</figcaption>
</figure>
<section id="S3.SS2.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS2.SSSx1.p1" class="ltx_para">
<p id="S3.SS2.SSSx1.p1.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‣ III-B Effect of Data Set Sizes ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the effect of different data set sizes in synthetic pre-training. As hypothesized, we observe an improved mAP@50 with an increase of the real-world data set size, and an increase of the synthetic pre-training data set size with a diminishing return for larger data sets. Furthermore, we observe the hypothesized interactive effect, such that the improvement from using synthetic pre-training data is disproportionally larger for smaller real-world data sets.</p>
</div>
<div id="S3.SS2.SSSx1.p2" class="ltx_para">
<p id="S3.SS2.SSSx1.p2.1" class="ltx_p">Our found effects are consistent with the numbers of those reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, that is, improvements of about +1.0 to +5.0 mAP@50 by using additional synthetic training data. We note that in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, a 3D LiDAR object detection task was examined instead of a 2D object detection task.
Compared to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, in our work, those improvements are not only on one class but over the whole 10 and 6 classes of VisDrone and SeaDronesSee, respectively.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Effect of using pre-trained weights</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The use of initial weights pre-trained on large scale image data sets like ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> or COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> has become a standard for many vision machine learning tasks in recent years. We hypothesized that there could be an interaction between using such pre-trained weights and synthetic <span id="S3.SS3.p1.1.1" class="ltx_text">(pre-)training</span>. For example, such pre-trained weights could encode invariance to image noise, which is prominent in real world images, but may be missing in synthetically generated images. The use of pre-trained initial weights would thereby allow a purely synthetically trained model to obtain the information it could not obtain from the synthetic data, thereby introducing an interactive effect. If this hypothesis were true, using pre-trained weights would improve the performance of a purely synthetically trained model disproportionally more than it improves the performance of a model trained on real world data.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Effect of COCO pre-trained weights on performance given by mAP@50 evaluated on VisDrone.</figcaption>
<table id="S3.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.1.1.1" class="ltx_tr">
<th id="S3.T5.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T5.1.1.1.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">random initial weights</th>
<th id="S3.T5.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">Pre-trained on COCO</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.1.2.1" class="ltx_tr">
<th id="S3.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Real</th>
<td id="S3.T5.1.2.1.2" class="ltx_td ltx_align_right ltx_border_t">43.1</td>
<td id="S3.T5.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">43.9</td>
</tr>
<tr id="S3.T5.1.3.2" class="ltx_tr">
<th id="S3.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Synthetic</th>
<td id="S3.T5.1.3.2.2" class="ltx_td ltx_align_right">7.5</td>
<td id="S3.T5.1.3.2.3" class="ltx_td ltx_align_right">10.2</td>
</tr>
<tr id="S3.T5.1.4.3" class="ltx_tr">
<th id="S3.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">SyntheticToReal</th>
<td id="S3.T5.1.4.3.2" class="ltx_td ltx_align_right ltx_border_bb">44.7</td>
<td id="S3.T5.1.4.3.3" class="ltx_td ltx_align_right ltx_border_bb">45.0</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS3.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS3.SSSx1.p1" class="ltx_para">
<p id="S3.SS3.SSSx1.p1.1" class="ltx_p">Table <a href="#S3.T5" title="TABLE V ‣ III-C Effect of using pre-trained weights ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the effect of using COCO pre-trained weights as initial weights for the training.
Due to the small effect sizes, we would argue that our results are inconclusive at this time. We observe a more significant absolute improvement of the mAP@50 when using COCO pre-trained weights on pure DGTA-VisDrone training than on pure VisDrone training, which would speak for the hypothesized interactive effect. However, when observing the absolute obtained mAP@50 of those different conditions, this observed difference in improvements would also be consistent with an effect of diminishing returns of improvements for a larger mAP@50. This effect would be that starting from a weaker performance baseline, the same improvement (using COCO pre-trained weights) would yield a larger improvement than when starting from a more robust baseline.</p>
</div>
<div id="S3.SS3.SSSx1.p2" class="ltx_para">
<p id="S3.SS3.SSSx1.p2.1" class="ltx_p">So we conclude that there is at least no strong effect where using pre-trained weights trained on real-world imagery would disproportionally improve the performance of synthetic training. However, note that there might be other factors to consider, such as training time and stability.</p>
</div>
</section>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Effect of Good/Bad graphics settings</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">One of the parameters that one would intuitively look at when observing synthetic training data or when trying to improve the use of synthetic training data is the realism of this training data. This could be framed as closing the Sim-To-Real gap as discussed above.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">A similar but not fully congruent perspective on this coming from game development is trying to improve the realism of games by improving their graphics quality. In many cases, those improvements of graphics quality come with high computational demands (higher polygon models, higher resolution textures, demanding physics simulations like particle effects and lighting) and high amounts of labor, e.g. from graphics artists.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Therefore, from an engineering perspective, this raises the question, how well spent this effort is to obtain the final goal of increasing the synthetically trained model’s performance.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">To answer this question, we conducted experiments comparing the effects of synthetic training data obtained from GTAV either set to the highest or lowest possible graphics settings.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Performance between low quality and high quality in-game settings (given by mAP@50). We compare the performance for only Real training on VisDrone against the performance with synthetic data as DGTA-VisDrone.</figcaption>
<table id="S3.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T6.1.1.1" class="ltx_tr">
<td id="S3.T6.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<th id="S3.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Real</th>
<th id="S3.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Synthetic</th>
<th id="S3.T6.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">SyntheticToReal</th>
</tr>
<tr id="S3.T6.1.2.2" class="ltx_tr">
<td id="S3.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">Real data Baseline</td>
<td id="S3.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">43.9</td>
<td id="S3.T6.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="S3.T6.1.2.2.4" class="ltx_td ltx_align_left ltx_border_t">-</td>
</tr>
<tr id="S3.T6.1.3.3" class="ltx_tr">
<td id="S3.T6.1.3.3.1" class="ltx_td ltx_align_left">Low Quality</td>
<td id="S3.T6.1.3.3.2" class="ltx_td ltx_align_left">-</td>
<td id="S3.T6.1.3.3.3" class="ltx_td ltx_align_left">8.4</td>
<td id="S3.T6.1.3.3.4" class="ltx_td ltx_align_left">45.1</td>
</tr>
<tr id="S3.T6.1.4.4" class="ltx_tr">
<td id="S3.T6.1.4.4.1" class="ltx_td ltx_align_left ltx_border_bb">High Quality</td>
<td id="S3.T6.1.4.4.2" class="ltx_td ltx_align_left ltx_border_bb">-</td>
<td id="S3.T6.1.4.4.3" class="ltx_td ltx_align_left ltx_border_bb">10.2</td>
<td id="S3.T6.1.4.4.4" class="ltx_td ltx_align_left ltx_border_bb">45.0</td>
</tr>
</tbody>
</table>
</figure>
<section id="S3.SS4.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS4.SSSx1.p1" class="ltx_para">
<p id="S3.SS4.SSSx1.p1.1" class="ltx_p">Table <a href="#S3.T6" title="TABLE VI ‣ III-D Effect of Good/Bad graphics settings ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> shows the obtained model performance using synthetic data obtained from GTAV on the lowest and highest graphics settings, respectively. We observe that for purely synthetic training, the model trained with higher graphics setting images outperforms the one trained on lower graphics quality images. This effect, however, is small.</p>
</div>
<div id="S3.SS4.SSSx1.p2" class="ltx_para">
<p id="S3.SS4.SSSx1.p2.1" class="ltx_p">There is no observable effect in the synthetic pre-training condition with transfer training on the real-world data set.</p>
</div>
</section>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.5.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.6.2" class="ltx_text ltx_font_italic">Aligning Domain Distributions</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">Modifying the DeepGTAV tools to extract metadata allows us to automate parts of the data generation process by aligning the metadata distributions of the real and synthetic data sets. Instead of laboriously setting the correct metadata settings in the synthetic data generation process, one could fall back to the corresponding real data set to adapt the parameters automatically. Aligning the distributions may result in higher synthetic data quality or efficiency.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para">
<p id="S3.SS5.p2.1" class="ltx_p">For instance, in SeaDronesSee, every image is annotated with the capture time stamp. By bootstrap sampling from the time distribution, we sample a new data set of 100k synthetic images with the correct time distribution. As before, we train a Yolov5
model and test it on the SeaDronesSee test set (Synthetic Only), and we transfer train it on SeaDronesSee (Synthetic-To-Real).</p>
</div>
<section id="S3.SS5.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS5.SSSx1.p1" class="ltx_para">
<p id="S3.SS5.SSSx1.p1.1" class="ltx_p">Table <a href="#S3.T7" title="TABLE VII ‣ Findings ‣ III-E Aligning Domain Distributions ‣ III Experimental Evaluation ‣ Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> shows that aligning the time helps in synthetic only training and synthetic pre-training by increasing the performance over the unaligned baseline by +4.1 and +0.2 mAP@50, respectively. The performance increase is mainly due to SeaDronesSee only featuring day-time images, such that sampling synthetic night images deteriorates the performance.</p>
</div>
<div id="S3.SS5.SSSx1.p2" class="ltx_para">
<p id="S3.SS5.SSSx1.p2.1" class="ltx_p">Similarly, we can also automatically adjust the camera angle. The images in the Cattle data set have been taken from a downward-facing camera where a gimbal corrects for UAV angular movement. We sample from DGTA-Cattle only these images that fall into the range of these angles (with an error threshold of at most 20 degrees). This reduces the original 40k images to only roughly 10% of the images (3,954). We train an EfficientDet-<math id="S3.SS5.SSSx1.p2.1.m1.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S3.SS5.SSSx1.p2.1.m1.1a"><mrow id="S3.SS5.SSSx1.p2.1.m1.1.1" xref="S3.SS5.SSSx1.p2.1.m1.1.1.cmml"><mi id="S3.SS5.SSSx1.p2.1.m1.1.1.2" xref="S3.SS5.SSSx1.p2.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSSx1.p2.1.m1.1.1.1" xref="S3.SS5.SSSx1.p2.1.m1.1.1.1.cmml">​</mo><mn id="S3.SS5.SSSx1.p2.1.m1.1.1.3" xref="S3.SS5.SSSx1.p2.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSSx1.p2.1.m1.1b"><apply id="S3.SS5.SSSx1.p2.1.m1.1.1.cmml" xref="S3.SS5.SSSx1.p2.1.m1.1.1"><times id="S3.SS5.SSSx1.p2.1.m1.1.1.1.cmml" xref="S3.SS5.SSSx1.p2.1.m1.1.1.1"></times><ci id="S3.SS5.SSSx1.p2.1.m1.1.1.2.cmml" xref="S3.SS5.SSSx1.p2.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS5.SSSx1.p2.1.m1.1.1.3.cmml" xref="S3.SS5.SSSx1.p2.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSSx1.p2.1.m1.1c">D0</annotation></semantics></math> on this subset.</p>
</div>
</section>
<section id="S3.SS5.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Findings</h4>

<div id="S3.SS5.SSSx2.p1" class="ltx_para">
<p id="S3.SS5.SSSx2.p1.1" class="ltx_p">Interestingly, the performance of the synthetic only training improves over the more extensive unaligned DGTA-Cattle training. This is likely due to the limited capacity of an EfficientDet-<math id="S3.SS5.SSSx2.p1.1.m1.1" class="ltx_Math" alttext="D0" display="inline"><semantics id="S3.SS5.SSSx2.p1.1.m1.1a"><mrow id="S3.SS5.SSSx2.p1.1.m1.1.1" xref="S3.SS5.SSSx2.p1.1.m1.1.1.cmml"><mi id="S3.SS5.SSSx2.p1.1.m1.1.1.2" xref="S3.SS5.SSSx2.p1.1.m1.1.1.2.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.SS5.SSSx2.p1.1.m1.1.1.1" xref="S3.SS5.SSSx2.p1.1.m1.1.1.1.cmml">​</mo><mn id="S3.SS5.SSSx2.p1.1.m1.1.1.3" xref="S3.SS5.SSSx2.p1.1.m1.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.SSSx2.p1.1.m1.1b"><apply id="S3.SS5.SSSx2.p1.1.m1.1.1.cmml" xref="S3.SS5.SSSx2.p1.1.m1.1.1"><times id="S3.SS5.SSSx2.p1.1.m1.1.1.1.cmml" xref="S3.SS5.SSSx2.p1.1.m1.1.1.1"></times><ci id="S3.SS5.SSSx2.p1.1.m1.1.1.2.cmml" xref="S3.SS5.SSSx2.p1.1.m1.1.1.2">𝐷</ci><cn type="integer" id="S3.SS5.SSSx2.p1.1.m1.1.1.3.cmml" xref="S3.SS5.SSSx2.p1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.SSSx2.p1.1.m1.1c">D0</annotation></semantics></math> model resulting in the model distributing its performance across many other angular viewpoints, which are unnecessary for the performance in this use-case. This experiment illustrates that less but more targeted data may be sufficient to reach the same performance while reducing the need to filter the data by only aligning the metadata distributions manually.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>Effect of time and angle alignment on performance (given as mAP@50). </figcaption>
<table id="S3.T7.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T7.1.1.1" class="ltx_tr">
<th id="S3.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">SeaDronesSee</th>
<td id="S3.T7.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt">Synthetic</td>
<td id="S3.T7.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">SyntheticToReal</td>
</tr>
<tr id="S3.T7.1.2.2" class="ltx_tr">
<th id="S3.T7.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Only real training</th>
<td id="S3.T7.1.2.2.2" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S3.T7.1.2.2.3" class="ltx_td ltx_align_right ltx_border_t">55.8</td>
</tr>
<tr id="S3.T7.1.3.3" class="ltx_tr">
<th id="S3.T7.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Unaligned Synthetic Pre-training</th>
<td id="S3.T7.1.3.3.2" class="ltx_td ltx_align_right">10.5</td>
<td id="S3.T7.1.3.3.3" class="ltx_td ltx_align_right">60.3</td>
</tr>
<tr id="S3.T7.1.4.4" class="ltx_tr">
<th id="S3.T7.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Time Aligned</th>
<td id="S3.T7.1.4.4.2" class="ltx_td ltx_align_right"><span id="S3.T7.1.4.4.2.1" class="ltx_text ltx_font_bold">14.6</span></td>
<td id="S3.T7.1.4.4.3" class="ltx_td ltx_align_right"><span id="S3.T7.1.4.4.3.1" class="ltx_text ltx_font_bold">60.5</span></td>
</tr>
<tr id="S3.T7.1.5.5" class="ltx_tr">
<th id="S3.T7.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt ltx_border_tt">Cattle</th>
<td id="S3.T7.1.5.5.2" class="ltx_td ltx_border_tt ltx_border_tt"></td>
<td id="S3.T7.1.5.5.3" class="ltx_td ltx_border_tt ltx_border_tt"></td>
</tr>
<tr id="S3.T7.1.6.6" class="ltx_tr">
<th id="S3.T7.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Only real training</th>
<td id="S3.T7.1.6.6.2" class="ltx_td ltx_align_right ltx_border_t">-</td>
<td id="S3.T7.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t">78.4</td>
</tr>
<tr id="S3.T7.1.7.7" class="ltx_tr">
<th id="S3.T7.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Unaligned Synthetic Pre-training</th>
<td id="S3.T7.1.7.7.2" class="ltx_td ltx_align_right">29.2</td>
<td id="S3.T7.1.7.7.3" class="ltx_td ltx_align_right"><span id="S3.T7.1.7.7.3.1" class="ltx_text ltx_font_bold">85.8</span></td>
</tr>
<tr id="S3.T7.1.8.8" class="ltx_tr">
<th id="S3.T7.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Angle Aligned (Subset)</th>
<td id="S3.T7.1.8.8.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T7.1.8.8.2.1" class="ltx_text ltx_font_bold">36.6</span></td>
<td id="S3.T7.1.8.8.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="S3.T7.1.8.8.3.1" class="ltx_text ltx_font_bold">85.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Limitations and Conclusions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We demonstrated that synthetic data can be leveraged for object detection on UAVs. We can improve the performance over only real training by synthetic pre-training on multiple application scenarios. Synthetic-only training yields satisfactory results, but performances are not yet competitive to real training.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">From our ablations, we conclude that the use of more synthetic training data improves the performance. The use of weights pre-trained on large scale image data sets constantly improves the performance, although we find no interactive effect with synthetic training. The graphics quality of the simulation engine appears to be important for purely synthetic training but not for synthetic pre-training. In general, metadata alignment is vital for the usefulness and data efficiency of synthetic training data.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We hope that the adaptation of the DeepGTAV tools helps cast light on object detection on UAVs via synthetically generated footage.
In future works, the capabilities of the DeepGTAV framework to produce object segmentation data, LiDAR data and video could be leveraged.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Adão, J. Hruška, L. Pádua, J. Bessa, E. Peres, R. Morais, and
J. J. Sousa, “Hyperspectral imaging: A review on uav-based sensors, data
processing and applications for agriculture and forestry,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Remote
Sensing</em>, vol. 9, no. 11, p. 1110, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
E. Lygouras, N. Santavas, A. Taitzoglou, K. Tarchanidis, A. Mitropoulos, and
A. Gasteratos, “Unsupervised human detection with an embedded vision system
on a fully autonomous uav for search and rescue operations,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Sensors</em>,
vol. 19, no. 16, p. 3542, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
R. Geraldes, A. Goncalves, T. Lai, M. Villerabel, W. Deng, A. Salta,
K. Nakayama, Y. Matsuo, and H. Prendinger, “Uav-based situational awareness
system using deep learning,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 7, pp.
122 583–122 594, 2019.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
K. T. San, S. J. Mun, Y. H. Choe, and Y. S. Chang, “Uav delivery monitoring
system,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">MATEC Web of Conferences</em>, vol. 151.   EDP Sciences, 2018, p. 04011.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. Zhu, L. Wen, D. Du, X. Bian, H. Ling, Q. Hu, Q. Nie, H. Cheng, C. Liu,
X. Liu <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Visdrone-det2018: The vision meets drone object
detection in image challenge results,” in <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Proceedings of the European
Conference on Computer Vision (ECCV) Workshops</em>, 2018, pp. 0–0.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. A. Varga, B. Kiefer, M. Messmer, and A. Zell, “Seadronessee: A maritime
benchmark for detecting humans in open water,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2105.01922</em>, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
GDPR. General data protection regulation. [Online]. Available:
<a target="_blank" href="https://gdpr.eu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://gdpr.eu/</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
ExposingAI. Ms-celeb-1m privacy infringment. [Online]. Available:
<a target="_blank" href="https://exposing.ai/msceleb/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://exposing.ai/msceleb/</a>

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, and Q. Tian,
“The unmanned aerial vehicle benchmark: Object detection and tracking,” in
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision (ECCV)</em>,
2018, pp. 370–386.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. Kiefer, M. Messmer, and A. Zell, “Leveraging domain labels for object
detection from uavs,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.12677</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Messmer, B. Kiefer, and A. Zell, “Gaining scale invariance in uav bird’s
eye view object detection by adaptive resizing,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2101.12694</em>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Kar, A. Prakash, M.-Y. Liu, E. Cameracci, J. Yuan, M. Rusiniak, D. Acuna,
A. Torralba, and S. Fidler, “Meta-sim: Learning to generate synthetic
datasets,” in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, 2019, pp. 4551–4560.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. Srivastava, A. K. Singh, and G. M. Hegde, “Multi modal semantic
segmentation using synthetic data,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.13676</em>,
2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
B. Hurl, K. Czarnecki, and S. Waslander, “Precise synthetic image and lidar
(presil) dataset for autonomous vehicle perception,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2019 IEEE
Intelligent Vehicles Symposium (IV)</em>.   IEEE, 2019, pp. 2522–2529.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Fonder and M. V. Droogenbroeck, “Mid-air: A multi-modal dataset for
extremely low altitude drone flights,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Conference on Computer
Vision and Pattern Recognition Workshop (CVPRW)</em>, June 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
R. Games. Grand theft auto v. [Online]. Available:
<a target="_blank" href="https://www.rockstargames.com/de/games/V" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.rockstargames.com/de/games/V</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar, K. Rosaen, and
R. Vasudevan, “Driving in the matrix: Can virtual worlds replace
human-generated annotations for real world tasks?” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1610.01983</em>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Angus, M. ElBalkini, S. Khan, A. Harakeh, O. Andrienko, C. Reading,
S. Waslander, and K. Czarnecki, “Unlimited road-scene synthetic annotation
(ursa) dataset,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2018 21st International Conference on Intelligent
Transportation Systems (ITSC)</em>.   IEEE,
2018, pp. 985–992.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
X. Yue, B. Wu, S. A. Seshia, K. Keutzer, and A. L. Sangiovanni-Vincentelli, “A
lidar point cloud generator: from a virtual world to autonomous driving,” in
<em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 ACM on International Conference on Multimedia
Retrieval</em>, 2018, pp. 458–464.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity visual and
physical simulation for autonomous vehicles,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Field and service
robotics</em>.   Springer, 2018, pp.
621–635.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open
urban driving simulator,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Conference on robot learning</em>.   PMLR, 2017, pp. 1–16.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
W. Qiu and A. Yuille, “Unrealcv: Connecting computer vision to unreal
engine,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2016, pp. 909–916.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. I. Nikolenko <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Synthetic data for deep learning,”
<em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1909.11512</em>, vol. 3, 2019.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
H. Fan, L. Wen, D. Du, P. Zhu, Q. Hu, H. Ling, M. Shah, B. Wang, B. Dong,
D. Yuan <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Visdrone-sot2020: The vision meets drone single
object tracking challenge results,” in <em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.   Springer, 2020, pp. 728–749.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Z. Pei, X. Qi, Y. Zhang, M. Ma, and Y.-H. Yang, “Human trajectory prediction
in crowded scene using social-affinity long short-term memory,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition</em>, vol. 93, pp. 273–282, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for uav
tracking,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2016, pp. 445–461.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, “Drone-based object counting by
spatially regularized regional proposal network,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the IEEE International Conference on Computer Vision</em>, 2017, pp. 4145–4153.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
T. N. Mundhenk, G. Konjevod, W. A. Sakla, and K. Boakye, “A large contextual
dataset for classification, detection and counting of cars with deep
learning,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2016, pp. 785–800.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
S. Li and D.-Y. Yeung, “Visual object tracking for unmanned aerial vehicles: A
benchmark and new motion models,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI
Conference on Artificial Intelligence</em>, vol. 31, no. 1, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, “The highd dataset: A
drone dataset of naturalistic vehicle trajectories on german highways for
validation of highly automated driving systems,” in <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2018 21st
International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2018, pp. 2118–2125.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
J. C. van Gemert, C. R. Verschoor, P. Mettes, K. Epema, L. P. Koh, and S. Wich,
“Nature conservation drones for automatic localization and counting of
animals,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision</em>.   Springer, 2014, pp. 255–270.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
F. Ofli, P. Meier, M. Imran, C. Castillo, D. Tuia, N. Rey, J. Briant,
P. Millet, F. Reinhard, M. Parkan <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Combining human computing
and machine learning to make sense of big (aerial) data for disaster
response,” <em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic">Big data</em>, vol. 4, no. 1, pp. 47–59, 2016.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
I. Bozcan and E. Kayacan, “Au-air: A multi-modal unmanned aerial vehicle
dataset for low altitude traffic surveillance,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2020, pp. 8504–8510.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
F. Kong, B. Huang, K. Bradbury, and J. Malof, “The synthinel-1 dataset: a
collection of high resolution synthetic overhead imagery for building
segmentation,” in <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision</em>, 2020, pp. 1814–1823.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko, A. Efros, and
T. Darrell, “Cycada: Cycle-consistent adversarial domain adaptation,” in
<em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>.   PMLR, 2018, pp. 1989–1998.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
G. French, M. Mackiewicz, and M. Fisher, “Self-ensembling for visual domain
adaptation,” <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1706.05208</em>, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P. Li, X. Liang, D. Jia, and E. P. Xing, “Semantic-aware grad-gan for
virtual-to-real urban scene adaption,” <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1801.01726</em>, 2018.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Zou, Z. Yu, B. Kumar, and J. Wang, “Unsupervised domain adaptation for
semantic segmentation via class-balanced self-training,” in
<em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the European conference on computer vision (ECCV)</em>,
2018, pp. 289–305.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool, “Domain adaptive faster
r-cnn for object detection in the wild,” in <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2018, pp. 3339–3348.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State,
O. Shapira, and S. Birchfield, “Structured domain randomization: Bridging
the reality gap by context-aware synthetic data,” in <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">2019
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2019, pp. 7249–7255.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference on intelligent
robots and systems (IROS)</em>.   IEEE,
2017, pp. 23–30.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Y.-H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker,
“Learning to adapt structured output space for semantic segmentation,” in
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2018, pp. 7472–7481.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
S. R. Richter, H. A. AlHaija, and V. Koltun, “Enhancing photorealism
enhancement,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2105.04619</em>, 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image
translation using cycle-consistent adversarial networks,” in
<em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer vision</em>,
2017, pp. 2223–2232.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Airbus. Airbus ship dataset. [Online]. Available:
<a target="_blank" href="https://www.kaggle.com/c/airbus-ship-detection" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/c/airbus-ship-detection</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
W. Shao, R. Kawakami, R. Yoshihashi, S. You, H. Kawase, and T. Naemura,
“Cattle detection and counting in uav images based on convolutional neural
networks,” <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">International Journal of Remote Sensing</em>, vol. 41, no. 1,
pp. 31–52, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
B. Hurl. Deepgtav-presil code. [Online]. Available:
<a target="_blank" href="https://github.com/bradenhurl/DeepGTAV-PreSIL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/bradenhurl/DeepGTAV-PreSIL</a>

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
A. Ruano. Deepgtav code. [Online]. Available:
<a target="_blank" href="https://github.com/aitorzip/DeepGTAV" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/aitorzip/DeepGTAV</a>

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
A. Blade. Script hook v. [Online]. Available:
<a target="_blank" href="http://www.dev-c.com/gtav/scripthookv/" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.dev-c.com/gtav/scripthookv/</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
A. Ruano. Deepgtav-vpilot code. [Online]. Available:
<a target="_blank" href="https://github.com/aitorzip/VPilot" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/aitorzip/VPilot</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Gta vision export. [Online]. Available:
<a target="_blank" href="https://github.com/umautobots/GTAVisionExport" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/umautobots/GTAVisionExport</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
dorahamu. Gtav mod - simple increase traffic and pedestrians. [Online].
Available:
<a target="_blank" href="https://de.gta5-mods.com/misc/simple-increase-traffic-and-pedestrian" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://de.gta5-mods.com/misc/simple-increase-traffic-and-pedestrian</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
VenJam1n. Gtav mod - no chromatric aberration and lens distortion. [Online].
Available:
<a target="_blank" href="https://de.gta5-mods.com/misc/no-chromatic-aberration-lens-distortion-1-41" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://de.gta5-mods.com/misc/no-chromatic-aberration-lens-distortion-1-41</a>

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
F. S. Division. Gtav mod - heap limit adjuster. [Online]. Available:
<a target="_blank" href="https://de.gta5-mods.com/tools/heap-limit-adjuster-600-mb-of-heap" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://de.gta5-mods.com/tools/heap-limit-adjuster-600-mb-of-heap</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Nvidia. Nvidia geforce experience. [Online]. Available:
<a target="_blank" href="https://www.nvidia.com/de-de/geforce/geforce-experience/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.nvidia.com/de-de/geforce/geforce-experience/</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
P. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, “Vision meets drones: A
challenge,” <em id="bib.bib56.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.07437</em>, 2018.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
M. Tan, R. Pang, and Q. V. Le, “Efficientdet: Scalable and efficient object
detection,” in <em id="bib.bib57.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition</em>, 2020, pp. 10 781–10 790.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib58.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
J. Redmon and A. Farhadi, “YOLO9000: better, faster, stronger,” in
<em id="bib.bib59.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2017, pp. 7263–7271.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
——, “Yolov3: An incremental improvement,” <em id="bib.bib60.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1804.02767</em>, 2018.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “YOLOv4: Optimal Speed and
Accuracy of Object Detection,” <em id="bib.bib61.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.10934</em>,
2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
G. Jocher, L. Changyu, A. Hogan, L. Yu, changyu98, P. Rai, and T. Sullivan,
“ultralytics/yolov5: Initial Release,” Jun. 2020. [Online]. Available:
<a target="_blank" href="https://doi.org/10.5281/zenodo.3908560" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.3908560</a>

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">European conference on computer vision</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
P. with Code. Object detection on coco test-dev. [Online]. Available:
<a target="_blank" href="https://paperswithcode.com/sota/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://paperswithcode.com/sota/</a>

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
S. GmbH, “A pytorch implementation of efficientdet object detection,”
<em id="bib.bib65.1.1" class="ltx_emph ltx_font_italic">https://github.com/signatrix/efficientdet</em>, 2020.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
G. Jocher, A. Stoken, J. Borovec, NanoCode012, A. Chaurasia, TaoXie,
L. Changyu, A. V, Laughing, tkianai, yxNONG, A. Hogan, lorenzomammana,
AlexWang1900, J. Hajek, L. Diaconu, Marc, Y. Kwon, oleg, wanghaoyang0106,
Y. Defretin, A. Lohia, ml5ah, B. Milanko, B. Fineran, D. Khromov, D. Yiwei,
Doug, Durgesh, and F. Ingham, “ultralytics/yolov5: v5.0 - YOLOv5-P6 1280
models, AWS, Supervise.ly and YouTube integrations,” Apr. 2021. [Online].
Available: <a target="_blank" href="https://doi.org/10.5281/zenodo.4679653" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.5281/zenodo.4679653</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
transformations for deep neural networks,” in <em id="bib.bib67.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2017, pp. 1492–1500.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
G. Varol, J. Romero, X. Martin, N. Mahmood, M. J. Black, I. Laptev, and
C. Schmid, “Learning from synthetic humans,” in <em id="bib.bib68.1.1" class="ltx_emph ltx_font_italic">Proceedings of the
IEEE conference on computer vision and pattern recognition</em>, 2017, pp.
109–117.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
B. Kiefer and M. Messmer. Seadronessee benchmark. [Online]. Available:
<a target="_blank" href="https://seadronessee.cs.uni-tuebingen.de/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://seadronessee.cs.uni-tuebingen.de/</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib70.1.1" class="ltx_emph ltx_font_italic">2009 IEEE conference on
computer vision and pattern recognition</em>.   Ieee, 2009, pp. 248–255.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.12251" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.12252" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.12252">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.12252" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.12253" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:36:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
