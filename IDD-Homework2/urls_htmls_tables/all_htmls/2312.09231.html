<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.09231] Reliability in Semantic Segmentation: Can We Use Synthetic Data?</title><meta property="og:description" content="Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles.
By nature of the task, however, the re…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reliability in Semantic Segmentation: Can We Use Synthetic Data?">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reliability in Semantic Segmentation: Can We Use Synthetic Data?">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.09231">

<!--Generated on Tue Feb 27 13:46:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">
Reliability in Semantic Segmentation: Can We Use Synthetic Data?
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thibaut Loiseau<sup id="id9.2.id1" class="ltx_sup"><span id="id9.2.id1.1" class="ltx_text ltx_font_italic">2,†</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tuan-Hung Vu<sup id="id10.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mickael Chen<sup id="id11.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Patrick Pérez<sup id="id12.2.id1" class="ltx_sup">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matthieu Cord<sup id="id13.2.id1" class="ltx_sup"><span id="id13.2.id1.1" class="ltx_text ltx_font_italic">1,3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id14.2.id1" class="ltx_sup">1</sup>Valeo.ai
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id15.2.id1" class="ltx_sup">2</sup>ENPC ParisTech
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id16.2.id1" class="ltx_sup">3</sup>Sorbonne University 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id17.id1" class="ltx_p">Assessing the reliability of perception models to covariate shifts and out-of-distribution (OOD) detection is crucial for safety-critical applications such as autonomous vehicles.
By nature of the task, however, the relevant data is difficult to collect and annotate.
In this paper, we challenge cutting-edge generative models to automatically synthesize data for assessing reliability in semantic segmentation.
By fine-tuning Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, we perform zero-shot generation of synthetic data in OOD domains or inpainted with OOD objects.
Synthetic data is employed to provide an initial assessment of pretrained segmenters, thereby offering insights into their performance when confronted with real edge cases.
Through extensive experiments, we demonstrate a high correlation between the performance on synthetic data and the performance on real OOD data, showing the validity approach.
Furthermore, we illustrate how synthetic data can be utilized to enhance the calibration and OOD detection capabilities of segmenters.</p>
</div>
<div id="p2" class="ltx_para ltx_align_center">
<img src="/html/2312.09231/assets/x1.png" id="p2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="157" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.5.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">An overview of our study.<span id="S0.F1.6.2.1" class="ltx_text ltx_font_medium"> Starting with in-domain data, we generate synthetic samples with the required ground truths for metrics computation. The quantitative comparison involves synthetic data <span id="S0.F1.6.2.1.1" class="ltx_text ltx_font_italic">vs</span>.<span id="S0.F1.6.2.1.2" class="ltx_text"></span> real data with various domain shifts and OOD objects.</span></span></figcaption>
</figure><span id="footnotex1" class="ltx_note ltx_centering ltx_role_footnotetext"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">$\dagger$</sup><span class="ltx_note_type">footnotetext: </span>This work was done during an internship at Valeo.ai</span></span></span>
<section id="S1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Fueled by extensive training data, modern neural networks consistently advance accuracy across various perception benchmarks.
Despite the rapid adoption of deep networks in safety-critical applications, reliability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite> has been an overlooked factor when designing and training these models.
Recent efforts are geared toward enhancing model robustness under covariate shifts in data distributions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite> and improving the model’s ability to detect the <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">unknown</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.
For open-world validation, in-domain data is no longer sufficient <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>; reliable and trustworthy systems demand more rigorous testing on diverse distributions, potentially exhibiting unknown objects.
However, collecting and annotating data is a challenging and costly endeavor.
Data collection campaigns can be quickly overwhelmed by the growing number of out-of-distribution (OOD) conditions and objects.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">While real data remains the ultimate resource for validating real-world applications, recent advances in generative networks bring hopes for a potential automatic validation framework that can gradually alleviate the need for collecting and annotating real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>.
Trained on massive datasets, state-of-the-art generative networks are regarded as foundation generalists that encompass vast semantic and domain knowledge.
Furthermore, text-to-image models like Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, empowered by large vision-language models, possess the seamless prompting ability that enables zero-shot generation.
Via prompting, one can intentionally generate data from any target domain with minimal effort, allowing stress-testing perception models even under extremely rare conditions.
For instance, although fires or floods rarely occur in reality, self-driving cars are deemed robust and resilient; testing in such conditions is therefore crucial.
Moreover, in the open world, objects not encountered during training may appear anywhere; a reliable system should be capable of recognizing its limitations and classifying such objects as <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">unknown</span>, referred to as OOD detection.
Recent OOD detection datasets attempt to benchmark perception models with random objects found on the road <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>; however, the number of test images and the variety of OOD objects are very limited.
To tackle this challenge cost-effectively, zero-shot generative inpainting emerges as a promising tool, enabling validation across a much wider range of objects.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this work, we exploit generative networks as a general validator, targeting multiple faces of reliability.
We benchmark segmentation robustness under global covariate shifts and the ability to locally detect OOD objects.
Further than that, our synthetic data can be used for calibrating pretrained models to targeted domains and for improving the OOD detection performance.
To validate our proposal, we compare our synthetic data against using real test data from existing benchmarks on similar tasks, i.e. ACDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and SegmentMeIfYouCan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>.
Extensive experiments demonstrate our findings as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Synthetic data can serve as a reliable indicator of the real-life performance of segmenters under covariate shifts.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Pretrained segmenters can leverage synthetic data to improve confidence calibration.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Synthetic inpainted data is valuable for benchmarking OOD detection. Our high-quality synthetic data is featured in the official BRAVO benchmark.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://valeoai.github.io/bravo/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://valeoai.github.io/bravo/</a></span></span></span></p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">Training on inpainted OOD objects helps models become more discerning when encountering real OOD objects.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Covariate Shifts.</span> Modern machine learning models, notably deep networks, fall short in preserving their robustness and estimating their prediction confidence in the presence of covariate shifts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>.
The last few years have witnessed the emergence of various benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> that address the need for assessing models’ reliability under different distributional shifts.
Hendrycks et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite> propose a pioneering benchmark featuring data corrupted by various synthetic perturbations such as noise, blur, and brightness, etc.
Taori et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> emphasize the importance of realistic shifts in reliability assessment; they highlight the disparity between natural and synthetic shifts, asserting that there is minimal to no robustness transfer from synthetic to natural distribution shifts.
Sign et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite> studiy low-shot robustness to natural distribution shifts, highlighting the robustness properties of advanced architecture and pretraining strategies.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, de Jorge et al. extend beyond the standard classification setup to address the reliability problem in semantic segmentation.
Most findings are largely aligned with previous works on classification; one interesting finding is on the disconnection between robustness and confidence calibration, urging more attention to calibration during segmenter design and training.
On the same research line, one intriguing aspect is to study the connections between in-domain and out-of-domain robustness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>; different findings suggest either positive, none or even negative correlations between ID and OOD robustness, which indeed largely depends on the type of shifts.
In line with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, we focus on segmentation as the primary research task.
Different from previous works, we advocate for the use of advanced generative models to generate realistic synthetic data for testing segmenters under arbitrary covariate shifts.
We align with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite> regarding the realism of testing data and argue for the rapid maturation of generative models that are progressively narrowing the gap between synthetic and real shifts.
Our goal is to study whether synthetic data can be a superior choice compared to ID data in correlation studies against OOD robustness – referred to as real-shift robustness in the subsequent text to avoid confusion with OOD detection.
Motivated by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, we also attempt to address the calibration problem by leveraging our synthetic data.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_bold">OOD Object Detection.</span>
In addition to robustness and calibration, the ability to detect the “unknown” is an equally important aspect, completing the reliability assessment for trustworthy systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>]</cite>.
In semantic segmentation, several datasets are available for evaluating OOD detection on the road, namely LostAndFound <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, StreetHazards <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> (synthetic), BDD-Anomaly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>, Fishyscapes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite> (synthetic) and SegmentMeIfYouCan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>.
Encountering and capturing images of OOD objects in real-world scenarios, without deliberately placing objects on the road, is quite uncommon.
Effectively, existing datasets are limited in scale, both in terms of the number of images and the variety of object classes.
In this work, we propose leveraging advanced zero-shot inpainting techniques to augment an existing segmentation dataset with inpainted OOD objects; enabling the generation of highly realistic synthetic data for testing and training OOD detection.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Synthetic Data for Testing.</span>
Generative models have been exploited to generate training data either for image classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, or semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.
Only recently, a few works have delved into the topic of generative testing data.
Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite> exploit diffusion models to realistically edit images, controlling over various object attributes.
This approach enables stress-testing models and understanding their sensitivity to different attributes.
Using Stable Diffusion, LANCE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite> generates counterfactual images capable of challenging any given perception model.
To the extent of our knowledge, no existing works have proposed to generate testing data for segmentation reliability.</p>
</div>
</section>
<section id="S3" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Reliability Under Covariate Shifts</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we explore whether synthetic data can be used to assess the robustness of pretrained segmenters in the presence of covariate shifts to unseen domains.
We describe the data generation process and present the benchmarking results for a wide range of segmenters on our synthetic data.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Generating images in arbitrary domains</h3>

<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2312.09231/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="214" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2312.09231/assets/x3.png" id="S3.F2.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="461" height="210" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Generating data with covariate shifts.<span id="S3.F2.4.2.1" class="ltx_text ltx_font_medium"> Training (top) and Sampling (bottom) processes for producing the synthetic data with shifts. Note that we only use images and labels from the original domain.</span></span></figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2312.09231/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="380" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.15.3.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Robustness Correlation<span id="S3.F3.4.2.2" class="ltx_text ltx_font_medium"> on real <span id="S3.F3.4.2.2.1" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.F3.4.2.2.2" class="ltx_text"></span> synthetic covariate shifts across <math id="S3.F3.3.1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.F3.3.1.1.m1.1b"><mn id="S3.F3.3.1.1.m1.1.1" xref="S3.F3.3.1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.F3.3.1.1.m1.1c"><cn type="integer" id="S3.F3.3.1.1.m1.1.1.cmml" xref="S3.F3.3.1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.1.1.m1.1d">40</annotation></semantics></math> pretrained segmenters with different architectures and network sizes. Each subplot in the second row displays the scattered mIoUs (synthetic <span id="S3.F3.4.2.2.3" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.F3.4.2.2.4" class="ltx_text"></span> real) and the linear regression line accompanied by <math id="S3.F3.4.2.2.m2.1" class="ltx_Math" alttext="95\%" display="inline"><semantics id="S3.F3.4.2.2.m2.1b"><mrow id="S3.F3.4.2.2.m2.1.1" xref="S3.F3.4.2.2.m2.1.1.cmml"><mn id="S3.F3.4.2.2.m2.1.1.2" xref="S3.F3.4.2.2.m2.1.1.2.cmml">95</mn><mo id="S3.F3.4.2.2.m2.1.1.1" xref="S3.F3.4.2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.4.2.2.m2.1c"><apply id="S3.F3.4.2.2.m2.1.1.cmml" xref="S3.F3.4.2.2.m2.1.1"><csymbol cd="latexml" id="S3.F3.4.2.2.m2.1.1.1.cmml" xref="S3.F3.4.2.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.F3.4.2.2.m2.1.1.2.cmml" xref="S3.F3.4.2.2.m2.1.1.2">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.4.2.2.m2.1d">95\%</annotation></semantics></math> confidence intervals (CI). The subplots in the top row illustrate the Pearson Correlation Coefficients, (i) between Cityscapes mIoUs <span id="S3.F3.4.2.2.5" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.F3.4.2.2.6" class="ltx_text"></span> real-shift mIoUs (PCC_CS <span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>), as well as (ii) between synthetic-shift mIoUs <span id="S3.F3.4.2.2.7" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.F3.4.2.2.8" class="ltx_text"></span> real-shift mIoUs (PCC_Syn <span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>). The subplots are sorted from left to right based on domain gaps to Cityscapes, represented by PCC_CS (<span class="ltx_rule" style="width:4.5pt;height:7.0pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:4.5pt;height:3.9pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:4.5pt;height:1.9pt;background:black;display:inline-block;"> </span>). The robustness results on synthetic data exhibit a strong correlation with those on real data, particularly in the case of the most distant shifts like ‘snow’ and ‘night’.</span></span></figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our goal here is to obtain pairs of images and semantic masks, with the images belonging to visual domains for which we lack data.
To this end, we leverage a pretrained text-to-image Stable Diffusion (SD) model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, repurposed as a semantic conditioned model called ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>.
The ControlNet is trained solely on images and segmentation ground truths from the Cityscapes dataset; at train time, the text prompts are captions automatically extracted using CLIP-interrogator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>.
As a result, the model is able to perform mask-to-image generation of driving scenes while retaining the ability to steer the generation through text prompting of Stable Diffusion.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">To generate synthetic data, we prompt the trained ControlNet by the concatenation of target domain descriptions and CLIP-interrogator captions obtained from Cityscapes validation images.
Also, segmentation masks in the Cityscapes validation set are used to condition the generative process.
Thanks to zero-shot prompting, the synthetic images are aligned with the semantic condition while displaying the visual properties of arbitrary target domains.
 <a href="#S3.F2" title="Figure 2 ‣ 3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a> illustrates the training and generation steps.
Detailed technical descriptions and more visualizations are in <a href="#A1" title="Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Robustness Assessment with Synthetic Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">With the pipeline outlined in <a href="#S3.SS1" title="3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3.1</span></a>, one can generate synthetic data to assess the robustness of pretrained segmenters in any target unseen domains through zero-shot prompting.
To quantify robustness, we employ the traditional mean Intersection-over-Union (mIoU) score, measuring the correct overlap between semantic predictions and ground-truth masks.
Given that our synthetic dataset comprises pairs of segmentation masks and synthetic images, one can straightforwardly derive synthetic scores for any pretrained segmenters.
We here investigate whether synthetic performance can correlatively reflect the performance on real data in target domains under covariate shifts.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Experimental Setups.</span>
We address weather shifts and geographical shifts, which are often encountered in the context of autonomous driving.
These covariate shifts are exhibited in the two existing real datasets: the Adverse Conditions Dataset (ACDC) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite> and the Indian Driving Dataset (IDD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite>.
We utilize those real data to quantify the quality of our synthetic data and to validate its usefulness.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.4" class="ltx_p">Synthetic data are generated by conditioning on semantic masks from the Cityscapes validation set.
To prompt the ControlNet, we concatenate CLIP-interrogator’s caption with domain description following a simple template <span id="S3.SS2.p3.4.4" class="ltx_text ltx_font_typewriter">[<math id="S3.SS2.p3.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p3.1.1.m1.1a"><mo id="S3.SS2.p3.1.1.m1.1.1" xref="S3.SS2.p3.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.1.m1.1b"><lt id="S3.SS2.p3.1.1.m1.1.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.1.m1.1c">&lt;</annotation></semantics></math>caption<math id="S3.SS2.p3.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.2.2.m2.1a"><mo id="S3.SS2.p3.2.2.m2.1.1" xref="S3.SS2.p3.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.2.m2.1b"><gt id="S3.SS2.p3.2.2.m2.1.1.cmml" xref="S3.SS2.p3.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.2.m2.1c">&gt;</annotation></semantics></math>, in <math id="S3.SS2.p3.3.3.m3.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S3.SS2.p3.3.3.m3.1a"><mo id="S3.SS2.p3.3.3.m3.1.1" xref="S3.SS2.p3.3.3.m3.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.3.m3.1b"><lt id="S3.SS2.p3.3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.3.m3.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.3.m3.1c">&lt;</annotation></semantics></math>domain<math id="S3.SS2.p3.4.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="S3.SS2.p3.4.4.m4.1a"><mo id="S3.SS2.p3.4.4.m4.1.1" xref="S3.SS2.p3.4.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.4.m4.1b"><gt id="S3.SS2.p3.4.4.m4.1.1.cmml" xref="S3.SS2.p3.4.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.4.m4.1c">&gt;</annotation></semantics></math>]</span> where <span id="S3.SS2.p3.4.5" class="ltx_text ltx_font_typewriter">domain</span> is either ‘india’, ‘fog’, ‘rain’, ‘snow’ or ‘night’.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">For testing, we gather a collection of 40 publicly available segmenters trained <span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_italic">only on Cityscapes</span>, representatives of a variety of backbones, segmentation architectures, and network sizes.
The full list of models is in <a href="#A1" title="Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Results.</span>
In <a href="#S3.F3" title="Figure 3 ‣ 3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, we present our main results of this study.
Our primary metric is the Pearson Correlation Coefficient (PCC) between the mIoUs on testing data <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.SS2.p5.1.3" class="ltx_text"></span> the mIoUs on real-shift data from ACDC’s splits or IDD.
The testing data can be either the Cityscapes validation set (CS) or our synthetic data (syn); the idea is to see which testing data –whether real CS or our synthetic– correlates more with the real-shift data.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">We organize our results based on increasing domain gaps relative to the Cityscapes domain.
The domain gaps are quantified by the Pearson correlation between Cityscapes mIoUs and real-shift mIoUs, annotated as PCC_CS and visualized as red bars <span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span> in the subplots of <a href="#S3.F3" title="Figure 3 ‣ 3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>.
Moving from left to right, <span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_italic">i.e</span>.<span id="S3.SS2.p6.1.2" class="ltx_text"></span> with growing domain gaps, we observe a widening discrepancy between PCC_CS and PCC_Syn.
Here, PCC_Syn (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>) represents the Pearson correlation between synthetic mIoUs and real-shift mIoUs.
In domains with small gaps, PCC_CS and PCC_Syn are relatively comparable. However, in domains with more adverse shifts, such as ‘snow’ and ‘night’, PCC_Syn outperforms PCC_CS significantly, exceeding PCC_CS in ‘night’ by more than double.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p">In <a href="#S3.F4" title="Figure 4 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, we analyze the results for the ‘night’ condition using the most robust models across different architectures, ranging from traditional convnets to recent transformer networks.
We use the Semantic-FPN score as the reference to normalize the scores of other models.
This normalization aims to illustrate the relative improvement in robustness concerning architecture.
We rank the models from left to right based on their performance on real night data from the ACDC-night split.
The consistently increasing trend of synthetic scores (<span class="ltx_rule" style="width:5.0pt;height:2.2pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:5.0pt;height:4.3pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>) from left to right demonstrates a strong correlation with the ranking based on real scores.
Conversely, Cityscapes scores (<span class="ltx_rule" style="width:5.0pt;height:6.5pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span><span class="ltx_rule" style="width:5.0pt;height:7.5pt;background:black;display:inline-block;"> </span>) are not indicative of night performance; that is, a higher mIoU obtained on Cityscapes does not immediately translate into a higher mIoU at night.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2312.09231/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Day-night shift.<span id="S3.F4.4.2.1" class="ltx_text ltx_font_medium">
Models are ranked from left to right by their robustness on real night data – ACDC-Night mIoUs are shown on top of model names.
For each presented architecture, the most robust model on Cityscapes is tested; the Semantic-FPN, DeeplabV3+, and PSPNet models have ResNet-101 as backbone.
The Semantic-FPN model (lowest mIoU on ACDC-Night) serves as the reference for computing the relative mIoUs.
Blue bars or orange bars show the relative mIoUs when testing on our synthetic data (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) or testing on Cityscapes validation data (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>).
Cityscapes scores are not reliable for ranking models in the night domain.
Synthetic scores exhibit a stronger correlation with real night scores, as evidenced by the more consistently increasing trend in the blue bars from left to right.
</span></span></figcaption>
</figure>
<div id="S3.SS2.p8" class="ltx_para">
<p id="S3.SS2.p8.1" class="ltx_p">Since synthetic data can be generated in any desired quantity, a natural question arises: how many images are sufficient?
In addressing this question, we conducted experiments and presented the results in <a href="#S3.F5" title="Figure 5 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.
Our empirical finding suggests that <math id="S3.SS2.p8.1.m1.1" class="ltx_Math" alttext="\sim 500" display="inline"><semantics id="S3.SS2.p8.1.m1.1a"><mrow id="S3.SS2.p8.1.m1.1.1" xref="S3.SS2.p8.1.m1.1.1.cmml"><mi id="S3.SS2.p8.1.m1.1.1.2" xref="S3.SS2.p8.1.m1.1.1.2.cmml"></mi><mo id="S3.SS2.p8.1.m1.1.1.1" xref="S3.SS2.p8.1.m1.1.1.1.cmml">∼</mo><mn id="S3.SS2.p8.1.m1.1.1.3" xref="S3.SS2.p8.1.m1.1.1.3.cmml">500</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p8.1.m1.1b"><apply id="S3.SS2.p8.1.m1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p8.1.m1.1.1.1.cmml" xref="S3.SS2.p8.1.m1.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S3.SS2.p8.1.m1.1.1.2.cmml" xref="S3.SS2.p8.1.m1.1.1.2">absent</csymbol><cn type="integer" id="S3.SS2.p8.1.m1.1.1.3.cmml" xref="S3.SS2.p8.1.m1.1.1.3">500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p8.1.m1.1c">\sim 500</annotation></semantics></math> synthetic images are adequate for a stable and reliable assessment of robustness.</p>
</div>
<div id="S3.SS2.p9" class="ltx_para ltx_noindent">
<p id="S3.SS2.p9.1" class="ltx_p"><span id="S3.SS2.p9.1.1" class="ltx_text ltx_font_bold">Discussion.</span> In the recent work on reliability in semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, Jorge et al. systematically quantified the robustness of segmenters on real-shift data; similarly to ours, they draw comparisons from ACDC and IDD datasets.
One intriguing finding in this paper is that “..the larger the domain shift, the larger the improvement brought by more recent segmentation models”, hinting at a correlation between model robustness on in-domain data and covariate-shift data.
In our study, we delve deeper into this correlation, choosing to separately address different weather types instead of grouping them all together as done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.
In domains exhibiting small gaps to Cityscapes, such as IDD or ACDC-Fog, our conclusion aligns with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>.
However, as domain gaps increase, the discrepancy between Cityscapes mIoUs and real-shift mIoUs becomes more pronounced, resulting in poor PCC_CS scores.
On the contrary, synthetic mIoUs and real-shift mIoUs exhibit a strong correlation across shifts.
<span id="S3.SS2.p9.1.2" class="ltx_text ltx_font_italic">Our empirical study has validated that synthetic performance is a reliable indicator of model robustness in the presence of covariate shifts.</span></p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2312.09231/assets/x6.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.5.2.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Pearson Correlation <span id="S3.F5.2.1.2" class="ltx_text ltx_font_italic">vs</span>.<span id="S3.F5.2.1.3" class="ltx_text"></span> # Synthetic Samples.<span id="S3.F5.2.1.1" class="ltx_text ltx_font_medium"> The use of more synthetic samples contributes to increased stability in the results. Empirical plots demonstrate that approximately <math id="S3.F5.2.1.1.m1.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S3.F5.2.1.1.m1.1b"><mn id="S3.F5.2.1.1.m1.1.1" xref="S3.F5.2.1.1.m1.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.F5.2.1.1.m1.1c"><cn type="integer" id="S3.F5.2.1.1.m1.1.1.cmml" xref="S3.F5.2.1.1.m1.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.2.1.1.m1.1d">500</annotation></semantics></math> samples are sufficient for a stable correlation assessment.</span></span></figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2312.09231/assets/x7.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="461" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S3.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">ECE improvement using synthetic data.<span id="S3.F6.4.2.1" class="ltx_text ltx_font_medium"> The 40 segmenters are calibrated using either real-shift data or synthetic data. For each model, the relative ECE improvement (%) over its non-calibrated version is computed, visualized by <span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span> (synthetic shift) and <span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span> (real shift). The subplots are ranked by model robustness on real-shift data (left-to-right) and by the increased domain shift (top-to-bottom).
The improvements in calibration are observed when using synthetic data, although those are much less pronounced compared to when using real data.
The titles of the subplots indicate the percentage of models that showed improvement with synthetic data.
In the case of weather shifts, we observe that synthetic data is particularly helpful for more robust models (i.e., the right part), while for geographical shifts, the improvement is more significant for the least robust models.</span></span></figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Confidence Calibration with Synthetic Data</h3>

<figure id="S3.F7" class="ltx_figure">
<div id="S3.F7.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:238.5pt;height:196.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.5pt,8.7pt) scale(0.91905734915422,0.91905734915422) ;">
<table id="S3.F7.12.12" class="ltx_tabular ltx_align_middle">
<tr id="S3.F7.12.12.13" class="ltx_tr">
<td id="S3.F7.12.12.13.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="S3.F7.12.12.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.F7.12.12.13.2.1" class="ltx_text" style="font-size:80%;">Synthetic Images</span></td>
<td id="S3.F7.12.12.13.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.F7.12.12.13.3.1" class="ltx_text" style="font-size:80%;">Semantic-FPN</span></td>
<td id="S3.F7.12.12.13.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.F7.12.12.13.4.1" class="ltx_text" style="font-size:80%;">MobileNetV3</span></td>
<td id="S3.F7.12.12.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S3.F7.12.12.13.5.1" class="ltx_text" style="font-size:80%;">SegF-B5</span></td>
</tr>
<tr id="S3.F7.4.4.4" class="ltx_tr">
<td id="S3.F7.4.4.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S3.F7.4.4.4.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:37.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.7pt;transform:translate(-16.06pt,-16.06pt) rotate(-90deg) ;">
<p id="S3.F7.4.4.4.5.1.1" class="ltx_p"><span id="S3.F7.4.4.4.5.1.1.1" class="ltx_text" style="font-size:80%;">Flood</span></p>
</span></div>
</td>
<td id="S3.F7.1.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/flood_9.jpg" id="S3.F7.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.2.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/flood_9_pred_SemFPN-R101.png" id="S3.F7.2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.3.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/flood_9_pred_MobileNetV3.png" id="S3.F7.3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.4.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/flood_9_pred_SegFormer-B5.png" id="S3.F7.4.4.4.4.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
</tr>
<tr id="S3.F7.8.8.8" class="ltx_tr">
<td id="S3.F7.8.8.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S3.F7.8.8.8.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:45.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.1pt;transform:translate(-19.82pt,-19.82pt) rotate(-90deg) ;">
<p id="S3.F7.8.8.8.5.1.1" class="ltx_p"><span id="S3.F7.8.8.8.5.1.1.1" class="ltx_text" style="font-size:80%;">Autumn</span></p>
</span></div>
</td>
<td id="S3.F7.5.5.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/leaves_9.jpg" id="S3.F7.5.5.5.1.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.6.6.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/leaves_9_pred_SemFPN-R101.png" id="S3.F7.6.6.6.2.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.7.7.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/leaves_9_pred_MobileNetV3.png" id="S3.F7.7.7.7.3.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.8.8.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/leaves_9_pred_SegFormer-B5.png" id="S3.F7.8.8.8.4.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
</tr>
<tr id="S3.F7.12.12.12" class="ltx_tr">
<td id="S3.F7.12.12.12.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S3.F7.12.12.12.5.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:32.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.8pt;transform:translate(-13.67pt,-13.67pt) rotate(-90deg) ;">
<p id="S3.F7.12.12.12.5.1.1" class="ltx_p"><span id="S3.F7.12.12.12.5.1.1.1" class="ltx_text" style="font-size:80%;">Fire</span></p>
</span></div>
</td>
<td id="S3.F7.9.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/fire_9.jpg" id="S3.F7.9.9.9.1.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.10.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/fire_9_pred_SemFPN-R101.png" id="S3.F7.10.10.10.2.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.11.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/fire_9_pred_MobileNetV3.png" id="S3.F7.11.11.11.3.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
<td id="S3.F7.12.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec3_qual/fire_9_pred_SegFormer-B5.png" id="S3.F7.12.12.12.4.g1" class="ltx_graphics ltx_img_square" width="83" height="83" alt="Refer to caption"></td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.17.1.1" class="ltx_text" style="font-size:113%;">Figure 7</span>: </span><span id="S3.F7.18.2" class="ltx_text ltx_font_bold" style="font-size:113%;">Qualitative results.<span id="S3.F7.18.2.1" class="ltx_text ltx_font_medium"> Examples of rare conditions generated for testing and predictions from different models. Results of the strong model like SegFormer-B5 is visibly better than the Semantic-FPN and MobileNetV3.</span></span></figcaption>
</figure>
<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Confidence calibration is a crucial aspect of deep networks, particularly when employed in safety-critical applications such as autonomous driving.
Jorge et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> highlighted a disconnection between model robustness and calibration, asserting that “… despite the remarkable improvements in terms of robustness, recent models are not significantly better calibrated”.
Therefore, it is essential to devise techniques and protocols for recalibrating data, particularly in domains exhibiting covariate shifts.
Drawing inspiration from this, we explore the feasibility of using synthetic data to recalibrate pretrained segmenters.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">We perform temperature scaling using our synthetic data. Temperature scaling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>]</cite> is a well-established technique for calibrating pretrained models, typically conducted on a small validation set within the target domain.
In our study, for each segmenter, we utilized the same sets of synthetic data generated in <a href="#S3.SS2" title="3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 3.2</span></a> to optimize temperature scaling factors, with one adjustment made for each covariate shift.
For comparison, we replicate the process using real-shift data from ACDC and IDD.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.2" class="ltx_p"><a href="#S3.F6" title="Figure 6 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> reports the calibration improvement for the <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mn id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><cn type="integer" id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">40</annotation></semantics></math> pretrained segmenters using either real-shift data (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>) or synthetic data (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>).
The subplots are arranged in increasing domain gap order from top to bottom, with the segmenters ranked from left to right based on increasing robustness on real-shift data.
The Expected Calibration Error (ECE) quantifies the calibration results, with a lower ECE indicating a better-calibrated model.
For better visualization and interpretation, we present the relative ECE improvement, computed as the percentage decrease in ECE after calibration compared to the original ECE without calibration.
For example, a model with an ECE of 0.4 before calibration and an ECE of 0.2 after calibration will achieve a <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="(0.4-0.2)/0.4=50\%" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mrow id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml"><mrow id="S3.SS3.p3.2.m2.1.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS3.p3.2.m2.1.1.1.1.1.2" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p3.2.m2.1.1.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml"><mn id="S3.SS3.p3.2.m2.1.1.1.1.1.1.2" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.2.cmml">0.4</mn><mo id="S3.SS3.p3.2.m2.1.1.1.1.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS3.p3.2.m2.1.1.1.1.1.1.3" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.3.cmml">0.2</mn></mrow><mo stretchy="false" id="S3.SS3.p3.2.m2.1.1.1.1.1.3" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS3.p3.2.m2.1.1.1.2" xref="S3.SS3.p3.2.m2.1.1.1.2.cmml">/</mo><mn id="S3.SS3.p3.2.m2.1.1.1.3" xref="S3.SS3.p3.2.m2.1.1.1.3.cmml">0.4</mn></mrow><mo id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">=</mo><mrow id="S3.SS3.p3.2.m2.1.1.3" xref="S3.SS3.p3.2.m2.1.1.3.cmml"><mn id="S3.SS3.p3.2.m2.1.1.3.2" xref="S3.SS3.p3.2.m2.1.1.3.2.cmml">50</mn><mo id="S3.SS3.p3.2.m2.1.1.3.1" xref="S3.SS3.p3.2.m2.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><eq id="S3.SS3.p3.2.m2.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.2"></eq><apply id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1"><divide id="S3.SS3.p3.2.m2.1.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.1.2"></divide><apply id="S3.SS3.p3.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1"><minus id="S3.SS3.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.1"></minus><cn type="float" id="S3.SS3.p3.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.2">0.4</cn><cn type="float" id="S3.SS3.p3.2.m2.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.1.1.1.1.3">0.2</cn></apply><cn type="float" id="S3.SS3.p3.2.m2.1.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.1.3">0.4</cn></apply><apply id="S3.SS3.p3.2.m2.1.1.3.cmml" xref="S3.SS3.p3.2.m2.1.1.3"><csymbol cd="latexml" id="S3.SS3.p3.2.m2.1.1.3.1.cmml" xref="S3.SS3.p3.2.m2.1.1.3.1">percent</csymbol><cn type="integer" id="S3.SS3.p3.2.m2.1.1.3.2.cmml" xref="S3.SS3.p3.2.m2.1.1.3.2">50</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">(0.4-0.2)/0.4=50\%</annotation></semantics></math> relative improvement.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.2" class="ltx_p"><span id="S3.SS3.p4.2.1" class="ltx_text ltx_font_italic">We observe promising calibration results when employing our synthetic data.</span> While not as good as real-shift data, synthetic data achieves a promising success rate of <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="72.5\%" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mn id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">72.5</mn><mo id="S3.SS3.p4.1.m1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">72.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">72.5\%</annotation></semantics></math> on IDD and exceeds <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mn id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">90</mn><mo id="S3.SS3.p4.2.m2.1.1.1" xref="S3.SS3.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">90\%</annotation></semantics></math> on the four ACDC shifts.
Interestingly, in weather shifts, we empirically observe that more robust models derive greater benefits when calibrated using our synthetic data; the reverse is observed for ‘europe-india’ geographical shift.
While with real-shift data, robustness and calibration are not well correlated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>, our results suggest that a potential correlation might exist between the two factors when using synthetic data.
In <a href="#A2" title="Appendix B Calibration Details and Additional Results ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>, we provide technical details on our calibration approaches and additional results.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>On Practical Applicability</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">One significant advantage of our framework lies in its potential to address rare conditions simply through prompting.
The practical applicability of generative testing is tremendous.
Our results demonstrate promising signals; practitioners can begin assessing and ranking their pretrained models for a new, unseen target domain of interest <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">without the need for real data collection</span>.
In practice, our proposed generative benchmarking can serve as the initial step in a full validation pipeline, helping filter out non-robust prototypes and thereby saving on total operational costs.
Starting from complementing real-data validation, one can envision a future where generative techniques mature to the point of fully replacing real-data validation.
In <a href="#S3.F7" title="Figure 7 ‣ 3.3 Confidence Calibration with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>, we visualize some synthetic images and model predictions under rare conditions, such as being flooded with water, having autumn leaves scattered across the road, or having a building on fire.
We observe clear visual distinctions between weaker (Semantic-FPN and MobileNetV3) and stronger (SegF-B5) models, knowing that their Cityscapes scores do not differ significantly.
More examples are provided in <a href="#A3" title="Appendix C Qualitative Examples ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Reliability Against OOD Objects</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We now address the reliability of segmentation models in the presence of Out-of-Distribution (OOD) objects.
To begin, we explain our pipeline for inpainting random OOD objects into existing Cityscapes images.
Following that, we demonstrate how one can utilize inpainted images for OOD detection assessment and for enhancing OOD detection.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Inpainting Anomaly Objects</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We inpaint random objects into Cityscapes images.
To this end, we initially sample a location —- a square box to which we inpaint the new object.
We crop the box, upsample its content to match the preferred output size of the generative model, and inpaint an object guided by a text prompt.
In this step, we leverage Stable Diffusion inpainting capabilities, obtaining high-definition square images of the desired object.
This image is then resized and pasted back into the original image, creating a final high-definition synthetic image.
To ensure compositional consistency, we employ two techniques:
Firstly, we divide the cropped box into two regions by center cropping it again.
We inpaint only the inner region, leaving the outer region untouched, similar to the approach in RePaint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite>.
Secondly, after composing the final image, we address any remaining inconsistencies by applying a light noise over the entire picture and performing reverse diffusion again.
Technical details and visualizations are provided in <a href="#A1" title="Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">After inpainting, it is necessary to extract the mask corresponding to the new object.
To achieve this, we begin with a high-definition square image and apply the Grounded Segment Anything <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>, prompted with the name of the object.
This process yields a mask within the square image, which can then be repositioned in the full image.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div id="S4.F8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:198.7pt;height:133.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-116.7pt,78.5pt) scale(0.459859170966912,0.459859170966912) ;"><img src="/html/2312.09231/assets/x8.png" id="S4.F8.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="310" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.6.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.7.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Entropy Correlation.<span id="S4.F8.7.2.1" class="ltx_text ltx_font_medium"> The top-left inset reports the Pearson correlations between real-OOD entropy <span id="S4.F8.7.2.1.1" class="ltx_text ltx_font_italic">vs</span>.<span id="S4.F8.7.2.1.2" class="ltx_text"></span> synthetic-OOD entropy, computed either on ‘curated‘ (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) or all synthetic inpainted images (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>). Evaluations are performed on the same model set used in <a href="#S3.F3" title="Figure 3 ‣ 3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, with similar markers.</span></span></figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2312.09231/assets/x9.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.12.4.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Correlation in OOD Detection.<span id="S4.F9.6.3.3" class="ltx_text ltx_font_medium"> Each subplot scatters computed anomaly scores of segmenters on real OODs (y-axis) and on synthetic OODs (x-axis). The top row shows the four anomaly metrics utilized: FPR<sub id="S4.F9.6.3.3.1" class="ltx_sub"><span id="S4.F9.6.3.3.1.1" class="ltx_text ltx_font_italic">95</span></sub>, AUROC, AUPR<sub id="S4.F9.6.3.3.2" class="ltx_sub"><span id="S4.F9.6.3.3.2.1" class="ltx_text ltx_font_italic">IN</span></sub>, and AUPR<sub id="S4.F9.6.3.3.3" class="ltx_sub"><span id="S4.F9.6.3.3.3.1" class="ltx_text ltx_font_italic">OUT</span></sub>. The results are organized into two rows corresponding to two different confidence measures (i) Entropy and (ii) MaxLogit.
In the top-left corner of each subplot, an inset plots Pearson correlations to real OOD for ‘curated‘ (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) and ‘all‘ (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) synthetic sets.
Evaluations are performed on the same model set used in <a href="#S3.F3" title="Figure 3 ‣ 3.1 Generating images in arbitrary domains ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>, with similar markers. </span></span></figcaption>
</figure>
<figure id="S4.F10" class="ltx_figure">
<table id="S4.F10.16" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.F10.1.1" class="ltx_tr">
<td id="S4.F10.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="S4.F10.1.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="S4.F10.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;" colspan="4">
<span id="S4.F10.1.1.1.1" class="ltx_text" style="font-size:90%;">
 </span><svg id="S4.F10.1.1.1.pic1" class="ltx_picture" height="19.37" overflow="visible" version="1.1" width="460.15"><g transform="translate(0,19.37) matrix(1 0 0 -1 0 0) translate(269.45,0) translate(0,9.69)" fill="#000000" stroke="#000000" stroke-width="2.5pt"><path d="M -267.72 0 L 176.15 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 176.15 0)"><path d="M 12.83 0 C 9.03 0.71 2.85 2.85 -1.43 5.34 L -1.43 -5.34 C 2.85 -2.85 9.03 -0.71 12.83 0" style="stroke:none"></path></g><g fill="#FFFFFF"><path d="M -150.81 -9.69 h 222.88 v 19.37 h -222.88 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -146.66 -3.11)" fill="#000000" stroke="#000000"><foreignObject width="214.58" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S4.F10.1.1.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Better Models in AUROC on Synthetic</span></foreignObject></g></g></svg>
</td>
</tr>
<tr id="S4.F10.16.17" class="ltx_tr">
<td id="S4.F10.16.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="S4.F10.16.17.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.16.17.2.1" class="ltx_text" style="font-size:90%;">Images</span></td>
<td id="S4.F10.16.17.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.16.17.3.1" class="ltx_text" style="font-size:90%;">Semantic-FPN</span></td>
<td id="S4.F10.16.17.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.16.17.4.1" class="ltx_text" style="font-size:90%;">MobileNetV3</span></td>
<td id="S4.F10.16.17.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.16.17.5.1" class="ltx_text" style="font-size:90%;">SegFormer-B5</span></td>
<td id="S4.F10.16.17.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.16.17.6.1" class="ltx_text" style="font-size:90%;">SETR-PUP</span></td>
</tr>
<tr id="S4.F10.6.6" class="ltx_tr">
<td id="S4.F10.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S4.F10.6.6.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:6.1pt;height:38.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.8pt;transform:translate(-16.3pt,-16.3pt) rotate(-90deg) ;">
<p id="S4.F10.6.6.6.1.1" class="ltx_p"><span id="S4.F10.6.6.6.1.1.1" class="ltx_text" style="font-size:90%;color:#FF7F0E;">SMIYC</span></p>
</span></div>
</td>
<td id="S4.F10.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.2.2.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:90%;border-color: #FF7F0E;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/real_ood.png" id="S4.F10.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></span></td>
<td id="S4.F10.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/real-SemFPN-R101-ent.png" id="S4.F10.3.3.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/real-MobileNetV3-ent.png" id="S4.F10.4.4.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.5.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/real-SegFormerB5v3-ent.png" id="S4.F10.5.5.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/real-SETR-PUP-ent.png" id="S4.F10.6.6.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="S4.F10.11.11" class="ltx_tr">
<td id="S4.F10.11.11.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S4.F10.11.11.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:44.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.5pt;transform:translate(-18.25pt,-17.38pt) rotate(-90deg) ;">
<p id="S4.F10.11.11.6.1.1" class="ltx_p"><span id="S4.F10.11.11.6.1.1.1" class="ltx_text" style="font-size:90%;color:#1F77B4;">Synthetic</span></p>
</span></div>
</td>
<td id="S4.F10.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.7.7.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:90%;border-color: #1F77B4;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen1_ood.png" id="S4.F10.7.7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></span></td>
<td id="S4.F10.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen1-SemFPN-R101-ent.png" id="S4.F10.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen1-MobileNetV3-ent.png" id="S4.F10.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen1-SegFormerB5v3-ent.png" id="S4.F10.10.10.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen1-SETR-PUP-ent.png" id="S4.F10.11.11.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="S4.F10.16.16" class="ltx_tr">
<td id="S4.F10.16.16.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="S4.F10.16.16.6.1" class="ltx_inline-block ltx_transformed_outer" style="width:8.0pt;height:44.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.5pt;transform:translate(-18.25pt,-17.38pt) rotate(-90deg) ;">
<p id="S4.F10.16.16.6.1.1" class="ltx_p"><span id="S4.F10.16.16.6.1.1.1" class="ltx_text" style="font-size:90%;color:#1F77B4;">Synthetic</span></p>
</span></div>
</td>
<td id="S4.F10.12.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="S4.F10.12.12.1.1" class="ltx_text ltx_framed ltx_framed_rectangle" style="font-size:90%;border-color: #1F77B4;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen2_ood.png" id="S4.F10.12.12.1.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></span></td>
<td id="S4.F10.13.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen2-SemFPN-R101-ent.png" id="S4.F10.13.13.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.14.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen2-MobileNetV3-ent.png" id="S4.F10.14.14.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.15.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen2-SegFormerB5v3-ent.png" id="S4.F10.15.15.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="S4.F10.16.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/qualitative/sec4_qual/gen2-SETR-PUP-ent.png" id="S4.F10.16.16.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span id="S4.F10.21.1" class="ltx_text ltx_font_bold">Qualitative results.</span> Confidence maps are visualized for the four exemplified models on real data (first row) and synthetic inpainted data (second and third rows). Hotter colors correspond to higher OOD likelihood. Ideally, results should exhibit hot colors in OOD areas and cold colors everywhere else. We observe a strong correlation in model reactions to real and synthetic OOD regions, particularly for more recent and robust models.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Our end-to-end generation pipeline is fully automatic.
Through qualitative assessment, we achieve a satisfactory success rate in terms of generation realism; some inpainted images are illustrated in <a href="#S4.F10" title="Figure 10 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> and much more in <a href="#A3" title="Appendix C Qualitative Examples ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>.
However, this still leaves a few generations with artifacts, characterized by either unusual compositions or unrealistic details.
<span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_italic">We here question the criticality of realism in assessing OOD detection and, furthermore, in improving OOD detection.</span>
To this end, we construct two different sets: (i) all 23,040 images generated automatically and (ii) 656 curated images where we manually select the best images in terms of visual quality and realism.
We note that the manual selection process for the curated set is not exhaustive and is constrained by our allocated resources; there are many more high-quality images in the ‘all’ set.
In what follows, we present results using both curated and uncurated sets.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.3.3" class="ltx_tr">
<td id="S4.T1.3.3.4" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.3.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Method</span></td>
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">AUROC</span><span id="S4.T1.1.1.1.2" class="ltx_text" style="font-size:90%;"> (</span><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\uparrow</annotation></semantics></math><span id="S4.T1.1.1.1.3" class="ltx_text" style="font-size:90%;">)</span>
</td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">AUPR</span><span id="S4.T1.2.2.2.2" class="ltx_text" style="font-size:90%;"> (</span><math id="S4.T1.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T1.2.2.2.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.2.2.2.m1.1.1" xref="S4.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.m1.1c">\uparrow</annotation></semantics></math><span id="S4.T1.2.2.2.3" class="ltx_text" style="font-size:90%;">)</span>
</td>
<td id="S4.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S4.T1.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FPR95</span><span id="S4.T1.3.3.3.2" class="ltx_text" style="font-size:90%;"> (</span><math id="S4.T1.3.3.3.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T1.3.3.3.m1.1a"><mo mathsize="90%" stretchy="false" id="S4.T1.3.3.3.m1.1.1" xref="S4.T1.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.m1.1c">\downarrow</annotation></semantics></math><span id="S4.T1.3.3.3.3" class="ltx_text" style="font-size:90%;">)</span>
</td>
</tr>
<tr id="S4.T1.3.4" class="ltx_tr">
<td id="S4.T1.3.4.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.3.4.1.1" class="ltx_text" style="font-size:90%;">RbA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.T1.3.4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T1.3.4.1.4" class="ltx_text" style="font-size:90%;"> Swin-B</span>
</td>
<td id="S4.T1.3.4.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.4.2.1" class="ltx_text" style="font-size:90%;">95.6</span></td>
<td id="S4.T1.3.4.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.4.3.1" class="ltx_text" style="font-size:90%;">78.4</span></td>
<td id="S4.T1.3.4.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.4.4.1" class="ltx_text" style="font-size:90%;">11.8</span></td>
</tr>
<tr id="S4.T1.3.5" class="ltx_tr">
<td id="S4.T1.3.5.1" class="ltx_td ltx_align_left">
<span id="S4.T1.3.5.1.1" class="ltx_text" style="font-size:90%;">+ COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.T1.3.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.3.5.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">97.8</span></td>
<td id="S4.T1.3.5.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">85.3</span></td>
<td id="S4.T1.3.5.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.5.4.1" class="ltx_text" style="font-size:90%;">8.5</span></td>
</tr>
<tr id="S4.T1.3.6" class="ltx_tr">
<td id="S4.T1.3.6.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.6.1.1" class="ltx_text" style="font-size:90%;">+ Ours (curated)</span></td>
<td id="S4.T1.3.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.2.1" class="ltx_text" style="font-size:90%;">97.2</span></td>
<td id="S4.T1.3.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.3.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">84.9</span></td>
<td id="S4.T1.3.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.6.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">8.1</span></td>
</tr>
<tr id="S4.T1.3.7" class="ltx_tr">
<td id="S4.T1.3.7.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.7.1.1" class="ltx_text" style="font-size:90%;">+ Ours (all)</span></td>
<td id="S4.T1.3.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.2.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">97.3</span></td>
<td id="S4.T1.3.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.3.1" class="ltx_text" style="font-size:90%;">84.8</span></td>
<td id="S4.T1.3.7.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.7.4.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">8.2</span></td>
</tr>
<tr id="S4.T1.3.8" class="ltx_tr">
<td id="S4.T1.3.8.1" class="ltx_td ltx_align_left ltx_border_t">
<span id="S4.T1.3.8.1.1" class="ltx_text" style="font-size:90%;">RbA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.8.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.T1.3.8.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.T1.3.8.1.4" class="ltx_text" style="font-size:90%;"> Swin-L</span>
</td>
<td id="S4.T1.3.8.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.8.2.1" class="ltx_text" style="font-size:90%;">96.4</span></td>
<td id="S4.T1.3.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.8.3.1" class="ltx_text" style="font-size:90%;">79.6</span></td>
<td id="S4.T1.3.8.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.3.8.4.1" class="ltx_text" style="font-size:90%;">15.0</span></td>
</tr>
<tr id="S4.T1.3.9" class="ltx_tr">
<td id="S4.T1.3.9.1" class="ltx_td ltx_align_left">
<span id="S4.T1.3.9.1.1" class="ltx_text" style="font-size:90%;">+ COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.3.9.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a><span id="S4.T1.3.9.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
<td id="S4.T1.3.9.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.9.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">98.2</span></td>
<td id="S4.T1.3.9.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.9.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">88.7</span></td>
<td id="S4.T1.3.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.9.4.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">8.2</span></td>
</tr>
<tr id="S4.T1.3.10" class="ltx_tr">
<td id="S4.T1.3.10.1" class="ltx_td ltx_align_left"><span id="S4.T1.3.10.1.1" class="ltx_text" style="font-size:90%;">+ Ours (curated)</span></td>
<td id="S4.T1.3.10.2" class="ltx_td ltx_align_center"><span id="S4.T1.3.10.2.1" class="ltx_text" style="font-size:90%;">97.2</span></td>
<td id="S4.T1.3.10.3" class="ltx_td ltx_align_center"><span id="S4.T1.3.10.3.1" class="ltx_text" style="font-size:90%;">88.0</span></td>
<td id="S4.T1.3.10.4" class="ltx_td ltx_align_center"><span id="S4.T1.3.10.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">7.9</span></td>
</tr>
<tr id="S4.T1.3.11" class="ltx_tr">
<td id="S4.T1.3.11.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="S4.T1.3.11.1.1" class="ltx_text" style="font-size:90%;">+ Ours (all)</span></td>
<td id="S4.T1.3.11.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.11.2.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">98.1</span></td>
<td id="S4.T1.3.11.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.11.3.1" class="ltx_text ltx_ulem_uline" style="font-size:90%;">88.6</span></td>
<td id="S4.T1.3.11.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.3.11.4.1" class="ltx_text" style="font-size:90%;">8.3</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.8.1" class="ltx_text ltx_font_bold">Improving OOD detection</span> on real SMIYC benchmarking using our synthetic data. All results are obtained using the published code and default parameters.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Assess OOD Detection</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.5" class="ltx_p"><span id="S4.SS2.p1.5.1" class="ltx_text ltx_font_bold">Experimental setup.</span> To measure how the segmenters react to unseen OOD objects, we use standard anomaly detection metrics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, that are False Positive Rate at 95% true positives (<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\text{FPR}_{\text{95}}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mtext id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2a.cmml">FPR</mtext><mtext id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3a.cmml">95</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2a.cmml" xref="S4.SS2.p1.1.m1.1.1.2"><mtext id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">FPR</mtext></ci><ci id="S4.SS2.p1.1.m1.1.1.3a.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">95</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\text{FPR}_{\text{95}}</annotation></semantics></math>), Area Under ROC curve (<span id="S4.SS2.p1.5.2" class="ltx_text ltx_markedasmath">AUROC</span>), and Area Under Precision-Recall curve (<span id="S4.SS2.p1.5.3" class="ltx_text ltx_markedasmath">AUPR</span>). AUPR are declined into <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\text{AUPR}_{\text{IN}}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><msub id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mtext id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2a.cmml">AUPR</mtext><mtext id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3a.cmml">IN</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p1.4.m4.1.1.2a.cmml" xref="S4.SS2.p1.4.m4.1.1.2"><mtext id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">AUPR</mtext></ci><ci id="S4.SS2.p1.4.m4.1.1.3a.cmml" xref="S4.SS2.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">IN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\text{AUPR}_{\text{IN}}</annotation></semantics></math> and <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="\text{AUPR}_{\text{OUT}}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><msub id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mtext id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2a.cmml">AUPR</mtext><mtext id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3a.cmml">OUT</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2a.cmml" xref="S4.SS2.p1.5.m5.1.1.2"><mtext id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">AUPR</mtext></ci><ci id="S4.SS2.p1.5.m5.1.1.3a.cmml" xref="S4.SS2.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">OUT</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">\text{AUPR}_{\text{OUT}}</annotation></semantics></math>, that considers the in-distribution regions, respectively the out-of-distribution regions (the inpainted object), as positive regions to compute the Precision-Recall curves.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">All segmenters in our study are not designed to produce confidence scores.
We thus seek for various techniques to derive confidence scores from pretrained models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> and eventually narrow down the options to two measures: (i) Entropy of soft-probability predictions, and (ii) MaxLogit as the maximum logit value (before softmax) among the classes.
While Entropy is the traditional measure of uncertainty, MaxLogit is a recent and surprising finding that has been proven to be more effective in estimating OOD confidence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Results.</span>
For quantitative comparison, we leverage the SegmentMeIfYouCan (SMIYC) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>]</cite>, a recent dataset for OOD detection.
We resort to the RoadAnomaly21 split in SMIYC, due to similarity in object scales to our synthetic data.
We analyze the correlation between the OOD scores obtained on RoadAnomaly21 and one using our synthetic inpainted data.
 <a href="#S4.F8" title="Figure 8 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> reports our first analysis on the entropy in the OOD areas, either real (RoadAnomaly21) or generated.
For each model, we compute the Pearson Correlation (PCC) between real-OOD entropy and synthetic-OOD entropy; the computation is done on both ‘curated‘ and ‘all‘ sets.
In terms of entropy, we observe a very high correlation between real- and synthetic-OOD, reaching <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="0.94" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">0.94</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="float" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">0.94</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">0.94</annotation></semantics></math> PCC using both ‘curated‘ and ‘all‘ sets.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">We then analyze the correlation between real and synthetic anomaly metrics.
 <a href="#S4.F9" title="Figure 9 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a> presents our primary findings and <a href="#S4.F10" title="Figure 10 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> illustrates some qualitative results.
We observe a strong correlation with real scores when utilizing the ’curated’ set for computing synthetic scores; the curated PCCs (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>) are consistently around 0.8 across multiple metrics, irrespective of the two confidence measures.
Although the correlations are somewhat weaker when using all uncurated synthetic data (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>), such results remain acceptable, particularly when no effort is dedicated to curation.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text ltx_font_italic">Our results validate the potential of utilizing realistic synthetic data, inpainted with anomaly objects, for assessing OOD detection.
In OOD testing, it is quite important to use high-quality synthetic inpainted data. Nonetheless, even non-curated synthetic data can offer an acceptable estimation of real performance with minimal curation costs.</span></p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Improve OOD Detection</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this experiment, we investigate if synthetic inpainted data can be used to enhance a deep network’s ability to detect OOD objects.
To this end, we adopt the state-of-the-art RbA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> approach for OOD detection and train RbA models on our synthetic data.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">OOD metrics are computed on RoadAnomaly21 and reported in <a href="#S4.T1" title="Table 1 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.
The RbA models trained on our data significantly outperform the vanilla RbA model.
We reach comparable performance to the RbA variants that leverage the external COCO dataset for augmentation.
Notably, there are no clear differences between using ‘curated’ or ‘all’ sets.
<span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">We conjecture that, unlike benchmarking, training for OOD detection does not demand a high degree of realism from synthetic data.</span>
This explains why the simple strategy of copy-pasting COCO objects already proves effective.
All results are consistent across the two addressed backbones.  <a href="#S4.F10" title="Figure 10 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> illustrates a few qualitative results.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Takeaways</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">In this work, we explore the potential of synthetic data in reliability assessment for semantic segmentation networks.
Our promising results encourage further collective investigations into this research problem, paving the way for synthetic system validation, especially in safety-critical applications.
We summarize here our primary findings:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.i1.1.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="S5.I1.i1.1.1.m1.1b"><mo id="S5.I1.i1.1.1.m1.1.1" xref="S5.I1.i1.1.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="S5.I1.i1.1.1.m1.1c"><ci id="S5.I1.i1.1.1.m1.1.1.cmml" xref="S5.I1.i1.1.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.1.1.m1.1d">\triangleright</annotation></semantics></math></span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p"><span id="S5.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Reliability Under Covariate Shifts:</span> synthetic data can help assess the relative robustness of models in real-life covariate shifts, especially useful when shifts to the training condition are significant. Synthetic data can well complement real data in system validation, helping reduce the total operational cost. Pretrained models can be calibrated using synthetic data to better estimate prediction confidence in any shifted domains.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math id="S5.I1.i2.1.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="S5.I1.i2.1.1.m1.1b"><mo id="S5.I1.i2.1.1.m1.1.1" xref="S5.I1.i2.1.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="S5.I1.i2.1.1.m1.1c"><ci id="S5.I1.i2.1.1.m1.1.1.cmml" xref="S5.I1.i2.1.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.1.1.m1.1d">\triangleright</annotation></semantics></math></span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p"><span id="S5.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Reliability Against OOD Objects:</span> synthetic data is useful in both OOD testing and OOD training; however, the demands on synthetic data quality differ in these two cases. In OOD testing, the best result estimations are obtained with the most realistically inpainted data, which may require a certain amount of curation time for qualitative assessment. The curation task is not time-demanding and can be done quickly with a reasonable budget. On the other hand, for OOD training, no curation is actually needed to achieve improvements.</p>
</div>
</li>
</ul>
<p id="S5.p1.2" class="ltx_p"><span id="S5.p1.2.1" class="ltx_text ltx_font_bold">Limitations.</span> We focus our research on the task of semantic segmentation while keeping open the extension possibility to other critical tasks, such as object detection.
Our quantitative assessments are confined to existing publicly available datasets. However, our framework is fully zero-shot and can be applied to any domain of interest.
On the generative side, our study is restricted to Stable Diffusion and ControlNet due to our resource constraints.
Of note, although improvements in this area should enhance the results, similar insights are expected to be achieved.
<br class="ltx_break">
<br class="ltx_break"><span id="S5.p1.2.2" class="ltx_text ltx_font_bold">Acknowledgements.</span> This work is supported by ELSA - European Lighthouse on Secure and Safe AI funded by the European Union under grant agreement No. 101070617. We thank the authors of RELIS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> for providing us their pretrained checkpoints.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.4.4.1" class="ltx_text" style="font-size:90%;">cli [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.6.1" class="ltx_text" style="font-size:90%;">
clip-interrogator.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/pharmapsychotic/clip-interrogator" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/pharmapsychotic/clip-interrogator</a><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Besnier et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Victor Besnier, Himalaya Jain, Andrei Bursuc, Matthieu Cord, and Patrick
Pérez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">This dataset does not exist: training models from generated images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICASSP</span><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Blum et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Hermann Blum, Paul-Edouard Sarlin, Juan Nieto, Roland Siegwart, and Cesar
Cadena.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">The fishyscapes benchmark: Measuring blind spots in semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">2021.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Chan et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Robin Chan, Krzysztof Lis, Svenja Uhlemeyer, Hermann Blum, Sina Honari, Roland
Siegwart, Pascal Fua, Mathieu Salzmann, and Matthias Rottmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Segmentmeifyoucan: A benchmark for anomaly segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">de Jorge et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Pau de Jorge, Riccardo Volpi, Philip HS Torr, and Grégory Rogez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Reliability in semantic segmentation: Are we on the right track?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Geirhos et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer,
Matthias Bethge, Felix A Wichmann, and Wieland Brendel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">Partial success in closing the gap between human and machine vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib6.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Guo et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">On calibration of modern neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Ruifei He, Shuyang Sun, Xin Yu, Chuhui Xue, Wenqing Zhang, Philip Torr, Song
Bai, and Xiaojuan Qi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Is synthetic data from generative models ready for image recognition?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.9.1" class="ltx_text" style="font-size:90%;">2023.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.4.4.1" class="ltx_text" style="font-size:90%;">Hendrycks and Dietterich [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.6.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks and Thomas Dietterich.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">Benchmarking neural network robustness to common corruptions and
perturbations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.9.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib9.10.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.4.4.1" class="ltx_text" style="font-size:90%;">Hendrycks and Gimpel [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.6.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks and Kevin Gimpel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">A baseline for detecting misclassified and out-of-distribution
examples in neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.9.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib10.10.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">The many faces of robustness: A critical analysis of
out-of-distribution generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Hendrycks et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Dan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza
Mostajabi, Jacob Steinhardt, and Dawn Song.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Scaling out-of-distribution detection for real-world settings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICML</span><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Kirillov et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr
Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Koh et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang,
Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,
Irena Gao, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Wilds: A benchmark of in-the-wild distribution shifts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Le Moing et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Guillaume Le Moing, Tuan-Hung Vu, Himalaya Jain, Patrick Pérez, and
Matthieu Cord.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">Semantic palette: Guiding scene generation with class proportions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib15.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Daiqing Li, Huan Ling, Seung Wook Kim, Karsten Kreis, Sanja Fidler, and Antonio
Torralba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Imagenet-e: Benchmarking neural network robustness via attribute
editing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan
Li, Jianwei Yang, Hang Su, Jun Zhu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set
object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Lugmayr et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Andreas Lugmayr, Martin Danelljan, Andrés Romero, Fisher Yu, Radu
Timofte, and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Repaint: Inpainting using denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Marathe et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Aboli Marathe, Deva Ramanan, Rahee Walambe, and Ketan Kotecha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Wedge: A multi-weather autonomous driving dataset built from
generative vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPRW</span><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Miller et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh,
Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Accuracy on the line: on the strong correlation between
out-of-distribution and in-distribution generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Nayal et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Nazir Nayal, Misra Yavuz, Joao F Henriques, and Fatma Güney.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Rba: Segmenting unknown regions rejected by all.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Ovadia et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Can you trust your model’s uncertainty? evaluating predictive
uncertainty under dataset shift.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Pinggera et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Peter Pinggera, Sebastian Ramos, Stefan Gehrig, Uwe Franke, Carsten Rother, and
Rudolf Mester.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Lost and found: detecting small road hazards for self-driving
vehicles.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IROS</span><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Prabhu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Viraj Prabhu, Sriram Yenamandra, Prithvijit Chattopadhyay, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Lance: Stress-testing visual models by generating language-guided
counterfactual images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Recht et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Do imagenet classifiers generalize to imagenet?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICLR</span><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Rombach et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
Ommer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Sakaridis et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Christos Sakaridis, Dengxin Dai, and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Acdc: The adverse conditions dataset with correspondences for
semantic driving scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Sariyildiz et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and Yannis Kalantidis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Fake it till you make it: Learning transferable representations from
synthetic imagenet clones.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Singh et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Aaditya Singh, Kartik Sarangmath, Prithvijit Chattopadhyay, and Judy Hoffman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">Benchmarking low-shot robustness to natural distribution shifts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Taori et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
and Ludwig Schmidt.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">Measuring robustness to natural distribution shifts in image
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Teney et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Damien Teney, Yong Lin, Seong Joon Oh, and Ehsan Abbasnejad.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Id and ood performance are sometimes inversely correlated on
real-world datasets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Tran et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Dustin Tran, Jeremiah Liu, Michael W Dusenberry, Du Phan, Mark Collier, Jie
Ren, Kehang Han, Zi Wang, Zelda Mariet, Huiyi Hu, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">Plex: Towards reliability using pretrained large model extensions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv</span><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Varma et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Girish Varma, Anbumani Subramanian, Anoop Namboodiri, Manmohan Chandraker, and
CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">Idd: A dataset for exploring problems of autonomous navigation in
unconstrained environments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">WACV</span><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Wu et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Weijia Wu, Yuzhong Zhao, Mike Zheng Shou, Hong Zhou, and Chunhua Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Diffumask: Synthesizing images with pixel-level annotations for
semantic segmentation using diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Yang et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan
Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Openood: Benchmarking generalized out-of-distribution detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text ltx_font_italic" style="font-size:90%;">NeurIPS</span><span id="bib.bib36.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Yuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois Lafleche, Adela
Barriuso, Antonio Torralba, and Sanja Fidler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">Datasetgan: Efficient labeled data factory with minimal human effort.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.10.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_centering ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>

<div id="Ax1.p1" class="ltx_para">
<p id="Ax1.p1.1" class="ltx_p">In this appendix, we provide technical details for covariate shifts generation and OOD object inpainting (<a href="#A1" title="Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>), additional calibration details and results (<a href="#A2" title="Appendix B Calibration Details and Additional Results ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>), and more qualitative examples (<a href="#A3" title="Appendix C Qualitative Examples ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Appendix C</span></a>).</p>
</div>
</section>
<section id="A1" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Technical Details</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Covariate Shifts Training</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.2" class="ltx_p">We train a ControlNet on top of a frozen Stable Diffusion 1.5 for 2100 steps. The ControlNet used here is a trainable copy of the Stable Diffusion encoder only, as in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>. We use a batch size of 8 with 32 gradient accumulation steps, which makes an effective batch size of 256, and a learning rate of <math id="A1.SS1.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="A1.SS1.p1.1.m1.1a"><msup id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml"><mn id="A1.SS1.p1.1.m1.1.1.2" xref="A1.SS1.p1.1.m1.1.1.2.cmml">10</mn><mrow id="A1.SS1.p1.1.m1.1.1.3" xref="A1.SS1.p1.1.m1.1.1.3.cmml"><mo id="A1.SS1.p1.1.m1.1.1.3a" xref="A1.SS1.p1.1.m1.1.1.3.cmml">−</mo><mn id="A1.SS1.p1.1.m1.1.1.3.2" xref="A1.SS1.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><apply id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="A1.SS1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.p1.1.m1.1.1.2">10</cn><apply id="A1.SS1.p1.1.m1.1.1.3.cmml" xref="A1.SS1.p1.1.m1.1.1.3"><minus id="A1.SS1.p1.1.m1.1.1.3.1.cmml" xref="A1.SS1.p1.1.m1.1.1.3"></minus><cn type="integer" id="A1.SS1.p1.1.m1.1.1.3.2.cmml" xref="A1.SS1.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">10^{-5}</annotation></semantics></math>.
We use the training set of Cityscapes, and do a random horizontal crop of the images to get square images, and then resize them to 512 <math id="A1.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.SS1.p1.2.m2.1a"><mo id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><times id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">\times</annotation></semantics></math> 512, convenient of Stable Diffusion 1.5.
All other training hyperparameters are the per default settings on the official ControlNet repository.
The objective is to reconstruct the original images of Cityscapes using its semantic masks as input to the ControlNet, and the captions extracted with CLIP-interrogator as input to Stable Diffusion.</p>
</div>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Covariate Shifts Generation</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.2" class="ltx_p">To generate images with new styles, we take a semantic mask from the validation set of Cityscapes, crop and resize it as explained in the previous part.
We use nearest neighbor interpolation to keep good values for specific classes. We only use the part of the caption extracted with CLIP-interrogator which corresponds to a BLIP caption.
To this new caption, we add <span id="A1.SS2.p1.2.2" class="ltx_text ltx_font_typewriter">[, in <math id="A1.SS2.p1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="A1.SS2.p1.1.1.m1.1a"><mo id="A1.SS2.p1.1.1.m1.1.1" xref="A1.SS2.p1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.1.m1.1b"><lt id="A1.SS2.p1.1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.1.m1.1c">&lt;</annotation></semantics></math>domain<math id="A1.SS2.p1.2.2.m2.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="A1.SS2.p1.2.2.m2.1a"><mo id="A1.SS2.p1.2.2.m2.1.1" xref="A1.SS2.p1.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.2.2.m2.1b"><gt id="A1.SS2.p1.2.2.m2.1.1.cmml" xref="A1.SS2.p1.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.2.2.m2.1c">&gt;</annotation></semantics></math>]</span> depending on the domain we want to generalize to. Starting from pure noise, we use 25 DDIM steps with a guidance scale of 8. On a RTX 2080, one new image is generated in about 4 seconds.
All other sampling hyperparameters are the per default settings on the official ControlNet repository.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>OOD Objects Generation</h3>

<figure id="A1.F11" class="ltx_figure"><img src="/html/2312.09231/assets/x10.png" id="A1.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.3.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="A1.F11.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">OOD object data generation pipeline.<span id="A1.F11.4.2.1" class="ltx_text ltx_font_medium">
We use pretrained Stable Diffusion for inpainting and refining steps, and pretrained Grounded Segment Anything for mask extraction.</span></span></figcaption>
</figure>
<figure id="A1.F12" class="ltx_figure"><img src="/html/2312.09231/assets/x11.png" id="A1.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="425" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A1.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="A1.F12.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Refinement.<span id="A1.F12.4.2.1" class="ltx_text ltx_font_medium">
This example highlights the importance of the refinement step. The left image shows the state before refinement, whereas the right image displays the refined version. Upon zooming into the edge of the inpainting box, a clear distinction between the left and right is evident in the first image. Such difference is eliminated in the second image. Must be viewed in color.
</span></span></figcaption>
</figure>
<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">The OOD object generation pipeline described in <a href="#S4.SS1" title="4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 4.1</span></a>
is further illustrated in <a href="#A1.F11" title="Figure 11 ‣ A.3 OOD Objects Generation ‣ Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>.
It is composed of three steps.
Given a text prompt containing an object, the inpainting step generates a zoomed-in version of the object with the appropriate close-range background given as context.
The mask extraction step infers the anomaly mask from the zoomed-in generated image and the name of the object.
Both are in-pasted back in the original complete image or mask.
To reduce some composition artifacts, the composite image is refined with a noise/denoise step.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.4" class="ltx_p">In details, we first randomly choose a box size for the new object, uniformly sampled between a quarter and half the minimum dimension of the original image.
We also uniformly sample a location for the box in the bottom three quarters of the image.
This box will contain the new object we wish to add, and we refer to it as <span id="A1.SS3.p2.4.3" class="ltx_text ltx_font_italic">inpainted region</span>.
In addition to the inpainted region, we create a larger box, 1.5<math id="A1.SS3.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.SS3.p2.1.m1.1a"><mo id="A1.SS3.p2.1.m1.1.1" xref="A1.SS3.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.1.m1.1b"><times id="A1.SS3.p2.1.m1.1.1.cmml" xref="A1.SS3.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.1.m1.1c">\times</annotation></semantics></math> its height and width, with the inpainted region in its center.
The contour outside of the inpainted region will serve as <span id="A1.SS3.p2.4.4" class="ltx_text ltx_font_italic">context</span> for the inpainting process.
We then crop and resize the inpainted region with its context to a resolution of 512 <math id="A1.SS3.p2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.SS3.p2.2.m2.1a"><mo id="A1.SS3.p2.2.m2.1.1" xref="A1.SS3.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.2.m2.1b"><times id="A1.SS3.p2.2.m2.1.1.cmml" xref="A1.SS3.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.2.m2.1c">\times</annotation></semantics></math> 512.
We fully noise the inpainted region, but leave the clean context.
We then denoise the inpainted zone with the prompt <span id="A1.SS3.p2.4.2" class="ltx_text ltx_font_italic">“A photo of an <math id="A1.SS3.p2.3.1.m1.1" class="ltx_Math" alttext="[" display="inline"><semantics id="A1.SS3.p2.3.1.m1.1a"><mo stretchy="false" id="A1.SS3.p2.3.1.m1.1.1" xref="A1.SS3.p2.3.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.3.1.m1.1b"><ci id="A1.SS3.p2.3.1.m1.1.1.cmml" xref="A1.SS3.p2.3.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.3.1.m1.1c">[</annotation></semantics></math>object<math id="A1.SS3.p2.4.2.m2.1" class="ltx_Math" alttext="]" display="inline"><semantics id="A1.SS3.p2.4.2.m2.1a"><mo stretchy="false" id="A1.SS3.p2.4.2.m2.1.1" xref="A1.SS3.p2.4.2.m2.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.4.2.m2.1b"><ci id="A1.SS3.p2.4.2.m2.1.1.cmml" xref="A1.SS3.p2.4.2.m2.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.4.2.m2.1c">]</annotation></semantics></math>”</span>, with a guidance scale of 15.
The full patch is then resized and pasted on the original image, at its original position.
Some artifact might be still present as shown in <a href="#A1.F12" title="Figure 12 ‣ A.3 OOD Objects Generation ‣ Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>.
To remedy this, we refine the inpainted zone by noising and denoising it with 0.65 strength, with the default guidance scale of 7.5.
The effect of this refining step is shown in <a href="#A1.F12" title="Figure 12 ‣ A.3 OOD Objects Generation ‣ Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 12</span></a>.</p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p id="A1.SS3.p3.1" class="ltx_p">We list here all 42 objects used in our experiments, which are not present in Cityscapes’ classes: arcade machine,
armchair,
baby,
bag,
bathtub,
bench,
billboard,
book,
bottle,
box,
chair,
cheetah,
chimpanzee,
clock,
computer,
desk,
dolphin,
elephant,
flamingo,
giraffe,
gorilla,
graffiti,
hippopotamus,
kangaroo,
koala,
lamp,
lion,
microwave,
mirror,
panda,
penguin,
pillow,
plate,
polar bear,
radiator,
refrigerator,
sofa,
table,
tiger,
toilet,
vase,
and zebra.
</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>OOD Detection Training</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">For the OOD detection method in <a href="#S4.SS3" title="4.3 Improve OOD Detection ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section 4.3</span></a>, we used the codebase of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>.
We adapt the code to be able to use our generated data with the binary masks extracted from Grounded-SAM, as explained in <a href="#A1.SS3" title="A.3 OOD Objects Generation ‣ Appendix A Technical Details ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Section A.3</span></a>.
As in the original paper <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite>, we fine-tune the mask prediction MLP and classification layer after the transformer decoder.
To obtain all results reported in <a href="#S4.T1" title="Table 1 ‣ 4.1 Inpainting Anomaly Objects ‣ 4 Reliability Against OOD Objects ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a>, we used the recommended hyperparameters, and train the models for 5000 iterations.</p>
</div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Segmentation Models</h3>

<div id="A1.SS5.p1" class="ltx_para">
<p id="A1.SS5.p1.1" class="ltx_p">We list here all models used in our experiments: ANN-R101,
ANN-R50,
APCNet-R101,
APCNet-R50,
BiSeNetV1-R50,
BiSeNetV2-FCN,
CCNet-R101,
CCNet-R50,
ConvNext,
ConvNext-B-In1K,
ConvNext-B-In21K,
DLV3+ResNet101,
DLV3+ResNet18,
DLV3+ResNet50,
GCNet-R101,
GCNet-R50,
ICNet-R101,
ICNet-R18,
ICNet-R50,
MobileNetV3,
PSPNet-R101,
PSPNet-R18,
PSPNet-R50,
SETR-MLA,
SETR-Naive,
SETR-PUP,
SegFormer-B0,
SegFormer-B1,
SegFormer-B2,
SegFormer-B3,
SegFormer-B4,
SegFormer-B5,
SegFormer-B5-v2,
SegFormer-B5-v3,
Segmenter,
SemFPN-R101,
SemFPN-R50,
UpperNet-R101,
UpperNet-R18,
UpperNet-R50.
</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Calibration Details and Additional Results</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">We elaborate on our strategy for performing per-class calibration to obtain the synthetic results (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>) presented in <a href="#S3.F6" title="Figure 6 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.
Utilizing our synthetic data, a temperature scaling (TS) scalar is learned for each class.
When calibrating models on shifted domains, we choose the corresponding TS scalar based on model predictions. In the case of calibration results with real-shift data (<span class="ltx_rule" style="width:5.0pt;height:8.6pt;background:black;display:inline-block;"> </span>), only one scalar is learned for each model.</p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p">In <a href="#A2.F13.sf1" title="13(a) ‣ Figure 13 ‣ Appendix B Calibration Details and Additional Results ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">13(a)</span></a>, we compare per-class TS <span id="A2.p2.1.1" class="ltx_text ltx_font_italic">vs.</span> standard TS with one scalar per model, both applied on our synthetic data.
Both strategies enhance calibration results, highlighting the advantage of employing synthetic data for calibration.
Per-class TS demonstrates superiority for more robust models (right part of the plots), while its performance is weaker for less robust ones (left part of the plots).</p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p">In <a href="#A2.F13.sf2" title="13(b) ‣ Figure 13 ‣ Appendix B Calibration Details and Additional Results ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">13(b)</span></a>, we compare Cityscapes <span id="A2.p3.1.1" class="ltx_text ltx_font_italic">vs.</span> our synthetic data; in this experiment we adopt the standard TS with one scalar per model.
The results obtained from Cityscapes are clearly inferior to those achieved using our synthetic data, demonstrating the limitations when relying solely on in-domain data for calibration in shifted domains.</p>
</div>
<figure id="A2.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.09231/assets/x12.png" id="A2.F13.sf1.g1" class="ltx_graphics ltx_img_landscape" width="528" height="408" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F13.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="A2.F13.sf1.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Perclass Temperature Scaling (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) <span id="A2.F13.sf1.4.2.1" class="ltx_text ltx_font_italic">vs.</span> One Temperature Scaling (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>).<span id="A2.F13.sf1.4.2.2" class="ltx_text ltx_font_medium"> The Figure has the same structure as of <a href="#S3.F6" title="Figure 6 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> and the bars show relative ECE improvements. We compare the two strategies for performing calibration using synthetic data; both enhance calibration in shifted domains.</span></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.09231/assets/x13.png" id="A2.F13.sf2.g1" class="ltx_graphics ltx_img_landscape" width="528" height="408" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A2.F13.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="A2.F13.sf2.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Cityscapes (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>) <span id="A2.F13.sf2.4.2.1" class="ltx_text ltx_font_italic">vs.</span> our synthetic data with one TS (<span class="ltx_rule" style="width:4.5pt;height:7.7pt;background:black;display:inline-block;"> </span>).<span id="A2.F13.sf2.4.2.2" class="ltx_text ltx_font_medium"> The Figure has the same structure as of <a href="#S3.F6" title="Figure 6 ‣ 3.2 Robustness Assessment with Synthetic Data ‣ 3 Reliability Under Covariate Shifts ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> and the bars show relative ECE improvements. Our synthetic data is superior to Cityscapes in improving calibration in shifted domains.</span></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="A2.F13.3.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="A2.F13.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Additional Calibration Results.</span></figcaption>
</figure>
</section>
<section id="A3" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Qualitative Examples</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">We show more qualitative examples for synthetic covariate shifts in <a href="#A3.F14" title="Figure 14 ‣ Appendix C Qualitative Examples ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 14</span></a> and synthetic OOD objects in <a href="#A3.F15" title="Figure 15 ‣ Appendix C Qualitative Examples ‣ Reliability in Semantic Segmentation: Can We Use Synthetic Data?" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 15</span></a>.</p>
</div>
<figure id="A3.F14" class="ltx_figure">
<table id="A3.F14.64" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A3.F14.64.65" class="ltx_tr">
<td id="A3.F14.64.65.1" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="A3.F14.64.65.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.2.1" class="ltx_text" style="font-size:80%;">Synthetic Images</span></td>
<td id="A3.F14.64.65.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.3.1" class="ltx_text" style="font-size:80%;">Semantic-FPN</span></td>
<td id="A3.F14.64.65.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.4.1" class="ltx_text" style="font-size:80%;">MobileNetV3</span></td>
<td id="A3.F14.64.65.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.5.1" class="ltx_text" style="font-size:80%;">SegF-B5</span></td>
<td id="A3.F14.64.65.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.6.1" class="ltx_text" style="font-size:80%;">Synthetic Images</span></td>
<td id="A3.F14.64.65.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.7.1" class="ltx_text" style="font-size:80%;">Semantic-FPN</span></td>
<td id="A3.F14.64.65.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.8.1" class="ltx_text" style="font-size:80%;">MobileNetV3</span></td>
<td id="A3.F14.64.65.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F14.64.65.9.1" class="ltx_text" style="font-size:80%;">SegF-B5</span></td>
</tr>
<tr id="A3.F14.8.8" class="ltx_tr">
<td id="A3.F14.8.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.8.8.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:44.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:44.9pt;transform:translate(-18.9pt,-18.12pt) rotate(-90deg) ;">
<p id="A3.F14.8.8.9.1.1" class="ltx_p"><span id="A3.F14.8.8.9.1.1.1" class="ltx_text" style="font-size:80%;">Sun glare</span></p>
</span></div>
</td>
<td id="A3.F14.1.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_1.jpg" id="A3.F14.1.1.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_1_pred_SemFPN-R101.png" id="A3.F14.2.2.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.3.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_1_pred_MobileNetV3.png" id="A3.F14.3.3.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.4.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_1_pred_SegFormer-B5.png" id="A3.F14.4.4.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.5.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_4.jpg" id="A3.F14.5.5.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.6.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_4_pred_SemFPN-R101.png" id="A3.F14.6.6.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.7.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_4_pred_MobileNetV3.png" id="A3.F14.7.7.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.8.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/sun_4_pred_SegFormer-B5.png" id="A3.F14.8.8.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.16.16" class="ltx_tr">
<td id="A3.F14.16.16.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.16.16.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.6pt;height:37.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.7pt;transform:translate(-16.06pt,-16.06pt) rotate(-90deg) ;">
<p id="A3.F14.16.16.9.1.1" class="ltx_p"><span id="A3.F14.16.16.9.1.1.1" class="ltx_text" style="font-size:80%;">Flood</span></p>
</span></div>
</td>
<td id="A3.F14.9.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_1.jpg" id="A3.F14.9.9.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.10.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_1_pred_SemFPN-R101.png" id="A3.F14.10.10.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.11.11.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_1_pred_MobileNetV3.png" id="A3.F14.11.11.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.12.12.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_1_pred_SegFormer-B5.png" id="A3.F14.12.12.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.13.13.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_5.jpg" id="A3.F14.13.13.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.14.14.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_5_pred_SemFPN-R101.png" id="A3.F14.14.14.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.15.15.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_5_pred_MobileNetV3.png" id="A3.F14.15.15.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.16.16.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/flood_5_pred_SegFormer-B5.png" id="A3.F14.16.16.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.24.24" class="ltx_tr">
<td id="A3.F14.24.24.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.24.24.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:45.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:45.1pt;transform:translate(-19.82pt,-19.82pt) rotate(-90deg) ;">
<p id="A3.F14.24.24.9.1.1" class="ltx_p"><span id="A3.F14.24.24.9.1.1.1" class="ltx_text" style="font-size:80%;">Autumn</span></p>
</span></div>
</td>
<td id="A3.F14.17.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_1.jpg" id="A3.F14.17.17.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.18.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_1_pred_SemFPN-R101.png" id="A3.F14.18.18.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.19.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_1_pred_MobileNetV3.png" id="A3.F14.19.19.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.20.20.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_1_pred_SegFormer-B5.png" id="A3.F14.20.20.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.21.21.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_8.jpg" id="A3.F14.21.21.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.22.22.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_8_pred_SemFPN-R101.png" id="A3.F14.22.22.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.23.23.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_8_pred_MobileNetV3.png" id="A3.F14.23.23.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.24.24.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/leaves_8_pred_SegFormer-B5.png" id="A3.F14.24.24.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.32.32" class="ltx_tr">
<td id="A3.F14.32.32.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.32.32.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:32.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:32.8pt;transform:translate(-13.67pt,-13.67pt) rotate(-90deg) ;">
<p id="A3.F14.32.32.9.1.1" class="ltx_p"><span id="A3.F14.32.32.9.1.1.1" class="ltx_text" style="font-size:80%;">Fire</span></p>
</span></div>
</td>
<td id="A3.F14.25.25.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_1.jpg" id="A3.F14.25.25.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.26.26.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_1_pred_SemFPN-R101.png" id="A3.F14.26.26.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.27.27.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_1_pred_MobileNetV3.png" id="A3.F14.27.27.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.28.28.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_1_pred_SegFormer-B5.png" id="A3.F14.28.28.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.29.29.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_7.jpg" id="A3.F14.29.29.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.30.30.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_7_pred_SemFPN-R101.png" id="A3.F14.30.30.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.31.31.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_7_pred_MobileNetV3.png" id="A3.F14.31.31.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.32.32.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fire_7_pred_SegFormer-B5.png" id="A3.F14.32.32.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.40.40" class="ltx_tr">
<td id="A3.F14.40.40.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.40.40.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.0pt;height:31.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:31.2pt;transform:translate(-12.1pt,-11.32pt) rotate(-90deg) ;">
<p id="A3.F14.40.40.9.1.1" class="ltx_p"><span id="A3.F14.40.40.9.1.1.1" class="ltx_text" style="font-size:80%;">Fog</span></p>
</span></div>
</td>
<td id="A3.F14.33.33.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_2.jpg" id="A3.F14.33.33.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.34.34.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_2_pred_SemFPN-R101.png" id="A3.F14.34.34.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.35.35.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_2_pred_MobileNetV3.png" id="A3.F14.35.35.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.36.36.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_2_pred_SegFormer-B5.png" id="A3.F14.36.36.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.37.37.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_5.jpg" id="A3.F14.37.37.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.38.38.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_5_pred_SemFPN-R101.png" id="A3.F14.38.38.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.39.39.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_5_pred_MobileNetV3.png" id="A3.F14.39.39.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.40.40.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/fog_5_pred_SegFormer-B5.png" id="A3.F14.40.40.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.48.48" class="ltx_tr">
<td id="A3.F14.48.48.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.48.48.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:35.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:35.2pt;transform:translate(-14.88pt,-14.88pt) rotate(-90deg) ;">
<p id="A3.F14.48.48.9.1.1" class="ltx_p"><span id="A3.F14.48.48.9.1.1.1" class="ltx_text" style="font-size:80%;">Rain</span></p>
</span></div>
</td>
<td id="A3.F14.41.41.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_1.jpg" id="A3.F14.41.41.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.42.42.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_1_pred_SemFPN-R101.png" id="A3.F14.42.42.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.43.43.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_1_pred_MobileNetV3.png" id="A3.F14.43.43.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.44.44.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_1_pred_SegFormer-B5.png" id="A3.F14.44.44.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.45.45.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_5.jpg" id="A3.F14.45.45.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.46.46.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_5_pred_SemFPN-R101.png" id="A3.F14.46.46.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.47.47.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_5_pred_MobileNetV3.png" id="A3.F14.47.47.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.48.48.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/rain_5_pred_SegFormer-B5.png" id="A3.F14.48.48.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.56.56" class="ltx_tr">
<td id="A3.F14.56.56.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.56.56.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:5.5pt;height:37.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:37.1pt;transform:translate(-15.82pt,-15.82pt) rotate(-90deg) ;">
<p id="A3.F14.56.56.9.1.1" class="ltx_p"><span id="A3.F14.56.56.9.1.1.1" class="ltx_text" style="font-size:80%;">Snow</span></p>
</span></div>
</td>
<td id="A3.F14.49.49.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_2.jpg" id="A3.F14.49.49.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.50.50.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_2_pred_SemFPN-R101.png" id="A3.F14.50.50.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.51.51.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_2_pred_MobileNetV3.png" id="A3.F14.51.51.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.52.52.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_2_pred_SegFormer-B5.png" id="A3.F14.52.52.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.53.53.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_8.jpg" id="A3.F14.53.53.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.54.54.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_8_pred_SemFPN-R101.png" id="A3.F14.54.54.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.55.55.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_8_pred_MobileNetV3.png" id="A3.F14.55.55.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.56.56.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/snow_8_pred_SegFormer-B5.png" id="A3.F14.56.56.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
<tr id="A3.F14.64.64" class="ltx_tr">
<td id="A3.F14.64.64.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;">
<div id="A3.F14.64.64.9.1" class="ltx_inline-block ltx_transformed_outer" style="width:7.1pt;height:38.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:38.2pt;transform:translate(-15.56pt,-14.78pt) rotate(-90deg) ;">
<p id="A3.F14.64.64.9.1.1" class="ltx_p"><span id="A3.F14.64.64.9.1.1.1" class="ltx_text" style="font-size:80%;">Night</span></p>
</span></div>
</td>
<td id="A3.F14.57.57.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_1.jpg" id="A3.F14.57.57.1.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.58.58.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_1_pred_SemFPN-R101.png" id="A3.F14.58.58.2.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.59.59.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_1_pred_MobileNetV3.png" id="A3.F14.59.59.3.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.60.60.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_1_pred_SegFormer-B5.png" id="A3.F14.60.60.4.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.61.61.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_7.jpg" id="A3.F14.61.61.5.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.62.62.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_7_pred_SemFPN-R101.png" id="A3.F14.62.62.6.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.63.63.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_7_pred_MobileNetV3.png" id="A3.F14.63.63.7.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
<td id="A3.F14.64.64.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec3_quali/night_7_pred_SegFormer-B5.png" id="A3.F14.64.64.8.g1" class="ltx_graphics ltx_img_square" width="66" height="66" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_figure"><span id="A3.F14.69.1.1" class="ltx_text" style="font-size:113%;">Figure 14</span>: </span><span id="A3.F14.70.2" class="ltx_text ltx_font_bold" style="font-size:113%;">Qualitative results.<span id="A3.F14.70.2.1" class="ltx_text ltx_font_medium"> Examples of rare conditions generated for testing and predictions from different models. Results of the strong model like SegFormer-B5 is visibly better than the Semantic-FPN and MobileNetV3.</span></span></figcaption>
</figure>
<figure id="A3.F15" class="ltx_figure">
<table id="A3.F15.61" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="A3.F15.1.1" class="ltx_tr">
<td id="A3.F15.1.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r" style="padding-left:0.9pt;padding-right:0.9pt;"></td>
<td id="A3.F15.1.1.1" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;" colspan="4">
<span id="A3.F15.1.1.1.1" class="ltx_text" style="font-size:90%;">
 </span><svg id="A3.F15.1.1.1.pic1" class="ltx_picture" height="19.37" overflow="visible" version="1.1" width="460.15"><g transform="translate(0,19.37) matrix(1 0 0 -1 0 0) translate(269.45,0) translate(0,9.69)" fill="#000000" stroke="#000000" stroke-width="2.5pt"><path d="M -267.72 0 L 176.15 0" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 176.15 0)"><path d="M 12.83 0 C 9.03 0.71 2.85 2.85 -1.43 5.34 L -1.43 -5.34 C 2.85 -2.85 9.03 -0.71 12.83 0" style="stroke:none"></path></g><g fill="#FFFFFF"><path d="M -150.81 -9.69 h 222.88 v 19.37 h -222.88 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -146.66 -3.11)" fill="#000000" stroke="#000000"><foreignObject width="214.58" height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="A3.F15.1.1.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Better Models in AUROC on Synthetic</span></foreignObject></g></g></svg>
</td>
</tr>
<tr id="A3.F15.61.62" class="ltx_tr">
<td id="A3.F15.61.62.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F15.61.62.1.1" class="ltx_text" style="font-size:90%;">Images</span></td>
<td id="A3.F15.61.62.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F15.61.62.2.1" class="ltx_text" style="font-size:90%;">Semantic-FPN</span></td>
<td id="A3.F15.61.62.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F15.61.62.3.1" class="ltx_text" style="font-size:90%;">MobileNetV3</span></td>
<td id="A3.F15.61.62.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F15.61.62.4.1" class="ltx_text" style="font-size:90%;">SegFormer-B5</span></td>
<td id="A3.F15.61.62.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><span id="A3.F15.61.62.5.1" class="ltx_text" style="font-size:90%;">SETR-PUP</span></td>
</tr>
<tr id="A3.F15.6.6" class="ltx_tr">
<td id="A3.F15.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/baby_img_entropy_1024.png" id="A3.F15.2.2.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.3.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/baby_SemFPN-R101_entropy_1024.png" id="A3.F15.3.3.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.4.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/baby_MobileNetV3_entropy_1024.png" id="A3.F15.4.4.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.5.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/baby_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.5.5.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.6.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/baby_SETR-PUP_entropy_1024.png" id="A3.F15.6.6.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.11.11" class="ltx_tr">
<td id="A3.F15.7.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/bear_img_entropy_1024.png" id="A3.F15.7.7.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.8.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/bear_SemFPN-R101_entropy_1024.png" id="A3.F15.8.8.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.9.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/bear_MobileNetV3_entropy_1024.png" id="A3.F15.9.9.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.10.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/bear_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.10.10.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.11.11.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/bear_SETR-PUP_entropy_1024.png" id="A3.F15.11.11.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.16.16" class="ltx_tr">
<td id="A3.F15.12.12.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/billboard_img_entropy_1024.png" id="A3.F15.12.12.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.13.13.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/billboard_SemFPN-R101_entropy_1024.png" id="A3.F15.13.13.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.14.14.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/billboard_MobileNetV3_entropy_1024.png" id="A3.F15.14.14.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.15.15.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/billboard_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.15.15.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.16.16.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/billboard_SETR-PUP_entropy_1024.png" id="A3.F15.16.16.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.21.21" class="ltx_tr">
<td id="A3.F15.17.17.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/box_img_entropy_1024.png" id="A3.F15.17.17.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.18.18.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/box_SemFPN-R101_entropy_1024.png" id="A3.F15.18.18.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.19.19.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/box_MobileNetV3_entropy_1024.png" id="A3.F15.19.19.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.20.20.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/box_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.20.20.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.21.21.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/box_SETR-PUP_entropy_1024.png" id="A3.F15.21.21.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.26.26" class="ltx_tr">
<td id="A3.F15.22.22.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/flowerpot_img_entropy_1024.png" id="A3.F15.22.22.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.23.23.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/flowerpot_SemFPN-R101_entropy_1024.png" id="A3.F15.23.23.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.24.24.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/flowerpot_MobileNetV3_entropy_1024.png" id="A3.F15.24.24.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.25.25.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/flowerpot_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.25.25.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.26.26.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/flowerpot_SETR-PUP_entropy_1024.png" id="A3.F15.26.26.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.31.31" class="ltx_tr">
<td id="A3.F15.27.27.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/hippo_img_entropy_1024.png" id="A3.F15.27.27.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.28.28.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/hippo_SemFPN-R101_entropy_1024.png" id="A3.F15.28.28.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.29.29.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/hippo_MobileNetV3_entropy_1024.png" id="A3.F15.29.29.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.30.30.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/hippo_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.30.30.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.31.31.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/hippo_SETR-PUP_entropy_1024.png" id="A3.F15.31.31.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.36.36" class="ltx_tr">
<td id="A3.F15.32.32.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/lion_img_entropy_1024.png" id="A3.F15.32.32.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.33.33.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/lion_SemFPN-R101_entropy_1024.png" id="A3.F15.33.33.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.34.34.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/lion_MobileNetV3_entropy_1024.png" id="A3.F15.34.34.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.35.35.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/lion_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.35.35.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.36.36.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/lion_SETR-PUP_entropy_1024.png" id="A3.F15.36.36.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.41.41" class="ltx_tr">
<td id="A3.F15.37.37.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/monkey_img_entropy_1024.png" id="A3.F15.37.37.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.38.38.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/monkey_SemFPN-R101_entropy_1024.png" id="A3.F15.38.38.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.39.39.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/monkey_MobileNetV3_entropy_1024.png" id="A3.F15.39.39.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.40.40.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/monkey_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.40.40.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.41.41.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/monkey_SETR-PUP_entropy_1024.png" id="A3.F15.41.41.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.46.46" class="ltx_tr">
<td id="A3.F15.42.42.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/sofa_img_entropy_1024.png" id="A3.F15.42.42.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.43.43.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/sofa_SemFPN-R101_entropy_1024.png" id="A3.F15.43.43.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.44.44.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/sofa_MobileNetV3_entropy_1024.png" id="A3.F15.44.44.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.45.45.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/sofa_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.45.45.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.46.46.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/sofa_SETR-PUP_entropy_1024.png" id="A3.F15.46.46.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.51.51" class="ltx_tr">
<td id="A3.F15.47.47.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/tiger_img_entropy_1024.png" id="A3.F15.47.47.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.48.48.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/tiger_SemFPN-R101_entropy_1024.png" id="A3.F15.48.48.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.49.49.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/tiger_MobileNetV3_entropy_1024.png" id="A3.F15.49.49.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.50.50.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/tiger_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.50.50.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.51.51.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/tiger_SETR-PUP_entropy_1024.png" id="A3.F15.51.51.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.56.56" class="ltx_tr">
<td id="A3.F15.52.52.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/toilet_img_entropy_1024.png" id="A3.F15.52.52.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.53.53.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/toilet_SemFPN-R101_entropy_1024.png" id="A3.F15.53.53.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.54.54.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/toilet_MobileNetV3_entropy_1024.png" id="A3.F15.54.54.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.55.55.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/toilet_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.55.55.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.56.56.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/toilet_SETR-PUP_entropy_1024.png" id="A3.F15.56.56.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
<tr id="A3.F15.61.61" class="ltx_tr">
<td id="A3.F15.57.57.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/zebra_img_entropy_1024.png" id="A3.F15.57.57.1.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.58.58.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/zebra_SemFPN-R101_entropy_1024.png" id="A3.F15.58.58.2.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.59.59.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/zebra_MobileNetV3_entropy_1024.png" id="A3.F15.59.59.3.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.60.60.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/zebra_SegFormer-B5_v3_entropy_1024.png" id="A3.F15.60.60.4.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
<td id="A3.F15.61.61.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:0.9pt;padding-right:0.9pt;"><img src="/html/2312.09231/assets/figures/supplementary/sec4_quali/zebra_SETR-PUP_entropy_1024.png" id="A3.F15.61.61.5.g1" class="ltx_graphics ltx_img_landscape" width="124" height="62" alt="Refer to caption"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 15: </span><span id="A3.F15.66.1" class="ltx_text ltx_font_bold">Qualitative results.</span> Confidence maps are visualized for the four exemplified models on real data (first row) and synthetic inpainted data (second and third rows). Hotter colors correspond to higher OOD likelihood. Ideally, results should exhibit hot colors in OOD areas and cold colors everywhere else. We observe a strong correlation in model reactions to real and synthetic OOD regions, particularly for more recent and robust models.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.09230" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.09231" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.09231">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.09231" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.09232" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 13:46:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
