<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.18814] Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM).</title><meta property="og:description" content="Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.
Differential privacy (DP) is the de facto standard of privacy protection with provable guarantees.
Advances i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM).">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM).">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.18814">

<!--Generated on Sun May  5 15:11:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Belt and Brace: When Federated Learning Meets Differential Privacy
<span id="id1.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This article has been accepted by and is to appear in Communications of the ACM (CACM).</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xuebin Ren 
<br class="ltx_break">
<br class="ltx_break">
Shusen Yang 
<br class="ltx_break">
<br class="ltx_break">
Cong Zhao 
<br class="ltx_break">
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Xi’an Jiaotong University
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xuebinren@mail.xjtu.edu.cn">xuebinren@mail.xjtu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_address">Xi’an Jiaotong University
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shusenyang@mail.xjtu.edu.cn">shusenyang@mail.xjtu.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_address">Xi’an Jiaotong University
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:congzhao@xjtu.edu.cn">congzhao@xjtu.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julie McCann 
<br class="ltx_break">
<br class="ltx_break">
Zongben Xu 
<br class="ltx_break">
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Imperial College London
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:j.mccann@imperial.ac.uk">j.mccann@imperial.ac.uk</a>
</span>
<span class="ltx_contact ltx_role_address">Xi’an Jiaotong University
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zbxu@mail.xjtu.edu.cn">zbxu@mail.xjtu.edu.cn</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p"><span id="id2.id1.1" class="ltx_text">Federated learning (FL) has great potential for large-scale machine learning (ML) without exposing raw data.
Differential privacy (DP) is the <span id="id2.id1.1.1" class="ltx_text ltx_font_italic">de facto</span> standard of privacy protection with provable guarantees.
Advances in ML suggest that DP would be a perfect fit for FL with comprehensive privacy preservation. Hence, extensive efforts have been devoted to achieving practically usable FL with DP, which however is still challenging.
Practitioners often not only are not fully aware of its development and categorization, but also face a hard choice between privacy and utility.
Therefore, it calls for a holistic review of current advances and an investigation on the challenges and opportunities for highly usable FL systems with a DP guarantee.<span id="id2.id1.1.2" class="ltx_text">
<span id="id2.id1.1.2.1" class="ltx_text">In this article, we first introduce the primary concepts of FL and DP, and highlight the benefits of integration. We then review the current developments by categorizing different paradigms and notions. Aiming at usable FL with DP, we present the optimization principles to seek a better tradeoff between model utility and privacy loss. Finally, we discuss future challenges in the emergent areas and relevant research topics.<span id="id2.id1.1.2.1.1" class="ltx_text"></span></span></span></span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With the development of advanced algorithms, computing capabilities, and available datasets, machine learning (ML) have been widely adopted to solve real-world problems in various application domains.
The success of ML often relies on large amounts of application-specified training data<span id="S1.p1.1.1" class="ltx_text">, especially for large models like ChatGPT.<span id="S1.p1.1.1.1" class="ltx_text"> However, these data are often generated and scattered among enormous network edges or users’ end devices, and can be quite sensitive and impractical to be moved to a central location as the result of regulatory laws (e.g., GDPR) or privacy concerns <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. This fact has brought an inconvenient dilemma between large-scale ML and increasingly severe data isolation. The conflict between data hungriness and privacy awareness is becoming increasingly prominent <span id="S1.p1.1.1.1.1" class="ltx_text">in the artificial intelligence (AI) era<span id="S1.p1.1.1.1.1.1" class="ltx_text">.</span></span></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Google proposed FL as a potential solution to the above issue <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Through coordination between the central server and clients (devices participated in FL), FL collaboratively trains ML models over extensive data across geographies, which bridges up the gap between an ideal of big data utilization and the reality of data fragmentation everywhere.
<span id="S1.p2.1.1" class="ltx_text">By sharing locally trained models, FL not only minimizes the risks of raw data exposure but also eliminates the client-server communications.<span id="S1.p2.1.1.1" class="ltx_text">
<span id="S1.p2.1.1.1.1" class="ltx_text">Once proposed, it has been seen as a rising star in AI technology. Its recent usage in fine-tuning of large language models (LLMs) confirmed that again.<span id="S1.p2.1.1.1.1.1" class="ltx_text"></span></span></span></span></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The advancement of FL in privacy protection stems from the delicacy in restricting raw data sharing.
<span id="S1.p3.1.1" class="ltx_text">This is however far from sufficient, as gradients of deep models can even expose the privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> but FL gives no formal privacy guarantees.<span id="S1.p3.1.1.1" class="ltx_text">
Fortunately, <span id="S1.p3.1.1.1.1" class="ltx_text">differential privacy (DP), proposed by Dwork, allows controllable privacy guarantee, via formalizing the information derived from private data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
By adding proper noise, DP guarantees a query result does not disclose much information about the data.
Because of its rigorous formulation, DP has been the <span id="S1.p3.1.1.1.1.1" class="ltx_text ltx_font_italic">de facto</span> standard of privacy and applied in both ML and FL.<span id="S1.p3.1.1.1.1.2" class="ltx_text"></span></span></span></span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">As privacy in design, <span id="S1.p4.1.1" class="ltx_text">the emergence of DP and FL greatly encourages data sharing and utilization in reality.<span id="S1.p4.1.1.1" class="ltx_text">
On one hand, <span id="S1.p4.1.1.1.1" class="ltx_text">by restricting raw data exposure<span id="S1.p4.1.1.1.1.1" class="ltx_text">, FL enables ML model training over massively fragmented data. It also significantly enriches ML applications for extensive distributed scenarios.
On the other hand, by rigorously limiting the indirect information leakage, DP can strengthen the privacy in trained models with provable guarantees.
The complementarity of FL and DP in privacy suggests a promising future of their combination, which can significantly extend the applicable areas for both techniques and bring privacy-preserving large-scale ML to reality.
Specifically, FL has advantages in fusing geographically isolated datasets, while DP can offer provable guarantees and thus encourage sensitive data sharing.
Aimed at exploiting the potential of ML to its fullest, it is highly desirable and essential to build FL with DP to train and refine ML models with more comprehensive datasets.</span></span></span></span></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text">The benefit of privacy protection in both FL and DP comes at a cost in terms of data utility, albeit other issues.
FL clients often have limited capabilities and distribution-skewed datasets, causing insufficient and/or unbalanced training of global models with low utility.
DP algorithms hide the presence of any individual sample or client by adding noise to model parameters, also leading to possible utility loss.
Therefore, utility optimization, i.e., improving the model utility as much utility as possible for a given privacy guarantee is an essential problem in the combining use of FL and DP.
Given the great potential, studies on this problem have rapidly expanded in recent years.
However, they are often conducted based on various FL and DP paradigms concerning different security assumptions (e.g., whether the server is trustworthy) and levels of privacy granularity (e.g., sample or client).
Without a systematic review and clear categorization of existing paradigms, it is hard to precisely evaluate and compare their utility performance.
On the other hand, despite the paradigm differences, the utility optimization principles are quite similar. However, current studies often focus on specific algorithm design for different paradigms of FL with DP and there lacks some common pathways to follow. Meanwhile, the only few surveys on the intersection of DP and FL either have different focus other than the utility issue or lack high-level insights into the future challenges.
<span id="S1.p5.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text">Here, this article aims to provide a systematic overview of DP-enabled FL while focusing on high-level perspectives on its utility optimization techniques.
We begin by presenting an introduction to FL and DP respectively, highlighting the benefits of their combination. We then summarize research advances by categorizing the paradigms and software frameworks of FL with DP. Aiming at usable analytic results, we present the high-level principles and primary technical challenges in their utility optimization in several emerging scenarios. Finally, we discuss some related topics to FL with DP, which would also impact the achieved data utility.
Our review can benefit the general audience with a systematic understanding of the development and achievements on this topic. The perspectives on utility optimization for DP-enabled FL can offer some insights into research opportunities and challenges for usable AI services with privacy protection in both academia and industry.<span id="S1.p6.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Federated Learning</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview of Federated Learning</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">An FL system is essentially a distributed ML (or DML) system coordinated by a central server <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span id="footnote1.1" class="ltx_text" style="font-size:90%;">Decentralized FL is a special form where clients collaborate via peer-to-peer communication without a server.</span></span></span></span>, which helps multiple remote clients with separate datasets to collaboratively train an ML model, under a privacy constraint that any client does not expose its raw data.
There are two popular FL frameworks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. Federated stochastic gradient descent (FedSGD) is the federated version of the stochastic gradient descent (SGD) algorithm. In SGD for centralized ML, gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent. FedSGD uses a random fraction of clients and all their local data. The gradients are averaged by the server proportionally to the number of training samples on each client and used to make a gradient descent step. To overcome the communication bottleneck, federated averaging (FedAvg) allows clients to perform more than one batch update on the local dataset and exchange the updated parameters rather than the gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. FedAvg is a generalization of FedSGD since averaging the gradients would be equivalent to averaging the parameters themselves if all the clients begin with the same initialization.
So, generally FL works as follows:
1) Each participating client performs a local training procedure on its own dataset and sends the gradients or model updates to the server.
2) The server securely aggregates the received gradients or model updates, and updates the global model accordingly.
3) The server sends back the new global model to the corresponding clients.
4) The clients update their local models and prepare for the next iteration.
The above procedures are repeated until the global model converges or a sufficient number of iterations are applied.
FL is classified into cross-device FL that leverages up to millions of devices in the wide-area network, and cross-silo FL that ties up a handful of edge nodes with reliable backbones.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Comparison with Traditional DML</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2404.18814/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_img_landscape" width="207" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Building Blocks of FL Systems </figcaption>
</figure>
<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Despite being a typical DML paradigm, when compared with <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">traditional DML in data centers</span> for ML speedup, FL has many distinct characteristics (as shown in Fig. <a href="#S2.F1" title="Figure 1 ‣ 2.2 Comparison with Traditional DML ‣ 2 Federated Learning ‣ Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>):</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Privacy requirement</span>:
Unlike traditional DML in the data centers (where data can be arbitrarily scheduled among computing nodes), ensuring privacy protection lies at the center of FL, which strictly prohibits raw data sharing.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Data partitioning</span>: Data in FL are generated naturally or obtained from individual users, thus often being non-IID and imbalanced. Instead, data in traditional DML are usually manually scheduled to be almost shuffled or balanced.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">On-device learning:</span> In data centers, DML computing nodes are homogeneous, deployed centrally, and powerful. In contrast, FL is implemented with tens to millions of distributed clients with heterogeneous and limited computing capacities.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Communication:</span>
Traditional DML in data centers can enjoy Gigabytes bandwidth and communicate in a peer-to-peer manner. However, FL clients are usually connected to the server by the wide-area network and bandwidth constrained.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.1" class="ltx_p"><span id="S2.SS2.p6.1.1" class="ltx_text ltx_font_bold">Model aggregation:</span>
Model aggregation fuses training results (e.g., local models) from distributed nodes.
Compared to homogeneous sub-models in traditional DML, one challenge in FL is the prominent heterogeneity among local models due to either non-IIDness or varied training progresses.</p>
</div>
<div id="S2.SS2.p7" class="ltx_para">
<p id="S2.SS2.p7.1" class="ltx_p"><span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">System actors:</span>
Unlike the closed and fixed system of traditional DML, FL is often conceived as an open and scalable system consisting of massive clients owned by different individuals/organizations seeking for different benefits.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Privacy Threats in Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Due to above characteristics, e.g., geographically distribut-ed nature, open architecture, and complicated interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, various attack can be mounted against FL in both model training and serving (i.e., inference).
Instead of those for degrading system availability or compromising data integrity (e.g., poisoning attacks), we focus on privacy threats for snooping private information in FL.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Privacy Adversaries</span>.
Privacy may be disclosed to or inferred by anyone that has access to the information flow in FL.
Compared with ML over centralized data or traditional DML centrally deployed in datacenters, mutually distrusted entities in FL may all be viewed as privacy adversaries inferring others private information
The possible adversaries can be classified as insiders and outsiders. The former includes the server and participating clients, and the latter contains eavesdroppers over communication channels and third-party analysts (users) consume the final model.
Compared with the outsiders that are more likely to have black-box access (i.e., can only query via APIs) to the final model, insiders are generally more capable as they can often have white-box access (i.e., full access with prior knowledge) and substantially impact FL model training. The insiders can be further considered to be semi-honest and malicious. The former is also known as honest-but-curious, i.e., following the protocol correctly but tries to learn other entities’ private state. The latter may actively deviate from the protocol (e.g., modifying data or colluding with others) to achieve the goal.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">Privacy Attacks</span>.
Considering above adversaries, the following privacy attacks may exist in FL (shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ 2.3 Privacy Threats in Federated Learning ‣ 2 Federated Learning ‣ Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>):</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p"><span id="S2.SS3.p4.1.1" class="ltx_text ltx_font_italic">Membership inference</span> targeting a model aims to predict whether a given data sample was in its training set <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. It works by training multiple customized inference models to recognize noticeable patterns in the models’ outputs for the given sample.
In traditional ML centrally deployed, membership inference is normally mounted by third-party users.
In FL, it can be carried out by not only third-party users, but also communication eavesdroppers, and even participating clients and the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. This is because, the local, aggregated, accumulated and final forms of gradients or model parameters, all may expose private information about training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>.
Moreover, active attackers disguised as clients can selectively alter their gradient updates to significantly enhance the attack accuracy over the victim clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p"><span id="S2.SS3.p5.1.1" class="ltx_text ltx_font_italic">Class representative inference</span> tries to generate class representatives from the underlying distribution of the training data that the targeted model could have been trained on. In traditional ML, third-party users can achieve this goal by iteratively modifying the features of a random sample until a maximal confidence reaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, or training an inverse model, with black-box access to the targeted model. In FL, while a honest-but-curious server may partially recover some samples of honest clients by simply observing their uploaded gradients,
active malicious clients or a passive malicious server can exploit generative adversarial networks (GANs) to construct class representatives from not only the global data distribution but also specific clients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p"><span id="S2.SS3.p6.1.1" class="ltx_text ltx_font_italic">Other privacy attacks</span> include inferences for properties, and even the accurate training data (both inputs and labels). Different from above inferences in terms of properties characterizing an entire class, property inferences aim to infer those properties independent of the characteristic features. With some auxiliary data, a passive adversary trains a binary property classifier to predict whether the observed updates were based on the data with the property, while an active adversary can exploit multi-task learning to simultaneously conduct main FL training and infer the targeted property state with enhanced capability. Inferring accurate training data is also demonstrated possible under the <span id="S2.SS3.p6.1.2" class="ltx_text ltx_font_italic">deep leakage from gradient</span>, which optimizes the dummy inputs and labels via minimizing the difference between the dummy and targeted gradients for differentiable models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2404.18814/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="140" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Privacy threats in FL training </figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Related Privacy-preserving Techniques</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Cryptographic primitives and protocols, can restrict unauthorized access to confidential information, thus reducing the chances of privacy leakage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. For instance, homomorphic encryption (HE) supports dedicated operations on multiple encrypted data to produce ciphertexts that can be decrypted to generate desirable functional outcomes of original plaintexts.
<span id="S2.SS4.p1.1.1" class="ltx_text">Functional encryption (FE) authorizes the holder of a key associated with a specified function to directly learn the function output over encrypted data and nothing else.<span id="S2.SS4.p1.1.1.1" class="ltx_text">
Using secure multi-party computation (SMC), a set of parties jointly compute from their inputs without relying on a trusted third party or learning each other’s input.
<span id="S2.SS4.p1.1.1.1.1" class="ltx_text">
Cryptography implemented in software still requires error-free environment for execution and uncompromising storage of secret key. This naturally calls for hardware-assisted security. Trusted execution environments (TEEs) can create an isolated operating environment that ensures the confidentiality of the data and codes within, while enabling remote authentication and attestation.<span id="S2.SS4.p1.1.1.1.1.1" class="ltx_text">
<span id="S2.SS4.p1.1.1.1.1.1.1" class="ltx_text">In FL training, above technologies can be adopted either alone or in combination to guarantee desired confidentiality of the processed models.<span id="S2.SS4.p1.1.1.1.1.1.1.1" class="ltx_text"></span></span></span></span></span></span></p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p"><span id="S2.SS4.p2.1.1" class="ltx_text">However, note that privacy is essentially orthogonal to confidentiality. Whatever secure protocols and trusted systems are used, a final model will eventually be trained for consumption. Even if providing inference APIs only, model predictions may still reveal sensitive information as ML models inevitably carry some knowledge of training samples <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.<span id="S2.SS4.p2.1.1.1" class="ltx_text">
In general, models with poor generalization tend to leak more. Overfitting is one of the sufficient conditions of performing membership inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
Therefore, another line of defensive approaches is properly suppressing fine-grained model utility. For instance, regularization can undermine inference attacks by reducing overfitting. For deep learning, two useful strategies are model compression (or sparsification) that sets gradients below a threshold to zero and weight quantization that limits the parameter precision. However, these approaches provide intuitive protection only without rigorous guarantee <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</span></span></p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Differential Privacy</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">With provable guarantee of limiting privacy leakage even in securely aggregated results, differential privacy is promising to complement above technologies and strengthen FL.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview of Differential Privacy</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Through establishing a formal measure of privacy loss, DP allows rigorously controlling the (worst-case) information leakage.
Informally, it guarantees an algorithm’s output does not change much for two datasets differing by a single entry <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. To achieve DP, the basic idea is to properly randomize the relationship between data input and algorithmic output, e.g., by adding noise.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">DP has various models, as noise can be added to the different components or phases of algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Conventional DP assumes a trustworthy aggregator and adds minor noise to algorithm output, which is known as centralized DP (CDP). Assuming an honest-but-curious aggregator, local DP (LDP) randomizes data at users’ end before collection, and reconstructs utility from perturbed data of multiple uses.
From CDP to LDP, the trust model is weakened under the same DP parameter, while data uncertainty and accuracy loss becomes larger. To bridge the trust-accuracy gap, distributed DP (DDP) exploits cryptography to obtain high accuracy without a trusted aggregator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. There are currently two DDP paradigms, based on secure shuffling and secure aggregation respectively. Secure shuffling uses an anonymous communication channel to alleviate identification risks of messages and thereby relaxing the trust model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Secure aggregation replaces the trusted aggregator by secure computation protocols and thus can reduce noise and gain the same utility as in centralized model.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The prevalence of DP also comes from many delicate characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The post-processing property keeps the privacy guarantee of algorithms after arbitrary workflows. Composition theorems help to understand the composed privacy guarantee of a series of sub-algorithms and enables building complicated algorithms from simple operations.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2404.18814/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_img_square" width="184" height="149" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Approaches to achieve DP for ML </figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Differential Privacy for ML</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">DP has been applied in ML to prevent adversaries with access to the model from inferring the training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Except that <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">intrinsic privacy</span> can be achieved freely for some ML models with inner randomness <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, noise addition to different components of ML algorithms provides viable pathways for privacy-preserving ML with DP, as shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Optimization from the Perspective of FL ‣ 4.3 Improving Model Utility for FL with DP ‣ 4 Federated Learning with Differential Privacy ‣ Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_italic">Output perturbation</span> adds calibrated noise to the parameters of final models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>,
which, however, may have large (even unbounded) sensitivities and lead to severe model utility loss.
<span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">Input perturbation</span> randomizes training data and then constructs an approximate learning model on it <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Similar to LDP, it has learning limits and low model utility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
<span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">Objective perturbation</span> perturbs the objective functions of the optimization problem in ML. Although functional mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> allows its usage for complicated model functions, it is often infeasible to explicitly express the loss functions for most ML models, especially deep learning.
<span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">Gradient perturbation</span> that sanitizes parameter gradients during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> can ensure DP even for nonconvex objectives, making it much useful for deep models.
Differentially private SGD (DP-SGD) has now been the common practice for privacy-preserving ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. It works by sampling a min-batch of samples, clipping the <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="l_{2}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">l</mi><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝑙</ci><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">l_{2}</annotation></semantics></math> norm of the gradients computed on each sample, aggregating the clipped gradients, and adding Gaussian noise in each iteration. By incorporating gradient clipping, it can avoid the issue of unknown gradient sensitivity. Besides, it is often used with moments accountant for tracking a tighter privacy loss bound.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Federated Learning with Differential Privacy</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The wide application of DP in privacy-preserving ML shows the great potential of privacy-preserving FL with DP.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Benefits of FL with DP</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">DP with rigorous guarantee has been an essential technology for privacy-preserving data analysis and ML. Although it has been successfully integrated into distributed systems for data querying and analyses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, there is still a lack of DP-enhanced framework for large-scale distributed ML over massively scattered datasets.
FL supports flexible ML tasks with extensive models and scalable ML training for massively scattered datasets. Despite ensuring no direct data exposure by solely sharing intermediate parameters, it still lacks a formal privacy guarantee and may expose indirect privacy.
Therefore, when combining them together, FL with DP can realize large-scale and flexible distributed learning while preventing both direct and indirect privacy leakage.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">As complements of each other towards the same goal of encouraging massively confidential and sensitive data utilization, the combination of FL and DP can achieve paramount benefits for privacy protection in reliability.
<span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">FL empowers and prospers DP-based ML over large-scale siloed datasets.</span>
DP-based ML (especially deep learning) in the centralized setting, has made a rapid progress. However, data centralization and privacy regulations strongly hinders its further development.
As a result, DP-based ML wishes to meet large-scale data or data-extensive applications.
Fortunately, FL naturally enables DP-based ML over massively scattered data, thus greatly prospering its success.
<span id="S4.SS1.p2.1.2" class="ltx_text ltx_font_bold">DP completes and strengthens the reliability of FL via offering rigorous guarantee.</span>
The mission of FL is to train and refine ML models with more comprehensive end-user data, which is subject to the willingness of data owners.
Hence, provable privacy guarantee is key to the popularization of FL systems.
Beyond isolated datasets, privacy-preserved FL systems may encourage users to contribute more sensitive datasets.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Research Advances on FL with DP</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Due to above benefits, marrying FL with DP has attracted extensive interests from both the academia and industry.
We systematically review the advances according to different paradigms and privacy notions.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>FL with Centralized DP</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">It is natural to extend differentially private ML algorithms (e.g., DP-SGD) in centralized setting, to the context of FL to prevent information leakage from the training iterations and final model, against malicious clients or third-party users.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">DP has different granularity, relying on the precise definition of neighboring datasets.
Different from DP-SGD that provides <span id="S4.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">sample-level DP</span> for hiding the existence of any single sample, it is more meaningful to provide <span id="S4.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_italic">client-level DP</span> in FL, which ensures all the training data of a single client are protected. This also fits in the FL setting where each client computes a single model from all its local data.
Assuming a trusted central server, a straightforward idea is to apply DP into the aggregation of model updates for participating clients and hide any client’s influence on the model update, at the server. DP-SGD can be adapted to both FedAvg and FedSGD, which forms two DP variants, DP-FedAvg and DP-FedSGD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. In a high-level, they work as follows: 1) sampling a group of clients to train local models with total data; 2) clipping the model updates of clients to bound the norm of the total updates; 3) averaging the clipped updates; and 4) adding calibrated Gaussian noise to the average update.
The privacy amplification via subsampling and moment accountant still apply to compose the privacy loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.
However, when providing formal DP guarantee, a particular attention should be paid to a client dropout issue, which may violate the uniform sampling assumption.
Fortunately, recent studies show the possibilities of addressing in theory or bypassing with new framework.
Despite the existence of noise in both the intermediate model updates and final model, their privacy guarantees are much different as being quantified from different views.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>FL with Local DP</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">LDP implemented on local models can defend against untrusted server or other clients. Related studies can be categorized into two lines based on the FL architecture.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p"><span id="S4.SS2.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Noise before aggregation<span id="S4.SS2.SSS2.p2.1.1.1" class="ltx_text ltx_font_medium">.<span id="S4.SS2.SSS2.p2.1.1.1.1" class="ltx_text">
Considering an untrusted central server in practice, LDP can be applied to perturb gradients or model updates for individual client in each iterate.
A simple approach is to add Gaussian noise to individuals’ updates before uploading, which is also known as noising before model aggregation FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. For example, DP-FedSGD or DP-FedAvg can be further adapted into the LDP setting by offloading Gaussian noise addition to the clients’ side. Since the summation of multiple Gaussian noises still follows a Gaussian distribution, both the privacy loss at individual clients and the central server can be tracked simultaneously. FL algorithms with LDP, e.g., LDP-FedSGD, face the critical problem of the dimension dependency of communication and privacy. Besides communication overheads, given privacy parameter, the noise needed is substantially proportional to the dimension of model parameter vector. Through selecting a fraction of important dimensions, both noise variance and communication overhead can have a significant reduction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. Therefore, dimension reduction is commonly used for large models. For instance, updated gradients can be sampled in a subset to reduce communication and truncated in value to compress the noise variance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>.</span></span></span></p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p"><span id="S4.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Blind flooding with noise<span id="S4.SS2.SSS2.p3.1.1.1" class="ltx_text ltx_font_medium">.<span id="S4.SS2.SSS2.p3.1.1.1.1" class="ltx_text">
FL can be also implemented in a fully decentralized form without any central entity, thus avoiding a single point failure and improving efficiency for heterogeneous systems.
Its main feature is using peer-to-peer (P2P) communications other than a client-server architecture. A reasonable way to ensure model convergence with full information is to broadcast parameters to close neighbors, which, informally, faces even higher privacy risk than an untrusted server. Moreover, in some opportunistic networks (e.g., mobile crowd sensing or autonomous vehicle networks), the communication topology may be even time-varying and clients may meet unfamiliar neighbors frequently.
In such a case, LDP is necessary and effective to preserve the privacy of exchanged messages among individual clients. This lead to the problem of decentralized optimization with LDP, which aims to ensure model convergence over a sparse P2P network with noisy local models. However, lacking of a coordinating server, autonomous clients often have to adopt an asynchronous update pattern, which brings new challenges to the decentralized optimization in practice. Nonetheless, it has demonstrated that a differentially private asynchronous decentralized parallel SGD can converge at the same optimal rate as SGD, and have a comparable model utility as the synchronous mode while achieving relatively higher efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.</span></span></span></p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>FL with Distributed DP</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">As discussed before, DDP can bridge the utility-trust gap between LDP and CDP while eliminating the assumption of trusted server via two cryptographic techniques.</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p"><span id="S4.SS2.SSS3.p2.1.1" class="ltx_text ltx_font_bold">Privacy amplification by shuffling</span>
A line of DDP studies for FL concentrate on the aforementioned secure shuffling technique, which offers amplification of privacy-utility tradeoff via additional anonymization for DP. Before forwarding to the untrusted server, locally perturbed models with minor noise are first permuted randomly to eliminate their client identities by one or more trusted (i.e., secure) shufflers, which can be implemented as a trusted proxy or by delicate cryptographic primitives.
By devising the classic <span id="S4.SS2.SSS3.p2.1.2" class="ltx_text ltx_font_italic">encoder-shuffler-aggregator</span> (ESA) framework for adapting FL, LDP-SGD adapted with secure shuffling can achieve both strong iteration-level LDP and good overall CDP for final model, without noticeable accuracy loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
For high-dimensional parameters in deep models, shuffling the client identities only may still suffer from linkage attack from side channels. A solution is to split parameter vector and then shuffle the dividends to enhance anonymity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
To further trade off between privacy and utility, subsampling is also an important direction, which should consider the dimension importance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.
Reckoning the benefits of Renyi DP (RDP) and its stronger composition of privacy loss, beyond exploring RDP of subsampled mechanism, a natural extension is to further analyze and exploit RDP and RDP composition in the shuffled model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p"><span id="S4.SS2.SSS3.p3.1.1" class="ltx_text ltx_font_bold">Secure aggregation of small noises<span id="S4.SS2.SSS3.p3.1.1.1" class="ltx_text ltx_font_medium">.<span id="S4.SS2.SSS3.p3.1.1.1.1" class="ltx_text">
Secure aggregation protocols in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> overcomes the practical issue of random client dropouts in cross-device FL, paving the way for FL with DDP via secure aggregation.
However, such protocols often involve modular arithmetic, requiring the quantization of communicating contents (or discrete-valued inputs)for acceptable complexity.
Then, the noise for privacy protection of local models should be also generated in discrete value.
One solution is to generate and add minor discrete noise to the discretized parameters of individual clients before secure aggregation while outputting the aggregate parameters with moderate noise equivalent to the CDP model.
Binomial or Poisson distribution can approach a similar tradeoff between the utility and privacy of the Gaussian mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which however does not achieve RDP or enjoy the state-of-the-art composition and amplification. Simply using discrete Gaussian noise can yield RDP with sharp composition and subsampling-based amplification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, but relies on an uncommon sampling mechanism when implementing in software packages. Besides, the summation of discrete Gaussian is not closed and may cause privacy degradation.
Recently, Skellam mechanism can generate noise distributed according to the differences of two independent Poisson random variables <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Skellam noise is closed under summation and can leverage the common Poisson sampling tools to get privacy amplification and sharper RDP bound in theory.
However, it remains an important problem to develop a practical protocol for production-level FL systems.</span></span></span></p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>Platforms and Tools for FL with DP</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p"><span id="S4.SS2.SSS4.p1.1.1" class="ltx_text">Towards usable FL with DP, many software frameworks and platforms have been developed to support research-oriented simulations or production-oriented applications.<span id="S4.SS2.SSS4.p1.1.1.1" class="ltx_text">
For private deep learning, PySyft<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/OpenMined/PySyft" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/OpenMined/PySyft</a></span></span></span> is a Python library that supports FL and DP, and decouples model training from private data.
<span id="S4.SS2.SSS4.p1.1.1.1.1" class="ltx_text">Its current version mainly focuses on SMC and HE other than DP implementation.<span id="S4.SS2.SSS4.p1.1.1.1.1.1" class="ltx_text">
Dedicating to fair evaluation of FL algorithms for the research community, FedML<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://github.com/FedML-AI" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/FedML-AI</a></span></span></span> develops an open research library and standardized benchmark with diverse FL paradigms and configurations. The current version only integrates weak DP but provides low-level APIs for security primitives.
Similarly, by providing a high-level interface, PaddleFL<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://github.com/PaddlePaddle/PaddleFL" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/PaddlePaddle/PaddleFL</a></span></span></span> supports FL model development with DP and offers a baseline DP-SGD implementation.
Furthermore, despite the consideration of practical FL settings and recognition of privacy issues, other FL frameworks like FATE<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/FederatedAI/FATE" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/FederatedAI/FATE</a></span></span></span> and LEAF<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/TalwalkarLab/leaf" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/TalwalkarLab/leaf</a></span></span></span> still lack deep and flexible supports for DP implementation.
Recently, Sherpa.ai FL developed a unified framework for FL with DP, featuring comprehensive support for DP mechanisms and optimization techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Nevertheless, it mainly offers algorithm-level optimization and does not consider practical system implementation.
TensorFlow includes DP and FL implementations in its libraries TensorFlow Privacy and TensorFlow Federated<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a target="_blank" href="https://www.tensorflow.org/federated" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://www.tensorflow.org/federated</a></span></span></span>, respectively. Both libraries integrate seamlessly with existing TensorFlow models and allow training personalized models with DP. However, its integrated DP mechanisms are relatively fixed in design and do not support customized and flexible optimization.
Opacus<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://opacus.ai" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://opacus.ai</a></span></span></span> is a scalable and efficient library for PyTorch model training with DP.
It introduces an abstraction of privacy engine that attaches to the standard PyTorch optimizer, which makes DP-SGD implementation much easier without explicitly calling low-level APIs. Beyond ML in PyTorch, it can be easily used in PySyft FL workflows to implement FL with DP.</span></span></span></span></p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Improving Model Utility for FL with DP</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">Existing work underpins the baseline frameworks of FL with DP.
Aiming at usable FL with DP,
it is essential to pursue a better tradeoff between model utility and privacy.
By reviewing common techniques in the fields of DP, ML, and FL, some optimization principles are summarized below.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Optimization from the perspective of DP</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To seek better tradeoff, there are two directions: reducing unnecessary noise addition, and tracking privacy loss tightly.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Clipping bound estimation</span>: Sensitivity calibration determines the proper noise amplitude by correctly bounding the sensitivity value, is crucial for minimizing the noise variance while guaranteeing certain DP. As mentioned before, a common practice in DP-SGD, thus also in SGD-based FL with DP, is to bound the gradient sensitivity by gradient clipping and then add noise accordingly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. However, an underestimated clipping threshold may cause gradient bias and even model divergence while an overestimated one results in excessive noise addition. Thus, it is important to understand the impact of gradient clipping and dynamically identify the proper clipping bounds during training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. For instance, adaptive gradient clipping via divergence analysis or heuristic estimation, can provably or empirically reduce noise and produces models with higher utility <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p"><span id="S4.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Noise distribution optimization</span>: It aims to reduce noise variance by reshaping the noise distribution, thus decreasing unnecessary noise addition in DP.
It has been invested with lots of efforts. For instance, in traditional DP research, some discrete noise distribution and stair-case noise distribution via segmentation techniques have been used in DP algorithms to lessen the necessary noise scale while meeting the DP requirement. In fact, both Lapalce and Gaussian noise for DP are only some instances in a family of the whole distribution space satisfying DP definitions (as shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ 4.3.2 Optimization from the Perspective of FL ‣ 4.3 Improving Model Utility for FL with DP ‣ 4 Federated Learning with Differential Privacy ‣ Belt and Brace: When Federated Learning Meets Differential Privacy This article has been accepted by and is to appear in Communications of the ACM (CACM)." class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
Besides, to incorporating encryption primitives with less overheads, the discretization and quantization of data contents also require the same processing of noise generation for LDP and DDP.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p"><span id="S4.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Privacy loss composition</span>: The composition property of DP allows building complex FL models with DP primitives while composing privacy loss.
Traditionally, both sequential and advanced compositions offer fairly loose bounds.
Moment account analyzes a detailed distribution of the composed privacy loss variable and derives a much tighter bound with higher-order moments.
It shows acceptable utility with quite small privacy loss for DP-SGD via using amplification techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p">Privacy loss composition contributes to the optimization of privacy/utility tradeoff by tightly tracking privacy loss for multiple independent noise addition across DP mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite>. A relevant but opposite angle is to fix privacy budget and add correlated noises via wise budget division.
For instance, classic tree aggregation techniques add correlated noises rather than independent ones for repeated computations, which can get high utility while guaranteeing given DP. Inspired by the idea, an amplification-free algorithm adds correlated noise to the accumulation of mini-batch gradients, which achieves a nice tradeoff for DP-SGD without any amplification technique (and no uniform sampling and shuffling requirement) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p6" class="ltx_para">
<p id="S4.SS3.SSS1.p6.1" class="ltx_p"><span id="S4.SS3.SSS1.p6.1.1" class="ltx_text ltx_font_bold">Intrinsic DP computation</span>:
Many studies have shown that noise-free DP can be achieved by leveraging the inherent randomness of certain models or algorithms for model training, instead of using additional techniques or system components. Being aware of the intrinsic DP level, the designer or developer can save up much budget and add few noise, thus gaining utility without privacy degradation. For instance, by mapping the sampling process to an equivalent exponential mechanism, intrinsic DP in graph models can be effectively measured and leveraged in DP algorithm design. A novel federated model distillation framework can provide provable noise-free DP via random data sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. It has also been proved that data sketching for communication reduction in FL guarantees DP inherently <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Nonetheless, the intrinsic privacy is not very common and only exists in certain models or algorithms.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Optimization from the Perspective of FL</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Massive FL clients and the pervasively spatiotemporal sparsity of model parameters offer the chance to extract acceptable utility without significantly harming the privacy.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2404.18814/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_img_landscape" width="322" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustration of utility optimization techniques </figcaption>
</figure>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p"><span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">Updating frequency reduction</span>:
DP enhanced FL suffers from noise accumulation during excessive training epochs. For communication efficiency, too many training epochs also require much network bandwidth.
Therefore, it is highly desirable to reduce the model update frequency.
Compared with FedSGD, FedAvg allows clients to perform multiple local updates before aggregation, thus reducing global update frequency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.
A similar technique has been widely adopted in the DP applications with dynamic datasets or time-series data.
For instance, the data curator publishes perturbed data with DP noise at the timestamps with frequent changes while releasing approximate data without privacy budget consumption at non-changing timestamps.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p"><span id="S4.SS3.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Model parameters compression</span>:
Like the issue of frequent parameters updating, a long parameter vector heavily consumes the privacy budget (or incurs much noise with the fixed budget) and burdens the limited communication channel.
To this end, many aforementioned model compression approaches, including parameter filtering, low-rank approximation, random projection, gradient quantization, compressive sensing, etc. have been proposed for deep learning models. <span id="S4.SS3.SSS2.p3.1.2" class="ltx_text">For instance, similar studies include sampling and truncating a subset of gradient parameters in FL with CDP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, selecting top-K dimensions with large contributions in FL with LDP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, sampling dimensions in FL with DDP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.<span id="S4.SS3.SSS2.p3.1.2.1" class="ltx_text">
All these methods manage to empirically reduce both the communication bandwidth consumption and noise variance. However, lossy compression techniques, on the one hand, can effectively improve model utility via reducing the DP noise; on the other hand, they may lead to utility loss as some parameter information is eliminated. An immediate question is how to find the optimal compression rate for achieve best utility privacy tradeoff.</span></span></p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p"><span id="S4.SS3.SSS2.p4.1.1" class="ltx_text ltx_font_bold">Participating clients sampling</span>:
Besides reducing the update frequency and size of parameters, sampling the clients participating in DP-based FL training is also a promising approach to save privacy budget, communication overhead, and energy consumption. The rationale behind this approach comes from the amplification effect of sampling for DP, in which, by randomly sampling the DP protected FL clients in training epochs, much stronger privacy protection can be achieved while
minimizing the average consumption in communication and computation as well as privacy.
However, in practical cross-device FL, the set of available clients is usually dynamic without prior knowledge of the population. Moreover, as will discuss later, participating clients may drop out randomly. These issues make the assumption of uniform sampling unrealistic and cause severe challenges for gaining privacy-utility tradeoffs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Challenges and Discussions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text">Despite the great potential and opportunities of DP enhanced FL, there are still challenges in achieving usable FL with DP guarantee in emerging applications.<span id="S5.p1.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p"><math id="S5.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p2.1.m1.1a"><mo id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><ci id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.p2.1.1" class="ltx_text"> <span id="S5.p2.1.1.1" class="ltx_text ltx_font_bold">Vertical/Transfer federation</span>:
FL can be also categorized according to different data partition strategies.
The above-discussed FL in the generic form, mainly considers the horizontal data partition where each client holds a set of samples with the same feature space. Now, vertical FL where each party holds different features of the same set of samples has gained increasing attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. However, many existing studies on VFL are based on SMC for protecting confidentiality without considering privacy leakage in the final results. To achieve provable resistance to membership inference or reconstruction attacks, DP must be employed for safeguarding VFL. But it is more challenging than HFL because of two reasons. One is that the VFL algorithm design varies for different tasks and models and often requires case-by-case development. Another is the correlations among distributed attributes are more difficult to identify without spreading individual information to other parties. Besides the vertical federation, there are also scenarios where different parties may hold datasets with non-overlapping features and users. Federated transfer learning (FTL) can eliminate the shifts of feature spaces in this scenario by combining FL and domain adaptation. However, similar to VFL, achieving DP for FTL is still challenging as the gradient of individual instances has to be exchanged between participants. <span id="S5.p2.1.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p"><math id="S5.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p3.1.m1.1a"><mo id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.p3.1.1" class="ltx_text"> <span id="S5.p3.1.1.1" class="ltx_text ltx_font_bold">Large language models</span>:
With the emergence of large language models (LLMs) like ChatGPT, both FL and DP have begun to demonstrate a promising future in fine-tuning LLMs, while preserving privacy with respect to the private domain data. However, these LLMs often have several billions to hundreds of billions of parameters.
When applying DP and FL to LLMs, there will be multiple challenges concerning the huge number of parameters beyond the extra communication and computation burdens on resource-constrained participants.
Regardless of the DP model, the total amount of privacy noise has to be proportional to the number of parameters for enforcing DP on models, which would lead to huge utility loss.
Besides, the fine-tuning of pre-trained LLMs is also different from conventional model training.
The theoretical privacy guarantee in ML (e.g., DP-SGD) often assumes models are learned from scratch with many training iterations, instead of a fine-tuning mode with much fewer iterations.
Therefore, it is necessary to investigate new frameworks for applying both DP and FL and develop new theories for proper privacy guarantees in LLMs.<span id="S5.p3.1.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.2" class="ltx_p"><math id="S5.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p4.1.m1.1a"><mo id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><ci id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.p4.2.1" class="ltx_text"> <span id="S5.p4.2.1.1" class="ltx_text ltx_font_bold">FL over streams</span>:
In many realistic scenarios, training data are continuously generated in the form of streams at distributed clients. In such cases, FL systems have to conduct repetitive analyses on distributed streams. By inheriting online machine learning (OL), online federated learning can be naturally derived to avoid retraining models from scratch each time a new data fragment comes. However, achieving DP for OFL brings multiple challenges. The first is how to define privacy in the OFL setting, as the general DP notion works for static datasets only. Although existing privacy notions for data streams and FL seem to apply here, they still need to be clarified and formulated rigorously in the OFL setting. The second is the efficient algorithm. Taking the event-level LDP (i.e., ensuring <math id="S5.p4.2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S5.p4.2.1.m1.1a"><mi id="S5.p4.2.1.m1.1.1" xref="S5.p4.2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.p4.2.1.m1.1b"><ci id="S5.p4.2.1.m1.1.1.cmml" xref="S5.p4.2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.1.m1.1c">\epsilon</annotation></semantics></math>-LDP at each time instance) as an example, frequent uploading of local model updates accumulates huge communication costs and great utility loss, as the noise is proportional to the size of communication data. How to achieve communication and privacy efficiency without degrading overall model performance is thus an important but unsolved research problem.<span id="S5.p4.2.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text">Apart from adapting to the new settings, building usable DP-enhanced FL systems still needs to improve its robustness, consider fairness, and allow the data to be forgotten.<span id="S5.p5.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S5.p6" class="ltx_para">
<p id="S5.p6.1" class="ltx_p"><math id="S5.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p6.1.m1.1a"><mo id="S5.p6.1.m1.1.1" xref="S5.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p6.1.m1.1b"><ci id="S5.p6.1.m1.1.1.cmml" xref="S5.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p6.1.m1.1c">\bullet</annotation></semantics></math><span id="S5.p6.1.1" class="ltx_text"> <span id="S5.p6.1.1.1" class="ltx_text ltx_font_bold">Robustness</span>.
A robust FL system should be resilient to various failures and attacks caused by misbehaved participants.
Due to limited capabilities (e.g., battery limit), FL clients (e.g., smartphones) may drop out of FL training at any time unexpectedly.
The random client dropouts bring severe challenges to the practical design of differentially private FL. Except for requiring a more sophisticated design of secure aggregation protocols <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, some important assumptions may no longer hold for correctly measuring DP in FL. For instance, the DP amplification via shuffling and subsampling both rely on the assumption of clients correctly following the protocol.
Despite recent progress in theory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, building practical FL systems while addressing the above impacts simultaneously is still challenging.
Beyond robustness to dropouts of unintended client failure, defending against robustness attacks (e.g., model poisoning for Byzantine and backdoor attacks) mounted by malicious participants is much more challenging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Specifically, both data heterogeneity and model privacy protection in FL would prevent the server from accurately detecting anomalies and tracking specific participants.<span id="S5.p6.1.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S5.p7" class="ltx_para">
<p id="S5.p7.1" class="ltx_p"><math id="S5.p7.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p7.1.m1.1a"><mo id="S5.p7.1.m1.1.1" xref="S5.p7.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p7.1.m1.1b"><ci id="S5.p7.1.m1.1.1.cmml" xref="S5.p7.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p7.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Fairness</span>.
Privacy protection is only the first step to encouraging data sharing among a large population.
Fairness enforcement helps to mitigate the unintended bias on individuals with heterogeneous data.
However, the dilemma is that DP aims to obscure identifiable attributes while fairness requires the knowledge of individuals’ sensitive attribute values to avoid biased results.
The gradient clipping and noise addition in DP can exacerbate the unfairness by decreasing the accuracy of the model over underrepresented classes and subgroups.
So, the general tension between privacy and fairness calls for ethic-aware FL that respects both issues.
Meanwhile, gradient clipping and noise addition can also enhance the robustness to some extent, as discussed above.
This is also consistent <span id="S5.p7.1.2" class="ltx_text">with<span id="S5.p7.1.2.1" class="ltx_text"> the conclusion that there is a tension between fairness and robustness in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.
The constraints of fairness and robustness compete with each other, as robustness enhancement demands filtering out informative updates with significant model differences.
Therefore, there is a subtle relationship between privacy, fairness, and robustness in FL.
While existing studies concentrate on each two of them separately, it would be of significance to unify the interplay of the three simultaneously.</span></span></p>
</div>
<div id="S5.p8" class="ltx_para">
<p id="S5.p8.1" class="ltx_p"><math id="S5.p8.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S5.p8.1.m1.1a"><mo id="S5.p8.1.m1.1.1" xref="S5.p8.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S5.p8.1.m1.1b"><ci id="S5.p8.1.m1.1.1.cmml" xref="S5.p8.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p8.1.m1.1c">\bullet</annotation></semantics></math> <span id="S5.p8.1.1" class="ltx_text ltx_font_bold">Privacy right to be forgotten</span>.
The rights of privacy include the “right to be forgotten”, i.e., users can opt out of private data contribution without leaving any trace.
As ML models memorize much specific information about training samples, to ensure a specific private sample is totally forgotten, the concept of machine unlearning is proposed to eliminate its influence on trained models.
However, on the one hand, machine unlearning in the context of FL, i.e., federated unlearning, faces distinct challenges. Specifically, it is much harder to erase the influence of a client’s data, as the global model iteratively carries on all participating clients’ information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
A straightforward idea for resolving the problem is recording historical parameter updates of clients at the server, which may cause significant complexity.
On the other hand, existing machine unlearning has been demonstrated to leak privacy by observing the differences between the original and unlearned models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
DP seems to be one of the promising countermeasures. Therefore, it remains an open question about how to realize efficient and privacy-preserving solutions for federated unlearning.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text">With both privacy awareness and regulatory compliance, the meeting of FL and DP, will promote the development of artificial intelligence by unblocking the bottle-necking problem of large-scale ML .
The article presents a comprehensive overview of the developments, a clear categorization of current advances, and high-level perspectives on the utility optimization principles of FL with DP. This review aims to help the community to better understand the achievements in different ways of combining FL with DP, and the challenges of usable FL with rigorous privacy guarantees. Although FL and DP are increasingly promising in safeguarding private data in the AI era, their combination still faces severe challenges in emerging AI applications. Also, they need further consideration and improvements on other practical issues.<span id="S6.p1.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 308–318, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
N. Agarwal, P. Kairouz, and Z. Liu.

</span>
<span class="ltx_bibblock">The skellam mechanism for differentially private federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPS</span>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
N. Agarwal, A. T. Suresh, F. X. X. Yu, S. Kumar, and B. McMahan.

</span>
<span class="ltx_bibblock">cpsgd: Communication-efficient and differentially-private distributed
sgd.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPs</span>, pages 7564–7575, 2018.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Andrew, O. Thakkar, H. B. McMahan, and S. Ramaswamy.

</span>
<span class="ltx_bibblock">Differentially private learning with adaptive clipping.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv:1905.03871</span>, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proc. of AISTATS</span>, pages 2938–2948, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
V. Balcer, A. Cheu, M. Joseph, and J. Mao.

</span>
<span class="ltx_bibblock">Connecting robust shuffle privacy and pan-privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proc. of ACM-SIAM SODA</span>, pages 2384–2403, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
B. Balle, P. Kairouz, B. McMahan, O. D. Thakkar, and A. Thakurta.

</span>
<span class="ltx_bibblock">Privacy amplification via random check-ins.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPS</span>, 33, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Bater, X. He, W. Ehrich, A. Machanavajjhala, and J. Rogers.

</span>
<span class="ltx_bibblock">Shrinkwrap: efficient sql query processing in differentially private
data federations.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proc. VLDB Endow.</span>, 12(3):307–320, 2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Bonawitz, V. Ivanov, B. Kreuter, A. Marcedone, H. B. McMahan, S. Patel,
D. Ramage, A. Segal, and K. Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 1175–1191, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K. Chaudhuri and C. Monteleoni.

</span>
<span class="ltx_bibblock">Privacy-preserving logistic regression.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPS</span>, pages 289–296, 2009.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang.

</span>
<span class="ltx_bibblock">When machine unlearning jeopardizes privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 896–911, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
X. Chen, S. Z. Wu, and M. Hong.

</span>
<span class="ltx_bibblock">Understanding gradient clipping in private sgd: a geometric
perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPS</span>, 33, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Y. Cheng, Y. Liu, T. Chen, and Q. Yang.

</span>
<span class="ltx_bibblock">Federated learning for privacy-preserving ai.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 63(12):33–36, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. Cheu, A. Smith, J. Ullman, D. Zeber, and M. Zhilyaev.

</span>
<span class="ltx_bibblock">Distributed differential privacy via shuffling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proc. of Eurocrypt</span>, pages 375–403, 2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. C. Duchi, M. I. Jordan, and M. J. Wainwright.

</span>
<span class="ltx_bibblock">Privacy aware learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">J. ACM</span>, 61(6):1–57, 2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Dwork.

</span>
<span class="ltx_bibblock">A firm foundation for private data analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Comm. ACM</span>, 54(1):86–95, 2011.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Dwork, A. Roth, et al.

</span>
<span class="ltx_bibblock">The algorithmic foundations of differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Theoretical Computer
Science</span>, 9(3–4):211–407, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Ú. Erlingsson, V. Feldman, I. Mironov, A. Raghunathan, S. Song, K. Talwar,
and A. Thakurta.

</span>
<span class="ltx_bibblock">Encode, shuffle, analyze privacy revisited: Formalizations and
empirical evaluation.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv:2001.03618</span>, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
F. Farokhi.

</span>
<span class="ltx_bibblock">Distributionally-robust machine learning using locally
differentially-private data.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">arXiv:2006.13488</span>, 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
O. Fink, T. Netland, and S. Feuerriegelc.

</span>
<span class="ltx_bibblock">Artificial intelligence across company borders.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 65(1):34–36, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Fredrikson, S. Jha, and T. Ristenpart.

</span>
<span class="ltx_bibblock">Model inversion attacks that exploit confidence information and basic
countermeasures.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 1322–1333, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. C. Geyer, T. Klein, and M. Nabi.

</span>
<span class="ltx_bibblock">Differentially private federated learning: A client level
perspective.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPs</span>, 2017.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. M. Girgis, D. Data, S. Diggavi, A. T. Suresh, and P. Kairouz.

</span>
<span class="ltx_bibblock">On the rényi differential privacy of the shuffle model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, page 2321–2341, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
B. Hitaj, G. Ateniese, and F. Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 603–618, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
S. L. Hyland and S. Tople.

</span>
<span class="ltx_bibblock">On the intrinsic privacy of stochastic gradient descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">arXiv:1912.02919</span>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Ji, Z. C. Lipton, and C. Elkan.

</span>
<span class="ltx_bibblock">Differential privacy and machine learning: a survey and review.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv:1412.7584</span>, 2014.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
P. Kairouz, Z. Liu, and T. Steinke.

</span>
<span class="ltx_bibblock">The distributed discrete gaussian mechanism for federated learning
with secure aggregation.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv:2102.06387</span>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
P. Kairouz, B. McMahan, S. Song, O. Thakkar, A. Thakurta, and Z. Xu.

</span>
<span class="ltx_bibblock">Practical and private (deep) learning without sampling or shuffling.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv:2103.00039</span>, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Konecný, H. B. McMahan, F. X. Yu, P. Richtárik, A. T. Suresh,
and D. Bacon.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving communication
efficiency.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1610.05492, 2016.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Li, S. Hu, A. Beirami, and V. Smith.

</span>
<span class="ltx_bibblock">Ditto: Fair and robust federated learning through personalization.

</span>
<span class="ltx_bibblock">In <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proc. of ICML</span>, pages 6357–6368, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
T. Li, Z. Liu, V. Sekar, and V. Smith.

</span>
<span class="ltx_bibblock">Privacy for free: Communication-efficient learning with differential
privacy using sketches.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">arXiv:1911.00972</span>, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
G. Liu, X. Ma, Y. Yang, C. Wang, and J. Liu.

</span>
<span class="ltx_bibblock">Federaser: Enabling efficient client-level data removal from
federated learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE IWQOS</span>, pages 1–10, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
R. Liu, Y. Cao, H. Chen, R. Guo, and M. Yoshikawa.

</span>
<span class="ltx_bibblock">Flame: Differentially private federated learning in the shuffle
model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proc. of AAAI</span>, number 10, pages 8688–8696, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
R. Liu, Y. Cao, M. Yoshikawa, and H. Chen.

</span>
<span class="ltx_bibblock">Fedsel: Federated sgd under local differential privacy with top-k
dimension selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proc. of DASFAA</span>, pages 485–501, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proc. of AISTAS</span>, pages 1273–1282, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Proc. of ICLR</span>, pages 1–10.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
L. Melis, C. Song, E. De Cristofaro, and V. Shmatikov.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE S&amp;P</span>, pages 691–706, 2019.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
M. Nasr, R. Shokri, and A. Houmansadr.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE S&amp;P</span>, pages 739–753. IEEE, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
V. Pichapati, A. T. Suresh, F. X. Yu, S. J. Reddi, and S. Kumar.

</span>
<span class="ltx_bibblock">Adaclip: Adaptive clipping for private sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">arXiv:1908.07643</span>, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
M. Rigaki and S. Garcia.

</span>
<span class="ltx_bibblock">A survey of privacy attacks in machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv:2007.07646</span>, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
N. Rodríguez-Barroso, G. Stipcich, D. Jiménez-López, J. A.
Ruiz-Millán, E. Martínez-Cámara, G. González-Seco, M. V.
Luzón, M. A. Veganzones, and F. Herrera.

</span>
<span class="ltx_bibblock">Federated learning and differential privacy: Software tools analysis,
the sherpa. ai fl framework and methodological guidelines for preserving data
privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Inf. Fusion</span>, 64:270–292, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
A. Roy Chowdhury, C. Wang, X. He, A. Machanavajjhala, and S. Jha.

</span>
<span class="ltx_bibblock">Crypt<math id="bib.bib43.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="bib.bib43.1.m1.1a"><mi id="bib.bib43.1.m1.1.1" xref="bib.bib43.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="bib.bib43.1.m1.1b"><ci id="bib.bib43.1.m1.1.1.cmml" xref="bib.bib43.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib43.1.m1.1c">\epsilon</annotation></semantics></math>: Crypto-assisted differential privacy on untrusted
servers.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.2.1" class="ltx_text ltx_font_italic">Proc. of ACM SIGMOD</span>, pages 603–619, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
R. Shokri and V. Shmatikov.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Proc. of ACM CCS</span>, pages 1310–1321, 2015.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
R. Shokri, M. Stronati, C. Song, and V. Shmatikov.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE S&amp;P</span>, pages 3–18, 2017.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
L. Sun and L. Lyu.

</span>
<span class="ltx_bibblock">Federated model distillation with noise-free differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">arXiv:2009.05537</span>, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
L. Sun, J. Qian, and X. Chen.

</span>
<span class="ltx_bibblock">Ldp-fl: Practical private aggregation in federated learning with
local differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proc. of IJCAI</span>, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
S. Wagh, X. He, A. Machanavajjhala, and P. Mittal.

</span>
<span class="ltx_bibblock">Dp-cryptography: marrying differential privacy and cryptography in
emerging applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Comm. ACM</span>, 64(2):84–93, 2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Z. Wang, M. Song, Z. Zhang, Y. Song, Q. Wang, and H. Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Proc. of IEEE INFOCOM</span>, pages 2512–2520, 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
K. Wei, J. Li, M. Ding, C. Ma, H. H. Yang, F. Farokhi, S. Jin, T. Q. Quek, and
H. V. Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Inf. Forensics Security</span>, 15:3454–3469.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
J. Xu, W. Zhang, and F. Wang.

</span>
<span class="ltx_bibblock">A (dp)^ 2sgd: Asynchronous decentralized parallel stochastic
gradient descent with differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</span>, 2021.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
J. Zhang, Z. Zhang, X. Xiao, Y. Yang, and M. Winslett.

</span>
<span class="ltx_bibblock">Functional mechanism: regression analysis under differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proc. VLDB Endow.</span>, 5(11):1364–1375, 2012.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
L. Zhu, Z. Liu, and S. Han.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proc. of NeurIPS</span>, pages 14774–14784, 2019.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Y. Zhu and Y.-X. Wang.

</span>
<span class="ltx_bibblock">Poission subsampled rényi differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
7634–7642. PMLR, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.18813" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.18814" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.18814">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.18814" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.18815" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 15:11:25 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
