<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Setting up the Data Printer with Improved English to Ukrainian Machine Translation</title>
<!--Generated on Fri Jul 12 10:03:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.15196v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S1" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S2" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Supervised Finetuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S3" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>First Phase: Heuristic Filtering of Paracrawl</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S4" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Second Phase: Unsupervised Data Selection on Extended Multi30K</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S5" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Few-Shot Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S6" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S7" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S8" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S9" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Contributions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S10" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#S11" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span>References</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#A1" title="In Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Translation Examples</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#A1.SS0.SSS0.Px1" title="In Appendix A Translation Examples â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title">Sample of top 5 worst examples by BLEU from FLORES devtest set</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#A1.SS0.SSS0.Px2" title="In Appendix A Translation Examples â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_title">Sample of top 5 best examples by BLEU from FLORES devtest set</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Setting up the Data Printer with Improved English to Ukrainian Machine Translation</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">To build large language models for Ukrainian we need to expand our corpora with large amounts of new algorithmic tasks expressed in natural language. Examples of task performance expressed in English are abundant, so with a high-quality translation system our community will be enabled to curate datasets faster. To aid this goal, we introduce a recipe to build a translation system using supervised finetuning of a large pretrained language model with a noisy parallel dataset of 3M pairs of Ukrainian and English sentences followed by a second phase of training using 17K examples selected by k-fold perplexity filtering on another dataset of higher quality. Our decoder-only model named Dragoman beats performance of previous state of the art encoder-decoder models on the FLORES devtest set.

<br class="ltx_break"/>
<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id2.id1.1">Keywords:â€‰</span>machine translation, parameter-efficient fine tuning, large language models, unsupervised data selection, perplexity filtering</p>
</div>
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\NAT@set@cites</span>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p2">
<p class="ltx_p" id="p2.1"><span class="ltx_text" id="p2.1.1"></span></p>
</div>
<div class="ltx_logical-block" id="id1">
<div class="ltx_para" id="id1.p1">
<p class="ltx_p ltx_align_center" id="id1.p1.1"><span class="ltx_text ltx_font_bold" id="id1.p1.1.1" style="font-size:144%;">Setting up the Data Printer with Improved English to Ukrainian Machine Translation</span></p>
<br class="ltx_break ltx_centering"/>
<table class="ltx_tabular ltx_centering ltx_align_top" id="id1.p1.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="id1.p1.2.1.1">
<td class="ltx_td ltx_align_center" id="id1.p1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="id1.p1.2.1.1.1.1" style="font-size:120%;">Yurii Paniv, Dmytro Chaplynskyi, Nikita Trynus, Volodymyr Kyrylov</span></td>
</tr>
<tr class="ltx_tr" id="id1.p1.2.2.2">
<td class="ltx_td ltx_align_center" id="id1.p1.2.2.2.1">Ukrainian Catholic University, lang-uk initiative,</td>
</tr>
<tr class="ltx_tr" id="id1.p1.2.3.3">
<td class="ltx_td ltx_align_center" id="id1.p1.2.3.3.1">Igor Sikorsky Kyiv Polytechnic Institute,
UniversitÃ  della Svizzera italiana</td>
</tr>
<tr class="ltx_tr" id="id1.p1.2.4.4">
<td class="ltx_td ltx_align_center" id="id1.p1.2.4.4.1">paniv@ucu.edu.ua, chaplinsky.dmitry@gmail.com, trynus.nikita@lll.kpi.ua, vol@wilab.org.ua</td>
</tr>
</tbody>
</table>
<p class="ltx_p ltx_align_center" id="id1.p1.3"><span class="ltx_text ltx_font_italic" id="id1.p1.3.1">Abstract content</span></p>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">1.Â Â Â Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The availability of the data is the most important ingredient when one needs to pretrain general-purpose large language models for a specific natural language task or a set of tasks. While it is relatively easy to obtain a good and balanced dataset under specific domain for the English language, it is much harder to do the same for other under-resourced languages such as Ukrainian.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Since curating a corpus of tasks in Ukrainian is a large endeavor, and given a large body of work done for English, we consider existing instruction tuning datasets as a source of tasks to reuse in Ukrainian using automatic machine translation.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This work focuses on improving the current state of machine translation from English to Ukrainian.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We contribute a recipe for finetuning a large pretrained language model with publicly available data to build a translation system (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S3" title="3. First Phase: Heuristic Filtering of Paracrawl â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 3</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4" title="4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 4</span></a>). This matches state of the art performance of the best encoder-decoder model on a common multilingual benchmark using a consumer GPU with 24 GiB of VRAM. We release training, evaluation code, datasets, and model at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/lang-uk/dragoman" title="">https://github.com/lang-uk/dragoman</a>. Our main results are summarized in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S1.T1" title="Table 1 â€£ 1. Introduction â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a>. We provide examples of the top-5 best and worst translations on the FLORES devtest set in the <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#A1" title="Appendix A Translation Examples â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">AppendixÂ A</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We base pretrained model selection on evaluation in few-shot learning setting (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5" title="5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 5</span></a>). We find that itâ€™s a promising method to design tasks without training, and the model can perform comparably to specialized systems given increased inference budget and auxiliary translation scoring functions, yet still underperforms our finetuned recipe.</p>
</div>
<figure class="ltx_table" id="S1.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S1.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S1.T1.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S1.T1.1.1.1.m1.1"><semantics id="S1.T1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.m1.1.1" stretchy="false" xref="S1.T1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><ci id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S1.T1.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.2.1.1.1">Finetuned</span></td>
<td class="ltx_td ltx_border_t" id="S1.T1.1.2.1.2"></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S1.T1.1.3.2.1">Dragoman P, 10 beams (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S3" title="3. First Phase: Heuristic Filtering of Paracrawl â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 3</span></a>)</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.3.2.2">30.4</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S1.T1.1.4.3.1">Dragoman PT, 10 beams (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4" title="4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 4</span></a>)</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.4.3.2.1">32.3</span></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.5.4.1">
<span class="ltx_text ltx_font_bold" id="S1.T1.1.5.4.1.1">Zero shot and few shot</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5" title="5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 5</span></a>)</td>
<td class="ltx_td ltx_border_t" id="S1.T1.1.5.4.2"></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S1.T1.1.6.5.1">Llama 2 7B 2-shot, 10 beams</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.6.5.2">20.1</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S1.T1.1.7.6.1">Mistral-7B-v0.1 2-shot, 10 beams</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.7.6.2">24.9</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S1.T1.1.8.7.1">gpt-4 10-shot</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.8.7.2">29.5</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S1.T1.1.9.8.1">gpt-4-turbo-preview 0-shot</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.9.8.2">30.4</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_t" id="S1.T1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S1.T1.1.10.9.1.1">Pretrained encoder-decoder</span></td>
<td class="ltx_td ltx_border_t" id="S1.T1.1.10.9.2"></td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S1.T1.1.11.10.1">NLLB-3B, 10 beams</td>
<td class="ltx_td ltx_align_left" id="S1.T1.1.11.10.2">30.6</td>
</tr>
<tr class="ltx_tr" id="S1.T1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b" id="S1.T1.1.12.11.1">OPUS-MT, 10 beams</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S1.T1.1.12.11.2"><span class="ltx_text ltx_font_bold" id="S1.T1.1.12.11.2.1">32.2</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Main results. Our Dragoman models improve existing state of the art on translation from English to Ukrainian on FLORES-101 devtest <cite class="ltx_cite ltx_citemacro_citep">(Goyal etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib8" title="">2022</a>)</cite>, a multilingual benchmark of translated sentences from web articles. We compare to state of the art encoder-decoder models, NLLB-3B <cite class="ltx_cite ltx_citemacro_cite">Team etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib30" title="">2022</a>)</cite> and OPUS-MT <cite class="ltx_cite ltx_citemacro_cite">Tiedemann and Thottingal (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib32" title="">2020</a>)</cite>.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">2.Â Â Â Supervised Finetuning</h2>
<figure class="ltx_table" id="S2.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T2.1" style="width:455.2pt;height:88.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.8pt,5.6pt) scale(0.88765417123475,0.88765417123475) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.2.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.3.1">Pairs</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="4" id="S2.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.4.1">Filters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.5.1">Example Order</span></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t" id="S2.T2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.1.1.1">Best BLEU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S2.T2.1.1.1.1.1.1.m1.1"><semantics id="S2.T2.1.1.1.1.1.1.m1.1a"><mo id="S2.T2.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="S2.T2.1.1.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S2.T2.1.1.1.1.1.1.m1.1b"><ci id="S2.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T2.1.1.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T2.1.1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S2.T2.1.1.1.1.1.1.m1.1d">â†‘</annotation></semantics></math></span></th>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.1.1">Lang</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T2.1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.2.1">BPC</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T2.1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.3.1">LaBSE</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S2.T2.1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.2.1.4.1">Len diff</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T2.1.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.3.1.1">1m unfiltered</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.1.1.3.1.2">963k</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.3.1.3">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.1.1.3.1.4">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.3.1.5">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.1.1.3.1.6">-</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T2.1.1.3.1.7">Random</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S2.T2.1.1.3.1.8">28.26</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.4.2">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.4.2.1">1m filtered</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.4.2.2">958k</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.4.2.3">En/Uk</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.4.2.4">&lt;3.33</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.4.2.5">&gt;0.91</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.4.2.6">&lt;50</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.4.2.7">Random</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.4.2.8">29.47</td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.5.3">
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.5.3.1">3m filtered</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.5.3.2">2.9m</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.5.3.3">En/Uk</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.5.3.4">&lt;3.25</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.5.3.5">&gt;0.85</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.5.3.6">&lt;50</td>
<td class="ltx_td ltx_align_left" id="S2.T2.1.1.5.3.7">By LaBSE score, dissimilar first</td>
<td class="ltx_td ltx_align_right" id="S2.T2.1.1.5.3.8"><span class="ltx_text ltx_font_bold" id="S2.T2.1.1.5.3.8.1">30.37</span></td>
</tr>
<tr class="ltx_tr" id="S2.T2.1.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.1.1.6.4.1">8m filtered</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S2.T2.1.1.6.4.2">8m</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.1.1.6.4.3">En/Uk</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S2.T2.1.1.6.4.4">&lt;5</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.1.1.6.4.5">&gt;0.5</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S2.T2.1.1.6.4.6">&lt;50</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S2.T2.1.1.6.4.7">By LaBSE score, dissimilar first</td>
<td class="ltx_td ltx_align_right ltx_border_b" id="S2.T2.1.1.6.4.8">30.19</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span> Summary of experiments with Paracrawl subcorpora. Legend of filters: <span class="ltx_text ltx_font_bold" id="S2.T2.6.1">Lang</span> denotes language filters, <span class="ltx_text ltx_font_bold" id="S2.T2.7.2">BPC</span> denotes maximum sum of bits per character measures, <span class="ltx_text ltx_font_bold" id="S2.T2.8.3">LaBSE</span> denotes maximum sentence embedding cosine similarity between source and target sentences, <span class="ltx_text ltx_font_bold" id="S2.T2.9.4">Len diff</span> denotes maximum difference in length between source and target in characters. Example ordering impacts data loading in the training loop.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.3">We cast machine translation as a likelihood maximization of a density <math alttext="\operatorname{p}" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" mathvariant="normal" xref="S2.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">p</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\operatorname{p}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">roman_p</annotation></semantics></math> of Ukrainian sentences <math alttext='Y="\text{{Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´ĞµĞ½Ğµ Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ}}"\in\mathcal{Y}' class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">Y</mi><mo id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">=</mo><mrow id="S2.p1.2.m2.1.1.4" xref="S2.p1.2.m2.1.1.4.cmml"><mi id="S2.p1.2.m2.1.1.4.2" mathvariant="normal" xref="S2.p1.2.m2.1.1.4.2.cmml">"</mi><mo id="S2.p1.2.m2.1.1.4.1" xref="S2.p1.2.m2.1.1.4.1.cmml">â¢</mo><mtext id="S2.p1.2.m2.1.1.4.3" xref="S2.p1.2.m2.1.1.4.3a.cmml">Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´ĞµĞ½Ğµ Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ</mtext><mo id="S2.p1.2.m2.1.1.4.1a" xref="S2.p1.2.m2.1.1.4.1.cmml">â¢</mo><mi id="S2.p1.2.m2.1.1.4.4" mathvariant="normal" xref="S2.p1.2.m2.1.1.4.4.cmml">"</mi></mrow><mo id="S2.p1.2.m2.1.1.5" xref="S2.p1.2.m2.1.1.5.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1.6" xref="S2.p1.2.m2.1.1.6.cmml">ğ’´</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><and id="S2.p1.2.m2.1.1a.cmml" xref="S2.p1.2.m2.1.1"></and><apply id="S2.p1.2.m2.1.1b.cmml" xref="S2.p1.2.m2.1.1"><eq id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3"></eq><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ğ‘Œ</ci><apply id="S2.p1.2.m2.1.1.4.cmml" xref="S2.p1.2.m2.1.1.4"><times id="S2.p1.2.m2.1.1.4.1.cmml" xref="S2.p1.2.m2.1.1.4.1"></times><ci id="S2.p1.2.m2.1.1.4.2.cmml" xref="S2.p1.2.m2.1.1.4.2">"</ci><ci id="S2.p1.2.m2.1.1.4.3a.cmml" xref="S2.p1.2.m2.1.1.4.3"><mtext id="S2.p1.2.m2.1.1.4.3.cmml" xref="S2.p1.2.m2.1.1.4.3">Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´ĞµĞ½Ğµ Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ</mtext></ci><ci id="S2.p1.2.m2.1.1.4.4.cmml" xref="S2.p1.2.m2.1.1.4.4">"</ci></apply></apply><apply id="S2.p1.2.m2.1.1c.cmml" xref="S2.p1.2.m2.1.1"><in id="S2.p1.2.m2.1.1.5.cmml" xref="S2.p1.2.m2.1.1.5"></in><share href="https://arxiv.org/html/2404.15196v2#S2.p1.2.m2.1.1.4.cmml" id="S2.p1.2.m2.1.1d.cmml" xref="S2.p1.2.m2.1.1"></share><ci id="S2.p1.2.m2.1.1.6.cmml" xref="S2.p1.2.m2.1.1.6">ğ’´</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">Y="\text{{Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´ĞµĞ½Ğµ Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ}}"\in\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">italic_Y = " Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´ĞµĞ½Ğµ Ñ€ĞµÑ‡ĞµĞ½Ğ½Ñ " âˆˆ caligraphic_Y</annotation></semantics></math> conditioned on their English sources with quasi-instruction formatting: <math alttext='X="\text{[INST] translated sentence [/INST]}"\in\mathcal{X}' class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">X</mi><mo id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml">=</mo><mrow id="S2.p1.3.m3.1.1.4" xref="S2.p1.3.m3.1.1.4.cmml"><mi id="S2.p1.3.m3.1.1.4.2" mathvariant="normal" xref="S2.p1.3.m3.1.1.4.2.cmml">"</mi><mo id="S2.p1.3.m3.1.1.4.1" xref="S2.p1.3.m3.1.1.4.1.cmml">â¢</mo><mtext id="S2.p1.3.m3.1.1.4.3" xref="S2.p1.3.m3.1.1.4.3a.cmml">[INST] translated sentence [/INST]</mtext><mo id="S2.p1.3.m3.1.1.4.1a" xref="S2.p1.3.m3.1.1.4.1.cmml">â¢</mo><mi id="S2.p1.3.m3.1.1.4.4" mathvariant="normal" xref="S2.p1.3.m3.1.1.4.4.cmml">"</mi></mrow><mo id="S2.p1.3.m3.1.1.5" xref="S2.p1.3.m3.1.1.5.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1.6" xref="S2.p1.3.m3.1.1.6.cmml">ğ’³</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><and id="S2.p1.3.m3.1.1a.cmml" xref="S2.p1.3.m3.1.1"></and><apply id="S2.p1.3.m3.1.1b.cmml" xref="S2.p1.3.m3.1.1"><eq id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3"></eq><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">ğ‘‹</ci><apply id="S2.p1.3.m3.1.1.4.cmml" xref="S2.p1.3.m3.1.1.4"><times id="S2.p1.3.m3.1.1.4.1.cmml" xref="S2.p1.3.m3.1.1.4.1"></times><ci id="S2.p1.3.m3.1.1.4.2.cmml" xref="S2.p1.3.m3.1.1.4.2">"</ci><ci id="S2.p1.3.m3.1.1.4.3a.cmml" xref="S2.p1.3.m3.1.1.4.3"><mtext id="S2.p1.3.m3.1.1.4.3.cmml" xref="S2.p1.3.m3.1.1.4.3">[INST] translated sentence [/INST]</mtext></ci><ci id="S2.p1.3.m3.1.1.4.4.cmml" xref="S2.p1.3.m3.1.1.4.4">"</ci></apply></apply><apply id="S2.p1.3.m3.1.1c.cmml" xref="S2.p1.3.m3.1.1"><in id="S2.p1.3.m3.1.1.5.cmml" xref="S2.p1.3.m3.1.1.5"></in><share href="https://arxiv.org/html/2404.15196v2#S2.p1.3.m3.1.1.4.cmml" id="S2.p1.3.m3.1.1d.cmml" xref="S2.p1.3.m3.1.1"></share><ci id="S2.p1.3.m3.1.1.6.cmml" xref="S2.p1.3.m3.1.1.6">ğ’³</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">X="\text{[INST] translated sentence [/INST]}"\in\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">italic_X = " [INST] translated sentence [/INST] " âˆˆ caligraphic_X</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The density is parametrized using a neural network with frozen pretrained weights <math alttext="\theta" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_Î¸</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\operatorname{argmax}_{\phi}\operatorname{p}_{\theta,\phi}(Y|X)" class="ltx_Math" display="block" id="S2.E1.m1.4"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><msub id="S2.E1.m1.4.4.3" xref="S2.E1.m1.4.4.3.cmml"><mi id="S2.E1.m1.4.4.3.2" xref="S2.E1.m1.4.4.3.2.cmml">argmax</mi><mi id="S2.E1.m1.4.4.3.3" xref="S2.E1.m1.4.4.3.3.cmml">Ï•</mi></msub><mo id="S2.E1.m1.4.4a" lspace="0.167em" xref="S2.E1.m1.4.4.cmml">â¡</mo><mrow id="S2.E1.m1.4.4.2.2" xref="S2.E1.m1.4.4.2.3.cmml"><msub id="S2.E1.m1.3.3.1.1.1" xref="S2.E1.m1.3.3.1.1.1.cmml"><mi id="S2.E1.m1.3.3.1.1.1.2" mathvariant="normal" xref="S2.E1.m1.3.3.1.1.1.2.cmml">p</mi><mrow id="S2.E1.m1.2.2.2.4" xref="S2.E1.m1.2.2.2.3.cmml"><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">Î¸</mi><mo id="S2.E1.m1.2.2.2.4.1" xref="S2.E1.m1.2.2.2.3.cmml">,</mo><mi id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.2.cmml">Ï•</mi></mrow></msub><mo id="S2.E1.m1.4.4.2.2a" xref="S2.E1.m1.4.4.2.3.cmml">â¡</mo><mrow id="S2.E1.m1.4.4.2.2.2" xref="S2.E1.m1.4.4.2.3.cmml"><mo id="S2.E1.m1.4.4.2.2.2.2" stretchy="false" xref="S2.E1.m1.4.4.2.3.cmml">(</mo><mrow id="S2.E1.m1.4.4.2.2.2.1" xref="S2.E1.m1.4.4.2.2.2.1.cmml"><mi id="S2.E1.m1.4.4.2.2.2.1.2" xref="S2.E1.m1.4.4.2.2.2.1.2.cmml">Y</mi><mo fence="false" id="S2.E1.m1.4.4.2.2.2.1.1" xref="S2.E1.m1.4.4.2.2.2.1.1.cmml">|</mo><mi id="S2.E1.m1.4.4.2.2.2.1.3" xref="S2.E1.m1.4.4.2.2.2.1.3.cmml">X</mi></mrow><mo id="S2.E1.m1.4.4.2.2.2.3" stretchy="false" xref="S2.E1.m1.4.4.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4"><apply id="S2.E1.m1.4.4.3.cmml" xref="S2.E1.m1.4.4.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.1.cmml" xref="S2.E1.m1.4.4.3">subscript</csymbol><ci id="S2.E1.m1.4.4.3.2.cmml" xref="S2.E1.m1.4.4.3.2">argmax</ci><ci id="S2.E1.m1.4.4.3.3.cmml" xref="S2.E1.m1.4.4.3.3">italic-Ï•</ci></apply><apply id="S2.E1.m1.4.4.2.3.cmml" xref="S2.E1.m1.4.4.2.2"><apply id="S2.E1.m1.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.1.1.1.2">p</ci><list id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.4"><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ğœƒ</ci><ci id="S2.E1.m1.2.2.2.2.cmml" xref="S2.E1.m1.2.2.2.2">italic-Ï•</ci></list></apply><apply id="S2.E1.m1.4.4.2.2.2.1.cmml" xref="S2.E1.m1.4.4.2.2.2.1"><csymbol cd="latexml" id="S2.E1.m1.4.4.2.2.2.1.1.cmml" xref="S2.E1.m1.4.4.2.2.2.1.1">conditional</csymbol><ci id="S2.E1.m1.4.4.2.2.2.1.2.cmml" xref="S2.E1.m1.4.4.2.2.2.1.2">ğ‘Œ</ci><ci id="S2.E1.m1.4.4.2.2.2.1.3.cmml" xref="S2.E1.m1.4.4.2.2.2.1.3">ğ‘‹</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">\operatorname{argmax}_{\phi}\operatorname{p}_{\theta,\phi}(Y|X)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.4d">roman_argmax start_POSTSUBSCRIPT italic_Ï• end_POSTSUBSCRIPT roman_p start_POSTSUBSCRIPT italic_Î¸ , italic_Ï• end_POSTSUBSCRIPT ( italic_Y | italic_X )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.2">We implement the conditional language modeling objective by masking out tokens of <math alttext="X" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_X</annotation></semantics></math> when computing token-wise cross entropy of shifted targets.
We only optimize extra low rank adapter <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib10" title="">2022</a>)</cite> parameters <math alttext="\phi" class="ltx_Math" display="inline" id="S2.p3.2.m2.1"><semantics id="S2.p3.2.m2.1a"><mi id="S2.p3.2.m2.1.1" xref="S2.p3.2.m2.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.1b"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.1d">italic_Ï•</annotation></semantics></math> after nf4 quantization <cite class="ltx_cite ltx_citemacro_citep">(Dettmers etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib5" title="">2023</a>)</cite>. In practice we use large rank values and adapter mixture weights. All training runs proceed for one epoch and we use dropout <cite class="ltx_cite ltx_citemacro_citep">(Srivastava etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib28" title="">2014</a>)</cite> for regularization against data noise.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">We use Mistral-7B-v0.1 <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib11" title="">2023</a>)</cite> as a base pretrained decoder-only transformer, as it performs favorably in our few-shot experiments (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5" title="5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 5</span></a>).</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">3.Â Â Â First Phase: Heuristic Filtering of Paracrawl</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We use the publicly available Paracrawl dataset <cite class="ltx_cite ltx_citemacro_cite">BaÃ±Ã³n etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib1" title="">2020</a>)</cite>. This dataset contains 13,354,365 English-Ukrainian sentence pairs, collected by automatically matching similar sentences in large corpora of internet text.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">We have identified issues with translation pairs, including a significant number of repetitive or incorrect examples. We encounter a large subset of repetitive weather forecasts following the template â€œThe temperature in &lt;x&gt; is &lt;y&gt; degrees,â€ and sentences from site navigation menus. Additionally, many texts appear to be scraped from adult websites, containing low-quality, machine-translated samples. We have spotted numerous instances of incomplete or significantly incorrect translation pairs. Some target sentences were written in languages other than Ukrainian.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To control the quality of the sentences, we apply multiple heuristics.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Language filtering</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">gcld3 library<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google/cld3" title="">https://github.com/google/cld3</a></span></span></span> provides language detection capabilities. We remove all sentences that failed to verify as Ukrainian or English.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Perplexity thresholding</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.1">We score source and target sentences using two decoder-only models trained on different monolingual datasets <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib24" title="">2019</a>); Minixhofer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib19" title="">2022</a>)</cite> and sum their bits per character measures.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Translation mismatch filtering</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">LaBSE <cite class="ltx_cite ltx_citemacro_cite">Feng etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib6" title="">2022</a>)</cite> embeds sentences into a space, where similar sentences in different languages are close together. We use it to filter out badly aligned sentence pairs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Length filtering</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p1.1">The lengths of the original and translated sentences reveal examples that are too short or too long. Absolute differences of lengths point to pairs with long target for the short source and vice versa.</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px4.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px4.p2.1">We arbitrarily choose joint values of filtering thresholds to get the desired approximate example counts: 1 million, 3 million and 8 million. We perform multiple experiments with these splits while searching for optimal hyperparameters. We list threshold values in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S2.T2" title="Table 2 â€£ 2. Supervised Finetuning â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a> and best results for each subset.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">4.Â Â Â Second Phase: Unsupervised Data Selection on Extended Multi30K</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We use the best checkpoint from the previous finetuning phase to train on a high-quality dataset: Extended Multi30K from <cite class="ltx_cite ltx_citemacro_citet">Saichyshyna etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib27" title="">2023</a>)</cite>. Switching datasets gives us a performance boost of 1.97 BLEU. We additionally delete 11600 sentences from the dataset using unsupervised perplexity filtering pipeline gaining 0.35 on the dev set that translates to 0.3 BLEU on the devtest subset of FLORES.</p>
</div>
<figure class="ltx_figure" id="S4.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="322" id="S4.F1.g1" src="x1.png" width="427"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Distributions of sentence log probabilities for each fold superimposed on top of each other. Every bar color represents a unique fold; every vertical line denotes a 60<sup class="ltx_sup" id="S4.F1.2.1">th</sup> percentile cutoff threshold. The best percentile is chosen using grid search shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4.T3" title="Table 3 â€£ 4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 3</span></a>.</figcaption>
</figure>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.3">We use perplexity as a data selection criterion to calculate thresholds to filter out highly surprising sentences.
We apply the <math alttext="k" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_k</annotation></semantics></math>-fold cross-validation technique to make the perplexity evaluation in-domain. We split the training data into <math alttext="k=5" class="ltx_Math" display="inline" id="S4.p2.2.m2.1"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">k</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><eq id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></eq><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ‘˜</ci><cn id="S4.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S4.p2.2.m2.1d">italic_k = 5</annotation></semantics></math> folds and train <math alttext="k" class="ltx_Math" display="inline" id="S4.p2.3.m3.1"><semantics id="S4.p2.3.m3.1a"><mi id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><ci id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.p2.3.m3.1d">italic_k</annotation></semantics></math> models withholding one of the folds from each run. Then we score every sentence using the model that has not seen that sentence in training. Next, we sweep for acceptable threshold values by minimizing BLEU on the development set and report results in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4.T3" title="Table 3 â€£ 4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 3</span></a>. We plot the distribution of scores in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4.F1" title="Figure 1 â€£ 4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a>. We also provide threshold sweep results for training from base Mistral-7B-v0.1 checkpoint in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S11.T6" title="Table 6 â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 6</span></a>. By comparing finetuned results, we demonstrate that data from the second phase alone is not enough to match the performance of our best checkpoint.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.2.1">Threshold</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.3.1">Examples</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.1.1">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.m1.1.1" stretchy="false" xref="S4.T3.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_border_t" id="S4.T3.1.1.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.3.1.1">percentile</th>
<th class="ltx_td ltx_th ltx_th_row" id="S4.T3.2.3.1.2"></th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.3.1.3">dev</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.3.1.4">devtest</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.4.2.1">20<sup class="ltx_sup" id="S4.T3.2.4.2.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.2.4.2.2">5800</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.4.2.3">31.57</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.2.4.2.4">32.06</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.5.3.1">40<sup class="ltx_sup" id="S4.T3.2.5.3.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.5.3.2">11600</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.3">31.65</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.5.3.4">32.16</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.6.4.1">50<sup class="ltx_sup" id="S4.T3.2.6.4.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.6.4.2">14500</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.3">31.76</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.6.4.4">32.36</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.7.5.1">60<sup class="ltx_sup" id="S4.T3.2.7.5.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.7.5.2">17400</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S4.T3.2.7.5.3.1">31.80</span></td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.7.5.4"><span class="ltx_text ltx_font_bold" id="S4.T3.2.7.5.4.1">32.34</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.8.6.1">70<sup class="ltx_sup" id="S4.T3.2.8.6.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.8.6.2">20300</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.3">31.51</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.8.6.4">32.17</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.9.7.1">80<sup class="ltx_sup" id="S4.T3.2.9.7.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.9.7.2">23200</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.3">31.44</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.9.7.4">32.46</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.1">95.4<sup class="ltx_sup" id="S4.T3.2.2.1.1">th</sup> (<math alttext="2\sigma" class="ltx_Math" display="inline" id="S4.T3.2.2.1.m1.1"><semantics id="S4.T3.2.2.1.m1.1a"><mrow id="S4.T3.2.2.1.m1.1.1" xref="S4.T3.2.2.1.m1.1.1.cmml"><mn id="S4.T3.2.2.1.m1.1.1.2" xref="S4.T3.2.2.1.m1.1.1.2.cmml">2</mn><mo id="S4.T3.2.2.1.m1.1.1.1" xref="S4.T3.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S4.T3.2.2.1.m1.1.1.3" xref="S4.T3.2.2.1.m1.1.1.3.cmml">Ïƒ</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.1.m1.1b"><apply id="S4.T3.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1"><times id="S4.T3.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.1.m1.1.1.1"></times><cn id="S4.T3.2.2.1.m1.1.1.2.cmml" type="integer" xref="S4.T3.2.2.1.m1.1.1.2">2</cn><ci id="S4.T3.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.1.m1.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.1.m1.1c">2\sigma</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.1.m1.1d">2 italic_Ïƒ</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.2.2">28025</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.2.3">31.74</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.2.4">32.18</td>
</tr>
<tr class="ltx_tr" id="S4.T3.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.10.8.1">Full dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.2.10.8.2">29000</th>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.3">31.45</td>
<td class="ltx_td ltx_align_left" id="S4.T3.2.10.8.4">32.04</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Extended Multi30K log probability thresholds swept on FLORES dev set. We choose the best checkpoint based on model performance on FLORES dev subset using grid search for optimal perplexity threshold value.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">5.Â Â Â Few-Shot Translation</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Conditioning the model on a sequence of demonstrations of performing some task allows the model to learn this task in-context, also known as â€œfew-shot learningâ€ (e.g. <cite class="ltx_cite ltx_citemacro_citet">Brown etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib2" title="">2020</a>)</cite>), thanks to the ability of the Transformers to modulate representations of its future tokens using past context, implementing a specialized internal context-dependent learning algorithm inside its weights <cite class="ltx_cite ltx_citemacro_citep">(von Oswald etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib34" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">While few-shot learning allows to quickly try any task with a low number of demonstrations, <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib15" title="">2022</a>)</cite> have shown that parameter-efficient finetuning allows smaller models achieve better performance, effectively spending less floating point operations per test example at inference time.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">Setting up the model for finetuning requires a lot of work, and in-context learning allows to quickly probe capability of a large model using inference software that performs efficient management of key-value cache for speed <cite class="ltx_cite ltx_citemacro_cite">Kwon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib14" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">To test backbone models before finetuning, we attempt decoding translations with a basic prompt shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5.F2" title="Figure 2 â€£ 5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<p class="ltx_p" id="S5.F2.1">[INST] They are planning to host a party next weekend. [/INST] Ğ’Ğ¾Ğ½Ğ¸ Ğ¿Ğ»Ğ°Ğ½ÑƒÑÑ‚ÑŒ Ğ¿Ñ€Ğ¾Ğ²ĞµÑÑ‚Ğ¸ Ğ²ĞµÑ‡Ñ–Ñ€ĞºÑƒ Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¾Ğ³Ğ¾ Ğ²Ñ–ĞºĞµĞ½Ğ´Ñƒ. 
<br class="ltx_break"/>[INST] I enjoy swimming in the ocean and feeling the salty breeze. [/INST] ĞœĞµĞ½Ñ– Ğ¿Ğ¾Ğ´Ğ¾Ğ±Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ¿Ğ»Ğ°Ğ²Ğ°Ñ‚Ğ¸ Ğ² Ğ¾ĞºĞµĞ°Ğ½Ñ– Ñ‚Ğ° Ğ²Ñ–Ğ´Ñ‡ÑƒĞ²Ğ°Ñ‚Ğ¸ ÑĞ¾Ğ»Ğ¾Ğ½Ğ¸Ğ¹ Ğ²Ñ–Ñ‚ĞµÑ€. 
<br class="ltx_break"/>[INST]</p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Basic 2-shot prompt used for few-shot translation. <span class="ltx_text ltx_font_typewriter" id="S5.F2.4.1">[INST]</span> prefixes the beginning of the source sentence and <span class="ltx_text ltx_font_typewriter" id="S5.F2.5.2">[/INST]</span> denotes the beginning of the target translation. These separators are chosen arbitrarily (as in finetuning) and are not special vocabulary items, even though they bear visual resemblance to them.</figcaption>
</figure>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">We find that the model significantly underperforms compared to current state of the art translation models when using beam search <cite class="ltx_cite ltx_citemacro_citep">(Tillmann and Ney, <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib33" title="">2003</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">This decoding algorithm performs pruned breadth-first expansion, scoring target sentence prefixes using modelâ€™s own log probability, approximating maximum a-posteriori estimation of the best translation.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1">Inspection of the n-best list of translation candidates (beams) reveals that the models can produce high-quality translations, however assign low probabilities to them. We find the best possible translation by rescoring beams using the BLEU score as a loss function <cite class="ltx_cite ltx_citemacro_citep">(Kumar and Byrne, <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib13" title="">2004</a>)</cite> with respect to the reference translation (the so-called â€œoracleâ€).</p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">We employ this oracle rescoring strategy to gauge the potential capability of the model to produce good translations without finetuning, and find that in a regime of increased computation (large width of the beam) and assuming perfect selection capability, a base model is competitive with specialized alternatives. We sweep over a grid of multiple beam widths and report highest attainable BLEU scores in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5.T4" title="Table 4 â€£ 5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 4</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T4.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T4.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.2.1">Beams</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1">Oracle BLEU <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">â†‘</annotation></semantics></math></span></th>
<td class="ltx_td ltx_border_t" id="S5.T4.1.1.3"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.2.1">
<th class="ltx_td ltx_th ltx_th_row" id="S5.T4.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.1.2.1.2">Mistral-7B-v.01</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.1.2.1.3">Llama 2 7B</th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T4.1.3.2.1">3</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.2">27.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.3.2.3">24.55</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.4.3.1">5</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.2">29.20</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.4.3.3">26.64</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.5.4.1">10</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.2">31.53</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.5.4.3">28.76</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.6.5.1">15</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.2">32.81</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.6.5.3">29.09</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.7.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.7.6.1">20</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.2">33.54</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.7.6.3">27.64</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.8.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.8.7.1">25</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.2">34.27</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.8.7.3">26.35</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.9.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.9.8.1">30</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.9.8.2">33.99</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.9.8.3"><span class="ltx_text" id="S5.T4.1.9.8.3.1" style="font-size:90%;">(decoder failure)</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.10.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T4.1.10.9.1">35</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.10.9.2">34.94</td>
<td class="ltx_td" id="S5.T4.1.10.9.3"></td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.11.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b" id="S5.T4.1.11.10.1">40</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T4.1.11.10.2">34.61</td>
<td class="ltx_td ltx_border_b" id="S5.T4.1.11.10.3"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>We establish the upper bound of the latent capability of pretrained base models to produce high quality translations with by varying beam width on the task of translating sentences from FLORES dev given a 2-shot prompt. The ground truth oracle determines the best beam. We use beam search implementation by <cite class="ltx_cite ltx_citemacro_citet">Kwon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib14" title="">2023</a>)</cite> with presence penalty of 0.1.
The results do not improve monotonically with increasing beam size, and lengths of hypotheses grow with maximum beam size, yielding diminishing returns. This problem can be attributed to label bias <cite class="ltx_cite ltx_citemacro_cite">Murray and Chiang (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib20" title="">2018</a>)</cite>, and rectifying it will require extra regularization.</figcaption>
</figure>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1">Consecutive sentences in FLORES are samples from the same document. We hypothesize, dynamically adjusting the prompt by inserting previous translations will improve results. We observe that the model indeed improves translation of certain words such as proper nouns through access to correct definitions provided in the context (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5.F3" title="Figure 3 â€£ 5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a>), however its overall performance degrades in other examples.</p>
</div>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F3.1">Source:
<span class="ltx_text ltx_font_smallcaps ltx_framed ltx_framed_underline" id="S5.F3.1.1" style="font-size:90%;">RSPCA</span><span class="ltx_text ltx_font_smallcaps" id="S5.F3.1.2" style="font-size:90%;"> New South Wales chief inspector David Oâ€™Shannessy told the ABC that surveillance and inspections of abattoirs should be commonplace in Australia.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F3.2">Hypothesis given random 2-shot context: Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ñ–Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¾Ñ€ <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.2.1" style="color:#FF0000;">Ğ Ğ¡ĞŸĞšĞ</span><span class="ltx_text" id="S5.F3.2.2" style="color:#FF0000;"> ĞĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞŸÑ–Ğ²Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ£ĞµĞ»ÑŒÑÑƒ Ğ”ĞµĞ²Ñ–Ğ´ Ğâ€™Ğ¨ĞµĞ½Ğ½ĞµÑÑ– Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ğ² ABC, Ñ‰Ğ¾ ÑĞ¿Ğ¾ÑÑ‚ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ‚Ğ° Ñ–Ğ½ÑĞ¿ĞµĞºÑ†Ñ–Ñ— Ğ°Ğ±Ğ±Ğ°Ñ‚ÑÑ‚Ğ² Ğ¿Ğ¾Ğ²Ğ¸Ğ½Ğ½Ñ– Ğ±ÑƒÑ‚Ğ¸ Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¸Ğ¼ ÑĞ²Ğ¸Ñ‰ĞµĞ¼ Ğ² ĞĞ²ÑÑ‚Ñ€Ğ°Ğ»Ñ–Ñ—.</span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F3.3">Context example: <span class="ltx_text ltx_font_smallcaps" id="S5.F3.3.1" style="font-size:90%;">[INST] Animal Liberation and the <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.1">Royal Society for the Prevention of Cruelty</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.2">to Animals</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.3">(RSPCA)</span> are again calling for the mandatory installation of CCTV cameras in all Australian abattoirs. [/INST]<span class="ltx_text ltx_font_upright" id="S5.F3.3.1.4"> ĞÑ€Ğ³Ğ°Ğ½Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ—Ğ²Ñ–Ğ»ÑŒĞ½ĞµĞ½Ğ½Ñ Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½ Ñ– <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.4.1">ĞšĞ¾Ñ€Ğ¾Ğ»Ñ–Ğ²ÑÑŒĞºĞµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¸ÑÑ‚Ğ²Ğ¾ Ñ–Ğ· Ğ·Ğ°Ğ¿Ğ¾Ğ±Ñ–Ğ³Ğ°Ğ½Ğ½Ñ</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.4.2">Ğ¶Ğ¾Ñ€ÑÑ‚Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ· Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½Ğ°Ğ¼Ğ¸</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.3.1.4.3">(ĞšĞ¢Ğ—Ğ–ĞŸĞ¢)</span> Ğ·Ğ½Ğ¾Ğ²Ñƒ Ğ·Ğ°ĞºĞ»Ğ¸ĞºĞ°ÑÑ‚ÑŒ Ğ´Ğ¾ Ğ¾Ğ±Ğ¾Ğ²â€™ÑĞ·ĞºĞ¾Ğ²Ğ¾Ñ— ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ¸ ĞºĞ°Ğ¼ĞµÑ€ ÑĞ¿Ğ¾ÑÑ‚ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ğ½Ğ° Ğ²ÑÑ–Ñ… Ğ°Ğ²ÑÑ‚Ñ€Ğ°Ğ»Ñ–Ğ¹ÑÑŒĞºĞ¸Ñ… Ğ±Ñ–Ğ¹Ğ½ÑÑ….</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.F3.4">Hypothesis given relevant 2-shot context: <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.4.1" style="font-size:90%;">Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ñ–Ğ½ÑĞ¿ĞµĞºÑ‚Ğ¾Ñ€ ĞšĞ¾Ñ€Ğ¾Ğ»Ñ–Ğ²ÑÑŒĞºĞ¾Ğ³Ğ¾ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¸ÑÑ‚Ğ²Ğ°</span><span class="ltx_text" id="S5.F3.4.2" style="font-size:90%;"> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.4.2.1">Ñ–Ğ· Ğ·Ğ°Ğ¿Ğ¾Ğ±Ñ–Ğ³Ğ°Ğ½Ğ½Ñ Ğ¶Ğ¾Ñ€ÑÑ‚Ğ¾ĞºĞ¾Ğ¼Ñƒ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ· Ñ‚Ğ²Ğ°Ñ€Ğ¸Ğ½Ğ°Ğ¼Ğ¸</span> <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.F3.4.2.2" style="color:#0000FF;">(ĞšĞ¢Ğ—Ğ–ĞŸĞ¢)</span><span class="ltx_text" id="S5.F3.4.2.3" style="color:#0000FF;"> ĞĞ¾Ğ²Ğ¾Ğ³Ğ¾ ĞŸÑ–Ğ²Ğ´ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ£ĞµĞ»ÑŒÑÑƒ Ğ”ĞµĞ²Ñ–Ğ´ Ğâ€™Ğ¨ĞµĞ½Ğ½ĞµÑÑ– Ğ·Ğ°ÑĞ²Ğ¸Ğ², Ñ‰Ğ¾ ÑĞ¿Ğ¾ÑÑ‚ĞµÑ€ĞµĞ¶ĞµĞ½Ğ½Ñ Ñ‚Ğ° Ñ–Ğ½ÑĞ¿ĞµĞºÑ†Ñ–Ñ— Ğ±Ñ–Ğ¹Ğ½Ñ– Ğ¿Ğ¾Ğ²Ğ¸Ğ½Ğ½Ñ– Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾ÑˆĞ¸Ñ€ĞµĞ½Ğ¸Ğ¼Ğ¸ Ğ² ĞĞ²ÑÑ‚Ñ€Ğ°Ğ»Ñ–Ñ—.</span></span></p>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Few-shot translation with contextual prompting allows the model to learn named entities on the fly. Without context, the model makes a wrong guess trying to transliterate the abbreviation.</figcaption>
</figure>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p" id="S5.p10.1">We additionally attempt basic 0-shot with a system prompt <span class="ltx_text ltx_font_typewriter" id="S5.p10.1.1">You translate English sentences into native Ukrainian.</span>, and 10-shot prompting using automatic prompt selection based on similarity between source sentences experiments with GPT-4 and GPT-4 Turbo and find that commerical systems perform similarly to other open source systems, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S1.T1" title="Table 1 â€£ 1. Introduction â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">6.Â Â Â Discussion and Limitations</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Single-sentence translation</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Our system is trained on demonstrations of standalone sentence pairs.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Decoder-only models with long context windows</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">We choose to finetune existing decoder-only models since the choice of models with almost the same architecture but different massive pretraining data is abundant. The number of open-source models released recently and their constant improvement offers a good prospective for the machine translation tasks.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p2.1">These models receive gradient from all outputs during pretraining, and the self-attention mechanism can see the input, the partial output, and access past examples of translations in its context window using induction heads <cite class="ltx_cite ltx_citemacro_citep">(Olsson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib21" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p3.1">For efficiency, we only train on examples with single short sentence pairs and do not pack context windows full of tokens as done in pretraining. In our early experiments, we find that our models still generalize to inputs longer that what is seen in training. This generalization behavior is often attributed to relative position embeddings <cite class="ltx_cite ltx_citemacro_cite">Dai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib4" title="">2019</a>); CsordÃ¡s etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib3" title="">2021</a>)</cite>. We leave evaluation of long context attention stability under these conditions for future work.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Training on the noisy dataset</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">Data cleaning has a positive effect on the resulting metrics. However, our models trained on 8 million filtered, examples perform worse than models trained on 3 million examples (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S2.T2" title="Table 2 â€£ 2. Supervised Finetuning â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>).</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Tokenizer performance</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px4.p1.1">We used the LLaMA and Mistral tokenizers during our experiments, which use at least twice as many tokens to compress a sentence in Ukrainian of the same length as an English sentence in character. In practice, that means that generating a sentence in Ukrainian takes at least twice as many steps to generate. We show a distribution of sentence token lengths in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S6.F4" title="Figure 4 â€£ Tokenizer performance â€£ 6. Discussion and Limitations â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="243" id="S6.F4.g1" src="x2.png" width="427"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Comparison of tokenizer compression rates between English and Ukrainian using the Mistral-7B tokenizer on the FLORES dev set.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px5">
<h3 class="ltx_title ltx_title_paragraph">Evaluation</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p1.1">We choose BLEU-4 score <cite class="ltx_cite ltx_citemacro_cite">Papineni etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib22" title="">2002</a>)</cite> as our core evaluation metric and model selection criterion. BLEU-4 measures 4-gram precisions, where grams are defined as words. We use the implementation and rely on tokenization decisions of <cite class="ltx_cite ltx_citemacro_citet">Post (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib23" title="">2018</a>)</cite>. This metric is sensitive to minor differences that do not affect the meaning of the sentence, for example case inflections that tend to cascade to multiple adjacent words. BLEU is known to poorly correlate with human judgement of translation quality, and <cite class="ltx_cite ltx_citemacro_citet">Freitag etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib7" title="">2022</a>)</cite> recommend learned metrics.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p2">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p2.1">Choosing an appropriate learned metric for judgements of translation quality of Ukrainian requires careful consideration, and incorporation of data informed by the language community, such as a curated corpus of grammar corrections that reflects proper modern use of language <cite class="ltx_cite ltx_citemacro_cite">Syvokon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS0.SSS0.Px5.p3">
<p class="ltx_p" id="S6.SS0.SSS0.Px5.p3.1">Regardless of limitations of BLEU, improvement in BLEU still signals improvement in translation quality in our regime.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px6">
<h3 class="ltx_title ltx_title_paragraph">WMT22</h3>
<div class="ltx_para" id="S6.SS0.SSS0.Px6.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px6.p1.1">Our reviewers have pointed out that WMT22 benchmark <cite class="ltx_cite ltx_citemacro_cite">Kocmi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib12" title="">2022</a>)</cite> includes a test set for Ukrainian. Our model achieves 24.72 on the WMT22 test set without any postprocessing, ranking behind the best result of <cite class="ltx_cite ltx_citemacro_citet">Roussis and Papavassiliou (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib25" title="">2022</a>)</cite> at 25.2 BLEU. We note that the submission that scores relatively low on the WMT22 test, scores comparably to our results on FLORES. These data distribution properties require closer exploration.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">7.Â Â Â Related Work</h2>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Translation to Ukrainian</h3>
<div class="ltx_para" id="S7.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Maksymenko etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib17" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib18" title="">b</a>)</cite> explore translation controllability by conditioning the model on text embeddings that encode style by finetuning an encoder-decoder model. They claim high quality translations on a private test set.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Instruction-tuned language models</h3>
<div class="ltx_para" id="S7.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px2.p1.1"><cite class="ltx_cite ltx_citemacro_citet">ÃœstÃ¼n etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib36" title="">2024</a>)</cite> explore large-scale translation efforts to produce a multilingual instruction-tuned language model Aya. This work translates large datasets like the Flan Collection <cite class="ltx_cite ltx_citemacro_citep">(Longpre etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib16" title="">2023</a>)</cite> using the NLLB-3B model <cite class="ltx_cite ltx_citemacro_citep">(Team etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib30" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Translation systems</h3>
<div class="ltx_para" id="S7.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Han etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib9" title="">2021</a>)</cite> provide an iterated backtranslation recipe to bootstrap neural machine translation systems using generative models: zero-shot translation ability is used to produce candidates for few-shot demonstrations. Filtered few-shot demonstrations are used to sample new sentences for further finetuning for translation in two directions.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px4">
<h3 class="ltx_title ltx_title_paragraph">Translation benchmarks</h3>
<div class="ltx_para" id="S7.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px4.p1.1">Besides FLORES-101 (<cite class="ltx_cite ltx_citemacro_citet">Goyal etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib8" title="">2022</a>)</cite>, or FLORES-200 <cite class="ltx_cite ltx_citemacro_citep">(Team etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib30" title="">2022</a>)</cite>, both include the same data for Ukrainian) dataset used in this work, <cite class="ltx_cite ltx_citemacro_citet">Tiedemann (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib31" title="">2020</a>)</cite> provides an additional dataset for multilingual evaluation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS0.SSS0.Px5">
<h3 class="ltx_title ltx_title_paragraph">Data selection techniques</h3>
<div class="ltx_para" id="S7.SS0.SSS0.Px5.p1">
<p class="ltx_p" id="S7.SS0.SSS0.Px5.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Yang and Li (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib35" title="">2023</a>)</cite> propose a perplexity filtering pipeline, in which the data is split into k folds to classify low quality augmentation generations produced by surrogate language models. <cite class="ltx_cite ltx_citemacro_citet">Sachdeva etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.15196v2#bib.bib26" title="">2024</a>)</cite> provide recipes on curating data for language models by directly asking language models to score examples.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">8.Â Â Â Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this work, we build a translation system using a two-phase data cleaning pipeline. We demonstrate matching performance to state-of-the-art encoder-decoder models for English-Ukrainian translation task. Notably, our system exhibits superior performance compared to the NLLB model, which was instrumental in generating the Aya dataset and contributed significantly to the advancement of multilingual language models. Improved machine translation could bring new capabilities to the next generation of large language models trained for the Ukrainian language. The recent improvements made for decoder-only backbones and the general dynamics of this process encourages us: we firmly believe that recipes we propose in this paper can be used to improve the quality of the translation by simply upgrading the backbone model.</p>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">9.Â Â Â Contributions</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Yurii Paniv worked on unsupervised data selection on extended Multi30K dataset, Dmytro Chaplynskyi performed initial training using heuristic filtering of Paracrawl, Nikita Trynus worked on evaluation, Volodymyr Kyrylov designed few-shot learning experiments and evaluation.</p>
</div>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">10.Â Â Â Acknowledgements</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">The authors would like to thank Talents for Ukraine project of Kyiv School of Economics for the grant on compute, used for this paper, Ukrainian Catholic University, Roman Kyslyi, and Oleksii Turuta for the fruitful discussions and inspiration. We would also like to thank paper reviewers for their comments.</p>
</div>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_font_bold ltx_title_section" style="font-size:120%;">11.Â Â Â References</h2>
<div class="ltx_para" id="S11.p1">
<span class="ltx_ERROR undefined" id="S11.p1.1">\c@NAT@ctr</span>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography"></h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">BaÃ±Ã³n etÂ al. (2020)</span>
<span class="ltx_bibblock">
Marta BaÃ±Ã³n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel EsplÃ -Gomis, MikelÂ L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio OrtizÂ Rojas, Leopoldo PlaÂ Sempere, Gema RamÃ­rez-SÃ¡nchez, Elsa SarrÃ­as, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.417" title="">ParaCrawl: Web-scale acquisition of parallel corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4555â€“4567, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CsordÃ¡s etÂ al. (2021)</span>
<span class="ltx_bibblock">
RÃ³bert CsordÃ¡s, Kazuki Irie, and Juergen Schmidhuber. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.49" title="">The devil is in the detail: Simple tricks improve systematic generalization of transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 619â€“634, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1285" title="">Transformer-XL: Attentive language models beyond a fixed-length context</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 2978â€“2988, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers etÂ al. (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf" title="">Qlora: Efficient finetuning of quantized llms</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</em>, volumeÂ 36, pages 10088â€“10115. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. (2022)</span>
<span class="ltx_bibblock">
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2007.01852" title="">Language-agnostic bert sentence embedding</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag etÂ al. (2022)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and AndrÃ© F.Â T. Martins. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.2" title="">Results of WMT22 metrics shared task: Stop using BLEU â€“ neural metrics are better and more robust</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 46â€“68, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2022)</span>
<span class="ltx_bibblock">
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, DaÂ Ju, Sanjana Krishnan, Marcâ€™Aurelio Ranzato, Francisco GuzmÃ¡n, and Angela Fan. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00474" title="">The Flores-101 evaluation benchmark for low-resource and multilingual machine translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Transactions of the Association for Computational Linguistics</em>, 10:522â€“538.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han etÂ al. (2021)</span>
<span class="ltx_bibblock">
JesseÂ Michael Han, Igor Babuschkin, Harrison Edwards, Arvind Neelakantan, Tao Xu, Stanislas Polu, Alex Ray, Pranav Shyam, Aditya Ramesh, Alec Radford, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:238583718" title="">Unsupervised neural machine translation with generative language models only</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">ArXiv</em>, abs/2110.05448.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
EdwardÂ J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">LoRA: Low-rank adaptation of large language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2023)</span>
<span class="ltx_bibblock">
AlbertÂ Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, DevendraÂ Singh Chaplot, Diego deÂ las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, LÃ©lioÂ Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, TevenÂ Le Scao, Thibaut Lavril, Thomas Wang, TimothÃ©e Lacroix, and WilliamÂ El Sayed. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06825" title="">Mistral 7b</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, OndÅ™ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal NovÃ¡k, Martin Popel, and Maja PopoviÄ‡. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.1" title="">Findings of the 2022 conference on machine translation (WMT22)</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 1â€“45, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar and Byrne (2004)</span>
<span class="ltx_bibblock">
Shankar Kumar and William Byrne. 2004.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/N04-1022" title="">Minimum Bayes-risk decoding for statistical machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004</em>, pages 169â€“176, Boston, Massachusetts, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, CodyÂ Hao Yu, JosephÂ E. Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.06180" title="">Efficient memory management for large language model serving with PagedAttention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.05638" title="">Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Longpre etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shayne Longpre, LeÂ Hou, TuÂ Vu, Albert Webson, HyungÂ Won Chung, YiÂ Tay, Denny Zhou, QuocÂ V Le, Barret Zoph, Jason Wei, etÂ al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2301.13688" title="">The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2301.13688</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maksymenko etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Daniil Maksymenko, Nataliia Saichyshyna, Oleksii Turuta, Marcin Paprzycki, Maria Ganzha, and Mirela Alhasani. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:264108506" title="">Controllability for english-ukrainian machine translation by using style transfer techniques</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2023 18th Conference on Computer Science and Intelligence Systems (FedCSIS)</em>, pages 1059â€“1068.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maksymenko etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Daniil Maksymenko, Olena Turuta, Nataliia Saichyshyna, Maksym Yerokhin, and Oleksii Turuta. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.multi3generation-1.1" title="">Controllability for English-Ukrainian machine translation based on specialized corpora</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 1st International Workshop on Multilingual, Multimodal and Multitask Language Generation</em>, pages 1â€“9, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minixhofer etÂ al. (2022)</span>
<span class="ltx_bibblock">
Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.293" title="">WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3992â€“4006, Seattle, United States. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murray and Chiang (2018)</span>
<span class="ltx_bibblock">
Kenton Murray and David Chiang. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W18-6322" title="">Correcting length bias in neural machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 212â€“223, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olsson etÂ al. (2022)</span>
<span class="ltx_bibblock">
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html" title="">In-context learning and induction heads</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Transformer Circuits Thread</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni etÂ al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">BLEU: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311â€“318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.aclweb.org/anthology/W18-6319" title="">A call for clarity in reporting BLEU scores</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186â€“191, Belgium, Brussels. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" title="">Language models are unsupervised multitask learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roussis and Papavassiliou (2022)</span>
<span class="ltx_bibblock">
Dimitrios Roussis and Vassilis Papavassiliou. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.31" title="">The ARC-NKUA submission for the English-Ukrainian general machine translation shared task at WMT22</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 358â€“365, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachdeva etÂ al. (2024)</span>
<span class="ltx_bibblock">
Noveen Sachdeva, Benjamin Coleman, Wang-Cheng Kang, Jianmo Ni, Lichan Hong, EdÂ H. Chi, James Caverlee, Julian McAuley, and DerekÂ Zhiyuan Cheng. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.09668" title="">How to train data-efficient llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saichyshyna etÂ al. (2023)</span>
<span class="ltx_bibblock">
Nataliia Saichyshyna, Daniil Maksymenko, Oleksii Turuta, Andriy Yerokhin, Andrii Babii, and Olena Turuta. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.unlp-1.7" title="">Extension Multi30K: Multimodal dataset for integrated vision and language research in Ukrainian</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP)</em>, pages 54â€“61, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava etÂ al. (2014)</span>
<span class="ltx_bibblock">
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://jmlr.org/papers/v15/srivastava14a.html" title="">Dropout: A simple way to prevent neural networks from overfitting</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Journal of Machine Learning Research</em>, 15(56):1929â€“1958.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Syvokon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Oleksiy Syvokon, Olena Nahorna, Pavlo Kuchmiichuk, and Nastasiia Osidach. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.unlp-1.12" title="">UA-GEC: Grammatical error correction and fluency corpus for the Ukrainian language</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the Second Ukrainian Natural Language Processing Workshop (UNLP)</em>, pages 96â€“102, Dubrovnik, Croatia. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team etÂ al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, MartaÂ R. Costa-jussÃ , James Cross, Onur Ã‡elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, AlÂ Youngblood, Bapi Akula, Loic Barrault, GabrielÂ Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, KaushikÂ Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, NecipÂ Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco GuzmÃ¡n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.04672" title="">No language left behind: Scaling human-centered machine translation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2020)</span>
<span class="ltx_bibblock">
JÃ¶rg Tiedemann. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.139" title="">The tatoeba translation challenge â€“ realistic data sets for low resource and multilingual MT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 1174â€“1182, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann and Thottingal (2020)</span>
<span class="ltx_bibblock">
JÃ¶rg Tiedemann and Santhosh Thottingal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.eamt-1.61" title="">OPUS-MT â€“ building open translation services for the world</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</em>, pages 479â€“480, Lisboa, Portugal. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tillmann and Ney (2003)</span>
<span class="ltx_bibblock">
Christoph Tillmann and Hermann Ney. 2003.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/089120103321337458" title="">Word Reordering and a Dynamic Programming Beam Search Algorithm for Statistical Machine Translation</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Computational Linguistics</em>, 29(1):97â€“133.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">von Oswald etÂ al. (2023)</span>
<span class="ltx_bibblock">
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, JoÃ£o Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.07677" title="">Transformers learn in-context by gradient descent</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang and Li (2023)</span>
<span class="ltx_bibblock">
Heng Yang and KeÂ Li. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.105" title="">Boosting text augmentation via hybrid instance filtering framework</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 1652â€“1669, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ÃœstÃ¼n etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ahmet ÃœstÃ¼n, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel Dâ€™souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.07827" title="">Aya model: An instruction finetuned open-access multilingual language model</a>.

</span>
</li>
</ul>
</section>
<figure class="ltx_table" id="S11.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S11.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S11.T5.1.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.1.2"><span class="ltx_text ltx_font_bold" id="S11.T5.1.1.2.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.1.1">
<span class="ltx_text ltx_font_bold" id="S11.T5.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S11.T5.1.1.1.m1.1"><semantics id="S11.T5.1.1.1.m1.1a"><mo id="S11.T5.1.1.1.m1.1.1" stretchy="false" xref="S11.T5.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S11.T5.1.1.1.m1.1b"><ci id="S11.T5.1.1.1.m1.1.1.cmml" xref="S11.T5.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S11.T5.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S11.T5.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.1.3"><span class="ltx_text ltx_font_bold" id="S11.T5.1.1.3.1">spBLEU</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.1.4"><span class="ltx_text ltx_font_bold" id="S11.T5.1.1.4.1">chrF</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.1.5"><span class="ltx_text ltx_font_bold" id="S11.T5.1.1.5.1">chrF++</span></td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S11.T5.1.2.1.1.1">Finetuned</span></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.2.1.2"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.2.1.3"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.2.1.4"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.2.1.5"></td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.3.2">
<td class="ltx_td ltx_align_left" id="S11.T5.1.3.2.1">Dragoman P, 10 beams (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S3" title="3. First Phase: Heuristic Filtering of Paracrawl â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 3</span></a>)</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.3.2.2">30.38</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.3.2.3">37.93</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.3.2.4">59.49</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.3.2.5">56.41</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.4.3">
<td class="ltx_td ltx_align_left" id="S11.T5.1.4.3.1">Dragoman PT, 10 beams (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S4" title="4. Second Phase: Unsupervised Data Selection on Extended Multi30K â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 4</span></a>)</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S11.T5.1.4.3.2.1">32.34</span></td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S11.T5.1.4.3.3.1">39.93</span></td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S11.T5.1.4.3.4.1">60.72</span></td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S11.T5.1.4.3.5.1">57.82</span></td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.5.4.1">
<span class="ltx_text ltx_font_bold" id="S11.T5.1.5.4.1.1">Zero shot and few shot</span> (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2404.15196v2#S5" title="5. Few-Shot Translation â€£ Setting up the Data Printer with Improved English to Ukrainian Machine Translation"><span class="ltx_text ltx_ref_tag">sectionÂ 5</span></a>)</td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.5.4.2"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.5.4.3"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.5.4.4"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.5.4.5"></td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.6.5">
<td class="ltx_td ltx_align_left" id="S11.T5.1.6.5.1">LLaMa-2-7B 2-shot</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.6.5.2">20.1</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.6.5.3">26.78</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.6.5.4">49.22</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.6.5.5">46.29</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.7.6">
<td class="ltx_td ltx_align_left" id="S11.T5.1.7.6.1">RWKV-5-World-7B 0-shot</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.7.6.2">21.06</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.7.6.3">26.20</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.7.6.4">49.46</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.7.6.5">46.46</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.8.7">
<td class="ltx_td ltx_align_left" id="S11.T5.1.8.7.1">gpt-4 10-shot</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.8.7.2">29.48</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.8.7.3">37.94</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.8.7.4">58.37</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.8.7.5">55.38</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.9.8">
<td class="ltx_td ltx_align_left" id="S11.T5.1.9.8.1">gpt-4-turbo-preview 0-shot</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.9.8.2">30.36</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.9.8.3">36.75</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.9.8.4">59.18</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.9.8.5">56.19</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.10.9">
<td class="ltx_td ltx_align_left" id="S11.T5.1.10.9.1">Google Translate 0-shot</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.10.9.2">25.85</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.10.9.3">32.49</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.10.9.4">55.88</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.10.9.5">52.48</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T5.1.11.10.1"><span class="ltx_text ltx_font_bold" id="S11.T5.1.11.10.1.1">Pretrained</span></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.11.10.2"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.11.10.3"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.11.10.4"></td>
<td class="ltx_td ltx_border_t" id="S11.T5.1.11.10.5"></td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.12.11">
<td class="ltx_td ltx_align_left" id="S11.T5.1.12.11.1">NLLB 3B, 10 beams</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.12.11.2">30.46</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.12.11.3">37.22</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.12.11.4">58.11</td>
<td class="ltx_td ltx_align_left" id="S11.T5.1.12.11.5">55.32</td>
</tr>
<tr class="ltx_tr" id="S11.T5.1.13.12">
<td class="ltx_td ltx_align_left ltx_border_b" id="S11.T5.1.13.12.1">OPUS-MT, 10 beams</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S11.T5.1.13.12.2">32.2</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S11.T5.1.13.12.3">39.76</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S11.T5.1.13.12.4">60.23</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S11.T5.1.13.12.5">57.38</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>We evaluate generated translations with the sacrebleu library to calculate BLEU, spBLEU, chrF, and chrF++ metrics on the FLORES DEVTEST set. Metric spBLEU was calculated with default BLEU values and tokenizer flores101. Tokenization and detokenization are done using the modelsâ€™ default tokenizers. Evaluation is performed on detokenized sentences with corresponding reference sentences.</figcaption>
</figure>
<figure class="ltx_table" id="S11.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S11.T6.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S11.T6.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S11.T6.1.1.2"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.2.1">Threshold</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S11.T6.1.1.3"><span class="ltx_text ltx_font_bold" id="S11.T6.1.1.3.1">Examples</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T6.1.1.1">
<span class="ltx_text ltx_font_bold" id="S11.T6.1.1.1.1">BLEU</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S11.T6.1.1.1.m1.1"><semantics id="S11.T6.1.1.1.m1.1a"><mo id="S11.T6.1.1.1.m1.1.1" stretchy="false" xref="S11.T6.1.1.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S11.T6.1.1.1.m1.1b"><ci id="S11.T6.1.1.1.m1.1.1.cmml" xref="S11.T6.1.1.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S11.T6.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S11.T6.1.1.1.m1.1d">â†‘</annotation></semantics></math>
</td>
<td class="ltx_td ltx_border_t" id="S11.T6.1.1.4"></td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.3.1.1">percentile</th>
<th class="ltx_td ltx_th ltx_th_row" id="S11.T6.2.3.1.2"></th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.3.1.3">dev</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.3.1.4">devtest</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S11.T6.2.4.2.1">20<sup class="ltx_sup" id="S11.T6.2.4.2.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S11.T6.2.4.2.2">5800</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T6.2.4.2.3">25.14</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S11.T6.2.4.2.4">25.49</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.5.3.1">40<sup class="ltx_sup" id="S11.T6.2.5.3.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.5.3.2">11600</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.5.3.3">25.39</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.5.3.4">25.45</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.6.4.1">50<sup class="ltx_sup" id="S11.T6.2.6.4.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.6.4.2">14500</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.6.4.3">25.79</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.6.4.4">25.93</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.7.5.1">60<sup class="ltx_sup" id="S11.T6.2.7.5.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.7.5.2">17400</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.7.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S11.T6.2.7.5.3.1">26.07</span></td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.7.5.4"><span class="ltx_text ltx_font_bold" id="S11.T6.2.7.5.4.1">26.01</span></td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.8.6.1">70<sup class="ltx_sup" id="S11.T6.2.8.6.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.8.6.2">20300</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.8.6.3">26.00</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.8.6.4">25.72</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.9.7.1">80<sup class="ltx_sup" id="S11.T6.2.9.7.1.1">th</sup>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.9.7.2">23200</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.9.7.3">25.90</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.9.7.4">26.08</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.2.1">95.4<sup class="ltx_sup" id="S11.T6.2.2.1.1">th</sup> (<math alttext="2\sigma" class="ltx_Math" display="inline" id="S11.T6.2.2.1.m1.1"><semantics id="S11.T6.2.2.1.m1.1a"><mrow id="S11.T6.2.2.1.m1.1.1" xref="S11.T6.2.2.1.m1.1.1.cmml"><mn id="S11.T6.2.2.1.m1.1.1.2" xref="S11.T6.2.2.1.m1.1.1.2.cmml">2</mn><mo id="S11.T6.2.2.1.m1.1.1.1" xref="S11.T6.2.2.1.m1.1.1.1.cmml">â¢</mo><mi id="S11.T6.2.2.1.m1.1.1.3" xref="S11.T6.2.2.1.m1.1.1.3.cmml">Ïƒ</mi></mrow><annotation-xml encoding="MathML-Content" id="S11.T6.2.2.1.m1.1b"><apply id="S11.T6.2.2.1.m1.1.1.cmml" xref="S11.T6.2.2.1.m1.1.1"><times id="S11.T6.2.2.1.m1.1.1.1.cmml" xref="S11.T6.2.2.1.m1.1.1.1"></times><cn id="S11.T6.2.2.1.m1.1.1.2.cmml" type="integer" xref="S11.T6.2.2.1.m1.1.1.2">2</cn><ci id="S11.T6.2.2.1.m1.1.1.3.cmml" xref="S11.T6.2.2.1.m1.1.1.3">ğœ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S11.T6.2.2.1.m1.1c">2\sigma</annotation><annotation encoding="application/x-llamapun" id="S11.T6.2.2.1.m1.1d">2 italic_Ïƒ</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.2.2">28025</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.2.3">25.91</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.2.4">25.81</td>
</tr>
<tr class="ltx_tr" id="S11.T6.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.10.8.1">Full dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S11.T6.2.10.8.2">29000</th>
<td class="ltx_td ltx_align_left" id="S11.T6.2.10.8.3">25.74</td>
<td class="ltx_td ltx_align_left" id="S11.T6.2.10.8.4">25.67</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Evaluation scores for model, finetuned from Mistral-7B-v0.1 directly on Extended Multi30K dataset. We performed log probability thresholds sweep on FLORES dev set. We demonstrate that data from the second phase alone is not enough to match the performance of our best checkpoint. Perplexity filtering improves downstream performance over training on full Extended Multi30K dataset.
</figcaption>
</figure>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Translation Examples</h2>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Sample of top 5 worst examples by BLEU from FLORES devtest set</h3>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.1">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.1.2">While most of their food would be familiar to us, Romans did have their share of strange or unusual feast items, including wild boar, peacock, snails, and a type of rodent called a dormouse
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.3">Hypothesis:</span> Ğ¥Ğ¾Ñ‡Ğ° Ğ±Ñ–Ğ»ÑŒÑˆÑ–ÑÑ‚ÑŒ Ñ—Ñ…Ğ½Ñ–Ñ… Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ–Ğ² Ñ…Ğ°Ñ€Ñ‡ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ±ÑƒĞ»Ğ¾ Ğ± Ğ·Ğ½Ğ°Ğ¹Ğ¾Ğ¼Ğ¸Ğ¼ Ğ´Ğ»Ñ Ğ½Ğ°Ñ, Ñ€Ğ¸Ğ¼Ğ»ÑĞ½Ğ¸ Ğ¼Ğ°Ğ»Ğ¸ ÑĞ²Ğ¾Ñ Ñ‡Ğ°ÑÑ‚ĞºÑƒ Ğ´Ğ¸Ğ²Ğ½Ğ¸Ñ… Ğ°Ğ±Ğ¾ Ğ½ĞµĞ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹Ğ½Ğ¸Ñ… ÑÑ‚Ñ€Ğ°Ğ², Ğ²ĞºĞ»ÑÑ‡Ğ°ÑÑ‡Ğ¸ ĞºĞ°Ğ±Ğ°Ğ½Ğ°, Ğ¿Ğ°Ğ²Ğ¸Ñ‡Ğ°, Ñ€Ğ°Ğ²Ğ»Ğ¸ĞºÑ–Ğ² Ñ‚Ğ° Ğ³Ñ€Ğ¸Ğ·ÑƒĞ½Ñ–Ğ², Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ… ĞºÑƒĞ½Ğ¸Ñ†ÑĞ¼Ğ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.4">Reference:</span> ĞŸĞµÑ€ĞµĞ²Ğ°Ğ¶Ğ½Ğ¾ Ñ€Ğ¸Ğ¼Ğ»ÑĞ½Ğ¸ Ñ…Ğ°Ñ€Ñ‡ÑƒĞ²Ğ°Ğ»Ğ¸ÑÑŒ Ñ‚Ğ¸Ğ¼, Ñ‰Ğ¾ Ğ·Ğ½Ğ°Ğ¹Ğ¾Ğ¼Ğµ Ñ– Ğ½Ğ°Ğ¼, Ğ°Ğ»Ğµ Ğ±ÑƒĞ»Ğ¸ Ñƒ Ğ½Ğ¸Ñ… Ñ– ÑĞ²Ğ¾Ñ— Ğ´Ğ¸Ğ²Ğ½Ñ– Ñ‚Ğ° Ğ½ĞµĞ·Ğ²Ğ¸Ñ‡Ğ½Ñ– Ğ±ĞµĞ½ĞºĞµÑ‚Ğ½Ñ– Ñ‡Ğ°ÑÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ, ÑĞµÑ€ĞµĞ´ ÑĞºĞ¸Ñ… Ğ´Ğ¸ĞºĞ¸Ğ¹ ĞºĞ°Ğ±Ğ°Ğ½, Ğ¿Ğ°Ğ²Ğ¸Ñ‡, ÑĞ»Ğ¸Ğ¼Ğ°ĞºĞ¸ Ñ– Ğ³Ñ€Ğ¸Ğ·ÑƒĞ½ Ğ¿Ñ–Ğ´ Ğ½Ğ°Ğ·Ğ²Ğ¾Ñ Ğ²Ğ¾Ğ²Ñ‡Ğ¾Ğº.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.5">BLEU:</span> 1.72


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.6">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.1.7">"I was moved every time we did a rehearsal on this, from the bottom of my heart."
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.8">Hypothesis:</span> "Ğ¯ Ğ±ÑƒĞ² Ğ·Ğ²Ğ¾Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹ ĞºĞ¾Ğ¶Ğ½Ğ¾Ğ³Ğ¾ Ñ€Ğ°Ğ·Ñƒ, ĞºĞ¾Ğ»Ğ¸ Ğ¼Ğ¸ Ñ€Ğ¾Ğ±Ğ¸Ğ»Ğ¸ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ†Ñ–Ñ Ğ· Ñ†ÑŒĞ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¸Ğ²Ğ¾Ğ´Ñƒ, Ğ²Ñ–Ğ´ Ñ‰Ğ¸Ñ€Ğ¾Ğ³Ğ¾ ÑĞµÑ€Ñ†Ñ".
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.9">Reference:</span> ĞœĞµĞ½Ğµ Ñ†Ğµ Ñ‚Ğ¾Ñ€ĞºĞ°Ğ»Ğ¾ ĞºĞ¾Ğ¶ĞµĞ½ Ñ€Ğ°Ğ· Ğ¿Ñ–Ğ´ Ñ‡Ğ°Ñ Ñ€ĞµĞ¿ĞµÑ‚Ğ¸Ñ†Ñ–Ñ— Ğ´Ğ¾ Ğ³Ğ»Ğ¸Ğ±Ğ¸Ğ½Ğ¸ Ğ´ÑƒÑˆÑ–.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.10">BLEU:</span> 1.91


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.11">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.1.12">Feral children may have experienced severe child abuse or trauma before being abandoned or running away.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.13">Hypothesis:</span> Ğ”Ğ¸Ñ‚Ğ¸Ğ½Ğ°-Ğ´Ğ¸ĞºÑƒĞ½, Ğ¼Ğ¾Ğ¶Ğ»Ğ¸Ğ²Ğ¾, Ğ¿ĞµÑ€ĞµĞ¶Ğ¸Ğ»Ğ° Ğ¶Ğ¾Ñ€ÑÑ‚Ğ¾ĞºĞµ Ğ½Ğ°ÑĞ¸Ğ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ½Ğ°Ğ´ Ğ´Ñ–Ñ‚ÑŒĞ¼Ğ¸ Ğ°Ğ±Ğ¾ Ñ‚Ñ€Ğ°Ğ²Ğ¼Ñƒ, Ğ¿ĞµÑ€Ñˆ Ğ½Ñ–Ğ¶ Ğ±ÑƒÑ‚Ğ¸ ĞºĞ¸Ğ½ÑƒÑ‚Ğ¾Ñ Ğ°Ğ±Ğ¾ Ğ²Ñ‚ĞµĞºÑ‚Ğ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.14">Reference:</span> ĞœĞ¾Ğ¶Ğµ Ñ‚Ğ°ĞºĞµ Ğ±ÑƒÑ‚Ğ¸, Ñ‰Ğ¾ Ğ´Ğ¸ĞºĞ°Ñ€ÑÑŒĞºÑ– Ğ´Ñ–Ñ‚Ğ¸ Ğ±ÑƒĞ»Ğ¸ Ğ¾Ğ±â€™Ñ”ĞºÑ‚Ğ°Ğ¼Ğ¸ Ğ¶Ğ¾Ñ€ÑÑ‚Ğ¾ĞºĞ¾Ğ³Ğ¾ Ğ¿Ğ¾Ğ²Ğ¾Ğ´Ğ¶ĞµĞ½Ğ½Ñ Ğ°Ğ±Ğ¾ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ»Ğ¸ ÑĞµÑ€Ğ¹Ğ¾Ğ·Ğ½Ñ– Ñ‚Ñ€Ğ°Ğ²Ğ¼Ğ¸ Ğ¿ĞµÑ€ĞµĞ´ Ñ‚Ğ¸Ğ¼, ÑĞº Ñ—Ñ… Ğ¿Ğ¾Ğ»Ğ¸ÑˆĞ¸Ğ»Ğ¸ Ğ°Ğ±Ğ¾ Ğ²Ğ¾Ğ½Ğ¸ Ğ²Ñ‚ĞµĞºĞ»Ğ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.15">BLEU:</span> 2.31


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.16">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.1.17">Workers must often get their superiorsâ€™ approval for any decisions they make, and are expected to obey their superiorsâ€™ instructions without question.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.18">Hypothesis:</span> ĞŸÑ€Ğ°Ñ†Ñ–Ğ²Ğ½Ğ¸ĞºĞ¸ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ğ²Ğ¸Ğ½Ğ½Ñ– Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ÑƒĞ²Ğ°Ñ‚Ğ¸ ÑÑ…Ğ²Ğ°Ğ»ĞµĞ½Ğ½Ñ ÑĞ²Ğ¾Ñ—Ñ… Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¸ĞºÑ–Ğ² Ğ½Ğ° Ğ±ÑƒĞ´ÑŒ-ÑĞºÑ– Ñ€Ñ–ÑˆĞµĞ½Ğ½Ñ, ÑĞºÑ– Ğ²Ğ¾Ğ½Ğ¸ Ğ¿Ñ€Ğ¸Ğ¹Ğ¼Ğ°ÑÑ‚ÑŒ, Ñ– Ğ¿Ğ¾Ğ²Ğ¸Ğ½Ğ½Ñ– Ğ±ĞµĞ·Ğ·Ğ°Ğ¿ĞµÑ€ĞµÑ‡Ğ½Ğ¾ Ğ²Ğ¸ĞºĞ¾Ğ½ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ²ĞºĞ°Ğ·Ñ–Ğ²ĞºĞ¸ ÑĞ²Ğ¾Ñ—Ñ… Ğ½Ğ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ¸ĞºÑ–Ğ².
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.19">Reference:</span> ĞŸÑ€Ğ°Ñ†Ñ–Ğ²Ğ½Ğ¸ĞºĞ°Ğ¼ Ñ‡Ğ°ÑÑ‚Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ñ‚Ğ¸ ÑÑ…Ğ²Ğ°Ğ»ĞµĞ½Ğ½Ñ ĞºĞµÑ€Ñ–Ğ²Ğ½Ğ¸ĞºĞ° Ñ‰Ğ¾Ğ´Ğ¾ Ğ±ÑƒĞ´ÑŒ-ÑĞºĞ¸Ñ… Ñ€Ñ–ÑˆĞµĞ½ÑŒ, Ğ° Ñ‚Ğ°ĞºĞ¾Ğ¶ Ğ²Ñ–Ğ´ Ğ½Ğ¸Ñ… Ğ¾Ñ‡Ñ–ĞºÑƒÑÑ‚ÑŒ Ğ±ĞµĞ·Ğ·Ğ°Ğ¿ĞµÑ€ĞµÑ‡Ğ½Ğ¾Ğ³Ğ¾ Ğ´Ğ¾Ñ‚Ñ€Ğ¸Ğ¼Ğ°Ğ½Ğ½Ñ Ğ½Ğ°ÑÑ‚Ğ°Ğ½Ğ¾Ğ² ĞºĞµÑ€Ñ–Ğ²Ğ½Ğ¸ĞºÑ–Ğ².
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.20">BLEU:</span> 2.32


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.21">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px1.p1.1.22">Typically there will be a tuition fee to enroll in these educational programs.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.23">Hypothesis:</span> Ğ¯Ğº Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾, Ğ±ÑƒĞ´Ğµ Ğ¿Ğ»Ğ°Ñ‚Ğ° Ğ·Ğ° Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ, Ñ‰Ğ¾Ğ± Ğ·Ğ°Ñ€ĞµÑ”ÑÑ‚Ñ€ÑƒĞ²Ğ°Ñ‚Ğ¸ÑÑ Ğ² Ñ†Ğ¸Ñ… Ğ¾ÑĞ²Ñ–Ñ‚Ğ½Ñ–Ñ… Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ°Ñ….
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.24">Reference:</span> Ğ—Ğ°Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹ Ñ‚Ğ°ĞºÑ– Ğ¾ÑĞ²Ñ–Ñ‚Ğ½Ñ– Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¸ Ğ¿Ğ»Ğ°Ñ‚Ğ½Ñ–.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px1.p1.1.25">BLEU:</span> 2.62</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Sample of top 5 best examples by BLEU from FLORES devtest set</h3>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.1">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px2.p1.1.2">The East African Islands are in the Indian Ocean off the eastern coast of Africa.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.3">Hypothesis:</span> Ğ¡Ñ…Ñ–Ğ´Ğ½Ğ¾Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑÑŒĞºÑ– Ğ¾ÑÑ‚Ñ€Ğ¾Ğ²Ğ¸ Ğ·Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑŒÑÑ Ğ² Ğ†Ğ½Ğ´Ñ–Ğ¹ÑÑŒĞºĞ¾Ğ¼Ñƒ Ğ¾ĞºĞµĞ°Ğ½Ñ– Ğ±Ñ–Ğ»Ñ ÑÑ…Ñ–Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ·Ğ±ĞµÑ€ĞµĞ¶Ğ¶Ñ ĞÑ„Ñ€Ğ¸ĞºĞ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.4">Reference:</span> Ğ¡Ñ…Ñ–Ğ´Ğ½Ğ¾Ğ°Ñ„Ñ€Ğ¸ĞºĞ°Ğ½ÑÑŒĞºÑ– Ğ¾ÑÑ‚Ñ€Ğ¾Ğ²Ğ¸ Ğ·Ğ½Ğ°Ñ…Ğ¾Ğ´ÑÑ‚ÑŒÑÑ Ğ² Ğ†Ğ½Ğ´Ñ–Ğ¹ÑÑŒĞºĞ¾Ğ¼Ñƒ Ğ¾ĞºĞµĞ°Ğ½Ñ– Ğ±Ñ–Ğ»Ñ ÑÑ…Ñ–Ğ´Ğ½Ğ¾Ğ³Ğ¾ ÑƒĞ·Ğ±ĞµÑ€ĞµĞ¶Ğ¶Ñ ĞÑ„Ñ€Ğ¸ĞºĞ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.5">BLEU:</span> 100.00


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.6">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px2.p1.1.7">Earlier the Chinese news agency Xinhua reported a plane to be hijacked.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.8">Hypothesis:</span> Ğ Ğ°Ğ½Ñ–ÑˆĞµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑÑŒĞºĞµ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ğ¹Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ¾ Ğ¡Ñ–Ğ½ÑŒÑ…ÑƒĞ° Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾ Ğ²Ğ¸ĞºÑ€Ğ°Ğ´ĞµĞ½Ğ½Ñ Ğ»Ñ–Ñ‚Ğ°ĞºĞ°.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.9">Reference:</span> Ğ Ğ°Ğ½Ñ–ÑˆĞµ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑÑŒĞºĞµ Ñ–Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ñ–Ğ¹Ğ½Ğµ Ğ°Ğ³ĞµĞ½Ñ‚ÑÑ‚Ğ²Ğ¾ Ğ¡Ñ–Ğ½ÑŒÑ…ÑƒĞ° Ğ¿Ğ¾Ğ²Ñ–Ğ´Ğ¾Ğ¼Ğ¸Ğ»Ğ¾ Ğ¿Ñ€Ğ¾ Ğ²Ğ¸ĞºÑ€Ğ°Ğ´ĞµĞ½Ğ½Ñ Ğ»Ñ–Ñ‚Ğ°ĞºĞ°.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.10">BLEU:</span> 100.00


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.11">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px2.p1.1.12">For instance, they didnâ€™t have corn, nor tomatoes, nor potatoes, nor cocoa, and no ancient Roman ever tasted a turkey.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.13">Hypothesis:</span> ĞĞ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, Ñƒ Ğ½Ğ¸Ñ… Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ğ½Ñ– ĞºÑƒĞºÑƒÑ€ÑƒĞ´Ğ·Ğ¸, Ğ½Ñ– Ğ¿Ğ¾Ğ¼Ñ–Ğ´Ğ¾Ñ€Ñ–Ğ², Ğ½Ñ– ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¿Ğ»Ñ–, Ğ½Ñ– ĞºĞ°ĞºĞ°Ğ¾, Ñ– Ğ¶Ğ¾Ğ´ĞµĞ½ ÑÑ‚Ğ°Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ½Ñ–Ğ¹ Ñ€Ğ¸Ğ¼Ğ»ÑĞ½Ğ¸Ğ½ Ğ½Ñ–ĞºĞ¾Ğ»Ğ¸ Ğ½Ğµ ÑĞºÑƒÑˆÑ‚ÑƒĞ²Ğ°Ğ² Ñ–Ğ½Ğ´Ğ¸Ñ‡ĞºÑƒ.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.14">Reference:</span> ĞĞ°Ğ¿Ñ€Ğ¸ĞºĞ»Ğ°Ğ´, Ñƒ Ğ½Ğ¸Ñ… Ğ½Ğµ Ğ±ÑƒĞ»Ğ¾ Ğ½Ñ– ĞºÑƒĞºÑƒÑ€ÑƒĞ´Ğ·Ğ¸, Ğ½Ñ– Ğ¿Ğ¾Ğ¼Ñ–Ğ´Ğ¾Ñ€Ñ–Ğ², Ğ½Ñ– ĞºĞ°Ñ€Ñ‚Ğ¾Ğ¿Ğ»Ñ–, Ğ½Ñ– ĞºĞ°ĞºĞ°Ğ¾, Ñ– Ğ¶Ğ¾Ğ´ĞµĞ½ ÑÑ‚Ğ°Ñ€Ğ¾Ğ´Ğ°Ğ²Ğ½Ñ–Ğ¹ Ñ€Ğ¸Ğ¼Ğ»ÑĞ½Ğ¸Ğ½ Ğ½Ñ–ĞºĞ¾Ğ»Ğ¸ Ğ½Ğµ ĞºÑƒÑˆÑ‚ÑƒĞ²Ğ°Ğ² Ñ–Ğ½Ğ´Ğ¸Ñ‡ĞºÑƒ.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.15">BLEU:</span> 90.95


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.16">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px2.p1.1.17">The luminosity and rotation are used together to determine a starâ€™s Rossby number, which is related to plasma flow.
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.18">Hypothesis:</span> Ğ¡Ğ²Ñ–Ñ‚Ğ½Ñ–ÑÑ‚ÑŒ Ñ– Ğ¾Ğ±ĞµÑ€Ñ‚Ğ°Ğ½Ğ½Ñ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‚ÑŒÑÑ Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ Ğ¾ÑÑĞ±Ñ– Ğ·Ñ–Ñ€ĞºĞ¸, ÑĞºĞµ Ğ¿Ğ¾Ğ²â€™ÑĞ·Ğ°Ğ½Ğµ Ğ· Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼ Ğ¿Ğ»Ğ°Ğ·Ğ¼Ğ¸.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.19">Reference:</span> Ğ¡Ğ²Ñ–Ñ‚Ğ½Ñ–ÑÑ‚ÑŒ Ñ– Ğ¾Ğ±ĞµÑ€Ñ‚Ğ°Ğ½Ğ½Ñ Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‚ÑŒÑÑ Ñ€Ğ°Ğ·Ğ¾Ğ¼ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ½Ñ Ñ‡Ğ¸ÑĞ»Ğ° Ğ Ğ¾ÑÑĞ±Ñ– Ğ·Ñ–Ñ€ĞºĞ¸, ÑĞºĞµ Ğ¿Ğ¾Ğ²â€™ÑĞ·Ğ°Ğ½Ğµ Ğ· Ğ¿Ğ»Ğ°Ğ·Ğ¼Ğ¾Ğ²Ğ¸Ğ¼ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ¼.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.20">BLEU:</span> 83.26


<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.21">Source:</span> <span class="ltx_text ltx_font_smallcaps" id="A1.SS0.SSS0.Px2.p1.1.22">But being placed in the "high tropics" just a few degrees north of equator you will need to deal with both heat (always) and strong sun (when the sky is clear, more rarely).
<br class="ltx_break"/></span><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.23">Hypothesis:</span> ĞĞ»Ğµ Ğ¿ĞµÑ€ĞµĞ±ÑƒĞ²Ğ°ÑÑ‡Ğ¸ Ğ² "Ğ²Ğ¸ÑĞ¾ĞºĞ¸Ñ… Ñ‚Ñ€Ğ¾Ğ¿Ñ–ĞºĞ°Ñ…" Ğ²ÑÑŒĞ¾Ğ³Ğ¾ Ğ² Ğ´ĞµĞºÑ–Ğ»ÑŒĞºĞ¾Ñ… Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ°Ñ… Ğ½Ğ° Ğ¿Ñ–Ğ²Ğ½Ñ–Ñ‡ Ğ²Ñ–Ğ´ ĞµĞºĞ²Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ²Ğ°Ğ¼ Ğ´Ğ¾Ğ²ĞµĞ´ĞµÑ‚ÑŒÑÑ Ğ¼Ğ°Ñ‚Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ñƒ ÑĞº Ğ· ÑĞ¿ĞµĞºĞ¾Ñ (Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸), Ñ‚Ğ°Ğº Ñ– Ğ· ÑĞ¸Ğ»ÑŒĞ½Ğ¸Ğ¼ ÑĞ¾Ğ½Ñ†ĞµĞ¼ (ĞºĞ¾Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğµ, Ñ€Ñ–Ğ´ÑˆĞµ).
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.24">Reference:</span> ĞĞ»Ğµ, Ğ¿ĞµÑ€ĞµĞ±ÑƒĞ²Ğ°ÑÑ‡Ğ¸ Ğ² "Ğ²Ğ¸ÑĞ¾ĞºĞ¸Ñ… Ñ‚Ñ€Ğ¾Ğ¿Ñ–ĞºĞ°Ñ…" Ğ²ÑÑŒĞ¾Ğ³Ğ¾ Ğ² Ğ´ĞµĞºÑ–Ğ»ÑŒĞºĞ¾Ñ… Ğ³Ñ€Ğ°Ğ´ÑƒÑĞ°Ñ… Ğ½Ğ° Ğ¿Ñ–Ğ²Ğ½Ñ–Ñ‡ Ğ²Ñ–Ğ´ ĞµĞºĞ²Ğ°Ñ‚Ğ¾Ñ€Ğ°, Ğ²Ğ°Ğ¼ Ğ´Ğ¾Ğ²ĞµĞ´ĞµÑ‚ÑŒÑÑ Ğ¼Ğ°Ñ‚Ğ¸ ÑĞ¿Ñ€Ğ°Ğ²Ñƒ ÑĞº Ğ·Ñ– ÑĞ¿ĞµĞºĞ¾Ñ (Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸), Ñ‚Ğ°Ğº Ñ– Ğ· Ğ¿Ğ°Ğ»ÑÑ‡Ğ¸Ğ¼ ÑĞ¾Ğ½Ñ†ĞµĞ¼ (ĞºĞ¾Ğ»Ğ¸ Ğ½ĞµĞ±Ğ¾ Ñ‡Ğ¸ÑÑ‚Ğµ, Ñ€Ñ–Ğ´ÑˆĞµ).
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="A1.SS0.SSS0.Px2.p1.1.25">BLEU:</span> 82.47</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jul 12 10:03:29 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
