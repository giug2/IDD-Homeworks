<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2306.09486] FedMultimodal: A Benchmark For Multimodal Federated Learning</title><meta property="og:description" content="Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients subm…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="FedMultimodal: A Benchmark For Multimodal Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="FedMultimodal: A Benchmark For Multimodal Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2306.09486">

<!--Generated on Thu Feb 29 00:25:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Federated Learning,  Multimodal Learning,  Multimodal Benchmark">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">FedMultimodal: 
<br class="ltx_break">A Benchmark For Multimodal Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiantian Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tiantianf@usc.edu">tiantianf@usc.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Digbalay Bose
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:dbose@usc.edu">dbose@usc.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tuo Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tuozhang@usc.edu">tuozhang@usc.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rajat Hebbar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:rajatheb@usc.edu">rajatheb@usc.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anil Ramakrishna
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id17.1.id1" class="ltx_text ltx_affiliation_institution">Amazon Alexa AI</span><span id="id18.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id19.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id20.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:aniramak@amazon.com">aniramak@amazon.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rahul Gupta
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id21.1.id1" class="ltx_text ltx_affiliation_institution">Amazon Alexa AI</span><span id="id22.2.id2" class="ltx_text ltx_affiliation_city">Boston</span><span id="id23.3.id3" class="ltx_text ltx_affiliation_state">MA</span><span id="id24.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:gupra@amazon.com">gupra@amazon.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mi Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id25.1.id1" class="ltx_text ltx_affiliation_institution">The Ohio State University</span><span id="id26.2.id2" class="ltx_text ltx_affiliation_city">Columbus</span><span id="id27.3.id3" class="ltx_text ltx_affiliation_state">OH</span><span id="id28.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:mizhang.1@osu.edu">mizhang.1@osu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Salman Avestimehr
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id29.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id30.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id31.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id32.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:avestime@usc.edu">avestime@usc.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shrikanth Narayanan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id33.1.id1" class="ltx_text ltx_affiliation_institution">University of Southern California</span><span id="id34.2.id2" class="ltx_text ltx_affiliation_city">Los Angeles</span><span id="id35.3.id3" class="ltx_text ltx_affiliation_state">CA</span><span id="id36.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:shri@sipi.usc.edu">shri@sipi.usc.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id37.id1" class="ltx_p">Over the past few years, Federated Learning (FL) has become an emerging machine learning technique to tackle data privacy challenges through collaborative training. In the Federated Learning algorithm, the clients submit a locally trained model, and the server aggregates these parameters until convergence. Despite significant efforts that have been made to FL in fields like computer vision, audio, and natural language processing, the FL applications utilizing multimodal data streams remain largely unexplored. It is known that multimodal learning has broad real-world applications in emotion recognition, healthcare, multimedia, and social media, while user privacy persists as a critical concern. Specifically, there are no existing FL benchmarks targeting multimodal applications or related tasks. In order to facilitate the research in multimodal FL, we introduce FedMultimodal, the first FL benchmark for multimodal learning covering five representative multimodal applications from ten commonly used datasets with a total of eight unique modalities. FedMultimodal offers a systematic FL pipeline, enabling end-to-end modeling framework ranging from data partition and feature extraction to FL benchmark algorithms and model evaluation. Unlike existing FL benchmarks, FedMultimodal provides a standardized approach to assess the robustness of FL against three common data corruptions in real-life multimodal applications: missing modalities, missing labels, and erroneous labels. We hope that FedMultimodal can accelerate numerous future research directions, including designing multimodal FL algorithms toward extreme data heterogeneity, robustness multimodal FL, and efficient multimodal FL. The datasets and benchmark results can be accessed at: <a target="_blank" href="https://github.com/usc-sail/fed-multimodal" title="" class="ltx_ref ltx_href">https://github.com/usc-sail/fed-multimodal</a>.</p>
</div>
<div class="ltx_keywords">Federated Learning, Multimodal Learning, Multimodal Benchmark
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journalyear"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_copyright"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_conference"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; August 6–10, 2023; Long Beach, CA, USA</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_booktitle"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD ’23), August 6–10, 2023, Long Beach, CA, USA</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_doi"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3580305.3599825</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_isbn"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>979-8-4007-0103-0/23/08</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Distributed computing methodologies</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">With rapid advances in machine learning (ML) <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al<span class="ltx_text">.</span>, <a href="#bib.bib40" title="" class="ltx_ref">2015</a>)</cite> in the past decade, modern mobile devices and wearable sensors <cite class="ltx_cite ltx_citemacro_citep">(Booth et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2019b</a>; Pantelopoulos and Bourbakis, <a href="#bib.bib53" title="" class="ltx_ref">2009</a>)</cite> have revolutionized applications and services in industries ranging from entertainment and transportation to healthcare and defense, significantly changing how people live, work, and interact with each other. These intelligent sensing devices, equipped with sensors of multiple modalities, can capture diverse information about a user, including but not limited to physiological, emotional, and rich spatiotemporal contextual information <cite class="ltx_cite ltx_citemacro_citep">(Feng and Narayanan, <a href="#bib.bib23" title="" class="ltx_ref">2019b</a>; Booth et al<span class="ltx_text">.</span>, <a href="#bib.bib7" title="" class="ltx_ref">2019a</a>; Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2021a</a>; Patel et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2012</a>; Becerik-Gerber et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>. These data records are typically transmitted to remote servers for centralized training of the ML models. However, the collection of human-centered data raises significant concerns about compromising user privacy due to association with sensitive environments and contexts, ranging from homes, workplaces, and business meetings to hospitals and schools <cite class="ltx_cite ltx_citemacro_citep">(Raij et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2011</a>)</cite>. Therefore, it is critical to ensure that modern ML systems can protect user privacy by preventing any unauthorized access to data <cite class="ltx_cite ltx_citemacro_citep">(Mireshghallah et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2020</a>; Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In response to this, ML practitioners have developed Federated Learning as an alternative paradigm to build models, without the need to transfer user data from the edge devices <cite class="ltx_cite ltx_citemacro_citep">(Konečnỳ et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2016</a>)</cite>. Unlike centralized training, models are trained locally using locally stored data, and updated parameters are transmitted to the server instead of raw data. FL allows clients to train a model collaboratively without sharing their local data, making it one of the most emerging privacy-enhancing learning algorithms in ML research <cite class="ltx_cite ltx_citemacro_citep">(Kairouz et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Experimental considerations between FedMultimodal and existing multimodal FL studies.</figcaption>
<table id="S1.T1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Missing</span></th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Erroneous</span></th>
<th id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Missing</span></th>
</tr>
<tr id="S1.T1.1.2.2" class="ltx_tr">
<th id="S1.T1.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S1.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S1.T1.1.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Modailties</span></th>
<th id="S1.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S1.T1.1.2.2.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Labels</span></th>
<th id="S1.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S1.T1.1.2.2.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Labels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.3.1" class="ltx_tr">
<th id="S1.T1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S1.T1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">SSCL </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.3.1.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Saeed et al<span class="ltx_text">.</span><span id="S1.T1.1.3.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib60" title="" class="ltx_ref">2020</a><span id="S1.T1.1.3.1.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S1.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.3.1.2.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.3.1.3.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.3.1.4.1" class="ltx_text" style="font-size:90%;">✓</span></td>
</tr>
<tr id="S1.T1.1.4.2" class="ltx_tr">
<th id="S1.T1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.1.4.2.1.1" class="ltx_text" style="font-size:90%;">MMFed </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.4.2.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Xiong et al<span class="ltx_text">.</span><span id="S1.T1.1.4.2.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib75" title="" class="ltx_ref">2022</a><span id="S1.T1.1.4.2.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S1.T1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.4.2.2.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.4.2.3.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.4.2.4.1" class="ltx_text" style="font-size:90%;">✗</span></td>
</tr>
<tr id="S1.T1.1.5.3" class="ltx_tr">
<th id="S1.T1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.1.5.3.1.1" class="ltx_text" style="font-size:90%;">FedMsplit </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.5.3.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Chen and Zhang<span id="S1.T1.1.5.3.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib11" title="" class="ltx_ref">2022</a><span id="S1.T1.1.5.3.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S1.T1.1.5.3.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.5.3.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S1.T1.1.5.3.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.5.3.3.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.5.3.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.5.3.4.1" class="ltx_text" style="font-size:90%;">✗</span></td>
</tr>
<tr id="S1.T1.1.6.4" class="ltx_tr">
<th id="S1.T1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S1.T1.1.6.4.1.1" class="ltx_text" style="font-size:90%;">CreamFL </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.T1.1.6.4.1.2.1" class="ltx_text" style="font-size:90%;">(</span>Yu et al<span class="ltx_text">.</span><span id="S1.T1.1.6.4.1.3.2.1.1" class="ltx_text" style="font-size:90%;">, </span><a href="#bib.bib78" title="" class="ltx_ref">2023</a><span id="S1.T1.1.6.4.1.4.3" class="ltx_text" style="font-size:90%;">)</span></cite>
</th>
<td id="S1.T1.1.6.4.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.6.4.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S1.T1.1.6.4.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.6.4.3.1" class="ltx_text" style="font-size:90%;">✗</span></td>
<td id="S1.T1.1.6.4.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.6.4.4.1" class="ltx_text" style="font-size:90%;">✗</span></td>
</tr>
<tr id="S1.T1.1.7.5" class="ltx_tr">
<th id="S1.T1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S1.T1.1.7.5.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedMultimodal (Ours)</span></th>
<td id="S1.T1.1.7.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S1.T1.1.7.5.2.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S1.T1.1.7.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S1.T1.1.7.5.3.1" class="ltx_text" style="font-size:90%;">✓</span></td>
<td id="S1.T1.1.7.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S1.T1.1.7.5.4.1" class="ltx_text" style="font-size:90%;">✓</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Previous works in FL have primarily focused on designing robust and efficient algorithms for federated model training. <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">FedAvg</span> <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite> was the earliest FL optimization algorithm to train the model in the distribution mechanism.
In <span id="S1.p3.1.2" class="ltx_text ltx_font_italic">FedAvg</span>, each client executes local model updates before submitting the updates to the server.
Even though <span id="S1.p3.1.3" class="ltx_text ltx_font_italic">FedAvg</span> offers possibilities for deploying FL in the wild, it often encounters slow convergence as a consequence of gradient drifting from data heterogeneity. As such, researchers have proposed algorithms such as stochastic Controlled Averaging Algorithm (SCAFFOLD) <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite> and FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite> to minimize the impact of gradient drift for heterogeneous data. For example, SCAFFOLD accelerates the training speed through control variates which prevent the client gradients from drifting away from the global optima. Similarly, <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite> introduced adaptive optimization algorithms, FedOpt, that allow server optimization through momentum.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To facilitate FL research in more diverse problem domains, a number of FL benchmarks have been developed in the past few years.
For example, LEAF <cite class="ltx_cite ltx_citemacro_citep">(Caldas et al<span class="ltx_text">.</span>, <a href="#bib.bib9" title="" class="ltx_ref">2018</a>)</cite> was the earliest FL benchmark which includes multiple FL training tasks on 5 datasets covering various computer vision and NLP tasks.
FedML <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>, besides providing an open-source library and a platform for federated learning deployment, it includes multiple FL benchmarks on computer vision and health <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib30" title="" class="ltx_ref">2021b</a>)</cite>, data mining <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite>, IoT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2021a</a>)</cite>, and NLP <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>.
More recently, <cite class="ltx_cite ltx_citemacro_citep">(Lai et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> announced a multi-domain FL benchmark called FedScale. FedScale included implementations with 20 realistic FL datasets mainly in computer vision and natural language processing applications.
<cite class="ltx_cite ltx_citemacro_citep">(Dimitriadis et al<span class="ltx_text">.</span>, <a href="#bib.bib15" title="" class="ltx_ref">2022</a>)</cite> introduced an FL simulation tool named FLUTE, which covers the application of CV, NLP, and audio tasks.
Meanwhile, <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite> presented an audio-centric federated learning framework, FedAudio, which focused on speech emotion recognition, keyword spotting, and audio event classification. Further, FLamby <cite class="ltx_cite ltx_citemacro_citep">(Terrail et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2022</a>)</cite> is a recently proposed FL benchmark for a wide range of healthcare applications such as identifying lung nodules and predicting death risks. FederatedScope <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2022</a>)</cite> also incorporates various benchmarks for federated learning in CV, NLP, and data mining <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib72" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Existing Multimodal Federated Learning Works:</span> While existing FL benchmarks largely focus on unimodal applications such as computer vision (CV), natural language processing (NLP), and speech recognition, a significant number of real-world applications are associated with multimodal data streams.
As listed in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <cite class="ltx_cite ltx_citemacro_citep">(Saeed et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2020</a>)</cite> was one of the earliest to investigate FL using multi-sensory data. They proposed a self-supervised learning approach called Scalogram-signal Correspondence Learning (SSCL) to learn robust multi-modal representations in FL. More recently, <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2022</a>)</cite> designed a multi-modal FL framework named MMFed using the cross-attention mechanism. Moreover, <cite class="ltx_cite ltx_citemacro_citep">(Chen and Zhang, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite> proposed an FL framework called FedMSplit that targeted the issue of missing modalities in the multimodal setup. CreamFL <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2023</a>)</cite> provides a multi-modal FL framework using contrastive representation-level ensemble to learn a larger server model from heterogeneous clients across multi-modalities. However, existing multimodal FL frameworks perform their evaluation using their defined experimental setups, thus making it challenging for researchers to compare their methods with existing state-of-the-art fairly and effectively.</p>
</div>
<figure id="S1.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Overview of the 10 datasets included in FedMultimodal.</figcaption>
<table id="S1.T2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S1.T2.1.1.1" class="ltx_tr">
<td id="S1.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Task</span></td>
<td id="S1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T2.1.1.1.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.1.1.2.1.1" class="ltx_p"><span id="S1.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Dataset</span></span>
</span>
</td>
<td id="S1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Partition</span></td>
<td id="S1.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Client Num.</span></td>
<td id="S1.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Modalities</span></td>
<td id="S1.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Features</span></td>
<td id="S1.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="S1.T2.1.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Metirc</span></td>
<td id="S1.T2.1.1.1.8" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T2.1.1.1.8.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S1.T2.1.1.1.8.1.1" class="ltx_p"><span id="S1.T2.1.1.1.8.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Validation
<br class="ltx_break">Protocol</span></span>
</span>
</td>
<td id="S1.T2.1.1.1.9" class="ltx_td ltx_align_center ltx_border_tt">
<span id="S1.T2.1.1.1.9.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.1.1.9.1.1" class="ltx_p"><span id="S1.T2.1.1.1.9.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Total
<br class="ltx_break">Instance</span></span>
</span>
</td>
</tr>
<tr id="S1.T2.1.2.2" class="ltx_tr">
<td id="S1.T2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.2.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.2.2.1.1.1" class="ltx_p"><span id="S1.T2.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ER</span></span>
</span>
</td>
<td id="S1.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.2.2.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.2.2.2.1.1" class="ltx_p"><span id="S1.T2.1.2.2.2.1.1.1" class="ltx_text" style="font-size:90%;">MELD</span></span>
<span id="S1.T2.1.2.2.2.1.2" class="ltx_p"><span id="S1.T2.1.2.2.2.1.2.1" class="ltx_text" style="font-size:90%;">CREMA-D</span></span>
</span>
</td>
<td id="S1.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.2.2.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.2.2.3.1.1" class="ltx_p"><span id="S1.T2.1.2.2.3.1.1.1" class="ltx_text" style="font-size:90%;">Natural</span></span>
<span id="S1.T2.1.2.2.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.2.2.3.1.2.1" class="ltx_text" style="font-size:90%;">Natural</span></span>
</span>
</td>
<td id="S1.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.2.2.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:14.2pt;">
<span id="S1.T2.1.2.2.4.1.1" class="ltx_p"><span id="S1.T2.1.2.2.4.1.1.1" class="ltx_text" style="font-size:90%;">86</span></span>
<span id="S1.T2.1.2.2.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.2.2.4.1.2.1" class="ltx_text" style="font-size:90%;">72</span></span>
</span>
</td>
<td id="S1.T2.1.2.2.5" class="ltx_td ltx_align_right ltx_border_t">
<span id="S1.T2.1.2.2.5.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.2.2.5.2" class="ltx_text" style="font-size:90%;">1.7cmAudio, Text</span>
</td>
<td id="S1.T2.1.2.2.6" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.2.2.7" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.2.2.8" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.2.2.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S1.T2.1.3.3" class="ltx_tr">
<td id="S1.T2.1.3.3.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Audio, Video</span></td>
<td id="S1.T2.1.3.3.2" class="ltx_td ltx_align_right">
<span id="S1.T2.1.3.3.2.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.3.3.2.2" class="ltx_text" style="font-size:90%;">3cmMFCCs, MobileBert</span>
</td>
<td id="S1.T2.1.3.3.3" class="ltx_td"></td>
<td id="S1.T2.1.3.3.4" class="ltx_td"></td>
<td id="S1.T2.1.3.3.5" class="ltx_td"></td>
<td id="S1.T2.1.3.3.6" class="ltx_td"></td>
<td id="S1.T2.1.3.3.7" class="ltx_td"></td>
<td id="S1.T2.1.3.3.8" class="ltx_td"></td>
<td id="S1.T2.1.3.3.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.4.4" class="ltx_tr">
<td id="S1.T2.1.4.4.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.4.4.1.1" class="ltx_text" style="font-size:90%;">MFCCs, MobileNetV2</span></td>
<td id="S1.T2.1.4.4.2" class="ltx_td ltx_align_center">
<span id="S1.T2.1.4.4.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span id="S1.T2.1.4.4.2.1.1" class="ltx_p"><span id="S1.T2.1.4.4.2.1.1.1" class="ltx_text" style="font-size:90%;">UAR</span></span>
</span>
</td>
<td id="S1.T2.1.4.4.3" class="ltx_td ltx_align_center">
<span id="S1.T2.1.4.4.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.4.4.3.1.1" class="ltx_p"><span id="S1.T2.1.4.4.3.1.1.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
<span id="S1.T2.1.4.4.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.4.4.3.1.2.1" class="ltx_text" style="font-size:90%;">5-Fold</span></span>
</span>
</td>
<td id="S1.T2.1.4.4.4" class="ltx_td ltx_align_center">
<span id="S1.T2.1.4.4.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.4.4.4.1.1" class="ltx_p"><span id="S1.T2.1.4.4.4.1.1.1" class="ltx_text" style="font-size:90%;">9,718</span></span>
<span id="S1.T2.1.4.4.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.4.4.4.1.2.1" class="ltx_text" style="font-size:90%;">4,798</span></span>
</span>
</td>
<td id="S1.T2.1.4.4.5" class="ltx_td"></td>
<td id="S1.T2.1.4.4.6" class="ltx_td"></td>
<td id="S1.T2.1.4.4.7" class="ltx_td"></td>
<td id="S1.T2.1.4.4.8" class="ltx_td"></td>
<td id="S1.T2.1.4.4.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.5.5" class="ltx_tr">
<td id="S1.T2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.5.5.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.5.5.1.1.1" class="ltx_p"><span id="S1.T2.1.5.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">MAR</span></span>
</span>
</td>
<td id="S1.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.5.5.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.5.5.2.1.1" class="ltx_p"><span id="S1.T2.1.5.5.2.1.1.1" class="ltx_text" style="font-size:90%;">UCF101</span></span>
<span id="S1.T2.1.5.5.2.1.2" class="ltx_p"><span id="S1.T2.1.5.5.2.1.2.1" class="ltx_text" style="font-size:90%;">MiT10</span></span>
<span id="S1.T2.1.5.5.2.1.3" class="ltx_p"><span id="S1.T2.1.5.5.2.1.3.1" class="ltx_text" style="font-size:90%;">MiT51</span></span>
</span>
</td>
<td id="S1.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.5.5.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.5.5.3.1.1" class="ltx_p"><span id="S1.T2.1.5.5.3.1.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
<span id="S1.T2.1.5.5.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.5.5.3.1.2.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
<span id="S1.T2.1.5.5.3.1.3" class="ltx_p ltx_align_center"><span id="S1.T2.1.5.5.3.1.3.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
</span>
</td>
<td id="S1.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.5.5.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:14.2pt;">
<span id="S1.T2.1.5.5.4.1.1" class="ltx_p"><span id="S1.T2.1.5.5.4.1.1.1" class="ltx_text" style="font-size:90%;">100</span></span>
<span id="S1.T2.1.5.5.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.5.5.4.1.2.1" class="ltx_text" style="font-size:90%;">200</span></span>
<span id="S1.T2.1.5.5.4.1.3" class="ltx_p ltx_align_center"><span id="S1.T2.1.5.5.4.1.3.1" class="ltx_text" style="font-size:90%;">2000</span></span>
</span>
</td>
<td id="S1.T2.1.5.5.5" class="ltx_td ltx_align_right ltx_border_t">
<span id="S1.T2.1.5.5.5.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.5.5.5.2" class="ltx_text" style="font-size:90%;">1.7cmAudio, Video</span>
</td>
<td id="S1.T2.1.5.5.6" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.5.5.7" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.5.5.8" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.5.5.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S1.T2.1.6.6" class="ltx_tr">
<td id="S1.T2.1.6.6.1" class="ltx_td ltx_align_center"><span id="S1.T2.1.6.6.1.1" class="ltx_text" style="font-size:90%;">Audio, Video</span></td>
<td id="S1.T2.1.6.6.2" class="ltx_td"></td>
<td id="S1.T2.1.6.6.3" class="ltx_td"></td>
<td id="S1.T2.1.6.6.4" class="ltx_td"></td>
<td id="S1.T2.1.6.6.5" class="ltx_td"></td>
<td id="S1.T2.1.6.6.6" class="ltx_td"></td>
<td id="S1.T2.1.6.6.7" class="ltx_td"></td>
<td id="S1.T2.1.6.6.8" class="ltx_td"></td>
<td id="S1.T2.1.6.6.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.7.7" class="ltx_tr">
<td id="S1.T2.1.7.7.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Audio, Video</span></td>
<td id="S1.T2.1.7.7.2" class="ltx_td ltx_align_right">
<span id="S1.T2.1.7.7.2.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.7.7.2.2" class="ltx_text" style="font-size:90%;">3cmMFCCs, MobileNetV2</span>
</td>
<td id="S1.T2.1.7.7.3" class="ltx_td"></td>
<td id="S1.T2.1.7.7.4" class="ltx_td"></td>
<td id="S1.T2.1.7.7.5" class="ltx_td"></td>
<td id="S1.T2.1.7.7.6" class="ltx_td"></td>
<td id="S1.T2.1.7.7.7" class="ltx_td"></td>
<td id="S1.T2.1.7.7.8" class="ltx_td"></td>
<td id="S1.T2.1.7.7.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.8.8" class="ltx_tr">
<td id="S1.T2.1.8.8.1" class="ltx_td ltx_align_center"><span id="S1.T2.1.8.8.1.1" class="ltx_text" style="font-size:90%;">MFCCs, MobileNetV2</span></td>
<td id="S1.T2.1.8.8.2" class="ltx_td"></td>
<td id="S1.T2.1.8.8.3" class="ltx_td"></td>
<td id="S1.T2.1.8.8.4" class="ltx_td"></td>
<td id="S1.T2.1.8.8.5" class="ltx_td"></td>
<td id="S1.T2.1.8.8.6" class="ltx_td"></td>
<td id="S1.T2.1.8.8.7" class="ltx_td"></td>
<td id="S1.T2.1.8.8.8" class="ltx_td"></td>
<td id="S1.T2.1.8.8.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.9.9" class="ltx_tr">
<td id="S1.T2.1.9.9.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.9.9.1.1" class="ltx_text" style="font-size:90%;">MFCCs, MobileNetV2</span></td>
<td id="S1.T2.1.9.9.2" class="ltx_td ltx_align_center">
<span id="S1.T2.1.9.9.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span id="S1.T2.1.9.9.2.1.1" class="ltx_p ltx_align_center"><span id="S1.T2.1.9.9.2.1.1.1" class="ltx_text" style="font-size:90%;">Top1</span></span>
<span id="S1.T2.1.9.9.2.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.9.9.2.1.2.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
</span>
</td>
<td id="S1.T2.1.9.9.3" class="ltx_td ltx_align_center">
<span id="S1.T2.1.9.9.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.9.9.3.1.1" class="ltx_p"><span id="S1.T2.1.9.9.3.1.1.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
</span>
</td>
<td id="S1.T2.1.9.9.4" class="ltx_td ltx_align_center">
<span id="S1.T2.1.9.9.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.9.9.4.1.1" class="ltx_p"><span id="S1.T2.1.9.9.4.1.1.1" class="ltx_text" style="font-size:90%;">6,837</span></span>
<span id="S1.T2.1.9.9.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.9.9.4.1.2.1" class="ltx_text" style="font-size:90%;">41.6K</span></span>
<span id="S1.T2.1.9.9.4.1.3" class="ltx_p ltx_align_center"><span id="S1.T2.1.9.9.4.1.3.1" class="ltx_text" style="font-size:90%;">157.6K</span></span>
</span>
</td>
<td id="S1.T2.1.9.9.5" class="ltx_td"></td>
<td id="S1.T2.1.9.9.6" class="ltx_td"></td>
<td id="S1.T2.1.9.9.7" class="ltx_td"></td>
<td id="S1.T2.1.9.9.8" class="ltx_td"></td>
<td id="S1.T2.1.9.9.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.10.10" class="ltx_tr">
<td id="S1.T2.1.10.10.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.10.10.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.10.10.1.1.1" class="ltx_p"><span id="S1.T2.1.10.10.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">HAR</span></span>
</span>
</td>
<td id="S1.T2.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.10.10.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.10.10.2.1.1" class="ltx_p"><span id="S1.T2.1.10.10.2.1.1.1" class="ltx_text" style="font-size:90%;">UCI-HAR</span></span>
<span id="S1.T2.1.10.10.2.1.2" class="ltx_p"><span id="S1.T2.1.10.10.2.1.2.1" class="ltx_text" style="font-size:90%;">KU-HAR</span></span>
</span>
</td>
<td id="S1.T2.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.10.10.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.10.10.3.1.1" class="ltx_p"><span id="S1.T2.1.10.10.3.1.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
<span id="S1.T2.1.10.10.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.10.10.3.1.2.1" class="ltx_text" style="font-size:90%;">Natural</span></span>
</span>
</td>
<td id="S1.T2.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.10.10.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:14.2pt;">
<span id="S1.T2.1.10.10.4.1.1" class="ltx_p"><span id="S1.T2.1.10.10.4.1.1.1" class="ltx_text" style="font-size:90%;">105</span></span>
<span id="S1.T2.1.10.10.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.10.10.4.1.2.1" class="ltx_text" style="font-size:90%;">66</span></span>
</span>
</td>
<td id="S1.T2.1.10.10.5" class="ltx_td ltx_align_right ltx_border_t">
<span id="S1.T2.1.10.10.5.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.10.10.5.2" class="ltx_text" style="font-size:90%;">1.7cmAcc, Gyro</span>
</td>
<td id="S1.T2.1.10.10.6" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.10.10.7" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.10.10.8" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.10.10.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S1.T2.1.11.11" class="ltx_tr">
<td id="S1.T2.1.11.11.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.11.11.1.1" class="ltx_text" style="font-size:90%;">Acc, Gyro</span></td>
<td id="S1.T2.1.11.11.2" class="ltx_td ltx_align_right">
<span id="S1.T2.1.11.11.2.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.11.11.2.2" class="ltx_text" style="font-size:90%;">3cmRaw</span>
</td>
<td id="S1.T2.1.11.11.3" class="ltx_td"></td>
<td id="S1.T2.1.11.11.4" class="ltx_td"></td>
<td id="S1.T2.1.11.11.5" class="ltx_td"></td>
<td id="S1.T2.1.11.11.6" class="ltx_td"></td>
<td id="S1.T2.1.11.11.7" class="ltx_td"></td>
<td id="S1.T2.1.11.11.8" class="ltx_td"></td>
<td id="S1.T2.1.11.11.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.12.12" class="ltx_tr">
<td id="S1.T2.1.12.12.1" class="ltx_td ltx_align_right"><span id="S1.T2.1.12.12.1.1" class="ltx_text" style="font-size:90%;">Raw</span></td>
<td id="S1.T2.1.12.12.2" class="ltx_td ltx_align_center">
<span id="S1.T2.1.12.12.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span id="S1.T2.1.12.12.2.1.1" class="ltx_p"><span id="S1.T2.1.12.12.2.1.1.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S1.T2.1.12.12.3" class="ltx_td ltx_align_center">
<span id="S1.T2.1.12.12.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.12.12.3.1.1" class="ltx_p"><span id="S1.T2.1.12.12.3.1.1.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
<span id="S1.T2.1.12.12.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.12.12.3.1.2.1" class="ltx_text" style="font-size:90%;">5-Fold</span></span>
</span>
</td>
<td id="S1.T2.1.12.12.4" class="ltx_td ltx_align_center">
<span id="S1.T2.1.12.12.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.12.12.4.1.1" class="ltx_p"><span id="S1.T2.1.12.12.4.1.1.1" class="ltx_text" style="font-size:90%;">8,979</span></span>
<span id="S1.T2.1.12.12.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.12.12.4.1.2.1" class="ltx_text" style="font-size:90%;">10.3K</span></span>
</span>
</td>
<td id="S1.T2.1.12.12.5" class="ltx_td"></td>
<td id="S1.T2.1.12.12.6" class="ltx_td"></td>
<td id="S1.T2.1.12.12.7" class="ltx_td"></td>
<td id="S1.T2.1.12.12.8" class="ltx_td"></td>
<td id="S1.T2.1.12.12.9" class="ltx_td"></td>
</tr>
<tr id="S1.T2.1.13.13" class="ltx_tr">
<td id="S1.T2.1.13.13.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.13.13.1.1.1" class="ltx_p"><span id="S1.T2.1.13.13.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Health</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.13.13.2.1.1" class="ltx_p"><span id="S1.T2.1.13.13.2.1.1.1" class="ltx_text" style="font-size:90%;">PTB-XL</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.13.13.3.1.1" class="ltx_p"><span id="S1.T2.1.13.13.3.1.1.1" class="ltx_text" style="font-size:90%;">Natural</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:14.2pt;">
<span id="S1.T2.1.13.13.4.1.1" class="ltx_p"><span id="S1.T2.1.13.13.4.1.1.1" class="ltx_text" style="font-size:90%;">34</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.5.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.13.13.5.2" class="ltx_text" style="font-size:90%;">1.8cmI-AVF, V1-V6</span>
</td>
<td id="S1.T2.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.6.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.13.13.6.2" class="ltx_text" style="font-size:90%;">3cmRaw</span>
</td>
<td id="S1.T2.1.13.13.7" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.7.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span id="S1.T2.1.13.13.7.1.1" class="ltx_p"><span id="S1.T2.1.13.13.7.1.1.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.8" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.8.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.13.13.8.1.1" class="ltx_p"><span id="S1.T2.1.13.13.8.1.1.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
</span>
</td>
<td id="S1.T2.1.13.13.9" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.13.13.9.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.13.13.9.1.1" class="ltx_p"><span id="S1.T2.1.13.13.9.1.1.1" class="ltx_text" style="font-size:90%;">21.7K</span></span>
</span>
</td>
</tr>
<tr id="S1.T2.1.14.14" class="ltx_tr">
<td id="S1.T2.1.14.14.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.14.14.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.14.14.1.1.1" class="ltx_p"><span id="S1.T2.1.14.14.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SM</span></span>
</span>
</td>
<td id="S1.T2.1.14.14.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.14.14.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S1.T2.1.14.14.2.1.1" class="ltx_p"><span id="S1.T2.1.14.14.2.1.1.1" class="ltx_text" style="font-size:90%;">Hateful-Memes</span></span>
<span id="S1.T2.1.14.14.2.1.2" class="ltx_p"><span id="S1.T2.1.14.14.2.1.2.1" class="ltx_text" style="font-size:90%;">CrisisMMD</span></span>
</span>
</td>
<td id="S1.T2.1.14.14.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.14.14.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.14.14.3.1.1" class="ltx_p"><span id="S1.T2.1.14.14.3.1.1.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
<span id="S1.T2.1.14.14.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.14.14.3.1.2.1" class="ltx_text" style="font-size:90%;">Synthetic</span></span>
</span>
</td>
<td id="S1.T2.1.14.14.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.14.14.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:14.2pt;">
<span id="S1.T2.1.14.14.4.1.1" class="ltx_p"><span id="S1.T2.1.14.14.4.1.1.1" class="ltx_text" style="font-size:90%;">50</span></span>
<span id="S1.T2.1.14.14.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.14.14.4.1.2.1" class="ltx_text" style="font-size:90%;">100</span></span>
</span>
</td>
<td id="S1.T2.1.14.14.5" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T2.1.14.14.5.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.14.14.5.2" class="ltx_text" style="font-size:90%;">1.8cmImage, Text</span>
</td>
<td id="S1.T2.1.14.14.6" class="ltx_td ltx_align_right ltx_border_t">
<span id="S1.T2.1.14.14.6.1" class="ltx_ERROR undefined">\pbox</span><span id="S1.T2.1.14.14.6.2" class="ltx_text" style="font-size:90%;">3cmMobileNetV2, MobileBert</span>
</td>
<td id="S1.T2.1.14.14.7" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.14.14.8" class="ltx_td ltx_border_t"></td>
<td id="S1.T2.1.14.14.9" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="S1.T2.1.15.15" class="ltx_tr">
<td id="S1.T2.1.15.15.1" class="ltx_td ltx_align_right ltx_border_bb"><span id="S1.T2.1.15.15.1.1" class="ltx_text" style="font-size:90%;">MobileNetV2, MobileBert</span></td>
<td id="S1.T2.1.15.15.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T2.1.15.15.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:21.3pt;">
<span id="S1.T2.1.15.15.2.1.1" class="ltx_p"><span id="S1.T2.1.15.15.2.1.1.1" class="ltx_text" style="font-size:90%;">AUC</span></span>
<span id="S1.T2.1.15.15.2.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.15.15.2.1.2.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S1.T2.1.15.15.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T2.1.15.15.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S1.T2.1.15.15.3.1.1" class="ltx_p"><span id="S1.T2.1.15.15.3.1.1.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
<span id="S1.T2.1.15.15.3.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.15.15.3.1.2.1" class="ltx_text" style="font-size:90%;">Pre-defined</span></span>
</span>
</td>
<td id="S1.T2.1.15.15.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S1.T2.1.15.15.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S1.T2.1.15.15.4.1.1" class="ltx_p"><span id="S1.T2.1.15.15.4.1.1.1" class="ltx_text" style="font-size:90%;">10.0K</span></span>
<span id="S1.T2.1.15.15.4.1.2" class="ltx_p ltx_align_center"><span id="S1.T2.1.15.15.4.1.2.1" class="ltx_text" style="font-size:90%;">18.1K</span></span>
</span>
</td>
<td id="S1.T2.1.15.15.5" class="ltx_td ltx_border_bb"></td>
<td id="S1.T2.1.15.15.6" class="ltx_td ltx_border_bb"></td>
<td id="S1.T2.1.15.15.7" class="ltx_td ltx_border_bb"></td>
<td id="S1.T2.1.15.15.8" class="ltx_td ltx_border_bb"></td>
<td id="S1.T2.1.15.15.9" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</figure>
<div id="S1.p6" class="ltx_para ltx_noindent">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_bold">Our Contributions:</span> In this work, we introduce FedMultimodal, a FL benchmark for multimodal applications. We summarize our key contributions as follows:</p>
</div>
<div id="S1.p7" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">FedMultimodal includes <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">ten representative datasets</span> covering <span id="S1.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">five diverse application scenarios</span> – emotion recognition, multimodal action recognition, human activity recognition, healthcare, and social media – that are well aligned with FL. We present systematic benchmark results on the above datasets to facilitate researchers to fairly compare their algorithms.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">To help the community accurately compare performance and ensure <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">reproducibility</span>, FedMultimodal presents an <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">open-source end-to-end FL simulation framework</span> and includes capabilities to perform data partitioning, feature processing, and multimodal training.
FedMultimodal offers support for several popular FL optimizers including FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, FedRS <cite class="ltx_cite ltx_citemacro_citep">(Li and Zhan, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, SCAFFOLD <cite class="ltx_cite ltx_citemacro_citep">(Karimireddy et al<span class="ltx_text">.</span>, <a href="#bib.bib36" title="" class="ltx_ref">2020</a>)</cite>, and FedOpt <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite>, and provide flexibility that allows users to customized the trainers on the included datasets. The source codes and user guides are available at <a target="_blank" href="https://github.com/usc-sail/fed-multimodal" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_bold">https://github.com/usc-sail/fed-multimodal</a>.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">In addition to ensuring accessibility and reproducibility, the benchmark provides a <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">robustness assessment module</span> that allows researchers to simulate challenges uniquely tied to multimodal FL applications in real-world scenarios. As listed in Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, previous works on multimodal FL provide limited assessments of the robustness under real-world settings. Specifically, FedMultimodal emulates <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_bold">missing modalities</span>, <span id="S1.I1.i3.p1.1.3" class="ltx_text ltx_font_bold">missing labels</span>, and <span id="S1.I1.i3.p1.1.4" class="ltx_text ltx_font_bold">erroneous labels</span> on top of the provided datasets to simulate scenarios when deploying FL systems in real-world settings. This is a crucial difference and a unique contribution of FedMultimodal compared to existing FL literature.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Multimodal Datasets and Tasks</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Table <a href="#S1.T2" title="Table 2 ‣ 1. Introduction ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides an overview of the 10 datasets included in
FedMultimodal. These 10 multimodal datasets cover five diverse tasks – Emotion Recognition, Multimedia Action Recognition, Human Activity Recognition, Healthcare, and Social Media classification. One important reason we select these 10 datasets is that they are publicly available, thus ensuring ease of accessibility and reproducibility. In this section, we provide a brief overview of each included dataset and the corresponding tasks.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Emotion Recognition (ER)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Emotion recognition (ER) has broad applicability of ER in virtual assistant-based tasks, human behavior analysis, and AI-assisted education, making it a valuable research topic in FL <cite class="ltx_cite ltx_citemacro_citep">(Tsouvalas et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2022</a>; Feng and Narayanan, <a href="#bib.bib22" title="" class="ltx_ref">2022</a>)</cite>. FedMultimodal benchmark incorporates two widely used datasets in this category: MELD and CREMA-D.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.p2.1" class="ltx_p"><span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">MELD</span> is a multiparty dialog dataset <cite class="ltx_cite ltx_citemacro_citep">(Poria et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2018</a>)</cite> containing over 9k utterances with audio and transcripts data from the Friends TV series. Due to the imbalanced label distribution in the dataset, we keep 4 emotions with the most samples i.e., neutral, happy, sad, and angry.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.1" class="ltx_p"><span id="S2.SS1.p3.1.1" class="ltx_text ltx_font_bold">CREMA-D</span> has 7,442 audio-visual clips recorded by 91 actors <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2014</a>)</cite>. Each speaker was instructed to utter 12 sentences emulating 6 emotions: neutral, happy, anger, disgust, fear, and sad.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Multimodal Action Recognition (MAR)</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The task of MAR consists of classifying a video into action categories based on underlying visual and audio modalities. In FedMultimodal, we include two well-known MAR testbeds: UCF101 and Moments in Time (MiT).</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">UCF101</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Soomro et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2012</a>)</cite> consists of 13,320 web videos with 101 sport-based action labels. However, data associated with only 51 labels are presented with video and audio information, resulting in less than 7,000 videos for the experiments. The duration of the videos ranges from several seconds to over 20 seconds. We subsample the video at the frame rate of 1Hz to reduce the computation overhead.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Moments in Time (MiT)</span> is a large-scale MAR ( 1 million) dataset <cite class="ltx_cite ltx_citemacro_citep">(Monfort et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite> with short (3 seconds) videos with overall list of 339 action labels. It is worth noting that MiT is a challenging dataset, with state-of-the-art top-1 accuracy close to 35% <cite class="ltx_cite ltx_citemacro_citep">(Ryoo et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2019</a>)</cite>. Given the inherent difficulty of this task, we tackle the easier classification problem by creating partitions of data with fewer distinct labels. We create two sub-datasets, MiT10 and MiT51, from the original MiT dataset. MiT10 and MiT51 contain videos of the 10 and the 51 most frequent labels. Similar to the UCF101 setting, we subsample the video every 10 frames to accommodate the computing constraints in FL.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Human Activity Recognition (HAR)</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">HAR identifies human actions based on wearable data such as accelerometers and gyroscopes. Due to the nature of its wearable-friendly attribute, it has become a prevalent research topic in FL <cite class="ltx_cite ltx_citemacro_citep">(Sannara et al<span class="ltx_text">.</span>, <a href="#bib.bib62" title="" class="ltx_ref">2021</a>)</cite>. FedMultimodal provides the implementation on two HAR datasets: UCI-HAR and KU-HAR. In our experiments, we treat the accelerometer and gyroscope data as two different modalities.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">UCI-HAR</span> dataset <cite class="ltx_cite ltx_citemacro_citep">(Anguita et al<span class="ltx_text">.</span>, <a href="#bib.bib4" title="" class="ltx_ref">2013</a>)</cite> consists of smartphone sensors (Accelerometer and Gyroscope) data from 30 subjects (19-48 yrs old) performing six daily activities: walking, walking upstairs, walking downstairs, sitting, standing, laying. The participants wear smartphones on their waists during the collection phase. The accelerometer and gyroscope data are sampled at 50Hz.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.1" class="ltx_p"><span id="S2.SS3.p3.1.1" class="ltx_text ltx_font_bold">KU-HAR</span> is a recent human activity recognition dataset <cite class="ltx_cite ltx_citemacro_citep">(Sikder and Nahid, <a href="#bib.bib63" title="" class="ltx_ref">2021</a>)</cite> collected with a group of 90 participants (75 male and 15 female) on 18 different activities. Instead of evaluating the 18 activities, we decided to keep 6 activities existing in the UCI-HAR dataset while adding jumping and running activities.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2306.09486/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>The overall architecture of the end-to-end multimodal federated learning framework included in FedMultimodal.</figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4. </span>Healthcare</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Healthcare ML applications have made immense progress in a range of domains e.g., heart-disease classification over the last decade. FedMultimodal explores the problem of ECG classification based on the PTB-XL <cite class="ltx_cite ltx_citemacro_citep">(Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite> dataset.
<span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_bold">PTB-XL</span> <cite class="ltx_cite ltx_citemacro_citep">(Wagner et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2020</a>)</cite> includes over 20,000 clinical 12-lead ECG recordings from 18,885 patients for a multi-label classification task. There are 5 classes describing ECG diagnosis, including normal ECG, myocardial infarction, ST/T change, conduction disturbance, and hypertrophy. As suggested by <cite class="ltx_cite ltx_citemacro_citep">(Strodthoff et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2020</a>)</cite>, we use the ECG data provided at the sampling frequency of 100 Hz. We separate the readings from electrodes I, II, III, AVL, AVR, AVF, and V1-V6 as two modalities suggested by <cite class="ltx_cite ltx_citemacro_citep">(Alday et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.5. </span>Social Media (SM)</h3>

<div id="S2.SS5.p1" class="ltx_para">
<p id="S2.SS5.p1.1" class="ltx_p">Social media has become an increasingly important tool during disasters and emergencies for people to track the latest updates in the area, especially the impact (e.g., property damage, injury, and death) of the disaster, as well as urgent needs for help. However, the widespread adoption of social media has also drawn significant concerns about spreading misinformation, thus urging the need to identify and mitigate this misleading and harmful content. To accelerate FL research in this domain, FedMultimodal incorporates two social-media-based multimodal datasets related to hateful content and crisis information classification.</p>
</div>
<div id="S2.SS5.p2" class="ltx_para ltx_noindent">
<p id="S2.SS5.p2.1" class="ltx_p"><span id="S2.SS5.p2.1.1" class="ltx_text ltx_font_bold">Hateful Memes</span> dataset was released from the 2020 Hateful Memes Challenge <cite class="ltx_cite ltx_citemacro_citep">(Kiela et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite> that focus on detecting hateful speech in memes. The database includes 10,000 multimodal data with image and text pairs with binary classes.</p>
</div>
<div id="S2.SS5.p3" class="ltx_para ltx_noindent">
<p id="S2.SS5.p3.1" class="ltx_p"><span id="S2.SS5.p3.1.1" class="ltx_text ltx_font_bold">CrisisMMD</span> <cite class="ltx_cite ltx_citemacro_citep">(Alam et al<span class="ltx_text">.</span>, <a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> comprises 18.1k tweets containing both paired visual and textual information. It collects relevant tweets from seven prominent natural disasters such as California Wildfires (2017). One of the purposes of the dataset is to identify the impact of the disaster like utility damage and injured or dead people.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>End-to-end Multimodal Federated Learning Framework</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To benchmark the performance of the multimodal datasets described in Section <a href="#S2" title="2. Multimodal Datasets and Tasks ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> as well as to support future research in the area of multimodal federated learning, we have built an end-to-end multimodal federated learning research framework.
Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3. Human Activity Recognition (HAR) ‣ 2. Multimodal Datasets and Tasks ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> <span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3. Human Activity Recognition (HAR) ‣ 2. Multimodal Datasets and Tasks ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> uses image sources from https://openmoji.org/</span></span></span> illustrates the overall architecture of the framework.
As shown, our framework covers the complete pipeline of multimodal federated learning, which includes six key components: (1) non-IID data partitioning, (2) feature processing, (3) multimodal models, (4) fusion schemes, (5) federated optimizers, and (6) real-world noise factor emulator.
In particular, one key difference between FedMultimodal and existing multimodal FL literature is that FedMultimodal takes the real-world noise factors into consideration and examines model robustness to three real-world noise factors: missing modalities, missing labels, and erroneous labels.
In this section, we describe each of the six key components in detail.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Non-IID Data Partitioning</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">The non-IID data partitioning is a fundamental step in emulating FL experiments. The first partition scheme is through the unique client identifier. For example, speech-related datasets, like CREMA-D and MELD, comprise speech-text or speech-visual data organized by speaker IDs. Hence, it is natural to use speaker IDs to partition the client data in FL, creating authentic non-IID data distributions. Similarly, we consider partitions in datasets like KU-HAR and PTB-XL comprise data with based on participant IDs and clinical site IDs, respectively.
On the other hand, other multimodal datasets used in this paper, including MAR and SM datasets, do not have such realistic client partitions thus requiring ML practitioners to synthesize non-IID data distributions. Following prior works, we partition these datasets using Dirichlet distribution with <math id="S3.SS1.p1.1.m1.2" class="ltx_Math" alttext="\alpha\in\{0.1,5.0\}" display="inline"><semantics id="S3.SS1.p1.1.m1.2a"><mrow id="S3.SS1.p1.1.m1.2.3" xref="S3.SS1.p1.1.m1.2.3.cmml"><mi id="S3.SS1.p1.1.m1.2.3.2" xref="S3.SS1.p1.1.m1.2.3.2.cmml">α</mi><mo id="S3.SS1.p1.1.m1.2.3.1" xref="S3.SS1.p1.1.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS1.p1.1.m1.2.3.3.2" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml"><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.3.2.1" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">{</mo><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">0.1</mn><mo id="S3.SS1.p1.1.m1.2.3.3.2.2" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2.cmml">5.0</mn><mo stretchy="false" id="S3.SS1.p1.1.m1.2.3.3.2.3" xref="S3.SS1.p1.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.2b"><apply id="S3.SS1.p1.1.m1.2.3.cmml" xref="S3.SS1.p1.1.m1.2.3"><in id="S3.SS1.p1.1.m1.2.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.1"></in><ci id="S3.SS1.p1.1.m1.2.3.2.cmml" xref="S3.SS1.p1.1.m1.2.3.2">𝛼</ci><set id="S3.SS1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS1.p1.1.m1.2.3.3.2"><cn type="float" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">0.1</cn><cn type="float" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">5.0</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.2c">\alpha\in\{0.1,5.0\}</annotation></semantics></math> to control the level of data heterogeneity, where <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\alpha=0.1" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">α</mi><mo id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><eq id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></eq><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝛼</ci><cn type="float" id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\alpha=0.1</annotation></semantics></math> and <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="\alpha=5.0" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">α</mi><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">5.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><eq id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></eq><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝛼</ci><cn type="float" id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">5.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\alpha=5.0</annotation></semantics></math> represents high heterogeneity and low heterogeneity, respectively. Although the original UCI-HAR datasets consist of data partitioned by participants, each participant performed the same amount of activities from each category, making the label distribution IID in UCI-HAR. Hence, we increase the heterogeneity of data distribution by dividing each participant’s data using the Dirichlet distribution.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Feature Processing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Instead of training the model using raw input data like images and texts from scratch, FedMultimodal leverages well-established pre-trained models as backbone networks to extract features for the training downstream models. Unlike the centralized training paradigm, feature processing in federated learning benchmarks demands considerations in computation efficiency and feasibility. Mainly, the selected feature needs to align with the computation capabilities available on the edge computing devices. For example, it is unrealistic to assume that edge devices could load and run large transformer-based <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib69" title="" class="ltx_ref">2017</a>)</cite> models for inference purposes without sacrificing system performance. Hence, we focus on implementing mobile-friendly feature extraction pipelines in FedMultimodal which are listed below, targeting swift computation, efficient storage, and ease of deployment.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Visual</span>: For the visual data, our benchmark supports <span id="S3.I1.i1.p1.1.2" class="ltx_text ltx_font_bold">MobileNetV2</span> <cite class="ltx_cite ltx_citemacro_citep">(Howard et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2017</a>)</cite> and <span id="S3.I1.i1.p1.1.3" class="ltx_text ltx_font_bold">MobileViT</span> <cite class="ltx_cite ltx_citemacro_citep">(Mehta and Rastegari, <a href="#bib.bib48" title="" class="ltx_ref">2021</a>)</cite> as the embedding network to extract latent presentations. The complete MobileNetV2 and MobileViT have 4.3M and 2.7M parameters, respectively, making them practical visual feature backbones in FL. Due to space constraints, we report benchmark results with MobileNetV2 in this paper.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Text</span>: FedMultimodal integrates both <span id="S3.I1.i2.p1.1.2" class="ltx_text ltx_font_bold">MobileBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2020</a>)</cite> and <span id="S3.I1.i2.p1.1.3" class="ltx_text ltx_font_bold">DistillBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Sanh et al<span class="ltx_text">.</span>, <a href="#bib.bib61" title="" class="ltx_ref">2019</a>)</cite> to extract representations from textual data. MobileBERT uses a bottleneck structure to reduce the parameter size from 340M to 25M when compared to BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib14" title="" class="ltx_ref">2019</a>)</cite>, while DistillBERT applies a knowledge distillation process that decreases the BERT model to 66M parameters. We decide to benchmark with the MobileBERT feature backbone given the page constraints.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Audio</span>: FedMultimodal uses Mel-frequency cepstral coefficients (MFCCs) due to their widespread usage in the state-of-the-art speech recognition models like Wav2Vec 2.0 <cite class="ltx_cite ltx_citemacro_citep">(Chen and Rudnicky, <a href="#bib.bib12" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p"><span id="S3.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Other Modalities</span>: We use the raw data with the remaining modalities in the FedMultimodal. These data streams include accelerometer, gyroscope, and ECG readings.</p>
</div>
</li>
</ul>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2306.09486/assets/model.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="341" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The architecture of the basic model.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Multimodal Models</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Model Design Principles.</span>
Compared to remote servers, edge computing nodes are more appropriate for lightweight computing tasks due to constraints in computation resources, storage capabilities, battery capacities, and communication bandwidths. When designing ML models for resource-constrained devices, a significant design consideration is to reduce the number of parameters in edge ML models, thus reducing memory and execution latency. Such models can either be the backbone feature extraction models or application-specific prediction models. One major design principle of FedMultimodal is to study lightweight but effective solutions for multimodal FL learning instead of training models with multi-million parameters.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Model Architecture.</span>
With this design principle in mind, we construct ML models mainly based on the 1D Conv-RNN/MLP architecture. Even though the transformer-based model has achieved SOTA performance in diverse applications, these models typically include millions or even billions of parameters, making them impractical to use in FL settings as a result of massive computations, memory usage, and battery consumption during back-propagation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a href="#bib.bib71" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">An example model architecture is presented in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2. Feature Processing ‣ 3. End-to-end Multimodal Federated Learning Framework ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Specifically, the multimodal model in FedMultimodal includes an encoder, a modality-fusion block, and a downstream classifier. The encoder part follows either Conv+RNN architecture or RNN-only architecture. The encoder that adopts Conv+RNN architecture takes the input of audio, accelerometer, gyroscope, and ECG information, otherwise uses RNN-only architecture. Following encoder modules, FedMultimodal uses a late-fusion mechanism to combine modality-specific representations into a multimodal representation. The multimodal representation is then fed through 2 dense layers for downstream predictions.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Hyperparameters for training FL models with multimodal data using FedAvg.</figcaption>
<table id="S3.T3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.1.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.1.1.1.1.1" class="ltx_p"><span id="S3.T3.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Multimodal
<br class="ltx_break">Dataset</span></span>
</span>
</th>
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.1.1.1.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.1.1.2.1.1" class="ltx_p"><span id="S3.T3.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Client
<br class="ltx_break">Sample Rate</span></span>
</span>
</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.1.1.1.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:42.7pt;">
<span id="S3.T3.1.1.1.3.1.1" class="ltx_p"><span id="S3.T3.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Learning
<br class="ltx_break">Rate</span></span>
</span>
</th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S3.T3.1.1.1.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.1.1.4.1.1" class="ltx_p"><span id="S3.T3.1.1.1.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Training Rounds</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.1.2.1" class="ltx_tr">
<td id="S3.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.1.2.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.2.1.1.1.1" class="ltx_p"><span id="S3.T3.1.2.1.1.1.1.1" class="ltx_text" style="font-size:90%;">MELD</span></span>
<span id="S3.T3.1.2.1.1.1.2" class="ltx_p"><span id="S3.T3.1.2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">CREMA-D</span></span>
</span>
</td>
<td id="S3.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.1.2.1.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.2.1.2.1.1" class="ltx_p"><span id="S3.T3.1.2.1.2.1.1.1" class="ltx_text" style="font-size:90%;">10%</span></span>
<span id="S3.T3.1.2.1.2.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.2.1.2.1.2.1" class="ltx_text" style="font-size:90%;">10%</span></span>
</span>
</td>
<td id="S3.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.1.2.1.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S3.T3.1.2.1.3.1.1" class="ltx_p"><span id="S3.T3.1.2.1.3.1.1.1" class="ltx_text" style="font-size:90%;">0.01</span></span>
<span id="S3.T3.1.2.1.3.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.2.1.3.1.2.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
</span>
</td>
<td id="S3.T3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T3.1.2.1.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:48.4pt;">
<span id="S3.T3.1.2.1.4.1.1" class="ltx_p"><span id="S3.T3.1.2.1.4.1.1.1" class="ltx_text" style="font-size:90%;">200</span></span>
<span id="S3.T3.1.2.1.4.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.2.1.4.1.2.1" class="ltx_text" style="font-size:90%;">200</span></span>
</span>
</td>
</tr>
<tr id="S3.T3.1.3.2" class="ltx_tr">
<td id="S3.T3.1.3.2.1" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.3.2.1.1.1" class="ltx_p"><span id="S3.T3.1.3.2.1.1.1.1" class="ltx_text" style="font-size:90%;">UCF101</span></span>
<span id="S3.T3.1.3.2.1.1.2" class="ltx_p"><span id="S3.T3.1.3.2.1.1.2.1" class="ltx_text" style="font-size:90%;">MiT10</span></span>
<span id="S3.T3.1.3.2.1.1.3" class="ltx_p"><span id="S3.T3.1.3.2.1.1.3.1" class="ltx_text" style="font-size:90%;">MiT51</span></span>
</span>
</td>
<td id="S3.T3.1.3.2.2" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.2.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.3.2.2.1.1" class="ltx_p"><span id="S3.T3.1.3.2.2.1.1.1" class="ltx_text" style="font-size:90%;">10%</span></span>
<span id="S3.T3.1.3.2.2.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.2.1.2.1" class="ltx_text" style="font-size:90%;">5%</span></span>
<span id="S3.T3.1.3.2.2.1.3" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.2.1.3.1" class="ltx_text" style="font-size:90%;">5%</span></span>
</span>
</td>
<td id="S3.T3.1.3.2.3" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.2.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S3.T3.1.3.2.3.1.1" class="ltx_p"><span id="S3.T3.1.3.2.3.1.1.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
<span id="S3.T3.1.3.2.3.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.3.1.2.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
<span id="S3.T3.1.3.2.3.1.3" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.3.1.3.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
</span>
</td>
<td id="S3.T3.1.3.2.4" class="ltx_td ltx_align_center">
<span id="S3.T3.1.3.2.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:48.4pt;">
<span id="S3.T3.1.3.2.4.1.1" class="ltx_p"><span id="S3.T3.1.3.2.4.1.1.1" class="ltx_text" style="font-size:90%;">200</span></span>
<span id="S3.T3.1.3.2.4.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.4.1.2.1" class="ltx_text" style="font-size:90%;">300</span></span>
<span id="S3.T3.1.3.2.4.1.3" class="ltx_p ltx_align_center"><span id="S3.T3.1.3.2.4.1.3.1" class="ltx_text" style="font-size:90%;">300</span></span>
</span><span id="S3.T3.1.3.2.4.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:19.4pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="S3.T3.1.4.3" class="ltx_tr">
<td id="S3.T3.1.4.3.1" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.3.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.4.3.1.1.1" class="ltx_p"><span id="S3.T3.1.4.3.1.1.1.1" class="ltx_text" style="font-size:90%;">UCI-HAR</span></span>
<span id="S3.T3.1.4.3.1.1.2" class="ltx_p"><span id="S3.T3.1.4.3.1.1.2.1" class="ltx_text" style="font-size:90%;">KU-HAR</span></span>
</span>
</td>
<td id="S3.T3.1.4.3.2" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.3.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.4.3.2.1.1" class="ltx_p"><span id="S3.T3.1.4.3.2.1.1.1" class="ltx_text" style="font-size:90%;">10%</span></span>
<span id="S3.T3.1.4.3.2.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.4.3.2.1.2.1" class="ltx_text" style="font-size:90%;">10%</span></span>
</span>
</td>
<td id="S3.T3.1.4.3.3" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.3.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S3.T3.1.4.3.3.1.1" class="ltx_p"><span id="S3.T3.1.4.3.3.1.1.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
<span id="S3.T3.1.4.3.3.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.4.3.3.1.2.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
</span>
</td>
<td id="S3.T3.1.4.3.4" class="ltx_td ltx_align_center">
<span id="S3.T3.1.4.3.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:48.4pt;">
<span id="S3.T3.1.4.3.4.1.1" class="ltx_p"><span id="S3.T3.1.4.3.4.1.1.1" class="ltx_text" style="font-size:90%;">200</span></span>
<span id="S3.T3.1.4.3.4.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.4.3.4.1.2.1" class="ltx_text" style="font-size:90%;">200</span></span>
</span><span id="S3.T3.1.4.3.4.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:13.6pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="S3.T3.1.5.4" class="ltx_tr">
<td id="S3.T3.1.5.4.1" class="ltx_td ltx_align_center">
<span id="S3.T3.1.5.4.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.5.4.1.1.1" class="ltx_p"><span id="S3.T3.1.5.4.1.1.1.1" class="ltx_text" style="font-size:90%;">PTB-XL</span></span>
</span>
</td>
<td id="S3.T3.1.5.4.2" class="ltx_td ltx_align_center">
<span id="S3.T3.1.5.4.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.5.4.2.1.1" class="ltx_p"><span id="S3.T3.1.5.4.2.1.1.1" class="ltx_text" style="font-size:90%;">25%</span></span>
</span>
</td>
<td id="S3.T3.1.5.4.3" class="ltx_td ltx_align_center">
<span id="S3.T3.1.5.4.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S3.T3.1.5.4.3.1.1" class="ltx_p"><span id="S3.T3.1.5.4.3.1.1.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
</span>
</td>
<td id="S3.T3.1.5.4.4" class="ltx_td ltx_align_center">
<span id="S3.T3.1.5.4.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:48.4pt;">
<span id="S3.T3.1.5.4.4.1.1" class="ltx_p"><span id="S3.T3.1.5.4.4.1.1.1" class="ltx_text" style="font-size:90%;">200</span></span>
</span><span id="S3.T3.1.5.4.4.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:9.7pt;background:black;display:inline-block;"></span>
</td>
</tr>
<tr id="S3.T3.1.6.5" class="ltx_tr">
<td id="S3.T3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T3.1.6.5.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:54.1pt;">
<span id="S3.T3.1.6.5.1.1.1" class="ltx_p"><span id="S3.T3.1.6.5.1.1.1.1" class="ltx_text" style="font-size:90%;">Hateful-Memes</span></span>
<span id="S3.T3.1.6.5.1.1.2" class="ltx_p"><span id="S3.T3.1.6.5.1.1.2.1" class="ltx_text" style="font-size:90%;">CrisisMMD</span></span>
</span>
</td>
<td id="S3.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T3.1.6.5.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T3.1.6.5.2.1.1" class="ltx_p"><span id="S3.T3.1.6.5.2.1.1.1" class="ltx_text" style="font-size:90%;">25%</span></span>
<span id="S3.T3.1.6.5.2.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.6.5.2.1.2.1" class="ltx_text" style="font-size:90%;">10%</span></span>
</span>
</td>
<td id="S3.T3.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T3.1.6.5.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:35.6pt;">
<span id="S3.T3.1.6.5.3.1.1" class="ltx_p"><span id="S3.T3.1.6.5.3.1.1.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
<span id="S3.T3.1.6.5.3.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.6.5.3.1.2.1" class="ltx_text" style="font-size:90%;">0.05</span></span>
</span>
</td>
<td id="S3.T3.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">
<span id="S3.T3.1.6.5.4.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:51.2pt;">
<span id="S3.T3.1.6.5.4.1.1" class="ltx_p"><span id="S3.T3.1.6.5.4.1.1.1" class="ltx_text" style="font-size:90%;">200</span></span>
<span id="S3.T3.1.6.5.4.1.2" class="ltx_p ltx_align_center"><span id="S3.T3.1.6.5.4.1.2.1" class="ltx_text" style="font-size:90%;">200</span></span>
</span><span id="S3.T3.1.6.5.4.2" class="ltx_text" style="font-size:90%;"> </span><span class="ltx_rule" style="width:0.0pt;height:12.6pt;background:black;display:inline-block;"></span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Benchmarking performance. Text colors in red and blue denote the best performance using <span id="S3.T4.22.1" class="ltx_text ltx_font_italic">Concatanation-based Fusion</span> and <span id="S3.T4.23.2" class="ltx_text ltx_font_italic">Attention-based Fusion</span>, respectively. <math id="S3.T4.2.m1.1" class="ltx_Math" alttext="\dagger" display="inline"><semantics id="S3.T4.2.m1.1b"><mo id="S3.T4.2.m1.1.1" xref="S3.T4.2.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.m1.1c"><ci id="S3.T4.2.m1.1.1.cmml" xref="S3.T4.2.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.m1.1d">\dagger</annotation></semantics></math> indicates the best performance score of the corresponding dataset.</figcaption>
<table id="S3.T4.19" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.19.18.1" class="ltx_tr">
<th id="S3.T4.19.18.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.19.18.1.2" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.19.18.1.3" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt"></th>
<th id="S3.T4.19.18.1.4" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S3.T4.19.18.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T4.19.18.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Concatanation-based Fusion</span></th>
<th id="S3.T4.19.18.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4"><span id="S3.T4.19.18.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Attention-based Fusion</span></th>
</tr>
<tr id="S3.T4.3.1" class="ltx_tr">
<th id="S3.T4.3.1.2" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S3.T4.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row">
<span id="S3.T4.3.1.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.3.1.3.1.1" class="ltx_p"><span id="S3.T4.3.1.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ML Datasets</span></span>
</span>
</th>
<th id="S3.T4.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row"><math id="S3.T4.3.1.1.m1.1" class="ltx_Math" alttext="\mathbf{\alpha}" display="inline"><semantics id="S3.T4.3.1.1.m1.1a"><mi mathsize="90%" id="S3.T4.3.1.1.m1.1.1" xref="S3.T4.3.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.T4.3.1.1.m1.1b"><ci id="S3.T4.3.1.1.m1.1.1.cmml" xref="S3.T4.3.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.1.1.m1.1c">\mathbf{\alpha}</annotation></semantics></math></th>
<th id="S3.T4.3.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S3.T4.3.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Metric</span></th>
<th id="S3.T4.3.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedAvg</span></th>
<th id="S3.T4.3.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedProx</span></th>
<th id="S3.T4.3.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedRS</span></th>
<th id="S3.T4.3.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedOpt</span></th>
<th id="S3.T4.3.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedAvg</span></th>
<th id="S3.T4.3.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedProx</span></th>
<th id="S3.T4.3.1.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.11.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedRS</span></th>
<th id="S3.T4.3.1.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T4.3.1.12.1" class="ltx_text ltx_font_bold" style="font-size:90%;">FedOpt</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.7.5" class="ltx_tr">
<th id="S3.T4.7.5.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S3.T4.7.5.5.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:49.8pt;">
<span id="S3.T4.7.5.5.1.1" class="ltx_p"><span id="S3.T4.7.5.5.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Natural
<br class="ltx_break">Partition</span></span>
</span>
</th>
<th id="S3.T4.7.5.6" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<span id="S3.T4.7.5.6.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.7.5.6.1.1" class="ltx_p"><span id="S3.T4.7.5.6.1.1.1" class="ltx_text" style="font-size:90%;">MELD</span></span>
<span id="S3.T4.7.5.6.1.2" class="ltx_p"><span id="S3.T4.7.5.6.1.2.1" class="ltx_text" style="font-size:90%;">CREMA-D</span></span>
<span id="S3.T4.7.5.6.1.3" class="ltx_p"><span id="S3.T4.7.5.6.1.3.1" class="ltx_text" style="font-size:90%;">KU-HAR</span></span>
<span id="S3.T4.7.5.6.1.4" class="ltx_p"><span id="S3.T4.7.5.6.1.4.1" class="ltx_text" style="font-size:90%;">PTB-XL</span></span>
</span>
</th>
<th id="S3.T4.7.5.7" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T4.7.5.7.1" class="ltx_text" style="font-size:90%;">-</span></th>
<td id="S3.T4.7.5.8" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.8.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.8.1.1" class="ltx_p"><span id="S3.T4.7.5.8.1.1.1" class="ltx_text" style="font-size:90%;">UAR</span></span>
<span id="S3.T4.7.5.8.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.8.1.2.1" class="ltx_text" style="font-size:90%;">UAR</span></span>
<span id="S3.T4.7.5.8.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.8.1.3.1" class="ltx_text" style="font-size:90%;">F1</span></span>
<span id="S3.T4.7.5.8.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.8.1.4.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S3.T4.7.5.9" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.9.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.9.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.9.1.1.1" class="ltx_text" style="font-size:90%;">48.08</span></span>
<span id="S3.T4.7.5.9.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.9.1.2.1" class="ltx_text" style="font-size:90%;">61.52</span></span>
<span id="S3.T4.7.5.9.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.9.1.3.1" class="ltx_text" style="font-size:90%;">61.56</span></span>
<span id="S3.T4.7.5.9.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.9.1.4.1" class="ltx_text" style="font-size:90%;">61.87</span></span>
</span>
</td>
<td id="S3.T4.7.5.10" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.10.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.10.1.1" class="ltx_p"><span id="S3.T4.7.5.10.1.1.1" class="ltx_text" style="font-size:90%;">48.47</span></span>
<span id="S3.T4.7.5.10.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.10.1.2.1" class="ltx_text" style="font-size:90%;">61.64</span></span>
<span id="S3.T4.7.5.10.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.10.1.3.1" class="ltx_text" style="font-size:90%;">61.13</span></span>
<span id="S3.T4.7.5.10.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.10.1.4.1" class="ltx_text" style="font-size:90%;">61.00</span></span>
</span>
</td>
<td id="S3.T4.7.5.11" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.11.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.11.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.11.1.1.1" class="ltx_text" style="font-size:90%;">49.21</span></span>
<span id="S3.T4.7.5.11.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.11.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">62.17</span><span id="S3.T4.7.5.11.1.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.7.5.11.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.11.1.3.1" class="ltx_text" style="font-size:90%;">61.26</span></span>
<span id="S3.T4.7.5.11.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.11.1.4.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S3.T4.4.2.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.4.2.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.4.2.1.1.2" class="ltx_p"><span id="S3.T4.4.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">50.66</span><span id="S3.T4.4.2.1.1.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.4.2.1.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.4.2.1.1.3.1" class="ltx_text" style="font-size:90%;">61.14</span></span>
<span id="S3.T4.4.2.1.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.4.2.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">68.82</span><span id="S3.T4.4.2.1.1.4.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.4.2.1.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.4.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">62.83<sup id="S3.T4.4.2.1.1.1.1.1" class="ltx_sup"><span id="S3.T4.4.2.1.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.4.2.1.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
</span>
</td>
<td id="S3.T4.7.5.12" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.12.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.12.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.12.1.1.1" class="ltx_text" style="font-size:90%;">54.37</span></span>
<span id="S3.T4.7.5.12.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.12.1.2.1" class="ltx_text" style="font-size:90%;">61.66</span></span>
<span id="S3.T4.7.5.12.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.12.1.3.1" class="ltx_text" style="font-size:90%;">61.78</span></span>
<span id="S3.T4.7.5.12.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.12.1.4.1" class="ltx_text" style="font-size:90%;">61.88</span></span>
</span>
</td>
<td id="S3.T4.7.5.13" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.13.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.13.1.1" class="ltx_p"><span id="S3.T4.7.5.13.1.1.1" class="ltx_text" style="font-size:90%;">54.67</span></span>
<span id="S3.T4.7.5.13.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.13.1.2.1" class="ltx_text" style="font-size:90%;">62.03</span></span>
<span id="S3.T4.7.5.13.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.13.1.3.1" class="ltx_text" style="font-size:90%;">61.78</span></span>
<span id="S3.T4.7.5.13.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.13.1.4.1" class="ltx_text" style="font-size:90%;">61.71</span></span>
</span>
</td>
<td id="S3.T4.7.5.14" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.14.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.7.5.14.1.1" class="ltx_p"><span id="S3.T4.7.5.14.1.1.1" class="ltx_text" style="font-size:90%;">53.82</span></span>
<span id="S3.T4.7.5.14.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.14.1.2.1" class="ltx_text" style="font-size:90%;">60.41</span></span>
<span id="S3.T4.7.5.14.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.14.1.3.1" class="ltx_text" style="font-size:90%;">62.04</span></span>
<span id="S3.T4.7.5.14.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.14.1.4.1" class="ltx_text" style="font-size:90%;">-</span></span>
</span>
</td>
<td id="S3.T4.7.5.4" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.7.5.4.3" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.5.3.2.1.1" class="ltx_p"><span id="S3.T4.5.3.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">55.37<sup id="S3.T4.5.3.2.1.1.1.1" class="ltx_sup"><span id="S3.T4.5.3.2.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.5.3.2.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.6.4.3.2.2" class="ltx_p ltx_align_center"><span id="S3.T4.6.4.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">62.66<sup id="S3.T4.6.4.3.2.2.1.1" class="ltx_sup"><span id="S3.T4.6.4.3.2.2.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.6.4.3.2.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.7.5.4.3.3" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">71.41<sup id="S3.T4.7.5.4.3.3.1.1" class="ltx_sup"><span id="S3.T4.7.5.4.3.3.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.7.5.4.3.3.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.7.5.4.3.4" class="ltx_p ltx_align_center"><span id="S3.T4.7.5.4.3.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">62.42</span><span id="S3.T4.7.5.4.3.4.2" class="ltx_text" style="font-size:90%;"></span></span>
</span>
</td>
</tr>
<tr id="S3.T4.13.11" class="ltx_tr">
<th id="S3.T4.13.11.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">
<span id="S3.T4.13.11.7.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:49.8pt;">
<span id="S3.T4.13.11.7.1.1" class="ltx_p"><span id="S3.T4.13.11.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Synthetic
<br class="ltx_break">Partition</span></span>
</span>
</th>
<th id="S3.T4.13.11.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">
<span id="S3.T4.13.11.8.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.13.11.8.1.1" class="ltx_p"><span id="S3.T4.13.11.8.1.1.1" class="ltx_text" style="font-size:90%;">UCF101</span></span>
<span id="S3.T4.13.11.8.1.2" class="ltx_p"><span id="S3.T4.13.11.8.1.2.1" class="ltx_text" style="font-size:90%;">MiT10</span></span>
<span id="S3.T4.13.11.8.1.3" class="ltx_p"><span id="S3.T4.13.11.8.1.3.1" class="ltx_text" style="font-size:90%;">MiT51</span></span>
<span id="S3.T4.13.11.8.1.4" class="ltx_p"><span id="S3.T4.13.11.8.1.4.1" class="ltx_text" style="font-size:90%;">Hateful-Memes</span></span>
<span id="S3.T4.13.11.8.1.5" class="ltx_p"><span id="S3.T4.13.11.8.1.5.1" class="ltx_text" style="font-size:90%;">CrisisMMD</span></span>
<span id="S3.T4.13.11.8.1.6" class="ltx_p"><span id="S3.T4.13.11.8.1.6.1" class="ltx_text" style="font-size:90%;">UCI-HAR</span></span>
</span>
</th>
<th id="S3.T4.13.11.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T4.13.11.9.1" class="ltx_text" style="font-size:90%;">5.0</span></th>
<td id="S3.T4.13.11.10" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.10.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.13.11.10.1.1" class="ltx_p"><span id="S3.T4.13.11.10.1.1.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.13.11.10.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.10.1.2.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.13.11.10.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.10.1.3.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.13.11.10.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.10.1.4.1" class="ltx_text" style="font-size:90%;">AUC</span></span>
<span id="S3.T4.13.11.10.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.10.1.5.1" class="ltx_text" style="font-size:90%;">F1</span></span>
<span id="S3.T4.13.11.10.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.10.1.6.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S3.T4.13.11.11" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.11.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.13.11.11.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.1.1" class="ltx_text" style="font-size:90%;">67.98</span></span>
<span id="S3.T4.13.11.11.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.2.1" class="ltx_text" style="font-size:90%;">55.39</span></span>
<span id="S3.T4.13.11.11.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.3.1" class="ltx_text" style="font-size:90%;">28.96</span></span>
<span id="S3.T4.13.11.11.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.4.1" class="ltx_text" style="font-size:90%;">58.23</span></span>
<span id="S3.T4.13.11.11.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.5.1" class="ltx_text" style="font-size:90%;">43.67</span></span>
<span id="S3.T4.13.11.11.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.11.1.6.1" class="ltx_text" style="font-size:90%;">78.78</span></span>
</span>
</td>
<td id="S3.T4.8.6.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.8.6.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.8.6.1.1.2" class="ltx_p"><span id="S3.T4.8.6.1.1.2.1" class="ltx_text" style="font-size:90%;">67.98</span></span>
<span id="S3.T4.8.6.1.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.8.6.1.1.3.1" class="ltx_text" style="font-size:90%;">55.39</span></span>
<span id="S3.T4.8.6.1.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.8.6.1.1.4.1" class="ltx_text" style="font-size:90%;">28.62</span></span>
<span id="S3.T4.8.6.1.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.8.6.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">59.90<sup id="S3.T4.8.6.1.1.1.1.1" class="ltx_sup"><span id="S3.T4.8.6.1.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.8.6.1.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.8.6.1.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.8.6.1.1.5.1" class="ltx_text" style="font-size:90%;">43.37</span></span>
<span id="S3.T4.8.6.1.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.8.6.1.1.6.1" class="ltx_text" style="font-size:90%;">78.07</span></span>
</span>
</td>
<td id="S3.T4.9.7.2" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.9.7.2.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.9.7.2.1.2" class="ltx_p"><span id="S3.T4.9.7.2.1.2.1" class="ltx_text" style="font-size:90%;">67.98</span></span>
<span id="S3.T4.9.7.2.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.9.7.2.1.3.1" class="ltx_text" style="font-size:90%;">55.29</span></span>
<span id="S3.T4.9.7.2.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.9.7.2.1.4.1" class="ltx_text" style="font-size:90%;">27.67</span></span>
<span id="S3.T4.9.7.2.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.9.7.2.1.5.1" class="ltx_text" style="font-size:90%;">57.98</span></span>
<span id="S3.T4.9.7.2.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.9.7.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">44.30<sup id="S3.T4.9.7.2.1.1.1.1" class="ltx_sup"><span id="S3.T4.9.7.2.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.9.7.2.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.9.7.2.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.9.7.2.1.6.1" class="ltx_text" style="font-size:90%;">77.87</span></span>
</span>
</td>
<td id="S3.T4.13.11.12" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.12.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.13.11.12.1.1" class="ltx_p"><span id="S3.T4.13.11.12.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">74.38</span><span id="S3.T4.13.11.12.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.12.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.12.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">55.47</span><span id="S3.T4.13.11.12.1.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.12.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.12.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">35.01</span><span id="S3.T4.13.11.12.1.3.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.12.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.12.1.4.1" class="ltx_text" style="font-size:90%;">58.97</span></span>
<span id="S3.T4.13.11.12.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.12.1.5.1" class="ltx_text" style="font-size:90%;">43.44</span></span>
<span id="S3.T4.13.11.12.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.12.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">84.83</span><span id="S3.T4.13.11.12.1.6.2" class="ltx_text" style="font-size:90%;"></span></span>
</span>
</td>
<td id="S3.T4.13.11.13" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.13.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.13.11.13.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.1.1" class="ltx_text" style="font-size:90%;">75.13</span></span>
<span id="S3.T4.13.11.13.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.2.1" class="ltx_text" style="font-size:90%;">57.10</span></span>
<span id="S3.T4.13.11.13.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.3.1" class="ltx_text" style="font-size:90%;">33.90</span></span>
<span id="S3.T4.13.11.13.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.4.1" class="ltx_text" style="font-size:90%;">57.83</span></span>
<span id="S3.T4.13.11.13.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.5.1" class="ltx_text" style="font-size:90%;">39.11</span></span>
<span id="S3.T4.13.11.13.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.13.1.6.1" class="ltx_text" style="font-size:90%;">77.75</span></span>
</span>
</td>
<td id="S3.T4.10.8.3" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.10.8.3.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.10.8.3.1.2" class="ltx_p"><span id="S3.T4.10.8.3.1.2.1" class="ltx_text" style="font-size:90%;">74.51</span></span>
<span id="S3.T4.10.8.3.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.10.8.3.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">57.93<sup id="S3.T4.10.8.3.1.1.1.1" class="ltx_sup"><span id="S3.T4.10.8.3.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.10.8.3.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.10.8.3.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.10.8.3.1.3.1" class="ltx_text" style="font-size:90%;">34.46</span></span>
<span id="S3.T4.10.8.3.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.10.8.3.1.4.1" class="ltx_text" style="font-size:90%;">59.09</span></span>
<span id="S3.T4.10.8.3.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.10.8.3.1.5.1" class="ltx_text" style="font-size:90%;">39.36</span></span>
<span id="S3.T4.10.8.3.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.10.8.3.1.6.1" class="ltx_text" style="font-size:90%;">77.38</span></span>
</span>
</td>
<td id="S3.T4.13.11.14" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.14.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.13.11.14.1.1" class="ltx_p"><span id="S3.T4.13.11.14.1.1.1" class="ltx_text" style="font-size:90%;">75.27</span></span>
<span id="S3.T4.13.11.14.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.14.1.2.1" class="ltx_text" style="font-size:90%;">56.82</span></span>
<span id="S3.T4.13.11.14.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.14.1.3.1" class="ltx_text" style="font-size:90%;">33.74</span></span>
<span id="S3.T4.13.11.14.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.14.1.4.1" class="ltx_text" style="font-size:90%;">56.67</span></span>
<span id="S3.T4.13.11.14.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.14.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">41.01</span><span id="S3.T4.13.11.14.1.5.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.14.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.14.1.6.1" class="ltx_text" style="font-size:90%;">76.82</span></span>
</span>
</td>
<td id="S3.T4.13.11.6" class="ltx_td ltx_align_center ltx_border_t">
<span id="S3.T4.13.11.6.3" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.11.9.4.1.1" class="ltx_p"><span id="S3.T4.11.9.4.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">75.89<sup id="S3.T4.11.9.4.1.1.1.1" class="ltx_sup"><span id="S3.T4.11.9.4.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.11.9.4.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.6.3.4" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.6.3.4.1" class="ltx_text" style="font-size:90%;">57.25</span></span>
<span id="S3.T4.12.10.5.2.2" class="ltx_p ltx_align_center"><span id="S3.T4.12.10.5.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">35.62<sup id="S3.T4.12.10.5.2.2.1.1" class="ltx_sup"><span id="S3.T4.12.10.5.2.2.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.12.10.5.2.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.6.3.5" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.6.3.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">59.51</span><span id="S3.T4.13.11.6.3.5.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.13.11.6.3.6" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.6.3.6.1" class="ltx_text" style="font-size:90%;">38.74</span></span>
<span id="S3.T4.13.11.6.3.3" class="ltx_p ltx_align_center"><span id="S3.T4.13.11.6.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">85.17<sup id="S3.T4.13.11.6.3.3.1.1" class="ltx_sup"><span id="S3.T4.13.11.6.3.3.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></span>
</span>
</td>
</tr>
<tr id="S3.T4.19.17" class="ltx_tr">
<th id="S3.T4.19.17.7" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.7.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:49.8pt;">
<span id="S3.T4.19.17.7.1.1" class="ltx_p"><span id="S3.T4.19.17.7.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Synthetic
<br class="ltx_break">Partition</span></span>
</span>
</th>
<th id="S3.T4.19.17.8" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.8.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:56.9pt;">
<span id="S3.T4.19.17.8.1.1" class="ltx_p"><span id="S3.T4.19.17.8.1.1.1" class="ltx_text" style="font-size:90%;">UCF101</span></span>
<span id="S3.T4.19.17.8.1.2" class="ltx_p"><span id="S3.T4.19.17.8.1.2.1" class="ltx_text" style="font-size:90%;">MiT10</span></span>
<span id="S3.T4.19.17.8.1.3" class="ltx_p"><span id="S3.T4.19.17.8.1.3.1" class="ltx_text" style="font-size:90%;">MiT51</span></span>
<span id="S3.T4.19.17.8.1.4" class="ltx_p"><span id="S3.T4.19.17.8.1.4.1" class="ltx_text" style="font-size:90%;">Hateful-Memes</span></span>
<span id="S3.T4.19.17.8.1.5" class="ltx_p"><span id="S3.T4.19.17.8.1.5.1" class="ltx_text" style="font-size:90%;">CrisisMMD</span></span>
<span id="S3.T4.19.17.8.1.6" class="ltx_p"><span id="S3.T4.19.17.8.1.6.1" class="ltx_text" style="font-size:90%;">UCI-HAR</span></span>
</span>
</th>
<th id="S3.T4.19.17.9" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S3.T4.19.17.9.1" class="ltx_text" style="font-size:90%;">0.1</span></th>
<td id="S3.T4.19.17.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.10.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.10.1.1" class="ltx_p"><span id="S3.T4.19.17.10.1.1.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.19.17.10.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.10.1.2.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.19.17.10.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.10.1.3.1" class="ltx_text" style="font-size:90%;">Acc</span></span>
<span id="S3.T4.19.17.10.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.10.1.4.1" class="ltx_text" style="font-size:90%;">AUC</span></span>
<span id="S3.T4.19.17.10.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.10.1.5.1" class="ltx_text" style="font-size:90%;">F1</span></span>
<span id="S3.T4.19.17.10.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.10.1.6.1" class="ltx_text" style="font-size:90%;">F1</span></span>
</span>
</td>
<td id="S3.T4.19.17.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.11.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.11.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.1.1" class="ltx_text" style="font-size:90%;">64.57</span></span>
<span id="S3.T4.19.17.11.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.2.1" class="ltx_text" style="font-size:90%;">44.84</span></span>
<span id="S3.T4.19.17.11.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.3.1" class="ltx_text" style="font-size:90%;">28.63</span></span>
<span id="S3.T4.19.17.11.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.4.1" class="ltx_text" style="font-size:90%;">51.02</span></span>
<span id="S3.T4.19.17.11.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.5.1" class="ltx_text" style="font-size:90%;">9.90</span></span>
<span id="S3.T4.19.17.11.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.11.1.6.1" class="ltx_text" style="font-size:90%;">77.50</span></span>
</span>
</td>
<td id="S3.T4.19.17.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.12.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.12.1.1" class="ltx_p"><span id="S3.T4.19.17.12.1.1.1" class="ltx_text" style="font-size:90%;">64.55</span></span>
<span id="S3.T4.19.17.12.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.12.1.2.1" class="ltx_text" style="font-size:90%;">50.03</span></span>
<span id="S3.T4.19.17.12.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.12.1.3.1" class="ltx_text" style="font-size:90%;">27.98</span></span>
<span id="S3.T4.19.17.12.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.12.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">59.86</span><span id="S3.T4.19.17.12.1.4.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.12.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.12.1.5.1" class="ltx_text" style="font-size:90%;">10.65</span></span>
<span id="S3.T4.19.17.12.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.12.1.6.1" class="ltx_text" style="font-size:90%;">77.34</span></span>
</span>
</td>
<td id="S3.T4.19.17.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.13.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.13.1.1" class="ltx_p"><span id="S3.T4.19.17.13.1.1.1" class="ltx_text" style="font-size:90%;">61.17</span></span>
<span id="S3.T4.19.17.13.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.13.1.2.1" class="ltx_text" style="font-size:90%;">45.92</span></span>
<span id="S3.T4.19.17.13.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.13.1.3.1" class="ltx_text" style="font-size:90%;">27.92</span></span>
<span id="S3.T4.19.17.13.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.13.1.4.1" class="ltx_text" style="font-size:90%;">51.28</span></span>
<span id="S3.T4.19.17.13.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.13.1.5.1" class="ltx_text" style="font-size:90%;">9.28</span></span>
<span id="S3.T4.19.17.13.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.13.1.6.1" class="ltx_text" style="font-size:90%;">73.68</span></span>
</span>
</td>
<td id="S3.T4.19.17.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.14.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.14.1.1" class="ltx_p"><span id="S3.T4.19.17.14.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">74.17</span><span id="S3.T4.19.17.14.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.14.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.14.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">50.10</span><span id="S3.T4.19.17.14.1.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.14.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.14.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">33.46</span><span id="S3.T4.19.17.14.1.3.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.14.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.14.1.4.1" class="ltx_text" style="font-size:90%;">58.08</span></span>
<span id="S3.T4.19.17.14.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.14.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">26.82</span><span id="S3.T4.19.17.14.1.5.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.14.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.14.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#FF0000;">78.97</span></span>
</span>
</td>
<td id="S3.T4.19.17.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.15.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.15.1.1" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.1.1" class="ltx_text" style="font-size:90%;">74.53</span></span>
<span id="S3.T4.19.17.15.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.2.1" class="ltx_text" style="font-size:90%;">42.96</span></span>
<span id="S3.T4.19.17.15.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.3.1" class="ltx_text" style="font-size:90%;">32.41</span></span>
<span id="S3.T4.19.17.15.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.4.1" class="ltx_text" style="font-size:90%;">49.68</span></span>
<span id="S3.T4.19.17.15.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.5.1" class="ltx_text" style="font-size:90%;">8.49</span></span>
<span id="S3.T4.19.17.15.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.15.1.6.1" class="ltx_text" style="font-size:90%;">76.66</span></span>
</span>
</td>
<td id="S3.T4.19.17.16" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.16.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.16.1.1" class="ltx_p"><span id="S3.T4.19.17.16.1.1.1" class="ltx_text" style="font-size:90%;">74.71</span></span>
<span id="S3.T4.19.17.16.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.16.1.2.1" class="ltx_text" style="font-size:90%;">45.47</span></span>
<span id="S3.T4.19.17.16.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.16.1.3.1" class="ltx_text" style="font-size:90%;">32.55</span></span>
<span id="S3.T4.19.17.16.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.16.1.4.1" class="ltx_text" style="font-size:90%;">59.44</span></span>
<span id="S3.T4.19.17.16.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.16.1.5.1" class="ltx_text" style="font-size:90%;">25.31</span></span>
<span id="S3.T4.19.17.16.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.16.1.6.1" class="ltx_text" style="font-size:90%;">76.58</span></span>
</span>
</td>
<td id="S3.T4.19.17.17" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.17.1" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.19.17.17.1.1" class="ltx_p"><span id="S3.T4.19.17.17.1.1.1" class="ltx_text" style="font-size:90%;">73.27</span></span>
<span id="S3.T4.19.17.17.1.2" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.17.1.2.1" class="ltx_text" style="font-size:90%;">46.23</span></span>
<span id="S3.T4.19.17.17.1.3" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.17.1.3.1" class="ltx_text" style="font-size:90%;">31.99</span></span>
<span id="S3.T4.19.17.17.1.4" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.17.1.4.1" class="ltx_text" style="font-size:90%;">49.80</span></span>
<span id="S3.T4.19.17.17.1.5" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.17.1.5.1" class="ltx_text" style="font-size:90%;">10.12</span></span>
<span id="S3.T4.19.17.17.1.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.17.1.6.1" class="ltx_text" style="font-size:90%;">68.65</span></span>
</span>
</td>
<td id="S3.T4.19.17.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S3.T4.19.17.6.6" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:28.5pt;">
<span id="S3.T4.14.12.1.1.1" class="ltx_p"><span id="S3.T4.14.12.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">75.05<sup id="S3.T4.14.12.1.1.1.1.1" class="ltx_sup"><span id="S3.T4.14.12.1.1.1.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.14.12.1.1.1.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.15.13.2.2.2" class="ltx_p ltx_align_center"><span id="S3.T4.15.13.2.2.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">50.76<sup id="S3.T4.15.13.2.2.2.1.1" class="ltx_sup"><span id="S3.T4.15.13.2.2.2.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.15.13.2.2.2.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.16.14.3.3.3" class="ltx_p ltx_align_center"><span id="S3.T4.16.14.3.3.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">35.35<sup id="S3.T4.16.14.3.3.3.1.1" class="ltx_sup"><span id="S3.T4.16.14.3.3.3.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.16.14.3.3.3.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.17.15.4.4.4" class="ltx_p ltx_align_center"><span id="S3.T4.17.15.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">60.51<sup id="S3.T4.17.15.4.4.4.1.1" class="ltx_sup"><span id="S3.T4.17.15.4.4.4.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.17.15.4.4.4.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.18.16.5.5.5" class="ltx_p ltx_align_center"><span id="S3.T4.18.16.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">27.59<sup id="S3.T4.18.16.5.5.5.1.1" class="ltx_sup"><span id="S3.T4.18.16.5.5.5.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span><span id="S3.T4.18.16.5.5.5.2" class="ltx_text" style="font-size:90%;"></span></span>
<span id="S3.T4.19.17.6.6.6" class="ltx_p ltx_align_center"><span id="S3.T4.19.17.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;color:#0000FF;">79.80<sup id="S3.T4.19.17.6.6.6.1.1" class="ltx_sup"><span id="S3.T4.19.17.6.6.6.1.1.1" class="ltx_text ltx_font_medium">†</span></sup></span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Fusion Schemes</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In this work, we present two basic fusion approaches: <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">concatenation-based fusion</span> and <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">attention-based fusion</span>. In the concatenation-based fusion, the average pooling operation is first performed on the GRU output. After that, we concatenate the pooling embeddings to form the multimodal embedding. On the other hand, the attention-based fusion concatenates the temporal output from each modality without the average pooling step. We apply an attention mechanism similar to hierarchical attention <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib77" title="" class="ltx_ref">2016</a>)</cite>. Given the concatenated multimodal data <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">h</annotation></semantics></math>, the attention procedures are as follows:</p>
<table id="S7.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.1" class="ltx_math_unparsed" alttext="\displaystyle u=tanh(Wh+" display="inline"><semantics id="S3.Ex1.m1.1a"><mrow id="S3.Ex1.m1.1b"><mi id="S3.Ex1.m1.1.1">u</mi><mo id="S3.Ex1.m1.1.2">=</mo><mi id="S3.Ex1.m1.1.3">t</mi><mi id="S3.Ex1.m1.1.4">a</mi><mi id="S3.Ex1.m1.1.5">n</mi><mi id="S3.Ex1.m1.1.6">h</mi><mrow id="S3.Ex1.m1.1.7"><mo stretchy="false" id="S3.Ex1.m1.1.7.1">(</mo><mi id="S3.Ex1.m1.1.7.2">W</mi><mi id="S3.Ex1.m1.1.7.3">h</mi><mo id="S3.Ex1.m1.1.7.4">+</mo></mrow></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle u=tanh(Wh+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.1" class="ltx_math_unparsed" alttext="\displaystyle b);a=softmax(u^{T}c)" display="inline"><semantics id="S3.Ex1.m2.1a"><mrow id="S3.Ex1.m2.1b"><mi id="S3.Ex1.m2.1.1">b</mi><mo stretchy="false" id="S3.Ex1.m2.1.2">)</mo><mo id="S3.Ex1.m2.1.3">;</mo><mi id="S3.Ex1.m2.1.4">a</mi><mo id="S3.Ex1.m2.1.5">=</mo><mi id="S3.Ex1.m2.1.6">s</mi><mi id="S3.Ex1.m2.1.7">o</mi><mi id="S3.Ex1.m2.1.8">f</mi><mi id="S3.Ex1.m2.1.9">t</mi><mi id="S3.Ex1.m2.1.10">m</mi><mi id="S3.Ex1.m2.1.11">a</mi><mi id="S3.Ex1.m2.1.12">x</mi><mo stretchy="false" id="S3.Ex1.m2.1.13">(</mo><msup id="S3.Ex1.m2.1.14"><mi id="S3.Ex1.m2.1.14.2">u</mi><mi id="S3.Ex1.m2.1.14.3">T</mi></msup><mi id="S3.Ex1.m2.1.15">c</mi><mo stretchy="false" id="S3.Ex1.m2.1.16">)</mo></mrow><annotation encoding="application/x-tex" id="S3.Ex1.m2.1c">\displaystyle b);a=softmax(u^{T}c)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex2.m1.1" class="ltx_Math" alttext="\displaystyle v" display="inline"><semantics id="S3.Ex2.m1.1a"><mi id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.1b"><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.1c">\displaystyle v</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex2.m2.1" class="ltx_Math" alttext="\displaystyle=\sum_{i}a_{i}h_{i}" display="inline"><semantics id="S3.Ex2.m2.1a"><mrow id="S3.Ex2.m2.1.1" xref="S3.Ex2.m2.1.1.cmml"><mi id="S3.Ex2.m2.1.1.2" xref="S3.Ex2.m2.1.1.2.cmml"></mi><mo id="S3.Ex2.m2.1.1.1" xref="S3.Ex2.m2.1.1.1.cmml">=</mo><mrow id="S3.Ex2.m2.1.1.3" xref="S3.Ex2.m2.1.1.3.cmml"><mstyle displaystyle="true" id="S3.Ex2.m2.1.1.3.1" xref="S3.Ex2.m2.1.1.3.1.cmml"><munder id="S3.Ex2.m2.1.1.3.1a" xref="S3.Ex2.m2.1.1.3.1.cmml"><mo movablelimits="false" id="S3.Ex2.m2.1.1.3.1.2" xref="S3.Ex2.m2.1.1.3.1.2.cmml">∑</mo><mi id="S3.Ex2.m2.1.1.3.1.3" xref="S3.Ex2.m2.1.1.3.1.3.cmml">i</mi></munder></mstyle><mrow id="S3.Ex2.m2.1.1.3.2" xref="S3.Ex2.m2.1.1.3.2.cmml"><msub id="S3.Ex2.m2.1.1.3.2.2" xref="S3.Ex2.m2.1.1.3.2.2.cmml"><mi id="S3.Ex2.m2.1.1.3.2.2.2" xref="S3.Ex2.m2.1.1.3.2.2.2.cmml">a</mi><mi id="S3.Ex2.m2.1.1.3.2.2.3" xref="S3.Ex2.m2.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex2.m2.1.1.3.2.1" xref="S3.Ex2.m2.1.1.3.2.1.cmml">​</mo><msub id="S3.Ex2.m2.1.1.3.2.3" xref="S3.Ex2.m2.1.1.3.2.3.cmml"><mi id="S3.Ex2.m2.1.1.3.2.3.2" xref="S3.Ex2.m2.1.1.3.2.3.2.cmml">h</mi><mi id="S3.Ex2.m2.1.1.3.2.3.3" xref="S3.Ex2.m2.1.1.3.2.3.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m2.1b"><apply id="S3.Ex2.m2.1.1.cmml" xref="S3.Ex2.m2.1.1"><eq id="S3.Ex2.m2.1.1.1.cmml" xref="S3.Ex2.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.Ex2.m2.1.1.2.cmml" xref="S3.Ex2.m2.1.1.2">absent</csymbol><apply id="S3.Ex2.m2.1.1.3.cmml" xref="S3.Ex2.m2.1.1.3"><apply id="S3.Ex2.m2.1.1.3.1.cmml" xref="S3.Ex2.m2.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.3.1.1.cmml" xref="S3.Ex2.m2.1.1.3.1">subscript</csymbol><sum id="S3.Ex2.m2.1.1.3.1.2.cmml" xref="S3.Ex2.m2.1.1.3.1.2"></sum><ci id="S3.Ex2.m2.1.1.3.1.3.cmml" xref="S3.Ex2.m2.1.1.3.1.3">𝑖</ci></apply><apply id="S3.Ex2.m2.1.1.3.2.cmml" xref="S3.Ex2.m2.1.1.3.2"><times id="S3.Ex2.m2.1.1.3.2.1.cmml" xref="S3.Ex2.m2.1.1.3.2.1"></times><apply id="S3.Ex2.m2.1.1.3.2.2.cmml" xref="S3.Ex2.m2.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.3.2.2.1.cmml" xref="S3.Ex2.m2.1.1.3.2.2">subscript</csymbol><ci id="S3.Ex2.m2.1.1.3.2.2.2.cmml" xref="S3.Ex2.m2.1.1.3.2.2.2">𝑎</ci><ci id="S3.Ex2.m2.1.1.3.2.2.3.cmml" xref="S3.Ex2.m2.1.1.3.2.2.3">𝑖</ci></apply><apply id="S3.Ex2.m2.1.1.3.2.3.cmml" xref="S3.Ex2.m2.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m2.1.1.3.2.3.1.cmml" xref="S3.Ex2.m2.1.1.3.2.3">subscript</csymbol><ci id="S3.Ex2.m2.1.1.3.2.3.2.cmml" xref="S3.Ex2.m2.1.1.3.2.3.2">ℎ</ci><ci id="S3.Ex2.m2.1.1.3.2.3.3.cmml" xref="S3.Ex2.m2.1.1.3.2.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m2.1c">\displaystyle=\sum_{i}a_{i}h_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.7" class="ltx_p">The concatenated multimodal data <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">h</annotation></semantics></math> is first fed through a one-layer MLP to get representation <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">u</annotation></semantics></math>. We then use a context vector <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">c</annotation></semantics></math> to obtain a normalized importance score through a softmax function. After that, we compute the final multimodal embedding <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mi id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><ci id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">v</annotation></semantics></math> as a weighted sum of <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mi id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><ci id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">h</annotation></semantics></math> based on the weights <math id="S3.SS4.p2.6.m6.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS4.p2.6.m6.1a"><mi id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><ci id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">a</annotation></semantics></math>. Here, we can further implement a multi-head attention mechanism by having multiple <math id="S3.SS4.p2.7.m7.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S3.SS4.p2.7.m7.1a"><mi id="S3.SS4.p2.7.m7.1.1" xref="S3.SS4.p2.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m7.1b"><ci id="S3.SS4.p2.7.m7.1.1.cmml" xref="S3.SS4.p2.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m7.1c">c</annotation></semantics></math>. We would also stress that this attention mechanism is lightweight, thus making it realistic to deploy on a variety of edge devices. In addition, the attention mechanism allows us to mask the missing modalities in the computation, providing a simple yet effective solution to train FL models with missing modalities.</p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Federated Optimizers</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">First, most existing FL training algorithms are validated in unimodal settings, and their efficacy on multimodal tasks remains unexplored. As a result, FedMultimodal is suited to several popular FL algorithms, including FedAvg <cite class="ltx_cite ltx_citemacro_citep">(McMahan et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2017</a>)</cite>, FedProx <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a href="#bib.bib41" title="" class="ltx_ref">2020</a>)</cite>, FedRS <cite class="ltx_cite ltx_citemacro_citep">(Li and Zhan, <a href="#bib.bib42" title="" class="ltx_ref">2021</a>)</cite>, and FedOpt <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite>. Particularly, FedOpt holds state-of-the-art performance across multiple unimodal applications <cite class="ltx_cite ltx_citemacro_citep">(Reddi et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2020</a>)</cite>. One objective of FedMultimodal is to provide comprehensive evaluations across various FL algorithms.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6. </span>Real-world Noise Factor Emulator</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Prior literature (see Table <a href="#S1.T1" title="Table 1 ‣ 1. Introduction ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) on multimodal FL provides little or no assessment of their robustness in real-life settings. In order to provide a comprehensive evaluation of multimodal FL models toward safe and robust deployment, FedMultimodal enables the emulation of missing modalities, missing labels, and erroneous labels for real-world multimodal FL.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.1" class="ltx_p"><span id="S3.SS6.p2.1.1" class="ltx_text ltx_font_bold">Missing Modality.</span> In practice, data sources, whether they are microphones, cameras, mobile hardware, or medical electrodes, are prone to data imperfections or complete data losses (e.g., missing modalities) caused by firmware malfunctions, network disconnections, or sensor damages <cite class="ltx_cite ltx_citemacro_citep">(Feng and Narayanan, <a href="#bib.bib21" title="" class="ltx_ref">2019a</a>)</cite>. Hence, it is critical to design an emulator module to synthesize the cases of missing modalities for some clients. FedMultimodal provides the simulations of missing modalities as suggested by <cite class="ltx_cite ltx_citemacro_citep">(Chen and Zhang, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, where the availability of each modality follows a Bernoulli distribution. We set an equal missing rate <math id="S3.SS6.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S3.SS6.p2.1.m1.1a"><mi id="S3.SS6.p2.1.m1.1.1" xref="S3.SS6.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p2.1.m1.1b"><ci id="S3.SS6.p2.1.m1.1.1.cmml" xref="S3.SS6.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p2.1.m1.1c">q</annotation></semantics></math> for each modality in the following experiments.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para ltx_noindent">
<p id="S3.SS6.p3.1" class="ltx_p"><span id="S3.SS6.p3.1.1" class="ltx_text ltx_font_bold">Missing Labels.</span> Not only can the multimodal FL encounter data imperfection challenges, it can also suffer from missing label problems. Surprisingly, most prior works have made the ideal assumption that the data stored on edge devices are fully annotated with ground-truth labels. However, in a more realistic real-world FL setting, we argue that only a portion of the data can come with labels. As such, FedMultimodal allows the missing label simulation to assess the risk of decreased robustness.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para ltx_noindent">
<p id="S3.SS6.p4.4" class="ltx_p"><span id="S3.SS6.p4.4.1" class="ltx_text ltx_font_bold">Erroneous Labels.</span> In addition to missing labels, real-world FL implementations encounter label noise as a result of bias, skill differences, and labeling errors from the annotators. Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite>, we apply a label error generation process described in <cite class="ltx_cite ltx_citemacro_citep">(Northcutt et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite>. In summary, the erroneous labels are generated using a transition matrix <math id="S3.SS6.p4.1.m1.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.SS6.p4.1.m1.1a"><mi id="S3.SS6.p4.1.m1.1.1" xref="S3.SS6.p4.1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.1.m1.1b"><ci id="S3.SS6.p4.1.m1.1.1.cmml" xref="S3.SS6.p4.1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.1.m1.1c">Q</annotation></semantics></math>, where <math id="S3.SS6.p4.2.m2.3" class="ltx_Math" alttext="Q_{i,j}=\mathbb{P}(y=j|y=i)" display="inline"><semantics id="S3.SS6.p4.2.m2.3a"><mrow id="S3.SS6.p4.2.m2.3.3" xref="S3.SS6.p4.2.m2.3.3.cmml"><msub id="S3.SS6.p4.2.m2.3.3.3" xref="S3.SS6.p4.2.m2.3.3.3.cmml"><mi id="S3.SS6.p4.2.m2.3.3.3.2" xref="S3.SS6.p4.2.m2.3.3.3.2.cmml">Q</mi><mrow id="S3.SS6.p4.2.m2.2.2.2.4" xref="S3.SS6.p4.2.m2.2.2.2.3.cmml"><mi id="S3.SS6.p4.2.m2.1.1.1.1" xref="S3.SS6.p4.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS6.p4.2.m2.2.2.2.4.1" xref="S3.SS6.p4.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.SS6.p4.2.m2.2.2.2.2" xref="S3.SS6.p4.2.m2.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS6.p4.2.m2.3.3.2" xref="S3.SS6.p4.2.m2.3.3.2.cmml">=</mo><mrow id="S3.SS6.p4.2.m2.3.3.1" xref="S3.SS6.p4.2.m2.3.3.1.cmml"><mi id="S3.SS6.p4.2.m2.3.3.1.3" xref="S3.SS6.p4.2.m2.3.3.1.3.cmml">ℙ</mi><mo lspace="0em" rspace="0em" id="S3.SS6.p4.2.m2.3.3.1.2" xref="S3.SS6.p4.2.m2.3.3.1.2.cmml">​</mo><mrow id="S3.SS6.p4.2.m2.3.3.1.1.1" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS6.p4.2.m2.3.3.1.1.1.2" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.SS6.p4.2.m2.3.3.1.1.1.1" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.cmml"><mi id="S3.SS6.p4.2.m2.3.3.1.1.1.1.2" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.2.cmml">y</mi><mo id="S3.SS6.p4.2.m2.3.3.1.1.1.1.3" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.3.cmml">=</mo><mrow id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.cmml"><mi id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.2" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.2.cmml">j</mi><mo fence="false" id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.1" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.1.cmml">|</mo><mi id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.3" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.3.cmml">y</mi></mrow><mo id="S3.SS6.p4.2.m2.3.3.1.1.1.1.5" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.5.cmml">=</mo><mi id="S3.SS6.p4.2.m2.3.3.1.1.1.1.6" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.6.cmml">i</mi></mrow><mo stretchy="false" id="S3.SS6.p4.2.m2.3.3.1.1.1.3" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.2.m2.3b"><apply id="S3.SS6.p4.2.m2.3.3.cmml" xref="S3.SS6.p4.2.m2.3.3"><eq id="S3.SS6.p4.2.m2.3.3.2.cmml" xref="S3.SS6.p4.2.m2.3.3.2"></eq><apply id="S3.SS6.p4.2.m2.3.3.3.cmml" xref="S3.SS6.p4.2.m2.3.3.3"><csymbol cd="ambiguous" id="S3.SS6.p4.2.m2.3.3.3.1.cmml" xref="S3.SS6.p4.2.m2.3.3.3">subscript</csymbol><ci id="S3.SS6.p4.2.m2.3.3.3.2.cmml" xref="S3.SS6.p4.2.m2.3.3.3.2">𝑄</ci><list id="S3.SS6.p4.2.m2.2.2.2.3.cmml" xref="S3.SS6.p4.2.m2.2.2.2.4"><ci id="S3.SS6.p4.2.m2.1.1.1.1.cmml" xref="S3.SS6.p4.2.m2.1.1.1.1">𝑖</ci><ci id="S3.SS6.p4.2.m2.2.2.2.2.cmml" xref="S3.SS6.p4.2.m2.2.2.2.2">𝑗</ci></list></apply><apply id="S3.SS6.p4.2.m2.3.3.1.cmml" xref="S3.SS6.p4.2.m2.3.3.1"><times id="S3.SS6.p4.2.m2.3.3.1.2.cmml" xref="S3.SS6.p4.2.m2.3.3.1.2"></times><ci id="S3.SS6.p4.2.m2.3.3.1.3.cmml" xref="S3.SS6.p4.2.m2.3.3.1.3">ℙ</ci><apply id="S3.SS6.p4.2.m2.3.3.1.1.1.1.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1"><and id="S3.SS6.p4.2.m2.3.3.1.1.1.1a.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1"></and><apply id="S3.SS6.p4.2.m2.3.3.1.1.1.1b.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1"><eq id="S3.SS6.p4.2.m2.3.3.1.1.1.1.3.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.3"></eq><ci id="S3.SS6.p4.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.2">𝑦</ci><apply id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4"><csymbol cd="latexml" id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.1.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.1">conditional</csymbol><ci id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.2.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.2">𝑗</ci><ci id="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.3.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.4.3">𝑦</ci></apply></apply><apply id="S3.SS6.p4.2.m2.3.3.1.1.1.1c.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1"><eq id="S3.SS6.p4.2.m2.3.3.1.1.1.1.5.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.5"></eq><share href="#S3.SS6.p4.2.m2.3.3.1.1.1.1.4.cmml" id="S3.SS6.p4.2.m2.3.3.1.1.1.1d.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1"></share><ci id="S3.SS6.p4.2.m2.3.3.1.1.1.1.6.cmml" xref="S3.SS6.p4.2.m2.3.3.1.1.1.1.6">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.2.m2.3c">Q_{i,j}=\mathbb{P}(y=j|y=i)</annotation></semantics></math> indicates the chance of ground-truth label <math id="S3.SS6.p4.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.SS6.p4.3.m3.1a"><mi id="S3.SS6.p4.3.m3.1.1" xref="S3.SS6.p4.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.3.m3.1b"><ci id="S3.SS6.p4.3.m3.1.1.cmml" xref="S3.SS6.p4.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.3.m3.1c">i</annotation></semantics></math> being erroneous annotated with label <math id="S3.SS6.p4.4.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S3.SS6.p4.4.m4.1a"><mi id="S3.SS6.p4.4.m4.1.1" xref="S3.SS6.p4.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p4.4.m4.1b"><ci id="S3.SS6.p4.4.m4.1.1.cmml" xref="S3.SS6.p4.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p4.4.m4.1c">j</annotation></semantics></math>.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2306.09486/assets/x2.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="428" height="129" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Performance comparisons between multimodal and unimodal learning under FL settings.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments and Discussion</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Experimental Details</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">Setup.</span>
We adopt the RNN-only model architecture to the video and text modalities while utilizing the Conv-RNN model architecture in other modalities. Specifically, the model with the convolutional module consists of 3 convolution layers with the number of filters in {16, 32, 64} and the filter kernel size of <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="5\times 5" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mrow id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mn id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.1.m1.1.1.1" xref="S4.SS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><times id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">5</cn><cn type="integer" id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">5\times 5</annotation></semantics></math>. Moreover, we set the hidden layer size of RNN as 128. We choose ReLU as the activation function and the dropout rate as 0.2. The number of attention heads is 6 in all experiments. We fixed the batch size for all datasets to 16 and the local epoch to 1 for all experiments.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.4" class="ltx_p">Additionally, we set the training epochs as 200 for all datasets except the MiT sub-datasets. However, the total training epoch is 300 in MiT10 and MiT51 as these 2 datasets contain more data than the other datasets. Hyperparameter details such as the learning and client sampling rates for each dataset are listed in Table <a href="#S3.T3" title="Table 3 ‣ 3.3. Multimodal Models ‣ 3. End-to-end Multimodal Federated Learning Framework ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Due to the limited number of clients in the Hateful Memes dataset and PTB-xl datasets, we apply a higher client sample rate in these 2 datasets. In the FedOpt algorithm, we search the server learning rate in a range from <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="10^{-3}" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><msup id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mn id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml"><mo id="S4.SS1.p2.1.m1.1.1.3a" xref="S4.SS1.p2.1.m1.1.1.3.cmml">−</mo><mn id="S4.SS1.p2.1.m1.1.1.3.2" xref="S4.SS1.p2.1.m1.1.1.3.2.cmml">3</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">10</cn><apply id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"><minus id="S4.SS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS1.p2.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS1.p2.1.m1.1.1.3.2">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">10^{-3}</annotation></semantics></math> to <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="2.5\times 10^{-3}" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml"><mn id="S4.SS1.p2.2.m2.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.cmml">2.5</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.cmml">×</mo><msup id="S4.SS1.p2.2.m2.1.1.3" xref="S4.SS1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS1.p2.2.m2.1.1.3.2" xref="S4.SS1.p2.2.m2.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.p2.2.m2.1.1.3.3" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml"><mo id="S4.SS1.p2.2.m2.1.1.3.3a" xref="S4.SS1.p2.2.m2.1.1.3.3.cmml">−</mo><mn id="S4.SS1.p2.2.m2.1.1.3.3.2" xref="S4.SS1.p2.2.m2.1.1.3.3.2.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1"></times><cn type="float" id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.2">2.5</cn><apply id="S4.SS1.p2.2.m2.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3">superscript</csymbol><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.2">10</cn><apply id="S4.SS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"><minus id="S4.SS1.p2.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3"></minus><cn type="integer" id="S4.SS1.p2.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p2.2.m2.1.1.3.3.2">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">2.5\times 10^{-3}</annotation></semantics></math>. Meanwhile, the proximal term ranges from <math id="S4.SS1.p2.3.m3.1" class="ltx_Math" alttext="10^{-2}" display="inline"><semantics id="S4.SS1.p2.3.m3.1a"><msup id="S4.SS1.p2.3.m3.1.1" xref="S4.SS1.p2.3.m3.1.1.cmml"><mn id="S4.SS1.p2.3.m3.1.1.2" xref="S4.SS1.p2.3.m3.1.1.2.cmml">10</mn><mrow id="S4.SS1.p2.3.m3.1.1.3" xref="S4.SS1.p2.3.m3.1.1.3.cmml"><mo id="S4.SS1.p2.3.m3.1.1.3a" xref="S4.SS1.p2.3.m3.1.1.3.cmml">−</mo><mn id="S4.SS1.p2.3.m3.1.1.3.2" xref="S4.SS1.p2.3.m3.1.1.3.2.cmml">2</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.3.m3.1b"><apply id="S4.SS1.p2.3.m3.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.3.m3.1.1.1.cmml" xref="S4.SS1.p2.3.m3.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p2.3.m3.1.1.2.cmml" xref="S4.SS1.p2.3.m3.1.1.2">10</cn><apply id="S4.SS1.p2.3.m3.1.1.3.cmml" xref="S4.SS1.p2.3.m3.1.1.3"><minus id="S4.SS1.p2.3.m3.1.1.3.1.cmml" xref="S4.SS1.p2.3.m3.1.1.3"></minus><cn type="integer" id="S4.SS1.p2.3.m3.1.1.3.2.cmml" xref="S4.SS1.p2.3.m3.1.1.3.2">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.3.m3.1c">10^{-2}</annotation></semantics></math> to <math id="S4.SS1.p2.4.m4.1" class="ltx_Math" alttext="10^{0}" display="inline"><semantics id="S4.SS1.p2.4.m4.1a"><msup id="S4.SS1.p2.4.m4.1.1" xref="S4.SS1.p2.4.m4.1.1.cmml"><mn id="S4.SS1.p2.4.m4.1.1.2" xref="S4.SS1.p2.4.m4.1.1.2.cmml">10</mn><mn id="S4.SS1.p2.4.m4.1.1.3" xref="S4.SS1.p2.4.m4.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.4.m4.1b"><apply id="S4.SS1.p2.4.m4.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.4.m4.1.1.1.cmml" xref="S4.SS1.p2.4.m4.1.1">superscript</csymbol><cn type="integer" id="S4.SS1.p2.4.m4.1.1.2.cmml" xref="S4.SS1.p2.4.m4.1.1.2">10</cn><cn type="integer" id="S4.SS1.p2.4.m4.1.1.3.cmml" xref="S4.SS1.p2.4.m4.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.4.m4.1c">10^{0}</annotation></semantics></math> in the FedProx algorithm.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2306.09486/assets/missing_modality.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="568" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Relative performance changes under different missing modality rates.</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Evaluation Metrics.</span>
We follow established practices from the literature while conducting evaluations on each dataset. Specifically, evaluation metrics (e.g., F1) and validation protocols (e.g., predefined splits) are two fundamental components to ensure comparability with past (and future) works. With datasets that provide a pre-defined partition for training/validation/testing, we repeat the experiments 5 times using different seeds. We perform 5-fold cross-validation on datasets without such pre-defined experimenting rules. We provide details about our evaluation methods in Table <a href="#S1.T2" title="Table 2 ‣ 1. Introduction ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2306.09486/assets/missing_labels.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="550" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Relative performance changes under different label missing rates.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2306.09486/assets/x3.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Relative performance changes under different erroneous label rates.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Overall Performance</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">We first report the comparisons between two fusion mechanisms (<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">attention-based fusion</span> and <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_bold">concatenation-based fusion</span>) in Table <a href="#S3.T4" title="Table 4 ‣ 3.3. Multimodal Models ‣ 3. End-to-end Multimodal Federated Learning Framework ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. From the results, we can find that the attention-based fusion mechanism outperforms concatenation-based fusion in the majority of the datasets. Specifically, the attention-based fusion mechanism leads to better performances in most high data heterogeneity conditions, but it underperforms the concatenation-based fusion in two synthetic datasets with <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="\alpha=5.0" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mrow id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">α</mi><mo id="S4.SS2.p1.1.m1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">5.0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><eq id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1"></eq><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝛼</ci><cn type="float" id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">5.0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">\alpha=5.0</annotation></semantics></math> (low data heterogeneity). Moreover, we observe that the FedOpt algorithm consistently yields better baselines compared to other FL algorithms with a few exceptions in low data heterogeneity conditions. However, we would like to highlight that, in practice, FedOpt requires additional hyperparameter tuning on the server learning rate to reach the best performance. Overall, these results imply that the fusion mechanism is a critical factor impacting multimodal model performance in data heterogeneous FL.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Moreover, HAR tasks are associated with the highest performance scores, suggesting the simplicity of this learning task. In contrast, classification results on the Hateful Memes dataset and CrisisMMD dataset imply that social media classification is a challenging task using FL, with the best model performance on the CrisisMMD dataset below 30%. A plausible explanation is that the pre-trained models that we rely on are not generalized to social media data, generating image and textual features that are unrepresentative of downstream learning models. On the other hand, performances on the MiT51 dataset demonstrate similar findings pointed out from <cite class="ltx_cite ltx_citemacro_citep">(Monfort et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2019</a>)</cite>, validating that MiT is a challenging dataset. However, reducing the number of labels indeed simplifies the predicting task, resulting in moderate model performance on MiT10.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Uni-modality vs. Multi-modalities</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">One fundamental research question centering around multimodal learning is its performance compared to unimodal models. For example, the previous multimodal benchmark <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>, with an emphasis on centralized learning setup, demonstrates that unimodal learning could yield similar performance to multimodal models with fewer parameters. Similar to MultiBench, FedMultimodal provides the unimodal FL to compare with multimodal FL baselines. We summarize the benchmark comparisons between unimodal FL and multimodal FL in Figure <a href="#S3.F3" title="Figure 3 ‣ 3.6. Real-world Noise Factor Emulator ‣ 3. End-to-end Multimodal Federated Learning Framework ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The comparisons use datasets with natural partitions or high data heterogeneity partitions. Overall, we observe that unimodal learning provides competitive performance compared with the multimodal FL benchmarks, complying with centralized benchmark results reported in <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2021</a>)</cite>. Nevertheless, in most scenarios, multimodal learning still outperforms unimodal learning, whereas the performance gap between multimodal and unimodal FL is within 5% in the majority of the datasets.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Impact of Missing Modalities</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.3" class="ltx_p">As described in earlier sections, a unique challenge associated with multimodal learning is dealing with scenarios of missing modalities <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a href="#bib.bib78" title="" class="ltx_ref">2023</a>; Chen and Zhang, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>. In this section, we benchmark our selected datasets with different rates of missing modalities. In this experiment, we assume that the availability of each modality follows a Bernoulli distribution with a missing rate of <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mi id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><ci id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">q</annotation></semantics></math>. Following the experiment protocol presented by <cite class="ltx_cite ltx_citemacro_citep">(Chen and Zhang, <a href="#bib.bib11" title="" class="ltx_ref">2022</a>)</cite>, we set a uniform missing rate of <math id="S4.SS4.p1.2.m2.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S4.SS4.p1.2.m2.1a"><mi id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><ci id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">q</annotation></semantics></math> for each modality, where <math id="S4.SS4.p1.3.m3.5" class="ltx_Math" alttext="q\in\{0.1,0.2,0.3,0.4,0.5\}" display="inline"><semantics id="S4.SS4.p1.3.m3.5a"><mrow id="S4.SS4.p1.3.m3.5.6" xref="S4.SS4.p1.3.m3.5.6.cmml"><mi id="S4.SS4.p1.3.m3.5.6.2" xref="S4.SS4.p1.3.m3.5.6.2.cmml">q</mi><mo id="S4.SS4.p1.3.m3.5.6.1" xref="S4.SS4.p1.3.m3.5.6.1.cmml">∈</mo><mrow id="S4.SS4.p1.3.m3.5.6.3.2" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml"><mo stretchy="false" id="S4.SS4.p1.3.m3.5.6.3.2.1" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">{</mo><mn id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">0.1</mn><mo id="S4.SS4.p1.3.m3.5.6.3.2.2" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS4.p1.3.m3.2.2" xref="S4.SS4.p1.3.m3.2.2.cmml">0.2</mn><mo id="S4.SS4.p1.3.m3.5.6.3.2.3" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS4.p1.3.m3.3.3" xref="S4.SS4.p1.3.m3.3.3.cmml">0.3</mn><mo id="S4.SS4.p1.3.m3.5.6.3.2.4" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS4.p1.3.m3.4.4" xref="S4.SS4.p1.3.m3.4.4.cmml">0.4</mn><mo id="S4.SS4.p1.3.m3.5.6.3.2.5" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">,</mo><mn id="S4.SS4.p1.3.m3.5.5" xref="S4.SS4.p1.3.m3.5.5.cmml">0.5</mn><mo stretchy="false" id="S4.SS4.p1.3.m3.5.6.3.2.6" xref="S4.SS4.p1.3.m3.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.5b"><apply id="S4.SS4.p1.3.m3.5.6.cmml" xref="S4.SS4.p1.3.m3.5.6"><in id="S4.SS4.p1.3.m3.5.6.1.cmml" xref="S4.SS4.p1.3.m3.5.6.1"></in><ci id="S4.SS4.p1.3.m3.5.6.2.cmml" xref="S4.SS4.p1.3.m3.5.6.2">𝑞</ci><set id="S4.SS4.p1.3.m3.5.6.3.1.cmml" xref="S4.SS4.p1.3.m3.5.6.3.2"><cn type="float" id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">0.1</cn><cn type="float" id="S4.SS4.p1.3.m3.2.2.cmml" xref="S4.SS4.p1.3.m3.2.2">0.2</cn><cn type="float" id="S4.SS4.p1.3.m3.3.3.cmml" xref="S4.SS4.p1.3.m3.3.3">0.3</cn><cn type="float" id="S4.SS4.p1.3.m3.4.4.cmml" xref="S4.SS4.p1.3.m3.4.4">0.4</cn><cn type="float" id="S4.SS4.p1.3.m3.5.5.cmml" xref="S4.SS4.p1.3.m3.5.5">0.5</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.5c">q\in\{0.1,0.2,0.3,0.4,0.5\}</annotation></semantics></math>. As described in the multimodal model section, attention-based fusion allows us to train the model even with missing data through masking. To train the model with the missing entries, we fill the missing data with 0 <cite class="ltx_cite ltx_citemacro_citep">(Parthasarathy and Sundaram, <a href="#bib.bib54" title="" class="ltx_ref">2020</a>)</cite> while masking out the corresponding data points in calculating attention scores.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2306.09486/assets/x4.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="424" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Relative performance changes with 30% data corrupted (missing modalities vs. missing labels vs. erroneous labels).</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">We present the relative model performance changes at different missing modality rates in Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1. Experimental Details ‣ 4. Experiments and Discussion ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. From the graph, we find that the relative performance changes with missing rates below 30% are not substantial, suggesting that a small amount of missing modalities in deployment does not impact the final model performance in multimodal FL. Furthermore, we observe that the model performance starts to decline substantially at the missing rate of 50%. Surprisingly, we observe that half of the models have relative performance decreases that are under 10%, suggesting that the provided baseline models that use attention-based fusion still learn useful information in these cases. However, we find that the missing modality introduces a significantly larger impact on CrisisMMD data and the HAR applications compared to the other multimodal applications we evaluated. This suggests that future FL HAR research should carefully consider missing modalities as a part of the evaluation.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Impact of Missing Labels</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.2" class="ltx_p">Missing labels is a widely presented challenge in FL. In this section, we perform evaluations of missing label conditions using the FedMultimodal benchmark. Similar to the missing modality experiment, we assume that the availability of each label follows a Bernoulli distribution with a missing rate <math id="S4.SS5.p1.1.m1.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS5.p1.1.m1.1a"><mi id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><ci id="S4.SS5.p1.1.m1.1.1.cmml" xref="S4.SS5.p1.1.m1.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">l</annotation></semantics></math>. We apply the FedMultimodal benchmark to emulate the missing label rate <math id="S4.SS5.p1.2.m2.5" class="ltx_Math" alttext="l\in\{0.1,0.2,0.3,0.4,0.5\}" display="inline"><semantics id="S4.SS5.p1.2.m2.5a"><mrow id="S4.SS5.p1.2.m2.5.6" xref="S4.SS5.p1.2.m2.5.6.cmml"><mi id="S4.SS5.p1.2.m2.5.6.2" xref="S4.SS5.p1.2.m2.5.6.2.cmml">l</mi><mo id="S4.SS5.p1.2.m2.5.6.1" xref="S4.SS5.p1.2.m2.5.6.1.cmml">∈</mo><mrow id="S4.SS5.p1.2.m2.5.6.3.2" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml"><mo stretchy="false" id="S4.SS5.p1.2.m2.5.6.3.2.1" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">{</mo><mn id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">0.1</mn><mo id="S4.SS5.p1.2.m2.5.6.3.2.2" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.SS5.p1.2.m2.2.2" xref="S4.SS5.p1.2.m2.2.2.cmml">0.2</mn><mo id="S4.SS5.p1.2.m2.5.6.3.2.3" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.SS5.p1.2.m2.3.3" xref="S4.SS5.p1.2.m2.3.3.cmml">0.3</mn><mo id="S4.SS5.p1.2.m2.5.6.3.2.4" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.SS5.p1.2.m2.4.4" xref="S4.SS5.p1.2.m2.4.4.cmml">0.4</mn><mo id="S4.SS5.p1.2.m2.5.6.3.2.5" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">,</mo><mn id="S4.SS5.p1.2.m2.5.5" xref="S4.SS5.p1.2.m2.5.5.cmml">0.5</mn><mo stretchy="false" id="S4.SS5.p1.2.m2.5.6.3.2.6" xref="S4.SS5.p1.2.m2.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.5b"><apply id="S4.SS5.p1.2.m2.5.6.cmml" xref="S4.SS5.p1.2.m2.5.6"><in id="S4.SS5.p1.2.m2.5.6.1.cmml" xref="S4.SS5.p1.2.m2.5.6.1"></in><ci id="S4.SS5.p1.2.m2.5.6.2.cmml" xref="S4.SS5.p1.2.m2.5.6.2">𝑙</ci><set id="S4.SS5.p1.2.m2.5.6.3.1.cmml" xref="S4.SS5.p1.2.m2.5.6.3.2"><cn type="float" id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">0.1</cn><cn type="float" id="S4.SS5.p1.2.m2.2.2.cmml" xref="S4.SS5.p1.2.m2.2.2">0.2</cn><cn type="float" id="S4.SS5.p1.2.m2.3.3.cmml" xref="S4.SS5.p1.2.m2.3.3">0.3</cn><cn type="float" id="S4.SS5.p1.2.m2.4.4.cmml" xref="S4.SS5.p1.2.m2.4.4">0.4</cn><cn type="float" id="S4.SS5.p1.2.m2.5.5.cmml" xref="S4.SS5.p1.2.m2.5.5">0.5</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.5c">l\in\{0.1,0.2,0.3,0.4,0.5\}</annotation></semantics></math>. The goal of this experiment is to quantify the impact of missing labels on the overall performance of benchmarks, hence we do not integrate any mitigation methodologies, such as semi-supervised learning or self-supervised learning in our experiments.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Results on relative model performance changes at different label missing ratios using the FedMultimodal framework are presented in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.1. Experimental Details ‣ 4. Experiments and Discussion ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Overall, we can observe that missing labels have a reduced impact on the model performance when compared to the missing modality scenario. For instance, the model performance suffers less than 10% relative performance decreases in the majority of the datasets with the exception of KU-HAR datasets. When the missing label ratio is below 50%, we observe minor performance decreases in all datasets. Surprisingly, we identify that CrisisMMD yields worse performance at 30% than at 10% missing label ratio. We conjecture that the reason behind this result might be attributed to the data labeling quality of the Hateful-Memes dataset. For example, determining whether the content is hateful or not can be very subjective, and such subjectiveness could hurt labeling quality.</p>
</div>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6. </span>Impact of Erroneous Labels</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.5" class="ltx_p">Besides missing modalities and missing labels, erroneous labels frequently exist in FL <cite class="ltx_cite ltx_citemacro_citep">(Yaldiz et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2023</a>)</cite>. In this section, we report our benchmark performance with erroneous labels. Similar to previous experiments, we search the erroneous label ratio <math id="S4.SS6.p1.1.m1.5" class="ltx_Math" alttext="l\in\{0.1,0.2,0.3,0.4,0.5\}" display="inline"><semantics id="S4.SS6.p1.1.m1.5a"><mrow id="S4.SS6.p1.1.m1.5.6" xref="S4.SS6.p1.1.m1.5.6.cmml"><mi id="S4.SS6.p1.1.m1.5.6.2" xref="S4.SS6.p1.1.m1.5.6.2.cmml">l</mi><mo id="S4.SS6.p1.1.m1.5.6.1" xref="S4.SS6.p1.1.m1.5.6.1.cmml">∈</mo><mrow id="S4.SS6.p1.1.m1.5.6.3.2" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml"><mo stretchy="false" id="S4.SS6.p1.1.m1.5.6.3.2.1" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">{</mo><mn id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">0.1</mn><mo id="S4.SS6.p1.1.m1.5.6.3.2.2" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.2.2" xref="S4.SS6.p1.1.m1.2.2.cmml">0.2</mn><mo id="S4.SS6.p1.1.m1.5.6.3.2.3" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.3.3" xref="S4.SS6.p1.1.m1.3.3.cmml">0.3</mn><mo id="S4.SS6.p1.1.m1.5.6.3.2.4" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.4.4" xref="S4.SS6.p1.1.m1.4.4.cmml">0.4</mn><mo id="S4.SS6.p1.1.m1.5.6.3.2.5" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">,</mo><mn id="S4.SS6.p1.1.m1.5.5" xref="S4.SS6.p1.1.m1.5.5.cmml">0.5</mn><mo stretchy="false" id="S4.SS6.p1.1.m1.5.6.3.2.6" xref="S4.SS6.p1.1.m1.5.6.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.5b"><apply id="S4.SS6.p1.1.m1.5.6.cmml" xref="S4.SS6.p1.1.m1.5.6"><in id="S4.SS6.p1.1.m1.5.6.1.cmml" xref="S4.SS6.p1.1.m1.5.6.1"></in><ci id="S4.SS6.p1.1.m1.5.6.2.cmml" xref="S4.SS6.p1.1.m1.5.6.2">𝑙</ci><set id="S4.SS6.p1.1.m1.5.6.3.1.cmml" xref="S4.SS6.p1.1.m1.5.6.3.2"><cn type="float" id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1">0.1</cn><cn type="float" id="S4.SS6.p1.1.m1.2.2.cmml" xref="S4.SS6.p1.1.m1.2.2">0.2</cn><cn type="float" id="S4.SS6.p1.1.m1.3.3.cmml" xref="S4.SS6.p1.1.m1.3.3">0.3</cn><cn type="float" id="S4.SS6.p1.1.m1.4.4.cmml" xref="S4.SS6.p1.1.m1.4.4">0.4</cn><cn type="float" id="S4.SS6.p1.1.m1.5.5.cmml" xref="S4.SS6.p1.1.m1.5.5">0.5</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.5c">l\in\{0.1,0.2,0.3,0.4,0.5\}</annotation></semantics></math>, where <math id="S4.SS6.p1.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.SS6.p1.2.m2.1a"><mi id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><ci id="S4.SS6.p1.2.m2.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">l</annotation></semantics></math> represents the amount of data with erroneous labels. Similar to the experiment setup in <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite>, our benchmark defines the sparsity of erroneous label transition matrix <math id="S4.SS6.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S4.SS6.p1.3.m3.1a"><mi id="S4.SS6.p1.3.m3.1.1" xref="S4.SS6.p1.3.m3.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.3.m3.1b"><ci id="S4.SS6.p1.3.m3.1.1.cmml" xref="S4.SS6.p1.3.m3.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.3.m3.1c">\mathbf{Q}</annotation></semantics></math> as 0.4. The error sparsity specifies the possible number of unique labels <math id="S4.SS6.p1.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS6.p1.4.m4.1a"><mi id="S4.SS6.p1.4.m4.1.1" xref="S4.SS6.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.4.m4.1b"><ci id="S4.SS6.p1.4.m4.1.1.cmml" xref="S4.SS6.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.4.m4.1c">k</annotation></semantics></math> that one label can be wrongly annotated with, with a small sparsity error rate corresponding to a larger <math id="S4.SS6.p1.5.m5.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS6.p1.5.m5.1a"><mi id="S4.SS6.p1.5.m5.1.1" xref="S4.SS6.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.5.m5.1b"><ci id="S4.SS6.p1.5.m5.1.1.cmml" xref="S4.SS6.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.5.m5.1c">k</annotation></semantics></math>.</p>
</div>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">The complete results of the relative model performance changes at different levels of erroneous label ratios are shown in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.1. Experimental Details ‣ 4. Experiments and Discussion ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Compared to the missing modalities experiment, the erroneous label condition leads to substantially larger performance decreases. For example, more than half of the datasets have the relative performance decreases above 10% at the erroneous label ratio of 30%. Moreover, a 20% performance drop can be identified from these datasets when the erroneous label ratio reaches 50%. To compare the impact of data corruption conditions in FL, we plot the relative performance changes with different data corruption conditions at the data corrupted ratio of 30% in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4. Impact of Missing Modalities ‣ 4. Experiments and Discussion ‣ FedMultimodal: A Benchmark For Multimodal Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We can observe that performances of multimodal FL are more susceptible to label noises than missing modalities or missing labels. Based on these observations, our future benchmark directions also include implementations of backdoor attacks and mitigation in FedMultimodal.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Limitations and Future Work</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p"><span id="S5.p1.1.1" class="ltx_text ltx_font_bold">Scale of Datasets and Models.</span> The dataset selection criteria of FedMultimodal ensures that the chosen datasets are representative and diversified across different dimensions such as application scenarios, data size, and number of clients. In addition, FedMultimodal only includes ML models that align with the use cases of FL, taking into account the computational limitations of edge devices.
We acknowledge that FedMultimodal currently does not cover several promising multimodal applications, such as medical image analysis, autonomous driving, and virtual reality, and the range of the supported models is limited. We will continuously update FedMultimodal to support new tasks such as Ego4D <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2022</a>)</cite>, as well as newer feature extraction models.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Scale of Modality Fusion Schemes.</span> Currently, FedMultimodal includes two basic approaches for modality fusion: concatenation and attention. Modality fusion under FL remains an open problem, and our objective is to draw attention to the need for developing more advanced modality fusion schemes under FL <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2016</a>; Jaegle et al<span class="ltx_text">.</span>, <a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Data Heterogeneity.</span> As discussed in previous sections, addressing the data heterogeneity challenge is critical in FL. While many FL studies focus their experiments on the unimodal setup, there is a lack of extensive research on tackling data heterogeneity in multimodal FL. To address this gap, the FedMulitmodal benchmark provides opportunities to facilitate fundamental research in this direction. In the future, it is of further interest to explore knowledge-transfer learning approaches as suggested in <cite class="ltx_cite ltx_citemacro_citep">(Cho et al<span class="ltx_text">.</span>, <a href="#bib.bib13" title="" class="ltx_ref">2022</a>; Lin et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2020</a>; Itahara et al<span class="ltx_text">.</span>, <a href="#bib.bib32" title="" class="ltx_ref">2021</a>)</cite>, within the context of multimodal FL.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><span id="S5.p5.1.1" class="ltx_text ltx_font_bold">Label Scarcity.</span> One major challenge for FL is the lack of qualitative labels. FedMultimodal enables researchers to efficiently perform experiments on multimodal FL with missing labels by providing the ability to emulate experimental conditions with missing labels. We hope FedMultimodal brings unique benefits for ML practitioners to develop self-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al<span class="ltx_text">.</span>, <a href="#bib.bib85" title="" class="ltx_ref">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">2020</a>; Dong and Voiculescu, <a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite> and semi-supervised learning <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2021b</a>; Kang et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2022</a>; Zhao et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2020</a>)</cite> algorithms under multimodal FL.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><span id="S5.p6.1.1" class="ltx_text ltx_font_bold">Privacy Leakage.</span> While sharing model updates is considered to be more private than sharing raw data, recent works have revealed that FL can still be susceptible to privacy attacks. These attacks include (but are not limited to) membership inference attacks <cite class="ltx_cite ltx_citemacro_citep">(Melis et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2019</a>)</cite>, reconstruction attacks <cite class="ltx_cite ltx_citemacro_citep">(Zhu and Han, <a href="#bib.bib84" title="" class="ltx_ref">2020</a>; Geng et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>, attribute inference attacks <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021b</a>)</cite> and label inference attacks <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>. Consequently, an emerging research direction for expanding FedMultimodal is to explore the privacy leakages in multimodal FL. Apart from identifying privacy risks associated with multimodal FL, it is also crucial to investigate privacy-enhancing techniques, such as differential privacy <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib73" title="" class="ltx_ref">2020</a>; Dwork, <a href="#bib.bib17" title="" class="ltx_ref">2006</a>; Feng et al<span class="ltx_text">.</span>, <a href="#bib.bib24" title="" class="ltx_ref">2022</a>)</cite> and secure aggregation <cite class="ltx_cite ltx_citemacro_citep">(Bonawitz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite> as promising areas of research within the scope of FedMultimodal to mitigate privacy attacks.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this paper, we presented a new framework for multimodal federated learning, named FedMultimodal, which enables federated learning in multimodal applications. We further established a reproducible benchmark of results for 5 multimodal FL applications covering 10 datasets for future comparisons. We also benchmarked results on model robustness to missing modalities, missing labels, and noisy labels in each of these tasks.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgement</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work is in part supported by USC Amazon Center, as well as research gift awards from Intel, Meta, and Konica Minolta.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alam et al<span id="bib.bib2.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Firoj Alam, Ferda Ofli,
and Muhammad Imran. 2018.

</span>
<span class="ltx_bibblock">Crisismmd: Multimodal twitter datasets from natural
disasters. In <em id="bib.bib2.3.1" class="ltx_emph ltx_font_italic">Twelfth international AAAI
conference on web and social media</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alday et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Erick A Perez Alday, Annie
Gu, Amit J Shah, Chad Robichaux,
An-Kwok Ian Wong, Chengyu Liu,
Feifei Liu, Ali Bahrami Rad,
Andoni Elola, Salman Seyedi,
et al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Classification of 12-lead ecgs: the
physionet/computing in cardiology challenge 2020.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">Physiological measurement</em>
41, 12 (2020),
124003.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anguita et al<span id="bib.bib4.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Davide Anguita, Alessandro
Ghio, Luca Oneto, Xavier Parra Perez,
and Jorge Luis Reyes Ortiz.
2013.

</span>
<span class="ltx_bibblock">A public domain dataset for human activity
recognition using smartphones. In <em id="bib.bib4.3.1" class="ltx_emph ltx_font_italic">Proceedings of
the 21th international European symposium on artificial neural networks,
computational intelligence and machine learning</em>. 437–442.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Becerik-Gerber et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Burçin Becerik-Gerber,
Gale M. Lucas, Ashrant Aryal,
Mohamad Awada, Mario Bergés,
Sarah Billington, Olga Boric-Lubecke,
Ali Ghahramani, Arsalan Heydarian,
Christoph Höelscher, Farrokh
Jazizadeh, Azam Khan, Jared Langevin,
Ruying Liu, Frederick Marks,
Matthew Louis Mauriello, Elizabeth L.
Murnane, Haeyoung Noh, Marco Pritoni,
Shawn C Roll, Davide Schaumann,
Mir Hasan Seyedrezaei, John Ellor Taylor,
Jie Zhao, and Runhe Zhu.
2022.

</span>
<span class="ltx_bibblock">The field of human building interaction for
convergent research and innovation for intelligent built environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">Scientific Reports</em> 12
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonawitz et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir
Ivanov, Ben Kreuter, Antonio Marcedone,
H Brendan McMahan, Sarvar Patel,
Daniel Ramage, Aaron Segal, and
Karn Seth. 2017.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving
machine learning. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">proceedings of the 2017 ACM
SIGSAC Conference on Computer and Communications Security</em>.
1175–1191.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Booth et al<span id="bib.bib7.2.2.1" class="ltx_text">.</span> (2019a)</span>
<span class="ltx_bibblock">
Brandon M Booth, Tiantian
Feng, Abhishek Jangalwa, and
Shrikanth S Narayanan. 2019a.

</span>
<span class="ltx_bibblock">Toward robust interpretable human movement pattern
analysis in a workplace setting. In <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>. IEEE, 7630–7634.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Booth et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2019b)</span>
<span class="ltx_bibblock">
Brandon M Booth, Karel
Mundnich, Tiantian Feng, Amrutha
Nadarajan, Tiago H Falk, Jennifer L
Villatte, Emilio Ferrara, and Shrikanth
Narayanan. 2019b.

</span>
<span class="ltx_bibblock">Multimodal human and environmental sensing for
longitudinal behavioral studies in naturalistic settings: Framework for
sensor selection, deployment, and management.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">Journal of medical Internet research</em>
21, 8 (2019),
e12832.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caldas et al<span id="bib.bib9.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai
Meher Karthik Duddu, Peter Wu, Tian Li,
Jakub Konečnỳ, H Brendan
McMahan, Virginia Smith, and Ameet
Talwalkar. 2018.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.01097</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2014)</span>
<span class="ltx_bibblock">
Houwei Cao, David G
Cooper, Michael K Keutmann, Ruben C Gur,
Ani Nenkova, and Ragini Verma.
2014.

</span>
<span class="ltx_bibblock">Crema-d: Crowd-sourced emotional multimodal actors
dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">IEEE transactions on affective computing</em>
5, 4 (2014),
377–390.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Zhang (2022)</span>
<span class="ltx_bibblock">
Jiayi Chen and Aidong
Zhang. 2022.

</span>
<span class="ltx_bibblock">FedMSplit: Correlation-Adaptive Federated
Multi-Task Learning across Multimodal Split Networks. In
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining</em>. 87–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Rudnicky (2021)</span>
<span class="ltx_bibblock">
Li-Wei Chen and
Alexander Rudnicky. 2021.

</span>
<span class="ltx_bibblock">Exploring Wav2vec 2.0 fine-tuning for improved
speech emotion recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.06309</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al<span id="bib.bib13.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yae Jee Cho, Andre
Manoel, Gauri Joshi, Robert Sim, and
Dimitrios Dimitriadis. 2022.

</span>
<span class="ltx_bibblock">Heterogeneous ensemble knowledge transfer for
training large models in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.12703</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span id="bib.bib14.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei
Chang, Kenton Lee, and Kristina
Toutanova. 2019.

</span>
<span class="ltx_bibblock">BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/1810.04805
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dimitriadis et al<span id="bib.bib15.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Dimitrios Dimitriadis,
Mirian Hipolito Garcia, Daniel Madrigal
Diaz, Andre Manoel, and Robert Sim.
2022.

</span>
<span class="ltx_bibblock">Flute: A scalable, extensible framework for
high-performance federated learning simulations.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.13789</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong and Voiculescu (2021)</span>
<span class="ltx_bibblock">
Nanqing Dong and Irina
Voiculescu. 2021.

</span>
<span class="ltx_bibblock">Federated contrastive learning for decentralized
unlabeled medical images. In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Medical Image
Computing and Computer Assisted Intervention–MICCAI 2021: 24th International
Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings,
Part III 24</em>. Springer, 378–387.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dwork (2006)</span>
<span class="ltx_bibblock">
Cynthia Dwork.
2006.

</span>
<span class="ltx_bibblock">Differential privacy. In
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Automata, Languages and Programming: 33rd
International Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006,
Proceedings, Part II 33</em>. Springer, 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Tiantian Feng, Brandon M
Booth, Brooke Baldwin-Rodríguez,
Felipe Osorno, and Shrikanth
Narayanan. 2021a.

</span>
<span class="ltx_bibblock">A multimodal analysis of physical activity, sleep,
and work shift in nurses with wearable sensor data.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.3.1" class="ltx_emph ltx_font_italic">Scientific reports</em> 11,
1 (2021), 8693.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Tiantian Feng, Hanieh
Hashemi, Rajat Hebbar, Murali Annavaram,
and Shrikanth S Narayanan.
2021b.

</span>
<span class="ltx_bibblock">Attribute inference attack of speech emotion
recognition in federated learning settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.13416</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tiantian Feng, Rajat
Hebbar, Nicholas Mehlman, Xuan Shi,
Aditya Kommineni, and Shrikanth
Narayanan. 2023.

</span>
<span class="ltx_bibblock">A Review of Speech-centric Trustworthy Machine
Learning: Privacy, Safety, and Fairness.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">APSIPA Transactions on Signal and Information
Processing</em> 12, 3
(2023).

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1561/116.00000084" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1561/116.00000084</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Narayanan (2019a)</span>
<span class="ltx_bibblock">
Tiantian Feng and
Shrikanth Narayanan. 2019a.

</span>
<span class="ltx_bibblock">Imputing missing data in large-scale multivariate
biomedical wearable recordings using bidirectional recurrent neural networks
with temporal activation regularization. In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2019
41st Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC)</em>. IEEE, 2529–2534.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Narayanan (2022)</span>
<span class="ltx_bibblock">
Tiantian Feng and
Shrikanth Narayanan. 2022.

</span>
<span class="ltx_bibblock">Semi-FedSER: Semi-supervised Learning for Speech
Emotion Recognition On Federated Learning using Multiview Pseudo-Labeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2203.08810</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Narayanan (2019b)</span>
<span class="ltx_bibblock">
Tiantian Feng and
Shrikanth S Narayanan. 2019b.

</span>
<span class="ltx_bibblock">Discovering optimal variable-length time series
motifs in large-scale wearable recordings of human bio-behavioral signals.
In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP)</em>. IEEE,
7615–7619.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span id="bib.bib24.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tiantian Feng, Raghuveer
Peri, and Shrikanth Narayanan.
2022.

</span>
<span class="ltx_bibblock">User-Level Differential Privacy against Attribute
Inference Attack of Speech Emotion Recognition on Federated Learning. In
<em id="bib.bib24.3.1" class="ltx_emph ltx_font_italic">Proc. Interspeech 2022</em>.
5055–5059.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.21437/Interspeech.2022-10060" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.21437/Interspeech.2022-10060</a>

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Chong Fu, Xuhong Zhang,
Shouling Ji, Jinyin Chen,
Jingzheng Wu, Shanqing Guo,
Jun Zhou, Alex X Liu, and
Ting Wang. 2022.

</span>
<span class="ltx_bibblock">Label inference attacks against vertical federated
learning. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">31st USENIX Security Symposium
(USENIX Security 22)</em>. 1397–1414.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jiahui Geng, Yongli Mou,
Feifei Li, Qing Li, Oya
Beyan, Stefan Decker, and Chunming
Rong. 2021.

</span>
<span class="ltx_bibblock">Towards General Deep Leakage in Federated
Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.09074</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew
Westbury, Eugene Byrne, Zachary Chavis,
Antonino Furnari, Rohit Girdhar,
Jackson Hamburger, Hao Jiang,
Miao Liu, Xingyu Liu, et al<span id="bib.bib27.3.1" class="ltx_text">.</span>
2022.

</span>
<span class="ltx_bibblock">Ego4d: Around the world in 3,000 hours of
egocentric video. In <em id="bib.bib27.4.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition</em>.
18995–19012.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Chaoyang He, Keshav
Balasubramanian, Emir Ceyani, Yu Rong,
Peilin Zhao, Junzhou Huang,
Murali Annavaram, and Salman
Avestimehr. 2021a.

</span>
<span class="ltx_bibblock">FedGraphNN: A Federated Learning System and
Benchmark for Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2104.07145
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li,
Jinhyun So, Mi Zhang,
Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh,
Hang Qiu, Li Shen,
Peilin Zhao, Yan Kang,
Yang Liu, Ramesh Raskar,
Qiang Yang, Murali Annavaram, and
Salman Avestimehr. 2020.

</span>
<span class="ltx_bibblock">FedML: A Research Library and Benchmark for
Federated Machine Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.13518</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Chaoyang He,
Alay Dilipbhai Shah, Zhenheng Tang,
Di Fan1Adarshan Naiynar Sivashunmugam,
Keerti Bhogaraju, Mita Shimpi,
Li Shen, Xiaowen Chu,
Mahdi Soltanolkotabi, and Salman
Avestimehr. 2021b.

</span>
<span class="ltx_bibblock">Fedcv: a federated learning framework for diverse
computer vision tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.11066</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Howard et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Andrew G Howard, Menglong
Zhu, Bo Chen, Dmitry Kalenichenko,
Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam.
2017.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks
for mobile vision applications.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1704.04861</em>
(2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Itahara et al<span id="bib.bib32.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sohei Itahara, Takayuki
Nishio, Yusuke Koda, Masahiro Morikura,
and Koji Yamamoto. 2021.

</span>
<span class="ltx_bibblock">Distillation-based semi-supervised federated
learning for communication-efficient collaborative training with non-iid
private data.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>
22, 1 (2021),
191–205.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaegle et al<span id="bib.bib33.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Andrew Jaegle, Felix
Gimeno, Andy Brock, Oriol Vinyals,
Andrew Zisserman, and Joao Carreira.
2021.

</span>
<span class="ltx_bibblock">Perceiver: General perception with iterative
attention. In <em id="bib.bib33.3.1" class="ltx_emph ltx_font_italic">International conference on machine
learning</em>. PMLR, 4651–4664.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kairouz et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Peter Kairouz, H Brendan
McMahan, Brendan Avent, Aurélien
Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary
Charles, Graham Cormode, Rachel
Cummings, et al<span id="bib.bib34.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.4.1" class="ltx_emph ltx_font_italic">Foundations and Trends® in
Machine Learning</em> 14, 1–2
(2021), 1–210.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yan Kang, Yang Liu, and
Xinle Liang. 2022.

</span>
<span class="ltx_bibblock">Fedcvt: Semi-supervised vertical federated learning
with cross-view training.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">ACM Transactions on Intelligent Systems and
Technology (TIST)</em> 13, 4
(2022), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimireddy et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy,
Satyen Kale, Mehryar Mohri,
Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. 2020.

</span>
<span class="ltx_bibblock">Scaffold: Stochastic controlled averaging for
federated learning. In <em id="bib.bib36.3.1" class="ltx_emph ltx_font_italic">International Conference on
Machine Learning</em>. PMLR, 5132–5143.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kiela et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Douwe Kiela, Hamed
Firooz, Aravind Mohan, Vedanuj Goswami,
Amanpreet Singh, Pratik Ringshia, and
Davide Testuggine. 2020.

</span>
<span class="ltx_bibblock">The hateful memes challenge: Detecting hate speech
in multimodal memes.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
2611–2624.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Konečnỳ et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jakub Konečnỳ,
H Brendan McMahan, Felix X Yu,
Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon.
2016.

</span>
<span class="ltx_bibblock">Federated learning: Strategies for improving
communication efficiency.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05492</em>
(2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Fan Lai, Yinwei Dai,
Xiangfeng Zhu, Harsha V Madhyastha, and
Mosharaf Chowdhury. 2021.

</span>
<span class="ltx_bibblock">FedScale: Benchmarking model and system performance
of federated learning. In <em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Proceedings of the First
Workshop on Systems Challenges in Reliable and Secure Federated Learning</em>.
1–3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al<span id="bib.bib40.2.2.1" class="ltx_text">.</span> (2015)</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua
Bengio, and Geoffrey Hinton.
2015.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.3.1" class="ltx_emph ltx_font_italic">nature</em> 521,
7553 (2015), 436–444.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu,
Manzil Zaheer, Maziar Sanjabi,
Ameet Talwalkar, and Virginia Smith.
2020.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning and Systems</em>
2 (2020), 429–450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Zhan (2021)</span>
<span class="ltx_bibblock">
Xin-Chun Li and De-Chuan
Zhan. 2021.

</span>
<span class="ltx_bibblock">Fedrs: Federated learning with restricted softmax
for label distribution non-iid data. In
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 27th ACM SIGKDD Conference on
Knowledge Discovery &amp; Data Mining</em>. 995–1005.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Paul Pu Liang, Yiwei Lyu,
Xiang Fan, Zetian Wu,
Yun Cheng, Jason Wu,
Leslie Chen, Peter Wu,
Michelle A Lee, Yuke Zhu,
et al<span id="bib.bib43.3.1" class="ltx_text">.</span> 2021.

</span>
<span class="ltx_bibblock">Multibench: Multiscale benchmarks for multimodal
representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.07502</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Chaoyang
He, Zihang Zeng, Hulin Wang,
Yufen Huang, Mahdi Soltanolkotabi,
Xiang Ren, and Salman Avestimehr.
2021.

</span>
<span class="ltx_bibblock">Fednlp: Benchmarking federated learning methods for
natural language processing tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.08815</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Lin, Lingjing Kong,
Sebastian U Stich, and Martin Jaggi.
2020.

</span>
<span class="ltx_bibblock">Ensemble distillation for robust model fusion in
federated learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em> 33 (2020),
2351–2363.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Jiasen Lu, Jianwei Yang,
Dhruv Batra, and Devi Parikh.
2016.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual
question answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em> 29 (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McMahan et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider
Moore, Daniel Ramage, Seth Hampson,
and Blaise Aguera y Arcas.
2017.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks
from decentralized data. In <em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">Artificial
intelligence and statistics</em>. PMLR, 1273–1282.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta and Rastegari (2021)</span>
<span class="ltx_bibblock">
Sachin Mehta and
Mohammad Rastegari. 2021.

</span>
<span class="ltx_bibblock">Mobilevit: light-weight, general-purpose, and
mobile-friendly vision transformer.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2110.02178</em>
(2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Melis et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng
Song, Emiliano De Cristofaro, and
Vitaly Shmatikov. 2019.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in
collaborative learning. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">2019 IEEE Symposium on
Security and Privacy (SP)</em>. IEEE, 691–706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mireshghallah et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Fatemehsadat Mireshghallah,
Mohammadkazem Taram, Praneeth Vepakomma,
Abhishek Singh, Ramesh Raskar, and
Hadi Esmaeilzadeh. 2020.

</span>
<span class="ltx_bibblock">Privacy in deep learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.12254</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monfort et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mathew Monfort, Alex
Andonian, Bolei Zhou, Kandan
Ramakrishnan, Sarah Adel Bargal, Tom
Yan, Lisa Brown, Quanfu Fan,
Dan Gutfreund, Carl Vondrick,
et al<span id="bib.bib51.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Moments in time dataset: one million videos for
event understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.4.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and
machine intelligence</em> 42, 2
(2019), 502–508.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Northcutt et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Curtis Northcutt, Lu
Jiang, and Isaac Chuang.
2021.

</span>
<span class="ltx_bibblock">Confident learning: Estimating uncertainty in
dataset labels.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Journal of Artificial Intelligence Research</em>
70 (2021), 1373–1411.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pantelopoulos and Bourbakis (2009)</span>
<span class="ltx_bibblock">
Alexandros Pantelopoulos and
Nikolaos G Bourbakis. 2009.

</span>
<span class="ltx_bibblock">A survey on wearable sensor-based systems for
health monitoring and prognosis.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Systems, Man, and
Cybernetics, Part C (Applications and Reviews)</em> 40,
1 (2009), 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parthasarathy and Sundaram (2020)</span>
<span class="ltx_bibblock">
Srinivas Parthasarathy and
Shiva Sundaram. 2020.

</span>
<span class="ltx_bibblock">Training strategies to handle missing modalities
for audio-visual expression recognition. In
<em id="bib.bib54.1.1" class="ltx_emph ltx_font_italic">Companion Publication of the 2020 International
Conference on Multimodal Interaction</em>. 400–404.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Shyamal Patel, Hyung
Park, Paolo Bonato, Leighton Chan, and
Mary Rodgers. 2012.

</span>
<span class="ltx_bibblock">A review of wearable sensors and systems with
application in rehabilitation.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">Journal of neuroengineering and
rehabilitation</em> 9, 1
(2012), 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poria et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Soujanya Poria, Devamanyu
Hazarika, Navonil Majumder, Gautam Naik,
Erik Cambria, and Rada Mihalcea.
2018.

</span>
<span class="ltx_bibblock">Meld: A multimodal multi-party dataset for emotion
recognition in conversations.

</span>
<span class="ltx_bibblock"><em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.02508</em>
(2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raij et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2011)</span>
<span class="ltx_bibblock">
Andrew Raij, Animikh
Ghosh, Santosh Kumar, and Mani
Srivastava. 2011.

</span>
<span class="ltx_bibblock">Privacy risks emerging from the adoption of
innocuous wearable sensors in the mobile environment. In
<em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems</em>. 11–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddi et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary
Charles, Manzil Zaheer, Zachary Garrett,
Keith Rush, Jakub Konečnỳ,
Sanjiv Kumar, and H Brendan McMahan.
2020.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2003.00295</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryoo et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Michael S Ryoo, AJ
Piergiovanni, Mingxing Tan, and Anelia
Angelova. 2019.

</span>
<span class="ltx_bibblock">Assemblenet: Searching for multi-stream neural
connectivity in video architectures.

</span>
<span class="ltx_bibblock"><em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1905.13209</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saeed et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Aaqib Saeed, Flora D
Salim, Tanir Ozcelebi, and Johan
Lukkien. 2020.

</span>
<span class="ltx_bibblock">Federated self-supervised learning of multisensor
representations for embedded intelligence.

</span>
<span class="ltx_bibblock"><em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Journal</em>
8, 2 (2020),
1030–1040.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanh et al<span id="bib.bib61.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Victor Sanh, Lysandre
Debut, Julien Chaumond, and Thomas
Wolf. 2019.

</span>
<span class="ltx_bibblock">DistilBERT, a distilled version of BERT: smaller,
faster, cheaper and lighter.

</span>
<span class="ltx_bibblock"><em id="bib.bib61.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1910.01108</em>
(2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sannara et al<span id="bib.bib62.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
EK Sannara, Francois
Portet, Philippe Lalanda, and VEGA
German. 2021.

</span>
<span class="ltx_bibblock">A federated learning aggregation algorithm for
pervasive computing: Evaluation and comparison. In
<em id="bib.bib62.3.1" class="ltx_emph ltx_font_italic">2021 IEEE International Conference on Pervasive
Computing and Communications (PerCom)</em>. IEEE, 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sikder and Nahid (2021)</span>
<span class="ltx_bibblock">
Niloy Sikder and
Abdullah-Al Nahid. 2021.

</span>
<span class="ltx_bibblock">KU-HAR: An open dataset for heterogeneous human
activity recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters</em>
146 (2021), 46–54.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soomro et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2012)</span>
<span class="ltx_bibblock">
Khurram Soomro,
Amir Roshan Zamir, and Mubarak Shah.
2012.

</span>
<span class="ltx_bibblock">UCF101: A dataset of 101 human actions classes from
videos in the wild.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1212.0402</em>
(2012).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strodthoff et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Nils Strodthoff, Patrick
Wagner, Tobias Schaeffter, and Wojciech
Samek. 2020.

</span>
<span class="ltx_bibblock">Deep learning for ECG analysis: Benchmarks and
insights from PTB-XL.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Biomedical and Health
Informatics</em> 25, 5
(2020), 1519–1528.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Hongkun Yu,
Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou.
2020.

</span>
<span class="ltx_bibblock">Mobilebert: a compact task-agnostic bert for
resource-limited devices.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.02984</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Terrail et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jean Ogier du Terrail,
Samy-Safwan Ayed, Edwige Cyffers,
Felix Grimberg, Chaoyang He,
Regis Loeb, Paul Mangold,
Tanguy Marchand, Othmane Marfoq,
Erum Mushtaq, et al<span id="bib.bib67.3.1" class="ltx_text">.</span>
2022.

</span>
<span class="ltx_bibblock">FLamby: Datasets and Benchmarks for Cross-Silo
Federated Learning in Realistic Healthcare Settings.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.04620</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsouvalas et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Vasileios Tsouvalas, Tanir
Ozcelebi, and Nirvana Meratnia.
2022.

</span>
<span class="ltx_bibblock">Privacy-preserving Speech Emotion Recognition
through Semi-Supervised Federated Learning. In
<em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">2022 IEEE International Conference on Pervasive
Computing and Communications Workshops and other Affiliated Events (PerCom
Workshops)</em>. IEEE, 359–364.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib69.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin.
2017.

</span>
<span class="ltx_bibblock">Attention is All you Need. In
<em id="bib.bib69.3.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing
Systems</em>, I. Guyon,
U. Von Luxburg, S. Bengio,
H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates,
Inc.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Wagner, Nils
Strodthoff, Ralf-Dieter Bousseljot,
Dieter Kreiseler, Fatima I Lunze,
Wojciech Samek, and Tobias Schaeffter.
2020.

</span>
<span class="ltx_bibblock">PTB-XL, a large publicly available
electrocardiography dataset.

</span>
<span class="ltx_bibblock"><em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Scientific data</em> 7,
1 (2020), 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib71.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Meng Wang, Weijie Fu,
Xiangnan He, Shijie Hao, and
Xindong Wu. 2020.

</span>
<span class="ltx_bibblock">A survey on large-scale machine learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Knowledge and Data
Engineering</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span id="bib.bib72.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhen Wang, Weirui Kuang,
Yuexiang Xie, Liuyi Yao,
Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022.

</span>
<span class="ltx_bibblock">FederatedScope-GNN: Towards a Unified,
Comprehensive and Efficient Package for Federated Graph Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM SIGKDD Conference
on Knowledge Discovery and Data Mining</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib73.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li,
Ming Ding, Chuan Ma,
Howard H Yang, Farhad Farokhi,
Shi Jin, Tony QS Quek, and
H Vincent Poor. 2020.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy:
Algorithms and performance analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib73.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics
and Security</em> 15 (2020),
3454–3469.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuexiang Xie, Zhen Wang,
Daoyuan Chen, Dawei Gao,
Liuyi Yao, Weirui Kuang,
Yaliang Li, Bolin Ding, and
Jingren Zhou. 2022.

</span>
<span class="ltx_bibblock">FederatedScope: A Comprehensive and Flexible
Federated Learning Platform via Message Passing.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2204.05011
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Baochen Xiong, Xiaoshan
Yang, Fan Qi, and Changsheng Xu.
2022.

</span>
<span class="ltx_bibblock">A unified framework for multi-modal federated
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Neurocomputing</em> 480
(2022), 110–118.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yaldiz et al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Duygu Yaldiz, Tuo Zhang,
and Salman Avestimehr. 2023.

</span>
<span class="ltx_bibblock">Secure Federated Learning against Model Poisoning
Attacks via Client Filtering.

</span>
<span class="ltx_bibblock"><em id="bib.bib76.3.1" class="ltx_emph ltx_font_italic">ArXiv</em> abs/2304.00160
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib77.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Zichao Yang, Diyi Yang,
Chris Dyer, Xiaodong He,
Alex Smola, and Eduard Hovy.
2016.

</span>
<span class="ltx_bibblock">Hierarchical attention networks for document
classification. In <em id="bib.bib77.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2016
conference of the North American chapter of the association for computational
linguistics: human language technologies</em>. 1480–1489.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span id="bib.bib78.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Qiying Yu, Yimu Wang,
Ke Xu, Yang Liu, and
Jingjing Liu. 2023.

</span>
<span class="ltx_bibblock">Multimodal Federated Learning via Contrastive
Representation Ensemble. In <em id="bib.bib78.3.1" class="ltx_emph ltx_font_italic">International
Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://openreview.net/forum?id=Hnk1WRMAYqg" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Hnk1WRMAYqg</a>

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Fengda Zhang, Kun Kuang,
Zhaoyang You, Tao Shen,
Jun Xiao, Yin Zhang,
Chao Wu, Yueting Zhuang, and
Xiaolin Li. 2020.

</span>
<span class="ltx_bibblock">Federated unsupervised representation learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2010.08982</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Tuo Zhang, Tiantian Feng,
Samiul Alam, Sunwoo Lee,
Mi Zhang, Shrikanth S Narayanan, and
Salman Avestimehr. 2022.

</span>
<span class="ltx_bibblock">FedAudio: A Federated Learning Benchmark for Audio
Tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.15707</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Tuo Zhang, Lei Gao,
Chaoyang He, Mi Zhang,
Bhaskar Krishnamachari, and Salman
Avestimehr. 2021a.

</span>
<span class="ltx_bibblock">Federated Learning for the Internet of Things:
Applications, Challenges, and Opportunities.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">IEEE Internet of Things Magazine</em>
5 (2021), 24–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Zhengming Zhang, Yaoqing
Yang, Zhewei Yao, Yujun Yan,
Joseph E Gonzalez, Kannan Ramchandran,
and Michael W Mahoney. 2021b.

</span>
<span class="ltx_bibblock">Improving semi-supervised federated learning by
reducing the gradient diversity of models. In <em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">2021
IEEE International Conference on Big Data (Big Data)</em>. IEEE,
1214–1225.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Yuchen Zhao, Hanyang Liu,
Honglin Li, Payam Barnaghi, and
Hamed Haddadi. 2020.

</span>
<span class="ltx_bibblock">Semi-supervised federated learning for activity
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2011.00851</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu and Han (2020)</span>
<span class="ltx_bibblock">
Ligeng Zhu and Song
Han. 2020.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock">In <em id="bib.bib84.1.1" class="ltx_emph ltx_font_italic">Federated learning</em>.
Springer, 17–31.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span id="bib.bib85.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Weiming Zhuang, Xin Gan,
Yonggang Wen, Shuai Zhang, and
Shuai Yi. 2021.

</span>
<span class="ltx_bibblock">Collaborative unsupervised visual representation
learning from decentralized data. In <em id="bib.bib85.3.1" class="ltx_emph ltx_font_italic">Proceedings
of the IEEE/CVF international conference on computer vision</em>.
4912–4921.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2306.09485" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2306.09486" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2306.09486">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2306.09486" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2306.09487" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 00:25:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
