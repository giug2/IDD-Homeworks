<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.07333] RSVQA: Visual Question Answering for Remote Sensing Data</title><meta property="og:description" content="This paper introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information which can be useful for a wide range of tasks including land cover clas…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="RSVQA: Visual Question Answering for Remote Sensing Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="RSVQA: Visual Question Answering for Remote Sensing Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.07333">

<!--Generated on Sun Mar 17 09:00:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Visual Question Answering,  Deep learning,  Dataset,  Natural Language,  Convolution Neural Networks,  Recurrent Neural Networks,  Very High Resolution,  OpenStreetMap
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">RSVQA: Visual Question Answering for Remote Sensing Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sylvain Lobry, 
Diego Marcos,
Jesse Murray,
Devis Tuia, 

</span><span class="ltx_author_notes">Sylvain Lobry, Diego Marcos, Jesse Murray and Devis Tuia are with Laboratory of Geo-Information Science and Remote Sensing, Wageningen University, The Netherlands email: work@sylvainlobry.com</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">This paper introduces the task of visual question answering for remote sensing data (RSVQA). Remote sensing images contain a wealth of information which can be useful for a wide range of tasks including land cover classification, object counting or detection. However, most of the available methodologies are task-specific, thus inhibiting generic and easy access to the information contained in remote sensing data. As a consequence, accurate remote sensing product generation still requires expert knowledge. With RSVQA, we propose a system to extract information from remote sensing data that is accessible to every user: we use questions formulated in natural language and use them to interact with the images. With the system, images can be queried to obtain high level information specific to the image content or relational dependencies between objects visible in the images. Using an automatic method introduced in this article, we built two datasets (using low and high resolution data) of image/question/answer triplets. The information required to build the questions and answers is queried from OpenStreetMap (OSM). The datasets can be used to train (when using supervised methods) and evaluate models to solve the RSVQA task. We report the results obtained by applying a model based on Convolutional Neural Networks (CNNs) for the visual part and on a Recurrent Neural Network (RNN) for the natural language part to this task. The model is trained on the two datasets, yielding promising results in both cases.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Visual Question Answering, Deep learning, Dataset, Natural Language, Convolution Neural Networks, Recurrent Neural Networks, Very High Resolution, OpenStreetMap

</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text" style="font-size:90%;">This is the pre-acceptance version, to read the final version published in the journal IEEE Transactions on Geoscience and Remote Sensing, please go to: <a target="_blank" href="https://doi.org/10.1109/TGRS.2020.2988782" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/TGRS.2020.2988782</a>.
</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Remote sensing data is widely used as an <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">indirect</em> source of information. From land cover/land use to crowd estimation, environmental or urban area monitoring, remote sensing images are used in a wide range of tasks of high societal relevance. For instance, remote sensing data can be used as a source of information for 6 of the 17 sustainable development goals as defined by the United Nations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Due to the critical nature of the problems that can be addressed using remote sensing data, significant effort has been made to increase its availability in the last decade. For instance, Sentinel-2 satellites provide multispectral data with a relatively short revisiting time, in open-access. However, while substantial effort has been dedicated to improving the means of direct information extraction from Sentinel-2 data in the framework of a given task (e.g. classification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>), the ability to use remote sensing data as a <em id="S1.p1.1.2" class="ltx_emph ltx_font_italic">direct</em> source of information is currently limited to experts within the remote sensing and computer vision communities. This constraint, imposed by the technical nature of the task, reduces both the scale and variety of the problems that could be addressed with such information as well as the number of potential end-users. This is particularly true when targeting specific applications (detecting particular objects, <em id="S1.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em> thatched roofs or buildings in a developing country <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>) which would today call for important research efforts. The targeted tasks are often multiple and changing in the scope of a project calls for strong expert knowledge, limiting the information which can be extracted from remote sensing data. To address these constraints, we introduce the problem of visual question answering (VQA) for remote sensing data.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2003.07333/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="592" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example of tasks achievable by a visual question answering model for remote sensing data.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">VQA is a new task in computer vision, introduced in its current form by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The objective of VQA is to answer a free-form and open-ended question about a given image. As the questions can be unconstrained, a VQA model applied to remote sensing data could serve as a generic solution to classical problems involving remote sensing data (e.g. ”Is there a thatched roof in this image?” for thatched roof detection), but also very specific tasks involving relations between objects of different nature (e.g. ”Is there a thatched roof on the right of the river?”). Examples of potential questions are shown in <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>.
<br class="ltx_break"></p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To the best of our knowledge, this is the first time (after the first exploration in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>) that VQA has been applied to extract information from remote sensing data. It builds on the task of generating descriptions of images through combining image and natural language processing to provide the user with easily accessible, high-level semantic information. These descriptions are then used for image retrieval and intelligence generation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. As seen in this introduction, VQA systems rely on the recent advances in deep learning. Deep learning based methods, thanks to their ability to extract high-level features, have been successfully developed for remote sensing data as reviewed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Nowadays, this family of methods is used to tackle a variety of tasks; for scene classification, an early work by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> evaluated the possibility to adapt networks pre-trained on large natural image databases (such as ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>) to classify hyperspectral remote sensing images. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> used an intermediate high level representation using recurrent attention maps to classify images. Object detection is also often approached using deep learning methods. To this effect, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduced an object detection dataset and evaluated classical deep learning approaches. Methods taking into account the specificity of remote sensing data have been developed, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> which proposed to modify the classical approach by generating rotatable region proposal which are particularly relevant for top-view imagery. Deep learning methods have also been developed for semantic segmentation. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, the authors evaluated different strategies for segmenting remote sensing data. More recently, a contest organized on the dataset of building segmentation created by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> has motivated the development of a number of new methods to improve results on this task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> introduced a contest including three tasks: road extraction, building detection and land cover classification. Best results for each challenge were obtained using deep neural networks: <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Natural language processing has also been used in remote sensing. For instance, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> used a convolutional neural network (CNN) to generate classification probabilities for a given image, and used a recurrent neural network (RNN) to generate its description. In a similar fashion, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> used a CNN to obtain a multi semantic level representation of an image (object, land class, landscape) and generate a description using a simple static model. More recently, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> uses an encoder/decoder type of architecture where a CNN encodes the image and a RNN decodes it to a textual representation, while <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> projects the textual representation and the image to a common space. While these works are use cases of natural language processing, they do not enable interactions with the user as we propose with VQA.
<br class="ltx_break"></p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">A VQA model is generally made of 4 distinct components: 1) a visual feature extractor, 2) a language feature extractor, 3) a fusion step between the two modalities and 4) a prediction component.
Since VQA is a relatively new task, an important number of methodological developments have been published in both the computer vision and natural language processing communities during the past 5 years, reviewed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. VQA models are able to benefit from advances in the computer vision and automatic language processing communities for the features extraction components. However, the multi-modal fusion has been less explored and therefore, an important amount of work has been dedicated to this step. First VQA models relied on a non-spatial fusion method, <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">i.e.</em> a point-wise multiplication between the visual and language feature vectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Being straightforward, this method does not allow every component from both feature vectors to interact with each other. This interaction would ideally be achieved by multiplying the first feature vector by the transpose of the other, but this operation would be computationally intractable in practice. Instead, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> proposed a fusion method which first selects relevant visual features based on the textual feature (attention step) and then, combines them with the textual feature. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, the authors used Tucker decomposition to achieve a similar purpose in one step. While these attention mechanisms are interesting for finding visual elements aligned with the words within the question, they require the image to be divided in a regular grid for the computation of the attention, and this is not suitable to objects of varying size. A solution is presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which learns an object detector to select relevant parts of the image. In this research, we use a non-spatial fusion step to keep the model part relatively simple. Most traditional VQA works are designed for a specific dataset, either composed of natural images (with questions covering an unconstrained range of topics) or synthetic images. While interesting for the methodological developments that they have facilitated, these datasets limit the potential applications of such systems to other problems. Indeed, it has been shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> that VQA models trained on a specific dataset do not generalize well to other datasets. This <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">generalization gap</em> raises questions concerning the applicability of such models to specific tasks.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">A notable use-case of VQA is helping visually impaired people through natural language interactions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Images acquired by visually impaired people represent an important domain shift, and as such a challenge for the applicability of VQA models. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, the authors confirm that networks trained on generic datasets do not generalize to their specific one. However, they manage to obtain much better results by fine-tuning or training models from scratch on their task-specific dataset.
<br class="ltx_break"></p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this study, we propose a new application for VQA, specifically for the interaction with remote sensing images. To this effect, we propose the first remote sensing-oriented VQA datasets, and evaluate the applicability of this task on remote sensing images. We propose a method to automatically generate remote sensing-oriented VQA datasets from already available human annotations in <a href="#S2" title="II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section II</span></a> and generate two datasets. We then use this newly-generated data to train our proposed RSVQA model with a non-spatial fusion step described in <a href="#S3" title="III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section III</span></a>. Finally, the results are evaluated and discussed in <a href="#S4" title="IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section IV</span></a>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Our contribution are the following:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">a method to generate remote sensing-oriented VQA datasets;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">2 datasets;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">the proposed RSVQA model.</p>
</div>
</li>
</ul>
<p id="S1.p8.2" class="ltx_p">This work extends the preliminary study of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> by considering and disclosing a second larger dataset consisting of very high resolution images. This second dataset helps testing the spatial generalization capability of VQA and provides an extensive discussion highlighting remaining challenges.
The method to generate the dataset, the RSVQA model and the two datasets are available on <a target="_blank" href="https://rsvqa.sylvainlobry.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://rsvqa.sylvainlobry.com/</a>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Datasets</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Method</span>
</h3>

<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x2.png" id="S2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="130" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Question construction procedure. Dash lines represent optional paths</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x3.png" id="S2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="257" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Construction path for sample questions.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustration of the question construction procedure.</figcaption>
</figure>
<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">As seen in the introduction, a main limiting factor for VQA is the availability of task-specific datasets. As such, we aim at providing a collection of remote sensing images with questions and answers associated to them. To do so, we took inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, in which the authors build a dataset of question/answer pairs about synthetic images following an automated procedure. However, in this study we are interested in real data (discussed in <a href="#S2.SS2" title="II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-B</span></span></a>). Therefore, we use the openly accessible OpenStreetMap data containing geo-localized information provided by volunteers. By leveraging this data, we can automatically extract the information required to obtain question/answer pairs relevant to real remotely sensed data and create a dataset made of (image, question, answer) triplets.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The first step of the database construction is to create the questions. The second step is to compute the answers to the questions, using the OSM features belonging to the image footprint. Note that multiple question/answer pairs are extracted for each image.</p>
</div>
<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS1.5.1.1" class="ltx_text">II-A</span>1 </span>Question contruction</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">Our method to construct the questions is illustrated in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. It consists of four main components:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">choice of an element category (highlighted in red in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(a));</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">application of attributes to the element (highlighted in green in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(a));</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">selection based on the relative location to another element (highlighted in green in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(a))</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">construction of the question (highlighted in blue in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(a)).</p>
</div>
</li>
</ol>
<p id="S2.SS1.SSS1.p1.2" class="ltx_p">Examples of question constructions are shown in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(b). These four components are detailed in the following.
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p2.1" class="ltx_p"><span id="S2.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Element category selection</span>:
First, an element category is randomly selected from the element catalog. This catalog is built by extracting the elements from one of the following OSM layers: <em id="S2.SS1.SSS1.p2.1.2" class="ltx_emph ltx_font_italic">road</em>, <em id="S2.SS1.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">water area</em>, <em id="S2.SS1.SSS1.p2.1.4" class="ltx_emph ltx_font_italic">building</em> and <em id="S2.SS1.SSS1.p2.1.5" class="ltx_emph ltx_font_italic">land use</em>. While roads and water areas are directly treated as elements, buildings and land use related objects are defined based on their ”type” field, as defined in the OSM data specification. Examples of land use objects include residential area, construction area, religious places, …Buildings are divided in two categories: commercial (e.g. retail, supermarket, …) and residential (e.g. house, apartments, …).</p>
</div>
<div id="S2.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p3.1" class="ltx_p"><span id="S2.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Attributes application</span>:
The second (optional) step is to refine the previously selected element category. To do so, we randomly select from one of the two possible attribute categories:</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Shape</span>:
each element can be either square, rectangular or circular. Whether an element belongs to one of these shape types is decided based on basic geometrical properties (i.e. hard thresholds on area-to-perimeter ratio and area-to-circumscribed circle area ratio).</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Size</span>:
using hard thresholds on the surface area, elements can be considered ”small”, ”medium” or ”large”. As we are interested in information at different scales in the two datasets, we use different threshold values, which are described in <a href="#S2.T1" title="TABLE I ‣ II-A1 Question contruction ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table I</span></a>.</p>
</div>
</li>
</ul>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.12" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.12.13.1" class="ltx_tr">
<th id="S2.T1.12.13.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Scale</th>
<th id="S2.T1.12.13.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Small</th>
<th id="S2.T1.12.13.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Medium</th>
<th id="S2.T1.12.13.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Large</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.6.6" class="ltx_tr">
<td id="S2.T1.6.6.7" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Low resolution</td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.1.1.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.m1.1b"><lt id="S2.T1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.m1.1c">&lt;</annotation></semantics></math> 3000m<sup id="S2.T1.2.2.2.1" class="ltx_sup">2</sup>
</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.3.3.3.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.3.3.3.m1.1a"><mo id="S2.T1.3.3.3.m1.1.1" xref="S2.T1.3.3.3.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.m1.1b"><lt id="S2.T1.3.3.3.m1.1.1.cmml" xref="S2.T1.3.3.3.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.m1.1c">&lt;</annotation></semantics></math> 10000m<sup id="S2.T1.4.4.4.1" class="ltx_sup">2</sup>
</td>
<td id="S2.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S2.T1.5.5.5.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S2.T1.5.5.5.m1.1a"><mo id="S2.T1.5.5.5.m1.1.1" xref="S2.T1.5.5.5.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.m1.1b"><geq id="S2.T1.5.5.5.m1.1.1.cmml" xref="S2.T1.5.5.5.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.m1.1c">\geq</annotation></semantics></math> 10000m<sup id="S2.T1.6.6.6.1" class="ltx_sup">2</sup>
</td>
</tr>
<tr id="S2.T1.12.12" class="ltx_tr">
<td id="S2.T1.12.12.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">High resolution</td>
<td id="S2.T1.8.8.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T1.7.7.1.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.7.7.1.m1.1a"><mo id="S2.T1.7.7.1.m1.1.1" xref="S2.T1.7.7.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.m1.1b"><lt id="S2.T1.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.m1.1c">&lt;</annotation></semantics></math> 100m<sup id="S2.T1.8.8.2.1" class="ltx_sup">2</sup>
</td>
<td id="S2.T1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T1.9.9.3.m1.1" class="ltx_Math" alttext="&lt;" display="inline"><semantics id="S2.T1.9.9.3.m1.1a"><mo id="S2.T1.9.9.3.m1.1.1" xref="S2.T1.9.9.3.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.3.m1.1b"><lt id="S2.T1.9.9.3.m1.1.1.cmml" xref="S2.T1.9.9.3.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.3.m1.1c">&lt;</annotation></semantics></math> 500m<sup id="S2.T1.10.10.4.1" class="ltx_sup">2</sup>
</td>
<td id="S2.T1.12.12.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<math id="S2.T1.11.11.5.m1.1" class="ltx_Math" alttext="\geq" display="inline"><semantics id="S2.T1.11.11.5.m1.1a"><mo id="S2.T1.11.11.5.m1.1.1" xref="S2.T1.11.11.5.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.5.m1.1b"><geq id="S2.T1.11.11.5.m1.1.1.cmml" xref="S2.T1.11.11.5.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.5.m1.1c">\geq</annotation></semantics></math> 500m<sup id="S2.T1.12.12.6.1" class="ltx_sup">2</sup>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Thresholds for size attributes according to the dataset scale. When dealing with low resolution data, visible objects of interest are larger. To deal with this disparity, we adapt the size thresholds to the resolution of the images.</figcaption>
</figure>
<div id="S2.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p4.1" class="ltx_p"><span id="S2.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Relative position</span>:
Another possibility to refine the element is to look at its relative position compared to another element. We define 5 relations: ”left of”, ”top of”, ”right of”, ”bottom of”, ”next to”. Note that these relative positions are understood in the image space (i.e. geographically). The special case of ”next to” is handled as a hard threshold on the relative distance between the two objects (less than 1000m). When looking at relative positions, we select the second element following the procedure previously defined.</p>
</div>
<div id="S2.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS1.p5.1" class="ltx_p"><span id="S2.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Question construction</span>:
At this point of the procedure, we have an element (e.g. road), with an optional attribute (e.g. small road) and an optional relative position (e.g. small road on the left of a water area). The final step is to generate a ”base question” about this element. We define 5 types of questions of interest (”Question catalog” in <a href="#S2.F2" title="Figure 2 ‣ II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>(a)), from which a specific type is randomly selected to obtain a base question. For instance, in the case of comparison questions, we randomly choose among ”less than”, ”equals to” and ”more than” and construct a second element.</p>
</div>
<div id="S2.SS1.SSS1.p6" class="ltx_para">
<p id="S2.SS1.SSS1.p6.1" class="ltx_p">This base question is then turned into a natural language question using pre-defined templates for each question type and object. For some question types (e.g. count), more than one template is defined (e.g. ’How many <span id="S2.SS1.SSS1.p6.1.1" class="ltx_text ltx_framed ltx_framed_underline">     </span> are there?’, ’What is the number of <span id="S2.SS1.SSS1.p6.1.2" class="ltx_text ltx_framed ltx_framed_underline">     </span>?’ or ’What is the amount of <span id="S2.SS1.SSS1.p6.1.3" class="ltx_text ltx_framed ltx_framed_underline">     </span>?’). In this case, the template to be used is randomly selected. The stochastic process ensures the diversity, both in the question types and the question templates used.</p>
</div>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS1.SSS2.5.1.1" class="ltx_text">II-A</span>2 </span>Answer construction</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p">:
To obtain the answer to the constructed question, we extract the objects from the OSM database corresponding to the image footprint. The objects <math id="S2.SS1.SSS2.p1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS1.SSS2.p1.1.m1.1a"><mi id="S2.SS1.SSS2.p1.1.m1.1.1" xref="S2.SS1.SSS2.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p1.1.m1.1b"><ci id="S2.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p1.1.m1.1c">b</annotation></semantics></math> corresponding to the element category and its attributes are then selected and used depending on the question type:</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p"><span id="S2.I3.i1.p1.1.1" class="ltx_text ltx_font_bold">Count</span>:
In the case of counting, the answer is simply the number of objects <math id="S2.I3.i1.p1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.I3.i1.p1.1.m1.1a"><mi id="S2.I3.i1.p1.1.m1.1.1" xref="S2.I3.i1.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.I3.i1.p1.1.m1.1b"><ci id="S2.I3.i1.p1.1.m1.1.1.cmml" xref="S2.I3.i1.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i1.p1.1.m1.1c">b</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p"><span id="S2.I3.i2.p1.1.1" class="ltx_text ltx_font_bold">Presence</span>:
A presence question is answered by comparing the number of objects <math id="S2.I3.i2.p1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.I3.i2.p1.1.m1.1a"><mi id="S2.I3.i2.p1.1.m1.1.1" xref="S2.I3.i2.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.I3.i2.p1.1.m1.1b"><ci id="S2.I3.i2.p1.1.m1.1.1.cmml" xref="S2.I3.i2.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i2.p1.1.m1.1c">b</annotation></semantics></math> to 0.</p>
</div>
</li>
<li id="S2.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i3.p1" class="ltx_para">
<p id="S2.I3.i3.p1.1" class="ltx_p"><span id="S2.I3.i3.p1.1.1" class="ltx_text ltx_font_bold">Area</span>:
The answer to a question about the area is the sum of the areas of the objects <math id="S2.I3.i3.p1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.I3.i3.p1.1.m1.1a"><mi id="S2.I3.i3.p1.1.m1.1.1" xref="S2.I3.i3.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.I3.i3.p1.1.m1.1b"><ci id="S2.I3.i3.p1.1.m1.1.1.cmml" xref="S2.I3.i3.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i3.p1.1.m1.1c">b</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I3.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i4.p1" class="ltx_para">
<p id="S2.I3.i4.p1.1" class="ltx_p"><span id="S2.I3.i4.p1.1.1" class="ltx_text ltx_font_bold">Comparison</span>:
Comparison is a specific case for which a second element and the relative position statement is needed. This question is then answered by comparing the number of objects <math id="S2.I3.i4.p1.1.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.I3.i4.p1.1.m1.1a"><mi id="S2.I3.i4.p1.1.m1.1.1" xref="S2.I3.i4.p1.1.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.I3.i4.p1.1.m1.1b"><ci id="S2.I3.i4.p1.1.m1.1.1.cmml" xref="S2.I3.i4.p1.1.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i4.p1.1.m1.1c">b</annotation></semantics></math> to the ones of the second element.</p>
</div>
</li>
<li id="S2.I3.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i5.p1" class="ltx_para">
<p id="S2.I3.i5.p1.1" class="ltx_p"><span id="S2.I3.i5.p1.1.1" class="ltx_text ltx_font_bold">Rural/Urban</span>:
The case of rural/urban questions is handled in a specific way. In this case, we do not create a specific element, but rather count the number of buildings (both commercial or residential). This number of buildings is then thresholded to a predefined number depending on the resolution of the input data (to obtain a density) to answer the question. Note that we are using a generic definition of rural and urban areas but this can be easily adapted using the precise definition of each country.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Data</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Following the method presented in <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>, we construct two datasets with different characteristics.
 
<br class="ltx_break"><span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_bold">Low resolution (LR)</span>:</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2003.07333/assets/Images/SpatialExtentLR_New.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="650" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Images selected for the LR dataset over the Netherlands. Each point represent one Sentinel-2 image which was later split into tiles. Red points represent training samples, green pentagon represents the validation image, and blue triangle is for the test image. Note that one training image is not visible (as it overlaps with the left-most image).</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">this dataset is based on Sentinel-2 images acquired over the Netherlands. Sentinel-2 satellites provide 10m resolution (for the visible bands used in this dataset) images with frequent updates (around 5 days) at a global scale. These images are openly available through ESA’s Copernicus Open Access Hub<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://scihub.copernicus.eu/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://scihub.copernicus.eu/</a></span></span></span>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.7" class="ltx_p">To generate the dataset, we selected 9 Sentinel-2 tiles covering the Netherlands with a low cloud cover (selected tiles are shown in <a href="#S2.F3" title="Figure 3 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>). These tiles were divided in <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="772" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mn id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">772</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><cn type="integer" id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">772</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">772</annotation></semantics></math> images of size <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="256\times 256" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml"><mn id="S2.SS2.p3.2.m2.1.1.2" xref="S2.SS2.p3.2.m2.1.1.2.cmml">256</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p3.2.m2.1.1.1" xref="S2.SS2.p3.2.m2.1.1.1.cmml">×</mo><mn id="S2.SS2.p3.2.m2.1.1.3" xref="S2.SS2.p3.2.m2.1.1.3.cmml">256</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><apply id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1"><times id="S2.SS2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1.1"></times><cn type="integer" id="S2.SS2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.p3.2.m2.1.1.2">256</cn><cn type="integer" id="S2.SS2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.p3.2.m2.1.1.3">256</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">256\times 256</annotation></semantics></math> (covering <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="6.55km^{2}" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><mrow id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mn id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">6.55</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p3.3.m3.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.cmml">​</mo><mi id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml">k</mi><mo lspace="0em" rspace="0em" id="S2.SS2.p3.3.m3.1.1.1a" xref="S2.SS2.p3.3.m3.1.1.1.cmml">​</mo><msup id="S2.SS2.p3.3.m3.1.1.4" xref="S2.SS2.p3.3.m3.1.1.4.cmml"><mi id="S2.SS2.p3.3.m3.1.1.4.2" xref="S2.SS2.p3.3.m3.1.1.4.2.cmml">m</mi><mn id="S2.SS2.p3.3.m3.1.1.4.3" xref="S2.SS2.p3.3.m3.1.1.4.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><times id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1"></times><cn type="float" id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">6.55</cn><ci id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3">𝑘</ci><apply id="S2.SS2.p3.3.m3.1.1.4.cmml" xref="S2.SS2.p3.3.m3.1.1.4"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.4.1.cmml" xref="S2.SS2.p3.3.m3.1.1.4">superscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.4.2.cmml" xref="S2.SS2.p3.3.m3.1.1.4.2">𝑚</ci><cn type="integer" id="S2.SS2.p3.3.m3.1.1.4.3.cmml" xref="S2.SS2.p3.3.m3.1.1.4.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">6.55km^{2}</annotation></semantics></math>) retaining the RGB bands. From these, we constructed <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="77^{\prime}232" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mrow id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml"><msup id="S2.SS2.p3.4.m4.1.1.2" xref="S2.SS2.p3.4.m4.1.1.2.cmml"><mn id="S2.SS2.p3.4.m4.1.1.2.2" xref="S2.SS2.p3.4.m4.1.1.2.2.cmml">77</mn><mo id="S2.SS2.p3.4.m4.1.1.2.3" xref="S2.SS2.p3.4.m4.1.1.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p3.4.m4.1.1.1" xref="S2.SS2.p3.4.m4.1.1.1.cmml">​</mo><mn id="S2.SS2.p3.4.m4.1.1.3" xref="S2.SS2.p3.4.m4.1.1.3.cmml">232</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><apply id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1"><times id="S2.SS2.p3.4.m4.1.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1.1"></times><apply id="S2.SS2.p3.4.m4.1.1.2.cmml" xref="S2.SS2.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.4.m4.1.1.2.1.cmml" xref="S2.SS2.p3.4.m4.1.1.2">superscript</csymbol><cn type="integer" id="S2.SS2.p3.4.m4.1.1.2.2.cmml" xref="S2.SS2.p3.4.m4.1.1.2.2">77</cn><ci id="S2.SS2.p3.4.m4.1.1.2.3.cmml" xref="S2.SS2.p3.4.m4.1.1.2.3">′</ci></apply><cn type="integer" id="S2.SS2.p3.4.m4.1.1.3.cmml" xref="S2.SS2.p3.4.m4.1.1.3">232</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">77^{\prime}232</annotation></semantics></math> questions and answers following the methodology presented in <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>. We split the data in a training set (<math id="S2.SS2.p3.5.m5.1" class="ltx_Math" alttext="77.8\%" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mrow id="S2.SS2.p3.5.m5.1.1" xref="S2.SS2.p3.5.m5.1.1.cmml"><mn id="S2.SS2.p3.5.m5.1.1.2" xref="S2.SS2.p3.5.m5.1.1.2.cmml">77.8</mn><mo id="S2.SS2.p3.5.m5.1.1.1" xref="S2.SS2.p3.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.5.m5.1b"><apply id="S2.SS2.p3.5.m5.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1"><csymbol cd="latexml" id="S2.SS2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.p3.5.m5.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.p3.5.m5.1.1.2">77.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">77.8\%</annotation></semantics></math> of the original tiles), a validation set (<math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="11.1\%" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mrow id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml"><mn id="S2.SS2.p3.6.m6.1.1.2" xref="S2.SS2.p3.6.m6.1.1.2.cmml">11.1</mn><mo id="S2.SS2.p3.6.m6.1.1.1" xref="S2.SS2.p3.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><apply id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1"><csymbol cd="latexml" id="S2.SS2.p3.6.m6.1.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p3.6.m6.1.1.2.cmml" xref="S2.SS2.p3.6.m6.1.1.2">11.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">11.1\%</annotation></semantics></math>) and a test set (<math id="S2.SS2.p3.7.m7.1" class="ltx_Math" alttext="11.1\%" display="inline"><semantics id="S2.SS2.p3.7.m7.1a"><mrow id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml"><mn id="S2.SS2.p3.7.m7.1.1.2" xref="S2.SS2.p3.7.m7.1.1.2.cmml">11.1</mn><mo id="S2.SS2.p3.7.m7.1.1.1" xref="S2.SS2.p3.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.1b"><apply id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1"><csymbol cd="latexml" id="S2.SS2.p3.7.m7.1.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p3.7.m7.1.1.2.cmml" xref="S2.SS2.p3.7.m7.1.1.2">11.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.1c">11.1\%</annotation></semantics></math>) at the tile level (the spatial split is shown in <a href="#S2.F3" title="Figure 3 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>). This allows to limit spatial correlation between the different splits.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">High resolution (HR)</span>:</p>
</div>
<figure id="S2.F4" class="ltx_figure"><img src="/html/2003.07333/assets/Images/SpatialExtentUSGS_New.png" id="S2.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="500" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Extent of the HR dataset with a zoom on the Portland, Manhattan (New York City) and Philadelphia areas. Each point represent one image (generally of size <math id="S2.F4.2.m1.1" class="ltx_Math" alttext="5000\times 5000" display="inline"><semantics id="S2.F4.2.m1.1b"><mrow id="S2.F4.2.m1.1.1" xref="S2.F4.2.m1.1.1.cmml"><mn id="S2.F4.2.m1.1.1.2" xref="S2.F4.2.m1.1.1.2.cmml">5000</mn><mo lspace="0.222em" rspace="0.222em" id="S2.F4.2.m1.1.1.1" xref="S2.F4.2.m1.1.1.1.cmml">×</mo><mn id="S2.F4.2.m1.1.1.3" xref="S2.F4.2.m1.1.1.3.cmml">5000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F4.2.m1.1c"><apply id="S2.F4.2.m1.1.1.cmml" xref="S2.F4.2.m1.1.1"><times id="S2.F4.2.m1.1.1.1.cmml" xref="S2.F4.2.m1.1.1.1"></times><cn type="integer" id="S2.F4.2.m1.1.1.2.cmml" xref="S2.F4.2.m1.1.1.2">5000</cn><cn type="integer" id="S2.F4.2.m1.1.1.3.cmml" xref="S2.F4.2.m1.1.1.3">5000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F4.2.m1.1d">5000\times 5000</annotation></semantics></math>) which was later split into tiles. The images cover the New York City/Long Island region, Philadelphia and Portland. Red points represent training samples, green pentagons represent validation samples, and blue indicators are for the test sets (blue triangles for test set 1, blue stars for test set 2).</figcaption>
</figure>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p">this dataset uses 15cm resolution aerial RGB images extracted from the High Resolution Orthoimagery (HRO) data collection of the USGS. This collection covers most urban areas of the USA, along with a few areas of interest (e.g. national parks). For most areas covered by the dataset, only one tile is available with acquisition dates ranging from year 2000 to 2016, with various sensors. The tiles are openly accessible through USGS’ EarthExplorer tool<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://earthexplorer.usgs.gov/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://earthexplorer.usgs.gov/</a></span></span></span>.</p>
</div>
<div id="S2.SS2.p6" class="ltx_para">
<p id="S2.SS2.p6.9" class="ltx_p">From this collection, we extracted <math id="S2.SS2.p6.1.m1.1" class="ltx_Math" alttext="161" display="inline"><semantics id="S2.SS2.p6.1.m1.1a"><mn id="S2.SS2.p6.1.m1.1.1" xref="S2.SS2.p6.1.m1.1.1.cmml">161</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.1.m1.1b"><cn type="integer" id="S2.SS2.p6.1.m1.1.1.cmml" xref="S2.SS2.p6.1.m1.1.1">161</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.1.m1.1c">161</annotation></semantics></math> tiles belonging to the North-East coast of the USA (see <a href="#S2.F4" title="Figure 4 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>) that were split into <math id="S2.SS2.p6.2.m2.1" class="ltx_Math" alttext="10^{\prime}659" display="inline"><semantics id="S2.SS2.p6.2.m2.1a"><mrow id="S2.SS2.p6.2.m2.1.1" xref="S2.SS2.p6.2.m2.1.1.cmml"><msup id="S2.SS2.p6.2.m2.1.1.2" xref="S2.SS2.p6.2.m2.1.1.2.cmml"><mn id="S2.SS2.p6.2.m2.1.1.2.2" xref="S2.SS2.p6.2.m2.1.1.2.2.cmml">10</mn><mo id="S2.SS2.p6.2.m2.1.1.2.3" xref="S2.SS2.p6.2.m2.1.1.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p6.2.m2.1.1.1" xref="S2.SS2.p6.2.m2.1.1.1.cmml">​</mo><mn id="S2.SS2.p6.2.m2.1.1.3" xref="S2.SS2.p6.2.m2.1.1.3.cmml">659</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.2.m2.1b"><apply id="S2.SS2.p6.2.m2.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1"><times id="S2.SS2.p6.2.m2.1.1.1.cmml" xref="S2.SS2.p6.2.m2.1.1.1"></times><apply id="S2.SS2.p6.2.m2.1.1.2.cmml" xref="S2.SS2.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p6.2.m2.1.1.2.1.cmml" xref="S2.SS2.p6.2.m2.1.1.2">superscript</csymbol><cn type="integer" id="S2.SS2.p6.2.m2.1.1.2.2.cmml" xref="S2.SS2.p6.2.m2.1.1.2.2">10</cn><ci id="S2.SS2.p6.2.m2.1.1.2.3.cmml" xref="S2.SS2.p6.2.m2.1.1.2.3">′</ci></apply><cn type="integer" id="S2.SS2.p6.2.m2.1.1.3.cmml" xref="S2.SS2.p6.2.m2.1.1.3">659</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.2.m2.1c">10^{\prime}659</annotation></semantics></math> images of size <math id="S2.SS2.p6.3.m3.1" class="ltx_Math" alttext="512\times 512" display="inline"><semantics id="S2.SS2.p6.3.m3.1a"><mrow id="S2.SS2.p6.3.m3.1.1" xref="S2.SS2.p6.3.m3.1.1.cmml"><mn id="S2.SS2.p6.3.m3.1.1.2" xref="S2.SS2.p6.3.m3.1.1.2.cmml">512</mn><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.p6.3.m3.1.1.1" xref="S2.SS2.p6.3.m3.1.1.1.cmml">×</mo><mn id="S2.SS2.p6.3.m3.1.1.3" xref="S2.SS2.p6.3.m3.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.3.m3.1b"><apply id="S2.SS2.p6.3.m3.1.1.cmml" xref="S2.SS2.p6.3.m3.1.1"><times id="S2.SS2.p6.3.m3.1.1.1.cmml" xref="S2.SS2.p6.3.m3.1.1.1"></times><cn type="integer" id="S2.SS2.p6.3.m3.1.1.2.cmml" xref="S2.SS2.p6.3.m3.1.1.2">512</cn><cn type="integer" id="S2.SS2.p6.3.m3.1.1.3.cmml" xref="S2.SS2.p6.3.m3.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.3.m3.1c">512\times 512</annotation></semantics></math> (each covering <math id="S2.SS2.p6.4.m4.1" class="ltx_Math" alttext="5898m^{2}" display="inline"><semantics id="S2.SS2.p6.4.m4.1a"><mrow id="S2.SS2.p6.4.m4.1.1" xref="S2.SS2.p6.4.m4.1.1.cmml"><mn id="S2.SS2.p6.4.m4.1.1.2" xref="S2.SS2.p6.4.m4.1.1.2.cmml">5898</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p6.4.m4.1.1.1" xref="S2.SS2.p6.4.m4.1.1.1.cmml">​</mo><msup id="S2.SS2.p6.4.m4.1.1.3" xref="S2.SS2.p6.4.m4.1.1.3.cmml"><mi id="S2.SS2.p6.4.m4.1.1.3.2" xref="S2.SS2.p6.4.m4.1.1.3.2.cmml">m</mi><mn id="S2.SS2.p6.4.m4.1.1.3.3" xref="S2.SS2.p6.4.m4.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.4.m4.1b"><apply id="S2.SS2.p6.4.m4.1.1.cmml" xref="S2.SS2.p6.4.m4.1.1"><times id="S2.SS2.p6.4.m4.1.1.1.cmml" xref="S2.SS2.p6.4.m4.1.1.1"></times><cn type="integer" id="S2.SS2.p6.4.m4.1.1.2.cmml" xref="S2.SS2.p6.4.m4.1.1.2">5898</cn><apply id="S2.SS2.p6.4.m4.1.1.3.cmml" xref="S2.SS2.p6.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p6.4.m4.1.1.3.1.cmml" xref="S2.SS2.p6.4.m4.1.1.3">superscript</csymbol><ci id="S2.SS2.p6.4.m4.1.1.3.2.cmml" xref="S2.SS2.p6.4.m4.1.1.3.2">𝑚</ci><cn type="integer" id="S2.SS2.p6.4.m4.1.1.3.3.cmml" xref="S2.SS2.p6.4.m4.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.4.m4.1c">5898m^{2}</annotation></semantics></math>). We constructed <math id="S2.SS2.p6.5.m5.1" class="ltx_Math" alttext="1^{\prime}066^{\prime}316" display="inline"><semantics id="S2.SS2.p6.5.m5.1a"><mrow id="S2.SS2.p6.5.m5.1.1" xref="S2.SS2.p6.5.m5.1.1.cmml"><msup id="S2.SS2.p6.5.m5.1.1.2" xref="S2.SS2.p6.5.m5.1.1.2.cmml"><mn id="S2.SS2.p6.5.m5.1.1.2.2" xref="S2.SS2.p6.5.m5.1.1.2.2.cmml">1</mn><mo id="S2.SS2.p6.5.m5.1.1.2.3" xref="S2.SS2.p6.5.m5.1.1.2.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p6.5.m5.1.1.1" xref="S2.SS2.p6.5.m5.1.1.1.cmml">​</mo><msup id="S2.SS2.p6.5.m5.1.1.3" xref="S2.SS2.p6.5.m5.1.1.3.cmml"><mn id="S2.SS2.p6.5.m5.1.1.3.2" xref="S2.SS2.p6.5.m5.1.1.3.2.cmml">066</mn><mo id="S2.SS2.p6.5.m5.1.1.3.3" xref="S2.SS2.p6.5.m5.1.1.3.3.cmml">′</mo></msup><mo lspace="0em" rspace="0em" id="S2.SS2.p6.5.m5.1.1.1a" xref="S2.SS2.p6.5.m5.1.1.1.cmml">​</mo><mn id="S2.SS2.p6.5.m5.1.1.4" xref="S2.SS2.p6.5.m5.1.1.4.cmml">316</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.5.m5.1b"><apply id="S2.SS2.p6.5.m5.1.1.cmml" xref="S2.SS2.p6.5.m5.1.1"><times id="S2.SS2.p6.5.m5.1.1.1.cmml" xref="S2.SS2.p6.5.m5.1.1.1"></times><apply id="S2.SS2.p6.5.m5.1.1.2.cmml" xref="S2.SS2.p6.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p6.5.m5.1.1.2.1.cmml" xref="S2.SS2.p6.5.m5.1.1.2">superscript</csymbol><cn type="integer" id="S2.SS2.p6.5.m5.1.1.2.2.cmml" xref="S2.SS2.p6.5.m5.1.1.2.2">1</cn><ci id="S2.SS2.p6.5.m5.1.1.2.3.cmml" xref="S2.SS2.p6.5.m5.1.1.2.3">′</ci></apply><apply id="S2.SS2.p6.5.m5.1.1.3.cmml" xref="S2.SS2.p6.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p6.5.m5.1.1.3.1.cmml" xref="S2.SS2.p6.5.m5.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS2.p6.5.m5.1.1.3.2.cmml" xref="S2.SS2.p6.5.m5.1.1.3.2">066</cn><ci id="S2.SS2.p6.5.m5.1.1.3.3.cmml" xref="S2.SS2.p6.5.m5.1.1.3.3">′</ci></apply><cn type="integer" id="S2.SS2.p6.5.m5.1.1.4.cmml" xref="S2.SS2.p6.5.m5.1.1.4">316</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.5.m5.1c">1^{\prime}066^{\prime}316</annotation></semantics></math> questions and answers following the methodology presented in <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>. We split the data in a training set (<math id="S2.SS2.p6.6.m6.1" class="ltx_Math" alttext="61.5\%" display="inline"><semantics id="S2.SS2.p6.6.m6.1a"><mrow id="S2.SS2.p6.6.m6.1.1" xref="S2.SS2.p6.6.m6.1.1.cmml"><mn id="S2.SS2.p6.6.m6.1.1.2" xref="S2.SS2.p6.6.m6.1.1.2.cmml">61.5</mn><mo id="S2.SS2.p6.6.m6.1.1.1" xref="S2.SS2.p6.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.6.m6.1b"><apply id="S2.SS2.p6.6.m6.1.1.cmml" xref="S2.SS2.p6.6.m6.1.1"><csymbol cd="latexml" id="S2.SS2.p6.6.m6.1.1.1.cmml" xref="S2.SS2.p6.6.m6.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.6.m6.1.1.2.cmml" xref="S2.SS2.p6.6.m6.1.1.2">61.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.6.m6.1c">61.5\%</annotation></semantics></math> of the tiles), a validation set (<math id="S2.SS2.p6.7.m7.1" class="ltx_Math" alttext="11.2\%" display="inline"><semantics id="S2.SS2.p6.7.m7.1a"><mrow id="S2.SS2.p6.7.m7.1.1" xref="S2.SS2.p6.7.m7.1.1.cmml"><mn id="S2.SS2.p6.7.m7.1.1.2" xref="S2.SS2.p6.7.m7.1.1.2.cmml">11.2</mn><mo id="S2.SS2.p6.7.m7.1.1.1" xref="S2.SS2.p6.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.7.m7.1b"><apply id="S2.SS2.p6.7.m7.1.1.cmml" xref="S2.SS2.p6.7.m7.1.1"><csymbol cd="latexml" id="S2.SS2.p6.7.m7.1.1.1.cmml" xref="S2.SS2.p6.7.m7.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.7.m7.1.1.2.cmml" xref="S2.SS2.p6.7.m7.1.1.2">11.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.7.m7.1c">11.2\%</annotation></semantics></math>), and test sets (<math id="S2.SS2.p6.8.m8.1" class="ltx_Math" alttext="20.5\%" display="inline"><semantics id="S2.SS2.p6.8.m8.1a"><mrow id="S2.SS2.p6.8.m8.1.1" xref="S2.SS2.p6.8.m8.1.1.cmml"><mn id="S2.SS2.p6.8.m8.1.1.2" xref="S2.SS2.p6.8.m8.1.1.2.cmml">20.5</mn><mo id="S2.SS2.p6.8.m8.1.1.1" xref="S2.SS2.p6.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.8.m8.1b"><apply id="S2.SS2.p6.8.m8.1.1.cmml" xref="S2.SS2.p6.8.m8.1.1"><csymbol cd="latexml" id="S2.SS2.p6.8.m8.1.1.1.cmml" xref="S2.SS2.p6.8.m8.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.8.m8.1.1.2.cmml" xref="S2.SS2.p6.8.m8.1.1.2">20.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.8.m8.1c">20.5\%</annotation></semantics></math> for test set 1, <math id="S2.SS2.p6.9.m9.1" class="ltx_Math" alttext="6.8\%" display="inline"><semantics id="S2.SS2.p6.9.m9.1a"><mrow id="S2.SS2.p6.9.m9.1.1" xref="S2.SS2.p6.9.m9.1.1.cmml"><mn id="S2.SS2.p6.9.m9.1.1.2" xref="S2.SS2.p6.9.m9.1.1.2.cmml">6.8</mn><mo id="S2.SS2.p6.9.m9.1.1.1" xref="S2.SS2.p6.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p6.9.m9.1b"><apply id="S2.SS2.p6.9.m9.1.1.cmml" xref="S2.SS2.p6.9.m9.1.1"><csymbol cd="latexml" id="S2.SS2.p6.9.m9.1.1.1.cmml" xref="S2.SS2.p6.9.m9.1.1.1">percent</csymbol><cn type="float" id="S2.SS2.p6.9.m9.1.1.2.cmml" xref="S2.SS2.p6.9.m9.1.1.2">6.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p6.9.m9.1c">6.8\%</annotation></semantics></math> for test set 2). As it can be seen in <a href="#S2.F4" title="Figure 4 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, test set 1 covers similar regions as the training and validation sets, while test set 2 covers the city of Philadelphia, which is not seen during the training. Note that this second test set also uses another sensor (marked as unknown on the USGS data catalog), not seen during training.</p>
</div>
<figure id="S2.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x4.png" id="S2.F5.sf1.g1" class="ltx_graphics ltx_img_square" width="406" height="333" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Distribution of answers for the LR dataset</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S2.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x5.png" id="S2.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="350" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Distribution of answers for the HR dataset (numerical answers are ordered, and 0 is the most frequent)</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Distributions of answers in the Low resolution (LR) and High resolution (HR) datasets.</figcaption>
</figure>
<div id="S2.SS2.p7" class="ltx_para ltx_noindent">
<p id="S2.SS2.p7.1" class="ltx_p"><span id="S2.SS2.p7.1.1" class="ltx_text ltx_font_bold">Differences between the two datasets</span>:
<br class="ltx_break">Due to their characteristics, the two datasets represent two different possible use cases of VQA:</p>
<ul id="S2.I4" class="ltx_itemize">
<li id="S2.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i1.p1" class="ltx_para">
<p id="S2.I4.i1.p1.1" class="ltx_p">The LR dataset allows for large spatial and temporal coverage thanks to the frequent acquisitions made by Sentinel-2. This characteristic could be of interest for future applications of VQA such as large scale queries (e.g. rural/urban questions) or temporal (which is out of the scope of this study). However, due to the relatively low resolution (10m), some objects can not be seen on such images (such as small houses, roads, trees, …). This fact severely limits the questions to which the model could give an accurate answer.</p>
</div>
</li>
<li id="S2.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I4.i2.p1" class="ltx_para">
<p id="S2.I4.i2.p1.1" class="ltx_p">Thanks to the much finer resolution of the HR dataset, a quantity of information of interest to answer typical questions is present. Therefore, in contrast to the LR dataset, questions concerning objects’ coverage or counting relatively small objects can possibly be answered from such data. However, data of such resolution is generally less frequently updated and more expensive to acquire.</p>
</div>
</li>
</ul>
</div>
<div id="S2.SS2.p8" class="ltx_para">
<p id="S2.SS2.p8.1" class="ltx_p">Based on these differences, we constructed different types of questions for the two datasets. Questions concerning the area of objects are only asked in the HR dataset. On the other hand, questions about urban/rural area classification are only asked in the LR dataset, as the level of zoom of images from the HR dataset would prevent a meaningful answer from being provided.</p>
</div>
<figure id="S2.F6" class="ltx_figure"><img src="/html/2003.07333/assets/Images/count_distribution_LR_log_New.png" id="S2.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Frequencies of exact counting answers in the LR dataset. Only the left part of the histogram is shown (until 200 objects), the largest (single) count being 17139. <math id="S2.F6.2.m1.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S2.F6.2.m1.1b"><mrow id="S2.F6.2.m1.1.1" xref="S2.F6.2.m1.1.1.cmml"><mn id="S2.F6.2.m1.1.1.2" xref="S2.F6.2.m1.1.1.2.cmml">50</mn><mo id="S2.F6.2.m1.1.1.1" xref="S2.F6.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.F6.2.m1.1c"><apply id="S2.F6.2.m1.1.1.cmml" xref="S2.F6.2.m1.1.1"><csymbol cd="latexml" id="S2.F6.2.m1.1.1.1.cmml" xref="S2.F6.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S2.F6.2.m1.1.1.2.cmml" xref="S2.F6.2.m1.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F6.2.m1.1d">50\%</annotation></semantics></math> of the answers are less than 7 objects in the tile.</figcaption>
</figure>
<div id="S2.SS2.p9" class="ltx_para">
<p id="S2.SS2.p9.1" class="ltx_p">To account for the data distributions and error margins we also quantize different answers in both datasets:</p>
<ul id="S2.I5" class="ltx_itemize">
<li id="S2.I5.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i1.p1" class="ltx_para">
<p id="S2.I5.i1.p1.1" class="ltx_p">Counting in LR: as the coverage is relatively large (6.55km<sup id="S2.I5.i1.p1.1.1" class="ltx_sup">2</sup>), the number of small objects contained in one tile can be high, giving a heavy tailed distribution for the numerical answers, as shown in <a href="#S2.F6" title="Figure 6 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>. More precisely, while 26.7% of the numerical answers are ’0’ and 50% of the answers are less than ’7’, the highest numerical answer goes up to ’17139’. In addition to making the problem complex, we can argue that allowing such a range of numerical answer does not make sense on data of this resolution. Indeed, it would be in most cases impossible to distinguish 17139 objects on an image of 65536 pixels. Therefore, numerical answers are quantized into the following categories:</p>
<ul id="S2.I5.i1.I1" class="ltx_itemize">
<li id="S2.I5.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I5.i1.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I5.i1.I1.i1.p1" class="ltx_para">
<p id="S2.I5.i1.I1.i1.p1.1" class="ltx_p">’0’;</p>
</div>
</li>
<li id="S2.I5.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I5.i1.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I5.i1.I1.i2.p1" class="ltx_para">
<p id="S2.I5.i1.I1.i2.p1.1" class="ltx_p">’between 1 and 10’;</p>
</div>
</li>
<li id="S2.I5.i1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I5.i1.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I5.i1.I1.i3.p1" class="ltx_para">
<p id="S2.I5.i1.I1.i3.p1.1" class="ltx_p">’between 11 and 100’;</p>
</div>
</li>
<li id="S2.I5.i1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I5.i1.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I5.i1.I1.i4.p1" class="ltx_para">
<p id="S2.I5.i1.I1.i4.p1.1" class="ltx_p">’between 101 and 1000’;</p>
</div>
</li>
<li id="S2.I5.i1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S2.I5.i1.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S2.I5.i1.I1.i5.p1" class="ltx_para">
<p id="S2.I5.i1.I1.i5.p1.1" class="ltx_p">’more than 1000’.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S2.I5.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I5.i2.p1" class="ltx_para">
<p id="S2.I5.i2.p1.1" class="ltx_p">In a similar manner, we quantize questions regarding the area in the HR dataset. A great majority (60.9%) of the answer of this type are ’0m<sup id="S2.I5.i2.p1.1.1" class="ltx_sup">2</sup>’, while the distribution also presents a heavy tail. Therefore, we use the same quantization as the one proposed for counts for the LR dataset. Note that we do not quantize purely numerical answers (i.e. answers to questions of type ’count’) as the maximum number of objects is 89 in our dataset. Counting answers therefore correspond to 89 classes in the model in this case (see <a href="#S3" title="III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section III</span></a>).</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Discussion</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p"><span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_bold">Questions/Answers distributions</span>:
<br class="ltx_break">We show the final distribution of answers per question type for both datasets in Figure <a href="#S2.F5" title="Figure 5 ‣ II-B Data ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. We can see that most question types (with the exception of ’rural/urban’ questions in the LR dataset, asked only once per image) are close to evenly distributed by construction. The answer ’no’ is dominating the answers’ distribution for the HR dataset with a frequency of 37.7%. In the LR dataset, the answer ’yes’ occurs 34.9% of the time while the ’no’ frequency is 34.3%. The strongest imbalance occurs for the answer ’0’ in the HR dataset (with a frequency of 60.9% for the numerical answer). This imbalance is greatly reduced by the quantization process described in the previous paragraph.
<br class="ltx_break"></p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">Limitations of the proposed method</span>:
<br class="ltx_break">While the proposed method for image/question/answer triplets generation has the advantage of being automatic and easily scalable while using data annotated by humans, a few limitations have been observed. First, it can happen that some annotations are missing or badly registered <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Furthermore, it was not possible to match the acquisition date of the imagery to the one of OSM. The main reason being that it is impossible to know if a newly added element appeared at the same time in reality or if it was just entered for the first time in OSM. As OSM is the main source of data for our process, errors in OSM will negatively impact the accuracy of our databases.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Furthermore, due to the templates used to automatically construct questions and provide answers, the set of questions and answers is more limited than what it is in traditional VQA datasets (9 possible answers for the LR dataset, 98 for the HR dataset).</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">VQA Model</span>
</h2>

<figure id="S3.F7" class="ltx_figure"><img src="/html/2003.07333/assets/x6.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Framework of the proposed Visual Question Answering model.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We investigate the difficulty of the VQA task for remote sensing using a basic VQA model based on deep learning. An illustration of the proposed network is shown in <a href="#S3.F7" title="Figure 7 ‣ III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>. In their simple form, VQA models are composed of three parts <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A.</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p">feature extraction;</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">B.</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p">fusion of these features to obtain a single feature vector representing both the visual information and the question;</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C.</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p">prediction based on this vector.</p>
</div>
</li>
</ul>
<p id="S3.p1.2" class="ltx_p">As the model shown in <a href="#S3.F7" title="Figure 7 ‣ III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a> is learned end-to-end, the vector obtained after the fusion (in green in <a href="#S3.F7" title="Figure 7 ‣ III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>) can be seen as a joint embedding of both the image and the question which is used as an input for the prediction step. We detail each of these 3 parts in the following.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Feature extraction</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The first component of our VQA model is the feature extraction. Its purpose is to obtain a low-dimensional representation of the information contained in the image and the question.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>Visual part</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">To extract information from a 2D image, a common choice is to use a Convolutional Neural Network (CNN).
Specifically, we use a Resnet-152 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. The principal motivation for this choice is that this architecture manages to avoid the undesirable <em id="S3.SS1.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">degradation</em> problem (decreasing performance with deeper networks) by using residual mappings of the layers’ inputs which are easier to learn than the common choice of direct mappings. This architecture has been succesfully used in a wide range of work in the remote sensing community (e.g. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>). The last average pooling layer and fully connected layer are replaced by a <math id="S3.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S3.SS1.SSS1.p1.1.m1.1a"><mrow id="S3.SS1.SSS1.p1.1.m1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS1.p1.1.m1.1.1.2" xref="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p1.1.m1.1.1.1" xref="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS1.SSS1.p1.1.m1.1.1.3" xref="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p1.1.m1.1b"><apply id="S3.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1"><times id="S3.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S3.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p1.1.m1.1c">1\times 1</annotation></semantics></math> 2D convolution which outputs a total of 2048 features which are vectorized. A final fully connected layer is learned to obtain a 1200 dimension vector.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Language part</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">The feature vector is obtained using the skip-thoughts model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> trained on the BookCorpus dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. This model is a recurrent neural network, which aims at producing a vector representing a sequence of words (in our case, a question). To make this vector informative, the model is trained in the following way: it encodes a sentence from a book in a latent space, and tries to decode it to obtain the two adjacent sentences in the book. By doing so, it ensures that the latent space embeds semantic information. Note that this semantic information is not remote sensing specific due to the BookCorpus dataset it has been trained on. However, several works, including <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, have successfully applied non-domain specific NLP models to remote sensing. In our model, we use the encoder which is then followed by a fully-connected layer (from size 2400 elements to 1200).</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Fusion</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">At this step, we have two feature vectors (one representing the image, one representing the question) of the same size. To merge them into a single vector, we use a simple strategy: a point-wise multiplication after applying the hyperbolic tangent function to the vectors’ elements. While being a fixed (i.e. not learnt) operation, the end-to-end training of our model encourages both feature vectors to be comparable with respect to this operation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Prediction</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Finally, we project this 1200 dimensional vector to the answer space by using a MLP with one hidden layer of 256 elements. We formulate the problem as a classification task, in which each possible answer is a class. Therefore, the size of the output vector depends on the number of possible answers.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.5.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.6.2" class="ltx_text ltx_font_italic">Training procedure</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We train the model using the Adam optimizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> with a learning rate of <math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><msup id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mn id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml"><mo id="S3.SS4.p1.1.m1.1.1.3a" xref="S3.SS4.p1.1.m1.1.1.3.cmml">−</mo><mn id="S3.SS4.p1.1.m1.1.1.3.2" xref="S3.SS4.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">10</cn><apply id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3"><minus id="S3.SS4.p1.1.m1.1.1.3.1.cmml" xref="S3.SS4.p1.1.m1.1.1.3"></minus><cn type="integer" id="S3.SS4.p1.1.m1.1.1.3.2.cmml" xref="S3.SS4.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">10^{-5}</annotation></semantics></math> until convergence (150 epochs in the case of the LR dataset, and 35 epochs in the case of the HR dataset). We use a dropout of 0.5 for every fully connected layer. Due to the difference of input size between the two datasets (HR images are 4 times larger), we use batches of 70 instances for the HR dataset and 280 for the LR dataset. Furthermore, when the questions do not contain a positional component relative to the image space (i.e. ”left of”, ”top of”, ”right of” or ”bottom of”, see <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>), we augment the image space by randomly applying vertical and/or horizontal flipping</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and discussion</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We report the results obtained by our model on the test sets of the LR and HR datasets. In both cases, 3 model runs have been trained and we report both the average and the standard deviation of our results to limit the variability coming from the stochastic nature of the optimization.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The numerical evaluation is achieved using the accuracy, defined in our case as the ratio of correct answers. We report the accuracy per question type (see <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>), the average of these accuracies (AA) and the overall accuracy (OA).
<br class="ltx_break"></p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">We show some predictions of the model on the different test sets in <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a> and <a href="#S4.F9" title="Figure 9 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a> to qualitatively assess the results. Numerical performance of the proposed model on the LR dataset is reported in <a href="#S4.T2" title="TABLE II ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table II</span></a> and the confusion matrix is shown in <a href="#S4.F10" title="Figure 10 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 10</span></a>. The performance on both tests sets of the HR dataset are reported in <a href="#S4.T3" title="TABLE III ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table III</span></a> and the confusion matrices are shown in <a href="#S4.F11" title="Figure 11 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>.
<br class="ltx_break"></p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Results on the test set of the low resolution dataset. The standard deviation is reported in brackets.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Type</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Count</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">67.01% (0.59%)</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Presence</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">87.46% (0.06%)</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Comparison</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">81.50% (0.03%)</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Rural/Urban</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">90.00% (1.41%)</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AA</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">81.49% (0.49%)</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">OA</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">79.08% (0.20%)</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Results on both test sets of the high resolution dataset. The standard deviation is reported in brackets.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Type</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Accuracy</th>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r"></th>
<th id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Test set 1</th>
<th id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r">Test set 2</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.3.1" class="ltx_tr">
<td id="S4.T3.1.3.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Count</td>
<td id="S4.T3.1.3.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.63% (0.11%)</td>
<td id="S4.T3.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">61.47% (0.08%)</td>
</tr>
<tr id="S4.T3.1.4.2" class="ltx_tr">
<td id="S4.T3.1.4.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Presence</td>
<td id="S4.T3.1.4.2.2" class="ltx_td ltx_align_center ltx_border_r">90.43% (0.04%)</td>
<td id="S4.T3.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r">86.26% (0.47%)</td>
</tr>
<tr id="S4.T3.1.5.3" class="ltx_tr">
<td id="S4.T3.1.5.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Comparison</td>
<td id="S4.T3.1.5.3.2" class="ltx_td ltx_align_center ltx_border_r">88.19% (0.08%)</td>
<td id="S4.T3.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r">85.94% (0.12%)</td>
</tr>
<tr id="S4.T3.1.6.4" class="ltx_tr">
<td id="S4.T3.1.6.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Area</td>
<td id="S4.T3.1.6.4.2" class="ltx_td ltx_align_center ltx_border_r">85.24% (0.05%)</td>
<td id="S4.T3.1.6.4.3" class="ltx_td ltx_align_center ltx_border_r">76.33% (0.50%)</td>
</tr>
<tr id="S4.T3.1.7.5" class="ltx_tr">
<td id="S4.T3.1.7.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">AA</td>
<td id="S4.T3.1.7.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">83.12% (0.03%)</td>
<td id="S4.T3.1.7.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.50% (0.29%)</td>
</tr>
<tr id="S4.T3.1.8.6" class="ltx_tr">
<td id="S4.T3.1.8.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">OA</td>
<td id="S4.T3.1.8.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">83.23% (0.02%)</td>
<td id="S4.T3.1.8.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">78.23% (0.25%)</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2003.07333/assets/x7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="401" height="581" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Samples from the high resolution test sets: (a)-(f) are from the first set of the HR dataset, (g)-(i) are from the second set of the HR dataset.</figcaption>
</figure>
<figure id="S4.F9" class="ltx_figure"><img src="/html/2003.07333/assets/x8.png" id="S4.F9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="664" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Samples from the low resolution test set.</figcaption>
</figure>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">General accuracy assessment:
<br class="ltx_break"></span>The proposed model achieves an overall accuracy of 79% on the low resolution dataset (see <a href="#S4.T2" title="TABLE II ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table II</span></a>) and of 83% on the first test set of the high resolution dataset (<a href="#S4.T3" title="TABLE III ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table III</span></a>), indicating that the task of automatically answering question based on remote sensing images is possible. When looking at the accuracies per question type (in Tables <a href="#S4.T2" title="TABLE II ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="TABLE III ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>), it can be noted that the model performs inconsistently with respect to the task the question is tackling: while a question about the presence of an object is generally well answered (87.46% in the LR dataset, 90.43% in the first test set of the HR dataset), counting questions gives poorer performances (67.01% and 68.63% respectively). This can be explained by the fact that presence questions can be seen as simplified counting questions to which the answers are restricted to two options: ”0” or ”1 or more”. Classical VQA models are known to struggle with the counting task <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. An issue which partly explains these performances in the counting task is the separation of connected instances. This problem has been raised for the case of buildings in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> and is illustrated in <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(f), where the ground truth is indicating three buildings, which could also be only one. We found another illustration of this phenomenon in the second test set in <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(i). This issue mostly arises when counting roads or buildings.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">Thanks to the answers’ quantization, questions regarding the areas of objects are generally well answered with an accuracy of 85.24% in the first test set of the HR dataset. This is illustrated in Figures <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(a,b), where presence of buildings (by the mean of the covered area) is well detected.</p>
</div>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">However, we found that our model performs poorly with questions regarding the relative positions of objects, such as those illustrated in Figures <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(c-e). While <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(c) is correct, despite the question being difficult, <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(d) shows a small mistake from the model and <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(e) is completely incorrect. These problems can be explained by the fact that the questions are on high semantic level and therefore difficult for a model considering a simple fusion scheme, as the one presented in <a href="#S3" title="III VQA Model ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section III</span></a>.
<br class="ltx_break"></p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Regarding the low resolution dataset, rural/urban questions are generally well answered (90% of accuracy), as shown in <a href="#S4.F9" title="Figure 9 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>(a,b). Note that the ground truth for this type of questions is defined as a hard threshold on the number of buildings, which causes an area as the one shown in <a href="#S4.F9" title="Figure 9 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>(b) to be labeled as urban.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">However, the low resolution of Sentinel-2 images can be problematic when answering questions about relatively small objects. For instance, in Figures <a href="#S4.F9" title="Figure 9 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>(c,d), we can not see any water area nor determine the type of buildings, which causes the model’s answer to be unreliable.
<br class="ltx_break"></p>
</div>
<div id="S4.p9" class="ltx_para ltx_noindent">
<p id="S4.p9.1" class="ltx_p"><span id="S4.p9.1.1" class="ltx_text ltx_font_bold">Generalization to unseen areas:
<br class="ltx_break"></span>The performances on the second test set of the HR dataset show that the generalization to new geographic areas is problematic for the model, with an accuracy drop of approximately 5%. This new domain has a stronger impact on the most difficult tasks (counting and area computation). This can be explained when looking at Figures <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>(g-i). We can see that the domain shift is important on the image space, as a different sensor was used for the acquisition. Furthermore, the urban organization of Philadelphia is different from that of the city of New York. This causes the buildings to go undetected by the model in <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(h), while the parkings can still be detected in <a href="#S4.F8" title="Figure 8 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>(g) possibly thanks to the cars. This decrease in performance could be reduced by using domain adaptation techniques. Such a method could be developed for the image space only (a review of domain adaptation for remote sensing is done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>) or at the question/image level (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, which presents a method for domain adaptation in the context of VQA).
<br class="ltx_break"></p>
</div>
<div id="S4.p10" class="ltx_para ltx_noindent">
<p id="S4.p10.1" class="ltx_p"><span id="S4.p10.1.1" class="ltx_text ltx_font_bold">Answer’s categories:
<br class="ltx_break"></span>The confusion matrices indicate that the models generally provide logical answers, even when making mistakes (e.g. it might answer ”yes” instead of ”no” to a question about the presence of an object, but not a number). Rare exceptions to this are observed for the first test set of the HR dataset (see <a href="#S4.F11" title="Figure 11 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>(a)), on which the model gives 23 illogical answers (out of the 316941 questions of this test set).
<br class="ltx_break"></p>
</div>
<figure id="S4.F10" class="ltx_figure"><img src="/html/2003.07333/assets/x9.png" id="S4.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Confusion matrix for the low resolution dataset (logarithm scale) on the test set. Red lines group answers by type (”Yes/No”, ”Rural/Urban”, numbers).</figcaption>
</figure>
<figure id="S4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x10.png" id="S4.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="323" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Test set 1</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2003.07333/assets/x11.png" id="S4.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="415" height="323" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Test set 2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Subsets of the confusion matrices for the high resolution dataset (counts are at logarithm scale) on both test sets. Red lines group answers by type (”Yes/No”, areas, numbers).</figcaption>
</figure>
<div id="S4.p11" class="ltx_para ltx_noindent">
<p id="S4.p11.1" class="ltx_p"><span id="S4.p11.1.1" class="ltx_text ltx_font_bold">Language biases:
<br class="ltx_break"></span>A common issue in VQA models, raised in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, is the fact that strong language biases are captured by the model. When this is the case, the answer provided by the model mostly depends on the question, rather than on the image. To assess this, we evaluated the proposed models by randomly selecting an image from the test set for each question. We obtained an overall accuracy of 73.78% on the LR test set, 73.78% on the first test set of the HR dataset and 72.51% on the second test set. This small drop of accuracy indicates that indeed, the models rely more on the questions than on the image to provide an answer. Furthermore, the strongest drop of accuracy is seen on the HR dataset, indicating that the proposed model extracts more information from the high resolution data.</p>
</div>
<figure id="S4.F12" class="ltx_figure"><img src="/html/2003.07333/assets/x12.png" id="S4.F12.g1" class="ltx_graphics ltx_centering ltx_img_square" width="323" height="280" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Evolution of the accuracies (evaluated on the first HR test set) after training with subsets of different size of the HR training set.</figcaption>
</figure>
<div id="S4.p12" class="ltx_para ltx_noindent">
<p id="S4.p12.1" class="ltx_p"><span id="S4.p12.1.1" class="ltx_text ltx_font_bold">Importance of the number of training samples:
<br class="ltx_break"></span>We show in <a href="#S4.F12" title="Figure 12 ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Figure 12</span></a> the evolution of the accuracies when the model is trained with a fraction of the HR training samples. When using only 1% of the available training samples, the model already gets 65% in average accuracy (vs 83% for the model trained on the whole training set). However, it can be seen that, for numerical tasks (counts and area estimation), larger amounts of samples are needed to achieve the performances reported in <a href="#S4.T3" title="TABLE III ‣ IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table III</span></a>. This experiment also shows that the performances start to plateau after 10% of the training data is used: this indicates that the proposed model would not profit substantially from a larger dataset.</p>
</div>
<div id="S4.p13" class="ltx_para ltx_noindent">
<p id="S4.p13.1" class="ltx_p"><span id="S4.p13.1.1" class="ltx_text ltx_font_bold">Restricted set of questions:
<br class="ltx_break"></span>While not appearing in the numerical evaluation, an important issue with our results is the relative lack of diversity in the dataset. Indeed, due to the source of our data (OSM), the questions are only on a specific set of static objects (e.g. buildings, roads, …). Other objects of interest for applications of a VQA system to remote sensing would also include different static objects (e.g. thatched roofs mentioned in <a href="#S1" title="I Introduction ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section I</span></a>), moving objects (e.g. cars), or seasonal aspects (e.g. for crop monitoring). Including these objects would require another source of data, or manual construction of question/answer pairs.</p>
</div>
<div id="S4.p14" class="ltx_para">
<p id="S4.p14.1" class="ltx_p">Another limitation comes from the dataset construction method described in <a href="#S2.SS1" title="II-A Method ‣ II Datasets ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">subsection <span class="ltx_text">II-A</span></span></a>. We defined five types of questions (count, comparison, presence, area, rural/urban classification). However, they only start to cover the range of questions which would be of interest. For instance, questions about the distance between two points (defined by textual descriptions), segmentation questions (e.g. ”where are the buildings in this image?”) or higher semantic level question (e.g. ”does this area feel safe?”) could be added.
<br class="ltx_break"></p>
</div>
<div id="S4.p15" class="ltx_para">
<p id="S4.p15.1" class="ltx_p">While the first limitation (due to the data source) could be tackled using other databases (e.g. from national institutes) and the second limitation (due to the proposed method) could be solved by adding other question construction functions to the model, it would be beneficial to use human annotators using a procedure similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> to diversify the samples.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We introduce the task of Visual Question Answering from remote sensing images as a generic and accessible way of extracting information from remotely sensed data. We present a method for building datasets for VQA, which can be extended and adapted to different data sources, and we proposed two datasets targeting different applications. The first dataset uses Sentinel-2 images, while the second dataset uses very high resolution (30cm) aerial orthophotos from USGS.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We analyze these datasets using a model based on deep learning, using both convolutional and recurrent neural networks to analyze the images and associated questions. The most probable answer from a predefined set is then selected.
<br class="ltx_break"></p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">This first analysis shows promising results, suggesting the potential for future applications of such systems. These results outline future research directions which are needed to overcome language biases and difficult tasks such as counting. The former can be tackled using an attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, while the latter could be tackled by using dedicated components for counting questions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> in a modular approach.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Issues regarding the current database raised in <a href="#S4" title="IV Results and discussion ‣ RSVQA: Visual Question Answering for Remote Sensing Data" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">section IV</span></a> also need to be addressed to obtain a system capable of answering a more realistic range of questions. This can be done by making the proposed dataset construction method more complex or by using human annotators.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors would like to thank CNES for the funding of this study (R&amp;T project ”Application des techniques de Visual Question Answering à des données d’imagerie satellitaire”).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
K. Anderson, B. Ryan, W. Sonntag, A. Kavvada, and L. Friedl, “Earth
observation in service of the 2030 agenda for sustainable development,”
</span><em id="bib.bib1.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Geo-spatial Information Science</em><span id="bib.bib1.3.3" class="ltx_text" style="font-size:90%;">, vol. 20, no. 2, pp. 77–96, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Y. Gu, J. Chanussot, X. Jia, and J. A. Benediktsson, “Multiple kernel
learning for hyperspectral image classification: A review,” </span><em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
Transactions on Geoscience and Remote Sensing</em><span id="bib.bib2.3.3" class="ltx_text" style="font-size:90%;">, vol. 55, no. 11, pp.
6547–6565, Nov 2017.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
S. Li, W. Song, L. Fang, Y. Chen, P. Ghamisi, and J. A.
Benediktsson, “Deep learning for hyperspectral image classification: An
overview,” </span><em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</em><span id="bib.bib3.3.3" class="ltx_text" style="font-size:90%;">, pp.
1–20, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
J. E. Vargas-Muñoz, S. Lobry, A. X. Falcão, and D. Tuia, “Correcting
rural building annotations in openstreetmap using convolutional neural
networks,” </span><em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ISPRS Journal of Photogrammetry and Remote Sensing</em><span id="bib.bib4.3.3" class="ltx_text" style="font-size:90%;">, vol.
147, pp. 283–293, 2019.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: Visual Question Answering,” in </span><em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International
Conference on Computer Vision</em><span id="bib.bib5.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
S. Lobry, J. Murray, D. Marcos, and D. Tuia, “Visual question
answering from remote sensing images,” in </span><em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International
Geoscience and Remote Sensing Symposium</em><span id="bib.bib6.3.3" class="ltx_text" style="font-size:90%;">, July 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Z. Shi and Z. Zou, “Can a machine generate humanlike language descriptions
for a remote sensing image?” </span><em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and
Remote Sensing</em><span id="bib.bib7.3.3" class="ltx_text" style="font-size:90%;">, vol. 55, no. 6, pp. 3623–3634, June 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
X. X. Zhu, D. Tuia, L. Mou, G. Xia, L. Zhang, F. Xu, and
F. Fraundorfer, “Deep learning in remote sensing: A comprehensive review
and list of resources,” </span><em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Magazine</em><span id="bib.bib8.3.3" class="ltx_text" style="font-size:90%;">,
vol. 5, no. 4, pp. 8–36, Dec 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
F. Hu, G.-S. Xia, J. Hu, and L. Zhang, “Transferring deep convolutional neural
networks for the scene classification of high-resolution remote sensing
imagery,” </span><em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote Sensing</em><span id="bib.bib9.3.3" class="ltx_text" style="font-size:90%;">, vol. 7, no. 11, pp. 14 680–14 707, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A
Large-Scale Hierarchical Image Database,” in </span><em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on
Computer Vision and Pattern Recognition</em><span id="bib.bib10.3.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Q. Wang, S. Liu, J. Chanussot, and X. Li, “Scene classification with
recurrent attention of VHR remote sensing images,” </span><em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions
on Geoscience and Remote Sensing</em><span id="bib.bib11.3.3" class="ltx_text" style="font-size:90%;">, vol. 57, no. 2, pp. 1155–1167, Feb 2019.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
and L. Zhang, “DOTA: A large-scale dataset for object detection in aerial
images,” in </span><em id="bib.bib12.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib12.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 3974–3983.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">
Q. Li, L. Mou, Q. Xu, Y. Zhang, and X. X. Zhu, “R</span><sup id="bib.bib13.3.2" class="ltx_sup"><span id="bib.bib13.3.2.1" class="ltx_text" style="font-size:90%;">3</span></sup><span id="bib.bib13.4.3" class="ltx_text" style="font-size:90%;">-Net: A deep
network for multioriented vehicle detection in aerial images and videos,”
</span><em id="bib.bib13.5.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</em><span id="bib.bib13.6.5" class="ltx_text" style="font-size:90%;">, pp. 1–15, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
M. Volpi and D. Tuia, “Dense semantic labeling of subdecimeter resolution
images with convolutional neural networks,” </span><em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on
Geoscience and Remote Sensing</em><span id="bib.bib14.3.3" class="ltx_text" style="font-size:90%;">, vol. 55, no. 2, pp. 881–893, 2016.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
E. Maggiori, Y. Tarabalka, G. Charpiat, and P. Alliez, “Can semantic labeling
methods generalize to any city? the INRIA aerial image labeling
benchmark,” in </span><em id="bib.bib15.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Geoscience and Remote Sensing
Symposium</em><span id="bib.bib15.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2017, pp.
3226–3229.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
B. Huang, K. Lu, N. Audeberr, A. Khalel, Y. Tarabalka, J. Malof, A. Boulch,
B. Le Saux, L. Collins, K. Bradbury </span><em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib16.3.3" class="ltx_text" style="font-size:90%;">, “Large-scale semantic
classification: outcome of the first year of INRIA aerial image labeling
benchmark,” in </span><em id="bib.bib16.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Geoscience and Remote Sensing
Symposium</em><span id="bib.bib16.5.5" class="ltx_text" style="font-size:90%;">.   IEEE, 2018, pp.
6947–6950.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
I. Demir, K. Koperski, D. Lindenbaum, G. Pang, J. Huang, S. Basu, F. Hughes,
D. Tuia, and R. Raskar, “Deepglobe 2018: A challenge to parse the earth
through satellite images,” in </span><em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision
and Pattern Recognition Workshops</em><span id="bib.bib17.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
L. Zhou, C. Zhang, and M. Wu, “D-linknet: Linknet with pretrained encoder and
dilated convolution for high resolution satellite imagery road extraction.”
in </span><em id="bib.bib18.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern Recognition
Workshops</em><span id="bib.bib18.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 182–186.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
R. Hamaguchi and S. Hikosaka, “Building detection from satellite imagery using
ensemble of size-specific detectors,” in </span><em id="bib.bib19.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">2018 IEEE/CVF Conference on
Computer Vision and Pattern Recognition Workshops (CVPRW)</em><span id="bib.bib19.3.3" class="ltx_text" style="font-size:90%;">.   IEEE, 2018, pp. 223–2234.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
C. Tian, C. Li, and J. Shi, “Dense fusion classmate network for land cover
classification.” in </span><em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops</em><span id="bib.bib20.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 192–196.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
X. Zhang, X. Li, J. An, L. Gao, B. Hou, and C. Li, “Natural
language description of remote sensing images based on deep learning,” in
</span><em id="bib.bib21.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE International Geoscience and Remote Sensing Symposium</em><span id="bib.bib21.3.3" class="ltx_text" style="font-size:90%;">, July 2017,
pp. 4798–4801.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
X. Zhang, X. Wang, X. Tang, H. Zhou, and C. Li, “Description generation for
remote sensing images using attribute attention mechanism,” </span><em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Remote
Sensing</em><span id="bib.bib22.3.3" class="ltx_text" style="font-size:90%;">, vol. 11, no. 6, p. 612, 2019.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
B. Wang, X. Lu, X. Zheng, and X. Li, “Semantic descriptions of high-resolution
remote sensing images,” </span><em id="bib.bib23.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Letters</em><span id="bib.bib23.3.3" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Q. Wu, D. Teney, P. Wang, C. Shen, A. Dick, and A. v.d. Hengel, “Visual
question answering: A survey of methods and datasets,” </span><em id="bib.bib24.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Computer
Vision and Image Understanding</em><span id="bib.bib24.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
A. Fukui, D. H. Park, D. Yang, A. Rohrbach, T. Darrell, and M. Rohrbach,
“Multimodal compact bilinear pooling for visual question answering and
visual grounding,” </span><em id="bib.bib25.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1606.01847</em><span id="bib.bib25.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
H. Ben-Younes, R. Cadene, M. Cord, and N. Thome, “MUTAN: Multimodal tucker
fusion for Visual Question Answering,” in </span><em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE
international conference on computer vision</em><span id="bib.bib26.3.3" class="ltx_text" style="font-size:90%;">, 2017, pp. 2612–2620.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang,
“Bottom-up and top-down attention for image captioning and visual question
answering,” in </span><em id="bib.bib27.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision
and pattern recognition</em><span id="bib.bib27.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 6077–6086.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
R. Shrestha, K. Kafle, and C. Kanan, “Answer them all! toward universal visual
question answering models,” in </span><em id="bib.bib28.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib28.3.3" class="ltx_text" style="font-size:90%;">, 2019, pp. 10 472–10 481.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
J. P. Bigham, C. Jayant, H. Ji, G. Little, A. Miller, R. C. Miller, R. Miller,
A. Tatarowicz, B. White, S. White </span><em id="bib.bib29.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">et al.</em><span id="bib.bib29.3.3" class="ltx_text" style="font-size:90%;">, “VizWiz: nearly real-time
answers to visual questions,” in </span><em id="bib.bib29.4.4" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM symposium on User interface
software and technology</em><span id="bib.bib29.5.5" class="ltx_text" style="font-size:90%;">.   ACM, 2010,
pp. 333–342.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
D. Gurari, Q. Li, A. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. Bigham,
“VizWiz Grand Challenge: Answering visual questions from blind people,”
</span><em id="bib.bib30.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib30.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. Zitnick, and
R. Girshick, “CLEVR: A diagnostic dataset for compositional language and
elementary visual reasoning,” </span><em id="bib.bib31.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib31.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” </span><em id="bib.bib32.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib32.3.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
S. Lobry and D. Tuia, “Deep Learning Models to Count Buildings in
High-Resolution Overhead Images,” in </span><em id="bib.bib33.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Joint Urban Remote Sensing
Event</em><span id="bib.bib33.3.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
R. Kiros, Y. Zhu, R. Salakhutdinov, R. Zemel, A. Torralba, R. Urtasun, and
S. Fidler, “Skip-thought vectors,” </span><em id="bib.bib34.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neural Information Processing
Systems</em><span id="bib.bib34.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and
S. Fidler, “Aligning books and movies: Towards story-like visual
explanations by watching movies and reading books,” in </span><em id="bib.bib35.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE
international conference on computer vision</em><span id="bib.bib35.3.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 19–27.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
A. Li, Z. Lu, L. Wang, T. Xiang, and J.-R. Wen, “Zero-shot scene
classification for high spatial resolution remote sensing images,”
</span><em id="bib.bib36.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Geoscience and Remote Sensing</em><span id="bib.bib36.3.3" class="ltx_text" style="font-size:90%;">, vol. 55, no. 7,
pp. 4157–4167, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
D. Kingma and J. Ba, “Adam: A method for stochastic optimization,”
</span><em id="bib.bib37.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</em><span id="bib.bib37.3.3" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Y. Zhang, J. Hare, and A. Prügel-Bennett, “Learning to count objects in
natural images for visual question answering,” in </span><em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">International
Conference on Learning Representations</em><span id="bib.bib38.3.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
D. Tuia, C. Persello, and L. Bruzzone, “Domain adaptation for the
classification of remote sensing data: An overview of recent advances,”
</span><em id="bib.bib39.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Geoscience and Remote Sensing Magazine</em><span id="bib.bib39.3.3" class="ltx_text" style="font-size:90%;">, vol. 4, no. 2, pp.
41–57, June 2016.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
W.-L. Chao, H. Hu, and F. Sha, “Cross-dataset adaptation for visual question
answering,” in </span><em id="bib.bib40.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and Pattern
Recognition</em><span id="bib.bib40.3.3" class="ltx_text" style="font-size:90%;">, 2018, pp. 5716–5725.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the V
in VQA matter: Elevating the role of image understanding in Visual
Question Answering,” in </span><em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Conference on Computer Vision and
Pattern Recognition</em><span id="bib.bib41.3.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
<figure id="id1" class="ltx_float biography">
<table id="id1.1" class="ltx_tabular">
<tr id="id1.1.1" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td"><img src="/html/2003.07333/assets/AuthorsPics/Sylvain2.jpg" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="84" height="125" alt="[Uncaptioned image]"></td>
<td id="id1.1.1.2" class="ltx_td">
<span id="id1.1.1.2.1" class="ltx_inline-block">
<span id="id1.1.1.2.1.1" class="ltx_p"><span id="id1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Sylvain Lobry</span> 
(S’16 - M’17) received the Engineering degree from Ecole pour l’Informatique et les Techniques Avancées (EPITA), Kremlin Bicêtre, France in 2013, the Master’s degree in science and technology from the University Pierre et Marie Currie (Paris 6), Paris in 2014, and the Ph.D. degree from Telecom Paris, Paris, France, in 2017. He is currently a Post-Doctoral Researcher with the Geo-Information Science and Remote Sensing Laboratory, Wageningen University, The Netherlands. His research interests include radar image processing and multimodal processing of heterogeneous satellite data.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id2" class="ltx_float biography">
<table id="id2.1" class="ltx_tabular">
<tr id="id2.1.1" class="ltx_tr">
<td id="id2.1.1.1" class="ltx_td"><img src="/html/2003.07333/assets/AuthorsPics/Diego.jpg" id="id2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="95" height="125" alt="[Uncaptioned image]"></td>
<td id="id2.1.1.2" class="ltx_td">
<span id="id2.1.1.2.1" class="ltx_inline-block">
<span id="id2.1.1.2.1.1" class="ltx_p"><span id="id2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Diego Marcos</span> 
Diego Marcos obtained an MSc degree on Computational Sciences and Engineering from the Ecole Polytechnique Fédérale de Lausanne, Switzerland, in 2014 and a Ph.D. degree in environmental sciences from Wageningen University, The Netherlands, in 2019. He is currently a Post-Doctoral Researcher with the Geo-Information Science and Remote Sensing Laboratory, at Wageningen University. His research interests include computer vision and deep learning interpretability applied to geospatial data.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2003.07333/assets/AuthorsPics/jesse.jpg" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="110" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Jesse Murray</span> 
received an MSc degree in Geo-Information Science from Wageningen University, Wageningen, The Netherlands, in 2019. He is currently a Ph.D. candidate with the Geodetic Engineering Laboratory, at the Ecole Polytechnique Fédérale de Lausanne, Switzerland. His research interests include image processing and 3D geometry reconstruction using computer vision and spatio-temporal data.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td"><img src="/html/2003.07333/assets/AuthorsPics/Devis.jpg" id="id4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="119" alt="[Uncaptioned image]"></td>
<td id="id4.1.1.2" class="ltx_td">
<span id="id4.1.1.2.1" class="ltx_inline-block">
<span id="id4.1.1.2.1.1" class="ltx_p"><span id="id4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Devis Tuia</span> 
Devis Tuia (S’07 - M’09 - SM’15) received the Ph.D. degree from the University of Lausanne, Lausanne, Switzerland, in 2009. He was a Post-Doctoral Researcher in Valéncia, Boulder, CO, USA, and École polytechnique fédérale de Lausanne (EPFL), Lausanne. From 2014 to 2017, he was an Assistant Professor with the University of Zurich, Zürich, Switzerland. He is currently a Full Professor with the Geo-Information Science and Remote Sensing Laboratory, Wageningen University, Wageningen, The Netherlands. His research interests include algorithms for data fusion of geospatial data (including remote sensing) using machine learning and computer vision.</span>
</span>
</td>
</tr>
</table>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.07332" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.07333" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.07333">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.07333" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.07334" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 09:00:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
