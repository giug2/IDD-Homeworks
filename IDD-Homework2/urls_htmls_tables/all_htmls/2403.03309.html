<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data</title>
<!--Generated on Mon Jun 10 01:04:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2403.03309v5/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS1" title="In 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Procedural imitation: Extracting and implanting real-world patterns in synthetic data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS2" title="In 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Zeros-Shot Material State Segmentation Benchmark</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS3" title="In 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>Summary of main contributions:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data generation: patterns extraction and infusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS1" title="In 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Unsupervised extraction of maps from images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS2" title="In 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Extracting Textures and Materials</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS2.SSS1" title="In 3.2 Extracting Textures and Materials ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Unsupervised texture extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS2.SSS2" title="In 3.2 Extracting Textures and Materials ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Generating material property maps from texture images</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Material State Segmentation Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4.SS1" title="In 4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation metric</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4.SS2" title="In 4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Additional benchmarks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S5" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Segment Anything Model (SAM) and Materialistic</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.SS1" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Results on other benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.SS2" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Conclusion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix: Dataset and Code Access and License</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Authors Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Appendix: 3D scene building</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A4" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Hardware</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A5" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Appendix: Asset Sources</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Creating PBR/SVBRDF materials by mixing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Net and Training</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.SS1" title="In Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.1 </span>Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.SS2" title="In Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.2 </span>Net Architecture and Training Parameters</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Learning Zero-Shot Material States Segmentation,
by Implanting Natural Image Patterns in Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Sagi Eppel <sup class="ltx_sup" id="id1.1.id1">1,2*</sup>, Jolina Yining Li <sup class="ltx_sup" id="id2.2.id2">2*</sup>, Manuel S. Drehwald<sup class="ltx_sup" id="id3.3.id3">2</sup>, Alan Aspuru-Guzik<sup class="ltx_sup" id="id4.4.id4">1,2</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id5.5.id5">1</sup>Vector institute, <sup class="ltx_sup" id="id6.6.id6">2</sup>University of Toronto, * Equal contributions
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas on plants or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are available at these URLs: <a class="ltx_ref ltx_href" href="https://sites.google.com/view/matseg" title="">1</a>, <a class="ltx_ref ltx_href" href="https://zenodo.org/records/11331618" title="">2</a>, <a class="ltx_ref ltx_href" href="https://e.pcloud.link/publink/show?code=kZxsXTZIk88l74Jb3YeMeOcjOlJJVIqvHj7" title="">3</a>, <a class="ltx_ref ltx_href" href="https://icedrive.net/s/XxgZSif7NgYRbjvDN5w9aiWZ1fR3" title="">4</a>. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation at these URLs: <a class="ltx_ref ltx_href" href="https://sites.google.com/view/infinitexture/home" title="">1</a>,<a class="ltx_ref ltx_href" href="https://zenodo.org/records/11391127" title="">2</a>, <a class="ltx_ref ltx_href" href="https://e.pcloud.link/publink/show?code=kZON5TZtxLfdvKrVCzn12NADBFRNuCKHm70" title="">3</a>, <a class="ltx_ref ltx_href" href="https://icedrive.net/s/jfY1xSDNkVwtYDYD4FN5wha2A8Pz" title="">4</a>.</p>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="330" id="S0.F1.g1" src="extracted/5655111/figures/Figure1_GOOD_Smaller.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S0.F1.4.1">Top: MatSeg benchmark images</span> and MatSeg trained net predictions. Predictions are shown as a material similarity map relative to a selected point in the image (marked green). <span class="ltx_text ltx_font_bold" id="S0.F1.5.2">Center: Synthetic 3D scenes</span> and their annotations. Different colors in the annotation stand for different materials, black is the background. <span class="ltx_text ltx_font_bold" id="S0.F1.6.3">Bottom: Textures and SVBRDF/PBR materials</span> automatically extracted from natural images. More samples can be seen at <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F10" title="In G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">figs.</span> <span class="ltx_text ltx_ref_tag">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F11" title="Figure 11 ‣ G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">11</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F7" title="Figure 7 ‣ G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a>.
</figcaption>
</figure>
<figure class="ltx_figure" id="S0.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S0.F2.g1" src="extracted/5655111/figures/Figure2F_small.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Extracting textures and patterns from real-world images and using them in synthetic scenes: <span class="ltx_text ltx_font_bold" id="S0.F2.3.1">3D/2D Scene Mapping:</span> Mapping materials into synthetic scenes using maps extracted from natural images: a) Select a random image. b) Split the image into channels (R,G, B, H,S,V) and select one random channel. c) Apply a random ramp threshold to the selected channel to get a soft binary map. d) Use the map to position material on objects and scenes. 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S0.F2.4.2">Material Extraction:</span> a) Pick an image. b) Divide the image into a grid. For every grid cell, extract the distribution of colors and gradients. c) Identify a region for which all cells have similar distributions as a uniform texture. d,e) Pick random channels from the extracted texture image, augment them, and use the resulting maps as property maps (roughness, metallic, height, etc.) for the SVBRDF/PBR material.
</figcaption>
</figure>
<figure class="ltx_figure" id="S0.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="100" id="S0.F3.g1" src="extracted/5655111/figures/benchmark_smaller.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>MatSeg benchmark with points-based annotation. Points of the same color are of the exact same material. Similar materials are marked as two or more dots of different colors in a box (top left).</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Materials and their states form a vast array of patterns and textures that define the physical and visual world. Minerals in rocks, sediment in soil, dust on surfaces, infection on leaves, stains on fruits, and foam in liquids are some of these almost infinite numbers of states and patterns. Segmenting these states in images is fundamental to the comprehension of the world and is essential for a wide range of tasks, from cooking and cleaning to construction and laboratory work.
Currently, there is no comprehensive dataset or benchmark that addresses this general task, and even the top foundation models perform poorly on this problem (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6" title="6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>).
This work presents the first general benchmark and datasets focused on this task, encompassing a wide range of material states and domains without being limited to specific materials or settings<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib59" title="">59</a>]</cite>. This includes dealing with gradual transitions and partial similarity between states, as well as scattered shapes and fuzzy boundaries (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> top).
For the benchmark, the goal is to achieve this using real-world images taken in the wild. The goal of the training set is to provide unlimited synthetic data that capture the complexity and variability of the real world while not being limited to a set of procedural rules and assets.
Manual annotation of material states is highly complex from a human perspective, especially for cases with soft boundaries and gradual transitions or sparse and scattered patterns. In specific settings, it is sometimes possible to find some method or additive to simplify the annotations. However, the unstructured and wide range of domains in the benchmark makes this unattainable in this case. Synthetic data can simulate soft and scattered states with perfect annotation. However, it fails to capture the vast complexity of material forms in the real world, and while it is possible to accurately represent specific domains using simulation or procedural rules<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib45" title="">45</a>]</cite>. Achieving this for the full range of material states in the world encompasses the entire material science and is unattainable.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Procedural imitation: Extracting and implanting real-world patterns in synthetic data</h3>
<div class="ltx_para ltx_noindent" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">This work offers a method to bridge this gap between synthetic and real data by automatically extracting patterns and textures from real-world images and using these to generate and map materials into synthetic scenes (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>). This approach combines the precision and scale of synthetic data with the vast variability of patterns in the real world.
The hypothesis is that simple image properties like value, hue, saturation, or one of the Red, Green, or Blue channels (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>) are often correlated with material regions or properties within a material texture<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib27" title="">27</a>]</cite>.
For example, the dry part of a leaf, stains on fabrics, or pollution in water will often be darker or brighter than their surroundings.
In these cases, the image brightness map will capture the material region.
Similarly, within a single material texture, regions with higher reflectivity, transparency, or roughness will often be darker or brighter, allowing a single image channel to capture the property distribution.
Assuming this is true for some fraction of cases and that important material states appear in a wide range of settings, extracting these maps from a broad range of images will capture these patterns.
Once captured, these patterns can be used in various synthetic scenes where the original correlation is not applied and therefore will be learned by the net in general form (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>).
Thus, patterns are automatically extracted in simple cases with a strong correlation between material and a simple image property but are then rendered and learned in other scenes where this correlation does not occur.
Since this is an unsupervised process, it will capture a significant amount of noise and unrelated patterns.
However, neural nets are very effective in learning from noisy data, as demonstrated by methods such as CLIP and GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib9" title="">9</a>]</cite>.
Conversely, neural nets tend to perform poorly when trained on clean data with a narrow domain and then are expected to generalize to broader tasks<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib29" title="">29</a>]</cite>.
The fact that the net trained on this infused data outperforms top foundation models (SAM, Materialistic<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a>]</cite>) on a wide range of benchmarks strongly supports this hypothesis.
We share the dataset and an extensive repository of extracted textures and SVBRDF/PBR materials collected by this method to enable the generation of future datasets (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A1" title="Appendix A Appendix: Dataset and Code Access and License ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Zeros-Shot Material State Segmentation Benchmark</h3>
<div class="ltx_para ltx_noindent" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">We present the first general benchmark for zero-shot material state segmentation. Benchmarks for specific material states like corrosion or dust have been published<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib18" title="">18</a>]</cite>; however, no general zero-shot benchmark for material states that encompasses a wide range of fields without being limited to a specific set of states has been published so far. The benchmark contains 1,220 real-world images with a wide range of materials and domains, such as food states (cooked / burned, etc.), plants (infected / dry), rocks / soil (minerals / sediment), construction / metals (rusted/worn), liquids (foam/precipitate), and many other states in a class-agnostic manner (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">figs.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="Figure 3 ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>). The goal is to evaluate the segmentation of material states without being limited to a specific class, domain, or setting. This includes materials with complex or scattered shapes, fuzzy boundaries, or gradual transitions. This makes manual polygon-based annotation unfeasible. To solve this, we use point-based and similarity-based annotations. Hence, for each image, we sample multiple points that represent the distribution of each material state (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>). Points with the exact same material state are grouped under the same label. We also define groups (materials) that have partial similarity. For example, points in group A are more similar to points in group B than to points in group C if materials A and B are similar but not identical. This captures the complexity of gradual transitions while also dealing with scattered shapes without needing to manually annotate the full image, which in many cases is unfeasible. The top image segmentation foundation models (SAM, Materialistic<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a>]</cite>) perform poorly on this benchmark. This shows that standard data collection approaches have a major gap for these types of tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Summary of main contributions:</h3>
<div class="ltx_para ltx_noindent" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">1) Constructing the first general material state segmentation benchmark and dataset. This enables, for the first time, training and evaluation on this fundamental task and exposes a major gap in the capabilities of top image segmentation models.
<br class="ltx_break"/>2) Demonstrating an unsupervised method for extracting patterns and textures from real-world images and infusing them into synthetic scenes. This method enables the creation of synthetic data that automatically encompasses much of the complexity of the real world. The net trained on this data outperforms the top methods in various zero-shot benchmarks.
<br class="ltx_break"/>3) In addition to the dataset and the net, we also share a giant repository of extracted textures and materials to enable future dataset generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Unsupervised texture segmentation and extraction:</span>
Methods for unsupervised texture extraction and image segmentation based on simple image features (color, shades, gradients) or statistical features have been extensively explored in the last 50 years<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib63" title="">63</a>]</cite>, but were mostly discarded in favor of deep learning approaches. These methods usually involve finding connected regions with uniform color and shades <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib27" title="">27</a>]</cite>, or splitting the image into a grid and finding cells with a similar statistical distribution<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib31" title="">31</a>]</cite>. The goal of this work is not to improve upon these methods, but rather to show that the pattern extracted by them while noisy and imprecise captures a broad range of features that are missing from leading data generation methods. Hence, these "ancient" methods offer a way to generate training data that can outperform modern data generation methods in important domains.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.p1.1.2">Synthetic data and the domain gap:</span>
Synthetic data and CGI images are commonly used to train machine learning models for computer vision tasks<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib36" title="">36</a>]</cite>. Similarly to games and movies, this data is crafted using human-made assets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib19" title="">19</a>]</cite>, scans, simulations, and procedural generation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib45" title="">45</a>]</cite>. However, they often fail to capture the diversity and complexity of the real world, leading to a domain gap. To address this, domain adaptation techniques such as GANs are employed to make images more photorealistic or adapt them to different conditions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib46" title="">46</a>]</cite>. These methods mainly adjust the images without changing the underlying scene. Additionally, some approaches use procedural rules and mathematical functions to better mimic the world, these methods generate a wide range of patterns but are still limited by the generation rules<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib10" title="">10</a>]</cite>.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.p1.1.3">Materials in synthetic scenes, representation, extraction and mapping:</span>

Materials in CGI and synthetic scenes are usually represented as SV-BRDF or PBR materials<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib43" title="">43</a>]</cite>. The term PBR is more common and will be used in this work. This representation is based on a few properties of the material surface, such as albedo, reflectance, normals, roughness, and transparency. Each property is represented as a 2D map that defines the value of this property on the surface. These maps are wrapped around the object to give it a material appearance. Generating these maps is done mostly manually by CGI artists and scans<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib3" title="">3</a>]</cite>. Currently, there are a few thousand PBRs that are publicly available in open repositories<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib61" title="">61</a>]</cite>. Recently, neural nets that generate PBR materials from input text or images have been suggested and embedded in various products<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib58" title="">58</a>]</cite>. However, since these methods are trained using existing PBR material repositories, it is unclear if they can generate beyond the distribution on which they were trained. As far as we know, these works focus on generating assets for CGI artists and were not tested for generating training data machine learning methods.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.p1.1.4">Zero-Shot and Class Agnostic Segmentation:</span>
Methods for identifying and segmenting materials and textures without being limited to a specific set of classes or properties are often called one-shot, zero-shot, or class-agnostic. Zero-shot segmentation usually receives a query or a point in the image as input and outputs the region of the image corresponding to the query<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a>]</cite> Zero-Shot methods for texture segmentation were trained using synthetic data by projecting multiple material textures into an image and generating images with known segmentation maps, which are used to train nets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a>]</cite>. These methods gave good results for simple, class-independent, materials and texture segmentation. However, the distribution of materials it generates is either random or depends on a set of limited pre-made assets. Both cases don’t aim to replicate the complexity of materials in the world, and up to this work, were not tested on material state segmentation.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data generation: patterns extraction and infusion</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The data infusion process described in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">figs.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS1" title="1.1 Procedural imitation: Extracting and implanting real-world patterns in synthetic data ‣ 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">1.1</span></a> serves two main purposes. The first is to map existing materials into synthetic scenes (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS1" title="3.1 Unsupervised extraction of maps from images ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>). The second application involves extracting texture maps and PBR/SVBRDF materials from images (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS2" title="3.2 Extracting Textures and Materials ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). Both applications rely on the assumptions outlined in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS1" title="1.1 Procedural imitation: Extracting and implanting real-world patterns in synthetic data ‣ 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">1.1</span></a> and are detailed below.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Unsupervised extraction of maps from images</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The extraction of maps from images was done by randomly picking one property of the image; this can be the saturation, hue, brightness, or one of the RGB channels (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>b). Each of these features transforms the image into a 2D topological map. The selected map passes through a fuzzy threshold (color ramp, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>c). This involves picking two random threshold values. Everything above the higher threshold turns 1, everything below the lower threshold turns 0, and everything in between is a linear interpolation between 0 and 1. This allows the map to have distinct segments while maintaining soft and gradual transitions. Repeating this process several times can create maps with several distinct regions. This map can be used to map materials onto the surface of objects and scenes (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.F4" title="In 3.1 Unsupervised extraction of maps from images ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Creating 2D Scenes:</span>
The simple way to use the extracted map is to define every map region as a different material in an image and map different textures into different regions (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.center.d). The resulting image is then overlayed on top of a random background image (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.center.e). Shadow-like effects can be achieved by using another extracted soft map to darken some regions according to the map value. In regions where the value of the map is between 0 and 1, several textures are mixed according to the values of the map (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.F4" title="In 3.1 Unsupervised extraction of maps from images ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="104" id="S3.F4.g1" src="extracted/5655111/figures/Synthehtic_Smaller.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Examples of 2D and 3D synthetic scenes (top) and their annotations (bottom). In the annotation, each material is marked as a different color. A mixture of materials marked as a mixture of their colors. The background is marked black. More samples can be seen at <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F11" title="In G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Creating 3D scenes:</span>
A more advanced approach is to use the generated maps to set PBR materials on object surfaces in 3D scenes using CGI software such as Blender<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib8" title="">8</a>]</cite>. This is done by creating 3D scenes and wrapping the extracted maps around objects’ surfaces in a process called UV mapping. Random PBR materials are then placed in each segment of the map (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.left.d,e). Background, random objects, ground, and illumination are added to generate full 3D scenes with physics-based rendered materials (PBR), natural illumination, shadows, and curved surfaces (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>.center).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Extracting Textures and Materials</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Materials in synthetic datasets are usually represented as SVBRDF/PBR, in which the spatial distribution of each material property (reflectivity, roughness, transparency, etc.) is given as a 2D map<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib43" title="">43</a>]</cite>. Extracting textures and PBR materials from images involves first identifying regions with a uniform texture (which implies a uniform material, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.right). This texture image is then used to extract maps corresponding to each material’s property.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Unsupervised texture extraction</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Unsupervised texture extraction is traditionally done by finding image regions with a uniform distribution of color and gradients<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib62" title="">62</a>]</cite>. Finding these regions is done by splitting the image into square cells of 40x40 pixels each and identifying square regions where the distribution of the RGB values and their gradients is similar for all the cells in the region. Regions of more than 6x6 cells with similar distributions between all cells were extracted (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.right). More accurately, a region was considered to have uniform texture if the similarity of the distributions between each pair of cells in the region had a Jensen-Shannon distance of less than 0.5. This similarity was tested separately for each of the RGB channels and their gradients. Finally, regions with too-uniform values or very high or low values are ignored, as they only contain smooth, textureless regions. It’s clear that this approach will fail to capture textures on bent surfaces or non-uniform illumination and shadows. However, the assumption is that for a large enough set of images, common material textures will appear in some cases in a simple setting and will be captured (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS1" title="1.1 Procedural imitation: Extracting and implanting real-world patterns in synthetic data ‣ 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">1.1</span></a>). Once captured, they will be projected into scenes with different illuminations, and bent surfaces and will be learned for the general case. Some textures extracted using this approach are shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">figs.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F8" title="Figure 8 ‣ G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Generating material property maps from texture images</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Extracting PBR material maps from RGB textures requires guessing the physical properties of the material at each point of the texture image. There is no straightforward way to achieve this using an RGB image. Once again, the assumption will be that the properties of the material (reflectivity, roughness, and transparency) are correlated, in some cases, with simple image properties like color(<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S1.SS1" title="1.1 Procedural imitation: Extracting and implanting real-world patterns in synthetic data ‣ 1 Introduction ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">1.1</span></a>), and for a common materials and a large enough set of images such correlation is bound to occur in some setting. Each material property map is generated by randomly choosing one of the six image channels (RGB or HSV), to represent this map (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> left), image augmentation, thresholding, and random scaling/shifting were also used. Normal maps were generated by taking the gradient of the height/bumpiness map(<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F2" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.left). In some cases, the map was chosen to have a uniform random value. Some results are shown in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F1" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">figs.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F7" title="Figure 7 ‣ G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">7</span></a>. In addition, materials were also generated by mixing maps of existing PBR materials as described in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6" title="Appendix F Creating PBR/SVBRDF materials by mixing ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Material State Segmentation Benchmark</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Establishing a benchmark for testing general material state segmentation beyond a specific set of materials and domains is essential for evaluating methods in this field. Such benchmarks must be based on real-world images and encompass a wide range of material states, environments, and domains. In addition, it needs to capture scattered and sparse shapes commonly formed by materials. Finally, to address mixtures and gradual transitions, the benchmark must capture partial similarity between materials. Currently, no benchmark is available that addresses these challenges.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Image collection and annotation:</span>

1220 images for the benchmark were manually taken from a wide variety of real-world scenes (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).
Including food at different levels of cooking, liquids, solutes, foams, plants, rocks, minerals, sediments, grounds, landscapes, construction sites, metals, relics, fabrics, labs and processes like cooking, wetting, rotting, infections, corrosion, stains, and many others.
The goal is to capture as much of the world as possible, both in terms of materials and environments. For materials with scatter, soft boundaries, and gradual transitions, it is often impossible to segment the entire region belonging to a material state. An alternative approach is to sample points in the image that represent the main materials’ states and their distribution. This allows annotators to focus on regions with clear annotation, but also to sample the harder and more complex regions without dealing with areas where the segmentation is unclear. The annotation procedure is as follows: 1) Pick points in each image that represent the distribution of each material state (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>). 2) Group the points according to their material state such that all points belonging to the exact same material state will have the same label or group (points of the same color in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> are in the same group). 3) Assign relative similarity between groups. For example, if material B is a transition state between materials A and C, we can define the points in group B to be partially similar to both groups A and C, while groups A and C are not similar to each other. This circumvents the almost impossible task of assigning numerical similarity to material states (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S0.F3" title="In Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation metric</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The evaluation approach is based on assessing the relative similarity between the points in the image. Every segmentation approach, soft or hard, can be easily converted to predict the similarity between two points in the image. For hard segmentation, this similarity will be one if the two points are in the same segment and zero if not.
For evaluation, we use the triplet metric which goes as follows: a) Select three points in the image and define one as an anchor. b) Ask which of the remaining two points is more similar to the anchor according to the ground truth (GT) and according to the prediction; if the predictions and GT agree, we assign this as correct. c) If the two points have identical similarity to the anchor (according to the GT), we ignore this triplet. This is done for all sets of GT triplets in the image, and the average per material is used. For this metric, a 50% score means a random guess and a 100% score means perfect segmentation. The results are given in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T1" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Additional benchmarks</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">While there are no other benchmarks for general zero-shot material state segmentation, there are a number of benchmarks that deal with subdomains of this task. These include segmentation of specific material states, like corrosion<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib48" title="">48</a>]</cite>, minerals/ores<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib28" title="">28</a>]</cite>, dust<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib18" title="">18</a>]</cite>, leaf disease<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib5" title="">5</a>]</cite>, soil types and states<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib50" title="">50</a>]</cite>, microscopy<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib40" title="">40</a>]</cite>, or material phases (liquids, solids, foams, or powders)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib23" title="">23</a>]</cite>. Although each of these benchmarks is relatively narrow, when combined, they offer a way to cross-check the results of the main benchmark. Materialistic is a benchmark<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a>]</cite> that contains 50 images with different materials, not limited to specific classes. With materials segments that cover the whole object or regions with straight, smooth boundaries. It consists of distinct material types with no states or gradual transitions. Comparing the results of this benchmark to MatSeg offers a way to examine how much these elements affect segmentation accuracy. To evaluate the nets on these benchmarks, we select a random point in each material segment and use the nets to predict the similarity of each pixel in the image to the selected point. To turn this map into a hard segmentation map, we choose a threshold that maximizes the IOU between the predicted and GT masks. We use the mean IOU as the benchmark score. This is done with multiple points to obtain accurate statistics. The results are given in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T2" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Segment Anything Model (SAM) and Materialistic</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text ltx_font_bold" id="S5.p1.1.1">Segment Anything Model (SAM)</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib34" title="">34</a>]</cite> is the current leading foundation model for image segmentation. It is designed to segment anything within an image when guided by positive input points placed inside the target segment and negative points placed outside it. Trained on a vast dataset of 11 million real images and 1 billion segments generated through a semi-manual annotation, SAM exemplifies the top performance achievable with extensive manual data annotation. The model’s efficacy was evaluated using 2, 4, and 8 input points, with an equal number of positive and negative points.

<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S5.p1.1.2">Materialistic<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S5.p1.1.2.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib53" title="">53</a><span class="ltx_text ltx_font_medium" id="S5.p1.1.2.2.2">]</span></cite></span> is the latest approach for segmenting materials based on visual similarity and can be used in a zero-shot manner to find regions of the same materials in images using an input point. Similar to MatSeg, it was trained using 3D synthetic data, with one major difference: all assets used for Materialistic training data are based on existing repositories, which are mostly manually made. The resuls of this net offers a way to isolate the effect of the infusion method used in MatSeg compared to standard 3D synthetic data approaches.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Results for the MatSeg benchmark are given in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T1" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.F5" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. The net trained on the MatSeg data achieves good accuracy and significantly outperforms both SAM and Materialistic on both hard and soft cases. It can be seen from <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.F5" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> that the net learned to predict highly scattered and complex patterns with soft and gradual transitions and achieves this for a wide range of domains. Hence, despite learning from synthetic data, the net manages to generalize for a wide range of real-world cases. In addition, it seems to be able to mitigate reflections and shadows <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.F5" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. Both Segment Anything (SAM) and Materialistic nets performed far worse on the benchmark (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T1" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">1</span></a>). From <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.F5" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>, it seems that both SAM and Materialistic search for the boundary of the material based on the boundary of the object or bulk regions and struggle with scattered shapes and soft boundaries. SAM and Materialistic also seem to be less sensitive to the material state and more focused on matching materials based on coarse-grain type (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.F5" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>). Both nets seem to search for bulk regions with smooth boundaries, which are likely to be more representative in their training set. SAM has failed to achieve high accuracy even after receiving 8 guiding input points (compared to 1 for other methods), suggesting that despite training on a vast amount of examples, it never encountered or learned such patterns. This seems to imply a major gap in the data collected for both SAM and Materialistic. This hypothesis is further supported by the fact that the SAM and Materialistic nets outperformed the MatSeg-trained net on the Materialistic test set (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T2" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">2</span></a>), which contains distinct materials that encompass either a full object or distinct regions with smooth hard boundaries. As such, their shape distribution resembles that of assets used in the Materialistic train set and the SAM polygon-based annotations. Hence, the MatSeg data seems to better model raw material states, while SAM and Materialistic seem to perform better on more structured scenes and object-guided segments.</p>
</div>
<figure class="ltx_figure" id="S6.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S6.F5.g1" src="extracted/5655111/figures/Results_Final_18.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Results: Predicted material similarity maps to a given point in the image. The input point is marked green in each panel. For SAM net the results are with 8 input points, 4 positive (marked green) and 4 negative (marked red). The materials similarity map is brighter for high similarity and dark for low similarity (similarity to the input point). More results can be seen at <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A7.F10" title="In G.2 Net Architecture and Training Parameters ‣ Appendix G Net and Training ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">10</span></a>.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results on the MatSeg Benchmark (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4" title="4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4</span></a>). The results are in triplet loss(<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4.SS1" title="4.1 Evaluation metric ‣ 4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>). Points (pts) are the number of input points used to guide the segmentation. MatSeg <span class="ltx_text ltx_font_bold" id="S6.T1.6.1">2D</span> and <span class="ltx_text ltx_font_bold" id="S6.T1.7.2">3D</span> stand for nets trained on only 2D or 3D scenes, and <span class="ltx_text ltx_font_bold" id="S6.T1.8.3">Mixed</span> stands for a net trained on a combination of 3D and 2D scenes. <span class="ltx_text ltx_font_bold" id="S6.T1.9.4">All</span> stands for evaluation on all triplets. <span class="ltx_text ltx_font_bold" id="S6.T1.10.5">Soft</span> stand for evaluation only on triplets with partial similarity between points(<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4" title="4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4</span></a>).</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T1.11">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.11.1.1">
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.1"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T1.11.1.1.2">2D</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T1.11.1.1.3">3D</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T1.11.1.1.4">Mixed</td>
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.5"></td>
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.6"></td>
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.7"></td>
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.8"></td>
<td class="ltx_td ltx_border_tt" id="S6.T1.11.1.1.9"></td>
</tr>
<tr class="ltx_tr" id="S6.T1.11.2.2">
<td class="ltx_td" id="S6.T1.11.2.2.1"></td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.2">MatSeg</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.3">MatSeg</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.4">MatSeg</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.5">Materialsitic</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.6">SAM</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.7">SAM</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.8">SAM</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.2.2.9">SAM</td>
</tr>
<tr class="ltx_tr" id="S6.T1.11.3.3">
<td class="ltx_td" id="S6.T1.11.3.3.1"></td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.2">1 point</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.3">1 point</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.4">1pt</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.5">1pt</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.6">1pts</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.7">2pts</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.8">4pts</td>
<td class="ltx_td ltx_align_center" id="S6.T1.11.3.3.9">8pts</td>
</tr>
<tr class="ltx_tr" id="S6.T1.11.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.1">All</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.2">0.91</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.3">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T1.11.4.4.4.1">0.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.5">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.6">0.69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.7">0.70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.8">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T1.11.4.4.9">0.74</td>
</tr>
<tr class="ltx_tr" id="S6.T1.11.5.5">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.1">Soft</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.2">0.84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.3">0.83</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.4"><span class="ltx_text ltx_font_bold" id="S6.T1.11.5.5.4.1">0.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.5">0.72</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.6">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.7">0.62</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.8">0.64</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T1.11.5.5.9">0.65</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results on various of datasets (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4.SS2" title="4.2 Additional benchmarks ‣ 4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). The results are in IOU for an optimal threshold (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S4.SS2" title="4.2 Additional benchmarks ‣ 4 Material State Segmentation Benchmark ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). Benchmarks with <span class="ltx_text ltx_font_bold" id="S6.T2.4.1">S</span>cattered shapes and <span class="ltx_text ltx_font_bold" id="S6.T2.5.2">G</span>radual transitions are marked (S/G). Points (pts) is the number of input points used to guide the segmentation. The top one-shot (one point) method is <span class="ltx_text ltx_font_bold" id="S6.T2.6.3">marked.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T2.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.7.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T2.7.1.1.1"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T2.7.1.1.2"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S6.T2.7.1.1.3">
<span class="ltx_text ltx_font_bold" id="S6.T2.7.1.1.3.1">S</span>catter</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.4">1 Point</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.5">1 Point</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.6">1 Point</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.7">2 Pts</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.8">4 Pts</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S6.T2.7.1.1.9">8 Pts</th>
</tr>
<tr class="ltx_tr" id="S6.T2.7.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S6.T2.7.2.2.1">Dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S6.T2.7.2.2.2">Field</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S6.T2.7.2.2.3">
<span class="ltx_text ltx_font_bold" id="S6.T2.7.2.2.3.1">G</span>radual</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.4">MatSeg</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.5">Materialistic</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.6">SAM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.7">SAM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.8">SAM</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column" id="S6.T2.7.2.2.9">SAM</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.7.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.7.3.1.1">Cu dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.7.3.1.2">Mineral/Ore</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T2.7.3.1.3">S</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.3.1.4.1">0.52</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.5">0.36</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.6">0.37</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.7">0.37</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.8">0.51</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T2.7.3.1.9">0.69</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.4.2.1">FeM dataset</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.4.2.2">Mineral/Ore</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.4.2.3">S</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.4.2.4.1">0.62</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.5">0.37</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.6">0.36</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.7">0.37</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.8">0.50</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.4.2.9">0.67</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.5.3.1">corrosao-segment</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.5.3.2">Corrosion/Metals</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.5.3.3">SG</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.5.3.4.1">0.69</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.5">0.49</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.6">0.48</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.7">0.48</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.8">0.54</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.5.3.9">0.56</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.6.4.1">Leaf diseases</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.6.4.2">Plants/Disease</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.6.4.3">SG</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.6.4.4.1">0.56</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.5">0.47</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.6">0.47</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.7">0.47</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.8">0.51</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.6.4.9">0.54</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.7.5.1">URDE</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.7.5.2">Dust</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.7.5.3">G</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.7.5.4.1">0.50</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.5">0.47</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.6">0.44</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.7">0.45</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.8">0.49</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.7.5.9">0.52</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.8.6.1">Soil-type-class-2</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.8.6.2">Soil States</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.8.6.3">G</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.8.6.4.1">0.62</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.5">0.53</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.6">0.60</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.7">0.61</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.8">0.68</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.8.6.9">0.72</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.9.7.1">Soil type classification</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.9.7.2">Soil/Rocks types</th>
<th class="ltx_td ltx_th ltx_th_row" id="S6.T2.7.9.7.3"></th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.4">0.69</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.5">0.71</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.6"><span class="ltx_text ltx_font_bold" id="S6.T2.7.9.7.6.1">0.79</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.7">0.79</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.8">0.86</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.9.7.9">0.88</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.10.8.1">NuInsSeg</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.10.8.2">Microscopy</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.10.8.3">SG</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.10.8.4.1">0.38</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.5">0.17</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.6">0.23</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.7">0.23</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.8">0.29</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.10.8.9">0.32</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.11.9.1">CryoNuSeg</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.11.9.2">Microscopy</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.11.9.3">SG</th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.11.9.4.1">0.45</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.5">0.28</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.6">0.26</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.7">0.26</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.8">0.28</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.11.9.9">0.28</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.12.10.1">Materialistic</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.12.10.2">General</th>
<th class="ltx_td ltx_th ltx_th_row" id="S6.T2.7.12.10.3"></th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.4">0.75</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.5"><span class="ltx_text ltx_font_bold" id="S6.T2.7.12.10.5.1">0.87</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.6">0.72</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.7">0.72</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.8">0.80</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.12.10.9">0.85</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.13.11.1">LabPics Chemistry</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T2.7.13.11.2">Chemistry/Phases</th>
<th class="ltx_td ltx_th ltx_th_row" id="S6.T2.7.13.11.3"></th>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.4">0.62</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.5">0.63</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.6"><span class="ltx_text ltx_font_bold" id="S6.T2.7.13.11.6.1">0.72</span></td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.7">0.72</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.8">0.74</td>
<td class="ltx_td ltx_align_left" id="S6.T2.7.13.11.9">0.75</td>
</tr>
<tr class="ltx_tr" id="S6.T2.7.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T2.7.14.12.1">LabPics Medical</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T2.7.14.12.2">Lab/Liquids</th>
<th class="ltx_td ltx_th ltx_th_row ltx_border_bb" id="S6.T2.7.14.12.3"></th>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.4"><span class="ltx_text ltx_font_bold" id="S6.T2.7.14.12.4.1">0.71</span></td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.5">0.68</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.6">0.69</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.7">0.69</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.8">0.71</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S6.T2.7.14.12.9">0.72</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Results on other benchmarks</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">The results of SAM, Materialistic, and MatSeg-trained nets on various benchmarks are given in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T2" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">2</span></a>. These results support the conclusion of the previous section. The MatSeg-trained net outperformed other methods in cases of scattered segments, soft/gradual boundaries, and segmenting relatively similar states. Whether it is dust on roads, leaf diseases, corrosion, microscopy, or mud and dry ground. In contrast, SAM and Materialistic outperform MatSeg in benchmarks with materials that form bulk smooth boundaries, appear in single bulk shapes, have a strong correlation to objects, and don’t have gradual transitions (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S6.T2" title="In 6 Results ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">table</span> <span class="ltx_text ltx_ref_tag">2</span></a>). Materialistic is more similar to SAM in its results, despite the fact that both MatSeg and Materialistic are trained on synthetic data, while SAM is trained on real images. This supports the hypothesis that the data infusion method in MatSeg taps a fundamentally different distribution that was missed by both manual annotations and synthetic data generation approaches used to train SAM and Materialistic. However, it should also be noted that the accuracy of all methods on these benchmarks is limited, suggesting that a significant gap remains in this field.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Conclusion</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Segmenting material states is fundamental to understanding nearly every aspect of the physical world and has numerous downstream applications. This work offers the first benchmark and dataset that deal with this task in a general zero-shot manner. This allows, for the first time, to train nets specifically for this task as well as to evaluate existing general methods. The results of this evaluation show that even the top foundation models struggle with this task. This work also offers a general, unsupervised approach for extracting patterns and textures from images and infusing them into synthetic scenes. This allows the creation of large-scale synthetic data that captures much of the complexity and diversity of the real world with no human effort. Nets trained on this data significantly outperformed the top foundation models on the MatSeg benchmark and many related zero-shot benchmarks (without training on the datasets).
This suggests that the infusion method offers a way to close a major gap between real-world and synthetic data, and tap into patterns distribution missed by leading data generation approaches. The benchmark, dataset, and 300,000 extracted textures and PBR materials have been made available (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A1" title="Appendix A Appendix: Dataset and Code Access and License ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Ambientcg, free pbr textures repositories.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ambientcg.com/" title="">https://ambientcg.com/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
freepbr, free pbr textures repositories.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://freepbr.com/" title="">https://freepbr.com/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Polyhaven free hdri repository.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://polyhaven.com" title="">https://polyhaven.com</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Peri Akiva, Matthew Purri, and Matthew Leotta.

</span>
<span class="ltx_bibblock">Self-supervised material and texture representation learning for remote sensing tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 8203–8215, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Fakhre Alam.

</span>
<span class="ltx_bibblock">Leaf disease segmentation dataset, March 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Clara Asmail.

</span>
<span class="ltx_bibblock">Bidirectional scattering distribution function (bsdf): a systematized bibliography.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Journal of research of the National Institute of Standards and Technology</span>, 96(2):215, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala.

</span>
<span class="ltx_bibblock">Opensurfaces: A richly annotated catalog of surface appearance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">ACM Transactions on graphics (TOG)</span>, 32(4):1–17, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
blender.org Community.

</span>
<span class="ltx_bibblock">Blender: A 3d modelling and rendering.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.blender.org/" title="">https://www.blender.org/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Advances in neural information processing systems</span>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Sean B Carroll.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Endless forms most beautiful: The new science of evo devo and the making of the animal kingdom</span>.

</span>
<span class="ltx_bibblock">Number 54. WW Norton &amp; Company, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al.

</span>
<span class="ltx_bibblock">Shapenet: An information-rich 3d model repository.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:1512.03012</span>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Patrick C Chen and Theodosios Pavlidis.

</span>
<span class="ltx_bibblock">Segmentation by texture using correlation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>, (1):64–69, 1983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Yu Chen, Chirag Rastogi, Zheyu Zhou, and William R Norris.

</span>
<span class="ltx_bibblock">A self-supervised miniature one-shot texture segmentation (mosts) model for real-time robot navigation and embedded applications.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2306.08814</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ta-Ying Cheng, Prafull Sharma, Andrew Markham, Niki Trigoni, and Varun Jampani.

</span>
<span class="ltx_bibblock">Zest: Zero-shot material transfer from a single image.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2404.06425</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Guy Barrett Coleman and Harry C Andrews.

</span>
<span class="ltx_bibblock">Image segmentation by clustering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the IEEE</span>, 67(5):773–785, 1979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Otávio da Fonseca Martins Gomes, Sidnei Paciornik, Michel Pedro Filippo, Gilson Alexandre Ostwald Pedro da Costa, and Guilherme Lucio Abelha Mota.

</span>
<span class="ltx_bibblock">Cu dataset – a copper ore labeled images dataset for segmentation training and testing, June 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Otávio da Fonseca Martins Gomes, Sidnei Paciornik, Michel Pedro Filippo, Gilson Alexandre Ostwald Pedro da Costa, and Guilherme Lucio Abelha Mota.

</span>
<span class="ltx_bibblock">Fem dataset – an iron ore labeled images dataset for segmentation training and testing, June 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Asanka De Silva, Rajitha Ranasinghe, Arooran Sounthararajah, Hamed Haghighi, and Jayantha Kodikara.

</span>
<span class="ltx_bibblock">A benchmark dataset for binary segmentation and quantification of dust emissions from unsealed roads.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Scientific Data</span>, 10(1):14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.

</span>
<span class="ltx_bibblock">Objaverse-xl: A universe of 10m+ 3d objects.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis, and Adrien Bousseau.

</span>
<span class="ltx_bibblock">Single-image svbrdf capture with a rendering-aware deep network.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">ACM Transactions on Graphics (ToG)</span>, 37(4):1–15, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Manuel S Drehwald, Sagi Eppel, Jolina Li, Han Hao, and Alan Aspuru-Guzik.

</span>
<span class="ltx_bibblock">One-shot recognition of any material anywhere using contrastive learning with physics-based rendering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 23524–23533, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Sagi Eppel.

</span>
<span class="ltx_bibblock">Class-independent sequential full image segmentation, using a convolutional net that finds a segment within an attention region, given a pointer pixel within this segment.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:1902.07810</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sagi Eppel, Haoping Xu, and Alan Aspuru-Guzik.

</span>
<span class="ltx_bibblock">Computer vision for liquid samples in hospitals and medical labs using hierarchical image segmentation and relations prediction.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2105.01456</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Sagi Eppel, Haoping Xu, Mor Bismuth, and Alan Aspuru-Guzik.

</span>
<span class="ltx_bibblock">Computer vision for recognition of materials and vessels in chemistry lab settings and the vector-labpics data set.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">ACS central science</span>, 6(10):1743–1752, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ye Fang, Zeyi Sun, Tong Wu, Jiaqi Wang, Ziwei Liu, Gordon Wetzstein, and Dahua Lin.

</span>
<span class="ltx_bibblock">Make-it-real: Unleashing large multimodal model’s ability for painting 3d objects with realistic materials.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2404.16829</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Michel Pedro Filippo, Otávio da Fonseca Martins Gomes, Gilson Alexandre Ostwald Pedro da Costa, and Guilherme Lucio Abelha Mota.

</span>
<span class="ltx_bibblock">Deep learning semantic segmentation of opaque and non-opaque minerals from epoxy resin in reflected light microscopy images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Minerals Engineering</span>, 170:107007, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
King-Sun Fu and JK Mui.

</span>
<span class="ltx_bibblock">A survey on image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Pattern recognition</span>, 13(1):3–16, 1981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Otávio da Fonseca Martins Gomes and Sidnei Paciornik.

</span>
<span class="ltx_bibblock">Multimodal microscopy for ore characterization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Scanning Electron Microscopy</span>. IntechOpen, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al.

</span>
<span class="ltx_bibblock">The many faces of robustness: A critical analysis of out-of-distribution generalization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 8340–8349, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, and Valentin Deschaintre.

</span>
<span class="ltx_bibblock">Generating procedural materials from text or image prompts.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">ACM SIGGRAPH 2023 Conference Proceedings</span>, pages 1–11, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Anne Humeau-Heurtier.

</span>
<span class="ltx_bibblock">Texture feature extraction methods: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE access</span>, 7:8975–9000, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Adobe Inc.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Image To Material | Substance 3D Sampler</span>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Stephen James, Paul Wohlhart, Mrinal Kalakrishnan, Dmitry Kalashnikov, Alex Irpan, Julian Ibarz, Sergey Levine, Raia Hadsell, and Konstantinos Bousmalis.

</span>
<span class="ltx_bibblock">Sim-to-real via sim-to-sim: Data-efficient robotic grasping via randomized-to-canonical adaptation networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 12627–12637, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4015–4026, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al.

</span>
<span class="ltx_bibblock">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">International journal of computer vision</span>, 128(7):1956–1981, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Manuel Lagunas, Sandra Malpica, Ana Serrano, Elena Garces, Diego Gutierrez, and Belen Masia.

</span>
<span class="ltx_bibblock">A similarity measure for material appearance.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:1905.01562</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhengqin Li, Mohammad Shafiei, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan Chandraker.

</span>
<span class="ltx_bibblock">Inverse rendering for complex indoor scenes: Shape, spatially-varying lighting and svbrdf from a single image.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 2475–2484, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie.

</span>
<span class="ltx_bibblock">A convnet for the 2020s.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 11976–11986, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Amirreza Mahbod, Christine Polak, Katharina Feldmann, Rumsha Khan, Katharina Gelles, Georg Dorffner, Ramona Woitek, Sepideh Hatamikia, and Isabella Ellinger.

</span>
<span class="ltx_bibblock">Nuinsseg: a fully annotated dataset for nuclei instance segmentation in h&amp;e-stained histological images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Scientific Data</span>, 11(1):295, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Amirreza Mahbod, Gerald Schaefer, Benjamin Bancher, Christine Löw, Georg Dorffner, Rupert Ecker, and Isabella Ellinger.

</span>
<span class="ltx_bibblock">Cryonuseg: A dataset for nuclei instance segmentation of cryosectioned h&amp;e-stained histological images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Computers in biology and medicine</span>, 132:104349, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Sergey I Nikolenko.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Synthetic data for deep learning</span>, volume 174.

</span>
<span class="ltx_bibblock">Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Jaehyun Park and Ludwik Kurz.

</span>
<span class="ltx_bibblock">Unsupervised segmentation of textured images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Information sciences</span>, 92(1-4):255–276, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Matt Pharr, Wenzel Jakob, and Greg Humphreys.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">Physically based rendering: From theory to implementation</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">International Conference on Machine Learning</span>, pages 8748–8763. PMLR, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Alexander Raistrick, Lahav Lipson, Zeyu Ma, Lingjie Mei, Mingzhe Wang, Yiming Zuo, Karhan Kayan, Hongyu Wen, Beining Han, Yihan Wang, et al.

</span>
<span class="ltx_bibblock">Infinite photorealistic worlds using procedural generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span>, pages 12630–12641, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Arun V Reddy, Ketul Shah, William Paul, Rohita Mocharla, Judy Hoffman, Kapil D Katyal, Dinesh Manocha, Celso M De Melo, and Rama Chellappa.

</span>
<span class="ltx_bibblock">Synthetic-to-real domain adaptation for action recognition: A dataset and baseline performances.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</span>, pages 11374–11381. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14</span>, pages 102–118. Springer, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Roboflow.

</span>
<span class="ltx_bibblock">Corrosion segment dataset, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Roboflow.

</span>
<span class="ltx_bibblock">Soil type class 2 dataset, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Roboflow.

</span>
<span class="ltx_bibblock">Soil type classification dataset, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa.

</span>
<span class="ltx_bibblock">Generate to adapt: Aligning domains using generative adversarial networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 8503–8512, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.

</span>
<span class="ltx_bibblock">Laion-5b: An open large-scale dataset for training next generation image-text models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2210.08402</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Prafull Sharma, Julien Philip, Michaël Gharbi, Bill Freeman, Fredo Durand, and Valentin Deschaintre.

</span>
<span class="ltx_bibblock">Materialistic: Selecting similar materials in images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">ACM Transactions on Graphics (TOG)</span>, 42(4):1–14, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Lucas de Assis Soares, Klaus Fabian Côco, Patrick Marques Ciarelli, and Evandro Ottoni Teatini Salles.

</span>
<span class="ltx_bibblock">A class-independent texture-separation method based on a pixel-wise binary classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Sensors</span>, 20(18):5432, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Jian Song, Hongruixuan Chen, and Naoto Yokoya.

</span>
<span class="ltx_bibblock">Syntheworld: A large-scale synthetic dataset for land cover mapping and building change detection.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span>, pages 8287–8296, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Jonathan Dyssel Stets, Rasmus Ahrenkiel Lyngby, Jeppe Revall Frisvad, and Anders Bjorholm Dahl.

</span>
<span class="ltx_bibblock">Material-based segmentation of objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Image Analysis: 21st Scandinavian Conference, SCIA 2019, Norrköping, Sweden, June 11–13, 2019, Proceedings 21</span>, pages 152–163. Springer, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Hideyuki Tamura, Shunji Mori, and Takashi Yamawaki.

</span>
<span class="ltx_bibblock">Textural features corresponding to visual perception.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">IEEE Transactions on Systems, man, and cybernetics</span>, 8(6):460–473, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Toggle3D.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Generate PBR Material with AI</span>, 2023.

</span>
<span class="ltx_bibblock">Accessed: 2024-06-03.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Paul Upchurch and Ransen Niu.

</span>
<span class="ltx_bibblock">A dense material segmentation dataset for indoor and outdoor scene parsing.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part VIII</span>, pages 450–466. Springer, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Ivan Ustyuzhaninov, Claudio Michaelis, Wieland Brendel, and Matthias Bethge.

</span>
<span class="ltx_bibblock">One-shot texture segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:1807.02654</span>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Giuseppe Vecchio and Valentin Deschaintre.

</span>
<span class="ltx_bibblock">Matsynth: A modern pbr materials dataset.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2401.06056</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Tuanfeng Y Wang, Hao Su, Qixing Huang, Jingwei Huang, Leonidas J Guibas, and Niloy J Mitra.

</span>
<span class="ltx_bibblock">Unsupervised texture transfer from images to model collections.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">ACM Trans. Graph.</span>, 35(6):177–1, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Joan S Weszka and Azriel Rosenfeld.

</span>
<span class="ltx_bibblock">Threshold evaluation techniques.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">IEEE Transactions on systems, man, and cybernetics</span>, 8(8):622–629, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Dorian Zgraggen.

</span>
<span class="ltx_bibblock">Cgbookcase free pbr textures library.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.cgbookcase.com/" title="">https://www.cgbookcase.com/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo Li, Han Zhao, Bichen Wu, Ravi Krishna, Joseph E Gonzalez, Alberto L Sangiovanni-Vincentelli, Sanjit A Seshia, et al.

</span>
<span class="ltx_bibblock">A review of single-source deep unsupervised visual domain adaptation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">IEEE Transactions on Neural Networks and Learning Systems</span>, 33(2):473–493, 2020.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix: Dataset and Code Access and License</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">The MatSeg dataset, benchmark, full documentation, python readers, croissant metadata, and generation scripts used to create the synthetic data are permanently available at these URLs: <a class="ltx_ref ltx_href" href="https://sites.google.com/view/matseg" title="">1</a>, <a class="ltx_ref ltx_href" href="https://zenodo.org/records/11331618" title="">2</a>, <a class="ltx_ref ltx_href" href="https://e.pcloud.link/publink/show?code=kZxsXTZIk88l74Jb3YeMeOcjOlJJVIqvHj7" title="">3</a>, <a class="ltx_ref ltx_href" href="https://icedrive.net/s/XxgZSif7NgYRbjvDN5w9aiWZ1fR3" title="">4</a>.
<br class="ltx_break"/>Over 300,000 extracted textures and SVBRDF/PBR materials, as well as the documented code used for extraction, are permanently available at these URLs: <a class="ltx_ref ltx_href" href="https://sites.google.com/view/infinitexture/home" title="">1</a>,<a class="ltx_ref ltx_href" href="https://zenodo.org/records/11391127" title="">2</a>, <a class="ltx_ref ltx_href" href="https://e.pcloud.link/publink/show?code=kZON5TZtxLfdvKrVCzn12NADBFRNuCKHm70" title="">3</a>, <a class="ltx_ref ltx_href" href="https://icedrive.net/s/jfY1xSDNkVwtYDYD4FN5wha2A8Pz" title="">4</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Nets trained on the MatSeg Dataset, including: code, trained weights, and evaluation scripts are available at these URLs:<a class="ltx_ref ltx_href" href="https://zenodo.org/records/11536561" title="">1</a>, <a class="ltx_ref ltx_href" href="https://icedrive.net/s/RTS2xXTv1Xzh8tbxYZ9hRkGuWktj" title="">2</a>, <a class="ltx_ref ltx_href" href="https://e.pcloud.link/publink/show?code=XZxIXTZG3APQej3f67JXdRjitC7zHl7jfuV" title="">3</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">The dataset and code are released under the CC0 1.0 Universal (CC0 1.0) Public Domain Dedication. To the extent possible under law, the authors have dedicated all copyright and related and neighbouring rights to this dataset to the public domain worldwide.
The MatSeg 2D and 3D scenes were generated using open-images dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib35" title="">35</a>]</cite> which is licensed under the <a class="ltx_ref ltx_href" href="https://www.apache.org/licenses/LICENSE-2.0" title="">Apache License</a>. For these components, you must comply with the terms of the Apache License. In addition, the MatSege3D dataset uses Shapenet 3D assets with <a class="ltx_ref ltx_href" href="https://github.com/justusschock/shapenet/blob/master/LICENSE" title="">GNU license</a>.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Authors Statement</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We, the author(s), bear full responsibility for the content and data presented in this paper. We confirm that all data and materials used comply with relevant ethical guidelines and legal requirements. In case of any violation of rights, whether related to copyright, privacy, or any other legal claims, we assume complete responsibility.</p>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Appendix: 3D scene building</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Once a map is generated (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#S3.SS1" title="3.1 Unsupervised extraction of maps from images ‣ 3 Data generation: patterns extraction and infusion ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">section</span> <span class="ltx_text ltx_ref_tag">3.1</span></a>), the 3D scene creation follows the standard methods described in previous works<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib21" title="">21</a>]</cite>: 1) Load random 3D objects<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib19" title="">19</a>]</cite>. 2) Load random PBR or BSDF material for each region of the map and use the generated UV map to set the materials on the object surface. 3) Load a panoramic HDRI image to act as a background and illumination source<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib3" title="">3</a>]</cite>. 4) Add random objects, a ground plane, and sometimes random light sources to create shadows and occlusion in the scene. 5) Set a random camera position and render the image. Scene generation was performed in Blender 4.0<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib8" title="">8</a>]</cite> with the Cycles rendering engine. The generation code and the dataset have been made available (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A1" title="Appendix A Appendix: Dataset and Code Access and License ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Hardware</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">The 2D scene generation and materials and textures extraction were done with no special hardware on a simple CPU (i7). For the 3D dataset, a 3090 RTX graphic card was used to accelerate rendering.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Appendix: Asset Sources</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Images for the extraction of maps and textures were taken from the Open Images v7 dataset (Apache License<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib35" title="">35</a>]</cite>). The 3D objects for 3D scene creation were downloaded from Shapenet (GNU license)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib11" title="">11</a>]</cite>. 600 panoramic HDRIs for 3d scenes backgrounds and illuminations were downloaded from HDRI Haven<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib3" title="">3</a>]</cite> with a <a class="ltx_ref ltx_href" href="https://polyhaven.com/license" title="">CC0 license</a>. The rendering was done in Blender 4 with Cycles rendering<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib8" title="">8</a>]</cite>. Textureless (smooth) materials were randomly created by selecting a random value for each property (color, transmission, and reflectivity) in the Blender BSDF node.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Creating PBR/SVBRDF materials by mixing</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">To increase the diversity of the generated PBR materials, we also mixed different PBR materials to generate new materials. This was done using previously used methods<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib61" title="">61</a>]</cite>, by taking the weighted average of the textures maps of the two materials. The mixing weights (ratios) were determined randomly either per map or per material (same weight for all properties map or a different weight for each property mixing).</p>
</div>
<figure class="ltx_figure" id="A6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="586" id="A6.F6.g1" src="extracted/5655111/appendix_Figures/NetStructure.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Net and training, a cross-section view. a) The RGB image is passed through the image encoder to create a descriptor map with a vector per pixel. This vector represents the material in the pixel. b) All the vectors in the region of a given material (based on the GT map) are averaged to create the average descriptor for this material. c) Each individual vector in the predicted descriptor map is matched to the average descriptor of each material in the batch using cosine similarity. This produces a 2D similarity map between each pixel and each material in the batch. d) Stacking the similarity maps and passing them through Softmax gives the predicted material probability map (predicted material per pixel). This map and the GT materials map are used to calculate the final cross-entropy loss. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Net and Training</h2>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">To test the MatSeg dataset, we use it to train a net for class-agnostic soft material segmentation. Previous works have shown that representing materials in images as vector embeddings and using cosine similarity between these vectors to predict how similar the materials are is an effective method for matching materials states<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib21" title="">21</a>]</cite>. This representation also has the advantage that it gives a soft similarity, which is good for representing mixtures, transition states, and partial similarities. To apply this approach to image segmentation, we use the Unet-style neural net, which predicts 128 long descriptors for each pixel in the image (128-layers output map). This 128 vector represents the material for each pixel (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6.F6" title="In Appendix F Creating PBR/SVBRDF materials by mixing ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>). The cosine similarity between the vectors of two pixels in the map gives the similarity of the materials in these two pixels. The average vector in a given region (group of pixels) belonging to the same material gives the average vector embedding for this specific material.</p>
</div>
<section class="ltx_subsection" id="A7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>Training</h3>
<div class="ltx_para ltx_noindent" id="A7.SS1.p1">
<p class="ltx_p" id="A7.SS1.p1.1">The training procedure is depicted in <a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6.F6" title="In Appendix F Creating PBR/SVBRDF materials by mixing ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a> and goes as follows: a) A training batch contains a few randomly sampled images from the MatSeg dataset. b) The images are passed through the net to produce an embedding map with a vector per pixel. These vectors are normalized using L2 normalization. c) The predicted vectors of all pixels belonging to the same material are averaged to create an average vector per material (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6.F6" title="In Appendix F Creating PBR/SVBRDF materials by mixing ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>). To determine which pixels belong to the same material, we use the Ground True (GT) mask of each material and take all the predicted vectors in this region from the descriptor map (<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#A6.F6" title="In Appendix F Creating PBR/SVBRDF materials by mixing ‣ Learning Zero-Shot Material States Segmentation, by Implanting Natural Image Patterns in Synthetic Data"><span class="ltx_text ltx_ref_tag">fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>). e) Each of the average material vectors in the batch is then matched with each vector embedding in the predicted descriptor map. This gives a 2D cosine similarity map between each material in the batch and each pixel in the image. f) The similarity maps for a given image are stacked together and passed through a softmax (temperature of 0.2) to obtain the probability maps for the match between each pixel and each material. g) These probability maps, along with the GT one-hot masks, are used to calculate a cross-entropy loss. Note that the GT maps are turned from soft to hard (one-hot) segments by selecting for each pixel the GT material with the highest concentration (the highest value according to the GT map).</p>
</div>
</section>
<section class="ltx_subsection" id="A7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Net Architecture and Training Parameters </h3>
<div class="ltx_para ltx_noindent" id="A7.SS2.p1">
<p class="ltx_p" id="A7.SS2.p1.1">The net architecture follows that of standard U-net, with a pretrained ConvNext<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2403.03309v5#bib.bib38" title="">38</a>]</cite> base as an encoder, a PSP layer, and three skip connections upsampling layers (code and training scripts supplied). Training was performed using the AdamW optimizer with a learning rate of 1e-5 on a single RTX 3090. The image size was randomly cropped and resized to between 250-900 pixels per dimension. Augmentation includes darkening/lighting, Gaussian blurring, partial and full decoloring, and white noise addition. The full code and trained models have been made available.</p>
</div>
<figure class="ltx_figure" id="A7.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="479" id="A7.F7.g1" src="extracted/5655111/appendix_Figures/AppendixPBR2_Small.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>More samples of the over 200k extracted/generated SVBRDF/PBR materials. </figcaption>
</figure>
<figure class="ltx_figure" id="A7.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="A7.F8.g1" src="extracted/5655111/appendix_Figures/AllTextures_Smaller.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>More samples of the over 200k extracted textures. </figcaption>
</figure>
<figure class="ltx_figure" id="A7.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="399" id="A7.F9.g1" src="extracted/5655111/appendix_Figures/Benchmarkappendix.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>More examples of benchmark images with points-based annotation. Points of the same color are of the exact same material.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="480" id="A7.F10.g1" src="extracted/5655111/appendix_Figures/Appendix_ResultsB_Smaller_FIXED_SMALLER.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>More examples for nets results. Predicted material similarity maps to a given point in the image. The input point is marked green in each panel. For SAM net the results are with 8 input points, 4 positive (marked green) and 4 negative (marked red). The materials similarity map is brighter for high similarity and dark for low similarity (similarity to the input point).</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="527" id="A7.F11.g1" src="extracted/5655111/appendix_Figures/Synthethic_Appendix_SmallerSmaller.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>More examples of 2D and 3D synthetic scenes (top) and their annotations (bottom). In the annotation, each material is marked as a different color. A mixture of materials marked as a mixture of their colors. The background is marked black. </figcaption>
</figure>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun 10 01:04:54 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
