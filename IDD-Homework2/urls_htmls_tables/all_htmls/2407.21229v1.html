<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2407.21229] Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration</title><meta property="og:description" content="Visual Question Answering (VQA) has recently emerged as a potential research domain, captivating the interest of many in the field of artificial intelligence and computer vision. Despite the prevalence of approaches in…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2407.21229">

<!--Generated on Mon Aug  5 15:05:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ngoc Son Nguyen
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Son Van Nguyen
</span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tung Le
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:lttung@fit.hcmus.edu.vn">lttung@fit.hcmus.edu.vn</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.1" class="ltx_p">Visual Question Answering (VQA) has recently emerged as a potential research domain, captivating the interest of many in the field of artificial intelligence and computer vision. Despite the prevalence of approaches in English, there is a notable lack of systems specifically developed for certain languages, particularly Vietnamese. This study aims to bridge this gap by conducting comprehensive experiments on the Vietnamese Visual Question Answering (ViVQA) dataset, demonstrating the effectiveness of our proposed model. In response to community interest, we have developed a model that enhances image representation capabilities, thereby improving overall performance in the ViVQA system. Specifically, our model integrates the Bootstrapping Language-Image Pre-training with frozen unimodal models (BLIP-2) and the convolutional neural network EfficientNet to extract and process both local and global features from images. This integration leverages the strengths of transformer-based architectures for capturing comprehensive contextual information and convolutional networks for detailed local features. By freezing the parameters of these pre-trained models, we significantly reduce the computational cost and training time, while maintaining high performance. This approach significantly improves image representation and enhances the performance of existing VQA systems. We then leverage a multi-modal fusion module based on a general-purpose multi-modal foundation model (BEiT-3) to fuse the information between visual and textual features. Our experimental findings demonstrate that our model surpasses competing baselines, achieving promising performance. This is particularly evident in its accuracy of <math id="id1.1.m1.1" class="ltx_Math" alttext="71.04\%" display="inline"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml"><mn id="id1.1.m1.1.1.2" xref="id1.1.m1.1.1.2.cmml">71.04</mn><mo id="id1.1.m1.1.1.1" xref="id1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><apply id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"><csymbol cd="latexml" id="id1.1.m1.1.1.1.cmml" xref="id1.1.m1.1.1.1">percent</csymbol><cn type="float" id="id1.1.m1.1.1.2.cmml" xref="id1.1.m1.1.1.2">71.04</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">71.04\%</annotation></semantics></math> on the test set of the ViVQA dataset, marking a significant advancement in our research area. The code is available at <a target="_blank" href="https://github.com/nngocson2002/ViVQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nngocson2002/ViVQA</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Visual Question Answering , ViVQA , EfficientNet , BLIP-2 , Convolutional.

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Computers and Electrical Engineering</span></span></span>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p1.2" class="ltx_p">[1]organization=Faculty of Mathematics and Computer Science, University of Science,city=
<br class="ltx_break">Ho Chi Minh,
country=Vietnam
</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p2.2" class="ltx_p">[2]organization=Faculty of Information Technology, University of Science,city=Ho Chi Minh,
country=Vietnam
</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p3.2" class="ltx_p">[3]organization=Vietnam National University,city=Ho Chi Minh,
country=Vietnam
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The rapid enhancement of multi-modal processing in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> has not only opened up vast potential but has also considered a range of remarkable challenges. Among these, the field of VQA emerges as a critical problem in comprehending multi-modalities. This task entails empowering computers to interpret and respond to queries based on the content of images, necessitating a synergistic integration of advancements in both image processing and natural language processing. Despite its complexities, VQA is an essential and challenging task in the ongoing evolution of multimedia data. It represents a significant step towards understanding and integrating diverse forms of information, a cornerstone in the development of more intelligent and versatile Artificial Intelligence (AI) systems.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">However, a notable challenge is that the majority of datasets used to train current VQA models are in English. Most research efforts in this field have prioritized languages with abundant resources, leaving low-resource languages like Vietnamese largely overlooked. This presents significant issues regarding the applicability of VQA models to questions written in Vietnamese and their ability to comprehend Vietnamese culture. In this context, focusing on the ViVQA task is invaluable, as it plays an important role in driving the development of innovative approaches in this specific domain. To address this gap, an increasing number of VQA models for Vietnamese have been established, such as those proposed in recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, significantly contributing to this field. Our work is also no exception, aiming to tackle the ViVQA challenge by developing a novel system for ViVQA that leverages models based on the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, which have been pre-trained on various tasks related to both vision and language, such as UNITER  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, and Flamingo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">A fundamental aspect of VQA systems is the effective extraction and representation of textual and visual features in meaningful ways. The development of pre-trained models in text and image processing plays a crucial role in this aspect. However, it is essential to recognize that each type of model brings unique strengths to the table and should be evaluated from diverse perspectives. In our model, multi-modal features are intricately connected and integrated through a specialized fusion module. Building on the successes of previous models, our approach focuses on refining the semantic representations of uni-modal.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Although the Transformer architecture is renowned for its ability to analyze complete images, it sometimes overlooks crucial small details. This oversight is a significant challenge in VQA, as these details often contain vital information for understanding and answering questions. Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> contended that a VQA model relied on the Transformer architecture, exclusively considering global dependencies, is unable to effectively capture image context information. Through studies, they also proved that using both local and global features in image processing improves performance. Clearly, it is essential to consider the contextual representation of images from both local and global perspectives. Therefore, with this consideration in mind, we propose an innovative integration of transformer-based and convolutional methods in image processing to enhance the feature extraction module in VQA. This integration aims to enhance the visual information by combining the detailed local features extracted by the Convolutional Neural Network (CNN) with the broad global features derived from Vision Transformer-based models. Our novel component has the potential to evolve alongside advancements in visual feature extraction models by incorporating current improvements into our sophisticated combination. The promising results on the Vietnamese VQA dataset demonstrate that our proposed approach is more efficient than existing competitive baselines and plays an important role in representing images in VQA processing.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper makes several pivotal contributions to the field of ViVQA, highlighted as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose an innovative methodological advancement by integrating transformer-based and convolutional methods in image processing. Our novel representation consists of local and global consideration through the content of images.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Our novel visual feature extraction is integrated into the ViVQA approach, employing the latest fusion module to deploy the efficient system.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Through the detailed experimental analysis and comparative results, our method demonstrates the potential performance over existing competitive baselines. This achievement is a significant stride in leveraging multi-view feature representation, enhancing the understanding of different modalities within ViVQA.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Overview</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">VQA methods typically consist of three primary components: modules for feature extraction from textual and visual inputs, a multi-modal fusion module, and a classifier or generator module. Initially, Antol et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> introduced the VQA task alongside a simple baseline, utilizing VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and Long Short-Term Memory (LSTM) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> for visual and textual feature extraction, respectively. They then fused these features into a joint representation for classifying candidate answers. Subsequently, Yuling Xi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> introduced an approach focusing on object relationships within images to address VQA tasks. Their method utilized a question-based attention mechanism to highlight relevant object regions, enabling accurate responses to questions regarding image content. Additionally, Yirui Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> proposed a baseline method emphasizing both global and local object relationships, underscoring the significance of comprehensive relation reasoning. Their approach leveraged information from the entire image for global understanding while the local aspect focused on modeling relationships among multiple objects to facilitate answer generation. Notably, these baselines all share the commonality of employing pre-trained CNN models like ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, and FasterRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for image feature extraction. In terms of text embedding, pre-trained word embeddings such as GloVe <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> or Word2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> are often used along with LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or Gated Recurrent Unit (GRU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> networks to extract linguistic features. In recent years, researchers have increasingly turned to Transformer-based models such as ViLBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, X-LXMERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, and VLMo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Consequently, later works often rely on these models, leveraging the capabilities of pre-trained models.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The primary focus of this paper is the examination of monolingual ViVQA. In previous research, Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> introduced a model that utilized Hierarchical Co-Attention <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, establishing the first baseline for ViVQA dataset. Their experiments demonstrated that their system outperformed two baseline models, LSTM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and BiLSTM, on the ViQA dataset. Following this, with the advent of transformers, Nguyen-Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> proposed an approach that incorporates a Bi-directional Cross-attention architecture. This model was designed to more effectively comprehend the complex interplay between visual and linguistic features, specifically in the Vietnamese context. Another notable contribution by Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> was the introduction of Parallel Attention Transformer (PAT), a novel multimodal learning scheme. This method effectively integrates the advantages of grammar and contextual understanding in Vietnamese. More recently, Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> proposed leveraging BEiT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as a fusion encoder to intricately model the deep interactions between images and questions. Furthermore, they employed BARTpho <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, a monolingual sequence-to-sequence model pre-trained for Vietnamese, to enhance question representations. Remarkably, this model has achieved the current state-of-the-art performance on the ViVQA dataset. This research significantly advances the field of VQA, particularly in the Vietnamese language context, showcasing the effectiveness of novel models in capturing the nuanced interactions between visual and linguistic elements.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>ViVQA Models in Previous Studies</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this section, we will briefly present two methods that currently achieve the best performance on the ViVQA task and point out their weaknesses, which motivates our study.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Multimodal Contextual Attention Model</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="2022" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mn id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">2022</mn><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><cn type="integer" id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">2022</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">2022</annotation></semantics></math>, Anh Duc Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> introduced the Multi-vision Contextual Attention (MCA) model, presenting an innovative approach to image feature extraction that seamlessly integrates both global and local features. This model excels in learning attentional embeddings, capturing diverse contexts from both images and questions while being mutually guided.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/mca.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="207" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.6.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.7.2" class="ltx_text" style="font-size:90%;">Architecture of Multi-vision Contextual Attention, which consists of four main modules: the features extraction networks to learn the image (<span id="S2.F1.7.2.1" class="ltx_text ltx_font_bold">the red-colored shapes</span>) and question (<span id="S2.F1.7.2.2" class="ltx_text ltx_font_bold">the orange-colored shape</span>) understanding; the multi-branch contextual attention fusion (<span id="S2.F1.7.2.3" class="ltx_text ltx_font_bold">the blue-colored shape</span>) to fuse visual and textual features; and the classifier (<span id="S2.F1.7.2.4" class="ltx_text ltx_font_bold">the green-colored shape</span>) to predict the answer.</span></figcaption>
</figure>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">As depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.2.1 Multimodal Contextual Attention Model ‣ 2.2 ViVQA Models in Previous Studies ‣ 2 Related Works ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the image initially extracts local features corresponding to specific regions or parts of objects in the image, acquired through the utilization of ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a convolutional neural network. Simultaneously, global features encapsulate fundamental information about the contextual surroundings within the image, including associations among objects, and are derived using the Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. For question embedding, PhoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is employed to extract features from questions. Based on the obtained visual and textual features, a multi-branch contextual attention fusion hierarchically integrates visual and textual attention features, guiding each other and fusing them into a multimodal feature representation. This pioneering model represents a significant advancement in feature extraction, showcasing the integration of diverse contextual elements for a more comprehensive understanding of multimodal data.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.1" class="ltx_p">Despite the combination of CNN and a Transformer in this model, it’s worth noting that ResNet is not the latest CNN model. In our proposed architecture, we aim to innovate by combining both local and global features inspired by the MCA model. Through this approach, we anticipate achieving better performance.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>BARTPhoBEiT</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> have recently introduced an innovative methodology that integrates pre-trained sequence-to-sequence and image Transformers customized for the Vietnamese language. This pioneering approach draws inspiration from BEiT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, serving as the fundamental framework for their model.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/bartphobeit2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.6.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S2.F2.7.2" class="ltx_text" style="font-size:90%;">The architecture of the BARTPhoBEiT model, which comprise three main modules: the image embedding (<span id="S2.F2.7.2.1" class="ltx_text ltx_font_bold">the red-colored shape</span>), the question embedding (<span id="S2.F2.7.2.2" class="ltx_text ltx_font_bold">the orange-colored shape</span>); the multimodal fusion (<span id="S2.F2.7.2.3" class="ltx_text ltx_font_bold">the blue-colored shape</span>); and the classifier (<span id="S2.F2.7.2.4" class="ltx_text ltx_font_bold">the green-colored shape</span>) to predict the answer.</span></figcaption>
</figure>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">As shown in Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2.2 BARTPhoBEiT ‣ 2.2 ViVQA Models in Previous Studies ‣ 2 Related Works ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the image embedding module utilizes BEiT v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> to extract global features from images, while for question embedding, BARTPho <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> is employed to extract question features. Subsequently, the visual and question features are fused using a Multiway Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. These Transformers dynamically route information to the appropriate expert for each modality, ensuring the capture of modality-specific details. This unique feature significantly enhances the model’s efficacy in handling diverse data modalities, emphasizing its versatility in cross-modal applications. Furthermore, the robustness of this model relies on the Transformer architecture, enabling seamless integration of an extensive volume of pre-trained data across diverse domains, resulting in state-of-the-art performance on the ViVQA dataset.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">Despite the commendable results of their work, we have identified the complexity of image processing and the absence of information about local features as areas for further exploration. Building on the insights from their study, we are motivated to focus on the development of methods that enhance the performance of image processing, particularly for the ViVQA task.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Dataset for ViVQA</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In the context of Vietnamese, the development of data for VQA is not as advanced as it is in English. The available resources are limited, presenting a scarcity of data specifically tailored for ViVQA. This dearth underscores the challenges and gaps in the field, as only a modest amount of data is currently accessible for research and improvement in ViVQA. It is challenging to create models that can effectively generalize to new inputs when there is insufficient data to capture the entire variety of text and images. Furthermore, a significant element influencing the models’ performance is the dataset’s quality. Predictions might be erroneous and ineffective generalization can result from incomplete and low-quality data. Within the scope of this study, our primary focus is on monolingual Vietnamese datasets. We will survey datasets that align with our research objectives to serve as the foundation for our model.</p>
</div>
<section id="S2.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>OpenViVQA Dataset</h4>

<div id="S2.SS3.SSS1.p1" class="ltx_para">
<p id="S2.SS3.SSS1.p1.1" class="ltx_p">In <math id="S2.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="2023" display="inline"><semantics id="S2.SS3.SSS1.p1.1.m1.1a"><mn id="S2.SS3.SSS1.p1.1.m1.1.1" xref="S2.SS3.SSS1.p1.1.m1.1.1.cmml">2023</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p1.1.m1.1b"><cn type="integer" id="S2.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p1.1.m1.1.1">2023</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p1.1.m1.1c">2023</annotation></semantics></math>, Nghia et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> introduced the OpenViVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> aims to showcase the inherently open-ended nature of both questions and answers. In order to seamlessly align with the intricacies of the Vietnamese language, the OpenViVQA dataset thoughtfully incorporates a diverse array of images captured within the rich tapestry of Vietnam’s landscapes. This deliberate inclusion not only serves to showcase the linguistic nuances but also aims to highlight the myriad characteristics that distinctly define Vietnam in comparison to other regions. By strategically curating scenes from Vietnam, the dataset offers a comprehensive representation of the country’s unique cultural, environmental, and visual elements, contributing to a nuanced understanding within the realm of VQA research.</p>
</div>
<div id="S2.SS3.SSS1.p2" class="ltx_para">
<p id="S2.SS3.SSS1.p2.4" class="ltx_p">The dataset is divided into two distinct components: <math id="S2.SS3.SSS1.p2.1.m1.1" class="ltx_Math" alttext="``" display="inline"><semantics id="S2.SS3.SSS1.p2.1.m1.1a"><mrow id="S2.SS3.SSS1.p2.1.m1.1.1" xref="S2.SS3.SSS1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.SS3.SSS1.p2.1.m1.1.1.2" xref="S2.SS3.SSS1.p2.1.m1.1.1.2.cmml">`</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS1.p2.1.m1.1.1.1" xref="S2.SS3.SSS1.p2.1.m1.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS3.SSS1.p2.1.m1.1.1.3" xref="S2.SS3.SSS1.p2.1.m1.1.1.3.cmml">`</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.1.m1.1b"><apply id="S2.SS3.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1"><times id="S2.SS3.SSS1.p2.1.m1.1.1.1.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1.1"></times><ci id="S2.SS3.SSS1.p2.1.m1.1.1.2.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1.2">`</ci><ci id="S2.SS3.SSS1.p2.1.m1.1.1.3.cmml" xref="S2.SS3.SSS1.p2.1.m1.1.1.3">`</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.1.m1.1c">``</annotation></semantics></math>Text QA<math id="S2.SS3.SSS1.p2.2.m2.1" class="ltx_Math" alttext='"' display="inline"><semantics id="S2.SS3.SSS1.p2.2.m2.1a"><mi mathvariant="normal" id="S2.SS3.SSS1.p2.2.m2.1.1" xref="S2.SS3.SSS1.p2.2.m2.1.1.cmml">"</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.2.m2.1b"><ci id="S2.SS3.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS1.p2.2.m2.1.1">"</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.2.m2.1c">"</annotation></semantics></math> and <math id="S2.SS3.SSS1.p2.3.m3.1" class="ltx_Math" alttext="``" display="inline"><semantics id="S2.SS3.SSS1.p2.3.m3.1a"><mrow id="S2.SS3.SSS1.p2.3.m3.1.1" xref="S2.SS3.SSS1.p2.3.m3.1.1.cmml"><mi mathvariant="normal" id="S2.SS3.SSS1.p2.3.m3.1.1.2" xref="S2.SS3.SSS1.p2.3.m3.1.1.2.cmml">`</mi><mo lspace="0em" rspace="0em" id="S2.SS3.SSS1.p2.3.m3.1.1.1" xref="S2.SS3.SSS1.p2.3.m3.1.1.1.cmml">​</mo><mi mathvariant="normal" id="S2.SS3.SSS1.p2.3.m3.1.1.3" xref="S2.SS3.SSS1.p2.3.m3.1.1.3.cmml">`</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.3.m3.1b"><apply id="S2.SS3.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS3.SSS1.p2.3.m3.1.1"><times id="S2.SS3.SSS1.p2.3.m3.1.1.1.cmml" xref="S2.SS3.SSS1.p2.3.m3.1.1.1"></times><ci id="S2.SS3.SSS1.p2.3.m3.1.1.2.cmml" xref="S2.SS3.SSS1.p2.3.m3.1.1.2">`</ci><ci id="S2.SS3.SSS1.p2.3.m3.1.1.3.cmml" xref="S2.SS3.SSS1.p2.3.m3.1.1.3">`</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.3.m3.1c">``</annotation></semantics></math>Non-text QA<math id="S2.SS3.SSS1.p2.4.m4.1" class="ltx_Math" alttext='"' display="inline"><semantics id="S2.SS3.SSS1.p2.4.m4.1a"><mi mathvariant="normal" id="S2.SS3.SSS1.p2.4.m4.1.1" xref="S2.SS3.SSS1.p2.4.m4.1.1.cmml">"</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS1.p2.4.m4.1b"><ci id="S2.SS3.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS3.SSS1.p2.4.m4.1.1">"</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS1.p2.4.m4.1c">"</annotation></semantics></math>. Specifically, the non-text QAs are designed to extract information pertinent to objects, encompassing their attributes and relational aspects. On the contrary, it is categorized as Text QA if it effectively leverages textual information emanating from the images themselves or employs the words present within the images as a means to pose questions. As a result of the inclusion of Text QA within the dataset, consequently making Optical Character Recognition (OCR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> a more involved process, our primary focus does not lie in this domain. Our paramount focus is intricately tied to the detailed examination of large objects depicted in the images. Thus, the OpenViVQA dataset does not align with our preference for this particular study.</p>
</div>
</section>
<section id="S2.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>ViVQA Dataset</h4>

<div id="S2.SS3.SSS2.p1" class="ltx_para">
<p id="S2.SS3.SSS2.p1.1" class="ltx_p">The inaugural dataset in ViVQA, crafted by Tran et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> in <math id="S2.SS3.SSS2.p1.1.m1.1" class="ltx_Math" alttext="2021" display="inline"><semantics id="S2.SS3.SSS2.p1.1.m1.1a"><mn id="S2.SS3.SSS2.p1.1.m1.1.1" xref="S2.SS3.SSS2.p1.1.m1.1.1.cmml">2021</mn><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p1.1.m1.1b"><cn type="integer" id="S2.SS3.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p1.1.m1.1.1">2021</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p1.1.m1.1c">2021</annotation></semantics></math>, stands as a pioneering contribution to the field. Drawing inspiration from established English VQA benchmarks and COCO-QA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, this dataset has evolved into a widely embraced benchmark for related research endeavors. The creation process involved leveraging machine translation for the conversion of questions and answers, followed by meticulous manual validation to ensure correctness and linguistic fluency in the translated content.</p>
</div>
<div id="S2.SS3.SSS2.p2" class="ltx_para">
<p id="S2.SS3.SSS2.p2.7" class="ltx_p">The ViVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> encompasses a collection of <math id="S2.SS3.SSS2.p2.1.m1.2" class="ltx_Math" alttext="10,328" display="inline"><semantics id="S2.SS3.SSS2.p2.1.m1.2a"><mrow id="S2.SS3.SSS2.p2.1.m1.2.3.2" xref="S2.SS3.SSS2.p2.1.m1.2.3.1.cmml"><mn id="S2.SS3.SSS2.p2.1.m1.1.1" xref="S2.SS3.SSS2.p2.1.m1.1.1.cmml">10</mn><mo id="S2.SS3.SSS2.p2.1.m1.2.3.2.1" xref="S2.SS3.SSS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS3.SSS2.p2.1.m1.2.2" xref="S2.SS3.SSS2.p2.1.m1.2.2.cmml">328</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.1.m1.2b"><list id="S2.SS3.SSS2.p2.1.m1.2.3.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.2.3.2"><cn type="integer" id="S2.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS3.SSS2.p2.1.m1.1.1">10</cn><cn type="integer" id="S2.SS3.SSS2.p2.1.m1.2.2.cmml" xref="S2.SS3.SSS2.p2.1.m1.2.2">328</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.1.m1.2c">10,328</annotation></semantics></math> images paired with <math id="S2.SS3.SSS2.p2.2.m2.2" class="ltx_Math" alttext="15,000" display="inline"><semantics id="S2.SS3.SSS2.p2.2.m2.2a"><mrow id="S2.SS3.SSS2.p2.2.m2.2.3.2" xref="S2.SS3.SSS2.p2.2.m2.2.3.1.cmml"><mn id="S2.SS3.SSS2.p2.2.m2.1.1" xref="S2.SS3.SSS2.p2.2.m2.1.1.cmml">15</mn><mo id="S2.SS3.SSS2.p2.2.m2.2.3.2.1" xref="S2.SS3.SSS2.p2.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS3.SSS2.p2.2.m2.2.2" xref="S2.SS3.SSS2.p2.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.2.m2.2b"><list id="S2.SS3.SSS2.p2.2.m2.2.3.1.cmml" xref="S2.SS3.SSS2.p2.2.m2.2.3.2"><cn type="integer" id="S2.SS3.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS3.SSS2.p2.2.m2.1.1">15</cn><cn type="integer" id="S2.SS3.SSS2.p2.2.m2.2.2.cmml" xref="S2.SS3.SSS2.p2.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.2.m2.2c">15,000</annotation></semantics></math> corresponding questions, intricately tailored to reflect the content within the images. To facilitate experimentation and evaluation, the dataset underwent a random partitioning into training and test sets, maintaining an <math id="S2.SS3.SSS2.p2.3.m3.1" class="ltx_Math" alttext="80:20" display="inline"><semantics id="S2.SS3.SSS2.p2.3.m3.1a"><mrow id="S2.SS3.SSS2.p2.3.m3.1.1" xref="S2.SS3.SSS2.p2.3.m3.1.1.cmml"><mn id="S2.SS3.SSS2.p2.3.m3.1.1.2" xref="S2.SS3.SSS2.p2.3.m3.1.1.2.cmml">80</mn><mo lspace="0.278em" rspace="0.278em" id="S2.SS3.SSS2.p2.3.m3.1.1.1" xref="S2.SS3.SSS2.p2.3.m3.1.1.1.cmml">:</mo><mn id="S2.SS3.SSS2.p2.3.m3.1.1.3" xref="S2.SS3.SSS2.p2.3.m3.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.3.m3.1b"><apply id="S2.SS3.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS3.SSS2.p2.3.m3.1.1"><ci id="S2.SS3.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS3.SSS2.p2.3.m3.1.1.1">:</ci><cn type="integer" id="S2.SS3.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS3.SSS2.p2.3.m3.1.1.2">80</cn><cn type="integer" id="S2.SS3.SSS2.p2.3.m3.1.1.3.cmml" xref="S2.SS3.SSS2.p2.3.m3.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.3.m3.1c">80:20</annotation></semantics></math> ratio. Noteworthy is the dataset’s categorization of questions into four distinct types: Object, Number, Color, and Location, constituting <math id="S2.SS3.SSS2.p2.4.m4.1" class="ltx_Math" alttext="41.55\%" display="inline"><semantics id="S2.SS3.SSS2.p2.4.m4.1a"><mrow id="S2.SS3.SSS2.p2.4.m4.1.1" xref="S2.SS3.SSS2.p2.4.m4.1.1.cmml"><mn id="S2.SS3.SSS2.p2.4.m4.1.1.2" xref="S2.SS3.SSS2.p2.4.m4.1.1.2.cmml">41.55</mn><mo id="S2.SS3.SSS2.p2.4.m4.1.1.1" xref="S2.SS3.SSS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.4.m4.1b"><apply id="S2.SS3.SSS2.p2.4.m4.1.1.cmml" xref="S2.SS3.SSS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.4.m4.1.1.1.cmml" xref="S2.SS3.SSS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S2.SS3.SSS2.p2.4.m4.1.1.2.cmml" xref="S2.SS3.SSS2.p2.4.m4.1.1.2">41.55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.4.m4.1c">41.55\%</annotation></semantics></math>, <math id="S2.SS3.SSS2.p2.5.m5.1" class="ltx_Math" alttext="14.81\%" display="inline"><semantics id="S2.SS3.SSS2.p2.5.m5.1a"><mrow id="S2.SS3.SSS2.p2.5.m5.1.1" xref="S2.SS3.SSS2.p2.5.m5.1.1.cmml"><mn id="S2.SS3.SSS2.p2.5.m5.1.1.2" xref="S2.SS3.SSS2.p2.5.m5.1.1.2.cmml">14.81</mn><mo id="S2.SS3.SSS2.p2.5.m5.1.1.1" xref="S2.SS3.SSS2.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.5.m5.1b"><apply id="S2.SS3.SSS2.p2.5.m5.1.1.cmml" xref="S2.SS3.SSS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.5.m5.1.1.1.cmml" xref="S2.SS3.SSS2.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S2.SS3.SSS2.p2.5.m5.1.1.2.cmml" xref="S2.SS3.SSS2.p2.5.m5.1.1.2">14.81</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.5.m5.1c">14.81\%</annotation></semantics></math>, <math id="S2.SS3.SSS2.p2.6.m6.1" class="ltx_Math" alttext="20.82\%" display="inline"><semantics id="S2.SS3.SSS2.p2.6.m6.1a"><mrow id="S2.SS3.SSS2.p2.6.m6.1.1" xref="S2.SS3.SSS2.p2.6.m6.1.1.cmml"><mn id="S2.SS3.SSS2.p2.6.m6.1.1.2" xref="S2.SS3.SSS2.p2.6.m6.1.1.2.cmml">20.82</mn><mo id="S2.SS3.SSS2.p2.6.m6.1.1.1" xref="S2.SS3.SSS2.p2.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.6.m6.1b"><apply id="S2.SS3.SSS2.p2.6.m6.1.1.cmml" xref="S2.SS3.SSS2.p2.6.m6.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.6.m6.1.1.1.cmml" xref="S2.SS3.SSS2.p2.6.m6.1.1.1">percent</csymbol><cn type="float" id="S2.SS3.SSS2.p2.6.m6.1.1.2.cmml" xref="S2.SS3.SSS2.p2.6.m6.1.1.2">20.82</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.6.m6.1c">20.82\%</annotation></semantics></math>, and <math id="S2.SS3.SSS2.p2.7.m7.1" class="ltx_Math" alttext="22.82\%" display="inline"><semantics id="S2.SS3.SSS2.p2.7.m7.1a"><mrow id="S2.SS3.SSS2.p2.7.m7.1.1" xref="S2.SS3.SSS2.p2.7.m7.1.1.cmml"><mn id="S2.SS3.SSS2.p2.7.m7.1.1.2" xref="S2.SS3.SSS2.p2.7.m7.1.1.2.cmml">22.82</mn><mo id="S2.SS3.SSS2.p2.7.m7.1.1.1" xref="S2.SS3.SSS2.p2.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS2.p2.7.m7.1b"><apply id="S2.SS3.SSS2.p2.7.m7.1.1.cmml" xref="S2.SS3.SSS2.p2.7.m7.1.1"><csymbol cd="latexml" id="S2.SS3.SSS2.p2.7.m7.1.1.1.cmml" xref="S2.SS3.SSS2.p2.7.m7.1.1.1">percent</csymbol><cn type="float" id="S2.SS3.SSS2.p2.7.m7.1.1.2.cmml" xref="S2.SS3.SSS2.p2.7.m7.1.1.2">22.82</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS2.p2.7.m7.1c">22.82\%</annotation></semantics></math> of the total questions, respectively.</p>
</div>
<div id="S2.SS3.SSS2.p3" class="ltx_para">
<p id="S2.SS3.SSS2.p3.1" class="ltx_p">This dataset is particularly pertinent to our research objectives as our paper predominantly delves into advanced image processing techniques. Our primary focus lies in extracting detailed information related to objects, encompassing their attributes, relational aspects, and the overall landscape depicted across the image. Consequently, we have deliberately chosen this dataset for our study, opting against OpenViVQA. This decision is attributed to the existence of questions related to textual content within images in OpenViVQA, which does not align with the specific aspects of image analysis and processing we are exploring in our research.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.4" class="ltx_p">In this paper, we approach the ViVQA task as a classification problem. Specifically, given an image <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p1.1.m1.1a"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">I</annotation></semantics></math> and a question <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="Q" display="inline"><semantics id="S3.p1.2.m2.1a"><mi id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">Q</annotation></semantics></math>, the goal is to determine the most probable answer <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="\hat{a}" display="inline"><semantics id="S3.p1.3.m3.1a"><mover accent="true" id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml"><mi id="S3.p1.3.m3.1.1.2" xref="S3.p1.3.m3.1.1.2.cmml">a</mi><mo id="S3.p1.3.m3.1.1.1" xref="S3.p1.3.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><apply id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1"><ci id="S3.p1.3.m3.1.1.1.cmml" xref="S3.p1.3.m3.1.1.1">^</ci><ci id="S3.p1.3.m3.1.1.2.cmml" xref="S3.p1.3.m3.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">\hat{a}</annotation></semantics></math> from a predefined set of answer options <math id="S3.p1.4.m4.1" class="ltx_Math" alttext="A" display="inline"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">A</annotation></semantics></math>:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E1.m1.4" class="ltx_Math" alttext="\hat{a}=\arg\max_{a\in A}P(a|I,Q,A)" display="block"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mover accent="true" id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml"><mi id="S3.E1.m1.4.4.3.2" xref="S3.E1.m1.4.4.3.2.cmml">a</mi><mo id="S3.E1.m1.4.4.3.1" xref="S3.E1.m1.4.4.3.1.cmml">^</mo></mover><mo id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml">=</mo><mrow id="S3.E1.m1.4.4.1" xref="S3.E1.m1.4.4.1.cmml"><mrow id="S3.E1.m1.4.4.1.3" xref="S3.E1.m1.4.4.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.1" xref="S3.E1.m1.4.4.1.3.1.cmml">arg</mi><mo lspace="0.167em" id="S3.E1.m1.4.4.1.3a" xref="S3.E1.m1.4.4.1.3.cmml">⁡</mo><mrow id="S3.E1.m1.4.4.1.3.2" xref="S3.E1.m1.4.4.1.3.2.cmml"><munder id="S3.E1.m1.4.4.1.3.2.1" xref="S3.E1.m1.4.4.1.3.2.1.cmml"><mi id="S3.E1.m1.4.4.1.3.2.1.2" xref="S3.E1.m1.4.4.1.3.2.1.2.cmml">max</mi><mrow id="S3.E1.m1.4.4.1.3.2.1.3" xref="S3.E1.m1.4.4.1.3.2.1.3.cmml"><mi id="S3.E1.m1.4.4.1.3.2.1.3.2" xref="S3.E1.m1.4.4.1.3.2.1.3.2.cmml">a</mi><mo id="S3.E1.m1.4.4.1.3.2.1.3.1" xref="S3.E1.m1.4.4.1.3.2.1.3.1.cmml">∈</mo><mi id="S3.E1.m1.4.4.1.3.2.1.3.3" xref="S3.E1.m1.4.4.1.3.2.1.3.3.cmml">A</mi></mrow></munder><mo lspace="0.167em" id="S3.E1.m1.4.4.1.3.2a" xref="S3.E1.m1.4.4.1.3.2.cmml">⁡</mo><mi id="S3.E1.m1.4.4.1.3.2.2" xref="S3.E1.m1.4.4.1.3.2.2.cmml">P</mi></mrow></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.4.4.1.2" xref="S3.E1.m1.4.4.1.2.cmml">​</mo><mrow id="S3.E1.m1.4.4.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.4.4.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.cmml"><mi id="S3.E1.m1.4.4.1.1.1.1.2" xref="S3.E1.m1.4.4.1.1.1.1.2.cmml">a</mi><mo fence="false" id="S3.E1.m1.4.4.1.1.1.1.1" xref="S3.E1.m1.4.4.1.1.1.1.1.cmml">|</mo><mrow id="S3.E1.m1.4.4.1.1.1.1.3.2" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml"><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">I</mi><mo id="S3.E1.m1.4.4.1.1.1.1.3.2.1" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml">Q</mi><mo id="S3.E1.m1.4.4.1.1.1.1.3.2.2" xref="S3.E1.m1.4.4.1.1.1.1.3.1.cmml">,</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">A</mi></mrow></mrow><mo stretchy="false" id="S3.E1.m1.4.4.1.1.1.3" xref="S3.E1.m1.4.4.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"></eq><apply id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"><ci id="S3.E1.m1.4.4.3.1.cmml" xref="S3.E1.m1.4.4.3.1">^</ci><ci id="S3.E1.m1.4.4.3.2.cmml" xref="S3.E1.m1.4.4.3.2">𝑎</ci></apply><apply id="S3.E1.m1.4.4.1.cmml" xref="S3.E1.m1.4.4.1"><times id="S3.E1.m1.4.4.1.2.cmml" xref="S3.E1.m1.4.4.1.2"></times><apply id="S3.E1.m1.4.4.1.3.cmml" xref="S3.E1.m1.4.4.1.3"><arg id="S3.E1.m1.4.4.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.1"></arg><apply id="S3.E1.m1.4.4.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2"><apply id="S3.E1.m1.4.4.1.3.2.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.1.3.2.1.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1">subscript</csymbol><max id="S3.E1.m1.4.4.1.3.2.1.2.cmml" xref="S3.E1.m1.4.4.1.3.2.1.2"></max><apply id="S3.E1.m1.4.4.1.3.2.1.3.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3"><in id="S3.E1.m1.4.4.1.3.2.1.3.1.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3.1"></in><ci id="S3.E1.m1.4.4.1.3.2.1.3.2.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3.2">𝑎</ci><ci id="S3.E1.m1.4.4.1.3.2.1.3.3.cmml" xref="S3.E1.m1.4.4.1.3.2.1.3.3">𝐴</ci></apply></apply><ci id="S3.E1.m1.4.4.1.3.2.2.cmml" xref="S3.E1.m1.4.4.1.3.2.2">𝑃</ci></apply></apply><apply id="S3.E1.m1.4.4.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.4.4.1.1.1.1.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.1">conditional</csymbol><ci id="S3.E1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E1.m1.4.4.1.1.1.1.2">𝑎</ci><list id="S3.E1.m1.4.4.1.1.1.1.3.1.cmml" xref="S3.E1.m1.4.4.1.1.1.1.3.2"><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝐼</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">𝑄</ci><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">𝐴</ci></list></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\hat{a}=\arg\max_{a\in A}P(a|I,Q,A)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">In this section, we present the detail of our ViVQA model architecture, illustrated in Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, drawing inspiration from BARTPhoBEiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> with specific modifications in the visual processing aspect. We also provide detailed descriptions of the main image processing steps employed in our model. Our proposal advocates for the implementation of visual processing with frozen parameters, effectively reducing both the model parameters and training costs.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/ourmodelfinal.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="297" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Overview of our architecture, which consists of four main components: (1) the Image Embedding module, (2) the Question Embedding module, (3) the Multi-modal Fusion module, and (4) the Classifier module for predicting the answer.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Image Embedding Module</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Utilizing the capabilities of pre-trained models, our image embedding module proves to be both effective and powerful, capturing valuable features through pretraining tasks. We employ both BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> to extract visual features from both local and global respectively, amalgamating them to derive the final representation for the image. A distinctive feature of our module is its separation from the overall architecture, ensuring that its parameters remain static throughout the training process. This design choice significantly reduces the overall number of parameters in our model and shortens the training duration.</p>
</div>
<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>BLIP-2</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">The first model in the BLIP family, Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (BLIP), was proposed by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. BLIP introduced a novel Vision-Language Pre-training (VLP) framework that supports a wider range of downstream tasks than existing methods. This is achieved by jointly training models on extensive language and image datasets, thereby leveraging the complementary strengths of both modalities.</p>
</div>
<div id="S3.SS1.SSS1.p2" class="ltx_para">
<p id="S3.SS1.SSS1.p2.1" class="ltx_p">Following this, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> proposed BLIP-2, an enhanced version of the original model with several adjustments in both the training strategy and the architecture. BLIP-2 introduces a pioneering method that leverages the capabilities of pre-trained vision models and language models. Unlike BLIP, which involves end-to-end training and often incurs high computational costs and lengthy training durations, BLIP-2 opts to freeze these pre-trained models during the training process. This strategy results in improved performance at a reduced computational cost and addresses the issue of catastrophic forgetting. Moreover, it ensures that the model capitalizes on the rich representations learned by the visual and language model, thereby enhancing its overall performance and effectiveness in comprehending both image and text inputs. With these strengths of BLIP-2, we decided to adopt it to take advantage of global image-level feature extraction.</p>
</div>
<div id="S3.SS1.SSS1.p3" class="ltx_para">
<p id="S3.SS1.SSS1.p3.1" class="ltx_p">The central aim of BLIP-2 is to facilitate mutual understanding between the image and language. To achieve this objective, the Querying Transformer (Q-Former) module is specifically engineered to bridge the gap between vision and language. Following the Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1.1 BLIP-2 ‣ 3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the Q-Former architecture in BLIP-2 incorporates (<span id="S3.SS1.SSS1.p3.1.1" class="ltx_text ltx_font_bold">left</span>) an image transformer utilizing an encoder-only transformer and (<span id="S3.SS1.SSS1.p3.1.2" class="ltx_text ltx_font_bold">right</span>) a text transformer, capable of functioning as both a text encoder and a text decoder. Both the image transformer and text transformer share the same self-attention layers. Given our focus on obtaining image representations for the ViVQA system, we are particularly interested in the image transformer within the Q-Former, which returns the visual features. A specific number of learnable query embeddings serve as input for the image transformer. The queries, each comprising 32 learned query tokens with a dimension of 768, contextualize the query tokens through the cross-attention mechanism with the representations of the image patches encoded by a large (frozen) ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The visual tokens serve as the output of the Q-Former.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/blip2.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The model architecture of Q-Former in the BLIP-2 framework. We utilize the pre-trained Q-Former to extract visual features from the output embeddings of the frozen image encoder.</span></figcaption>
</figure>
<div id="S3.SS1.SSS1.p4" class="ltx_para">
<p id="S3.SS1.SSS1.p4.2" class="ltx_p">Notably, a crucial stage involves representation learning, with the primary aim of pre-training within BLIP-2 being to refine the Q-Former’s ability to enable queries to extract visual representations that are maximally informative regarding the accompanying text. Consequently, the features of the image generated by BLIP-2 encompass an ample amount of information, particularly in terms of global aspects, satisfying our ViVQA system’s needs. The output query representation in <math id="S3.SS1.SSS1.p4.1.m1.1" class="ltx_Math" alttext="\mathbb{R}^{32\times\ 768}" display="inline"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><msup id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS1.p4.1.m1.1.1.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml"><mn id="S3.SS1.SSS1.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS1.p4.1.m1.1.1.3.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS1.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml"> 768</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><apply id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2">ℝ</ci><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3"><times id="S3.SS1.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2">32</cn><cn type="integer" id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3">768</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">\mathbb{R}^{32\times\ 768}</annotation></semantics></math>, significantly smaller than the size of the frozen image features (e.g., 257 × 1024 for ViT-L/14). Thus, the image <math id="S3.SS1.SSS1.p4.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><mi id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><ci id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">I</annotation></semantics></math> representation after BLIP-2 feature extraction can be formulated as:</p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E2.m1.1" class="ltx_Math" alttext="V_{G}=\text{BLIP-2}(I)\in\mathbb{R}^{32\times\ 768}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.2" xref="S3.E2.m1.1.2.cmml"><msub id="S3.E2.m1.1.2.2" xref="S3.E2.m1.1.2.2.cmml"><mi id="S3.E2.m1.1.2.2.2" xref="S3.E2.m1.1.2.2.2.cmml">V</mi><mi id="S3.E2.m1.1.2.2.3" xref="S3.E2.m1.1.2.2.3.cmml">G</mi></msub><mo id="S3.E2.m1.1.2.3" xref="S3.E2.m1.1.2.3.cmml">=</mo><mrow id="S3.E2.m1.1.2.4" xref="S3.E2.m1.1.2.4.cmml"><mtext id="S3.E2.m1.1.2.4.2" xref="S3.E2.m1.1.2.4.2a.cmml">BLIP-2</mtext><mo lspace="0em" rspace="0em" id="S3.E2.m1.1.2.4.1" xref="S3.E2.m1.1.2.4.1.cmml">​</mo><mrow id="S3.E2.m1.1.2.4.3.2" xref="S3.E2.m1.1.2.4.cmml"><mo stretchy="false" id="S3.E2.m1.1.2.4.3.2.1" xref="S3.E2.m1.1.2.4.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E2.m1.1.2.4.3.2.2" xref="S3.E2.m1.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.2.5" xref="S3.E2.m1.1.2.5.cmml">∈</mo><msup id="S3.E2.m1.1.2.6" xref="S3.E2.m1.1.2.6.cmml"><mi id="S3.E2.m1.1.2.6.2" xref="S3.E2.m1.1.2.6.2.cmml">ℝ</mi><mrow id="S3.E2.m1.1.2.6.3" xref="S3.E2.m1.1.2.6.3.cmml"><mn id="S3.E2.m1.1.2.6.3.2" xref="S3.E2.m1.1.2.6.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E2.m1.1.2.6.3.1" xref="S3.E2.m1.1.2.6.3.1.cmml">×</mo><mn id="S3.E2.m1.1.2.6.3.3" xref="S3.E2.m1.1.2.6.3.3.cmml"> 768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.2.cmml" xref="S3.E2.m1.1.2"><and id="S3.E2.m1.1.2a.cmml" xref="S3.E2.m1.1.2"></and><apply id="S3.E2.m1.1.2b.cmml" xref="S3.E2.m1.1.2"><eq id="S3.E2.m1.1.2.3.cmml" xref="S3.E2.m1.1.2.3"></eq><apply id="S3.E2.m1.1.2.2.cmml" xref="S3.E2.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.2.1.cmml" xref="S3.E2.m1.1.2.2">subscript</csymbol><ci id="S3.E2.m1.1.2.2.2.cmml" xref="S3.E2.m1.1.2.2.2">𝑉</ci><ci id="S3.E2.m1.1.2.2.3.cmml" xref="S3.E2.m1.1.2.2.3">𝐺</ci></apply><apply id="S3.E2.m1.1.2.4.cmml" xref="S3.E2.m1.1.2.4"><times id="S3.E2.m1.1.2.4.1.cmml" xref="S3.E2.m1.1.2.4.1"></times><ci id="S3.E2.m1.1.2.4.2a.cmml" xref="S3.E2.m1.1.2.4.2"><mtext id="S3.E2.m1.1.2.4.2.cmml" xref="S3.E2.m1.1.2.4.2">BLIP-2</mtext></ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝐼</ci></apply></apply><apply id="S3.E2.m1.1.2c.cmml" xref="S3.E2.m1.1.2"><in id="S3.E2.m1.1.2.5.cmml" xref="S3.E2.m1.1.2.5"></in><share href="#S3.E2.m1.1.2.4.cmml" id="S3.E2.m1.1.2d.cmml" xref="S3.E2.m1.1.2"></share><apply id="S3.E2.m1.1.2.6.cmml" xref="S3.E2.m1.1.2.6"><csymbol cd="ambiguous" id="S3.E2.m1.1.2.6.1.cmml" xref="S3.E2.m1.1.2.6">superscript</csymbol><ci id="S3.E2.m1.1.2.6.2.cmml" xref="S3.E2.m1.1.2.6.2">ℝ</ci><apply id="S3.E2.m1.1.2.6.3.cmml" xref="S3.E2.m1.1.2.6.3"><times id="S3.E2.m1.1.2.6.3.1.cmml" xref="S3.E2.m1.1.2.6.3.1"></times><cn type="integer" id="S3.E2.m1.1.2.6.3.2.cmml" xref="S3.E2.m1.1.2.6.3.2">32</cn><cn type="integer" id="S3.E2.m1.1.2.6.3.3.cmml" xref="S3.E2.m1.1.2.6.3.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">V_{G}=\text{BLIP-2}(I)\in\mathbb{R}^{32\times\ 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>EfficientNet</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.1" class="ltx_p">We integrate the EfficientNet architecture introduced by Tan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, a sophisticated CNN, for local image-level feature extraction. Intuitively, CNN architectures inherently utilize kernels sliding over regions of the image, effectively capturing information locally. Previous CNN architectures commonly face a trade-off between model complexity and performance, wherein as performance improves, model complexity increases. This trade-off involves scaling, which can occur through depth scaling, width scaling, and resolution scaling.</p>
</div>
<div id="S3.SS1.SSS2.p2" class="ltx_para">
<p id="S3.SS1.SSS2.p2.1" class="ltx_p">Depth scaling entails increasing the depth of a neural network, typically resulting in improved performance. Width scaling involves increasing the number of channels (or neurons) in each layer, thereby enhancing the model’s capacity to capture a broader range of features at each level and improving its representational power. Resolution scaling, meanwhile, requires resizing input images to higher resolutions, potentially leading to better performance as higher-resolution images contain more detailed information.</p>
</div>
<div id="S3.SS1.SSS2.p3" class="ltx_para">
<p id="S3.SS1.SSS2.p3.1" class="ltx_p">EfficientNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> is an improvement version basing on CNN that implements the concept of compound scaling, which enhances performance by uniformly scaling all dimensions of depth, width, and resolution. Its power is in how these three dimensions are balanced and adjusted using a systematic methodology. The model employs Mobile Inverted Bottleneck (MBConv) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> layers, combining depth-wise separable convolutions and inverted residual blocks. This bottleneck design enables efficient learning while preserving a high degree of representational power. The model’s potency has been demonstrated through experiments by Tan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, which showed that models in the EfficientNet family achieve performance equivalent to other models while maintaining low complexity. To leverage these capabilities, we employ an EfficientNet-B7 model, a pre-trained model based on adversarial training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. It is initialized with pre-trained weights on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, providing a robust foundation for directly extracting features for our ViVQA system.</p>
</div>
<div id="S3.SS1.SSS2.p4" class="ltx_para">
<p id="S3.SS1.SSS2.p4.7" class="ltx_p">The input image <math id="S3.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.SS1.SSS2.p4.1.m1.1a"><mi id="S3.SS1.SSS2.p4.1.m1.1.1" xref="S3.SS1.SSS2.p4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.1.m1.1b"><ci id="S3.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p4.1.m1.1.1">𝐼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.1.m1.1c">I</annotation></semantics></math>, denoted as <math id="S3.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="\mathbb{R}^{C\times H\times\ W}" display="inline"><semantics id="S3.SS1.SSS2.p4.2.m2.1a"><msup id="S3.SS1.SSS2.p4.2.m2.1.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS2.p4.2.m2.1.1.2" xref="S3.SS1.SSS2.p4.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS2.p4.2.m2.1.1.3" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS2.p4.2.m2.1.1.3.2" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.2.cmml">C</mi><mo lspace="0.222em" rspace="0.572em" id="S3.SS1.SSS2.p4.2.m2.1.1.3.1" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS1.SSS2.p4.2.m2.1.1.3.3" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.3.cmml">H</mi><mo lspace="0.222em" rspace="0.572em" id="S3.SS1.SSS2.p4.2.m2.1.1.3.1a" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.1.cmml">×</mo><mi id="S3.SS1.SSS2.p4.2.m2.1.1.3.4" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.4.cmml">W</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.2.m2.1b"><apply id="S3.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.2">ℝ</ci><apply id="S3.SS1.SSS2.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3"><times id="S3.SS1.SSS2.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.1"></times><ci id="S3.SS1.SSS2.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.2">𝐶</ci><ci id="S3.SS1.SSS2.p4.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.3">𝐻</ci><ci id="S3.SS1.SSS2.p4.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS2.p4.2.m2.1.1.3.4">𝑊</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.2.m2.1c">\mathbb{R}^{C\times H\times\ W}</annotation></semantics></math>, where <math id="S3.SS1.SSS2.p4.3.m3.2" class="ltx_Math" alttext="C,H" display="inline"><semantics id="S3.SS1.SSS2.p4.3.m3.2a"><mrow id="S3.SS1.SSS2.p4.3.m3.2.3.2" xref="S3.SS1.SSS2.p4.3.m3.2.3.1.cmml"><mi id="S3.SS1.SSS2.p4.3.m3.1.1" xref="S3.SS1.SSS2.p4.3.m3.1.1.cmml">C</mi><mo id="S3.SS1.SSS2.p4.3.m3.2.3.2.1" xref="S3.SS1.SSS2.p4.3.m3.2.3.1.cmml">,</mo><mi id="S3.SS1.SSS2.p4.3.m3.2.2" xref="S3.SS1.SSS2.p4.3.m3.2.2.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.3.m3.2b"><list id="S3.SS1.SSS2.p4.3.m3.2.3.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.2.3.2"><ci id="S3.SS1.SSS2.p4.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p4.3.m3.1.1">𝐶</ci><ci id="S3.SS1.SSS2.p4.3.m3.2.2.cmml" xref="S3.SS1.SSS2.p4.3.m3.2.2">𝐻</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.3.m3.2c">C,H</annotation></semantics></math> and <math id="S3.SS1.SSS2.p4.4.m4.1" class="ltx_Math" alttext="W" display="inline"><semantics id="S3.SS1.SSS2.p4.4.m4.1a"><mi id="S3.SS1.SSS2.p4.4.m4.1.1" xref="S3.SS1.SSS2.p4.4.m4.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.4.m4.1b"><ci id="S3.SS1.SSS2.p4.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p4.4.m4.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.4.m4.1c">W</annotation></semantics></math> stand for channels, height, and width respectively, undergoes processing within the pre-trained EfficientNet model. The features are extracted from the output of the final head before undergoing average pooling and a fully connected layer. This process generates a representation capturing the local features of the image, expressed as <math id="S3.SS1.SSS2.p4.5.m5.1" class="ltx_Math" alttext="\mathbb{R}^{2560\times\frac{H}{32}\times\ \frac{W}{32}}" display="inline"><semantics id="S3.SS1.SSS2.p4.5.m5.1a"><msup id="S3.SS1.SSS2.p4.5.m5.1.1" xref="S3.SS1.SSS2.p4.5.m5.1.1.cmml"><mi id="S3.SS1.SSS2.p4.5.m5.1.1.2" xref="S3.SS1.SSS2.p4.5.m5.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS2.p4.5.m5.1.1.3" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.cmml"><mn id="S3.SS1.SSS2.p4.5.m5.1.1.3.2" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.572em" id="S3.SS1.SSS2.p4.5.m5.1.1.3.1" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.1.cmml">×</mo><mfrac id="S3.SS1.SSS2.p4.5.m5.1.1.3.3" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.2" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3.2.cmml">H</mi><mn id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.3" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3.3.cmml">32</mn></mfrac><mo lspace="0.222em" rspace="0.572em" id="S3.SS1.SSS2.p4.5.m5.1.1.3.1a" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.1.cmml">×</mo><mfrac id="S3.SS1.SSS2.p4.5.m5.1.1.3.4" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4.cmml"><mi id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.2" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4.2.cmml">W</mi><mn id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.3" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4.3.cmml">32</mn></mfrac></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.5.m5.1b"><apply id="S3.SS1.SSS2.p4.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.5.m5.1.1.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.5.m5.1.1.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.2">ℝ</ci><apply id="S3.SS1.SSS2.p4.5.m5.1.1.3.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3"><times id="S3.SS1.SSS2.p4.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS2.p4.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.2">2560</cn><apply id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3"><divide id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3"></divide><ci id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3.2">𝐻</ci><cn type="integer" id="S3.SS1.SSS2.p4.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.3.3">32</cn></apply><apply id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4"><divide id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.1.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4"></divide><ci id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.2.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4.2">𝑊</ci><cn type="integer" id="S3.SS1.SSS2.p4.5.m5.1.1.3.4.3.cmml" xref="S3.SS1.SSS2.p4.5.m5.1.1.3.4.3">32</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.5.m5.1c">\mathbb{R}^{2560\times\frac{H}{32}\times\ \frac{W}{32}}</annotation></semantics></math>. It is noteworthy that our original image data is standardized to the dimensions of <math id="S3.SS1.SSS2.p4.6.m6.1" class="ltx_Math" alttext="\mathbb{R}^{3\times 224\times 224}" display="inline"><semantics id="S3.SS1.SSS2.p4.6.m6.1a"><msup id="S3.SS1.SSS2.p4.6.m6.1.1" xref="S3.SS1.SSS2.p4.6.m6.1.1.cmml"><mi id="S3.SS1.SSS2.p4.6.m6.1.1.2" xref="S3.SS1.SSS2.p4.6.m6.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS2.p4.6.m6.1.1.3" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.cmml"><mn id="S3.SS1.SSS2.p4.6.m6.1.1.3.2" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p4.6.m6.1.1.3.1" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS2.p4.6.m6.1.1.3.3" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.3.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p4.6.m6.1.1.3.1a" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS2.p4.6.m6.1.1.3.4" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.4.cmml">224</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.6.m6.1b"><apply id="S3.SS1.SSS2.p4.6.m6.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.6.m6.1.1.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.6.m6.1.1.2.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.2">ℝ</ci><apply id="S3.SS1.SSS2.p4.6.m6.1.1.3.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3"><times id="S3.SS1.SSS2.p4.6.m6.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS2.p4.6.m6.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.2">3</cn><cn type="integer" id="S3.SS1.SSS2.p4.6.m6.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.3">224</cn><cn type="integer" id="S3.SS1.SSS2.p4.6.m6.1.1.3.4.cmml" xref="S3.SS1.SSS2.p4.6.m6.1.1.3.4">224</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.6.m6.1c">\mathbb{R}^{3\times 224\times 224}</annotation></semantics></math>. Consequently, the image representation following EfficientNet feature extraction has dimensions <math id="S3.SS1.SSS2.p4.7.m7.1" class="ltx_Math" alttext="\mathbb{R}^{2560\times 7\times 7}" display="inline"><semantics id="S3.SS1.SSS2.p4.7.m7.1a"><msup id="S3.SS1.SSS2.p4.7.m7.1.1" xref="S3.SS1.SSS2.p4.7.m7.1.1.cmml"><mi id="S3.SS1.SSS2.p4.7.m7.1.1.2" xref="S3.SS1.SSS2.p4.7.m7.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS2.p4.7.m7.1.1.3" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.cmml"><mn id="S3.SS1.SSS2.p4.7.m7.1.1.3.2" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p4.7.m7.1.1.3.1" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS2.p4.7.m7.1.1.3.3" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS2.p4.7.m7.1.1.3.1a" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS2.p4.7.m7.1.1.3.4" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.4.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p4.7.m7.1b"><apply id="S3.SS1.SSS2.p4.7.m7.1.1.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS2.p4.7.m7.1.1.1.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1">superscript</csymbol><ci id="S3.SS1.SSS2.p4.7.m7.1.1.2.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.2">ℝ</ci><apply id="S3.SS1.SSS2.p4.7.m7.1.1.3.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3"><times id="S3.SS1.SSS2.p4.7.m7.1.1.3.1.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS2.p4.7.m7.1.1.3.2.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.2">2560</cn><cn type="integer" id="S3.SS1.SSS2.p4.7.m7.1.1.3.3.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.3">7</cn><cn type="integer" id="S3.SS1.SSS2.p4.7.m7.1.1.3.4.cmml" xref="S3.SS1.SSS2.p4.7.m7.1.1.3.4">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p4.7.m7.1c">\mathbb{R}^{2560\times 7\times 7}</annotation></semantics></math>.</p>
<table id="S3.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E3.m1.1" class="ltx_Math" alttext="V_{L}=\text{EfficientNet}(I)\in\mathbb{R}^{2560\times 7\times 7}" display="block"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.2" xref="S3.E3.m1.1.2.cmml"><msub id="S3.E3.m1.1.2.2" xref="S3.E3.m1.1.2.2.cmml"><mi id="S3.E3.m1.1.2.2.2" xref="S3.E3.m1.1.2.2.2.cmml">V</mi><mi id="S3.E3.m1.1.2.2.3" xref="S3.E3.m1.1.2.2.3.cmml">L</mi></msub><mo id="S3.E3.m1.1.2.3" xref="S3.E3.m1.1.2.3.cmml">=</mo><mrow id="S3.E3.m1.1.2.4" xref="S3.E3.m1.1.2.4.cmml"><mtext id="S3.E3.m1.1.2.4.2" xref="S3.E3.m1.1.2.4.2a.cmml">EfficientNet</mtext><mo lspace="0em" rspace="0em" id="S3.E3.m1.1.2.4.1" xref="S3.E3.m1.1.2.4.1.cmml">​</mo><mrow id="S3.E3.m1.1.2.4.3.2" xref="S3.E3.m1.1.2.4.cmml"><mo stretchy="false" id="S3.E3.m1.1.2.4.3.2.1" xref="S3.E3.m1.1.2.4.cmml">(</mo><mi id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml">I</mi><mo stretchy="false" id="S3.E3.m1.1.2.4.3.2.2" xref="S3.E3.m1.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.2.5" xref="S3.E3.m1.1.2.5.cmml">∈</mo><msup id="S3.E3.m1.1.2.6" xref="S3.E3.m1.1.2.6.cmml"><mi id="S3.E3.m1.1.2.6.2" xref="S3.E3.m1.1.2.6.2.cmml">ℝ</mi><mrow id="S3.E3.m1.1.2.6.3" xref="S3.E3.m1.1.2.6.3.cmml"><mn id="S3.E3.m1.1.2.6.3.2" xref="S3.E3.m1.1.2.6.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.2.6.3.1" xref="S3.E3.m1.1.2.6.3.1.cmml">×</mo><mn id="S3.E3.m1.1.2.6.3.3" xref="S3.E3.m1.1.2.6.3.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E3.m1.1.2.6.3.1a" xref="S3.E3.m1.1.2.6.3.1.cmml">×</mo><mn id="S3.E3.m1.1.2.6.3.4" xref="S3.E3.m1.1.2.6.3.4.cmml">7</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.2.cmml" xref="S3.E3.m1.1.2"><and id="S3.E3.m1.1.2a.cmml" xref="S3.E3.m1.1.2"></and><apply id="S3.E3.m1.1.2b.cmml" xref="S3.E3.m1.1.2"><eq id="S3.E3.m1.1.2.3.cmml" xref="S3.E3.m1.1.2.3"></eq><apply id="S3.E3.m1.1.2.2.cmml" xref="S3.E3.m1.1.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.2.2.1.cmml" xref="S3.E3.m1.1.2.2">subscript</csymbol><ci id="S3.E3.m1.1.2.2.2.cmml" xref="S3.E3.m1.1.2.2.2">𝑉</ci><ci id="S3.E3.m1.1.2.2.3.cmml" xref="S3.E3.m1.1.2.2.3">𝐿</ci></apply><apply id="S3.E3.m1.1.2.4.cmml" xref="S3.E3.m1.1.2.4"><times id="S3.E3.m1.1.2.4.1.cmml" xref="S3.E3.m1.1.2.4.1"></times><ci id="S3.E3.m1.1.2.4.2a.cmml" xref="S3.E3.m1.1.2.4.2"><mtext id="S3.E3.m1.1.2.4.2.cmml" xref="S3.E3.m1.1.2.4.2">EfficientNet</mtext></ci><ci id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1">𝐼</ci></apply></apply><apply id="S3.E3.m1.1.2c.cmml" xref="S3.E3.m1.1.2"><in id="S3.E3.m1.1.2.5.cmml" xref="S3.E3.m1.1.2.5"></in><share href="#S3.E3.m1.1.2.4.cmml" id="S3.E3.m1.1.2d.cmml" xref="S3.E3.m1.1.2"></share><apply id="S3.E3.m1.1.2.6.cmml" xref="S3.E3.m1.1.2.6"><csymbol cd="ambiguous" id="S3.E3.m1.1.2.6.1.cmml" xref="S3.E3.m1.1.2.6">superscript</csymbol><ci id="S3.E3.m1.1.2.6.2.cmml" xref="S3.E3.m1.1.2.6.2">ℝ</ci><apply id="S3.E3.m1.1.2.6.3.cmml" xref="S3.E3.m1.1.2.6.3"><times id="S3.E3.m1.1.2.6.3.1.cmml" xref="S3.E3.m1.1.2.6.3.1"></times><cn type="integer" id="S3.E3.m1.1.2.6.3.2.cmml" xref="S3.E3.m1.1.2.6.3.2">2560</cn><cn type="integer" id="S3.E3.m1.1.2.6.3.3.cmml" xref="S3.E3.m1.1.2.6.3.3">7</cn><cn type="integer" id="S3.E3.m1.1.2.6.3.4.cmml" xref="S3.E3.m1.1.2.6.3.4">7</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">V_{L}=\text{EfficientNet}(I)\in\mathbb{R}^{2560\times 7\times 7}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS2.p5" class="ltx_para">
<p id="S3.SS1.SSS2.p5.1" class="ltx_p">This approach allows us to capture intricate local features from the images, providing a rich and informative representation essential for subsequent stages in our model architecture.</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3 </span>Image Embedding</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.1" class="ltx_p">In this stage, we will amalgamate the representations of BLIP-2 and EfficientNet to derive the final output of the image embedding module. Initially, an inconsistency was observed in the dimensions of BLIP-2 (<a href="#S3.E2" title="In 3.1.1 BLIP-2 ‣ 3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) and EfficientNet (<a href="#S3.E3" title="In 3.1.2 EfficientNet ‣ 3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Thus, we will implement several steps to transform the representation of EfficientNet to align it with the output of BLIP-2. This transformation is crucial to facilitate operations for seamlessly combining the local and global features.</p>
</div>
<div id="S3.SS1.SSS3.p2" class="ltx_para">
<p id="S3.SS1.SSS3.p2.2" class="ltx_p">The transformation involves employing Adaptive Average Pooling to downsample from <math id="S3.SS1.SSS3.p2.1.m1.1" class="ltx_Math" alttext="\mathbb{R}^{2560\times 7\times 7}" display="inline"><semantics id="S3.SS1.SSS3.p2.1.m1.1a"><msup id="S3.SS1.SSS3.p2.1.m1.1.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p2.1.m1.1.1.2" xref="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS3.p2.1.m1.1.1.3" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.cmml"><mn id="S3.SS1.SSS3.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p2.1.m1.1.1.3.1" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.3.cmml">7</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p2.1.m1.1.1.3.1a" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p2.1.m1.1.1.3.4" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.4.cmml">7</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.1.m1.1b"><apply id="S3.SS1.SSS3.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.2">ℝ</ci><apply id="S3.SS1.SSS3.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3"><times id="S3.SS1.SSS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.2">2560</cn><cn type="integer" id="S3.SS1.SSS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.3">7</cn><cn type="integer" id="S3.SS1.SSS3.p2.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS3.p2.1.m1.1.1.3.4">7</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.1.m1.1c">\mathbb{R}^{2560\times 7\times 7}</annotation></semantics></math> to <math id="S3.SS1.SSS3.p2.2.m2.1" class="ltx_Math" alttext="\mathbb{R}^{2560\times 1\times 32}" display="inline"><semantics id="S3.SS1.SSS3.p2.2.m2.1a"><msup id="S3.SS1.SSS3.p2.2.m2.1.1" xref="S3.SS1.SSS3.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p2.2.m2.1.1.2" xref="S3.SS1.SSS3.p2.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS3.p2.2.m2.1.1.3" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.cmml"><mn id="S3.SS1.SSS3.p2.2.m2.1.1.3.2" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p2.2.m2.1.1.3.1" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p2.2.m2.1.1.3.3" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p2.2.m2.1.1.3.1a" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p2.2.m2.1.1.3.4" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.4.cmml">32</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p2.2.m2.1b"><apply id="S3.SS1.SSS3.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.2">ℝ</ci><apply id="S3.SS1.SSS3.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3"><times id="S3.SS1.SSS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.2">2560</cn><cn type="integer" id="S3.SS1.SSS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.3">1</cn><cn type="integer" id="S3.SS1.SSS3.p2.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS3.p2.2.m2.1.1.3.4">32</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p2.2.m2.1c">\mathbb{R}^{2560\times 1\times 32}</annotation></semantics></math>:</p>
<table id="S3.E4" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E4.m1.1" class="ltx_Math" alttext="V_{L}=\text{AdaptiveAveragePooling}(V_{L})\in\mathbb{R}^{2560\times 1\times 32}" display="block"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1" xref="S3.E4.m1.1.1.cmml"><msub id="S3.E4.m1.1.1.3" xref="S3.E4.m1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.3.2" xref="S3.E4.m1.1.1.3.2.cmml">V</mi><mi id="S3.E4.m1.1.1.3.3" xref="S3.E4.m1.1.1.3.3.cmml">L</mi></msub><mo id="S3.E4.m1.1.1.4" xref="S3.E4.m1.1.1.4.cmml">=</mo><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.cmml"><mtext id="S3.E4.m1.1.1.1.3" xref="S3.E4.m1.1.1.1.3a.cmml">AdaptiveAveragePooling</mtext><mo lspace="0em" rspace="0em" id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.E4.m1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.1.1.5" xref="S3.E4.m1.1.1.5.cmml">∈</mo><msup id="S3.E4.m1.1.1.6" xref="S3.E4.m1.1.1.6.cmml"><mi id="S3.E4.m1.1.1.6.2" xref="S3.E4.m1.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E4.m1.1.1.6.3" xref="S3.E4.m1.1.1.6.3.cmml"><mn id="S3.E4.m1.1.1.6.3.2" xref="S3.E4.m1.1.1.6.3.2.cmml">2560</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.1.6.3.1" xref="S3.E4.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E4.m1.1.1.6.3.3" xref="S3.E4.m1.1.1.6.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E4.m1.1.1.6.3.1a" xref="S3.E4.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E4.m1.1.1.6.3.4" xref="S3.E4.m1.1.1.6.3.4.cmml">32</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.cmml" xref="S3.E4.m1.1.1"><and id="S3.E4.m1.1.1a.cmml" xref="S3.E4.m1.1.1"></and><apply id="S3.E4.m1.1.1b.cmml" xref="S3.E4.m1.1.1"><eq id="S3.E4.m1.1.1.4.cmml" xref="S3.E4.m1.1.1.4"></eq><apply id="S3.E4.m1.1.1.3.cmml" xref="S3.E4.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.3.2">𝑉</ci><ci id="S3.E4.m1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.3.3">𝐿</ci></apply><apply id="S3.E4.m1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><times id="S3.E4.m1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.3a.cmml" xref="S3.E4.m1.1.1.1.3"><mtext id="S3.E4.m1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.3">AdaptiveAveragePooling</mtext></ci><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E4.m1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply><apply id="S3.E4.m1.1.1c.cmml" xref="S3.E4.m1.1.1"><in id="S3.E4.m1.1.1.5.cmml" xref="S3.E4.m1.1.1.5"></in><share href="#S3.E4.m1.1.1.1.cmml" id="S3.E4.m1.1.1d.cmml" xref="S3.E4.m1.1.1"></share><apply id="S3.E4.m1.1.1.6.cmml" xref="S3.E4.m1.1.1.6"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.6.1.cmml" xref="S3.E4.m1.1.1.6">superscript</csymbol><ci id="S3.E4.m1.1.1.6.2.cmml" xref="S3.E4.m1.1.1.6.2">ℝ</ci><apply id="S3.E4.m1.1.1.6.3.cmml" xref="S3.E4.m1.1.1.6.3"><times id="S3.E4.m1.1.1.6.3.1.cmml" xref="S3.E4.m1.1.1.6.3.1"></times><cn type="integer" id="S3.E4.m1.1.1.6.3.2.cmml" xref="S3.E4.m1.1.1.6.3.2">2560</cn><cn type="integer" id="S3.E4.m1.1.1.6.3.3.cmml" xref="S3.E4.m1.1.1.6.3.3">1</cn><cn type="integer" id="S3.E4.m1.1.1.6.3.4.cmml" xref="S3.E4.m1.1.1.6.3.4">32</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">V_{L}=\text{AdaptiveAveragePooling}(V_{L})\in\mathbb{R}^{2560\times 1\times 32}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS3.p3" class="ltx_para">
<p id="S3.SS1.SSS3.p3.2" class="ltx_p">Subsequently, its dimensions are permuted to <math id="S3.SS1.SSS3.p3.1.m1.1" class="ltx_Math" alttext="\mathbb{R}^{32\times 1\times 2560}" display="inline"><semantics id="S3.SS1.SSS3.p3.1.m1.1a"><msup id="S3.SS1.SSS3.p3.1.m1.1.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS3.p3.1.m1.1.1.2" xref="S3.SS1.SSS3.p3.1.m1.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS3.p3.1.m1.1.1.3" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.cmml"><mn id="S3.SS1.SSS3.p3.1.m1.1.1.3.2" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p3.1.m1.1.1.3.1" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p3.1.m1.1.1.3.3" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p3.1.m1.1.1.3.1a" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p3.1.m1.1.1.3.4" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.4.cmml">2560</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.1.m1.1b"><apply id="S3.SS1.SSS3.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.2">ℝ</ci><apply id="S3.SS1.SSS3.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3"><times id="S3.SS1.SSS3.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS3.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.2">32</cn><cn type="integer" id="S3.SS1.SSS3.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.3">1</cn><cn type="integer" id="S3.SS1.SSS3.p3.1.m1.1.1.3.4.cmml" xref="S3.SS1.SSS3.p3.1.m1.1.1.3.4">2560</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.1.m1.1c">\mathbb{R}^{32\times 1\times 2560}</annotation></semantics></math>, and another round of Adaptive Average Pooling is applied for downsampling to <math id="S3.SS1.SSS3.p3.2.m2.1" class="ltx_Math" alttext="\mathbb{R}^{32\times 1\times 768}" display="inline"><semantics id="S3.SS1.SSS3.p3.2.m2.1a"><msup id="S3.SS1.SSS3.p3.2.m2.1.1" xref="S3.SS1.SSS3.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS3.p3.2.m2.1.1.2" xref="S3.SS1.SSS3.p3.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S3.SS1.SSS3.p3.2.m2.1.1.3" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.cmml"><mn id="S3.SS1.SSS3.p3.2.m2.1.1.3.2" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p3.2.m2.1.1.3.1" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p3.2.m2.1.1.3.3" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS3.p3.2.m2.1.1.3.1a" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.1.cmml">×</mo><mn id="S3.SS1.SSS3.p3.2.m2.1.1.3.4" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.4.cmml">768</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p3.2.m2.1b"><apply id="S3.SS1.SSS3.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS3.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.2">ℝ</ci><apply id="S3.SS1.SSS3.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.3"><times id="S3.SS1.SSS3.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.1"></times><cn type="integer" id="S3.SS1.SSS3.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.2">32</cn><cn type="integer" id="S3.SS1.SSS3.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.3">1</cn><cn type="integer" id="S3.SS1.SSS3.p3.2.m2.1.1.3.4.cmml" xref="S3.SS1.SSS3.p3.2.m2.1.1.3.4">768</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p3.2.m2.1c">\mathbb{R}^{32\times 1\times 768}</annotation></semantics></math>:</p>
<table id="S3.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E5.m1.1" class="ltx_Math" alttext="V_{L}=\text{Permute}(V_{L})\in\mathbb{R}^{32\times 1\times 2560}" display="block"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1" xref="S3.E5.m1.1.1.cmml"><msub id="S3.E5.m1.1.1.3" xref="S3.E5.m1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.3.2" xref="S3.E5.m1.1.1.3.2.cmml">V</mi><mi id="S3.E5.m1.1.1.3.3" xref="S3.E5.m1.1.1.3.3.cmml">L</mi></msub><mo id="S3.E5.m1.1.1.4" xref="S3.E5.m1.1.1.4.cmml">=</mo><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.cmml"><mtext id="S3.E5.m1.1.1.1.3" xref="S3.E5.m1.1.1.1.3a.cmml">Permute</mtext><mo lspace="0em" rspace="0em" id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E5.m1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.E5.m1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E5.m1.1.1.5" xref="S3.E5.m1.1.1.5.cmml">∈</mo><msup id="S3.E5.m1.1.1.6" xref="S3.E5.m1.1.1.6.cmml"><mi id="S3.E5.m1.1.1.6.2" xref="S3.E5.m1.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E5.m1.1.1.6.3" xref="S3.E5.m1.1.1.6.3.cmml"><mn id="S3.E5.m1.1.1.6.3.2" xref="S3.E5.m1.1.1.6.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.1.1.6.3.1" xref="S3.E5.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E5.m1.1.1.6.3.3" xref="S3.E5.m1.1.1.6.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E5.m1.1.1.6.3.1a" xref="S3.E5.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E5.m1.1.1.6.3.4" xref="S3.E5.m1.1.1.6.3.4.cmml">2560</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.cmml" xref="S3.E5.m1.1.1"><and id="S3.E5.m1.1.1a.cmml" xref="S3.E5.m1.1.1"></and><apply id="S3.E5.m1.1.1b.cmml" xref="S3.E5.m1.1.1"><eq id="S3.E5.m1.1.1.4.cmml" xref="S3.E5.m1.1.1.4"></eq><apply id="S3.E5.m1.1.1.3.cmml" xref="S3.E5.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.3">subscript</csymbol><ci id="S3.E5.m1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.3.2">𝑉</ci><ci id="S3.E5.m1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.3.3">𝐿</ci></apply><apply id="S3.E5.m1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><times id="S3.E5.m1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.3"><mtext id="S3.E5.m1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.3">Permute</mtext></ci><apply id="S3.E5.m1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E5.m1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E5.m1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply><apply id="S3.E5.m1.1.1c.cmml" xref="S3.E5.m1.1.1"><in id="S3.E5.m1.1.1.5.cmml" xref="S3.E5.m1.1.1.5"></in><share href="#S3.E5.m1.1.1.1.cmml" id="S3.E5.m1.1.1d.cmml" xref="S3.E5.m1.1.1"></share><apply id="S3.E5.m1.1.1.6.cmml" xref="S3.E5.m1.1.1.6"><csymbol cd="ambiguous" id="S3.E5.m1.1.1.6.1.cmml" xref="S3.E5.m1.1.1.6">superscript</csymbol><ci id="S3.E5.m1.1.1.6.2.cmml" xref="S3.E5.m1.1.1.6.2">ℝ</ci><apply id="S3.E5.m1.1.1.6.3.cmml" xref="S3.E5.m1.1.1.6.3"><times id="S3.E5.m1.1.1.6.3.1.cmml" xref="S3.E5.m1.1.1.6.3.1"></times><cn type="integer" id="S3.E5.m1.1.1.6.3.2.cmml" xref="S3.E5.m1.1.1.6.3.2">32</cn><cn type="integer" id="S3.E5.m1.1.1.6.3.3.cmml" xref="S3.E5.m1.1.1.6.3.3">1</cn><cn type="integer" id="S3.E5.m1.1.1.6.3.4.cmml" xref="S3.E5.m1.1.1.6.3.4">2560</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">V_{L}=\text{Permute}(V_{L})\in\mathbb{R}^{32\times 1\times 2560}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<table id="S3.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E6.m1.1" class="ltx_Math" alttext="V_{L}=\text{AdaptiveAveragePooling}(V_{L})\in\mathbb{R}^{32\times 1\times 768}" display="block"><semantics id="S3.E6.m1.1a"><mrow id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><msub id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml"><mi id="S3.E6.m1.1.1.3.2" xref="S3.E6.m1.1.1.3.2.cmml">V</mi><mi id="S3.E6.m1.1.1.3.3" xref="S3.E6.m1.1.1.3.3.cmml">L</mi></msub><mo id="S3.E6.m1.1.1.4" xref="S3.E6.m1.1.1.4.cmml">=</mo><mrow id="S3.E6.m1.1.1.1" xref="S3.E6.m1.1.1.1.cmml"><mtext id="S3.E6.m1.1.1.1.3" xref="S3.E6.m1.1.1.1.3a.cmml">AdaptiveAveragePooling</mtext><mo lspace="0em" rspace="0em" id="S3.E6.m1.1.1.1.2" xref="S3.E6.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E6.m1.1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.1.cmml"><mi id="S3.E6.m1.1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.E6.m1.1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E6.m1.1.1.5" xref="S3.E6.m1.1.1.5.cmml">∈</mo><msup id="S3.E6.m1.1.1.6" xref="S3.E6.m1.1.1.6.cmml"><mi id="S3.E6.m1.1.1.6.2" xref="S3.E6.m1.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E6.m1.1.1.6.3" xref="S3.E6.m1.1.1.6.3.cmml"><mn id="S3.E6.m1.1.1.6.3.2" xref="S3.E6.m1.1.1.6.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.6.3.1" xref="S3.E6.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E6.m1.1.1.6.3.3" xref="S3.E6.m1.1.1.6.3.3.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E6.m1.1.1.6.3.1a" xref="S3.E6.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E6.m1.1.1.6.3.4" xref="S3.E6.m1.1.1.6.3.4.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.1b"><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><and id="S3.E6.m1.1.1a.cmml" xref="S3.E6.m1.1.1"></and><apply id="S3.E6.m1.1.1b.cmml" xref="S3.E6.m1.1.1"><eq id="S3.E6.m1.1.1.4.cmml" xref="S3.E6.m1.1.1.4"></eq><apply id="S3.E6.m1.1.1.3.cmml" xref="S3.E6.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.3.1.cmml" xref="S3.E6.m1.1.1.3">subscript</csymbol><ci id="S3.E6.m1.1.1.3.2.cmml" xref="S3.E6.m1.1.1.3.2">𝑉</ci><ci id="S3.E6.m1.1.1.3.3.cmml" xref="S3.E6.m1.1.1.3.3">𝐿</ci></apply><apply id="S3.E6.m1.1.1.1.cmml" xref="S3.E6.m1.1.1.1"><times id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.2"></times><ci id="S3.E6.m1.1.1.1.3a.cmml" xref="S3.E6.m1.1.1.1.3"><mtext id="S3.E6.m1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.3">AdaptiveAveragePooling</mtext></ci><apply id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E6.m1.1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply><apply id="S3.E6.m1.1.1c.cmml" xref="S3.E6.m1.1.1"><in id="S3.E6.m1.1.1.5.cmml" xref="S3.E6.m1.1.1.5"></in><share href="#S3.E6.m1.1.1.1.cmml" id="S3.E6.m1.1.1d.cmml" xref="S3.E6.m1.1.1"></share><apply id="S3.E6.m1.1.1.6.cmml" xref="S3.E6.m1.1.1.6"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.6.1.cmml" xref="S3.E6.m1.1.1.6">superscript</csymbol><ci id="S3.E6.m1.1.1.6.2.cmml" xref="S3.E6.m1.1.1.6.2">ℝ</ci><apply id="S3.E6.m1.1.1.6.3.cmml" xref="S3.E6.m1.1.1.6.3"><times id="S3.E6.m1.1.1.6.3.1.cmml" xref="S3.E6.m1.1.1.6.3.1"></times><cn type="integer" id="S3.E6.m1.1.1.6.3.2.cmml" xref="S3.E6.m1.1.1.6.3.2">32</cn><cn type="integer" id="S3.E6.m1.1.1.6.3.3.cmml" xref="S3.E6.m1.1.1.6.3.3">1</cn><cn type="integer" id="S3.E6.m1.1.1.6.3.4.cmml" xref="S3.E6.m1.1.1.6.3.4">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.1c">V_{L}=\text{AdaptiveAveragePooling}(V_{L})\in\mathbb{R}^{32\times 1\times 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS3.p4" class="ltx_para">
<p id="S3.SS1.SSS3.p4.1" class="ltx_p">Through the flattening process, we derive the ultimate output of the image embedding module:</p>
<table id="S3.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E7.m1.1" class="ltx_Math" alttext="V_{L}=\text{Flatten}(V_{L})\in\mathbb{R}^{32\times\ 768}" display="block"><semantics id="S3.E7.m1.1a"><mrow id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml"><msub id="S3.E7.m1.1.1.3" xref="S3.E7.m1.1.1.3.cmml"><mi id="S3.E7.m1.1.1.3.2" xref="S3.E7.m1.1.1.3.2.cmml">V</mi><mi id="S3.E7.m1.1.1.3.3" xref="S3.E7.m1.1.1.3.3.cmml">L</mi></msub><mo id="S3.E7.m1.1.1.4" xref="S3.E7.m1.1.1.4.cmml">=</mo><mrow id="S3.E7.m1.1.1.1" xref="S3.E7.m1.1.1.1.cmml"><mtext id="S3.E7.m1.1.1.1.3" xref="S3.E7.m1.1.1.1.3a.cmml">Flatten</mtext><mo lspace="0em" rspace="0em" id="S3.E7.m1.1.1.1.2" xref="S3.E7.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E7.m1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E7.m1.1.1.1.1.1.1" xref="S3.E7.m1.1.1.1.1.1.1.cmml"><mi id="S3.E7.m1.1.1.1.1.1.1.2" xref="S3.E7.m1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.E7.m1.1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E7.m1.1.1.1.1.1.3" xref="S3.E7.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E7.m1.1.1.5" xref="S3.E7.m1.1.1.5.cmml">∈</mo><msup id="S3.E7.m1.1.1.6" xref="S3.E7.m1.1.1.6.cmml"><mi id="S3.E7.m1.1.1.6.2" xref="S3.E7.m1.1.1.6.2.cmml">ℝ</mi><mrow id="S3.E7.m1.1.1.6.3" xref="S3.E7.m1.1.1.6.3.cmml"><mn id="S3.E7.m1.1.1.6.3.2" xref="S3.E7.m1.1.1.6.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S3.E7.m1.1.1.6.3.1" xref="S3.E7.m1.1.1.6.3.1.cmml">×</mo><mn id="S3.E7.m1.1.1.6.3.3" xref="S3.E7.m1.1.1.6.3.3.cmml"> 768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.1b"><apply id="S3.E7.m1.1.1.cmml" xref="S3.E7.m1.1.1"><and id="S3.E7.m1.1.1a.cmml" xref="S3.E7.m1.1.1"></and><apply id="S3.E7.m1.1.1b.cmml" xref="S3.E7.m1.1.1"><eq id="S3.E7.m1.1.1.4.cmml" xref="S3.E7.m1.1.1.4"></eq><apply id="S3.E7.m1.1.1.3.cmml" xref="S3.E7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.3.1.cmml" xref="S3.E7.m1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.1.1.3.2.cmml" xref="S3.E7.m1.1.1.3.2">𝑉</ci><ci id="S3.E7.m1.1.1.3.3.cmml" xref="S3.E7.m1.1.1.3.3">𝐿</ci></apply><apply id="S3.E7.m1.1.1.1.cmml" xref="S3.E7.m1.1.1.1"><times id="S3.E7.m1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.2"></times><ci id="S3.E7.m1.1.1.1.3a.cmml" xref="S3.E7.m1.1.1.1.3"><mtext id="S3.E7.m1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.3">Flatten</mtext></ci><apply id="S3.E7.m1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.1.1.1.1.1.cmml" xref="S3.E7.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E7.m1.1.1.1.1.1.1.2.cmml" xref="S3.E7.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E7.m1.1.1.1.1.1.1.3.cmml" xref="S3.E7.m1.1.1.1.1.1.1.3">𝐿</ci></apply></apply></apply><apply id="S3.E7.m1.1.1c.cmml" xref="S3.E7.m1.1.1"><in id="S3.E7.m1.1.1.5.cmml" xref="S3.E7.m1.1.1.5"></in><share href="#S3.E7.m1.1.1.1.cmml" id="S3.E7.m1.1.1d.cmml" xref="S3.E7.m1.1.1"></share><apply id="S3.E7.m1.1.1.6.cmml" xref="S3.E7.m1.1.1.6"><csymbol cd="ambiguous" id="S3.E7.m1.1.1.6.1.cmml" xref="S3.E7.m1.1.1.6">superscript</csymbol><ci id="S3.E7.m1.1.1.6.2.cmml" xref="S3.E7.m1.1.1.6.2">ℝ</ci><apply id="S3.E7.m1.1.1.6.3.cmml" xref="S3.E7.m1.1.1.6.3"><times id="S3.E7.m1.1.1.6.3.1.cmml" xref="S3.E7.m1.1.1.6.3.1"></times><cn type="integer" id="S3.E7.m1.1.1.6.3.2.cmml" xref="S3.E7.m1.1.1.6.3.2">32</cn><cn type="integer" id="S3.E7.m1.1.1.6.3.3.cmml" xref="S3.E7.m1.1.1.6.3.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.1c">V_{L}=\text{Flatten}(V_{L})\in\mathbb{R}^{32\times\ 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.SSS3.p5" class="ltx_para">
<p id="S3.SS1.SSS3.p5.1" class="ltx_p">Finally, the fusion of local features and global features is computed as follows:</p>
</div>
<div id="S3.SS1.SSS3.p6" class="ltx_para">
<table id="S3.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E8.m1.2" class="ltx_Math" alttext="V=\mathcal{F}(V_{G},V_{L})\in\mathbb{R}^{k\times 768}" display="block"><semantics id="S3.E8.m1.2a"><mrow id="S3.E8.m1.2.2" xref="S3.E8.m1.2.2.cmml"><mi id="S3.E8.m1.2.2.4" xref="S3.E8.m1.2.2.4.cmml">V</mi><mo id="S3.E8.m1.2.2.5" xref="S3.E8.m1.2.2.5.cmml">=</mo><mrow id="S3.E8.m1.2.2.2" xref="S3.E8.m1.2.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E8.m1.2.2.2.4" xref="S3.E8.m1.2.2.2.4.cmml">ℱ</mi><mo lspace="0em" rspace="0em" id="S3.E8.m1.2.2.2.3" xref="S3.E8.m1.2.2.2.3.cmml">​</mo><mrow id="S3.E8.m1.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.E8.m1.2.2.2.2.2.3" xref="S3.E8.m1.2.2.2.2.3.cmml">(</mo><msub id="S3.E8.m1.1.1.1.1.1.1" xref="S3.E8.m1.1.1.1.1.1.1.cmml"><mi id="S3.E8.m1.1.1.1.1.1.1.2" xref="S3.E8.m1.1.1.1.1.1.1.2.cmml">V</mi><mi id="S3.E8.m1.1.1.1.1.1.1.3" xref="S3.E8.m1.1.1.1.1.1.1.3.cmml">G</mi></msub><mo id="S3.E8.m1.2.2.2.2.2.4" xref="S3.E8.m1.2.2.2.2.3.cmml">,</mo><msub id="S3.E8.m1.2.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.2.2.cmml"><mi id="S3.E8.m1.2.2.2.2.2.2.2" xref="S3.E8.m1.2.2.2.2.2.2.2.cmml">V</mi><mi id="S3.E8.m1.2.2.2.2.2.2.3" xref="S3.E8.m1.2.2.2.2.2.2.3.cmml">L</mi></msub><mo stretchy="false" id="S3.E8.m1.2.2.2.2.2.5" xref="S3.E8.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E8.m1.2.2.6" xref="S3.E8.m1.2.2.6.cmml">∈</mo><msup id="S3.E8.m1.2.2.7" xref="S3.E8.m1.2.2.7.cmml"><mi id="S3.E8.m1.2.2.7.2" xref="S3.E8.m1.2.2.7.2.cmml">ℝ</mi><mrow id="S3.E8.m1.2.2.7.3" xref="S3.E8.m1.2.2.7.3.cmml"><mi id="S3.E8.m1.2.2.7.3.2" xref="S3.E8.m1.2.2.7.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.E8.m1.2.2.7.3.1" xref="S3.E8.m1.2.2.7.3.1.cmml">×</mo><mn id="S3.E8.m1.2.2.7.3.3" xref="S3.E8.m1.2.2.7.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E8.m1.2b"><apply id="S3.E8.m1.2.2.cmml" xref="S3.E8.m1.2.2"><and id="S3.E8.m1.2.2a.cmml" xref="S3.E8.m1.2.2"></and><apply id="S3.E8.m1.2.2b.cmml" xref="S3.E8.m1.2.2"><eq id="S3.E8.m1.2.2.5.cmml" xref="S3.E8.m1.2.2.5"></eq><ci id="S3.E8.m1.2.2.4.cmml" xref="S3.E8.m1.2.2.4">𝑉</ci><apply id="S3.E8.m1.2.2.2.cmml" xref="S3.E8.m1.2.2.2"><times id="S3.E8.m1.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.3"></times><ci id="S3.E8.m1.2.2.2.4.cmml" xref="S3.E8.m1.2.2.2.4">ℱ</ci><interval closure="open" id="S3.E8.m1.2.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.2.2"><apply id="S3.E8.m1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E8.m1.1.1.1.1.1.1.1.cmml" xref="S3.E8.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E8.m1.1.1.1.1.1.1.2.cmml" xref="S3.E8.m1.1.1.1.1.1.1.2">𝑉</ci><ci id="S3.E8.m1.1.1.1.1.1.1.3.cmml" xref="S3.E8.m1.1.1.1.1.1.1.3">𝐺</ci></apply><apply id="S3.E8.m1.2.2.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.2.2.2.2.1.cmml" xref="S3.E8.m1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.E8.m1.2.2.2.2.2.2.2.cmml" xref="S3.E8.m1.2.2.2.2.2.2.2">𝑉</ci><ci id="S3.E8.m1.2.2.2.2.2.2.3.cmml" xref="S3.E8.m1.2.2.2.2.2.2.3">𝐿</ci></apply></interval></apply></apply><apply id="S3.E8.m1.2.2c.cmml" xref="S3.E8.m1.2.2"><in id="S3.E8.m1.2.2.6.cmml" xref="S3.E8.m1.2.2.6"></in><share href="#S3.E8.m1.2.2.2.cmml" id="S3.E8.m1.2.2d.cmml" xref="S3.E8.m1.2.2"></share><apply id="S3.E8.m1.2.2.7.cmml" xref="S3.E8.m1.2.2.7"><csymbol cd="ambiguous" id="S3.E8.m1.2.2.7.1.cmml" xref="S3.E8.m1.2.2.7">superscript</csymbol><ci id="S3.E8.m1.2.2.7.2.cmml" xref="S3.E8.m1.2.2.7.2">ℝ</ci><apply id="S3.E8.m1.2.2.7.3.cmml" xref="S3.E8.m1.2.2.7.3"><times id="S3.E8.m1.2.2.7.3.1.cmml" xref="S3.E8.m1.2.2.7.3.1"></times><ci id="S3.E8.m1.2.2.7.3.2.cmml" xref="S3.E8.m1.2.2.7.3.2">𝑘</ci><cn type="integer" id="S3.E8.m1.2.2.7.3.3.cmml" xref="S3.E8.m1.2.2.7.3.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E8.m1.2c">V=\mathcal{F}(V_{G},V_{L})\in\mathbb{R}^{k\times 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S3.SS1.SSS3.p6.5" class="ltx_p">where <math id="S3.SS1.SSS3.p6.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.SSS3.p6.1.m1.1a"><mi id="S3.SS1.SSS3.p6.1.m1.1.1" xref="S3.SS1.SSS3.p6.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.1.m1.1b"><ci id="S3.SS1.SSS3.p6.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p6.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.1.m1.1c">k</annotation></semantics></math> relies on the operation <math id="S3.SS1.SSS3.p6.2.m2.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS1.SSS3.p6.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS3.p6.2.m2.1.1" xref="S3.SS1.SSS3.p6.2.m2.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.2.m2.1b"><ci id="S3.SS1.SSS3.p6.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p6.2.m2.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.2.m2.1c">\mathcal{F}</annotation></semantics></math>. This equation represents the calculation for combining the global features, denoted as <math id="S3.SS1.SSS3.p6.3.m3.1" class="ltx_Math" alttext="V_{G}" display="inline"><semantics id="S3.SS1.SSS3.p6.3.m3.1a"><msub id="S3.SS1.SSS3.p6.3.m3.1.1" xref="S3.SS1.SSS3.p6.3.m3.1.1.cmml"><mi id="S3.SS1.SSS3.p6.3.m3.1.1.2" xref="S3.SS1.SSS3.p6.3.m3.1.1.2.cmml">V</mi><mi id="S3.SS1.SSS3.p6.3.m3.1.1.3" xref="S3.SS1.SSS3.p6.3.m3.1.1.3.cmml">G</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.3.m3.1b"><apply id="S3.SS1.SSS3.p6.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p6.3.m3.1.1.1.cmml" xref="S3.SS1.SSS3.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p6.3.m3.1.1.2.cmml" xref="S3.SS1.SSS3.p6.3.m3.1.1.2">𝑉</ci><ci id="S3.SS1.SSS3.p6.3.m3.1.1.3.cmml" xref="S3.SS1.SSS3.p6.3.m3.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.3.m3.1c">V_{G}</annotation></semantics></math>, and the local features, denoted as <math id="S3.SS1.SSS3.p6.4.m4.1" class="ltx_Math" alttext="V_{L}" display="inline"><semantics id="S3.SS1.SSS3.p6.4.m4.1a"><msub id="S3.SS1.SSS3.p6.4.m4.1.1" xref="S3.SS1.SSS3.p6.4.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p6.4.m4.1.1.2" xref="S3.SS1.SSS3.p6.4.m4.1.1.2.cmml">V</mi><mi id="S3.SS1.SSS3.p6.4.m4.1.1.3" xref="S3.SS1.SSS3.p6.4.m4.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.4.m4.1b"><apply id="S3.SS1.SSS3.p6.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p6.4.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS3.p6.4.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p6.4.m4.1.1.2">𝑉</ci><ci id="S3.SS1.SSS3.p6.4.m4.1.1.3.cmml" xref="S3.SS1.SSS3.p6.4.m4.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.4.m4.1c">V_{L}</annotation></semantics></math>, through a fusion function <math id="S3.SS1.SSS3.p6.5.m5.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S3.SS1.SSS3.p6.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS3.p6.5.m5.1.1" xref="S3.SS1.SSS3.p6.5.m5.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p6.5.m5.1b"><ci id="S3.SS1.SSS3.p6.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p6.5.m5.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p6.5.m5.1c">\mathcal{F}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Text Embedding Module</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Throughout Tran et al.’s study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, they observed the prevalence of grammatical errors and translation inaccuracies in ViVQA questions, even when validated by highly qualified annotators. Therefore, instead of relying on PhoBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to extract question information, which predominantly comprehends word meanings in diverse contexts, our preference is to employ BARTpho <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. BARTpho stands out as a denoising autoencoder specifically crafted for pretraining sequence-to-sequence models geared towards Vietnamese language processing. The training regimen involves a two-step process: (1) corrupting the input text using a designated noising function and (2) learning to reconstruct the original text. This pretraining task empowers BARTpho with the capability to acquire robust representations resilient to noise, ambiguity, and variations in the input text. Hence, our approach leans towards leveraging the capabilities of BARTpho.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.5" class="ltx_p">BARTpho offers two distinct variants, specifically BARTpho<sub id="S3.SS2.p2.5.1" class="ltx_sub"><span id="S3.SS2.p2.5.1.1" class="ltx_text ltx_font_italic">syllyble</span></sub>, which is optimized for processing at the syllable level, and BARTpho<sub id="S3.SS2.p2.5.2" class="ltx_sub"><span id="S3.SS2.p2.5.2.1" class="ltx_text ltx_font_italic">word</span></sub>, which is designed for handling language at the word level. In the case of BARTPho<sub id="S3.SS2.p2.5.3" class="ltx_sub"><span id="S3.SS2.p2.5.3.1" class="ltx_text ltx_font_italic">word</span></sub>, the sentence must undergo preprocessing, being appropriately split into individual words and phrases. Subsequently, the input of complete sentences is fed into both the encoder and decoder, with the top hidden state of the decoder serving as a representation for each word. In our model, we use BARTpho<sub id="S3.SS2.p2.5.4" class="ltx_sub"><span id="S3.SS2.p2.5.4.1" class="ltx_text ltx_font_italic">word</span></sub> and the textual features <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">T</annotation></semantics></math> can be articulated as:</p>
<table id="S3.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E9.m1.2" class="ltx_Math" alttext="Q=\text{BARTpho}_{word}(T)\in\mathbb{R}^{(L+2)\times\ 1024}" display="block"><semantics id="S3.E9.m1.2a"><mrow id="S3.E9.m1.2.3" xref="S3.E9.m1.2.3.cmml"><mi id="S3.E9.m1.2.3.2" xref="S3.E9.m1.2.3.2.cmml">Q</mi><mo id="S3.E9.m1.2.3.3" xref="S3.E9.m1.2.3.3.cmml">=</mo><mrow id="S3.E9.m1.2.3.4" xref="S3.E9.m1.2.3.4.cmml"><msub id="S3.E9.m1.2.3.4.2" xref="S3.E9.m1.2.3.4.2.cmml"><mtext id="S3.E9.m1.2.3.4.2.2" xref="S3.E9.m1.2.3.4.2.2a.cmml">BARTpho</mtext><mrow id="S3.E9.m1.2.3.4.2.3" xref="S3.E9.m1.2.3.4.2.3.cmml"><mi id="S3.E9.m1.2.3.4.2.3.2" xref="S3.E9.m1.2.3.4.2.3.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.3.4.2.3.1" xref="S3.E9.m1.2.3.4.2.3.1.cmml">​</mo><mi id="S3.E9.m1.2.3.4.2.3.3" xref="S3.E9.m1.2.3.4.2.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.3.4.2.3.1a" xref="S3.E9.m1.2.3.4.2.3.1.cmml">​</mo><mi id="S3.E9.m1.2.3.4.2.3.4" xref="S3.E9.m1.2.3.4.2.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.3.4.2.3.1b" xref="S3.E9.m1.2.3.4.2.3.1.cmml">​</mo><mi id="S3.E9.m1.2.3.4.2.3.5" xref="S3.E9.m1.2.3.4.2.3.5.cmml">d</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S3.E9.m1.2.3.4.1" xref="S3.E9.m1.2.3.4.1.cmml">​</mo><mrow id="S3.E9.m1.2.3.4.3.2" xref="S3.E9.m1.2.3.4.cmml"><mo stretchy="false" id="S3.E9.m1.2.3.4.3.2.1" xref="S3.E9.m1.2.3.4.cmml">(</mo><mi id="S3.E9.m1.2.2" xref="S3.E9.m1.2.2.cmml">T</mi><mo stretchy="false" id="S3.E9.m1.2.3.4.3.2.2" xref="S3.E9.m1.2.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E9.m1.2.3.5" xref="S3.E9.m1.2.3.5.cmml">∈</mo><msup id="S3.E9.m1.2.3.6" xref="S3.E9.m1.2.3.6.cmml"><mi id="S3.E9.m1.2.3.6.2" xref="S3.E9.m1.2.3.6.2.cmml">ℝ</mi><mrow id="S3.E9.m1.1.1.1" xref="S3.E9.m1.1.1.1.cmml"><mrow id="S3.E9.m1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E9.m1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E9.m1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.cmml"><mi id="S3.E9.m1.1.1.1.1.1.1.2" xref="S3.E9.m1.1.1.1.1.1.1.2.cmml">L</mi><mo id="S3.E9.m1.1.1.1.1.1.1.1" xref="S3.E9.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E9.m1.1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.E9.m1.1.1.1.1.1.3" xref="S3.E9.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E9.m1.1.1.1.2" xref="S3.E9.m1.1.1.1.2.cmml">×</mo><mn id="S3.E9.m1.1.1.1.3" xref="S3.E9.m1.1.1.1.3.cmml"> 1024</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E9.m1.2b"><apply id="S3.E9.m1.2.3.cmml" xref="S3.E9.m1.2.3"><and id="S3.E9.m1.2.3a.cmml" xref="S3.E9.m1.2.3"></and><apply id="S3.E9.m1.2.3b.cmml" xref="S3.E9.m1.2.3"><eq id="S3.E9.m1.2.3.3.cmml" xref="S3.E9.m1.2.3.3"></eq><ci id="S3.E9.m1.2.3.2.cmml" xref="S3.E9.m1.2.3.2">𝑄</ci><apply id="S3.E9.m1.2.3.4.cmml" xref="S3.E9.m1.2.3.4"><times id="S3.E9.m1.2.3.4.1.cmml" xref="S3.E9.m1.2.3.4.1"></times><apply id="S3.E9.m1.2.3.4.2.cmml" xref="S3.E9.m1.2.3.4.2"><csymbol cd="ambiguous" id="S3.E9.m1.2.3.4.2.1.cmml" xref="S3.E9.m1.2.3.4.2">subscript</csymbol><ci id="S3.E9.m1.2.3.4.2.2a.cmml" xref="S3.E9.m1.2.3.4.2.2"><mtext id="S3.E9.m1.2.3.4.2.2.cmml" xref="S3.E9.m1.2.3.4.2.2">BARTpho</mtext></ci><apply id="S3.E9.m1.2.3.4.2.3.cmml" xref="S3.E9.m1.2.3.4.2.3"><times id="S3.E9.m1.2.3.4.2.3.1.cmml" xref="S3.E9.m1.2.3.4.2.3.1"></times><ci id="S3.E9.m1.2.3.4.2.3.2.cmml" xref="S3.E9.m1.2.3.4.2.3.2">𝑤</ci><ci id="S3.E9.m1.2.3.4.2.3.3.cmml" xref="S3.E9.m1.2.3.4.2.3.3">𝑜</ci><ci id="S3.E9.m1.2.3.4.2.3.4.cmml" xref="S3.E9.m1.2.3.4.2.3.4">𝑟</ci><ci id="S3.E9.m1.2.3.4.2.3.5.cmml" xref="S3.E9.m1.2.3.4.2.3.5">𝑑</ci></apply></apply><ci id="S3.E9.m1.2.2.cmml" xref="S3.E9.m1.2.2">𝑇</ci></apply></apply><apply id="S3.E9.m1.2.3c.cmml" xref="S3.E9.m1.2.3"><in id="S3.E9.m1.2.3.5.cmml" xref="S3.E9.m1.2.3.5"></in><share href="#S3.E9.m1.2.3.4.cmml" id="S3.E9.m1.2.3d.cmml" xref="S3.E9.m1.2.3"></share><apply id="S3.E9.m1.2.3.6.cmml" xref="S3.E9.m1.2.3.6"><csymbol cd="ambiguous" id="S3.E9.m1.2.3.6.1.cmml" xref="S3.E9.m1.2.3.6">superscript</csymbol><ci id="S3.E9.m1.2.3.6.2.cmml" xref="S3.E9.m1.2.3.6.2">ℝ</ci><apply id="S3.E9.m1.1.1.1.cmml" xref="S3.E9.m1.1.1.1"><times id="S3.E9.m1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.2"></times><apply id="S3.E9.m1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1"><plus id="S3.E9.m1.1.1.1.1.1.1.1.cmml" xref="S3.E9.m1.1.1.1.1.1.1.1"></plus><ci id="S3.E9.m1.1.1.1.1.1.1.2.cmml" xref="S3.E9.m1.1.1.1.1.1.1.2">𝐿</ci><cn type="integer" id="S3.E9.m1.1.1.1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E9.m1.1.1.1.3.cmml" xref="S3.E9.m1.1.1.1.3">1024</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E9.m1.2c">Q=\text{BARTpho}_{word}(T)\in\mathbb{R}^{(L+2)\times\ 1024}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.7" class="ltx_p">where L + 2 with L is the max sequence length, adding 2 special token [<math id="S3.SS2.p2.6.m1.1" class="ltx_Math" alttext="CLS" display="inline"><semantics id="S3.SS2.p2.6.m1.1a"><mrow id="S3.SS2.p2.6.m1.1.1" xref="S3.SS2.p2.6.m1.1.1.cmml"><mi id="S3.SS2.p2.6.m1.1.1.2" xref="S3.SS2.p2.6.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m1.1.1.1" xref="S3.SS2.p2.6.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.6.m1.1.1.3" xref="S3.SS2.p2.6.m1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.6.m1.1.1.1a" xref="S3.SS2.p2.6.m1.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.6.m1.1.1.4" xref="S3.SS2.p2.6.m1.1.1.4.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m1.1b"><apply id="S3.SS2.p2.6.m1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1"><times id="S3.SS2.p2.6.m1.1.1.1.cmml" xref="S3.SS2.p2.6.m1.1.1.1"></times><ci id="S3.SS2.p2.6.m1.1.1.2.cmml" xref="S3.SS2.p2.6.m1.1.1.2">𝐶</ci><ci id="S3.SS2.p2.6.m1.1.1.3.cmml" xref="S3.SS2.p2.6.m1.1.1.3">𝐿</ci><ci id="S3.SS2.p2.6.m1.1.1.4.cmml" xref="S3.SS2.p2.6.m1.1.1.4">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m1.1c">CLS</annotation></semantics></math>] and [<math id="S3.SS2.p2.7.m2.1" class="ltx_Math" alttext="SEP" display="inline"><semantics id="S3.SS2.p2.7.m2.1a"><mrow id="S3.SS2.p2.7.m2.1.1" xref="S3.SS2.p2.7.m2.1.1.cmml"><mi id="S3.SS2.p2.7.m2.1.1.2" xref="S3.SS2.p2.7.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m2.1.1.1" xref="S3.SS2.p2.7.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.7.m2.1.1.3" xref="S3.SS2.p2.7.m2.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.7.m2.1.1.1a" xref="S3.SS2.p2.7.m2.1.1.1.cmml">​</mo><mi id="S3.SS2.p2.7.m2.1.1.4" xref="S3.SS2.p2.7.m2.1.1.4.cmml">P</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m2.1b"><apply id="S3.SS2.p2.7.m2.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1"><times id="S3.SS2.p2.7.m2.1.1.1.cmml" xref="S3.SS2.p2.7.m2.1.1.1"></times><ci id="S3.SS2.p2.7.m2.1.1.2.cmml" xref="S3.SS2.p2.7.m2.1.1.2">𝑆</ci><ci id="S3.SS2.p2.7.m2.1.1.3.cmml" xref="S3.SS2.p2.7.m2.1.1.3">𝐸</ci><ci id="S3.SS2.p2.7.m2.1.1.4.cmml" xref="S3.SS2.p2.7.m2.1.1.4">𝑃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m2.1c">SEP</annotation></semantics></math>] representing the start-of-sentence and the end-of-sentence token.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Since the BLIP-2 in the image embedding module is predominantly trained on English data (specifically, images with English captions) and the text embedding module is tailored for Vietnamese, we set the parameters of the text embedding module to be trainable. This adaptability enables the text representation to dynamically adjust and align with the visual representation within the image embedding module.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Multi-modal Fusion Module</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Before forwarding the representations of visual and question features to BEiT-3 for multimodal fusion, a Feed-Forward Network (FFN) layer is applied on the textual features. The purpose is to diminish the dimensionality from 1024 (<a href="#S3.E9" title="In 3.2 Text Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>) to 768. This reduction is imperative due to the requirement of concatenating the embeddings of a given question and an image within this component. The dimension of the question is now given by:</p>
<table id="S3.E10" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E10.m1.2" class="ltx_Math" alttext="Q=\text{FFN}(Q)\in\mathbb{R}^{(L+2)\times\ 768}" display="block"><semantics id="S3.E10.m1.2a"><mrow id="S3.E10.m1.2.3" xref="S3.E10.m1.2.3.cmml"><mi id="S3.E10.m1.2.3.2" xref="S3.E10.m1.2.3.2.cmml">Q</mi><mo id="S3.E10.m1.2.3.3" xref="S3.E10.m1.2.3.3.cmml">=</mo><mrow id="S3.E10.m1.2.3.4" xref="S3.E10.m1.2.3.4.cmml"><mtext id="S3.E10.m1.2.3.4.2" xref="S3.E10.m1.2.3.4.2a.cmml">FFN</mtext><mo lspace="0em" rspace="0em" id="S3.E10.m1.2.3.4.1" xref="S3.E10.m1.2.3.4.1.cmml">​</mo><mrow id="S3.E10.m1.2.3.4.3.2" xref="S3.E10.m1.2.3.4.cmml"><mo stretchy="false" id="S3.E10.m1.2.3.4.3.2.1" xref="S3.E10.m1.2.3.4.cmml">(</mo><mi id="S3.E10.m1.2.2" xref="S3.E10.m1.2.2.cmml">Q</mi><mo stretchy="false" id="S3.E10.m1.2.3.4.3.2.2" xref="S3.E10.m1.2.3.4.cmml">)</mo></mrow></mrow><mo id="S3.E10.m1.2.3.5" xref="S3.E10.m1.2.3.5.cmml">∈</mo><msup id="S3.E10.m1.2.3.6" xref="S3.E10.m1.2.3.6.cmml"><mi id="S3.E10.m1.2.3.6.2" xref="S3.E10.m1.2.3.6.2.cmml">ℝ</mi><mrow id="S3.E10.m1.1.1.1" xref="S3.E10.m1.1.1.1.cmml"><mrow id="S3.E10.m1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E10.m1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E10.m1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.cmml"><mi id="S3.E10.m1.1.1.1.1.1.1.2" xref="S3.E10.m1.1.1.1.1.1.1.2.cmml">L</mi><mo id="S3.E10.m1.1.1.1.1.1.1.1" xref="S3.E10.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E10.m1.1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.3.cmml">2</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.E10.m1.1.1.1.1.1.3" xref="S3.E10.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E10.m1.1.1.1.2" xref="S3.E10.m1.1.1.1.2.cmml">×</mo><mn id="S3.E10.m1.1.1.1.3" xref="S3.E10.m1.1.1.1.3.cmml"> 768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E10.m1.2b"><apply id="S3.E10.m1.2.3.cmml" xref="S3.E10.m1.2.3"><and id="S3.E10.m1.2.3a.cmml" xref="S3.E10.m1.2.3"></and><apply id="S3.E10.m1.2.3b.cmml" xref="S3.E10.m1.2.3"><eq id="S3.E10.m1.2.3.3.cmml" xref="S3.E10.m1.2.3.3"></eq><ci id="S3.E10.m1.2.3.2.cmml" xref="S3.E10.m1.2.3.2">𝑄</ci><apply id="S3.E10.m1.2.3.4.cmml" xref="S3.E10.m1.2.3.4"><times id="S3.E10.m1.2.3.4.1.cmml" xref="S3.E10.m1.2.3.4.1"></times><ci id="S3.E10.m1.2.3.4.2a.cmml" xref="S3.E10.m1.2.3.4.2"><mtext id="S3.E10.m1.2.3.4.2.cmml" xref="S3.E10.m1.2.3.4.2">FFN</mtext></ci><ci id="S3.E10.m1.2.2.cmml" xref="S3.E10.m1.2.2">𝑄</ci></apply></apply><apply id="S3.E10.m1.2.3c.cmml" xref="S3.E10.m1.2.3"><in id="S3.E10.m1.2.3.5.cmml" xref="S3.E10.m1.2.3.5"></in><share href="#S3.E10.m1.2.3.4.cmml" id="S3.E10.m1.2.3d.cmml" xref="S3.E10.m1.2.3"></share><apply id="S3.E10.m1.2.3.6.cmml" xref="S3.E10.m1.2.3.6"><csymbol cd="ambiguous" id="S3.E10.m1.2.3.6.1.cmml" xref="S3.E10.m1.2.3.6">superscript</csymbol><ci id="S3.E10.m1.2.3.6.2.cmml" xref="S3.E10.m1.2.3.6.2">ℝ</ci><apply id="S3.E10.m1.1.1.1.cmml" xref="S3.E10.m1.1.1.1"><times id="S3.E10.m1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.2"></times><apply id="S3.E10.m1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1"><plus id="S3.E10.m1.1.1.1.1.1.1.1.cmml" xref="S3.E10.m1.1.1.1.1.1.1.1"></plus><ci id="S3.E10.m1.1.1.1.1.1.1.2.cmml" xref="S3.E10.m1.1.1.1.1.1.1.2">𝐿</ci><cn type="integer" id="S3.E10.m1.1.1.1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.1.1.1.3">2</cn></apply><cn type="integer" id="S3.E10.m1.1.1.1.3.cmml" xref="S3.E10.m1.1.1.1.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E10.m1.2c">Q=\text{FFN}(Q)\in\mathbb{R}^{(L+2)\times\ 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">This preprocessing step optimizes the dimensionality of the textual features, enhancing their compatibility for subsequent fusion with visual features. Subsequently, the visual (<a href="#S3.E8" title="In 3.1.3 Image Embedding ‣ 3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>) and question (<a href="#S3.E10" title="In 3.3 Multi-modal Fusion Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>) features are concatenated:</p>
<table id="S3.E11" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E11.m1.3" class="ltx_Math" alttext="f_{VQ}=\text{concatenate}(V,Q)\in\mathbb{R}^{(k+L+2)\times 768}" display="block"><semantics id="S3.E11.m1.3a"><mrow id="S3.E11.m1.3.4" xref="S3.E11.m1.3.4.cmml"><msub id="S3.E11.m1.3.4.2" xref="S3.E11.m1.3.4.2.cmml"><mi id="S3.E11.m1.3.4.2.2" xref="S3.E11.m1.3.4.2.2.cmml">f</mi><mrow id="S3.E11.m1.3.4.2.3" xref="S3.E11.m1.3.4.2.3.cmml"><mi id="S3.E11.m1.3.4.2.3.2" xref="S3.E11.m1.3.4.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E11.m1.3.4.2.3.1" xref="S3.E11.m1.3.4.2.3.1.cmml">​</mo><mi id="S3.E11.m1.3.4.2.3.3" xref="S3.E11.m1.3.4.2.3.3.cmml">Q</mi></mrow></msub><mo id="S3.E11.m1.3.4.3" xref="S3.E11.m1.3.4.3.cmml">=</mo><mrow id="S3.E11.m1.3.4.4" xref="S3.E11.m1.3.4.4.cmml"><mtext id="S3.E11.m1.3.4.4.2" xref="S3.E11.m1.3.4.4.2a.cmml">concatenate</mtext><mo lspace="0em" rspace="0em" id="S3.E11.m1.3.4.4.1" xref="S3.E11.m1.3.4.4.1.cmml">​</mo><mrow id="S3.E11.m1.3.4.4.3.2" xref="S3.E11.m1.3.4.4.3.1.cmml"><mo stretchy="false" id="S3.E11.m1.3.4.4.3.2.1" xref="S3.E11.m1.3.4.4.3.1.cmml">(</mo><mi id="S3.E11.m1.2.2" xref="S3.E11.m1.2.2.cmml">V</mi><mo id="S3.E11.m1.3.4.4.3.2.2" xref="S3.E11.m1.3.4.4.3.1.cmml">,</mo><mi id="S3.E11.m1.3.3" xref="S3.E11.m1.3.3.cmml">Q</mi><mo stretchy="false" id="S3.E11.m1.3.4.4.3.2.3" xref="S3.E11.m1.3.4.4.3.1.cmml">)</mo></mrow></mrow><mo id="S3.E11.m1.3.4.5" xref="S3.E11.m1.3.4.5.cmml">∈</mo><msup id="S3.E11.m1.3.4.6" xref="S3.E11.m1.3.4.6.cmml"><mi id="S3.E11.m1.3.4.6.2" xref="S3.E11.m1.3.4.6.2.cmml">ℝ</mi><mrow id="S3.E11.m1.1.1.1" xref="S3.E11.m1.1.1.1.cmml"><mrow id="S3.E11.m1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E11.m1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E11.m1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.cmml"><mi id="S3.E11.m1.1.1.1.1.1.1.2" xref="S3.E11.m1.1.1.1.1.1.1.2.cmml">k</mi><mo id="S3.E11.m1.1.1.1.1.1.1.1" xref="S3.E11.m1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E11.m1.1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.3.cmml">L</mi><mo id="S3.E11.m1.1.1.1.1.1.1.1a" xref="S3.E11.m1.1.1.1.1.1.1.1.cmml">+</mo><mn id="S3.E11.m1.1.1.1.1.1.1.4" xref="S3.E11.m1.1.1.1.1.1.1.4.cmml">2</mn></mrow><mo rspace="0.055em" stretchy="false" id="S3.E11.m1.1.1.1.1.1.3" xref="S3.E11.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E11.m1.1.1.1.2" xref="S3.E11.m1.1.1.1.2.cmml">×</mo><mn id="S3.E11.m1.1.1.1.3" xref="S3.E11.m1.1.1.1.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.E11.m1.3b"><apply id="S3.E11.m1.3.4.cmml" xref="S3.E11.m1.3.4"><and id="S3.E11.m1.3.4a.cmml" xref="S3.E11.m1.3.4"></and><apply id="S3.E11.m1.3.4b.cmml" xref="S3.E11.m1.3.4"><eq id="S3.E11.m1.3.4.3.cmml" xref="S3.E11.m1.3.4.3"></eq><apply id="S3.E11.m1.3.4.2.cmml" xref="S3.E11.m1.3.4.2"><csymbol cd="ambiguous" id="S3.E11.m1.3.4.2.1.cmml" xref="S3.E11.m1.3.4.2">subscript</csymbol><ci id="S3.E11.m1.3.4.2.2.cmml" xref="S3.E11.m1.3.4.2.2">𝑓</ci><apply id="S3.E11.m1.3.4.2.3.cmml" xref="S3.E11.m1.3.4.2.3"><times id="S3.E11.m1.3.4.2.3.1.cmml" xref="S3.E11.m1.3.4.2.3.1"></times><ci id="S3.E11.m1.3.4.2.3.2.cmml" xref="S3.E11.m1.3.4.2.3.2">𝑉</ci><ci id="S3.E11.m1.3.4.2.3.3.cmml" xref="S3.E11.m1.3.4.2.3.3">𝑄</ci></apply></apply><apply id="S3.E11.m1.3.4.4.cmml" xref="S3.E11.m1.3.4.4"><times id="S3.E11.m1.3.4.4.1.cmml" xref="S3.E11.m1.3.4.4.1"></times><ci id="S3.E11.m1.3.4.4.2a.cmml" xref="S3.E11.m1.3.4.4.2"><mtext id="S3.E11.m1.3.4.4.2.cmml" xref="S3.E11.m1.3.4.4.2">concatenate</mtext></ci><interval closure="open" id="S3.E11.m1.3.4.4.3.1.cmml" xref="S3.E11.m1.3.4.4.3.2"><ci id="S3.E11.m1.2.2.cmml" xref="S3.E11.m1.2.2">𝑉</ci><ci id="S3.E11.m1.3.3.cmml" xref="S3.E11.m1.3.3">𝑄</ci></interval></apply></apply><apply id="S3.E11.m1.3.4c.cmml" xref="S3.E11.m1.3.4"><in id="S3.E11.m1.3.4.5.cmml" xref="S3.E11.m1.3.4.5"></in><share href="#S3.E11.m1.3.4.4.cmml" id="S3.E11.m1.3.4d.cmml" xref="S3.E11.m1.3.4"></share><apply id="S3.E11.m1.3.4.6.cmml" xref="S3.E11.m1.3.4.6"><csymbol cd="ambiguous" id="S3.E11.m1.3.4.6.1.cmml" xref="S3.E11.m1.3.4.6">superscript</csymbol><ci id="S3.E11.m1.3.4.6.2.cmml" xref="S3.E11.m1.3.4.6.2">ℝ</ci><apply id="S3.E11.m1.1.1.1.cmml" xref="S3.E11.m1.1.1.1"><times id="S3.E11.m1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.2"></times><apply id="S3.E11.m1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1"><plus id="S3.E11.m1.1.1.1.1.1.1.1.cmml" xref="S3.E11.m1.1.1.1.1.1.1.1"></plus><ci id="S3.E11.m1.1.1.1.1.1.1.2.cmml" xref="S3.E11.m1.1.1.1.1.1.1.2">𝑘</ci><ci id="S3.E11.m1.1.1.1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.1.1.1.3">𝐿</ci><cn type="integer" id="S3.E11.m1.1.1.1.1.1.1.4.cmml" xref="S3.E11.m1.1.1.1.1.1.1.4">2</cn></apply><cn type="integer" id="S3.E11.m1.1.1.1.3.cmml" xref="S3.E11.m1.1.1.1.3">768</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E11.m1.3c">f_{VQ}=\text{concatenate}(V,Q)\in\mathbb{R}^{(k+L+2)\times 768}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Following this, the concatenated features are inputted into Multiway Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to jointly encode the image-question pair. Each Multiway Transformer block is composed of a shared self-attention module and a set of feed-forward networks tailored for diverse modalities. Each layer includes both a vision expert and a language expert, each responsible for processing the input tokens directed to them. Leveraging a group of modality experts incentivizes the model to capture a broader range of modality-specific information. The shared self-attention module acquires the ability to discern alignments between diverse modalities, facilitating profound fusion for tasks involving multiple modalities, such as vision-language tasks.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.3" class="ltx_p">The result will have the same dimension as the input <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="f_{VQ}" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><msub id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">f</mi><mrow id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.2" xref="S3.SS3.p4.1.m1.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.1.3.1" xref="S3.SS3.p4.1.m1.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.1.m1.1.1.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.cmml">Q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">𝑓</ci><apply id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><times id="S3.SS3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3.1"></times><ci id="S3.SS3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.2">𝑉</ci><ci id="S3.SS3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">f_{VQ}</annotation></semantics></math> (<a href="#S3.E11" title="In 3.3 Multi-modal Fusion Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>). The first vector in the output fusion is earmarked for classification, denoted as <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="\hat{f}_{VQ}\in\mathbb{R}^{1\times 768}" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mrow id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><msub id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml"><mover accent="true" id="S3.SS3.p4.2.m2.1.1.2.2" xref="S3.SS3.p4.2.m2.1.1.2.2.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2.2.2" xref="S3.SS3.p4.2.m2.1.1.2.2.2.cmml">f</mi><mo id="S3.SS3.p4.2.m2.1.1.2.2.1" xref="S3.SS3.p4.2.m2.1.1.2.2.1.cmml">^</mo></mover><mrow id="S3.SS3.p4.2.m2.1.1.2.3" xref="S3.SS3.p4.2.m2.1.1.2.3.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2.3.2" xref="S3.SS3.p4.2.m2.1.1.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.2.m2.1.1.2.3.1" xref="S3.SS3.p4.2.m2.1.1.2.3.1.cmml">​</mo><mi id="S3.SS3.p4.2.m2.1.1.2.3.3" xref="S3.SS3.p4.2.m2.1.1.2.3.3.cmml">Q</mi></mrow></msub><mo id="S3.SS3.p4.2.m2.1.1.1" xref="S3.SS3.p4.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml"><mi id="S3.SS3.p4.2.m2.1.1.3.2" xref="S3.SS3.p4.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p4.2.m2.1.1.3.3" xref="S3.SS3.p4.2.m2.1.1.3.3.cmml"><mn id="S3.SS3.p4.2.m2.1.1.3.3.2" xref="S3.SS3.p4.2.m2.1.1.3.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p4.2.m2.1.1.3.3.1" xref="S3.SS3.p4.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p4.2.m2.1.1.3.3.3" xref="S3.SS3.p4.2.m2.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><in id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1.1"></in><apply id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.2.1.cmml" xref="S3.SS3.p4.2.m2.1.1.2">subscript</csymbol><apply id="S3.SS3.p4.2.m2.1.1.2.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2.2"><ci id="S3.SS3.p4.2.m2.1.1.2.2.1.cmml" xref="S3.SS3.p4.2.m2.1.1.2.2.1">^</ci><ci id="S3.SS3.p4.2.m2.1.1.2.2.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2.2.2">𝑓</ci></apply><apply id="S3.SS3.p4.2.m2.1.1.2.3.cmml" xref="S3.SS3.p4.2.m2.1.1.2.3"><times id="S3.SS3.p4.2.m2.1.1.2.3.1.cmml" xref="S3.SS3.p4.2.m2.1.1.2.3.1"></times><ci id="S3.SS3.p4.2.m2.1.1.2.3.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2.3.2">𝑉</ci><ci id="S3.SS3.p4.2.m2.1.1.2.3.3.cmml" xref="S3.SS3.p4.2.m2.1.1.2.3.3">𝑄</ci></apply></apply><apply id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.3.1.cmml" xref="S3.SS3.p4.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.3.2.cmml" xref="S3.SS3.p4.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS3.p4.2.m2.1.1.3.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3"><times id="S3.SS3.p4.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3.1"></times><cn type="integer" id="S3.SS3.p4.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3.2">1</cn><cn type="integer" id="S3.SS3.p4.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\hat{f}_{VQ}\in\mathbb{R}^{1\times 768}</annotation></semantics></math>.
Prior to being input into the Classifier module, <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="\hat{f}_{VQ}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><msub id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mover accent="true" id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2.2" xref="S3.SS3.p4.3.m3.1.1.2.2.cmml">f</mi><mo id="S3.SS3.p4.3.m3.1.1.2.1" xref="S3.SS3.p4.3.m3.1.1.2.1.cmml">^</mo></mover><mrow id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml"><mi id="S3.SS3.p4.3.m3.1.1.3.2" xref="S3.SS3.p4.3.m3.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.SS3.p4.3.m3.1.1.3.1" xref="S3.SS3.p4.3.m3.1.1.3.1.cmml">​</mo><mi id="S3.SS3.p4.3.m3.1.1.3.3" xref="S3.SS3.p4.3.m3.1.1.3.3.cmml">Q</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">subscript</csymbol><apply id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2"><ci id="S3.SS3.p4.3.m3.1.1.2.1.cmml" xref="S3.SS3.p4.3.m3.1.1.2.1">^</ci><ci id="S3.SS3.p4.3.m3.1.1.2.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2.2">𝑓</ci></apply><apply id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3"><times id="S3.SS3.p4.3.m3.1.1.3.1.cmml" xref="S3.SS3.p4.3.m3.1.1.3.1"></times><ci id="S3.SS3.p4.3.m3.1.1.3.2.cmml" xref="S3.SS3.p4.3.m3.1.1.3.2">𝑉</ci><ci id="S3.SS3.p4.3.m3.1.1.3.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3.3">𝑄</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\hat{f}_{VQ}</annotation></semantics></math> undergoes processing through a pooler layer, involving the normalization, a FFN layer with hidden and output dimension of 768, and application of the hyperbolic tangent function as depicted below:</p>
<table id="S3.E12" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E12.m1.1" class="ltx_Math" alttext="\hat{f}_{VQ}=\text{Norm}(\hat{f}_{VQ})" display="block"><semantics id="S3.E12.m1.1a"><mrow id="S3.E12.m1.1.1" xref="S3.E12.m1.1.1.cmml"><msub id="S3.E12.m1.1.1.3" xref="S3.E12.m1.1.1.3.cmml"><mover accent="true" id="S3.E12.m1.1.1.3.2" xref="S3.E12.m1.1.1.3.2.cmml"><mi id="S3.E12.m1.1.1.3.2.2" xref="S3.E12.m1.1.1.3.2.2.cmml">f</mi><mo id="S3.E12.m1.1.1.3.2.1" xref="S3.E12.m1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E12.m1.1.1.3.3" xref="S3.E12.m1.1.1.3.3.cmml"><mi id="S3.E12.m1.1.1.3.3.2" xref="S3.E12.m1.1.1.3.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.3.3.1" xref="S3.E12.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E12.m1.1.1.3.3.3" xref="S3.E12.m1.1.1.3.3.3.cmml">Q</mi></mrow></msub><mo id="S3.E12.m1.1.1.2" xref="S3.E12.m1.1.1.2.cmml">=</mo><mrow id="S3.E12.m1.1.1.1" xref="S3.E12.m1.1.1.1.cmml"><mtext id="S3.E12.m1.1.1.1.3" xref="S3.E12.m1.1.1.1.3a.cmml">Norm</mtext><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.2" xref="S3.E12.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E12.m1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E12.m1.1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E12.m1.1.1.1.1.1.1" xref="S3.E12.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E12.m1.1.1.1.1.1.1.2" xref="S3.E12.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E12.m1.1.1.1.1.1.1.2.2" xref="S3.E12.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mo id="S3.E12.m1.1.1.1.1.1.1.2.1" xref="S3.E12.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E12.m1.1.1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E12.m1.1.1.1.1.1.1.3.2" xref="S3.E12.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E12.m1.1.1.1.1.1.1.3.1" xref="S3.E12.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E12.m1.1.1.1.1.1.1.3.3" xref="S3.E12.m1.1.1.1.1.1.1.3.3.cmml">Q</mi></mrow></msub><mo stretchy="false" id="S3.E12.m1.1.1.1.1.1.3" xref="S3.E12.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E12.m1.1b"><apply id="S3.E12.m1.1.1.cmml" xref="S3.E12.m1.1.1"><eq id="S3.E12.m1.1.1.2.cmml" xref="S3.E12.m1.1.1.2"></eq><apply id="S3.E12.m1.1.1.3.cmml" xref="S3.E12.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.3.1.cmml" xref="S3.E12.m1.1.1.3">subscript</csymbol><apply id="S3.E12.m1.1.1.3.2.cmml" xref="S3.E12.m1.1.1.3.2"><ci id="S3.E12.m1.1.1.3.2.1.cmml" xref="S3.E12.m1.1.1.3.2.1">^</ci><ci id="S3.E12.m1.1.1.3.2.2.cmml" xref="S3.E12.m1.1.1.3.2.2">𝑓</ci></apply><apply id="S3.E12.m1.1.1.3.3.cmml" xref="S3.E12.m1.1.1.3.3"><times id="S3.E12.m1.1.1.3.3.1.cmml" xref="S3.E12.m1.1.1.3.3.1"></times><ci id="S3.E12.m1.1.1.3.3.2.cmml" xref="S3.E12.m1.1.1.3.3.2">𝑉</ci><ci id="S3.E12.m1.1.1.3.3.3.cmml" xref="S3.E12.m1.1.1.3.3.3">𝑄</ci></apply></apply><apply id="S3.E12.m1.1.1.1.cmml" xref="S3.E12.m1.1.1.1"><times id="S3.E12.m1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.2"></times><ci id="S3.E12.m1.1.1.1.3a.cmml" xref="S3.E12.m1.1.1.1.3"><mtext id="S3.E12.m1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.3">Norm</mtext></ci><apply id="S3.E12.m1.1.1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E12.m1.1.1.1.1.1.1.1.cmml" xref="S3.E12.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E12.m1.1.1.1.1.1.1.2.cmml" xref="S3.E12.m1.1.1.1.1.1.1.2"><ci id="S3.E12.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E12.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E12.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E12.m1.1.1.1.1.1.1.2.2">𝑓</ci></apply><apply id="S3.E12.m1.1.1.1.1.1.1.3.cmml" xref="S3.E12.m1.1.1.1.1.1.1.3"><times id="S3.E12.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E12.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E12.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E12.m1.1.1.1.1.1.1.3.2">𝑉</ci><ci id="S3.E12.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E12.m1.1.1.1.1.1.1.3.3">𝑄</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E12.m1.1c">\hat{f}_{VQ}=\text{Norm}(\hat{f}_{VQ})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<table id="S3.E13" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E13.m1.1" class="ltx_Math" alttext="\hat{f}_{VQ}=\text{FFN}(\hat{f}_{VQ})" display="block"><semantics id="S3.E13.m1.1a"><mrow id="S3.E13.m1.1.1" xref="S3.E13.m1.1.1.cmml"><msub id="S3.E13.m1.1.1.3" xref="S3.E13.m1.1.1.3.cmml"><mover accent="true" id="S3.E13.m1.1.1.3.2" xref="S3.E13.m1.1.1.3.2.cmml"><mi id="S3.E13.m1.1.1.3.2.2" xref="S3.E13.m1.1.1.3.2.2.cmml">f</mi><mo id="S3.E13.m1.1.1.3.2.1" xref="S3.E13.m1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E13.m1.1.1.3.3" xref="S3.E13.m1.1.1.3.3.cmml"><mi id="S3.E13.m1.1.1.3.3.2" xref="S3.E13.m1.1.1.3.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E13.m1.1.1.3.3.1" xref="S3.E13.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E13.m1.1.1.3.3.3" xref="S3.E13.m1.1.1.3.3.3.cmml">Q</mi></mrow></msub><mo id="S3.E13.m1.1.1.2" xref="S3.E13.m1.1.1.2.cmml">=</mo><mrow id="S3.E13.m1.1.1.1" xref="S3.E13.m1.1.1.1.cmml"><mtext id="S3.E13.m1.1.1.1.3" xref="S3.E13.m1.1.1.1.3a.cmml">FFN</mtext><mo lspace="0em" rspace="0em" id="S3.E13.m1.1.1.1.2" xref="S3.E13.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E13.m1.1.1.1.1.1" xref="S3.E13.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E13.m1.1.1.1.1.1.2" xref="S3.E13.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E13.m1.1.1.1.1.1.1" xref="S3.E13.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E13.m1.1.1.1.1.1.1.2" xref="S3.E13.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E13.m1.1.1.1.1.1.1.2.2" xref="S3.E13.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mo id="S3.E13.m1.1.1.1.1.1.1.2.1" xref="S3.E13.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E13.m1.1.1.1.1.1.1.3" xref="S3.E13.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E13.m1.1.1.1.1.1.1.3.2" xref="S3.E13.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E13.m1.1.1.1.1.1.1.3.1" xref="S3.E13.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E13.m1.1.1.1.1.1.1.3.3" xref="S3.E13.m1.1.1.1.1.1.1.3.3.cmml">Q</mi></mrow></msub><mo stretchy="false" id="S3.E13.m1.1.1.1.1.1.3" xref="S3.E13.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E13.m1.1b"><apply id="S3.E13.m1.1.1.cmml" xref="S3.E13.m1.1.1"><eq id="S3.E13.m1.1.1.2.cmml" xref="S3.E13.m1.1.1.2"></eq><apply id="S3.E13.m1.1.1.3.cmml" xref="S3.E13.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E13.m1.1.1.3.1.cmml" xref="S3.E13.m1.1.1.3">subscript</csymbol><apply id="S3.E13.m1.1.1.3.2.cmml" xref="S3.E13.m1.1.1.3.2"><ci id="S3.E13.m1.1.1.3.2.1.cmml" xref="S3.E13.m1.1.1.3.2.1">^</ci><ci id="S3.E13.m1.1.1.3.2.2.cmml" xref="S3.E13.m1.1.1.3.2.2">𝑓</ci></apply><apply id="S3.E13.m1.1.1.3.3.cmml" xref="S3.E13.m1.1.1.3.3"><times id="S3.E13.m1.1.1.3.3.1.cmml" xref="S3.E13.m1.1.1.3.3.1"></times><ci id="S3.E13.m1.1.1.3.3.2.cmml" xref="S3.E13.m1.1.1.3.3.2">𝑉</ci><ci id="S3.E13.m1.1.1.3.3.3.cmml" xref="S3.E13.m1.1.1.3.3.3">𝑄</ci></apply></apply><apply id="S3.E13.m1.1.1.1.cmml" xref="S3.E13.m1.1.1.1"><times id="S3.E13.m1.1.1.1.2.cmml" xref="S3.E13.m1.1.1.1.2"></times><ci id="S3.E13.m1.1.1.1.3a.cmml" xref="S3.E13.m1.1.1.1.3"><mtext id="S3.E13.m1.1.1.1.3.cmml" xref="S3.E13.m1.1.1.1.3">FFN</mtext></ci><apply id="S3.E13.m1.1.1.1.1.1.1.cmml" xref="S3.E13.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E13.m1.1.1.1.1.1.1.1.cmml" xref="S3.E13.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E13.m1.1.1.1.1.1.1.2.cmml" xref="S3.E13.m1.1.1.1.1.1.1.2"><ci id="S3.E13.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E13.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E13.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E13.m1.1.1.1.1.1.1.2.2">𝑓</ci></apply><apply id="S3.E13.m1.1.1.1.1.1.1.3.cmml" xref="S3.E13.m1.1.1.1.1.1.1.3"><times id="S3.E13.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E13.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E13.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E13.m1.1.1.1.1.1.1.3.2">𝑉</ci><ci id="S3.E13.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E13.m1.1.1.1.1.1.1.3.3">𝑄</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E13.m1.1c">\hat{f}_{VQ}=\text{FFN}(\hat{f}_{VQ})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<table id="S3.E14" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E14.m1.1" class="ltx_Math" alttext="\hat{f}_{VQ}=\text{Tanh}(\hat{f}_{VQ})" display="block"><semantics id="S3.E14.m1.1a"><mrow id="S3.E14.m1.1.1" xref="S3.E14.m1.1.1.cmml"><msub id="S3.E14.m1.1.1.3" xref="S3.E14.m1.1.1.3.cmml"><mover accent="true" id="S3.E14.m1.1.1.3.2" xref="S3.E14.m1.1.1.3.2.cmml"><mi id="S3.E14.m1.1.1.3.2.2" xref="S3.E14.m1.1.1.3.2.2.cmml">f</mi><mo id="S3.E14.m1.1.1.3.2.1" xref="S3.E14.m1.1.1.3.2.1.cmml">^</mo></mover><mrow id="S3.E14.m1.1.1.3.3" xref="S3.E14.m1.1.1.3.3.cmml"><mi id="S3.E14.m1.1.1.3.3.2" xref="S3.E14.m1.1.1.3.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E14.m1.1.1.3.3.1" xref="S3.E14.m1.1.1.3.3.1.cmml">​</mo><mi id="S3.E14.m1.1.1.3.3.3" xref="S3.E14.m1.1.1.3.3.3.cmml">Q</mi></mrow></msub><mo id="S3.E14.m1.1.1.2" xref="S3.E14.m1.1.1.2.cmml">=</mo><mrow id="S3.E14.m1.1.1.1" xref="S3.E14.m1.1.1.1.cmml"><mtext id="S3.E14.m1.1.1.1.3" xref="S3.E14.m1.1.1.1.3a.cmml">Tanh</mtext><mo lspace="0em" rspace="0em" id="S3.E14.m1.1.1.1.2" xref="S3.E14.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E14.m1.1.1.1.1.1" xref="S3.E14.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E14.m1.1.1.1.1.1.2" xref="S3.E14.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E14.m1.1.1.1.1.1.1" xref="S3.E14.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E14.m1.1.1.1.1.1.1.2" xref="S3.E14.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E14.m1.1.1.1.1.1.1.2.2" xref="S3.E14.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mo id="S3.E14.m1.1.1.1.1.1.1.2.1" xref="S3.E14.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E14.m1.1.1.1.1.1.1.3" xref="S3.E14.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E14.m1.1.1.1.1.1.1.3.2" xref="S3.E14.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E14.m1.1.1.1.1.1.1.3.1" xref="S3.E14.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E14.m1.1.1.1.1.1.1.3.3" xref="S3.E14.m1.1.1.1.1.1.1.3.3.cmml">Q</mi></mrow></msub><mo stretchy="false" id="S3.E14.m1.1.1.1.1.1.3" xref="S3.E14.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E14.m1.1b"><apply id="S3.E14.m1.1.1.cmml" xref="S3.E14.m1.1.1"><eq id="S3.E14.m1.1.1.2.cmml" xref="S3.E14.m1.1.1.2"></eq><apply id="S3.E14.m1.1.1.3.cmml" xref="S3.E14.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E14.m1.1.1.3.1.cmml" xref="S3.E14.m1.1.1.3">subscript</csymbol><apply id="S3.E14.m1.1.1.3.2.cmml" xref="S3.E14.m1.1.1.3.2"><ci id="S3.E14.m1.1.1.3.2.1.cmml" xref="S3.E14.m1.1.1.3.2.1">^</ci><ci id="S3.E14.m1.1.1.3.2.2.cmml" xref="S3.E14.m1.1.1.3.2.2">𝑓</ci></apply><apply id="S3.E14.m1.1.1.3.3.cmml" xref="S3.E14.m1.1.1.3.3"><times id="S3.E14.m1.1.1.3.3.1.cmml" xref="S3.E14.m1.1.1.3.3.1"></times><ci id="S3.E14.m1.1.1.3.3.2.cmml" xref="S3.E14.m1.1.1.3.3.2">𝑉</ci><ci id="S3.E14.m1.1.1.3.3.3.cmml" xref="S3.E14.m1.1.1.3.3.3">𝑄</ci></apply></apply><apply id="S3.E14.m1.1.1.1.cmml" xref="S3.E14.m1.1.1.1"><times id="S3.E14.m1.1.1.1.2.cmml" xref="S3.E14.m1.1.1.1.2"></times><ci id="S3.E14.m1.1.1.1.3a.cmml" xref="S3.E14.m1.1.1.1.3"><mtext id="S3.E14.m1.1.1.1.3.cmml" xref="S3.E14.m1.1.1.1.3">Tanh</mtext></ci><apply id="S3.E14.m1.1.1.1.1.1.1.cmml" xref="S3.E14.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E14.m1.1.1.1.1.1.1.1.cmml" xref="S3.E14.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E14.m1.1.1.1.1.1.1.2.cmml" xref="S3.E14.m1.1.1.1.1.1.1.2"><ci id="S3.E14.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E14.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E14.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E14.m1.1.1.1.1.1.1.2.2">𝑓</ci></apply><apply id="S3.E14.m1.1.1.1.1.1.1.3.cmml" xref="S3.E14.m1.1.1.1.1.1.1.3"><times id="S3.E14.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E14.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E14.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E14.m1.1.1.1.1.1.1.3.2">𝑉</ci><ci id="S3.E14.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E14.m1.1.1.1.1.1.1.3.3">𝑄</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E14.m1.1c">\hat{f}_{VQ}=\text{Tanh}(\hat{f}_{VQ})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Classifier Module</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We integrate a classification head to predict the answer, consisting of the following components: (1) a FFN layer with hidden and output dimensions of 768 and 1536, (2) a normalization layer, (3) the GELU activation function, and (4) an additional FFN layer that maps the input dimension of 1536 to the number of classes (353 candidate answers).</p>
<table id="S3.E15" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E15.m1.1" class="ltx_Math" alttext="l=\text{FFN}(\hat{f}_{VQ})" display="block"><semantics id="S3.E15.m1.1a"><mrow id="S3.E15.m1.1.1" xref="S3.E15.m1.1.1.cmml"><mi id="S3.E15.m1.1.1.3" xref="S3.E15.m1.1.1.3.cmml">l</mi><mo id="S3.E15.m1.1.1.2" xref="S3.E15.m1.1.1.2.cmml">=</mo><mrow id="S3.E15.m1.1.1.1" xref="S3.E15.m1.1.1.1.cmml"><mtext id="S3.E15.m1.1.1.1.3" xref="S3.E15.m1.1.1.1.3a.cmml">FFN</mtext><mo lspace="0em" rspace="0em" id="S3.E15.m1.1.1.1.2" xref="S3.E15.m1.1.1.1.2.cmml">​</mo><mrow id="S3.E15.m1.1.1.1.1.1" xref="S3.E15.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E15.m1.1.1.1.1.1.2" xref="S3.E15.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E15.m1.1.1.1.1.1.1" xref="S3.E15.m1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E15.m1.1.1.1.1.1.1.2" xref="S3.E15.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E15.m1.1.1.1.1.1.1.2.2" xref="S3.E15.m1.1.1.1.1.1.1.2.2.cmml">f</mi><mo id="S3.E15.m1.1.1.1.1.1.1.2.1" xref="S3.E15.m1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mrow id="S3.E15.m1.1.1.1.1.1.1.3" xref="S3.E15.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E15.m1.1.1.1.1.1.1.3.2" xref="S3.E15.m1.1.1.1.1.1.1.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S3.E15.m1.1.1.1.1.1.1.3.1" xref="S3.E15.m1.1.1.1.1.1.1.3.1.cmml">​</mo><mi id="S3.E15.m1.1.1.1.1.1.1.3.3" xref="S3.E15.m1.1.1.1.1.1.1.3.3.cmml">Q</mi></mrow></msub><mo stretchy="false" id="S3.E15.m1.1.1.1.1.1.3" xref="S3.E15.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E15.m1.1b"><apply id="S3.E15.m1.1.1.cmml" xref="S3.E15.m1.1.1"><eq id="S3.E15.m1.1.1.2.cmml" xref="S3.E15.m1.1.1.2"></eq><ci id="S3.E15.m1.1.1.3.cmml" xref="S3.E15.m1.1.1.3">𝑙</ci><apply id="S3.E15.m1.1.1.1.cmml" xref="S3.E15.m1.1.1.1"><times id="S3.E15.m1.1.1.1.2.cmml" xref="S3.E15.m1.1.1.1.2"></times><ci id="S3.E15.m1.1.1.1.3a.cmml" xref="S3.E15.m1.1.1.1.3"><mtext id="S3.E15.m1.1.1.1.3.cmml" xref="S3.E15.m1.1.1.1.3">FFN</mtext></ci><apply id="S3.E15.m1.1.1.1.1.1.1.cmml" xref="S3.E15.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E15.m1.1.1.1.1.1.1.1.cmml" xref="S3.E15.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E15.m1.1.1.1.1.1.1.2.cmml" xref="S3.E15.m1.1.1.1.1.1.1.2"><ci id="S3.E15.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E15.m1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E15.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E15.m1.1.1.1.1.1.1.2.2">𝑓</ci></apply><apply id="S3.E15.m1.1.1.1.1.1.1.3.cmml" xref="S3.E15.m1.1.1.1.1.1.1.3"><times id="S3.E15.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E15.m1.1.1.1.1.1.1.3.1"></times><ci id="S3.E15.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E15.m1.1.1.1.1.1.1.3.2">𝑉</ci><ci id="S3.E15.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E15.m1.1.1.1.1.1.1.3.3">𝑄</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E15.m1.1c">l=\text{FFN}(\hat{f}_{VQ})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(15)</span></td>
</tr></tbody>
</table>
<table id="S3.E16" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E16.m1.1" class="ltx_Math" alttext="l=\text{Norm}(l)" display="block"><semantics id="S3.E16.m1.1a"><mrow id="S3.E16.m1.1.2" xref="S3.E16.m1.1.2.cmml"><mi id="S3.E16.m1.1.2.2" xref="S3.E16.m1.1.2.2.cmml">l</mi><mo id="S3.E16.m1.1.2.1" xref="S3.E16.m1.1.2.1.cmml">=</mo><mrow id="S3.E16.m1.1.2.3" xref="S3.E16.m1.1.2.3.cmml"><mtext id="S3.E16.m1.1.2.3.2" xref="S3.E16.m1.1.2.3.2a.cmml">Norm</mtext><mo lspace="0em" rspace="0em" id="S3.E16.m1.1.2.3.1" xref="S3.E16.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E16.m1.1.2.3.3.2" xref="S3.E16.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E16.m1.1.2.3.3.2.1" xref="S3.E16.m1.1.2.3.cmml">(</mo><mi id="S3.E16.m1.1.1" xref="S3.E16.m1.1.1.cmml">l</mi><mo stretchy="false" id="S3.E16.m1.1.2.3.3.2.2" xref="S3.E16.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E16.m1.1b"><apply id="S3.E16.m1.1.2.cmml" xref="S3.E16.m1.1.2"><eq id="S3.E16.m1.1.2.1.cmml" xref="S3.E16.m1.1.2.1"></eq><ci id="S3.E16.m1.1.2.2.cmml" xref="S3.E16.m1.1.2.2">𝑙</ci><apply id="S3.E16.m1.1.2.3.cmml" xref="S3.E16.m1.1.2.3"><times id="S3.E16.m1.1.2.3.1.cmml" xref="S3.E16.m1.1.2.3.1"></times><ci id="S3.E16.m1.1.2.3.2a.cmml" xref="S3.E16.m1.1.2.3.2"><mtext id="S3.E16.m1.1.2.3.2.cmml" xref="S3.E16.m1.1.2.3.2">Norm</mtext></ci><ci id="S3.E16.m1.1.1.cmml" xref="S3.E16.m1.1.1">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E16.m1.1c">l=\text{Norm}(l)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(16)</span></td>
</tr></tbody>
</table>
<table id="S3.E17" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E17.m1.1" class="ltx_Math" alttext="l=\text{GELU}(l)" display="block"><semantics id="S3.E17.m1.1a"><mrow id="S3.E17.m1.1.2" xref="S3.E17.m1.1.2.cmml"><mi id="S3.E17.m1.1.2.2" xref="S3.E17.m1.1.2.2.cmml">l</mi><mo id="S3.E17.m1.1.2.1" xref="S3.E17.m1.1.2.1.cmml">=</mo><mrow id="S3.E17.m1.1.2.3" xref="S3.E17.m1.1.2.3.cmml"><mtext id="S3.E17.m1.1.2.3.2" xref="S3.E17.m1.1.2.3.2a.cmml">GELU</mtext><mo lspace="0em" rspace="0em" id="S3.E17.m1.1.2.3.1" xref="S3.E17.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E17.m1.1.2.3.3.2" xref="S3.E17.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E17.m1.1.2.3.3.2.1" xref="S3.E17.m1.1.2.3.cmml">(</mo><mi id="S3.E17.m1.1.1" xref="S3.E17.m1.1.1.cmml">l</mi><mo stretchy="false" id="S3.E17.m1.1.2.3.3.2.2" xref="S3.E17.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E17.m1.1b"><apply id="S3.E17.m1.1.2.cmml" xref="S3.E17.m1.1.2"><eq id="S3.E17.m1.1.2.1.cmml" xref="S3.E17.m1.1.2.1"></eq><ci id="S3.E17.m1.1.2.2.cmml" xref="S3.E17.m1.1.2.2">𝑙</ci><apply id="S3.E17.m1.1.2.3.cmml" xref="S3.E17.m1.1.2.3"><times id="S3.E17.m1.1.2.3.1.cmml" xref="S3.E17.m1.1.2.3.1"></times><ci id="S3.E17.m1.1.2.3.2a.cmml" xref="S3.E17.m1.1.2.3.2"><mtext id="S3.E17.m1.1.2.3.2.cmml" xref="S3.E17.m1.1.2.3.2">GELU</mtext></ci><ci id="S3.E17.m1.1.1.cmml" xref="S3.E17.m1.1.1">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E17.m1.1c">l=\text{GELU}(l)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(17)</span></td>
</tr></tbody>
</table>
<table id="S3.E18" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E18.m1.1" class="ltx_Math" alttext="l=\text{FFN}(l)" display="block"><semantics id="S3.E18.m1.1a"><mrow id="S3.E18.m1.1.2" xref="S3.E18.m1.1.2.cmml"><mi id="S3.E18.m1.1.2.2" xref="S3.E18.m1.1.2.2.cmml">l</mi><mo id="S3.E18.m1.1.2.1" xref="S3.E18.m1.1.2.1.cmml">=</mo><mrow id="S3.E18.m1.1.2.3" xref="S3.E18.m1.1.2.3.cmml"><mtext id="S3.E18.m1.1.2.3.2" xref="S3.E18.m1.1.2.3.2a.cmml">FFN</mtext><mo lspace="0em" rspace="0em" id="S3.E18.m1.1.2.3.1" xref="S3.E18.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E18.m1.1.2.3.3.2" xref="S3.E18.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E18.m1.1.2.3.3.2.1" xref="S3.E18.m1.1.2.3.cmml">(</mo><mi id="S3.E18.m1.1.1" xref="S3.E18.m1.1.1.cmml">l</mi><mo stretchy="false" id="S3.E18.m1.1.2.3.3.2.2" xref="S3.E18.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E18.m1.1b"><apply id="S3.E18.m1.1.2.cmml" xref="S3.E18.m1.1.2"><eq id="S3.E18.m1.1.2.1.cmml" xref="S3.E18.m1.1.2.1"></eq><ci id="S3.E18.m1.1.2.2.cmml" xref="S3.E18.m1.1.2.2">𝑙</ci><apply id="S3.E18.m1.1.2.3.cmml" xref="S3.E18.m1.1.2.3"><times id="S3.E18.m1.1.2.3.1.cmml" xref="S3.E18.m1.1.2.3.1"></times><ci id="S3.E18.m1.1.2.3.2a.cmml" xref="S3.E18.m1.1.2.3.2"><mtext id="S3.E18.m1.1.2.3.2.cmml" xref="S3.E18.m1.1.2.3.2">FFN</mtext></ci><ci id="S3.E18.m1.1.1.cmml" xref="S3.E18.m1.1.1">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E18.m1.1c">l=\text{FFN}(l)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(18)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">Following this, a softmax function is applied to convert the output into a distribution within the range of 0 to 1:</p>
<table id="S3.E19" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S3.E19.m1.1" class="ltx_Math" alttext="\hat{p}=\text{softmax}(l)" display="block"><semantics id="S3.E19.m1.1a"><mrow id="S3.E19.m1.1.2" xref="S3.E19.m1.1.2.cmml"><mover accent="true" id="S3.E19.m1.1.2.2" xref="S3.E19.m1.1.2.2.cmml"><mi id="S3.E19.m1.1.2.2.2" xref="S3.E19.m1.1.2.2.2.cmml">p</mi><mo id="S3.E19.m1.1.2.2.1" xref="S3.E19.m1.1.2.2.1.cmml">^</mo></mover><mo id="S3.E19.m1.1.2.1" xref="S3.E19.m1.1.2.1.cmml">=</mo><mrow id="S3.E19.m1.1.2.3" xref="S3.E19.m1.1.2.3.cmml"><mtext id="S3.E19.m1.1.2.3.2" xref="S3.E19.m1.1.2.3.2a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S3.E19.m1.1.2.3.1" xref="S3.E19.m1.1.2.3.1.cmml">​</mo><mrow id="S3.E19.m1.1.2.3.3.2" xref="S3.E19.m1.1.2.3.cmml"><mo stretchy="false" id="S3.E19.m1.1.2.3.3.2.1" xref="S3.E19.m1.1.2.3.cmml">(</mo><mi id="S3.E19.m1.1.1" xref="S3.E19.m1.1.1.cmml">l</mi><mo stretchy="false" id="S3.E19.m1.1.2.3.3.2.2" xref="S3.E19.m1.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E19.m1.1b"><apply id="S3.E19.m1.1.2.cmml" xref="S3.E19.m1.1.2"><eq id="S3.E19.m1.1.2.1.cmml" xref="S3.E19.m1.1.2.1"></eq><apply id="S3.E19.m1.1.2.2.cmml" xref="S3.E19.m1.1.2.2"><ci id="S3.E19.m1.1.2.2.1.cmml" xref="S3.E19.m1.1.2.2.1">^</ci><ci id="S3.E19.m1.1.2.2.2.cmml" xref="S3.E19.m1.1.2.2.2">𝑝</ci></apply><apply id="S3.E19.m1.1.2.3.cmml" xref="S3.E19.m1.1.2.3"><times id="S3.E19.m1.1.2.3.1.cmml" xref="S3.E19.m1.1.2.3.1"></times><ci id="S3.E19.m1.1.2.3.2a.cmml" xref="S3.E19.m1.1.2.3.2"><mtext id="S3.E19.m1.1.2.3.2.cmml" xref="S3.E19.m1.1.2.3.2">softmax</mtext></ci><ci id="S3.E19.m1.1.1.cmml" xref="S3.E19.m1.1.1">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E19.m1.1c">\hat{p}=\text{softmax}(l)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(19)</span></td>
</tr></tbody>
</table>
<p id="S3.SS4.p2.2" class="ltx_p">The candidate with the highest probability is then chosen as the proper answer.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we conduct evaluations of our model using the ViVQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Additionally, we compare our model’s performance with that of previous ViVQA models that have achieved state-of-the-art results on the same dataset.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">By formulating the VQA task as classification, we employ Cross-Entropy Loss as the loss function and assess our model’s performance on the ViVQA dataset, utilizing accuracy as the main metric.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dataset</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As we mentioned in section <a href="#S2.SS3.SSS2" title="2.3.2 ViVQA Dataset ‣ 2.3 Dataset for ViVQA ‣ 2 Related Works ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3.2</span></a>, the ViVQA dataset is the subject of our investigations. A training set of 11,999 Q-A pairs and a test set of 3,001 Q-A pairs comprise the dataset. The detail of the dataset is shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1 Dataset ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<td id="S4.T1.2.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S4.T1.2.1.1.2" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.2.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></td>
<td id="S4.T1.2.1.1.3" class="ltx_td ltx_align_left ltx_border_tt"><span id="S4.T1.2.1.1.3.1" class="ltx_text ltx_font_bold">Test</span></td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_align_left ltx_border_t">No. Samples</td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_left ltx_border_t">11999</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_left ltx_border_t">3001</td>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<td id="S4.T1.2.3.3.1" class="ltx_td ltx_align_left">Longest Question Length</td>
<td id="S4.T1.2.3.3.2" class="ltx_td ltx_align_left">26</td>
<td id="S4.T1.2.3.3.3" class="ltx_td ltx_align_left">24</td>
</tr>
<tr id="S4.T1.2.4.4" class="ltx_tr">
<td id="S4.T1.2.4.4.1" class="ltx_td ltx_align_left">Longest Answer Length</td>
<td id="S4.T1.2.4.4.2" class="ltx_td ltx_align_left">4</td>
<td id="S4.T1.2.4.4.3" class="ltx_td ltx_align_left">4</td>
</tr>
<tr id="S4.T1.2.5.5" class="ltx_tr">
<td id="S4.T1.2.5.5.1" class="ltx_td ltx_align_left">Average Question Length</td>
<td id="S4.T1.2.5.5.2" class="ltx_td ltx_align_left">9.50</td>
<td id="S4.T1.2.5.5.3" class="ltx_td ltx_align_left">9.58</td>
</tr>
<tr id="S4.T1.2.6.6" class="ltx_tr">
<td id="S4.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_border_bb">Average Answer Length</td>
<td id="S4.T1.2.6.6.2" class="ltx_td ltx_align_left ltx_border_bb">1.78</td>
<td id="S4.T1.2.6.6.3" class="ltx_td ltx_align_left ltx_border_bb">1.78</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">The detail of ViVQA dataset.</span></figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation Metrics</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In formulating the ViVQA as a classification problem, we assess the model’s effectiveness using key metrics: F1 score, Precision, Recall, and Accuracy. In alignment with our objective to compare against the baseline in our experiment, we prioritize accuracy over other metrics. This choice stems from the fact that all baseline models are evaluated based on accuracy, with some not utilizing additional metrics. Thus, in our experimental endeavor to evaluate the influencing factors of the model, accuracy will serve as the primary metric for a more straightforward analysis.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Accuracy</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">Accuracy or exact match is defined as the ratio of the number of correct predictions to the total number of ground truth answers. Let <math id="S4.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.SSS1.p1.1.m1.1a"><mi id="S4.SS2.SSS1.p1.1.m1.1.1" xref="S4.SS2.SSS1.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.1.m1.1b"><ci id="S4.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.1.m1.1c">N</annotation></semantics></math> represent the total number of ground truth answers. The accuracy is calculated as follows:</p>
<table id="S4.E20" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E20.m1.1" class="ltx_Math" alttext="Acuracy=\frac{1}{N}\sum_{i=1}^{N}a_{i}" display="block"><semantics id="S4.E20.m1.1a"><mrow id="S4.E20.m1.1.1" xref="S4.E20.m1.1.1.cmml"><mrow id="S4.E20.m1.1.1.2" xref="S4.E20.m1.1.1.2.cmml"><mi id="S4.E20.m1.1.1.2.2" xref="S4.E20.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.3" xref="S4.E20.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1a" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.4" xref="S4.E20.m1.1.1.2.4.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1b" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.5" xref="S4.E20.m1.1.1.2.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1c" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.6" xref="S4.E20.m1.1.1.2.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1d" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.7" xref="S4.E20.m1.1.1.2.7.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.2.1e" xref="S4.E20.m1.1.1.2.1.cmml">​</mo><mi id="S4.E20.m1.1.1.2.8" xref="S4.E20.m1.1.1.2.8.cmml">y</mi></mrow><mo id="S4.E20.m1.1.1.1" xref="S4.E20.m1.1.1.1.cmml">=</mo><mrow id="S4.E20.m1.1.1.3" xref="S4.E20.m1.1.1.3.cmml"><mfrac id="S4.E20.m1.1.1.3.2" xref="S4.E20.m1.1.1.3.2.cmml"><mn id="S4.E20.m1.1.1.3.2.2" xref="S4.E20.m1.1.1.3.2.2.cmml">1</mn><mi id="S4.E20.m1.1.1.3.2.3" xref="S4.E20.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E20.m1.1.1.3.1" xref="S4.E20.m1.1.1.3.1.cmml">​</mo><mrow id="S4.E20.m1.1.1.3.3" xref="S4.E20.m1.1.1.3.3.cmml"><munderover id="S4.E20.m1.1.1.3.3.1" xref="S4.E20.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E20.m1.1.1.3.3.1.2.2" xref="S4.E20.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E20.m1.1.1.3.3.1.2.3" xref="S4.E20.m1.1.1.3.3.1.2.3.cmml"><mi id="S4.E20.m1.1.1.3.3.1.2.3.2" xref="S4.E20.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E20.m1.1.1.3.3.1.2.3.1" xref="S4.E20.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E20.m1.1.1.3.3.1.2.3.3" xref="S4.E20.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E20.m1.1.1.3.3.1.3" xref="S4.E20.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S4.E20.m1.1.1.3.3.2" xref="S4.E20.m1.1.1.3.3.2.cmml"><mi id="S4.E20.m1.1.1.3.3.2.2" xref="S4.E20.m1.1.1.3.3.2.2.cmml">a</mi><mi id="S4.E20.m1.1.1.3.3.2.3" xref="S4.E20.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E20.m1.1b"><apply id="S4.E20.m1.1.1.cmml" xref="S4.E20.m1.1.1"><eq id="S4.E20.m1.1.1.1.cmml" xref="S4.E20.m1.1.1.1"></eq><apply id="S4.E20.m1.1.1.2.cmml" xref="S4.E20.m1.1.1.2"><times id="S4.E20.m1.1.1.2.1.cmml" xref="S4.E20.m1.1.1.2.1"></times><ci id="S4.E20.m1.1.1.2.2.cmml" xref="S4.E20.m1.1.1.2.2">𝐴</ci><ci id="S4.E20.m1.1.1.2.3.cmml" xref="S4.E20.m1.1.1.2.3">𝑐</ci><ci id="S4.E20.m1.1.1.2.4.cmml" xref="S4.E20.m1.1.1.2.4">𝑢</ci><ci id="S4.E20.m1.1.1.2.5.cmml" xref="S4.E20.m1.1.1.2.5">𝑟</ci><ci id="S4.E20.m1.1.1.2.6.cmml" xref="S4.E20.m1.1.1.2.6">𝑎</ci><ci id="S4.E20.m1.1.1.2.7.cmml" xref="S4.E20.m1.1.1.2.7">𝑐</ci><ci id="S4.E20.m1.1.1.2.8.cmml" xref="S4.E20.m1.1.1.2.8">𝑦</ci></apply><apply id="S4.E20.m1.1.1.3.cmml" xref="S4.E20.m1.1.1.3"><times id="S4.E20.m1.1.1.3.1.cmml" xref="S4.E20.m1.1.1.3.1"></times><apply id="S4.E20.m1.1.1.3.2.cmml" xref="S4.E20.m1.1.1.3.2"><divide id="S4.E20.m1.1.1.3.2.1.cmml" xref="S4.E20.m1.1.1.3.2"></divide><cn type="integer" id="S4.E20.m1.1.1.3.2.2.cmml" xref="S4.E20.m1.1.1.3.2.2">1</cn><ci id="S4.E20.m1.1.1.3.2.3.cmml" xref="S4.E20.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S4.E20.m1.1.1.3.3.cmml" xref="S4.E20.m1.1.1.3.3"><apply id="S4.E20.m1.1.1.3.3.1.cmml" xref="S4.E20.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E20.m1.1.1.3.3.1.1.cmml" xref="S4.E20.m1.1.1.3.3.1">superscript</csymbol><apply id="S4.E20.m1.1.1.3.3.1.2.cmml" xref="S4.E20.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E20.m1.1.1.3.3.1.2.1.cmml" xref="S4.E20.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E20.m1.1.1.3.3.1.2.2.cmml" xref="S4.E20.m1.1.1.3.3.1.2.2"></sum><apply id="S4.E20.m1.1.1.3.3.1.2.3.cmml" xref="S4.E20.m1.1.1.3.3.1.2.3"><eq id="S4.E20.m1.1.1.3.3.1.2.3.1.cmml" xref="S4.E20.m1.1.1.3.3.1.2.3.1"></eq><ci id="S4.E20.m1.1.1.3.3.1.2.3.2.cmml" xref="S4.E20.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E20.m1.1.1.3.3.1.2.3.3.cmml" xref="S4.E20.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E20.m1.1.1.3.3.1.3.cmml" xref="S4.E20.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S4.E20.m1.1.1.3.3.2.cmml" xref="S4.E20.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E20.m1.1.1.3.3.2.1.cmml" xref="S4.E20.m1.1.1.3.3.2">subscript</csymbol><ci id="S4.E20.m1.1.1.3.3.2.2.cmml" xref="S4.E20.m1.1.1.3.3.2.2">𝑎</ci><ci id="S4.E20.m1.1.1.3.3.2.3.cmml" xref="S4.E20.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E20.m1.1c">Acuracy=\frac{1}{N}\sum_{i=1}^{N}a_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(20)</span></td>
</tr></tbody>
</table>
<table id="S4.E21" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E21.m1.4" class="ltx_Math" alttext="a_{i}=\begin{cases}1,&amp;\text{if}\ \hat{y_{i}}=y_{i}\\
0,&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S4.E21.m1.4a"><mrow id="S4.E21.m1.4.5" xref="S4.E21.m1.4.5.cmml"><msub id="S4.E21.m1.4.5.2" xref="S4.E21.m1.4.5.2.cmml"><mi id="S4.E21.m1.4.5.2.2" xref="S4.E21.m1.4.5.2.2.cmml">a</mi><mi id="S4.E21.m1.4.5.2.3" xref="S4.E21.m1.4.5.2.3.cmml">i</mi></msub><mo id="S4.E21.m1.4.5.1" xref="S4.E21.m1.4.5.1.cmml">=</mo><mrow id="S4.E21.m1.4.4" xref="S4.E21.m1.4.5.3.1.cmml"><mo id="S4.E21.m1.4.4.5" xref="S4.E21.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E21.m1.4.4.4" xref="S4.E21.m1.4.5.3.1.cmml"><mtr id="S4.E21.m1.4.4.4a" xref="S4.E21.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E21.m1.4.4.4b" xref="S4.E21.m1.4.5.3.1.cmml"><mrow id="S4.E21.m1.1.1.1.1.1.1.3" xref="S4.E21.m1.4.5.3.1.cmml"><mn id="S4.E21.m1.1.1.1.1.1.1.1" xref="S4.E21.m1.1.1.1.1.1.1.1.cmml">1</mn><mo id="S4.E21.m1.1.1.1.1.1.1.3.1" xref="S4.E21.m1.4.5.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E21.m1.4.4.4c" xref="S4.E21.m1.4.5.3.1.cmml"><mrow id="S4.E21.m1.2.2.2.2.2.1" xref="S4.E21.m1.2.2.2.2.2.1.cmml"><mrow id="S4.E21.m1.2.2.2.2.2.1.2" xref="S4.E21.m1.2.2.2.2.2.1.2.cmml"><mtext id="S4.E21.m1.2.2.2.2.2.1.2.2" xref="S4.E21.m1.2.2.2.2.2.1.2.2a.cmml">if</mtext><mo lspace="0.500em" rspace="0em" id="S4.E21.m1.2.2.2.2.2.1.2.1" xref="S4.E21.m1.2.2.2.2.2.1.2.1.cmml">​</mo><mover accent="true" id="S4.E21.m1.2.2.2.2.2.1.2.3" xref="S4.E21.m1.2.2.2.2.2.1.2.3.cmml"><msub id="S4.E21.m1.2.2.2.2.2.1.2.3.2" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2.cmml"><mi id="S4.E21.m1.2.2.2.2.2.1.2.3.2.2" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2.2.cmml">y</mi><mi id="S4.E21.m1.2.2.2.2.2.1.2.3.2.3" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2.3.cmml">i</mi></msub><mo id="S4.E21.m1.2.2.2.2.2.1.2.3.1" xref="S4.E21.m1.2.2.2.2.2.1.2.3.1.cmml">^</mo></mover></mrow><mo id="S4.E21.m1.2.2.2.2.2.1.1" xref="S4.E21.m1.2.2.2.2.2.1.1.cmml">=</mo><msub id="S4.E21.m1.2.2.2.2.2.1.3" xref="S4.E21.m1.2.2.2.2.2.1.3.cmml"><mi id="S4.E21.m1.2.2.2.2.2.1.3.2" xref="S4.E21.m1.2.2.2.2.2.1.3.2.cmml">y</mi><mi id="S4.E21.m1.2.2.2.2.2.1.3.3" xref="S4.E21.m1.2.2.2.2.2.1.3.3.cmml">i</mi></msub></mrow></mtd></mtr><mtr id="S4.E21.m1.4.4.4d" xref="S4.E21.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E21.m1.4.4.4e" xref="S4.E21.m1.4.5.3.1.cmml"><mrow id="S4.E21.m1.3.3.3.3.1.1.3" xref="S4.E21.m1.4.5.3.1.cmml"><mn id="S4.E21.m1.3.3.3.3.1.1.1" xref="S4.E21.m1.3.3.3.3.1.1.1.cmml">0</mn><mo id="S4.E21.m1.3.3.3.3.1.1.3.1" xref="S4.E21.m1.4.5.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E21.m1.4.4.4f" xref="S4.E21.m1.4.5.3.1.cmml"><mtext id="S4.E21.m1.4.4.4.4.2.1" xref="S4.E21.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E21.m1.4b"><apply id="S4.E21.m1.4.5.cmml" xref="S4.E21.m1.4.5"><eq id="S4.E21.m1.4.5.1.cmml" xref="S4.E21.m1.4.5.1"></eq><apply id="S4.E21.m1.4.5.2.cmml" xref="S4.E21.m1.4.5.2"><csymbol cd="ambiguous" id="S4.E21.m1.4.5.2.1.cmml" xref="S4.E21.m1.4.5.2">subscript</csymbol><ci id="S4.E21.m1.4.5.2.2.cmml" xref="S4.E21.m1.4.5.2.2">𝑎</ci><ci id="S4.E21.m1.4.5.2.3.cmml" xref="S4.E21.m1.4.5.2.3">𝑖</ci></apply><apply id="S4.E21.m1.4.5.3.1.cmml" xref="S4.E21.m1.4.4"><csymbol cd="latexml" id="S4.E21.m1.4.5.3.1.1.cmml" xref="S4.E21.m1.4.4.5">cases</csymbol><cn type="integer" id="S4.E21.m1.1.1.1.1.1.1.1.cmml" xref="S4.E21.m1.1.1.1.1.1.1.1">1</cn><apply id="S4.E21.m1.2.2.2.2.2.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1"><eq id="S4.E21.m1.2.2.2.2.2.1.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1.1"></eq><apply id="S4.E21.m1.2.2.2.2.2.1.2.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2"><times id="S4.E21.m1.2.2.2.2.2.1.2.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.1"></times><ci id="S4.E21.m1.2.2.2.2.2.1.2.2a.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.2"><mtext id="S4.E21.m1.2.2.2.2.2.1.2.2.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.2">if</mtext></ci><apply id="S4.E21.m1.2.2.2.2.2.1.2.3.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3"><ci id="S4.E21.m1.2.2.2.2.2.1.2.3.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3.1">^</ci><apply id="S4.E21.m1.2.2.2.2.2.1.2.3.2.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2"><csymbol cd="ambiguous" id="S4.E21.m1.2.2.2.2.2.1.2.3.2.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2">subscript</csymbol><ci id="S4.E21.m1.2.2.2.2.2.1.2.3.2.2.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2.2">𝑦</ci><ci id="S4.E21.m1.2.2.2.2.2.1.2.3.2.3.cmml" xref="S4.E21.m1.2.2.2.2.2.1.2.3.2.3">𝑖</ci></apply></apply></apply><apply id="S4.E21.m1.2.2.2.2.2.1.3.cmml" xref="S4.E21.m1.2.2.2.2.2.1.3"><csymbol cd="ambiguous" id="S4.E21.m1.2.2.2.2.2.1.3.1.cmml" xref="S4.E21.m1.2.2.2.2.2.1.3">subscript</csymbol><ci id="S4.E21.m1.2.2.2.2.2.1.3.2.cmml" xref="S4.E21.m1.2.2.2.2.2.1.3.2">𝑦</ci><ci id="S4.E21.m1.2.2.2.2.2.1.3.3.cmml" xref="S4.E21.m1.2.2.2.2.2.1.3.3">𝑖</ci></apply></apply><cn type="integer" id="S4.E21.m1.3.3.3.3.1.1.1.cmml" xref="S4.E21.m1.3.3.3.3.1.1.1">0</cn><ci id="S4.E21.m1.4.4.4.4.2.1a.cmml" xref="S4.E21.m1.4.4.4.4.2.1"><mtext id="S4.E21.m1.4.4.4.4.2.1.cmml" xref="S4.E21.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E21.m1.4c">a_{i}=\begin{cases}1,&amp;\text{if}\ \hat{y_{i}}=y_{i}\\
0,&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(21)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS1.p1.4" class="ltx_p">where <math id="S4.SS2.SSS1.p1.2.m1.1" class="ltx_Math" alttext="\hat{y_{i}}" display="inline"><semantics id="S4.SS2.SSS1.p1.2.m1.1a"><mover accent="true" id="S4.SS2.SSS1.p1.2.m1.1.1" xref="S4.SS2.SSS1.p1.2.m1.1.1.cmml"><msub id="S4.SS2.SSS1.p1.2.m1.1.1.2" xref="S4.SS2.SSS1.p1.2.m1.1.1.2.cmml"><mi id="S4.SS2.SSS1.p1.2.m1.1.1.2.2" xref="S4.SS2.SSS1.p1.2.m1.1.1.2.2.cmml">y</mi><mi id="S4.SS2.SSS1.p1.2.m1.1.1.2.3" xref="S4.SS2.SSS1.p1.2.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.SS2.SSS1.p1.2.m1.1.1.1" xref="S4.SS2.SSS1.p1.2.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.2.m1.1b"><apply id="S4.SS2.SSS1.p1.2.m1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1"><ci id="S4.SS2.SSS1.p1.2.m1.1.1.1.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1.1">^</ci><apply id="S4.SS2.SSS1.p1.2.m1.1.1.2.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.2.m1.1.1.2.1.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1.2">subscript</csymbol><ci id="S4.SS2.SSS1.p1.2.m1.1.1.2.2.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1.2.2">𝑦</ci><ci id="S4.SS2.SSS1.p1.2.m1.1.1.2.3.cmml" xref="S4.SS2.SSS1.p1.2.m1.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.2.m1.1c">\hat{y_{i}}</annotation></semantics></math> is the predicted answer, and <math id="S4.SS2.SSS1.p1.3.m2.1" class="ltx_Math" alttext="y_{i}" display="inline"><semantics id="S4.SS2.SSS1.p1.3.m2.1a"><msub id="S4.SS2.SSS1.p1.3.m2.1.1" xref="S4.SS2.SSS1.p1.3.m2.1.1.cmml"><mi id="S4.SS2.SSS1.p1.3.m2.1.1.2" xref="S4.SS2.SSS1.p1.3.m2.1.1.2.cmml">y</mi><mi id="S4.SS2.SSS1.p1.3.m2.1.1.3" xref="S4.SS2.SSS1.p1.3.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.3.m2.1b"><apply id="S4.SS2.SSS1.p1.3.m2.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS1.p1.3.m2.1.1.1.cmml" xref="S4.SS2.SSS1.p1.3.m2.1.1">subscript</csymbol><ci id="S4.SS2.SSS1.p1.3.m2.1.1.2.cmml" xref="S4.SS2.SSS1.p1.3.m2.1.1.2">𝑦</ci><ci id="S4.SS2.SSS1.p1.3.m2.1.1.3.cmml" xref="S4.SS2.SSS1.p1.3.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.3.m2.1c">y_{i}</annotation></semantics></math> is the ground truth answer for question <math id="S4.SS2.SSS1.p1.4.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS1.p1.4.m3.1a"><mi id="S4.SS2.SSS1.p1.4.m3.1.1" xref="S4.SS2.SSS1.p1.4.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p1.4.m3.1b"><ci id="S4.SS2.SSS1.p1.4.m3.1.1.cmml" xref="S4.SS2.SSS1.p1.4.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p1.4.m3.1c">i</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Recall</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.4" class="ltx_p">This metric, relying on tokenized versions, measures the proportion of correctly identified tokens in the ground truth answer that are also present in the predicted answer. Let <math id="S4.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S4.SS2.SSS2.p1.1.m1.1a"><msub id="S4.SS2.SSS2.p1.1.m1.1.1" xref="S4.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS2.p1.1.m1.1.1.2" xref="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml">P</mi><mi id="S4.SS2.SSS2.p1.1.m1.1.1.3" xref="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.1.m1.1b"><apply id="S4.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.1.m1.1c">P_{i}</annotation></semantics></math> represent the tokens in the predicted answer and <math id="S4.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="GT_{i}" display="inline"><semantics id="S4.SS2.SSS2.p1.2.m2.1a"><mrow id="S4.SS2.SSS2.p1.2.m2.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.1.1.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS2.p1.2.m2.1.1.1" xref="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS2.SSS2.p1.2.m2.1.1.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml"><mi id="S4.SS2.SSS2.p1.2.m2.1.1.3.2" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.2.cmml">T</mi><mi id="S4.SS2.SSS2.p1.2.m2.1.1.3.3" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.2.m2.1b"><apply id="S4.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1"><times id="S4.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.1"></times><ci id="S4.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.2">𝐺</ci><apply id="S4.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.2">𝑇</ci><ci id="S4.SS2.SSS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS2.p1.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.2.m2.1c">GT_{i}</annotation></semantics></math> denote the tokens in the ground truth answer for the question <math id="S4.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS2.p1.3.m3.1a"><mi id="S4.SS2.SSS2.p1.3.m3.1.1" xref="S4.SS2.SSS2.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.3.m3.1b"><ci id="S4.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS2.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.3.m3.1c">i</annotation></semantics></math>-th. The recall for the question <math id="S4.SS2.SSS2.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS2.p1.4.m4.1a"><mi id="S4.SS2.SSS2.p1.4.m4.1.1" xref="S4.SS2.SSS2.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.4.m4.1b"><ci id="S4.SS2.SSS2.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS2.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.4.m4.1c">i</annotation></semantics></math>-th is determined as follows:</p>
<table id="S4.E22" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E22.m1.1" class="ltx_Math" alttext="r_{i}=\frac{P_{i}\cap GT_{i}}{GT_{i}}" display="block"><semantics id="S4.E22.m1.1a"><mrow id="S4.E22.m1.1.1" xref="S4.E22.m1.1.1.cmml"><msub id="S4.E22.m1.1.1.2" xref="S4.E22.m1.1.1.2.cmml"><mi id="S4.E22.m1.1.1.2.2" xref="S4.E22.m1.1.1.2.2.cmml">r</mi><mi id="S4.E22.m1.1.1.2.3" xref="S4.E22.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E22.m1.1.1.1" xref="S4.E22.m1.1.1.1.cmml">=</mo><mfrac id="S4.E22.m1.1.1.3" xref="S4.E22.m1.1.1.3.cmml"><mrow id="S4.E22.m1.1.1.3.2" xref="S4.E22.m1.1.1.3.2.cmml"><msub id="S4.E22.m1.1.1.3.2.2" xref="S4.E22.m1.1.1.3.2.2.cmml"><mi id="S4.E22.m1.1.1.3.2.2.2" xref="S4.E22.m1.1.1.3.2.2.2.cmml">P</mi><mi id="S4.E22.m1.1.1.3.2.2.3" xref="S4.E22.m1.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S4.E22.m1.1.1.3.2.1" xref="S4.E22.m1.1.1.3.2.1.cmml">∩</mo><mrow id="S4.E22.m1.1.1.3.2.3" xref="S4.E22.m1.1.1.3.2.3.cmml"><mi id="S4.E22.m1.1.1.3.2.3.2" xref="S4.E22.m1.1.1.3.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E22.m1.1.1.3.2.3.1" xref="S4.E22.m1.1.1.3.2.3.1.cmml">​</mo><msub id="S4.E22.m1.1.1.3.2.3.3" xref="S4.E22.m1.1.1.3.2.3.3.cmml"><mi id="S4.E22.m1.1.1.3.2.3.3.2" xref="S4.E22.m1.1.1.3.2.3.3.2.cmml">T</mi><mi id="S4.E22.m1.1.1.3.2.3.3.3" xref="S4.E22.m1.1.1.3.2.3.3.3.cmml">i</mi></msub></mrow></mrow><mrow id="S4.E22.m1.1.1.3.3" xref="S4.E22.m1.1.1.3.3.cmml"><mi id="S4.E22.m1.1.1.3.3.2" xref="S4.E22.m1.1.1.3.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E22.m1.1.1.3.3.1" xref="S4.E22.m1.1.1.3.3.1.cmml">​</mo><msub id="S4.E22.m1.1.1.3.3.3" xref="S4.E22.m1.1.1.3.3.3.cmml"><mi id="S4.E22.m1.1.1.3.3.3.2" xref="S4.E22.m1.1.1.3.3.3.2.cmml">T</mi><mi id="S4.E22.m1.1.1.3.3.3.3" xref="S4.E22.m1.1.1.3.3.3.3.cmml">i</mi></msub></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E22.m1.1b"><apply id="S4.E22.m1.1.1.cmml" xref="S4.E22.m1.1.1"><eq id="S4.E22.m1.1.1.1.cmml" xref="S4.E22.m1.1.1.1"></eq><apply id="S4.E22.m1.1.1.2.cmml" xref="S4.E22.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E22.m1.1.1.2.1.cmml" xref="S4.E22.m1.1.1.2">subscript</csymbol><ci id="S4.E22.m1.1.1.2.2.cmml" xref="S4.E22.m1.1.1.2.2">𝑟</ci><ci id="S4.E22.m1.1.1.2.3.cmml" xref="S4.E22.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.E22.m1.1.1.3.cmml" xref="S4.E22.m1.1.1.3"><divide id="S4.E22.m1.1.1.3.1.cmml" xref="S4.E22.m1.1.1.3"></divide><apply id="S4.E22.m1.1.1.3.2.cmml" xref="S4.E22.m1.1.1.3.2"><intersect id="S4.E22.m1.1.1.3.2.1.cmml" xref="S4.E22.m1.1.1.3.2.1"></intersect><apply id="S4.E22.m1.1.1.3.2.2.cmml" xref="S4.E22.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E22.m1.1.1.3.2.2.1.cmml" xref="S4.E22.m1.1.1.3.2.2">subscript</csymbol><ci id="S4.E22.m1.1.1.3.2.2.2.cmml" xref="S4.E22.m1.1.1.3.2.2.2">𝑃</ci><ci id="S4.E22.m1.1.1.3.2.2.3.cmml" xref="S4.E22.m1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S4.E22.m1.1.1.3.2.3.cmml" xref="S4.E22.m1.1.1.3.2.3"><times id="S4.E22.m1.1.1.3.2.3.1.cmml" xref="S4.E22.m1.1.1.3.2.3.1"></times><ci id="S4.E22.m1.1.1.3.2.3.2.cmml" xref="S4.E22.m1.1.1.3.2.3.2">𝐺</ci><apply id="S4.E22.m1.1.1.3.2.3.3.cmml" xref="S4.E22.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E22.m1.1.1.3.2.3.3.1.cmml" xref="S4.E22.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E22.m1.1.1.3.2.3.3.2.cmml" xref="S4.E22.m1.1.1.3.2.3.3.2">𝑇</ci><ci id="S4.E22.m1.1.1.3.2.3.3.3.cmml" xref="S4.E22.m1.1.1.3.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E22.m1.1.1.3.3.cmml" xref="S4.E22.m1.1.1.3.3"><times id="S4.E22.m1.1.1.3.3.1.cmml" xref="S4.E22.m1.1.1.3.3.1"></times><ci id="S4.E22.m1.1.1.3.3.2.cmml" xref="S4.E22.m1.1.1.3.3.2">𝐺</ci><apply id="S4.E22.m1.1.1.3.3.3.cmml" xref="S4.E22.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E22.m1.1.1.3.3.3.1.cmml" xref="S4.E22.m1.1.1.3.3.3">subscript</csymbol><ci id="S4.E22.m1.1.1.3.3.3.2.cmml" xref="S4.E22.m1.1.1.3.3.3.2">𝑇</ci><ci id="S4.E22.m1.1.1.3.3.3.3.cmml" xref="S4.E22.m1.1.1.3.3.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E22.m1.1c">r_{i}=\frac{P_{i}\cap GT_{i}}{GT_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(22)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS2.p1.5" class="ltx_p">Let <math id="S4.SS2.SSS2.p1.5.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.SSS2.p1.5.m1.1a"><mi id="S4.SS2.SSS2.p1.5.m1.1.1" xref="S4.SS2.SSS2.p1.5.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS2.p1.5.m1.1b"><ci id="S4.SS2.SSS2.p1.5.m1.1.1.cmml" xref="S4.SS2.SSS2.p1.5.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p1.5.m1.1c">N</annotation></semantics></math> represent the total number of ground truth answers, so the overall recall is computed as:</p>
<table id="S4.E23" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E23.m1.1" class="ltx_Math" alttext="Recall=\frac{1}{N}\sum_{i=1}^{N}r_{i}" display="block"><semantics id="S4.E23.m1.1a"><mrow id="S4.E23.m1.1.1" xref="S4.E23.m1.1.1.cmml"><mrow id="S4.E23.m1.1.1.2" xref="S4.E23.m1.1.1.2.cmml"><mi id="S4.E23.m1.1.1.2.2" xref="S4.E23.m1.1.1.2.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.2.1" xref="S4.E23.m1.1.1.2.1.cmml">​</mo><mi id="S4.E23.m1.1.1.2.3" xref="S4.E23.m1.1.1.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.2.1a" xref="S4.E23.m1.1.1.2.1.cmml">​</mo><mi id="S4.E23.m1.1.1.2.4" xref="S4.E23.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.2.1b" xref="S4.E23.m1.1.1.2.1.cmml">​</mo><mi id="S4.E23.m1.1.1.2.5" xref="S4.E23.m1.1.1.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.2.1c" xref="S4.E23.m1.1.1.2.1.cmml">​</mo><mi id="S4.E23.m1.1.1.2.6" xref="S4.E23.m1.1.1.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.2.1d" xref="S4.E23.m1.1.1.2.1.cmml">​</mo><mi id="S4.E23.m1.1.1.2.7" xref="S4.E23.m1.1.1.2.7.cmml">l</mi></mrow><mo id="S4.E23.m1.1.1.1" xref="S4.E23.m1.1.1.1.cmml">=</mo><mrow id="S4.E23.m1.1.1.3" xref="S4.E23.m1.1.1.3.cmml"><mfrac id="S4.E23.m1.1.1.3.2" xref="S4.E23.m1.1.1.3.2.cmml"><mn id="S4.E23.m1.1.1.3.2.2" xref="S4.E23.m1.1.1.3.2.2.cmml">1</mn><mi id="S4.E23.m1.1.1.3.2.3" xref="S4.E23.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E23.m1.1.1.3.1" xref="S4.E23.m1.1.1.3.1.cmml">​</mo><mrow id="S4.E23.m1.1.1.3.3" xref="S4.E23.m1.1.1.3.3.cmml"><munderover id="S4.E23.m1.1.1.3.3.1" xref="S4.E23.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E23.m1.1.1.3.3.1.2.2" xref="S4.E23.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E23.m1.1.1.3.3.1.2.3" xref="S4.E23.m1.1.1.3.3.1.2.3.cmml"><mi id="S4.E23.m1.1.1.3.3.1.2.3.2" xref="S4.E23.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E23.m1.1.1.3.3.1.2.3.1" xref="S4.E23.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E23.m1.1.1.3.3.1.2.3.3" xref="S4.E23.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E23.m1.1.1.3.3.1.3" xref="S4.E23.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S4.E23.m1.1.1.3.3.2" xref="S4.E23.m1.1.1.3.3.2.cmml"><mi id="S4.E23.m1.1.1.3.3.2.2" xref="S4.E23.m1.1.1.3.3.2.2.cmml">r</mi><mi id="S4.E23.m1.1.1.3.3.2.3" xref="S4.E23.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E23.m1.1b"><apply id="S4.E23.m1.1.1.cmml" xref="S4.E23.m1.1.1"><eq id="S4.E23.m1.1.1.1.cmml" xref="S4.E23.m1.1.1.1"></eq><apply id="S4.E23.m1.1.1.2.cmml" xref="S4.E23.m1.1.1.2"><times id="S4.E23.m1.1.1.2.1.cmml" xref="S4.E23.m1.1.1.2.1"></times><ci id="S4.E23.m1.1.1.2.2.cmml" xref="S4.E23.m1.1.1.2.2">𝑅</ci><ci id="S4.E23.m1.1.1.2.3.cmml" xref="S4.E23.m1.1.1.2.3">𝑒</ci><ci id="S4.E23.m1.1.1.2.4.cmml" xref="S4.E23.m1.1.1.2.4">𝑐</ci><ci id="S4.E23.m1.1.1.2.5.cmml" xref="S4.E23.m1.1.1.2.5">𝑎</ci><ci id="S4.E23.m1.1.1.2.6.cmml" xref="S4.E23.m1.1.1.2.6">𝑙</ci><ci id="S4.E23.m1.1.1.2.7.cmml" xref="S4.E23.m1.1.1.2.7">𝑙</ci></apply><apply id="S4.E23.m1.1.1.3.cmml" xref="S4.E23.m1.1.1.3"><times id="S4.E23.m1.1.1.3.1.cmml" xref="S4.E23.m1.1.1.3.1"></times><apply id="S4.E23.m1.1.1.3.2.cmml" xref="S4.E23.m1.1.1.3.2"><divide id="S4.E23.m1.1.1.3.2.1.cmml" xref="S4.E23.m1.1.1.3.2"></divide><cn type="integer" id="S4.E23.m1.1.1.3.2.2.cmml" xref="S4.E23.m1.1.1.3.2.2">1</cn><ci id="S4.E23.m1.1.1.3.2.3.cmml" xref="S4.E23.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S4.E23.m1.1.1.3.3.cmml" xref="S4.E23.m1.1.1.3.3"><apply id="S4.E23.m1.1.1.3.3.1.cmml" xref="S4.E23.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E23.m1.1.1.3.3.1.1.cmml" xref="S4.E23.m1.1.1.3.3.1">superscript</csymbol><apply id="S4.E23.m1.1.1.3.3.1.2.cmml" xref="S4.E23.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E23.m1.1.1.3.3.1.2.1.cmml" xref="S4.E23.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E23.m1.1.1.3.3.1.2.2.cmml" xref="S4.E23.m1.1.1.3.3.1.2.2"></sum><apply id="S4.E23.m1.1.1.3.3.1.2.3.cmml" xref="S4.E23.m1.1.1.3.3.1.2.3"><eq id="S4.E23.m1.1.1.3.3.1.2.3.1.cmml" xref="S4.E23.m1.1.1.3.3.1.2.3.1"></eq><ci id="S4.E23.m1.1.1.3.3.1.2.3.2.cmml" xref="S4.E23.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E23.m1.1.1.3.3.1.2.3.3.cmml" xref="S4.E23.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E23.m1.1.1.3.3.1.3.cmml" xref="S4.E23.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S4.E23.m1.1.1.3.3.2.cmml" xref="S4.E23.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E23.m1.1.1.3.3.2.1.cmml" xref="S4.E23.m1.1.1.3.3.2">subscript</csymbol><ci id="S4.E23.m1.1.1.3.3.2.2.cmml" xref="S4.E23.m1.1.1.3.3.2.2">𝑟</ci><ci id="S4.E23.m1.1.1.3.3.2.3.cmml" xref="S4.E23.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E23.m1.1c">Recall=\frac{1}{N}\sum_{i=1}^{N}r_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(23)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Precision</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.4" class="ltx_p">Precision, in the context of tokenized forms, evaluates how many of the predicted tokens match those in the ground truth answer. Let <math id="S4.SS2.SSS3.p1.1.m1.1" class="ltx_Math" alttext="P_{i}" display="inline"><semantics id="S4.SS2.SSS3.p1.1.m1.1a"><msub id="S4.SS2.SSS3.p1.1.m1.1.1" xref="S4.SS2.SSS3.p1.1.m1.1.1.cmml"><mi id="S4.SS2.SSS3.p1.1.m1.1.1.2" xref="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml">P</mi><mi id="S4.SS2.SSS3.p1.1.m1.1.1.3" xref="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.1.m1.1b"><apply id="S4.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p1.1.m1.1.1.1.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.SSS3.p1.1.m1.1.1.2.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.SSS3.p1.1.m1.1.1.3.cmml" xref="S4.SS2.SSS3.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.1.m1.1c">P_{i}</annotation></semantics></math> represent the tokens in the predicted answer and <math id="S4.SS2.SSS3.p1.2.m2.1" class="ltx_Math" alttext="GT_{i}" display="inline"><semantics id="S4.SS2.SSS3.p1.2.m2.1a"><mrow id="S4.SS2.SSS3.p1.2.m2.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.cmml"><mi id="S4.SS2.SSS3.p1.2.m2.1.1.2" xref="S4.SS2.SSS3.p1.2.m2.1.1.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.SS2.SSS3.p1.2.m2.1.1.1" xref="S4.SS2.SSS3.p1.2.m2.1.1.1.cmml">​</mo><msub id="S4.SS2.SSS3.p1.2.m2.1.1.3" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.cmml"><mi id="S4.SS2.SSS3.p1.2.m2.1.1.3.2" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.2.cmml">T</mi><mi id="S4.SS2.SSS3.p1.2.m2.1.1.3.3" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.3.cmml">i</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.2.m2.1b"><apply id="S4.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1"><times id="S4.SS2.SSS3.p1.2.m2.1.1.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.1"></times><ci id="S4.SS2.SSS3.p1.2.m2.1.1.2.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.2">𝐺</ci><apply id="S4.SS2.SSS3.p1.2.m2.1.1.3.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.SSS3.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.SSS3.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.2">𝑇</ci><ci id="S4.SS2.SSS3.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.SSS3.p1.2.m2.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.2.m2.1c">GT_{i}</annotation></semantics></math> denote the tokens in the ground truth answer for the question <math id="S4.SS2.SSS3.p1.3.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS3.p1.3.m3.1a"><mi id="S4.SS2.SSS3.p1.3.m3.1.1" xref="S4.SS2.SSS3.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.3.m3.1b"><ci id="S4.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS3.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.3.m3.1c">i</annotation></semantics></math>-th. The precision for the question <math id="S4.SS2.SSS3.p1.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS3.p1.4.m4.1a"><mi id="S4.SS2.SSS3.p1.4.m4.1.1" xref="S4.SS2.SSS3.p1.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.4.m4.1b"><ci id="S4.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S4.SS2.SSS3.p1.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.4.m4.1c">i</annotation></semantics></math>-th is calculated as follows:</p>
<table id="S4.E24" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E24.m1.1" class="ltx_Math" alttext="p_{i}=\frac{P_{i}\cap GT_{i}}{P_{i}}" display="block"><semantics id="S4.E24.m1.1a"><mrow id="S4.E24.m1.1.1" xref="S4.E24.m1.1.1.cmml"><msub id="S4.E24.m1.1.1.2" xref="S4.E24.m1.1.1.2.cmml"><mi id="S4.E24.m1.1.1.2.2" xref="S4.E24.m1.1.1.2.2.cmml">p</mi><mi id="S4.E24.m1.1.1.2.3" xref="S4.E24.m1.1.1.2.3.cmml">i</mi></msub><mo id="S4.E24.m1.1.1.1" xref="S4.E24.m1.1.1.1.cmml">=</mo><mfrac id="S4.E24.m1.1.1.3" xref="S4.E24.m1.1.1.3.cmml"><mrow id="S4.E24.m1.1.1.3.2" xref="S4.E24.m1.1.1.3.2.cmml"><msub id="S4.E24.m1.1.1.3.2.2" xref="S4.E24.m1.1.1.3.2.2.cmml"><mi id="S4.E24.m1.1.1.3.2.2.2" xref="S4.E24.m1.1.1.3.2.2.2.cmml">P</mi><mi id="S4.E24.m1.1.1.3.2.2.3" xref="S4.E24.m1.1.1.3.2.2.3.cmml">i</mi></msub><mo id="S4.E24.m1.1.1.3.2.1" xref="S4.E24.m1.1.1.3.2.1.cmml">∩</mo><mrow id="S4.E24.m1.1.1.3.2.3" xref="S4.E24.m1.1.1.3.2.3.cmml"><mi id="S4.E24.m1.1.1.3.2.3.2" xref="S4.E24.m1.1.1.3.2.3.2.cmml">G</mi><mo lspace="0em" rspace="0em" id="S4.E24.m1.1.1.3.2.3.1" xref="S4.E24.m1.1.1.3.2.3.1.cmml">​</mo><msub id="S4.E24.m1.1.1.3.2.3.3" xref="S4.E24.m1.1.1.3.2.3.3.cmml"><mi id="S4.E24.m1.1.1.3.2.3.3.2" xref="S4.E24.m1.1.1.3.2.3.3.2.cmml">T</mi><mi id="S4.E24.m1.1.1.3.2.3.3.3" xref="S4.E24.m1.1.1.3.2.3.3.3.cmml">i</mi></msub></mrow></mrow><msub id="S4.E24.m1.1.1.3.3" xref="S4.E24.m1.1.1.3.3.cmml"><mi id="S4.E24.m1.1.1.3.3.2" xref="S4.E24.m1.1.1.3.3.2.cmml">P</mi><mi id="S4.E24.m1.1.1.3.3.3" xref="S4.E24.m1.1.1.3.3.3.cmml">i</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E24.m1.1b"><apply id="S4.E24.m1.1.1.cmml" xref="S4.E24.m1.1.1"><eq id="S4.E24.m1.1.1.1.cmml" xref="S4.E24.m1.1.1.1"></eq><apply id="S4.E24.m1.1.1.2.cmml" xref="S4.E24.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E24.m1.1.1.2.1.cmml" xref="S4.E24.m1.1.1.2">subscript</csymbol><ci id="S4.E24.m1.1.1.2.2.cmml" xref="S4.E24.m1.1.1.2.2">𝑝</ci><ci id="S4.E24.m1.1.1.2.3.cmml" xref="S4.E24.m1.1.1.2.3">𝑖</ci></apply><apply id="S4.E24.m1.1.1.3.cmml" xref="S4.E24.m1.1.1.3"><divide id="S4.E24.m1.1.1.3.1.cmml" xref="S4.E24.m1.1.1.3"></divide><apply id="S4.E24.m1.1.1.3.2.cmml" xref="S4.E24.m1.1.1.3.2"><intersect id="S4.E24.m1.1.1.3.2.1.cmml" xref="S4.E24.m1.1.1.3.2.1"></intersect><apply id="S4.E24.m1.1.1.3.2.2.cmml" xref="S4.E24.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="S4.E24.m1.1.1.3.2.2.1.cmml" xref="S4.E24.m1.1.1.3.2.2">subscript</csymbol><ci id="S4.E24.m1.1.1.3.2.2.2.cmml" xref="S4.E24.m1.1.1.3.2.2.2">𝑃</ci><ci id="S4.E24.m1.1.1.3.2.2.3.cmml" xref="S4.E24.m1.1.1.3.2.2.3">𝑖</ci></apply><apply id="S4.E24.m1.1.1.3.2.3.cmml" xref="S4.E24.m1.1.1.3.2.3"><times id="S4.E24.m1.1.1.3.2.3.1.cmml" xref="S4.E24.m1.1.1.3.2.3.1"></times><ci id="S4.E24.m1.1.1.3.2.3.2.cmml" xref="S4.E24.m1.1.1.3.2.3.2">𝐺</ci><apply id="S4.E24.m1.1.1.3.2.3.3.cmml" xref="S4.E24.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E24.m1.1.1.3.2.3.3.1.cmml" xref="S4.E24.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E24.m1.1.1.3.2.3.3.2.cmml" xref="S4.E24.m1.1.1.3.2.3.3.2">𝑇</ci><ci id="S4.E24.m1.1.1.3.2.3.3.3.cmml" xref="S4.E24.m1.1.1.3.2.3.3.3">𝑖</ci></apply></apply></apply><apply id="S4.E24.m1.1.1.3.3.cmml" xref="S4.E24.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E24.m1.1.1.3.3.1.cmml" xref="S4.E24.m1.1.1.3.3">subscript</csymbol><ci id="S4.E24.m1.1.1.3.3.2.cmml" xref="S4.E24.m1.1.1.3.3.2">𝑃</ci><ci id="S4.E24.m1.1.1.3.3.3.cmml" xref="S4.E24.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E24.m1.1c">p_{i}=\frac{P_{i}\cap GT_{i}}{P_{i}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(24)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS3.p1.5" class="ltx_p">Let <math id="S4.SS2.SSS3.p1.5.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.SSS3.p1.5.m1.1a"><mi id="S4.SS2.SSS3.p1.5.m1.1.1" xref="S4.SS2.SSS3.p1.5.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS3.p1.5.m1.1b"><ci id="S4.SS2.SSS3.p1.5.m1.1.1.cmml" xref="S4.SS2.SSS3.p1.5.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS3.p1.5.m1.1c">N</annotation></semantics></math> represent the total number of ground truth answers, so the overall recall is computed as:</p>
<table id="S4.E25" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E25.m1.1" class="ltx_Math" alttext="Precision=\frac{1}{N}\sum_{i=1}^{N}p_{i}" display="block"><semantics id="S4.E25.m1.1a"><mrow id="S4.E25.m1.1.1" xref="S4.E25.m1.1.1.cmml"><mrow id="S4.E25.m1.1.1.2" xref="S4.E25.m1.1.1.2.cmml"><mi id="S4.E25.m1.1.1.2.2" xref="S4.E25.m1.1.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.3" xref="S4.E25.m1.1.1.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1a" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.4" xref="S4.E25.m1.1.1.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1b" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.5" xref="S4.E25.m1.1.1.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1c" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.6" xref="S4.E25.m1.1.1.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1d" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.7" xref="S4.E25.m1.1.1.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1e" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.8" xref="S4.E25.m1.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1f" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.9" xref="S4.E25.m1.1.1.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.2.1g" xref="S4.E25.m1.1.1.2.1.cmml">​</mo><mi id="S4.E25.m1.1.1.2.10" xref="S4.E25.m1.1.1.2.10.cmml">n</mi></mrow><mo id="S4.E25.m1.1.1.1" xref="S4.E25.m1.1.1.1.cmml">=</mo><mrow id="S4.E25.m1.1.1.3" xref="S4.E25.m1.1.1.3.cmml"><mfrac id="S4.E25.m1.1.1.3.2" xref="S4.E25.m1.1.1.3.2.cmml"><mn id="S4.E25.m1.1.1.3.2.2" xref="S4.E25.m1.1.1.3.2.2.cmml">1</mn><mi id="S4.E25.m1.1.1.3.2.3" xref="S4.E25.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E25.m1.1.1.3.1" xref="S4.E25.m1.1.1.3.1.cmml">​</mo><mrow id="S4.E25.m1.1.1.3.3" xref="S4.E25.m1.1.1.3.3.cmml"><munderover id="S4.E25.m1.1.1.3.3.1" xref="S4.E25.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E25.m1.1.1.3.3.1.2.2" xref="S4.E25.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E25.m1.1.1.3.3.1.2.3" xref="S4.E25.m1.1.1.3.3.1.2.3.cmml"><mi id="S4.E25.m1.1.1.3.3.1.2.3.2" xref="S4.E25.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E25.m1.1.1.3.3.1.2.3.1" xref="S4.E25.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E25.m1.1.1.3.3.1.2.3.3" xref="S4.E25.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E25.m1.1.1.3.3.1.3" xref="S4.E25.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S4.E25.m1.1.1.3.3.2" xref="S4.E25.m1.1.1.3.3.2.cmml"><mi id="S4.E25.m1.1.1.3.3.2.2" xref="S4.E25.m1.1.1.3.3.2.2.cmml">p</mi><mi id="S4.E25.m1.1.1.3.3.2.3" xref="S4.E25.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E25.m1.1b"><apply id="S4.E25.m1.1.1.cmml" xref="S4.E25.m1.1.1"><eq id="S4.E25.m1.1.1.1.cmml" xref="S4.E25.m1.1.1.1"></eq><apply id="S4.E25.m1.1.1.2.cmml" xref="S4.E25.m1.1.1.2"><times id="S4.E25.m1.1.1.2.1.cmml" xref="S4.E25.m1.1.1.2.1"></times><ci id="S4.E25.m1.1.1.2.2.cmml" xref="S4.E25.m1.1.1.2.2">𝑃</ci><ci id="S4.E25.m1.1.1.2.3.cmml" xref="S4.E25.m1.1.1.2.3">𝑟</ci><ci id="S4.E25.m1.1.1.2.4.cmml" xref="S4.E25.m1.1.1.2.4">𝑒</ci><ci id="S4.E25.m1.1.1.2.5.cmml" xref="S4.E25.m1.1.1.2.5">𝑐</ci><ci id="S4.E25.m1.1.1.2.6.cmml" xref="S4.E25.m1.1.1.2.6">𝑖</ci><ci id="S4.E25.m1.1.1.2.7.cmml" xref="S4.E25.m1.1.1.2.7">𝑠</ci><ci id="S4.E25.m1.1.1.2.8.cmml" xref="S4.E25.m1.1.1.2.8">𝑖</ci><ci id="S4.E25.m1.1.1.2.9.cmml" xref="S4.E25.m1.1.1.2.9">𝑜</ci><ci id="S4.E25.m1.1.1.2.10.cmml" xref="S4.E25.m1.1.1.2.10">𝑛</ci></apply><apply id="S4.E25.m1.1.1.3.cmml" xref="S4.E25.m1.1.1.3"><times id="S4.E25.m1.1.1.3.1.cmml" xref="S4.E25.m1.1.1.3.1"></times><apply id="S4.E25.m1.1.1.3.2.cmml" xref="S4.E25.m1.1.1.3.2"><divide id="S4.E25.m1.1.1.3.2.1.cmml" xref="S4.E25.m1.1.1.3.2"></divide><cn type="integer" id="S4.E25.m1.1.1.3.2.2.cmml" xref="S4.E25.m1.1.1.3.2.2">1</cn><ci id="S4.E25.m1.1.1.3.2.3.cmml" xref="S4.E25.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S4.E25.m1.1.1.3.3.cmml" xref="S4.E25.m1.1.1.3.3"><apply id="S4.E25.m1.1.1.3.3.1.cmml" xref="S4.E25.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E25.m1.1.1.3.3.1.1.cmml" xref="S4.E25.m1.1.1.3.3.1">superscript</csymbol><apply id="S4.E25.m1.1.1.3.3.1.2.cmml" xref="S4.E25.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E25.m1.1.1.3.3.1.2.1.cmml" xref="S4.E25.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E25.m1.1.1.3.3.1.2.2.cmml" xref="S4.E25.m1.1.1.3.3.1.2.2"></sum><apply id="S4.E25.m1.1.1.3.3.1.2.3.cmml" xref="S4.E25.m1.1.1.3.3.1.2.3"><eq id="S4.E25.m1.1.1.3.3.1.2.3.1.cmml" xref="S4.E25.m1.1.1.3.3.1.2.3.1"></eq><ci id="S4.E25.m1.1.1.3.3.1.2.3.2.cmml" xref="S4.E25.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E25.m1.1.1.3.3.1.2.3.3.cmml" xref="S4.E25.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E25.m1.1.1.3.3.1.3.cmml" xref="S4.E25.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S4.E25.m1.1.1.3.3.2.cmml" xref="S4.E25.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E25.m1.1.1.3.3.2.1.cmml" xref="S4.E25.m1.1.1.3.3.2">subscript</csymbol><ci id="S4.E25.m1.1.1.3.3.2.2.cmml" xref="S4.E25.m1.1.1.3.3.2.2">𝑝</ci><ci id="S4.E25.m1.1.1.3.3.2.3.cmml" xref="S4.E25.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E25.m1.1c">Precision=\frac{1}{N}\sum_{i=1}^{N}p_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(25)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4 </span>F1 score</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">The F1 score is essentially computed from both recall (<a href="#S4.E22" title="In 4.2.2 Recall ‣ 4.2 Evaluation Metrics ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">22</span></a>) and precision (<a href="#S4.E24" title="In 4.2.3 Precision ‣ 4.2 Evaluation Metrics ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">24</span></a>) of each individual question. The F1 score for the question <math id="S4.SS2.SSS4.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.SSS4.p1.1.m1.1a"><mi id="S4.SS2.SSS4.p1.1.m1.1.1" xref="S4.SS2.SSS4.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.1.m1.1b"><ci id="S4.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.1.m1.1c">i</annotation></semantics></math>-th is calculated as follows:</p>
<table id="S4.E26" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E26.m1.4" class="ltx_Math" alttext="f_{i}=\begin{cases}0,&amp;\text{if}\ p_{i}=0,~{}r_{i}=0\\
\frac{2\times p_{i}\times r_{i}}{p_{i}+r_{i}},&amp;\text{otherwise}\end{cases}" display="block"><semantics id="S4.E26.m1.4a"><mrow id="S4.E26.m1.4.5" xref="S4.E26.m1.4.5.cmml"><msub id="S4.E26.m1.4.5.2" xref="S4.E26.m1.4.5.2.cmml"><mi id="S4.E26.m1.4.5.2.2" xref="S4.E26.m1.4.5.2.2.cmml">f</mi><mi id="S4.E26.m1.4.5.2.3" xref="S4.E26.m1.4.5.2.3.cmml">i</mi></msub><mo id="S4.E26.m1.4.5.1" xref="S4.E26.m1.4.5.1.cmml">=</mo><mrow id="S4.E26.m1.4.4" xref="S4.E26.m1.4.5.3.1.cmml"><mo id="S4.E26.m1.4.4.5" xref="S4.E26.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S4.E26.m1.4.4.4" xref="S4.E26.m1.4.5.3.1.cmml"><mtr id="S4.E26.m1.4.4.4a" xref="S4.E26.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E26.m1.4.4.4b" xref="S4.E26.m1.4.5.3.1.cmml"><mrow id="S4.E26.m1.1.1.1.1.1.1.3" xref="S4.E26.m1.4.5.3.1.cmml"><mn id="S4.E26.m1.1.1.1.1.1.1.1" xref="S4.E26.m1.1.1.1.1.1.1.1.cmml">0</mn><mo id="S4.E26.m1.1.1.1.1.1.1.3.1" xref="S4.E26.m1.4.5.3.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E26.m1.4.4.4c" xref="S4.E26.m1.4.5.3.1.cmml"><mrow id="S4.E26.m1.2.2.2.2.2.1.2" xref="S4.E26.m1.2.2.2.2.2.1.3.cmml"><mrow id="S4.E26.m1.2.2.2.2.2.1.1.1" xref="S4.E26.m1.2.2.2.2.2.1.1.1.cmml"><mrow id="S4.E26.m1.2.2.2.2.2.1.1.1.2" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.cmml"><mtext id="S4.E26.m1.2.2.2.2.2.1.1.1.2.2" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.2a.cmml">if</mtext><mo lspace="0.500em" rspace="0em" id="S4.E26.m1.2.2.2.2.2.1.1.1.2.1" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.1.cmml">​</mo><msub id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.cmml"><mi id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.2" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.2.cmml">p</mi><mi id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.3" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S4.E26.m1.2.2.2.2.2.1.1.1.1" xref="S4.E26.m1.2.2.2.2.2.1.1.1.1.cmml">=</mo><mn id="S4.E26.m1.2.2.2.2.2.1.1.1.3" xref="S4.E26.m1.2.2.2.2.2.1.1.1.3.cmml">0</mn></mrow><mo rspace="0.497em" id="S4.E26.m1.2.2.2.2.2.1.2.3" xref="S4.E26.m1.2.2.2.2.2.1.3a.cmml">,</mo><mrow id="S4.E26.m1.2.2.2.2.2.1.2.2" xref="S4.E26.m1.2.2.2.2.2.1.2.2.cmml"><msub id="S4.E26.m1.2.2.2.2.2.1.2.2.2" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2.cmml"><mi id="S4.E26.m1.2.2.2.2.2.1.2.2.2.2" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2.2.cmml">r</mi><mi id="S4.E26.m1.2.2.2.2.2.1.2.2.2.3" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2.3.cmml">i</mi></msub><mo id="S4.E26.m1.2.2.2.2.2.1.2.2.1" xref="S4.E26.m1.2.2.2.2.2.1.2.2.1.cmml">=</mo><mn id="S4.E26.m1.2.2.2.2.2.1.2.2.3" xref="S4.E26.m1.2.2.2.2.2.1.2.2.3.cmml">0</mn></mrow></mrow></mtd></mtr><mtr id="S4.E26.m1.4.4.4d" xref="S4.E26.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S4.E26.m1.4.4.4e" xref="S4.E26.m1.4.5.3.1.cmml"><mrow id="S4.E26.m1.3.3.3.3.1.1.3" xref="S4.E26.m1.3.3.3.3.1.1.1.cmml"><mstyle displaystyle="false" id="S4.E26.m1.3.3.3.3.1.1.1" xref="S4.E26.m1.3.3.3.3.1.1.1.cmml"><mfrac id="S4.E26.m1.3.3.3.3.1.1.1a" xref="S4.E26.m1.3.3.3.3.1.1.1.cmml"><mrow id="S4.E26.m1.3.3.3.3.1.1.1.2" xref="S4.E26.m1.3.3.3.3.1.1.1.2.cmml"><mn id="S4.E26.m1.3.3.3.3.1.1.1.2.2" xref="S4.E26.m1.3.3.3.3.1.1.1.2.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E26.m1.3.3.3.3.1.1.1.2.1" xref="S4.E26.m1.3.3.3.3.1.1.1.2.1.cmml">×</mo><msub id="S4.E26.m1.3.3.3.3.1.1.1.2.3" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3.cmml"><mi id="S4.E26.m1.3.3.3.3.1.1.1.2.3.2" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3.2.cmml">p</mi><mi id="S4.E26.m1.3.3.3.3.1.1.1.2.3.3" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S4.E26.m1.3.3.3.3.1.1.1.2.1a" xref="S4.E26.m1.3.3.3.3.1.1.1.2.1.cmml">×</mo><msub id="S4.E26.m1.3.3.3.3.1.1.1.2.4" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4.cmml"><mi id="S4.E26.m1.3.3.3.3.1.1.1.2.4.2" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4.2.cmml">r</mi><mi id="S4.E26.m1.3.3.3.3.1.1.1.2.4.3" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4.3.cmml">i</mi></msub></mrow><mrow id="S4.E26.m1.3.3.3.3.1.1.1.3" xref="S4.E26.m1.3.3.3.3.1.1.1.3.cmml"><msub id="S4.E26.m1.3.3.3.3.1.1.1.3.2" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2.cmml"><mi id="S4.E26.m1.3.3.3.3.1.1.1.3.2.2" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2.2.cmml">p</mi><mi id="S4.E26.m1.3.3.3.3.1.1.1.3.2.3" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2.3.cmml">i</mi></msub><mo id="S4.E26.m1.3.3.3.3.1.1.1.3.1" xref="S4.E26.m1.3.3.3.3.1.1.1.3.1.cmml">+</mo><msub id="S4.E26.m1.3.3.3.3.1.1.1.3.3" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3.cmml"><mi id="S4.E26.m1.3.3.3.3.1.1.1.3.3.2" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3.2.cmml">r</mi><mi id="S4.E26.m1.3.3.3.3.1.1.1.3.3.3" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3.3.cmml">i</mi></msub></mrow></mfrac></mstyle><mo id="S4.E26.m1.3.3.3.3.1.1.3.1" xref="S4.E26.m1.3.3.3.3.1.1.1.cmml">,</mo></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S4.E26.m1.4.4.4f" xref="S4.E26.m1.4.5.3.1.cmml"><mtext id="S4.E26.m1.4.4.4.4.2.1" xref="S4.E26.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E26.m1.4b"><apply id="S4.E26.m1.4.5.cmml" xref="S4.E26.m1.4.5"><eq id="S4.E26.m1.4.5.1.cmml" xref="S4.E26.m1.4.5.1"></eq><apply id="S4.E26.m1.4.5.2.cmml" xref="S4.E26.m1.4.5.2"><csymbol cd="ambiguous" id="S4.E26.m1.4.5.2.1.cmml" xref="S4.E26.m1.4.5.2">subscript</csymbol><ci id="S4.E26.m1.4.5.2.2.cmml" xref="S4.E26.m1.4.5.2.2">𝑓</ci><ci id="S4.E26.m1.4.5.2.3.cmml" xref="S4.E26.m1.4.5.2.3">𝑖</ci></apply><apply id="S4.E26.m1.4.5.3.1.cmml" xref="S4.E26.m1.4.4"><csymbol cd="latexml" id="S4.E26.m1.4.5.3.1.1.cmml" xref="S4.E26.m1.4.4.5">cases</csymbol><cn type="integer" id="S4.E26.m1.1.1.1.1.1.1.1.cmml" xref="S4.E26.m1.1.1.1.1.1.1.1">0</cn><apply id="S4.E26.m1.2.2.2.2.2.1.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2"><csymbol cd="ambiguous" id="S4.E26.m1.2.2.2.2.2.1.3a.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.3">formulae-sequence</csymbol><apply id="S4.E26.m1.2.2.2.2.2.1.1.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1"><eq id="S4.E26.m1.2.2.2.2.2.1.1.1.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.1"></eq><apply id="S4.E26.m1.2.2.2.2.2.1.1.1.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2"><times id="S4.E26.m1.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.1"></times><ci id="S4.E26.m1.2.2.2.2.2.1.1.1.2.2a.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.2"><mtext id="S4.E26.m1.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.2">if</mtext></ci><apply id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3">subscript</csymbol><ci id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.2">𝑝</ci><ci id="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.2.3.3">𝑖</ci></apply></apply><cn type="integer" id="S4.E26.m1.2.2.2.2.2.1.1.1.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.1.1.3">0</cn></apply><apply id="S4.E26.m1.2.2.2.2.2.1.2.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2"><eq id="S4.E26.m1.2.2.2.2.2.1.2.2.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.1"></eq><apply id="S4.E26.m1.2.2.2.2.2.1.2.2.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2"><csymbol cd="ambiguous" id="S4.E26.m1.2.2.2.2.2.1.2.2.2.1.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2">subscript</csymbol><ci id="S4.E26.m1.2.2.2.2.2.1.2.2.2.2.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2.2">𝑟</ci><ci id="S4.E26.m1.2.2.2.2.2.1.2.2.2.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.2.3">𝑖</ci></apply><cn type="integer" id="S4.E26.m1.2.2.2.2.2.1.2.2.3.cmml" xref="S4.E26.m1.2.2.2.2.2.1.2.2.3">0</cn></apply></apply><apply id="S4.E26.m1.3.3.3.3.1.1.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.3"><divide id="S4.E26.m1.3.3.3.3.1.1.1.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.3"></divide><apply id="S4.E26.m1.3.3.3.3.1.1.1.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2"><times id="S4.E26.m1.3.3.3.3.1.1.1.2.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.1"></times><cn type="integer" id="S4.E26.m1.3.3.3.3.1.1.1.2.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.2">2</cn><apply id="S4.E26.m1.3.3.3.3.1.1.1.2.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3"><csymbol cd="ambiguous" id="S4.E26.m1.3.3.3.3.1.1.1.2.3.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3">subscript</csymbol><ci id="S4.E26.m1.3.3.3.3.1.1.1.2.3.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3.2">𝑝</ci><ci id="S4.E26.m1.3.3.3.3.1.1.1.2.3.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.3.3">𝑖</ci></apply><apply id="S4.E26.m1.3.3.3.3.1.1.1.2.4.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4"><csymbol cd="ambiguous" id="S4.E26.m1.3.3.3.3.1.1.1.2.4.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4">subscript</csymbol><ci id="S4.E26.m1.3.3.3.3.1.1.1.2.4.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4.2">𝑟</ci><ci id="S4.E26.m1.3.3.3.3.1.1.1.2.4.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.2.4.3">𝑖</ci></apply></apply><apply id="S4.E26.m1.3.3.3.3.1.1.1.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3"><plus id="S4.E26.m1.3.3.3.3.1.1.1.3.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.1"></plus><apply id="S4.E26.m1.3.3.3.3.1.1.1.3.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E26.m1.3.3.3.3.1.1.1.3.2.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2">subscript</csymbol><ci id="S4.E26.m1.3.3.3.3.1.1.1.3.2.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2.2">𝑝</ci><ci id="S4.E26.m1.3.3.3.3.1.1.1.3.2.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.2.3">𝑖</ci></apply><apply id="S4.E26.m1.3.3.3.3.1.1.1.3.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E26.m1.3.3.3.3.1.1.1.3.3.1.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3">subscript</csymbol><ci id="S4.E26.m1.3.3.3.3.1.1.1.3.3.2.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3.2">𝑟</ci><ci id="S4.E26.m1.3.3.3.3.1.1.1.3.3.3.cmml" xref="S4.E26.m1.3.3.3.3.1.1.1.3.3.3">𝑖</ci></apply></apply></apply><ci id="S4.E26.m1.4.4.4.4.2.1a.cmml" xref="S4.E26.m1.4.4.4.4.2.1"><mtext id="S4.E26.m1.4.4.4.4.2.1.cmml" xref="S4.E26.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E26.m1.4c">f_{i}=\begin{cases}0,&amp;\text{if}\ p_{i}=0,~{}r_{i}=0\\
\frac{2\times p_{i}\times r_{i}}{p_{i}+r_{i}},&amp;\text{otherwise}\end{cases}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(26)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.SSS4.p1.2" class="ltx_p">Let <math id="S4.SS2.SSS4.p1.2.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S4.SS2.SSS4.p1.2.m1.1a"><mi id="S4.SS2.SSS4.p1.2.m1.1.1" xref="S4.SS2.SSS4.p1.2.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS4.p1.2.m1.1b"><ci id="S4.SS2.SSS4.p1.2.m1.1.1.cmml" xref="S4.SS2.SSS4.p1.2.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS4.p1.2.m1.1c">N</annotation></semantics></math> represent the total number of ground truth answers, so the overall F1 score is computed as:</p>
<table id="S4.E27" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S4.E27.m1.1" class="ltx_Math" alttext="F1=\frac{1}{N}\sum_{i=1}^{N}f_{i}" display="block"><semantics id="S4.E27.m1.1a"><mrow id="S4.E27.m1.1.1" xref="S4.E27.m1.1.1.cmml"><mrow id="S4.E27.m1.1.1.2" xref="S4.E27.m1.1.1.2.cmml"><mi id="S4.E27.m1.1.1.2.2" xref="S4.E27.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E27.m1.1.1.2.1" xref="S4.E27.m1.1.1.2.1.cmml">​</mo><mn id="S4.E27.m1.1.1.2.3" xref="S4.E27.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.E27.m1.1.1.1" xref="S4.E27.m1.1.1.1.cmml">=</mo><mrow id="S4.E27.m1.1.1.3" xref="S4.E27.m1.1.1.3.cmml"><mfrac id="S4.E27.m1.1.1.3.2" xref="S4.E27.m1.1.1.3.2.cmml"><mn id="S4.E27.m1.1.1.3.2.2" xref="S4.E27.m1.1.1.3.2.2.cmml">1</mn><mi id="S4.E27.m1.1.1.3.2.3" xref="S4.E27.m1.1.1.3.2.3.cmml">N</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E27.m1.1.1.3.1" xref="S4.E27.m1.1.1.3.1.cmml">​</mo><mrow id="S4.E27.m1.1.1.3.3" xref="S4.E27.m1.1.1.3.3.cmml"><munderover id="S4.E27.m1.1.1.3.3.1" xref="S4.E27.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E27.m1.1.1.3.3.1.2.2" xref="S4.E27.m1.1.1.3.3.1.2.2.cmml">∑</mo><mrow id="S4.E27.m1.1.1.3.3.1.2.3" xref="S4.E27.m1.1.1.3.3.1.2.3.cmml"><mi id="S4.E27.m1.1.1.3.3.1.2.3.2" xref="S4.E27.m1.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S4.E27.m1.1.1.3.3.1.2.3.1" xref="S4.E27.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E27.m1.1.1.3.3.1.2.3.3" xref="S4.E27.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E27.m1.1.1.3.3.1.3" xref="S4.E27.m1.1.1.3.3.1.3.cmml">N</mi></munderover><msub id="S4.E27.m1.1.1.3.3.2" xref="S4.E27.m1.1.1.3.3.2.cmml"><mi id="S4.E27.m1.1.1.3.3.2.2" xref="S4.E27.m1.1.1.3.3.2.2.cmml">f</mi><mi id="S4.E27.m1.1.1.3.3.2.3" xref="S4.E27.m1.1.1.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E27.m1.1b"><apply id="S4.E27.m1.1.1.cmml" xref="S4.E27.m1.1.1"><eq id="S4.E27.m1.1.1.1.cmml" xref="S4.E27.m1.1.1.1"></eq><apply id="S4.E27.m1.1.1.2.cmml" xref="S4.E27.m1.1.1.2"><times id="S4.E27.m1.1.1.2.1.cmml" xref="S4.E27.m1.1.1.2.1"></times><ci id="S4.E27.m1.1.1.2.2.cmml" xref="S4.E27.m1.1.1.2.2">𝐹</ci><cn type="integer" id="S4.E27.m1.1.1.2.3.cmml" xref="S4.E27.m1.1.1.2.3">1</cn></apply><apply id="S4.E27.m1.1.1.3.cmml" xref="S4.E27.m1.1.1.3"><times id="S4.E27.m1.1.1.3.1.cmml" xref="S4.E27.m1.1.1.3.1"></times><apply id="S4.E27.m1.1.1.3.2.cmml" xref="S4.E27.m1.1.1.3.2"><divide id="S4.E27.m1.1.1.3.2.1.cmml" xref="S4.E27.m1.1.1.3.2"></divide><cn type="integer" id="S4.E27.m1.1.1.3.2.2.cmml" xref="S4.E27.m1.1.1.3.2.2">1</cn><ci id="S4.E27.m1.1.1.3.2.3.cmml" xref="S4.E27.m1.1.1.3.2.3">𝑁</ci></apply><apply id="S4.E27.m1.1.1.3.3.cmml" xref="S4.E27.m1.1.1.3.3"><apply id="S4.E27.m1.1.1.3.3.1.cmml" xref="S4.E27.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E27.m1.1.1.3.3.1.1.cmml" xref="S4.E27.m1.1.1.3.3.1">superscript</csymbol><apply id="S4.E27.m1.1.1.3.3.1.2.cmml" xref="S4.E27.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E27.m1.1.1.3.3.1.2.1.cmml" xref="S4.E27.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E27.m1.1.1.3.3.1.2.2.cmml" xref="S4.E27.m1.1.1.3.3.1.2.2"></sum><apply id="S4.E27.m1.1.1.3.3.1.2.3.cmml" xref="S4.E27.m1.1.1.3.3.1.2.3"><eq id="S4.E27.m1.1.1.3.3.1.2.3.1.cmml" xref="S4.E27.m1.1.1.3.3.1.2.3.1"></eq><ci id="S4.E27.m1.1.1.3.3.1.2.3.2.cmml" xref="S4.E27.m1.1.1.3.3.1.2.3.2">𝑖</ci><cn type="integer" id="S4.E27.m1.1.1.3.3.1.2.3.3.cmml" xref="S4.E27.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E27.m1.1.1.3.3.1.3.cmml" xref="S4.E27.m1.1.1.3.3.1.3">𝑁</ci></apply><apply id="S4.E27.m1.1.1.3.3.2.cmml" xref="S4.E27.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S4.E27.m1.1.1.3.3.2.1.cmml" xref="S4.E27.m1.1.1.3.3.2">subscript</csymbol><ci id="S4.E27.m1.1.1.3.3.2.2.cmml" xref="S4.E27.m1.1.1.3.3.2.2">𝑓</ci><ci id="S4.E27.m1.1.1.3.3.2.3.cmml" xref="S4.E27.m1.1.1.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E27.m1.1c">F1=\frac{1}{N}\sum_{i=1}^{N}f_{i}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(27)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental Settings</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In our experiments, the hyperparameters presented in Table <a href="#S4.T2" title="Table 2 ‣ 4.3 Experimental Settings ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> were utilized. The training process spanned 20 epochs with a batch size of 65, a drop path rate of 0.3, a learning rate of 3e-5, weight decay set at 0.01, and a warmup ratio of 0.1. The input image resolution was 224 x 224. We employed AdamW <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> as the optimizer for updating parameters. The model training took place on a computing system equipped with NVIDIA TESLA P100-16GB GPUs and 32GB of RAM.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.2.3.1" class="ltx_tr">
<th id="S4.T2.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt"><span id="S4.T2.2.3.1.1.1" class="ltx_text ltx_font_bold">Hyperparameters</span></th>
<td id="S4.T2.2.3.1.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_tt"><span id="S4.T2.2.3.1.2.1" class="ltx_text ltx_font_bold">Value</span></td>
</tr>
<tr id="S4.T2.2.4.2" class="ltx_tr">
<th id="S4.T2.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Epochs</th>
<td id="S4.T2.2.4.2.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_t">20</td>
</tr>
<tr id="S4.T2.2.5.3" class="ltx_tr">
<th id="S4.T2.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Encoder layers</th>
<td id="S4.T2.2.5.3.2" class="ltx_td ltx_nopad_r ltx_align_left">6</td>
</tr>
<tr id="S4.T2.2.6.4" class="ltx_tr">
<th id="S4.T2.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Encoder attention heads</th>
<td id="S4.T2.2.6.4.2" class="ltx_td ltx_nopad_r ltx_align_left">6</td>
</tr>
<tr id="S4.T2.2.7.5" class="ltx_tr">
<th id="S4.T2.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Batch size</th>
<td id="S4.T2.2.7.5.2" class="ltx_td ltx_nopad_r ltx_align_left">65</td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AdamW <math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mi id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\epsilon</annotation></semantics></math>
</th>
<td id="S4.T2.1.1.2" class="ltx_td ltx_nopad_r ltx_align_left">1e-8</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<th id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AdamW <math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mi id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\beta</annotation></semantics></math>
</th>
<td id="S4.T2.2.2.2" class="ltx_td ltx_nopad_r ltx_align_left">(0.9, 0.999)</td>
</tr>
<tr id="S4.T2.2.8.6" class="ltx_tr">
<th id="S4.T2.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Weight decay</th>
<td id="S4.T2.2.8.6.2" class="ltx_td ltx_nopad_r ltx_align_left">0.01</td>
</tr>
<tr id="S4.T2.2.9.7" class="ltx_tr">
<th id="S4.T2.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate</th>
<td id="S4.T2.2.9.7.2" class="ltx_td ltx_nopad_r ltx_align_left">3e-5</td>
</tr>
<tr id="S4.T2.2.10.8" class="ltx_tr">
<th id="S4.T2.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Learning rate scheduler type</th>
<td id="S4.T2.2.10.8.2" class="ltx_td ltx_nopad_r ltx_align_left">Cosine</td>
</tr>
<tr id="S4.T2.2.11.9" class="ltx_tr">
<th id="S4.T2.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Warmup ratio</th>
<td id="S4.T2.2.11.9.2" class="ltx_td ltx_nopad_r ltx_align_left">0.1</td>
</tr>
<tr id="S4.T2.2.12.10" class="ltx_tr">
<th id="S4.T2.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Drop path</th>
<td id="S4.T2.2.12.10.2" class="ltx_td ltx_nopad_r ltx_align_left">0.3</td>
</tr>
<tr id="S4.T2.2.13.11" class="ltx_tr">
<th id="S4.T2.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Dropout</th>
<td id="S4.T2.2.13.11.2" class="ltx_td ltx_nopad_r ltx_align_left ltx_border_bb">✗</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.4.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.5.2" class="ltx_text" style="font-size:90%;">Hyperparameters for training process.</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Remarkably, within the multi-modal fusion module, a crucial hyperparameter during training is the drop path rate, also known as stochastic depth <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. The role of drop path is akin to dropout <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, a regularization technique employed to counteract overfitting in neural networks. Both methods introduce randomness during training, yet they operate at different levels of network architecture. Drop path randomly omits individual samples within a batch during training, thereby impacting the depth of the network’s computational graph. In contrast, dropout randomly eliminates units within layers, thereby altering the connectivity between layers. Drop path selectively bypasses specific connections within the neural network for each sample independently, while dropout affects all units within a layer. By randomly omitting paths (connections) during training, drop path encourages the network to acquire more robust features by discouraging reliance on specific paths. This diversification of learned features aids in mitigating overfitting by facilitating a broader exploration of the network’s parameter space.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Training Strategy</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In our paper, we adopted a systematic approach to training our model to ensure optimal performance. Initially, we partitioned our dataset into a training set and a validation set in an 80:20 split. This division allowed us to fine-tune and determine the most effective parameters for our model, with a particular emphasis on identifying the most suitable hyperparameters, including the drop path rate, the number of encoder layers, and the count of encoder attention heads. For the rest of the hyperparameters, we closely followed the training settings used for fine-tuning the BEiT-3 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> on the VQAv2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">To further refine our training process and ascertain the ideal number of training epochs necessary for model convergence, we employed a 5-fold cross-validation strategy on the training set. This approach enabled us to determine that our model reached convergence after an average of 20 epochs across all folds. Armed with this insight, we proceeded to train our model on the complete training set for the identified duration of 20 epochs.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">After selecting the optimal hyperparameters, we tested each model ten times using ten different random seeds. This approach allowed us to determine the statistical significance between any two models using a two-sample t-test.</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Experimental Results</h3>

<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Impact of Fusion Operation Types on the model</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In the initial phase of our study, we conducted an exploration to identify the optimal fusion function <math id="S4.SS4.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S4.SS4.SSS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS1.p1.1.m1.1.1" xref="S4.SS4.SSS1.p1.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p1.1.m1.1b"><ci id="S4.SS4.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p1.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p1.1.m1.1c">\mathcal{F}</annotation></semantics></math>, as defined in equation (<a href="#S3.E8" title="In 3.1.3 Image Embedding ‣ 3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>), for our model. This investigation encompassed various strategies, including element-wise multiplication, element-wise addition, and concatenation, applied to integrate both global and local image features. Despite the simplicity of the fusion strategies employed, they demonstrated noteworthy effectiveness in enhancing the overall performance of our model.</p>
</div>
<div id="S4.SS4.SSS1.p2" class="ltx_para">
<p id="S4.SS4.SSS1.p2.3" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.4.1 Impact of Fusion Operation Types on the model ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the outcomes of our experimentation. For the element-wise operations of multiplication and addition, we have features of visual <math id="S4.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="\in\mathbb{R}^{32\times 768}" display="inline"><semantics id="S4.SS4.SSS1.p2.1.m1.1a"><mrow id="S4.SS4.SSS1.p2.1.m1.1.1" xref="S4.SS4.SSS1.p2.1.m1.1.1.cmml"><mi id="S4.SS4.SSS1.p2.1.m1.1.1.2" xref="S4.SS4.SSS1.p2.1.m1.1.1.2.cmml"></mi><mo id="S4.SS4.SSS1.p2.1.m1.1.1.1" xref="S4.SS4.SSS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS4.SSS1.p2.1.m1.1.1.3" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.cmml"><mi id="S4.SS4.SSS1.p2.1.m1.1.1.3.2" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS4.SSS1.p2.1.m1.1.1.3.3" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.cmml"><mn id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.2" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.1" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.3" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.1.m1.1b"><apply id="S4.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1"><in id="S4.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.1"></in><csymbol cd="latexml" id="S4.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.2">absent</csymbol><apply id="S4.SS4.SSS1.p2.1.m1.1.1.3.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p2.1.m1.1.1.3.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS4.SSS1.p2.1.m1.1.1.3.2.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3"><times id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.1.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.1"></times><cn type="integer" id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.2.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.2">32</cn><cn type="integer" id="S4.SS4.SSS1.p2.1.m1.1.1.3.3.3.cmml" xref="S4.SS4.SSS1.p2.1.m1.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.1.m1.1c">\in\mathbb{R}^{32\times 768}</annotation></semantics></math>, and for concatenation, we have features in <math id="S4.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbb{R}^{64\times 768}" display="inline"><semantics id="S4.SS4.SSS1.p2.2.m2.1a"><msup id="S4.SS4.SSS1.p2.2.m2.1.1" xref="S4.SS4.SSS1.p2.2.m2.1.1.cmml"><mi id="S4.SS4.SSS1.p2.2.m2.1.1.2" xref="S4.SS4.SSS1.p2.2.m2.1.1.2.cmml">ℝ</mi><mrow id="S4.SS4.SSS1.p2.2.m2.1.1.3" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.cmml"><mn id="S4.SS4.SSS1.p2.2.m2.1.1.3.2" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.2.cmml">64</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS1.p2.2.m2.1.1.3.1" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.1.cmml">×</mo><mn id="S4.SS4.SSS1.p2.2.m2.1.1.3.3" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.3.cmml">768</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.2.m2.1b"><apply id="S4.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p2.2.m2.1.1.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1">superscript</csymbol><ci id="S4.SS4.SSS1.p2.2.m2.1.1.2.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.2">ℝ</ci><apply id="S4.SS4.SSS1.p2.2.m2.1.1.3.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.3"><times id="S4.SS4.SSS1.p2.2.m2.1.1.3.1.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.1"></times><cn type="integer" id="S4.SS4.SSS1.p2.2.m2.1.1.3.2.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.2">64</cn><cn type="integer" id="S4.SS4.SSS1.p2.2.m2.1.1.3.3.cmml" xref="S4.SS4.SSS1.p2.2.m2.1.1.3.3">768</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.2.m2.1c">\mathbb{R}^{64\times 768}</annotation></semantics></math>. It is evident that the concatenation operation outperforms the other operations, achieving the highest level of performance. Consequently, we have chosen concatenation as the fusion function <math id="S4.SS4.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S4.SS4.SSS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS1.p2.3.m3.1.1" xref="S4.SS4.SSS1.p2.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p2.3.m3.1b"><ci id="S4.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p2.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p2.3.m3.1c">\mathcal{F}</annotation></semantics></math> for subsequent experiments.</p>
</div>
<div id="S4.SS4.SSS1.p3" class="ltx_para">
<p id="S4.SS4.SSS1.p3.3" class="ltx_p">It is noteworthy that the outcome of the multiplication operation is markedly lower compared to concatenation and addition. Our investigation revealed that the diminished performance can be attributed to the multiplication of two features (local <math id="S4.SS4.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\in\mathbb{R}^{32\times 768}" display="inline"><semantics id="S4.SS4.SSS1.p3.1.m1.1a"><mrow id="S4.SS4.SSS1.p3.1.m1.1.1" xref="S4.SS4.SSS1.p3.1.m1.1.1.cmml"><mi id="S4.SS4.SSS1.p3.1.m1.1.1.2" xref="S4.SS4.SSS1.p3.1.m1.1.1.2.cmml"></mi><mo id="S4.SS4.SSS1.p3.1.m1.1.1.1" xref="S4.SS4.SSS1.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S4.SS4.SSS1.p3.1.m1.1.1.3" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S4.SS4.SSS1.p3.1.m1.1.1.3.2" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS4.SSS1.p3.1.m1.1.1.3.3" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.cmml"><mn id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.2" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.1" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.3" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p3.1.m1.1b"><apply id="S4.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1"><in id="S4.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.1"></in><csymbol cd="latexml" id="S4.SS4.SSS1.p3.1.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.2">absent</csymbol><apply id="S4.SS4.SSS1.p3.1.m1.1.1.3.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S4.SS4.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3"><times id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.1.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.1"></times><cn type="integer" id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.2.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.2">32</cn><cn type="integer" id="S4.SS4.SSS1.p3.1.m1.1.1.3.3.3.cmml" xref="S4.SS4.SSS1.p3.1.m1.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p3.1.m1.1c">\in\mathbb{R}^{32\times 768}</annotation></semantics></math> and global <math id="S4.SS4.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\in\mathbb{R}^{32\times 768}" display="inline"><semantics id="S4.SS4.SSS1.p3.2.m2.1a"><mrow id="S4.SS4.SSS1.p3.2.m2.1.1" xref="S4.SS4.SSS1.p3.2.m2.1.1.cmml"><mi id="S4.SS4.SSS1.p3.2.m2.1.1.2" xref="S4.SS4.SSS1.p3.2.m2.1.1.2.cmml"></mi><mo id="S4.SS4.SSS1.p3.2.m2.1.1.1" xref="S4.SS4.SSS1.p3.2.m2.1.1.1.cmml">∈</mo><msup id="S4.SS4.SSS1.p3.2.m2.1.1.3" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.cmml"><mi id="S4.SS4.SSS1.p3.2.m2.1.1.3.2" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS4.SSS1.p3.2.m2.1.1.3.3" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.cmml"><mn id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.2" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.1" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.3" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p3.2.m2.1b"><apply id="S4.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1"><in id="S4.SS4.SSS1.p3.2.m2.1.1.1.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.1"></in><csymbol cd="latexml" id="S4.SS4.SSS1.p3.2.m2.1.1.2.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.2">absent</csymbol><apply id="S4.SS4.SSS1.p3.2.m2.1.1.3.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p3.2.m2.1.1.3.1.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS4.SSS1.p3.2.m2.1.1.3.2.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.2">ℝ</ci><apply id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3"><times id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.1.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.1"></times><cn type="integer" id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.2.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.2">32</cn><cn type="integer" id="S4.SS4.SSS1.p3.2.m2.1.1.3.3.3.cmml" xref="S4.SS4.SSS1.p3.2.m2.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p3.2.m2.1c">\in\mathbb{R}^{32\times 768}</annotation></semantics></math>) resulting in a new matrix with dimensions <math id="S4.SS4.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\in\mathbb{R}^{32\times 768}" display="inline"><semantics id="S4.SS4.SSS1.p3.3.m3.1a"><mrow id="S4.SS4.SSS1.p3.3.m3.1.1" xref="S4.SS4.SSS1.p3.3.m3.1.1.cmml"><mi id="S4.SS4.SSS1.p3.3.m3.1.1.2" xref="S4.SS4.SSS1.p3.3.m3.1.1.2.cmml"></mi><mo id="S4.SS4.SSS1.p3.3.m3.1.1.1" xref="S4.SS4.SSS1.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S4.SS4.SSS1.p3.3.m3.1.1.3" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.cmml"><mi id="S4.SS4.SSS1.p3.3.m3.1.1.3.2" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS4.SSS1.p3.3.m3.1.1.3.3" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.cmml"><mn id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.2" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.2.cmml">32</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.1" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.1.cmml">×</mo><mn id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.3" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.3.cmml">768</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p3.3.m3.1b"><apply id="S4.SS4.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1"><in id="S4.SS4.SSS1.p3.3.m3.1.1.1.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.1"></in><csymbol cd="latexml" id="S4.SS4.SSS1.p3.3.m3.1.1.2.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.2">absent</csymbol><apply id="S4.SS4.SSS1.p3.3.m3.1.1.3.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p3.3.m3.1.1.3.1.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3">superscript</csymbol><ci id="S4.SS4.SSS1.p3.3.m3.1.1.3.2.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.2">ℝ</ci><apply id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3"><times id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.1.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.1"></times><cn type="integer" id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.2.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.2">32</cn><cn type="integer" id="S4.SS4.SSS1.p3.3.m3.1.1.3.3.3.cmml" xref="S4.SS4.SSS1.p3.3.m3.1.1.3.3.3">768</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p3.3.m3.1c">\in\mathbb{R}^{32\times 768}</annotation></semantics></math> that exhibits a high degree of sparsity. The boxplots in Figure <a href="#S4.F5" title="Figure 5 ‣ 4.4.1 Impact of Fusion Operation Types on the model ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> indicate that, on average, the elements in the matrices after addition and concatenation operations demonstrate elevated levels and substantial spreads. In contrast, the multiplication operation yields a sparse matrix characterized by elements that tend to approach zero closely, displaying a narrow range of spreads. This pattern suggests that the sparse matrix resulting from multiplication leads to information loss in the visual representation, thereby accounting for its diminished performance.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.3" class="ltx_tr">
<th id="S4.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T3.1.1.1.1" class="ltx_text ltx_font_bold">Operation (<math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\mathcal{F}</annotation></semantics></math>)</span></th>
<th id="S4.T3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T3.2.2.2.1" class="ltx_text ltx_font_bold">Accuracy (<math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mo id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><csymbol cd="latexml" id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T3.3.3.3.m1.1.1" xref="S4.T3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.4.1" class="ltx_tr">
<th id="S4.T3.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Element-wise Multiplication</th>
<td id="S4.T3.3.4.1.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">40.15</td>
</tr>
<tr id="S4.T3.3.5.2" class="ltx_tr">
<th id="S4.T3.3.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Element-wise Addition</th>
<td id="S4.T3.3.5.2.2" class="ltx_td ltx_nopad_r ltx_align_right">69.78</td>
</tr>
<tr id="S4.T3.3.6.3" class="ltx_tr">
<th id="S4.T3.3.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T3.3.6.3.1.1" class="ltx_text ltx_font_bold">Concatenation</span></th>
<td id="S4.T3.3.6.3.2" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb"><span id="S4.T3.3.6.3.2.1" class="ltx_text ltx_font_bold">71.04</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.7.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.5.1" class="ltx_text" style="font-size:90%;">The effects of fusion operation. <math id="S4.T3.5.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T3.5.1.m1.1b"><mo stretchy="false" id="S4.T3.5.1.m1.1.1" xref="S4.T3.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T3.5.1.m1.1c"><ci id="S4.T3.5.1.m1.1.1.cmml" xref="S4.T3.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.1.m1.1d">\uparrow</annotation></semantics></math> indicates that higher values are better.</span></figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/boxplot.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="340" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Boxplots illustrate the spread of mean values within the visual features for each operation.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Impact of CNNs on the model</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">In the subsequent phase, we conducted a meticulous investigation into the influence of various CNN architectures on the efficacy of our model. The choice of a CNN architecture plays a pivotal role in shaping our model’s performance. We comprehensively evaluated diverse CNN models, including ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> and VGGNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, alongside the EfficientNet family architecture. This assessment was rooted in accuracy metrics and an examination of the parameter count for each model.</p>
</div>
<div id="S4.SS4.SSS2.p2" class="ltx_para">
<p id="S4.SS4.SSS2.p2.1" class="ltx_p">Table <a href="#S4.T4" title="Table 4 ‣ 4.4.2 Impact of CNNs on the model ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents a synthesis of accuracy and parameter count data for each CNN model. The findings reveal that the EfficientNet family not only surpasses other models in terms of accuracy but also, with a moderate parameter count, exhibits superior performance compared to the conventional ResNet and VGG architectures. As observed, VGG achieves an accuracy of over 70<math id="S4.SS4.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS4.SSS2.p2.1.m1.1a"><mo id="S4.SS4.SSS2.p2.1.m1.1.1" xref="S4.SS4.SSS2.p2.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS2.p2.1.m1.1b"><csymbol cd="latexml" id="S4.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS4.SSS2.p2.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS2.p2.1.m1.1c">\%</annotation></semantics></math>, but it requires hundreds of millions of parameters. In contrast, EfficientNet achieves comparable accuracy with only tens of millions of parameters. We selected the large-scale model (EfficientNet-B7) as the optimal choice, offering elevated accuracy with reasonable model complexity for our model.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T4.2.2" class="ltx_tr">
<th id="S4.T4.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T4.2.2.3.1" class="ltx_text ltx_font_bold">CNN Models</span></th>
<th id="S4.T4.2.2.2" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T4.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy (<math id="S4.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T4.1.1.1.1.m1.1a"><mo id="S4.T4.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T4.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T4.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.2.2.2.m1.1a"><mo stretchy="false" id="S4.T4.2.2.2.m1.1.1" xref="S4.T4.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.m1.1b"><ci id="S4.T4.2.2.2.m1.1.1.cmml" xref="S4.T4.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T4.2.2.4" class="ltx_td ltx_nopad_r ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S4.T4.2.2.4.1" class="ltx_text ltx_font_bold">Params</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T4.2.3.1" class="ltx_tr">
<th id="S4.T4.2.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Resnet-18</th>
<td id="S4.T4.2.3.1.2" class="ltx_td ltx_align_right ltx_border_t">68.74</td>
<td id="S4.T4.2.3.1.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t">11.7 M</td>
</tr>
<tr id="S4.T4.2.4.2" class="ltx_tr">
<th id="S4.T4.2.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Resnet-34</th>
<td id="S4.T4.2.4.2.2" class="ltx_td ltx_align_right">68.61</td>
<td id="S4.T4.2.4.2.3" class="ltx_td ltx_nopad_r ltx_align_right">21.8 M</td>
</tr>
<tr id="S4.T4.2.5.3" class="ltx_tr">
<th id="S4.T4.2.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Resnet-50</th>
<td id="S4.T4.2.5.3.2" class="ltx_td ltx_align_right">69.44</td>
<td id="S4.T4.2.5.3.3" class="ltx_td ltx_nopad_r ltx_align_right">25.6 M</td>
</tr>
<tr id="S4.T4.2.6.4" class="ltx_tr">
<th id="S4.T4.2.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Resnet-101</th>
<td id="S4.T4.2.6.4.2" class="ltx_td ltx_align_right">69.11</td>
<td id="S4.T4.2.6.4.3" class="ltx_td ltx_nopad_r ltx_align_right">44.5 M</td>
</tr>
<tr id="S4.T4.2.7.5" class="ltx_tr">
<th id="S4.T4.2.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Resnet-152</th>
<td id="S4.T4.2.7.5.2" class="ltx_td ltx_align_right">69.94</td>
<td id="S4.T4.2.7.5.3" class="ltx_td ltx_nopad_r ltx_align_right">60.2 M</td>
</tr>
<tr id="S4.T4.2.8.6" class="ltx_tr">
<th id="S4.T4.2.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VGG-11</th>
<td id="S4.T4.2.8.6.2" class="ltx_td ltx_align_right">70.48</td>
<td id="S4.T4.2.8.6.3" class="ltx_td ltx_nopad_r ltx_align_right">132.9 M</td>
</tr>
<tr id="S4.T4.2.9.7" class="ltx_tr">
<th id="S4.T4.2.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VGG-13</th>
<td id="S4.T4.2.9.7.2" class="ltx_td ltx_align_right">70.44</td>
<td id="S4.T4.2.9.7.3" class="ltx_td ltx_nopad_r ltx_align_right">133.0 M</td>
</tr>
<tr id="S4.T4.2.10.8" class="ltx_tr">
<th id="S4.T4.2.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VGG-16</th>
<td id="S4.T4.2.10.8.2" class="ltx_td ltx_align_right">69.37</td>
<td id="S4.T4.2.10.8.3" class="ltx_td ltx_nopad_r ltx_align_right">138.4 M</td>
</tr>
<tr id="S4.T4.2.11.9" class="ltx_tr">
<th id="S4.T4.2.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">VGG-19</th>
<td id="S4.T4.2.11.9.2" class="ltx_td ltx_align_right">69.81</td>
<td id="S4.T4.2.11.9.3" class="ltx_td ltx_nopad_r ltx_align_right">143.7 M</td>
</tr>
<tr id="S4.T4.2.12.10" class="ltx_tr">
<th id="S4.T4.2.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B0</th>
<td id="S4.T4.2.12.10.2" class="ltx_td ltx_align_right">69.61</td>
<td id="S4.T4.2.12.10.3" class="ltx_td ltx_nopad_r ltx_align_right">5.3 M</td>
</tr>
<tr id="S4.T4.2.13.11" class="ltx_tr">
<th id="S4.T4.2.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B1</th>
<td id="S4.T4.2.13.11.2" class="ltx_td ltx_align_right">68.78</td>
<td id="S4.T4.2.13.11.3" class="ltx_td ltx_nopad_r ltx_align_right">7.8 M</td>
</tr>
<tr id="S4.T4.2.14.12" class="ltx_tr">
<th id="S4.T4.2.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B2</th>
<td id="S4.T4.2.14.12.2" class="ltx_td ltx_align_right">69.38</td>
<td id="S4.T4.2.14.12.3" class="ltx_td ltx_nopad_r ltx_align_right">9.1 M</td>
</tr>
<tr id="S4.T4.2.15.13" class="ltx_tr">
<th id="S4.T4.2.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B3</th>
<td id="S4.T4.2.15.13.2" class="ltx_td ltx_align_right">69.71</td>
<td id="S4.T4.2.15.13.3" class="ltx_td ltx_nopad_r ltx_align_right">12.2 M</td>
</tr>
<tr id="S4.T4.2.16.14" class="ltx_tr">
<th id="S4.T4.2.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B4</th>
<td id="S4.T4.2.16.14.2" class="ltx_td ltx_align_right">69.78</td>
<td id="S4.T4.2.16.14.3" class="ltx_td ltx_nopad_r ltx_align_right">19.3 M</td>
</tr>
<tr id="S4.T4.2.17.15" class="ltx_tr">
<th id="S4.T4.2.17.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B5</th>
<td id="S4.T4.2.17.15.2" class="ltx_td ltx_align_right">70.64</td>
<td id="S4.T4.2.17.15.3" class="ltx_td ltx_nopad_r ltx_align_right">30.4 M</td>
</tr>
<tr id="S4.T4.2.18.16" class="ltx_tr">
<th id="S4.T4.2.18.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Efficientnet-B6</th>
<td id="S4.T4.2.18.16.2" class="ltx_td ltx_align_right">70.24</td>
<td id="S4.T4.2.18.16.3" class="ltx_td ltx_nopad_r ltx_align_right">43.0 M</td>
</tr>
<tr id="S4.T4.2.19.17" class="ltx_tr">
<th id="S4.T4.2.19.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T4.2.19.17.1.1" class="ltx_text ltx_font_bold">Efficientnet-B7</span></th>
<td id="S4.T4.2.19.17.2" class="ltx_td ltx_align_right ltx_border_bb"><span id="S4.T4.2.19.17.2.1" class="ltx_text ltx_font_bold">71.04</span></td>
<td id="S4.T4.2.19.17.3" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb"><span id="S4.T4.2.19.17.3.1" class="ltx_text ltx_font_bold">66.3 M</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.6.2.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.4.1" class="ltx_text" style="font-size:90%;">Influence of CNN structures on model performance. <math id="S4.T4.4.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T4.4.1.m1.1b"><mo stretchy="false" id="S4.T4.4.1.m1.1.1" xref="S4.T4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T4.4.1.m1.1c"><ci id="S4.T4.4.1.m1.1.1.cmml" xref="S4.T4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.4.1.m1.1d">\uparrow</annotation></semantics></math> indicates that higher values are better.</span></figcaption>
</figure>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Comparison with other Methods</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Due to the limited research on VQA tasks in Vietnamese, we benchmarked our model against several existing approaches that have demonstrated promising results in this domain. Specifically, we compared our model to strategies proposed by other methods. LSTM-based approaches <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and pre-trained word embeddings (PhoW2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>) proved ineffective in handling the complexities of VQA. Conversely, methods based on transformers demonstrated excellent performance in addressing this challenging task.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<div id="S4.T5.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:95pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-114.1pt,24.8pt) scale(0.655182333981759,0.655182333981759) ;">
<table id="S4.T5.10.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T5.10.10.10" class="ltx_tr">
<th id="S4.T5.10.10.10.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T5.10.10.10.11.1" class="ltx_text ltx_font_bold">Models</span></th>
<th id="S4.T5.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T5.1.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy (<math id="S4.T5.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T5.1.1.1.1.1.m1.1a"><mo id="S4.T5.1.1.1.1.1.m1.1.1" xref="S4.T5.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T5.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T5.1.1.1.1.1.m1.1.1.cmml" xref="S4.T5.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.1.1.1.1.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T5.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T5.2.2.2.2.m1.1.1" xref="S4.T5.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.2.2.2.2.m1.1b"><ci id="S4.T5.2.2.2.2.m1.1.1.cmml" xref="S4.T5.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T5.4.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T5.4.4.4.4.1" class="ltx_text ltx_font_bold">Improvement</span> <math id="S4.T5.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T5.3.3.3.3.m1.1.1" xref="S4.T5.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.3.3.3.3.m1.1b"><ci id="S4.T5.3.3.3.3.m1.1.1.cmml" xref="S4.T5.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math> (<math id="S4.T5.4.4.4.4.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T5.4.4.4.4.m2.1a"><mo id="S4.T5.4.4.4.4.m2.1.1" xref="S4.T5.4.4.4.4.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T5.4.4.4.4.m2.1b"><csymbol cd="latexml" id="S4.T5.4.4.4.4.m2.1.1.cmml" xref="S4.T5.4.4.4.4.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.4.4.4.4.m2.1c">\%</annotation></semantics></math>)</th>
<th id="S4.T5.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.5.5.5.5.1" class="ltx_text ltx_font_bold">Precision (<math id="S4.T5.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T5.5.5.5.5.1.m1.1a"><mo id="S4.T5.5.5.5.5.1.m1.1.1" xref="S4.T5.5.5.5.5.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T5.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T5.5.5.5.5.1.m1.1.1.cmml" xref="S4.T5.5.5.5.5.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.5.5.5.5.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T5.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T5.6.6.6.6.m1.1.1" xref="S4.T5.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.6.6.6.6.m1.1b"><ci id="S4.T5.6.6.6.6.m1.1.1.cmml" xref="S4.T5.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T5.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.7.7.7.7.1" class="ltx_text ltx_font_bold">Recall (<math id="S4.T5.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T5.7.7.7.7.1.m1.1a"><mo id="S4.T5.7.7.7.7.1.m1.1.1" xref="S4.T5.7.7.7.7.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T5.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T5.7.7.7.7.1.m1.1.1.cmml" xref="S4.T5.7.7.7.7.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.7.7.7.7.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T5.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.8.8.8.8.m1.1a"><mo stretchy="false" id="S4.T5.8.8.8.8.m1.1.1" xref="S4.T5.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.8.8.8.8.m1.1b"><ci id="S4.T5.8.8.8.8.m1.1.1.cmml" xref="S4.T5.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T5.10.10.10.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T5.9.9.9.9.1" class="ltx_text ltx_font_bold">F1-score (<math id="S4.T5.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T5.9.9.9.9.1.m1.1a"><mo id="S4.T5.9.9.9.9.1.m1.1.1" xref="S4.T5.9.9.9.9.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T5.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T5.9.9.9.9.1.m1.1.1.cmml" xref="S4.T5.9.9.9.9.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.9.9.9.9.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T5.10.10.10.10.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.10.10.10.10.m1.1a"><mo stretchy="false" id="S4.T5.10.10.10.10.m1.1.1" xref="S4.T5.10.10.10.10.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.10.10.10.10.m1.1b"><ci id="S4.T5.10.10.10.10.m1.1.1.cmml" xref="S4.T5.10.10.10.10.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.10.10.10.10.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T5.10.10.11.1" class="ltx_tr">
<th id="S4.T5.10.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LTSM + PhoW2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T5.10.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">33.85</th>
<th id="S4.T5.10.10.11.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">37.19 (*)</th>
<td id="S4.T5.10.10.11.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.10.10.11.1.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S4.T5.10.10.11.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S4.T5.10.10.12.2" class="ltx_tr">
<th id="S4.T5.10.10.12.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Bi-LTSM + PhoW2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T5.10.10.12.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">33.97</th>
<th id="S4.T5.10.10.12.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">37.07 (*)</th>
<td id="S4.T5.10.10.12.2.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.12.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.12.2.6" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T5.10.10.13.3" class="ltx_tr">
<th id="S4.T5.10.10.13.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">HieCoAtt + PhoW2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</th>
<th id="S4.T5.10.10.13.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">34.96</th>
<th id="S4.T5.10.10.13.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">36.08 (*)</th>
<td id="S4.T5.10.10.13.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.13.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.13.3.6" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T5.10.10.14.4" class="ltx_tr">
<th id="S4.T5.10.10.14.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">PAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</th>
<th id="S4.T5.10.10.14.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">60.55</th>
<th id="S4.T5.10.10.14.4.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">10.49 (*)</th>
<td id="S4.T5.10.10.14.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.14.4.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.14.4.6" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T5.10.10.15.5" class="ltx_tr">
<th id="S4.T5.10.10.15.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>
</th>
<th id="S4.T5.10.10.15.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">60.76</th>
<th id="S4.T5.10.10.15.5.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">10.28 (*)</th>
<td id="S4.T5.10.10.15.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.15.5.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T5.10.10.15.5.6" class="ltx_td ltx_nopad_r ltx_align_center">-</td>
</tr>
<tr id="S4.T5.10.10.16.6" class="ltx_tr">
<th id="S4.T5.10.10.16.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BARTPhoBEiT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<th id="S4.T5.10.10.16.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">68.58</th>
<th id="S4.T5.10.10.16.6.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">2.46 (*)</th>
<td id="S4.T5.10.10.16.6.4" class="ltx_td ltx_align_center">69.31</td>
<td id="S4.T5.10.10.16.6.5" class="ltx_td ltx_align_center">68.58</td>
<td id="S4.T5.10.10.16.6.6" class="ltx_td ltx_nopad_r ltx_align_center">67.77</td>
</tr>
<tr id="S4.T5.10.10.17.7" class="ltx_tr">
<th id="S4.T5.10.10.17.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.10.10.17.7.1.1" class="ltx_text ltx_font_bold">Our model</span></th>
<th id="S4.T5.10.10.17.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T5.10.10.17.7.2.1" class="ltx_text ltx_font_bold">71.04</span></th>
<th id="S4.T5.10.10.17.7.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">-</th>
<td id="S4.T5.10.10.17.7.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.10.10.17.7.4.1" class="ltx_text ltx_font_bold">76.34</span></td>
<td id="S4.T5.10.10.17.7.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T5.10.10.17.7.5.1" class="ltx_text ltx_font_bold">76.29</span></td>
<td id="S4.T5.10.10.17.7.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T5.10.10.17.7.6.1" class="ltx_text ltx_font_bold">76.16</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T5.16.3.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S4.T5.14.2" class="ltx_text" style="font-size:90%;">Our model versus former baseline methods. We computed the percentage improvement in accuracy of our model compared to other baselines. (*) indicates statistically significant (<math id="S4.T5.13.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S4.T5.13.1.m1.1b"><mrow id="S4.T5.13.1.m1.1.1" xref="S4.T5.13.1.m1.1.1.cmml"><mi id="S4.T5.13.1.m1.1.1.2" xref="S4.T5.13.1.m1.1.1.2.cmml">p</mi><mo id="S4.T5.13.1.m1.1.1.1" xref="S4.T5.13.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T5.13.1.m1.1.1.3" xref="S4.T5.13.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T5.13.1.m1.1c"><apply id="S4.T5.13.1.m1.1.1.cmml" xref="S4.T5.13.1.m1.1.1"><lt id="S4.T5.13.1.m1.1.1.1.cmml" xref="S4.T5.13.1.m1.1.1.1"></lt><ci id="S4.T5.13.1.m1.1.1.2.cmml" xref="S4.T5.13.1.m1.1.1.2">𝑝</ci><cn type="float" id="S4.T5.13.1.m1.1.1.3.cmml" xref="S4.T5.13.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.13.1.m1.1d">p&lt;0.05</annotation></semantics></math>) improvements. <math id="S4.T5.14.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T5.14.2.m2.1b"><mo stretchy="false" id="S4.T5.14.2.m2.1.1" xref="S4.T5.14.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T5.14.2.m2.1c"><ci id="S4.T5.14.2.m2.1.1.cmml" xref="S4.T5.14.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.14.2.m2.1d">\uparrow</annotation></semantics></math> indicates that higher values are better.</span></figcaption>
</figure>
<div id="S4.SS4.SSS3.p2" class="ltx_para">
<p id="S4.SS4.SSS3.p2.1" class="ltx_p">The comparative results are presented in Table <a href="#S4.T5" title="Table 5 ‣ 4.4.3 Comparison with other Methods ‣ 4.4 Experimental Results ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. Our model achieves a remarkable accuracy of 71.04% on the test set, surpassing all other methods by a significant margin. This represents a substantial improvement of 37.19% over the least accurate model (LSTM + PhoW2Vec) and a notable gain of 2.46% over the second-best approach (BARTPhoBEiT). These results unequivocally demonstrate the superiority of our proposed method in addressing ViVQA.</p>
</div>
<div id="S4.SS4.SSS3.p3" class="ltx_para">
<p id="S4.SS4.SSS3.p3.1" class="ltx_p">Overall, many previous baselines lack information about local features, with the exception of MCA. The effectiveness of incorporating both local and global features has proven to be crucial for the success of previous literature. We will conduct an ablation study in the next section to demonstrate their influence. Although MCA considers both local and global features in image extraction and achieves commendable accuracy, it utilizes ViT, which is trained solely on image datasets. In contrast, our method leverages BLIP-2, which is pre-trained on both vision and language data. This vision-language pre-training enables BLIP-2 to capture more comprehensive and contextually rich information than that obtained from ViT.</p>
</div>
<div id="S4.SS4.SSS3.p4" class="ltx_para">
<p id="S4.SS4.SSS3.p4.1" class="ltx_p">With promising results achieved on the ViVQA dataset, our model has demonstrated effectiveness in the VQA task, particularly in its ability to extract, represent, and comprehend the relationship between images and text. To ensure the model effectively understands Vietnamese, we propose leveraging not only a powerful pre-trained model like BARTPho but also integrating a multi-modal information synthesis module like BEiT-3 to create a joint representation containing information from both text and images. Additionally, our model not only relies on the existing knowledge encoded in the pre-trained BARTPho model for Vietnamese, but we also refine it to suit multi-modal tasks between Vietnamese images and texts, thereby establishing a foundation for similar multi-modal studies in Vietnamese.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">We conducted an ablation study to thoroughly investigate the contributions of BLIP-2 and EfficientNet to the overall outcome. As depicted in Table <a href="#S4.T6" title="Table 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, models relying solely on BLIP-2 or EfficientNet for extracting image features exhibited lower accuracy. Whilst combining both methods led to improved performance, surpassing the results achieved with a singular approach.</p>
</div>
<figure id="S4.T6" class="ltx_table">
<div id="S4.T6.10" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:60.4pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-45.1pt,6.2pt) scale(0.827956670326245,0.827956670326245) ;">
<table id="S4.T6.10.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T6.10.10.10" class="ltx_tr">
<th id="S4.T6.10.10.10.11" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S4.T6.10.10.10.11.1" class="ltx_text ltx_font_bold">Visual Extractor</span></th>
<th id="S4.T6.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T6.1.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy (<math id="S4.T6.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T6.1.1.1.1.1.m1.1a"><mo id="S4.T6.1.1.1.1.1.m1.1.1" xref="S4.T6.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T6.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T6.1.1.1.1.1.m1.1.1.cmml" xref="S4.T6.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.1.1.1.1.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T6.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T6.2.2.2.2.m1.1.1" xref="S4.T6.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.2.2.2.2.m1.1b"><ci id="S4.T6.2.2.2.2.m1.1.1.cmml" xref="S4.T6.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T6.4.4.4.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<span id="S4.T6.4.4.4.4.1" class="ltx_text ltx_font_bold">Improvement</span> <math id="S4.T6.3.3.3.3.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.3.3.3.3.m1.1a"><mo stretchy="false" id="S4.T6.3.3.3.3.m1.1.1" xref="S4.T6.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.3.3.3.3.m1.1b"><ci id="S4.T6.3.3.3.3.m1.1.1.cmml" xref="S4.T6.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.3.3.3.3.m1.1c">\uparrow</annotation></semantics></math> (<math id="S4.T6.4.4.4.4.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T6.4.4.4.4.m2.1a"><mo id="S4.T6.4.4.4.4.m2.1.1" xref="S4.T6.4.4.4.4.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T6.4.4.4.4.m2.1b"><csymbol cd="latexml" id="S4.T6.4.4.4.4.m2.1.1.cmml" xref="S4.T6.4.4.4.4.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.4.4.4.4.m2.1c">\%</annotation></semantics></math>)</th>
<th id="S4.T6.6.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T6.5.5.5.5.1" class="ltx_text ltx_font_bold">Precision (<math id="S4.T6.5.5.5.5.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T6.5.5.5.5.1.m1.1a"><mo id="S4.T6.5.5.5.5.1.m1.1.1" xref="S4.T6.5.5.5.5.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T6.5.5.5.5.1.m1.1b"><csymbol cd="latexml" id="S4.T6.5.5.5.5.1.m1.1.1.cmml" xref="S4.T6.5.5.5.5.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.5.5.5.5.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T6.6.6.6.6.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.6.6.6.6.m1.1a"><mo stretchy="false" id="S4.T6.6.6.6.6.m1.1.1" xref="S4.T6.6.6.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.6.6.6.6.m1.1b"><ci id="S4.T6.6.6.6.6.m1.1.1.cmml" xref="S4.T6.6.6.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.6.6.6.6.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T6.8.8.8.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T6.7.7.7.7.1" class="ltx_text ltx_font_bold">Recall (<math id="S4.T6.7.7.7.7.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T6.7.7.7.7.1.m1.1a"><mo id="S4.T6.7.7.7.7.1.m1.1.1" xref="S4.T6.7.7.7.7.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T6.7.7.7.7.1.m1.1b"><csymbol cd="latexml" id="S4.T6.7.7.7.7.1.m1.1.1.cmml" xref="S4.T6.7.7.7.7.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.7.7.7.7.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T6.8.8.8.8.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.8.8.8.8.m1.1a"><mo stretchy="false" id="S4.T6.8.8.8.8.m1.1.1" xref="S4.T6.8.8.8.8.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.8.8.8.8.m1.1b"><ci id="S4.T6.8.8.8.8.m1.1.1.cmml" xref="S4.T6.8.8.8.8.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.8.8.8.8.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T6.10.10.10.10" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T6.9.9.9.9.1" class="ltx_text ltx_font_bold">F1-score (<math id="S4.T6.9.9.9.9.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T6.9.9.9.9.1.m1.1a"><mo id="S4.T6.9.9.9.9.1.m1.1.1" xref="S4.T6.9.9.9.9.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T6.9.9.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T6.9.9.9.9.1.m1.1.1.cmml" xref="S4.T6.9.9.9.9.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.9.9.9.9.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T6.10.10.10.10.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.10.10.10.10.m1.1a"><mo stretchy="false" id="S4.T6.10.10.10.10.m1.1.1" xref="S4.T6.10.10.10.10.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.10.10.10.10.m1.1b"><ci id="S4.T6.10.10.10.10.m1.1.1.cmml" xref="S4.T6.10.10.10.10.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.10.10.10.10.m1.1c">\uparrow</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T6.10.10.11.1" class="ltx_tr">
<th id="S4.T6.10.10.11.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">EfficientNet-B7</th>
<th id="S4.T6.10.10.11.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">53.95</th>
<th id="S4.T6.10.10.11.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">17.09 (*)</th>
<td id="S4.T6.10.10.11.1.4" class="ltx_td ltx_align_center ltx_border_t">61.92</td>
<td id="S4.T6.10.10.11.1.5" class="ltx_td ltx_align_center ltx_border_t">61.83</td>
<td id="S4.T6.10.10.11.1.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">61.67</td>
</tr>
<tr id="S4.T6.10.10.12.2" class="ltx_tr">
<th id="S4.T6.10.10.12.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BLIP-2</th>
<th id="S4.T6.10.10.12.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">69.61</th>
<th id="S4.T6.10.10.12.2.3" class="ltx_td ltx_align_right ltx_th ltx_th_row">1.43 (*)</th>
<td id="S4.T6.10.10.12.2.4" class="ltx_td ltx_align_center">74.82</td>
<td id="S4.T6.10.10.12.2.5" class="ltx_td ltx_align_center">74.83</td>
<td id="S4.T6.10.10.12.2.6" class="ltx_td ltx_nopad_r ltx_align_center">74.66</td>
</tr>
<tr id="S4.T6.10.10.13.3" class="ltx_tr">
<th id="S4.T6.10.10.13.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T6.10.10.13.3.1.1" class="ltx_text ltx_font_bold">BLIP-2 + EfficientNet-B7</span></th>
<th id="S4.T6.10.10.13.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb"><span id="S4.T6.10.10.13.3.2.1" class="ltx_text ltx_font_bold">71.04</span></th>
<th id="S4.T6.10.10.13.3.3" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb">-</th>
<td id="S4.T6.10.10.13.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.10.10.13.3.4.1" class="ltx_text ltx_font_bold">76.34</span></td>
<td id="S4.T6.10.10.13.3.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T6.10.10.13.3.5.1" class="ltx_text ltx_font_bold">76.29</span></td>
<td id="S4.T6.10.10.13.3.6" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb"><span id="S4.T6.10.10.13.3.6.1" class="ltx_text ltx_font_bold">76.16</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T6.16.3.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S4.T6.14.2" class="ltx_text" style="font-size:90%;">Ablation study for visual extractor. We computed the percentage improvement in accuracy of the combination of two visual extractors compared to using only one (*) indicates statistically significant (<math id="S4.T6.13.1.m1.1" class="ltx_Math" alttext="p&lt;0.05" display="inline"><semantics id="S4.T6.13.1.m1.1b"><mrow id="S4.T6.13.1.m1.1.1" xref="S4.T6.13.1.m1.1.1.cmml"><mi id="S4.T6.13.1.m1.1.1.2" xref="S4.T6.13.1.m1.1.1.2.cmml">p</mi><mo id="S4.T6.13.1.m1.1.1.1" xref="S4.T6.13.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.T6.13.1.m1.1.1.3" xref="S4.T6.13.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T6.13.1.m1.1c"><apply id="S4.T6.13.1.m1.1.1.cmml" xref="S4.T6.13.1.m1.1.1"><lt id="S4.T6.13.1.m1.1.1.1.cmml" xref="S4.T6.13.1.m1.1.1.1"></lt><ci id="S4.T6.13.1.m1.1.1.2.cmml" xref="S4.T6.13.1.m1.1.1.2">𝑝</ci><cn type="float" id="S4.T6.13.1.m1.1.1.3.cmml" xref="S4.T6.13.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.13.1.m1.1d">p&lt;0.05</annotation></semantics></math>) improvements. <math id="S4.T6.14.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T6.14.2.m2.1b"><mo stretchy="false" id="S4.T6.14.2.m2.1.1" xref="S4.T6.14.2.m2.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T6.14.2.m2.1c"><ci id="S4.T6.14.2.m2.1.1.cmml" xref="S4.T6.14.2.m2.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T6.14.2.m2.1d">\uparrow</annotation></semantics></math> indicates that higher values are better.</span></figcaption>
</figure>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Furthermore, we systematically selected the number of encoder attention heads and encoder layers in the multi-modal fusion module. In our system, this module plays a pivotal role in the model’s performance for combining information from the visual and question modalities. Initially, we fixed the number of encoder layers at <math id="S4.SS5.p2.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS5.p2.1.m1.1a"><mn id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><cn type="integer" id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">6</annotation></semantics></math> and varied the number of encoder attention heads. The results, as illustrated in Figure <a href="#S4.F6" title="Figure 6 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, highlight the optimal performance of our model with 6 encoder attention heads.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/attn_heads.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Ablation study for encoder attention heads.</span></figcaption>
</figure>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">Subsequently, maintaining 6 encoder attention heads constant, we adjusted the number of encoder layers. The outcomes, showcased in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, underscore that the optimal number of layers for the encoder is 6.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/encoder_layers.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="359" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Ablation study for encoder layers.</span></figcaption>
</figure>
<div id="S4.SS5.p4" class="ltx_para">
<p id="S4.SS5.p4.2" class="ltx_p">After obtaining the ultimate components of our model, we investigated the impact of freezing the visual extractor on model performance. Due to limited infrastructure in the experimental environment, we only used EfficientNet-B7 from our image embedding module to conduct the experiment. The results, depicted in Table <a href="#S4.T7" title="Table 7 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, indicate that freezing the EfficientNet-B7 led to an accuracy of 71.04<math id="S4.SS5.p4.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS5.p4.1.m1.1a"><mo id="S4.SS5.p4.1.m1.1.1" xref="S4.SS5.p4.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS5.p4.1.m1.1.1.cmml" xref="S4.SS5.p4.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.1.m1.1c">\%</annotation></semantics></math> within 20 epochs, whereas the visual extractor is not frozen, the accuracy is slightly lower at 70.44<math id="S4.SS5.p4.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS5.p4.2.m2.1a"><mo id="S4.SS5.p4.2.m2.1.1" xref="S4.SS5.p4.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS5.p4.2.m2.1b"><csymbol cd="latexml" id="S4.SS5.p4.2.m2.1.1.cmml" xref="S4.SS5.p4.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p4.2.m2.1c">\%</annotation></semantics></math>, with a significantly longer training time of 935 seconds. From these findings, we conclude that freezing the visual extractor can enhance efficiency without compromising accuracy, as well as saving a lot of training time, rendering it a viable strategy for training deep learning models in similar contexts.</p>
</div>
<figure id="S4.T7" class="ltx_table">
<div id="S4.T7.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:68.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(45.0pt,-7.1pt) scale(1.26215207532304,1.26215207532304) ;">
<table id="S4.T7.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T7.3.3.3" class="ltx_tr">
<th id="S4.T7.3.3.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.3.3.3.4.1" class="ltx_text ltx_font_bold">Visual Extractor</span></th>
<th id="S4.T7.3.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.3.3.3.5.1" class="ltx_text ltx_font_bold">Freeze</span></th>
<th id="S4.T7.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S4.T7.1.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy (<math id="S4.T7.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.T7.1.1.1.1.1.m1.1a"><mo id="S4.T7.1.1.1.1.1.m1.1.1" xref="S4.T7.1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.T7.1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T7.1.1.1.1.1.m1.1.1.cmml" xref="S4.T7.1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.1.1.1.1.1.m1.1c">\%</annotation></semantics></math>)</span> <math id="S4.T7.2.2.2.2.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.2.2.2.2.m1.1a"><mo stretchy="false" id="S4.T7.2.2.2.2.m1.1.1" xref="S4.T7.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.2.2.2.2.m1.1b"><ci id="S4.T7.2.2.2.2.m1.1.1.cmml" xref="S4.T7.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.2.2.2.2.m1.1c">\uparrow</annotation></semantics></math>
</th>
<th id="S4.T7.3.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.3.3.3.6.1" class="ltx_text ltx_font_bold">Epoch</span></th>
<th id="S4.T7.3.3.3.3" class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T7.3.3.3.3.1" class="ltx_text ltx_font_bold">Training Time (s) <math id="S4.T7.3.3.3.3.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.3.3.3.3.1.m1.1a"><mo stretchy="false" id="S4.T7.3.3.3.3.1.m1.1.1" xref="S4.T7.3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.3.3.3.3.1.m1.1b"><ci id="S4.T7.3.3.3.3.1.m1.1.1.cmml" xref="S4.T7.3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.3.3.3.3.1.m1.1c">\downarrow</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T7.3.3.4.1" class="ltx_tr">
<td id="S4.T7.3.3.4.1.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="2"><span id="S4.T7.3.3.4.1.1.1" class="ltx_text">EfficientNet-B7</span></td>
<td id="S4.T7.3.3.4.1.2" class="ltx_td ltx_align_center ltx_border_t">✓</td>
<td id="S4.T7.3.3.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T7.3.3.4.1.3.1" class="ltx_text ltx_font_bold">71.04</span></td>
<td id="S4.T7.3.3.4.1.4" class="ltx_td ltx_align_center ltx_border_t">20</td>
<td id="S4.T7.3.3.4.1.5" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t">225</td>
</tr>
<tr id="S4.T7.3.3.5.2" class="ltx_tr">
<td id="S4.T7.3.3.5.2.1" class="ltx_td ltx_align_center ltx_border_bb">✗</td>
<td id="S4.T7.3.3.5.2.2" class="ltx_td ltx_align_center ltx_border_bb">70.44</td>
<td id="S4.T7.3.3.5.2.3" class="ltx_td ltx_align_center ltx_border_bb">20</td>
<td id="S4.T7.3.3.5.2.4" class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb">935</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T7.9.3.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S4.T7.7.2" class="ltx_text" style="font-size:90%;">Ablation study for freezing the visual extractor. <math id="S4.T7.6.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S4.T7.6.1.m1.1b"><mo stretchy="false" id="S4.T7.6.1.m1.1.1" xref="S4.T7.6.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T7.6.1.m1.1c"><ci id="S4.T7.6.1.m1.1.1.cmml" xref="S4.T7.6.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.6.1.m1.1d">\uparrow</annotation></semantics></math> indicates that higher values are better, while <math id="S4.T7.7.2.m2.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T7.7.2.m2.1b"><mo stretchy="false" id="S4.T7.7.2.m2.1.1" xref="S4.T7.7.2.m2.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T7.7.2.m2.1c"><ci id="S4.T7.7.2.m2.1.1.cmml" xref="S4.T7.7.2.m2.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T7.7.2.m2.1d">\downarrow</annotation></semantics></math> indicates that lower values are better.</span></figcaption>
</figure>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Discussion</h3>

<section id="S4.SS6.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.1 </span>Dataset Challenges</h4>

<div id="S4.SS6.SSS1.p1" class="ltx_para">
<p id="S4.SS6.SSS1.p1.3" class="ltx_p">Despite the comprehensive validation of the dataset by professionals, certain samples exhibit ambiguity, grammatical issues in ViVQA questions, or a lack of relevance between questions and answers. This highlights the challenge posed by the dataset’s overall low quality, a significant hurdle for VQA tasks in Vietnamese, primarily due to the scarcity of high-quality datasets. As depicted in Figure <a href="#S4.F8" title="Figure 8 ‣ 4.6.1 Dataset Challenges ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, we illustrate instances where the ground truth answers are inappropriate, unclear, or unrelated to the corresponding questions. Particularly, discrepancies arise, such as the misidentification of the bear’s color as brown instead of black, or the color of the cows being described as brown while the ground truth indicates white. Moreover, descriptions indicating a sandwich topped with quality ingredients are contradicted by an irrelevant answer, <math id="S4.SS6.SSS1.p1.1.m1.1" class="ltx_Math" alttext="`" display="inline"><semantics id="S4.SS6.SSS1.p1.1.m1.1a"><mi mathvariant="normal" id="S4.SS6.SSS1.p1.1.m1.1.1" xref="S4.SS6.SSS1.p1.1.m1.1.1.cmml">`</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS1.p1.1.m1.1b"><ci id="S4.SS6.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS6.SSS1.p1.1.m1.1.1">`</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS1.p1.1.m1.1c">`</annotation></semantics></math>Dog’. Similarly, instances where the depicted scene clearly shows a man riding a horse but is misinterpreted as a <math id="S4.SS6.SSS1.p1.2.m2.1" class="ltx_Math" alttext="`" display="inline"><semantics id="S4.SS6.SSS1.p1.2.m2.1a"><mi mathvariant="normal" id="S4.SS6.SSS1.p1.2.m2.1.1" xref="S4.SS6.SSS1.p1.2.m2.1.1.cmml">`</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS1.p1.2.m2.1b"><ci id="S4.SS6.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS6.SSS1.p1.2.m2.1.1">`</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS1.p1.2.m2.1c">`</annotation></semantics></math>mountain’, and a girl taking a selfie, potentially in a restroom, yet labeled as a <math id="S4.SS6.SSS1.p1.3.m3.1" class="ltx_Math" alttext="`" display="inline"><semantics id="S4.SS6.SSS1.p1.3.m3.1a"><mi mathvariant="normal" id="S4.SS6.SSS1.p1.3.m3.1.1" xref="S4.SS6.SSS1.p1.3.m3.1.1.cmml">`</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS1.p1.3.m3.1b"><ci id="S4.SS6.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS6.SSS1.p1.3.m3.1.1">`</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS1.p1.3.m3.1c">`</annotation></semantics></math>Cage’, further underscore the poor quality of the dataset in this study. These findings highlight the poor quality of the ViVQA dataset in this study, presenting additional challenges for researchers interested in improving dataset quality.</p>
</div>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/223573.jpg" id="S4.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf1.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F8.sf1.5.2" class="ltx_text" style="font-size:90%;">
Example 1 
<br class="ltx_break"><span id="S4.F8.sf1.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Màu của gấu là gì? (What is the color of the bear?)
<br class="ltx_break"><span id="S4.F8.sf1.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Màu nâu (Brown)
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/397075.jpg" id="S4.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf2.4.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F8.sf2.5.2" class="ltx_text" style="font-size:90%;">
Example 2 
<br class="ltx_break"><span id="S4.F8.sf2.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Một số chuối xanh treo ở đâu? (Where are some green bananas?) 
<br class="ltx_break"><span id="S4.F8.sf2.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Tòa nhà (Building)
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/336320.jpg" id="S4.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf3.4.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F8.sf3.5.2" class="ltx_text" style="font-size:90%;">
Example 3 
<br class="ltx_break"><span id="S4.F8.sf3.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Những gì được phủ lên với một số thành phần ngon? (What’s topped some delicious ingredients?)
<br class="ltx_break"><span id="S4.F8.sf3.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Con chó (Dog)
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/521008.jpg" id="S4.F8.sf4.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf4.4.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S4.F8.sf4.5.2" class="ltx_text" style="font-size:90%;">
Example 4 
<br class="ltx_break"><span id="S4.F8.sf4.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Cô gái đang chụp ảnh selfie ở đâu? (Where is the girl taking a selfie?)
<br class="ltx_break"><span id="S4.F8.sf4.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Chuồng (Cage)
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/576052.jpg" id="S4.F8.sf5.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf5.4.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S4.F8.sf5.5.2" class="ltx_text" style="font-size:90%;">
Example 5 
<br class="ltx_break"><span id="S4.F8.sf5.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Người đó đã làm gì với một con ngựa? (What did that person do to a horse?)
<br class="ltx_break"><span id="S4.F8.sf5.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Núi (Mountain)
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/564687.jpg" id="S4.F8.sf6.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.sf6.4.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S4.F8.sf6.5.2" class="ltx_text" style="font-size:90%;">
Example 6 
<br class="ltx_break"><span id="S4.F8.sf6.5.2.1" class="ltx_text ltx_font_bold">Question</span>: Màu của con bò là gì? (What is the color of the cow?)
<br class="ltx_break"><span id="S4.F8.sf6.5.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: Màu trắng (White)
</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">A few instances of low-quality examples.</span></figcaption>
</figure>
</section>
<section id="S4.SS6.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.2 </span>Experimental Analysis</h4>

<div id="S4.SS6.SSS2.p1" class="ltx_para">
<p id="S4.SS6.SSS2.p1.1" class="ltx_p">To start, we present insightful examples to analyze the performance of our model. Through experimental analysis, strengths and weaknesses become readily apparent in practical scenarios. The specifics of these instances are illustrated in Figure <a href="#S4.F9" title="Figure 9 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<div id="S4.SS6.SSS2.p2" class="ltx_para">
<p id="S4.SS6.SSS2.p2.1" class="ltx_p">In Figure <a href="#S4.F9" title="Figure 9 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>-(a), both BLIP-2 and EfficientNet made individual incorrect predictions, suggesting that each model lacks comprehensive information from the image. BLIP-2, emphasizing the global context of the image, may overlook local details, while EfficientNet may struggle to capture broader contextual information.</p>
</div>
<div id="S4.SS6.SSS2.p3" class="ltx_para">
<p id="S4.SS6.SSS2.p3.1" class="ltx_p">In Figure <a href="#S4.F9" title="Figure 9 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>-(b), the model solely relying on BLIP-2 for visual extraction erroneously predicted the color of the vase as white, likely due to BLIP-2’s tendency to prioritize global features, where much of the image appears white. Consequently, EfficientNet provided a correct answer by considering local details and determining the color of the vase to be red.</p>
</div>
<div id="S4.SS6.SSS2.p4" class="ltx_para">
<p id="S4.SS6.SSS2.p4.1" class="ltx_p">In Figure <a href="#S4.F9" title="Figure 9 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>-(c), the model utilizing only EfficientNet for visual extraction predicted that the location where a woman is holding a sandwich is the kitchen. This is attributed to EfficientNet’s capability to focus on intricate details such as the sandwich and the dashboard of a car, which might resemble oven buttons. Conversely, BLIP-2, taking into account the global context, accurately identified the location as a car.</p>
</div>
<div id="S4.SS6.SSS2.p5" class="ltx_para">
<p id="S4.SS6.SSS2.p5.1" class="ltx_p">Through experimental evaluation, it became evident that our model, along with its constituent components BLIP-2 and EfficientNet, exhibits both strengths and weaknesses. BLIP-2 excels in capturing the global context of images but may overlook finer local details, while EfficientNet focuses on intricate local features but may struggle with broader contextual information. These insights highlight the importance of considering both global and local features in model development for visual understanding tasks.</p>
</div>
<figure id="S4.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/458328.jpg" id="S4.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf1.11.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F9.sf1.12.2" class="ltx_text" style="font-size:90%;">
Example 1 
<br class="ltx_break"><span id="S4.F9.sf1.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Con chó nhỏ ở đâu? (Where is the little dog?)
<br class="ltx_break"><span id="S4.F9.sf1.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F9.sf1.12.2.3" class="ltx_text" style="color:#00B300;">Xe ô tô (Car)
<br class="ltx_break"></span><span id="S4.F9.sf1.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F9.sf1.12.2.5" class="ltx_text" style="color:#FF0000;">Xe tải (Truck)
<br class="ltx_break"></span><span id="S4.F9.sf1.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F9.sf1.12.2.7" class="ltx_text" style="color:#FF0000;">Vali (Suitcase)
<br class="ltx_break"></span><span id="S4.F9.sf1.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F9.sf1.12.2.9" class="ltx_text" style="color:#00B300;">Xe ô tô (Car)</span>
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/439213.jpg" id="S4.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf2.11.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F9.sf2.12.2" class="ltx_text" style="font-size:90%;">
Example 2 
<br class="ltx_break"><span id="S4.F9.sf2.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Màu của chiếc bình là gì? (What is the color of the vase?) 
<br class="ltx_break"><span id="S4.F9.sf2.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F9.sf2.12.2.3" class="ltx_text" style="color:#00B300;">Màu đỏ (red)
<br class="ltx_break"></span><span id="S4.F9.sf2.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F9.sf2.12.2.5" class="ltx_text" style="color:#FF0000;">Màu trắng (white)
<br class="ltx_break"></span><span id="S4.F9.sf2.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F9.sf2.12.2.7" class="ltx_text" style="color:#00B300;">Màu đỏ (red)
<br class="ltx_break"></span><span id="S4.F9.sf2.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F9.sf2.12.2.9" class="ltx_text" style="color:#00B300;">Màu đỏ (red)</span>
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/478652.jpg" id="S4.F9.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.sf3.11.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F9.sf3.12.2" class="ltx_text" style="font-size:90%;">
Example 3 
<br class="ltx_break"><span id="S4.F9.sf3.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Người phụ nữ đang giữ bánh sandwich ở đâu? (Where is the woman holding the sandwich?)
<br class="ltx_break"><span id="S4.F9.sf3.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F9.sf3.12.2.3" class="ltx_text" style="color:#00B300;">Xe ô tô (Car)
<br class="ltx_break"></span><span id="S4.F9.sf3.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F9.sf3.12.2.5" class="ltx_text" style="color:#00B300;">Xe ô tô (Car)
<br class="ltx_break"></span><span id="S4.F9.sf3.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F9.sf3.12.2.7" class="ltx_text" style="color:#FF0000;">Phòng bếp (Kitchen)
<br class="ltx_break"></span><span id="S4.F9.sf3.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F9.sf3.12.2.9" class="ltx_text" style="color:#00B300;">Xe ô tô (Car)</span>
</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S4.F9.3.2" class="ltx_text" style="font-size:90%;">Examining experimental scenarios to illuminate model performance.</span></figcaption>
</figure>
<div id="S4.SS6.SSS2.p6" class="ltx_para">
<p id="S4.SS6.SSS2.p6.1" class="ltx_p">Another aspect we wish to address is the ambiguity observed in the answers provided in the dataset utilized for evaluation, as well as the complexity of the linguistic aspects. In Figures <a href="#S4.F10" title="Figure 10 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>-(a,b), ambiguity arises between the ground truth answer and the predicted answer. We believe that the predicted answer could potentially be correct regarding the location of the object in the image; specifically, the man and artwork could be situated in the bathroom. Furthermore, in Figure <a href="#S4.F10" title="Figure 10 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>-(c), there is an excessive amount of redundant information contained in the question, making it difficult for our model to extract relevant information. Consequently, our model predicts the overall scene (Room) instead of a specific place (Class). This underscores the importance of addressing ambiguity and redundant information in question-answer datasets, as they can significantly impact model performance and interpretation. With a sufficiently robust dataset, we believe we could tackle this challenge, representing the motivation driving our future focus.</p>
</div>
<figure id="S4.F10" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F10.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/451375.jpg" id="S4.F10.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf1.11.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F10.sf1.12.2" class="ltx_text" style="font-size:90%;">
Example 1 
<br class="ltx_break"><span id="S4.F10.sf1.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Người đàn ông đang đánh răng ở đâu? (Where is the man brushing his teeth?)
<br class="ltx_break"><span id="S4.F10.sf1.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F10.sf1.12.2.3" class="ltx_text" style="color:#00B300;">Phòng (Room)
<br class="ltx_break"></span><span id="S4.F10.sf1.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F10.sf1.12.2.5" class="ltx_text" style="color:#FF0000;">Phòng tắm (Bathroom)
<br class="ltx_break"></span><span id="S4.F10.sf1.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F10.sf1.12.2.7" class="ltx_text" style="color:#FF0000;">Phòng tắm (Bathroom)
<br class="ltx_break"></span><span id="S4.F10.sf1.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F10.sf1.12.2.9" class="ltx_text" style="color:#FF0000;">Phòng tắm (Bathroom)</span>
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F10.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/342295.jpg" id="S4.F10.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf2.11.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F10.sf2.12.2" class="ltx_text" style="font-size:90%;">
Example 2 
<br class="ltx_break"><span id="S4.F10.sf2.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Tác phẩm nghệ thuật được đóng khung và lưu trữ ở đâu? (Where is the artwork framed and stored?) 
<br class="ltx_break"><span id="S4.F10.sf2.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F10.sf2.12.2.3" class="ltx_text" style="color:#00B300;">Bồn tắm (Bathtub)
<br class="ltx_break"></span><span id="S4.F10.sf2.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F10.sf2.12.2.5" class="ltx_text" style="color:#FF0000;">Phòng tắm (Bathroom)
<br class="ltx_break"></span><span id="S4.F10.sf2.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F10.sf2.12.2.7" class="ltx_text" style="color:#FF0000;">Chậu (Pot)
<br class="ltx_break"></span><span id="S4.F10.sf2.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F10.sf2.12.2.9" class="ltx_text" style="color:#FF0000;">Phòng tắm (Bathroom)</span>
</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F10.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2407.21229/assets/Fig/examples/540253.jpg" id="S4.F10.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.sf3.11.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S4.F10.sf3.12.2" class="ltx_text" style="font-size:90%;">
Example 3 
<br class="ltx_break"><span id="S4.F10.sf3.12.2.1" class="ltx_text ltx_font_bold">Question</span>: Những gì chứa đầy những sinh viên nhìn chằm chằm và xung quanh một bảng đầy máy tính xách tay mở? (What’s filled with students staring at and around a table full of open laptops?)
<br class="ltx_break"><span id="S4.F10.sf3.12.2.2" class="ltx_text ltx_font_bold">Ground Truth</span>: <span id="S4.F10.sf3.12.2.3" class="ltx_text" style="color:#00B300;">Lớp học (Class)
<br class="ltx_break"></span><span id="S4.F10.sf3.12.2.4" class="ltx_text ltx_font_bold">BLIP-2</span>: <span id="S4.F10.sf3.12.2.5" class="ltx_text" style="color:#FF0000;">Phòng (Room)
<br class="ltx_break"></span><span id="S4.F10.sf3.12.2.6" class="ltx_text ltx_font_bold">EfficientNet</span>: <span id="S4.F10.sf3.12.2.7" class="ltx_text" style="color:#FF0000;">Máy vi tính (Computer)
<br class="ltx_break"></span><span id="S4.F10.sf3.12.2.8" class="ltx_text ltx_font_bold">Our Model</span>: <span id="S4.F10.sf3.12.2.9" class="ltx_text" style="color:#FF0000;">Phòng (Room)</span>
</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S4.F10.3.2" class="ltx_text" style="font-size:90%;">Ambiguity assessment in question-answer datasets.</span></figcaption>
</figure>
<div id="S4.SS6.SSS2.p7" class="ltx_para">
<p id="S4.SS6.SSS2.p7.2" class="ltx_p">An additional noteworthy aspect we would like to address pertains to the distribution of incorrectly predicted question types by our model. As depicted in Figure <a href="#S4.F11" title="Figure 11 ‣ 4.6.2 Experimental Analysis ‣ 4.6 Discussion ‣ 4 Experiments ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>, our model inaccurately predicted the question type <math id="S4.SS6.SSS2.p7.1.m1.1" class="ltx_Math" alttext="`" display="inline"><semantics id="S4.SS6.SSS2.p7.1.m1.1a"><mi mathvariant="normal" id="S4.SS6.SSS2.p7.1.m1.1.1" xref="S4.SS6.SSS2.p7.1.m1.1.1.cmml">`</mi><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p7.1.m1.1b"><ci id="S4.SS6.SSS2.p7.1.m1.1.1.cmml" xref="S4.SS6.SSS2.p7.1.m1.1.1">`</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p7.1.m1.1c">`</annotation></semantics></math>object’ in approximately 43.6<math id="S4.SS6.SSS2.p7.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S4.SS6.SSS2.p7.2.m2.1a"><mo id="S4.SS6.SSS2.p7.2.m2.1.1" xref="S4.SS6.SSS2.p7.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS2.p7.2.m2.1b"><csymbol cd="latexml" id="S4.SS6.SSS2.p7.2.m2.1.1.cmml" xref="S4.SS6.SSS2.p7.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS2.p7.2.m2.1c">\%</annotation></semantics></math> of cases, which accounts for nearly half of all incorrect predictions. This observation suggests that our model may lack sufficient information regarding the specific details of objects within the image. Consequently, addressing this limitation stands as a motivating factor for our future work.</p>
</div>
<figure id="S4.F11" class="ltx_figure"><img src="/html/2407.21229/assets/Fig/distribution.png" id="S4.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S4.F11.3.2" class="ltx_text" style="font-size:90%;">Distribution of incorrectly predicted question types.</span></figcaption>
</figure>
</section>
<section id="S4.SS6.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.6.3 </span>Transfer Learning Considerations</h4>

<div id="S4.SS6.SSS3.p1" class="ltx_para">
<p id="S4.SS6.SSS3.p1.1" class="ltx_p">Our architecture is capable of efficiently transferring knowledge from a source domain, where the model undergoes initial training, to a target domain characterized by limited data or a demand for rapid deployment. This capability is underpinned by the flexibility of our design, which facilitates the easy replacement and adaptation of the backbone components.</p>
</div>
<div id="S4.SS6.SSS3.p2" class="ltx_para">
<p id="S4.SS6.SSS3.p2.1" class="ltx_p">In the context of linguistic applications, while our primary focus has been on monolingual Vietnamese, the model architecture can be readily adapted to other monolingual languages with minimal modifications to the Text Embedding module presented in section <a href="#S3.SS2" title="3.2 Text Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>. This adaptability ensures that the linguistic component of our model can handle a diverse array of languages, thereby broadening its applicability. Similarly, in the visual domain, the model’s backbones presented in section <a href="#S3.SS1" title="3.1 Image Embedding Module ‣ 3 Methodology ‣ Advancing Vietnamese Visual Question Answering with Transformer and Convolutional Integration" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> can be replaced with alternative architectures, providing the flexibility to optimize performance based on specific visual tasks or datasets.</p>
</div>
<div id="S4.SS6.SSS3.p3" class="ltx_para">
<p id="S4.SS6.SSS3.p3.1" class="ltx_p">Moreover, the versatility of our model extends beyond typical visual-linguistic tasks to specialized domains, such as Medical Visual Question Answering (MedVQA). Although our current work primarily addresses everyday scenes and objects, the transition to the medical domain is facilitated by the consistent input format of text-image pairs. This consistency aligns seamlessly with our architecture, enabling the model to leverage pre-existing knowledge and adapt to the nuanced requirements of medical imagery and terminology.</p>
</div>
<div id="S4.SS6.SSS3.p4" class="ltx_para">
<p id="S4.SS6.SSS3.p4.1" class="ltx_p">The ability to transfer learning effectively across these varied domains underscores the robustness of our architecture. By allowing for the modular exchange of core components, such as the text and visual backbones, our model provides a scalable and adaptable solution that meets the demands of diverse applications. This not only enhances the model’s utility across different linguistic and visual tasks but also ensures that it can be efficiently repurposed for new, domain-specific challenges.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this study, we introduced a ViVQA system with the key integration of convolutional and base-transformer methods for vision embedding. The convolutional methods capture local features, while the base-transformer methods address global features, resulting in a more comprehensive image representation. Our experiments demonstrated the effectiveness of these combined approaches in enhancing our architecture’s performance. However, we also encountered challenges related to the quality of the ViVQA dataset. The dataset’s low quality posed significant difficulties, as it included several incorrectly labeled samples, which affected the overall performance. Despite our model’s correct predictions from a human-like perspective, these predictions were not reflected accurately in the performance evaluations due to the dataset’s inaccuracies. Furthermore, our model’s performance is limited when addressing open-ended questions. The current dataset only encompasses four types of questions, and questions that deviate from these predefined types are not handled effectively by our model.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Despite the promising results, we recognize the need to address several challenges in future work. Our plans include improving the quality of the ViVQA dataset by conducting thorough reviews to ensure the accuracy of both questions and answers. We also intend to employ data augmentation techniques to further enrich the dataset. Additionally, as analyzed, our model still faces difficulties with questions concerning objects. To overcome this limitation, we aim to enhance object information by incorporating object detection models into the visual extraction process.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Our objective is to significantly improve the overall performance of the ViVQA system and contribute to the advancement of more efficient and reliable natural language processing in Vietnamese. By addressing these challenges, we hope to develop a more robust and accurate ViVQA system capable of handling a wider range of queries and providing better support for Vietnamese language applications.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research is funded by University of Science, VNU-HCM, Vietnam under grant number CNTT 2024–12.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2023]</span>
<span class="ltx_bibblock">
W. Shi, H. Wang, X. Lou,

</span>
<span class="ltx_bibblock">Multi-modal graph reasoning for structured video text extraction,

</span>
<span class="ltx_bibblock">Computers and Electrical Engineering 107 (2023) 108641.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Xie, J. Zeng, H. Wang, L. Fan, Y. Song,

</span>
<span class="ltx_bibblock">Cross-modal fusion for multi-label image classification with attention mechanism,

</span>
<span class="ltx_bibblock">Computers and Electrical Engineering 101 (2022) 108002.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xing et al. [2022]</span>
<span class="ltx_bibblock">
L. Xing, H. Jin, H. an Li, Z. Li,

</span>
<span class="ltx_bibblock">Multi-scale vision transformer classification model with self-supervised learning and dilated convolution,

</span>
<span class="ltx_bibblock">Computers and Electrical Engineering 103 (2022) 108270.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Z. Li, Z. Huang, L. Guo, L. Shan, G. Yu, Z. Chong, Y. Zhang,

</span>
<span class="ltx_bibblock">Cognitive knowledge graph generation for grid fault handling based on attention mechanism combined with multi-modal factor fusion,

</span>
<span class="ltx_bibblock">Computers and Electrical Engineering 111 (2023) 108855.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2021]</span>
<span class="ltx_bibblock">
K. Q. Tran, A. T. Nguyen, A. T.-H. Le, K. V. Nguyen,

</span>
<span class="ltx_bibblock">ViVQA: Vietnamese visual question answering,

</span>
<span class="ltx_bibblock">in: K. Hu, J.-B. Kim, C. Zong, E. Chersoni (Eds.), Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation, Association for Computational Lingustics, Shanghai, China, 2021, pp. 683–691.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen-Tran et al. [2022]</span>
<span class="ltx_bibblock">
D.-M. Nguyen-Tran, T. Le, M. L. Nguyen, H. T. Nguyen,

</span>
<span class="ltx_bibblock">Bi-directional cross-attention network on Vietnamese visual question answering,

</span>
<span class="ltx_bibblock">in: S. Dita, A. Trillanes, R. I. Lucas (Eds.), Proceedings of the 36th Pacific Asia Conference on Language, Information and Computation, Association for Computational Linguistics, Manila, Philippines, 2022, pp. 834–841.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Nguyen [2023]</span>
<span class="ltx_bibblock">
N. H. Nguyen, K. V. Nguyen,

</span>
<span class="ltx_bibblock">Pat: Parallel attention transformer for visual question answering in vietnamese,

</span>
<span class="ltx_bibblock">in: 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR), 2023, pp. 1–6.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2023]</span>
<span class="ltx_bibblock">
K. V. Tran, K. V. Nguyen, N. L. T. Nguyen,

</span>
<span class="ltx_bibblock">Bartphobeit: Pre-trained sequence-to-sequence and image transformers models for vietnamese visual question answering,

</span>
<span class="ltx_bibblock">in: 2023 International Conference on Multimedia Analysis and Pattern Recognition (MAPR), 2023, pp. 1–6.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, I. Polosukhin,

</span>
<span class="ltx_bibblock">Attention is all you need,

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 30 (2017).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
Y.-C. Chen, L. Li, L. Yu, A. El Kholy, F. Ahmed, Z. Gan, Y. Cheng, J. Liu,

</span>
<span class="ltx_bibblock">Uniter: Universal image-text representation learning,

</span>
<span class="ltx_bibblock">in: A. Vedaldi, H. Bischof, T. Brox, J.-M. Frahm (Eds.), Computer Vision – ECCV 2020, Springer International Publishing, Cham, 2020, pp. 104–120.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al.,

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision,

</span>
<span class="ltx_bibblock">in: International conference on machine learning, PMLR, 2021, pp. 8748–8763.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, S. Hoi,

</span>
<span class="ltx_bibblock">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,

</span>
<span class="ltx_bibblock">in: International Conference on Machine Learning, PMLR, 2022, pp. 12888–12900.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. [2022]</span>
<span class="ltx_bibblock">
J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al.,

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning,

</span>
<span class="ltx_bibblock">Advances in Neural Information Processing Systems 35 (2022) 23716–23736.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2023]</span>
<span class="ltx_bibblock">
X. Shen, D. Han, Z. Guo, C. Chen, J. Hua, G. Luo,

</span>
<span class="ltx_bibblock">Local self-attention in transformer for visual question answering,

</span>
<span class="ltx_bibblock">Applied Intelligence 53 (2023) 16706–16723.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, D. Parikh,

</span>
<span class="ltx_bibblock">Vqa: Visual question answering,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE international conference on computer vision, 2015, pp. 2425–2433.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman [2015]</span>
<span class="ltx_bibblock">
K. Simonyan, A. Zisserman,

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition,

</span>
<span class="ltx_bibblock">3rd International Conference on Learning Representations (ICLR 2015) (2015) 1–14.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber [1997]</span>
<span class="ltx_bibblock">
S. Hochreiter, J. Schmidhuber,

</span>
<span class="ltx_bibblock">Long short-term memory,

</span>
<span class="ltx_bibblock">Neural Computation 9 (1997) 1735–1780.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al. [2020]</span>
<span class="ltx_bibblock">
Y. Xi, Y. Zhang, S. Ding, S. Wan,

</span>
<span class="ltx_bibblock">Visual question answering model based on visual relationship detection,

</span>
<span class="ltx_bibblock">Signal Processing: Image Communication 80 (2020) 115648.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2021]</span>
<span class="ltx_bibblock">
Y. Wu, Y. Ma, S. Wan,

</span>
<span class="ltx_bibblock">Multi-scale relation reasoning for multi-modal visual question answering,

</span>
<span class="ltx_bibblock">Signal Processing: Image Communication 96 (2021) 116319.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2016]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, J. Sun,

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2015]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, J. Sun,

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal networks,

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 28 (2015).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pennington et al. [2014]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, C. Manning,

</span>
<span class="ltx_bibblock">GloVe: Global vectors for word representation,

</span>
<span class="ltx_bibblock">in: Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Doha, Qatar, 2014, pp. 1532–1543.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mikolov et al. [2013]</span>
<span class="ltx_bibblock">
T. Mikolov, K. Chen, G. Corrado, J. Dean,

</span>
<span class="ltx_bibblock">Efficient estimation of word representations in vector space,

</span>
<span class="ltx_bibblock">Proceedings of Workshop at ICLR 2013 (2013).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. [2014]</span>
<span class="ltx_bibblock">
J. Chung, C. Gulcehre, K. Cho, Y. Bengio,

</span>
<span class="ltx_bibblock">Empirical evaluation of gated recurrent neural networks on sequence modeling,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1412.3555 (2014).

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2019]</span>
<span class="ltx_bibblock">
J. Lu, D. Batra, D. Parikh, S. Lee,

</span>
<span class="ltx_bibblock">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 32 (2019).

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. [2020]</span>
<span class="ltx_bibblock">
J. Cho, J. Lu, D. Schwenk, H. Hajishirzi, A. Kembhavi,

</span>
<span class="ltx_bibblock">X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers,

</span>
<span class="ltx_bibblock">in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Online, 2020, pp. 8785–8805.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al. [2022]</span>
<span class="ltx_bibblock">
H. Bao, W. Wang, L. Dong, Q. Liu, O. K. Mohammed, K. Aggarwal, S. Som, S. Piao, F. Wei,

</span>
<span class="ltx_bibblock">VLMo: Unified vision-language pre-training with mixture-of-modality-experts,

</span>
<span class="ltx_bibblock">in: A. H. Oh, A. Agarwal, D. Belgrave, K. Cho (Eds.), Advances in Neural Information Processing Systems, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2016]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, D. Parikh,

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question answering,

</span>
<span class="ltx_bibblock">Advances in neural information processing systems 29 (2016).

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023]</span>
<span class="ltx_bibblock">
W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som, F. Wei,

</span>
<span class="ltx_bibblock">Image as a foreign language: Beit pretraining for vision and vision-language tasks,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 19175–19186.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2022]</span>
<span class="ltx_bibblock">
N. L. Tran, D. M. Le, D. Q. Nguyen,

</span>
<span class="ltx_bibblock">BARTpho: Pre-trained Sequence-to-Sequence Models for Vietnamese,

</span>
<span class="ltx_bibblock">in: Proceedings of the 23rd Annual Conference of the International Speech Communication Association, 2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2023]</span>
<span class="ltx_bibblock">
A. D. Nguyen, T. Le, H. T. Nguyen,

</span>
<span class="ltx_bibblock">Combining multi-vision embedding contextual attention for vietnamese visual question answering,

</span>
<span class="ltx_bibblock">in: Image and Video Technology: 10th Pacific-Rim Symposium, PSIVT 2022, Virtual Event, November 12–14, 2022, Proceedings, Springer-Verlag, Berlin, Heidelberg, 2023, p. 172–185.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. [2021]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, N. Houlsby,

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale,

</span>
<span class="ltx_bibblock">in: International Conference on Learning Representations, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Tuan Nguyen [2020]</span>
<span class="ltx_bibblock">
D. Q. Nguyen, A. Tuan Nguyen,

</span>
<span class="ltx_bibblock">PhoBERT: Pre-trained language models for Vietnamese,

</span>
<span class="ltx_bibblock">in: T. Cohn, Y. He, Y. Liu (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 1037–1042.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2022]</span>
<span class="ltx_bibblock">
Z. Peng, L. Dong, H. Bao, Q. Ye, F. Wei,

</span>
<span class="ltx_bibblock">A unified view of masked image modeling,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2210.10615 (2022).

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2023]</span>
<span class="ltx_bibblock">
N. H. Nguyen, D. T. Vo, K. Van Nguyen, N. L.-T. Nguyen,

</span>
<span class="ltx_bibblock">Openvivqa: Task, dataset, and multimodal fusion models for visual question answering in vietnamese,

</span>
<span class="ltx_bibblock">Information Fusion 100 (2023) 101868.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2020]</span>
<span class="ltx_bibblock">
N. H. Nguyen, D. T. Vo, K. Van Nguyen, N. L.-T. Nguyen, OpenViVQA-dataset, [dataset], 2020. URL: <a target="_blank" href="https://huggingface.co/datasets/uitnlp/OpenViVQA-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://huggingface.co/datasets/uitnlp/OpenViVQA-dataset</a>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mishra et al. [2019]</span>
<span class="ltx_bibblock">
A. Mishra, S. Shekhar, A. K. Singh, A. Chakraborty,

</span>
<span class="ltx_bibblock">Ocr-vqa: Visual question answering by reading text in images,

</span>
<span class="ltx_bibblock">in: 2019 International Conference on Document Analysis and Recognition (ICDAR), 2019, pp. 947–952.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, C. L. Zitnick,

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context,

</span>
<span class="ltx_bibblock">in: D. Fleet, T. Pajdla, B. Schiele, T. Tuytelaars (Eds.), Computer Vision – ECCV 2014, Springer International Publishing, Cham, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran et al. [2020]</span>
<span class="ltx_bibblock">
K. Q. Tran, A. T. Nguyen, A. T.-H. Le, K. V. Nguyen, ViVQA: Vietnamese Question Answering Dataset - PACLIC 35, [dataset], 2020. URL: <a target="_blank" href="https://github.com/kh4nh12/ViVQA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kh4nh12/ViVQA</a>.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
J. Li, D. Li, S. Savarese, S. Hoi,

</span>
<span class="ltx_bibblock">Blip-2: bootstrapping language-image pre-training with frozen image encoders and large language models,

</span>
<span class="ltx_bibblock">in: Proceedings of the 40th International Conference on Machine Learning, ICML’23, JMLR.org, Honolulu, Hawaii, USA, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Le [2019]</span>
<span class="ltx_bibblock">
M. Tan, Q. Le,

</span>
<span class="ltx_bibblock">EfficientNet: Rethinking model scaling for convolutional neural networks,

</span>
<span class="ltx_bibblock">in: K. Chaudhuri, R. Salakhutdinov (Eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, PMLR, Long Beach, California, USA, 2019, pp. 6105–6114.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sandler et al. [2018]</span>
<span class="ltx_bibblock">
M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, L.-C. Chen,

</span>
<span class="ltx_bibblock">Mobilenetv2: Inverted residuals and linear bottlenecks,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510–4520.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2019]</span>
<span class="ltx_bibblock">
M. Tan, B. Chen, R. Pang, V. Vasudevan, M. Sandler, A. Howard, Q. V. Le,

</span>
<span class="ltx_bibblock">Mnasnet: Platform-aware neural architecture search for mobile,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 2820–2828.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2020]</span>
<span class="ltx_bibblock">
C. Xie, M. Tan, B. Gong, J. Wang, A. L. Yuille, Q. V. Le,

</span>
<span class="ltx_bibblock">Adversarial examples improve image recognition,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. [2009]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei,

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database,

</span>
<span class="ltx_bibblock">in: 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2019]</span>
<span class="ltx_bibblock">
I. Loshchilov, F. Hutter,

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization,

</span>
<span class="ltx_bibblock">in: International Conference on Learning Representations, 2019.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2016]</span>
<span class="ltx_bibblock">
G. Huang, Y. Sun, Z. Liu, D. Sedra, K. Q. Weinberger,

</span>
<span class="ltx_bibblock">Deep networks with stochastic depth,

</span>
<span class="ltx_bibblock">in: B. Leibe, J. Matas, N. Sebe, M. Welling (Eds.), Computer Vision – ECCV 2016, Springer International Publishing, Cham, 2016, pp. 646–661.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. [2014]</span>
<span class="ltx_bibblock">
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov,

</span>
<span class="ltx_bibblock">Dropout: A simple way to prevent neural networks from overfitting,

</span>
<span class="ltx_bibblock">Journal of Machine Learning Research 15 (2014) 1929–1958.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. [2017]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh,

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding in visual question answering,

</span>
<span class="ltx_bibblock">in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 6904–6913.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuan Nguyen et al. [2020]</span>
<span class="ltx_bibblock">
A. Tuan Nguyen, M. H. Dao, D. Q. Nguyen,

</span>
<span class="ltx_bibblock">A pilot study of text-to-SQL semantic parsing for Vietnamese,

</span>
<span class="ltx_bibblock">in: T. Cohn, Y. He, Y. Liu (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 4079–4085.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2407.21228" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2407.21229" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2407.21229">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2407.21229" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2407.21230" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Aug  5 15:05:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
