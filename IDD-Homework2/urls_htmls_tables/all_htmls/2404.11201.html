<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation</title>
<!--Generated on Wed May  1 15:38:04 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2404.11201v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S1" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S2" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Multilingual Interference.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Language-Specific Modeling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Sub-networks in Multi-task Models.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Neuron Structural Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1" title="In 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Identifying Specialized Neurons</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1.SSS0.Px1" title="In 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Activation Recording.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1.SSS0.Px2" title="In 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Neuron Selection.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS2" title="In 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Analysis on EC30</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS2.SSS1" title="In 3.2 Analysis on EC30 ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Neuron Overlaps Reflect Language Proximity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS2.SSS2" title="In 3.2 Analysis on EC30 ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>The Progression of Neuron Overlaps</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S4" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Neuron Specialization Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S4.SS1" title="In 4 Neuron Specialization Training ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Vanilla Feed-Forward Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S4.SS2" title="In 4 Neuron Specialization Training ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Specializing Task-Specific FFN</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS1" title="In 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS1.SSS0.Px1" title="In 5.1 Datasets ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">IWSLT.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS1.SSS0.Px2" title="In 5.1 Datasets ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">EC30.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS2" title="In 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Systems</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS2.SSS0.Px1" title="In 5.2 Systems ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Baselines:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS2.SSS0.Px2" title="In 5.2 Systems ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Adapters.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS2.SSS0.Px3" title="In 5.2 Systems ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">LaSS.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.SS3" title="In 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Implementation and Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results and Analyses</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS1" title="In 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Small-Scale Results on IWSLT</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS1.SSS0.Px1" title="In 6.1 Small-Scale Results on IWSLT ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Scaling up does not always reduce interference.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS2" title="In 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Large-Scale Results on EC-30</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS2.SSS0.Px1" title="In 6.2 Large-Scale Results on EC-30 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Efficiency Comparisons.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS2.SSS0.Px2" title="In 6.2 Large-Scale Results on EC-30 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Random Mask.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS2.SSS0.Px3" title="In 6.2 Large-Scale Results on EC-30 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">The role of threshold factor.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS3" title="In 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>The Impact of Reducing Interference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S7" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1" title="In Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Appendix</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS1" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Dataset details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS1.SSS0.Px1" title="In A.1 Dataset details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">IWSLT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS1.SSS0.Px2" title="In A.1 Dataset details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">EC30</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Model and Training Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2.SSS0.Px1" title="In A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Bilingual models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2.SSS0.Px2" title="In A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Language Pair Adapters.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2.SSS0.Px3" title="In A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">Language Family Adapters.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2.SSS0.Px4" title="In A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title">LaSS.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS3" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Pseudocode of Neuron Specialization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS4" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.4 </span>Result Details using ChrF++ and COMET</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS5" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.5 </span>Sparsity versus Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS6" title="In Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.6 </span>Visualization Details</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Neuron Specialization: Leveraging Intrinsic 
<br class="ltx_break"/>Task Modularity for Multilingual Machine Translation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shaomu Tan   Di Wu   Christof Monz
<br class="ltx_break"/>Language Technology Lab
<br class="ltx_break"/>University of Amsterdam 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">{s.tan, d.wu, c.monz}@uva.nl</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Training a unified multilingual model promotes knowledge transfer but inevitably introduces <span class="ltx_text ltx_font_italic" id="id2.id1.1">negative interference</span>. Language-specific modeling methods show promise in reducing interference. However, they often rely on heuristics to distribute capacity and struggle to foster cross-lingual transfer via isolated modules. In this paper, we explore intrinsic task modularity within multilingual networks and leverage these observations to circumvent interference under multilingual translation. We show that neurons in the feed-forward layers tend to be activated in a language-specific manner. Meanwhile, these specialized neurons exhibit structural overlaps that reflect language proximity, which progress across layers. Based on these findings, we propose <span class="ltx_text ltx_font_italic" id="id2.id1.2">Neuron Specialization</span>, an approach that identifies specialized neurons to modularize feed-forward layers and then continuously updates them through sparse networks. Extensive experiments show that our approach achieves consistent performance gains over strong baselines with additional analyses demonstrating reduced interference and increased knowledge transfer.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>We release code at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://anonymous.4open.science/r/NS-3D93" title="">https://anonymous.4open.science/r/NS-3D93</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Jointly training multilingual data in a unified model with a shared architecture for different languages has been a trend  <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib8" title="">2020</a>; Le Scao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib19" title="">2022</a>)</cite> encouraging knowledge transfer across languages, especially for low-resource languages <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib16" title="">2017</a>); Pires et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib28" title="">2019</a>)</cite>.
However, such a training paradigm also leads to <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">negative interference</span> due to conflicting optimization demands <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib38" title="">2020</a>)</cite>. This interference often causes performance degradation for high-resource languages <cite class="ltx_cite ltx_citemacro_cite">Li and Gong (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib20" title="">2021</a>); Pfeiffer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib26" title="">2022</a>)</cite> and can be further exacerbated by limited model capacity <cite class="ltx_cite ltx_citemacro_cite">Shaham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib33" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Modular-based methods, such as Language-specific modeling <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib43" title="">2020b</a>)</cite> and adapters <cite class="ltx_cite ltx_citemacro_cite">Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib3" title="">2019</a>)</cite>, aim to mitigate interference by balancing full parameter sharing with isolated or partially shared modules <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib27" title="">2023</a>)</cite>.
However, they heavily depend on heuristics for allocating task-specific capacity and face challenges in enabling knowledge transfer between modules <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib42" title="">2020a</a>)</cite>.
Specifically, such methods rely on prior knowledge for managing parameter sharing such as language-family adapters <cite class="ltx_cite ltx_citemacro_cite">Chronopoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib7" title="">2023</a>)</cite> or directly isolate parameters per language, which impedes transfer <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib29" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Research in vision and cognitive science has shown that unified multi-task models may spontaneously develop task-specific functional specializations for distinct tasks <cite class="ltx_cite ltx_citemacro_cite">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib41" title="">2019</a>); Dobs et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib10" title="">2022</a>)</cite>, a phenomenon also observed in mixture of experts Transformer systems <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib44" title="">2023</a>)</cite>.
These findings suggest that through multi-task training, networks naturally evolve towards specialized modularity to effectively manage diverse tasks, with the ablation of these specialized modules adversely affecting task performance <cite class="ltx_cite ltx_citemacro_cite">Pfeiffer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib27" title="">2023</a>)</cite>.
Despite these insights, exploiting the inherent structural signals for multi-task optimization remains largely unexplored.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we explore the intrinsic task-specific modularity within multi-task networks in Multilingual Machine Translation (MMT), treating each language pair as a separate task.
We focus on analyzing the intermediate activations in the Feed-Forward Networks (FFN) where most model parameters reside.
Our analysis shows that neurons activate in a language-specific way, yet they present structural overlaps that indicate language proximity.
Moreover, this pattern evolves across layers in the model, consistent with the transition of multilingual representations from language-specific to language-agnostic <cite class="ltx_cite ltx_citemacro_cite">Kudugunta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib18" title="">2019</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Building on these observations, we introduce <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">Neuron Specialization</span>, a novel method that leverages intrinsic task modularity to reduce interference and enhance knowledge transfer.
In general, our approach selectively updates the FFN parameters during back-propagation for different tasks to enhance task specificity.
Specifically, we first identify task-specific neurons from pre-trained multilingual translation models, using standard forward-pass validation processes without decoding.
We then specifically modularize FFN layers using these specialized neurons and continuously update FFNs via sparse networks.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Extensive experiments on small- (IWSLT) and large-scale EC30 <cite class="ltx_cite ltx_citemacro_cite">Tan and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib34" title="">2023</a>)</cite> multilingual translation datasets show that our method consistently achieves performance gains over strong baselines.
Moreover, we conduct in-depth analyses to demonstrate that our method effectively mitigates interference and enhances knowledge transfer in high and low-resource languages, respectively.
Our main contributions are summarized as follows:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We identify inherent multilingual modularity by showing that neurons activate in a language-specific manner and their overlapping patterns reflect language proximity.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Building on these findings, we enhance task specificity through sparse sub-networks, achieving consistent improvements in translation quality over strong baselines.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We employ analyses to show that our method effectively reduces interference in high-resource languages and boosts knowledge transfer in low-resource languages.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multilingual Interference.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Multilingual training enables knowledge transfer but also causes <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">interference</span>, largely due to optimization conflicts among various languages or tasks <cite class="ltx_cite ltx_citemacro_cite">Wang and Zhang (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib37" title="">2022</a>)</cite>. Methods addressing conflicts between tasks hold promise to reduce interference <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib38" title="">2020</a>)</cite>, yet they show limited effectiveness in practical applications <cite class="ltx_cite ltx_citemacro_cite">Xin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib40" title="">2022</a>)</cite>. Scaling up model size reduces interference directly but may lead to overly large models <cite class="ltx_cite ltx_citemacro_cite">Chang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib4" title="">2023</a>)</cite>, with risks of overfitting <cite class="ltx_cite ltx_citemacro_cite">Aharoni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib1" title="">2019</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Language-Specific Modeling.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Modular-based approaches enhance the unified model by adding language-dependent modules such as adapters <cite class="ltx_cite ltx_citemacro_cite">Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib3" title="">2019</a>)</cite> or language-aware layers <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib43" title="">2020b</a>)</cite>. Although the unified model serves as a common foundation, these approaches struggle to facilitate knowledge transfer among isolated modules due to a lack of clear inductive biases and thus heavy reliance on heuristics. For instance,  <cite class="ltx_cite ltx_citemacro_citet">Chronopoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib7" title="">2023</a>)</cite> rely on priori knowledge to control parameter sharing in language family adapters, <cite class="ltx_cite ltx_citemacro_citet">Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib3" title="">2019</a>); Pires et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib29" title="">2023</a>)</cite> isolate modules per language, hindering knowledge sharing.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.1">Additionally, these modular-based methods substantially increase the number of parameters, thereby leading to increased memory demands and slower inference times <cite class="ltx_cite ltx_citemacro_citep">(Liao et al., <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib21" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib22" title="">b</a>)</cite>.
Despite adapters normally being lightweight, they can easily accumulate to a significant parameter growth when dealing with many languages.
In contrast, our method leverages the model’s intrinsic modularity signals to promote task separation, without adding extra parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Sub-networks in Multi-task Models.</h5>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">The lottery ticket hypothesis <cite class="ltx_cite ltx_citemacro_cite">Frankle and Carbin (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib13" title="">2018</a>)</cite> states that within dense neural networks, sparse subnetworks can be found with iterative pruning to achieve the original network’s performance.
Following this premise, recent studies attempt to isolate sub-networks of a pre-trained unified model that captures task-specific features <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>); He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib14" title="">2023</a>); Choenni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib5" title="">2023a</a>)</cite>.
Nonetheless, unlike our method that identifies intrinsic modularity within the model, these approaches depend on fine-tuning to extract the task-specific sub-networks.
This process may not reflect the original model modularity and also can be particularly resource-consuming for multiple tasks.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">Specifically, these methods extract the task-specific sub-networks by fine-tuning the original unified multi-task model on specific tasks, followed by employing pruning to retain only the most changed parameters. We argue that this process faces several issues: 1) The sub-network might be an artifact of fine-tuning, suggesting the original model may not inherently possess such modularity. 2) This is further supported by the observation that different random seeds during fine-tuning lead to varied sub-networks and performance instability <cite class="ltx_cite ltx_citemacro_cite">Choenni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib5" title="">2023a</a>)</cite>. 3) The process is highly inefficient for models covering multiple tasks, as it necessitates separate fine-tuning for each task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Neuron Structural Analysis</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Recent work aims to identify a subset of parameters within pre-trained multi-task networks that are sensitive to distinct tasks.
This exploration is done by either 1) ablating model components to assess impacts on performance, such as <cite class="ltx_cite ltx_citemacro_citet">Dobs et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib10" title="">2022</a>)</cite> ablate task-specific filters in vision models by setting their output to zero; or 2) fine-tuning the unified model on task-specific data to extract sub-networks <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>); He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib14" title="">2023</a>); Choenni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib6" title="">2023b</a>)</cite>.
These approaches, however, raise a fundamental question, namely whether the modularity is inherent to the original model, or simply an artifact introduced by network modifications.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In this paper, we perform a thorough identification of task-specific modularity through the lens of neuron behaviors, without altering the original parameters or architectures. We focus on the neurons — the intermediate activations inside the Feed-Forward Networks (FFN) — to investigate if they indicate task-specific modularity features. As FFN neurons are active (&gt;0) or inactive (=0) due to the <math alttext="\mathit{ReLU}" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">𝑅𝑒𝐿𝑈</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">𝑅𝑒𝐿𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathit{ReLU}</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">italic_ReLU</annotation></semantics></math> activation function, this binary activation state offers a clear view of their contributions to the network’s output.
Intuitively, neurons that remain inactive for one task but show significant activation for another may be indicative of specialization for the latter.
Analyzing such modularity structures can improve our understanding of fundamental properties in multi-task models and yield insights to advance multi-task learning.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Identifying Specialized Neurons</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We choose multilingual translation as a testbed, treating each translation direction as a distinct task throughout the paper.
We start with a pre-trained multilingual model with <math alttext="d_{\mathit{ff}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">d</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑑</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">d_{\mathit{ff}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math> as its dimension of the FFN layer.
We hypothesize the existence of neuron subsets specialized for each task and describe the identification process of an FFN layer as follows.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Activation Recording.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.8">Given a validation dataset <math alttext="D_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">D_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> for the <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.1d">italic_t</annotation></semantics></math>-th task, we measure activation frequencies in an FFN layer during validation.
For each sample <math alttext="x_{i}\in D_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">x</mi><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml">∈</mo><msub id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.cmml">t</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1"><in id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.1"></in><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.2">𝑥</ci><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">x_{i}\in D_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, we record the state of each neuron after <math alttext="\mathit{ReLU}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">𝑅𝑒𝐿𝑈</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1">𝑅𝑒𝐿𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">\mathit{ReLU}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.4.m4.1d">italic_ReLU</annotation></semantics></math>, reflecting whether the neuron is active or inactive to the sample.
We use a binary vector <math alttext="a^{t}_{i}\in\mathbb{R}^{d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.1a"><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml"><msubsup id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.3.cmml">t</mi></msubsup><mo id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.2.cmml">ℝ</mi><msub id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3.cmml">𝑓𝑓</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1"><in id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.1"></in><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2">subscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.2">𝑎</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.2.3">𝑡</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.2">𝑑</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.3.3.3">𝑓𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.1c">a^{t}_{i}\in\mathbb{R}^{d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.5.m5.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> to store this neuron state information.
Note that this vector aggregates neuron activations for all tokens in the sample by taking the neuron union of them.
By further merging all of the binary vectors for all samples in <math alttext="D_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.6.m6.1"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.1a"><msub id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.1b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.1c">D_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, an accumulated vector <math alttext="a^{t}=\sum_{x_{i}\in D_{t}}a^{t}_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.7.m7.1"><semantics id="S3.SS1.SSS0.Px1.p1.7.m7.1a"><mrow id="S3.SS1.SSS0.Px1.p1.7.m7.1.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml"><msup id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.3.cmml">t</mi></msup><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1" rspace="0.111em" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml"><msub id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.cmml"><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.2.cmml">∑</mo><mrow id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.cmml"><msub id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.2.cmml">x</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.3.cmml">i</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.1.cmml">∈</mo><msub id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.3.cmml">t</mi></msub></mrow></msub><msubsup id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.3.cmml">i</mi><mi id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.3" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.3.cmml">t</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1"><eq id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.1"></eq><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.2">𝑎</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.2.3">𝑡</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3"><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1">subscript</csymbol><sum id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.2"></sum><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3"><in id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.1"></in><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.2">𝑥</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.2.3">𝑖</ci></apply><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2">subscript</csymbol><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2">superscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.2">𝑎</ci><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.2.3">𝑡</ci></apply><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.3.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.3.2.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.7.m7.1c">a^{t}=\sum_{x_{i}\in D_{t}}a^{t}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.7.m7.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can be derived, which denotes the frequency of each neuron being activated during a forward pass given a task-specific dataset <math alttext="D_{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.8.m8.1"><semantics id="S3.SS1.SSS0.Px1.p1.8.m8.1a"><msub id="S3.SS1.SSS0.Px1.p1.8.m8.1.1" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml">D</mi><mi id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.8.m8.1b"><apply id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.2">𝐷</ci><ci id="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.8.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.8.m8.1c">D_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.8.m8.1d">italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Neuron Selection.</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.5">We identify specialized neurons for each task <math alttext="t" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">italic_t</annotation></semantics></math> based on their activation frequency <math alttext="a^{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝑎</ci><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">a^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.2.m2.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>.
A subset of neurons <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><msubsup id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml">S</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml">k</mi><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.2">𝑆</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.3">𝑘</ci></apply><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is progressively selected based on the highest <math alttext="a^{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><msup id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">a</mi><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2">𝑎</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">a^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.4.m4.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> values until reaching a predefined threshold <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.5.m5.1d">italic_k</annotation></semantics></math>, where</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{i\in S_{k}^{t}}a^{t}_{(i)}&gt;=k\sum_{i=1}^{d_{\mathit{ff}}}a^{t}_{(i)}" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.3" xref="S3.E1.m1.2.3.cmml"><mrow id="S3.E1.m1.2.3.2" xref="S3.E1.m1.2.3.2.cmml"><munder id="S3.E1.m1.2.3.2.1" xref="S3.E1.m1.2.3.2.1.cmml"><mo id="S3.E1.m1.2.3.2.1.2" movablelimits="false" xref="S3.E1.m1.2.3.2.1.2.cmml">∑</mo><mrow id="S3.E1.m1.2.3.2.1.3" xref="S3.E1.m1.2.3.2.1.3.cmml"><mi id="S3.E1.m1.2.3.2.1.3.2" xref="S3.E1.m1.2.3.2.1.3.2.cmml">i</mi><mo id="S3.E1.m1.2.3.2.1.3.1" xref="S3.E1.m1.2.3.2.1.3.1.cmml">∈</mo><msubsup id="S3.E1.m1.2.3.2.1.3.3" xref="S3.E1.m1.2.3.2.1.3.3.cmml"><mi id="S3.E1.m1.2.3.2.1.3.3.2.2" xref="S3.E1.m1.2.3.2.1.3.3.2.2.cmml">S</mi><mi id="S3.E1.m1.2.3.2.1.3.3.2.3" xref="S3.E1.m1.2.3.2.1.3.3.2.3.cmml">k</mi><mi id="S3.E1.m1.2.3.2.1.3.3.3" xref="S3.E1.m1.2.3.2.1.3.3.3.cmml">t</mi></msubsup></mrow></munder><msubsup id="S3.E1.m1.2.3.2.2" xref="S3.E1.m1.2.3.2.2.cmml"><mi id="S3.E1.m1.2.3.2.2.2.2" xref="S3.E1.m1.2.3.2.2.2.2.cmml">a</mi><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.2.3.2.2.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.2.3.2.2.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.2.3.2.2.cmml">)</mo></mrow><mi id="S3.E1.m1.2.3.2.2.2.3" xref="S3.E1.m1.2.3.2.2.2.3.cmml">t</mi></msubsup></mrow><mo id="S3.E1.m1.2.3.1" lspace="0.278em" rspace="0.278em" xref="S3.E1.m1.2.3.1.cmml">&gt;=</mo><mrow id="S3.E1.m1.2.3.3" xref="S3.E1.m1.2.3.3.cmml"><mi id="S3.E1.m1.2.3.3.2" xref="S3.E1.m1.2.3.3.2.cmml">k</mi><mo id="S3.E1.m1.2.3.3.1" xref="S3.E1.m1.2.3.3.1.cmml">⁢</mo><mrow id="S3.E1.m1.2.3.3.3" xref="S3.E1.m1.2.3.3.3.cmml"><munderover id="S3.E1.m1.2.3.3.3.1" xref="S3.E1.m1.2.3.3.3.1.cmml"><mo id="S3.E1.m1.2.3.3.3.1.2.2" movablelimits="false" xref="S3.E1.m1.2.3.3.3.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.2.3.3.3.1.2.3" xref="S3.E1.m1.2.3.3.3.1.2.3.cmml"><mi id="S3.E1.m1.2.3.3.3.1.2.3.2" xref="S3.E1.m1.2.3.3.3.1.2.3.2.cmml">i</mi><mo id="S3.E1.m1.2.3.3.3.1.2.3.1" xref="S3.E1.m1.2.3.3.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.2.3.3.3.1.2.3.3" xref="S3.E1.m1.2.3.3.3.1.2.3.3.cmml">1</mn></mrow><msub id="S3.E1.m1.2.3.3.3.1.3" xref="S3.E1.m1.2.3.3.3.1.3.cmml"><mi id="S3.E1.m1.2.3.3.3.1.3.2" xref="S3.E1.m1.2.3.3.3.1.3.2.cmml">d</mi><mi id="S3.E1.m1.2.3.3.3.1.3.3" xref="S3.E1.m1.2.3.3.3.1.3.3.cmml">𝑓𝑓</mi></msub></munderover><msubsup id="S3.E1.m1.2.3.3.3.2" xref="S3.E1.m1.2.3.3.3.2.cmml"><mi id="S3.E1.m1.2.3.3.3.2.2.2" xref="S3.E1.m1.2.3.3.3.2.2.2.cmml">a</mi><mrow id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.3.3.3.2.cmml"><mo id="S3.E1.m1.2.2.1.3.1" stretchy="false" xref="S3.E1.m1.2.3.3.3.2.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">i</mi><mo id="S3.E1.m1.2.2.1.3.2" stretchy="false" xref="S3.E1.m1.2.3.3.3.2.cmml">)</mo></mrow><mi id="S3.E1.m1.2.3.3.3.2.2.3" xref="S3.E1.m1.2.3.3.3.2.2.3.cmml">t</mi></msubsup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.3.cmml" xref="S3.E1.m1.2.3"><geq id="S3.E1.m1.2.3.1.cmml" xref="S3.E1.m1.2.3.1"></geq><apply id="S3.E1.m1.2.3.2.cmml" xref="S3.E1.m1.2.3.2"><apply id="S3.E1.m1.2.3.2.1.cmml" xref="S3.E1.m1.2.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.1.cmml" xref="S3.E1.m1.2.3.2.1">subscript</csymbol><sum id="S3.E1.m1.2.3.2.1.2.cmml" xref="S3.E1.m1.2.3.2.1.2"></sum><apply id="S3.E1.m1.2.3.2.1.3.cmml" xref="S3.E1.m1.2.3.2.1.3"><in id="S3.E1.m1.2.3.2.1.3.1.cmml" xref="S3.E1.m1.2.3.2.1.3.1"></in><ci id="S3.E1.m1.2.3.2.1.3.2.cmml" xref="S3.E1.m1.2.3.2.1.3.2">𝑖</ci><apply id="S3.E1.m1.2.3.2.1.3.3.cmml" xref="S3.E1.m1.2.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.3.3.1.cmml" xref="S3.E1.m1.2.3.2.1.3.3">superscript</csymbol><apply id="S3.E1.m1.2.3.2.1.3.3.2.cmml" xref="S3.E1.m1.2.3.2.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.1.3.3.2.1.cmml" xref="S3.E1.m1.2.3.2.1.3.3">subscript</csymbol><ci id="S3.E1.m1.2.3.2.1.3.3.2.2.cmml" xref="S3.E1.m1.2.3.2.1.3.3.2.2">𝑆</ci><ci id="S3.E1.m1.2.3.2.1.3.3.2.3.cmml" xref="S3.E1.m1.2.3.2.1.3.3.2.3">𝑘</ci></apply><ci id="S3.E1.m1.2.3.2.1.3.3.3.cmml" xref="S3.E1.m1.2.3.2.1.3.3.3">𝑡</ci></apply></apply></apply><apply id="S3.E1.m1.2.3.2.2.cmml" xref="S3.E1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.2.1.cmml" xref="S3.E1.m1.2.3.2.2">subscript</csymbol><apply id="S3.E1.m1.2.3.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.2.2.2.1.cmml" xref="S3.E1.m1.2.3.2.2">superscript</csymbol><ci id="S3.E1.m1.2.3.2.2.2.2.cmml" xref="S3.E1.m1.2.3.2.2.2.2">𝑎</ci><ci id="S3.E1.m1.2.3.2.2.2.3.cmml" xref="S3.E1.m1.2.3.2.2.2.3">𝑡</ci></apply><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝑖</ci></apply></apply><apply id="S3.E1.m1.2.3.3.cmml" xref="S3.E1.m1.2.3.3"><times id="S3.E1.m1.2.3.3.1.cmml" xref="S3.E1.m1.2.3.3.1"></times><ci id="S3.E1.m1.2.3.3.2.cmml" xref="S3.E1.m1.2.3.3.2">𝑘</ci><apply id="S3.E1.m1.2.3.3.3.cmml" xref="S3.E1.m1.2.3.3.3"><apply id="S3.E1.m1.2.3.3.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.1.cmml" xref="S3.E1.m1.2.3.3.3.1">superscript</csymbol><apply id="S3.E1.m1.2.3.3.3.1.2.cmml" xref="S3.E1.m1.2.3.3.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.2.1.cmml" xref="S3.E1.m1.2.3.3.3.1">subscript</csymbol><sum id="S3.E1.m1.2.3.3.3.1.2.2.cmml" xref="S3.E1.m1.2.3.3.3.1.2.2"></sum><apply id="S3.E1.m1.2.3.3.3.1.2.3.cmml" xref="S3.E1.m1.2.3.3.3.1.2.3"><eq id="S3.E1.m1.2.3.3.3.1.2.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1.2.3.1"></eq><ci id="S3.E1.m1.2.3.3.3.1.2.3.2.cmml" xref="S3.E1.m1.2.3.3.3.1.2.3.2">𝑖</ci><cn id="S3.E1.m1.2.3.3.3.1.2.3.3.cmml" type="integer" xref="S3.E1.m1.2.3.3.3.1.2.3.3">1</cn></apply></apply><apply id="S3.E1.m1.2.3.3.3.1.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.1.3.1.cmml" xref="S3.E1.m1.2.3.3.3.1.3">subscript</csymbol><ci id="S3.E1.m1.2.3.3.3.1.3.2.cmml" xref="S3.E1.m1.2.3.3.3.1.3.2">𝑑</ci><ci id="S3.E1.m1.2.3.3.3.1.3.3.cmml" xref="S3.E1.m1.2.3.3.3.1.3.3">𝑓𝑓</ci></apply></apply><apply id="S3.E1.m1.2.3.3.3.2.cmml" xref="S3.E1.m1.2.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2">subscript</csymbol><apply id="S3.E1.m1.2.3.3.3.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.3.3.3.2.2.1.cmml" xref="S3.E1.m1.2.3.3.3.2">superscript</csymbol><ci id="S3.E1.m1.2.3.3.3.2.2.2.cmml" xref="S3.E1.m1.2.3.3.3.2.2.2">𝑎</ci><ci id="S3.E1.m1.2.3.3.3.2.2.3.cmml" xref="S3.E1.m1.2.3.3.3.2.2.3">𝑡</ci></apply><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\sum_{i\in S_{k}^{t}}a^{t}_{(i)}&gt;=k\sum_{i=1}^{d_{\mathit{ff}}}a^{t}_{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">∑ start_POSTSUBSCRIPT italic_i ∈ italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT &gt; = italic_k ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p3.8">Here, the value <math alttext="a_{(i)}^{t}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p3.1.m1.1a"><msubsup id="S3.SS1.SSS0.Px2.p3.1.m1.1.2" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mi id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.2" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.2.cmml">a</mi><mrow id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.cmml"><mo id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.3.1" stretchy="false" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.1.cmml">i</mi><mo id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.3.2" stretchy="false" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.cmml">)</mo></mrow><mi id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.3" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.2.2">𝑎</ci><ci id="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.1.1.1">𝑖</ci></apply><ci id="S3.SS1.SSS0.Px2.p3.1.m1.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p3.1.m1.1.2.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.1.m1.1c">a_{(i)}^{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.1.m1.1d">italic_a start_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is the frequency of the activation at dimension <math alttext="i" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p3.2.m2.1a"><mi id="S3.SS1.SSS0.Px2.p3.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.2.m2.1b"><ci id="S3.SS1.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.2.m2.1d">italic_i</annotation></semantics></math>, and <math alttext="\sum_{i=1}^{d_{\mathit{ff}}}a^{t}_{(i)}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.3.m3.1"><semantics id="S3.SS1.SSS0.Px2.p3.3.m3.1a"><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.cmml"><msubsup id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.cmml"><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.2.cmml">∑</mo><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.3.cmml">1</mn></mrow><msub id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.3.cmml">𝑓𝑓</mi></msub></msubsup><msubsup id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.cmml"><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.2" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.2.cmml">a</mi><mrow id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.cmml"><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.3.1" stretchy="false" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.cmml">(</mo><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.cmml">i</mi><mo id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.3.2" stretchy="false" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.cmml">)</mo></mrow><mi id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.3" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.3.cmml">t</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2"><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1">superscript</csymbol><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1">subscript</csymbol><sum id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.2"></sum><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3"><eq id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.1"></eq><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.2">𝑖</ci><cn id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.2.3.3">1</cn></apply></apply><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.2">𝑑</ci><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.1.3.3">𝑓𝑓</ci></apply></apply><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2">subscript</csymbol><apply id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.2">𝑎</ci><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.2.2.2.3">𝑡</ci></apply><ci id="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.3.m3.1.1.1.1">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.3.m3.1c">\sum_{i=1}^{d_{\mathit{ff}}}a^{t}_{(i)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.3.m3.1d">∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ( italic_i ) end_POSTSUBSCRIPT</annotation></semantics></math> is the total activation of all neurons for an FFN layer.
<math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.4.m4.1"><semantics id="S3.SS1.SSS0.Px2.p3.4.m4.1a"><mi id="S3.SS1.SSS0.Px2.p3.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.4.m4.1b"><ci id="S3.SS1.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.4.m4.1d">italic_k</annotation></semantics></math> is a threshold factor, varying from 0% to 100%, indicating the extent of neuron activation deemed necessary for specialization.
A lower <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.5.m5.1"><semantics id="S3.SS1.SSS0.Px2.p3.5.m5.1a"><mi id="S3.SS1.SSS0.Px2.p3.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p3.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.5.m5.1b"><ci id="S3.SS1.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.5.m5.1d">italic_k</annotation></semantics></math> value results in higher sparsity in specialized neurons; <math alttext="k=0" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.6.m6.1"><semantics id="S3.SS1.SSS0.Px2.p3.6.m6.1a"><mrow id="S3.SS1.SSS0.Px2.p3.6.m6.1.1" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.2" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.2.cmml">k</mi><mo id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.1" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.3" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.6.m6.1b"><apply id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1"><eq id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.1"></eq><ci id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.2">𝑘</ci><cn id="S3.SS1.SSS0.Px2.p3.6.m6.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p3.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.6.m6.1c">k=0</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.6.m6.1d">italic_k = 0</annotation></semantics></math> means no neuron will be involved, while <math alttext="k=100" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.7.m7.1"><semantics id="S3.SS1.SSS0.Px2.p3.7.m7.1a"><mrow id="S3.SS1.SSS0.Px2.p3.7.m7.1.1" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2.cmml">k</mi><mo id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.1" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.1.cmml">=</mo><mn id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.7.m7.1b"><apply id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1"><eq id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.1"></eq><ci id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.2">𝑘</ci><cn id="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px2.p3.7.m7.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.7.m7.1c">k=100</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.7.m7.1d">italic_k = 100</annotation></semantics></math> fully engages all neurons, the same as utilizing the full capacity of the original model.
This dynamic approach emphasizes the collective significance of neuron activations up to a factor of <math alttext="k" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p3.8.m8.1"><semantics id="S3.SS1.SSS0.Px2.p3.8.m8.1a"><mi id="S3.SS1.SSS0.Px2.p3.8.m8.1.1" xref="S3.SS1.SSS0.Px2.p3.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p3.8.m8.1b"><ci id="S3.SS1.SSS0.Px2.p3.8.m8.1.1.cmml" xref="S3.SS1.SSS0.Px2.p3.8.m8.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p3.8.m8.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p3.8.m8.1d">italic_k</annotation></semantics></math>.
In the end, we repeat these processes to obtain the specialized neurons of all FFN layers for each task.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S3.F1.g1" src="extracted/2404.11201v1/imgs/dec_0.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the first decoder FFN layer across all out-of-English translation directions to measure the degree of overlap. Darker cells indicate stronger overlaps, with the color threshold set from 40 to 80 to improve visibility.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Analysis on EC30</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section, we describe how we identify specialized neurons on EC30 <cite class="ltx_cite ltx_citemacro_cite">Tan and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib34" title="">2023</a>)</cite>, where we train an MMT model covering all directions.
EC30 is a multilingual translation benchmark that is carefully designed to consider diverse linguistic properties and real-world data distributions.
It collects high to low-resource languages, resulting in 30 diverse languages from 5 language families, allowing us to connect our observations with linguistic properties easily.
See Sections <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5" title="5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a> for details on data and models.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Neuron Overlaps Reflect Language Proximity</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.6">We identified specialized neurons following Section <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1" title="3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>, while setting the cumulative activation threshold <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_k</annotation></semantics></math> at 95%.
This implies that the set of specialized neurons covers approximately 95% of the total activations.
Intuitively, two similar tasks should have a high overlap between their specialized neuron sets.
Therefore, we examined the overlaps among specialized neurons across different tasks by calculating the Intersection over Union (IoU) scores: For task <math alttext="t_{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><msub id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml">t</mi><mi id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">t_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="t_{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msub id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml">t</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.2">𝑡</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">t_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">italic_t start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, with specialized neurons denoted as sets <math alttext="S^{i}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><msup id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml"><mi id="S3.SS2.SSS1.p1.4.m4.1.1.2" xref="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.4.m4.1.1.3" xref="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml">i</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><apply id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">S^{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_S start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="S^{j}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m5.1"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><msup id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml">j</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3">𝑗</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">S^{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.5.m5.1d">italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT</annotation></semantics></math>, their overlap is quantified by <math alttext="\text{IoU}(S^{i},S^{j})=\frac{|S^{i}\cap S^{j}|}{|S^{i}\cup S^{j}|}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.6.m6.4"><semantics id="S3.SS2.SSS1.p1.6.m6.4a"><mrow id="S3.SS2.SSS1.p1.6.m6.4.4" xref="S3.SS2.SSS1.p1.6.m6.4.4.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.4.4.2" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.cmml"><mtext id="S3.SS2.SSS1.p1.6.m6.4.4.2.4" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.4a.cmml">IoU</mtext><mo id="S3.SS2.SSS1.p1.6.m6.4.4.2.3" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.3.cmml">⁢</mo><mrow id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.3.cmml"><mo id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.3" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.3.cmml">(</mo><msup id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.3.cmml">i</mi></msup><mo id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.4" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.3.cmml">,</mo><msup id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.2" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.3" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.3.cmml">j</mi></msup><mo id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.5" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.SS2.SSS1.p1.6.m6.4.4.3" xref="S3.SS2.SSS1.p1.6.m6.4.4.3.cmml">=</mo><mfrac id="S3.SS2.SSS1.p1.6.m6.2.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.cmml"><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.2.cmml"><mo id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.2.1.cmml">|</mo><mrow id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.cmml"><msup id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.3.cmml">i</mi></msup><mo id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml">∩</mo><msup id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.3.cmml">j</mi></msup></mrow><mo id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S3.SS2.SSS1.p1.6.m6.2.2.2.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.cmml"><mo id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.2" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.cmml">|</mo><mrow id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.cmml"><msup id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.3.cmml">i</mi></msup><mo id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.1" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.1.cmml">∪</mo><msup id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.2" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.2.cmml">S</mi><mi id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.3" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.3.cmml">j</mi></msup></mrow><mo id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.3" stretchy="false" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.4b"><apply id="S3.SS2.SSS1.p1.6.m6.4.4.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4"><eq id="S3.SS2.SSS1.p1.6.m6.4.4.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.3"></eq><apply id="S3.SS2.SSS1.p1.6.m6.4.4.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2"><times id="S3.SS2.SSS1.p1.6.m6.4.4.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.3"></times><ci id="S3.SS2.SSS1.p1.6.m6.4.4.2.4a.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.4"><mtext id="S3.SS2.SSS1.p1.6.m6.4.4.2.4.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.4">IoU</mtext></ci><interval closure="open" id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2"><apply id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.3.3.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.4.4.2.2.2.2.3">𝑗</ci></apply></interval></apply><apply id="S3.SS2.SSS1.p1.6.m6.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2"><divide id="S3.SS2.SSS1.p1.6.m6.2.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2"></divide><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1"><abs id="S3.SS2.SSS1.p1.6.m6.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.2"></abs><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1"><intersect id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.1"></intersect><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1"><abs id="S3.SS2.SSS1.p1.6.m6.2.2.2.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.2"></abs><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1"><union id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.1"></union><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.2">𝑆</ci><ci id="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.2.2.2.1.1.3.3">𝑗</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.4c">\text{IoU}(S^{i},S^{j})=\frac{|S^{i}\cap S^{j}|}{|S^{i}\cup S^{j}|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.6.m6.4d">IoU ( italic_S start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) = divide start_ARG | italic_S start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ∩ italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT | end_ARG start_ARG | italic_S start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ∪ italic_S start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT | end_ARG</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.F1" title="Figure 1 ‣ Neuron Selection. ‣ 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> shows the IoU scores for specialized neurons across different tasks in the first decoder layer.
Figures for the other layers can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS6" title="A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.6</span></a>.
We first note a structural separation of neuron overlaps, indicating a preference for language specificity.
Notably, neuron overlap across language families is relatively low, a trend more pronounced in encoder layers (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F6" title="Figure 6 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>).
Secondly, this structural distinction generally correlates with language proximity as indicated by the clustering pattern in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.F1" title="Figure 1 ‣ Neuron Selection. ‣ 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.
This implies that target languages from the same family are more likely to activate similar neurons in the decoder, even when they use different writing systems, e.g., Arabic (ar) and Hebrew (he).
Overlaps also show linguistic traits beyond family ties, exemplified by notable overlaps between Maltese (mt) and languages in the Romance family due to vocabulary borrowing.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S3.F2.g1" src="extracted/2404.11201v1/imgs/iou_enc_dec_new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Progression of distribution of IoU scores for specialized neurons across layers on the EC30 dataset.
The scores are measured for different source and target languages in the Encoder and Decoder, respectively.
</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>The Progression of Neuron Overlaps</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To analyze how specialized neuron overlaps across tasks evolve within the model, we visualize the IoU score distribution across layers in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.F2" title="Figure 2 ‣ 3.2.1 Neuron Overlaps Reflect Language Proximity ‣ 3.2 Analysis on EC30 ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.
For each layer, we compute the pair-wise IoU scores between all possible tasks and then show them in a distribution.
Overall, we observe that from shallow to deeper layers, structural distinctions intensify in the decoder (decreasing IoU scores) and weaken in the encoder (increasing IoU scores).</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">On the one hand, all neuron overlaps increase as we move up the encoder, regardless of whether these tasks are similar or not.
This observation may suggest that the neurons in the encoder become more language-agnostic, as they attempt to map different scripts into semantic concepts.
As for the Decoder, the model presents intensified modularity in terms of overlaps of specialized neurons.
This can be seen by all overlaps becoming much smaller, indicating that the neurons behave more separately.
Additionally, we found the progression of neuron overlaps is similar to the evolution of multilingual representation: embedding gets closer in the encoder and becomes more dissimilar in the decoder <cite class="ltx_cite ltx_citemacro_cite">Kudugunta et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib18" title="">2019</a>)</cite>.
Our observations, highlighting the inherent features of the multilingual translation model, occur without modifying the network’s outputs or parameters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Neuron Specialization Training</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our neuron structural analysis showed the presence of specialized neurons within the Feed-Forward Network (FFN) layers of a multilingual network.
We hypothesize that continuously training the model, while leveraging these specialized neurons’ intrinsic modular features, can further enhance task-specific performance.
Building on this hypothesis, we propose <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Neuron Specialization</span>, an approach that leverages specialized neurons to modularize the FFN layers in a task-specific manner.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Vanilla Feed-Forward Network</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.6">We first revisit the Feed-Forward Network (FFN) in Transformer <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib35" title="">2017</a>)</cite>.
The FFN, crucial to our analysis, consists of two linear layers (fc1 and fc2) with a <math alttext="\mathit{ReLU}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">𝑅𝑒𝐿𝑈</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑅𝑒𝐿𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathit{ReLU}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_ReLU</annotation></semantics></math> activation function.
Specifically, the FFN block first processes the hidden state <math alttext="H\in\mathbb{R}^{n\times d}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">H</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1.3.2" xref="S4.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p1.2.m2.1.1.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1.3.3.2" xref="S4.SS1.p1.2.m2.1.1.3.3.2.cmml">n</mi><mo id="S4.SS1.p1.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S4.SS1.p1.2.m2.1.1.3.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><in id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></in><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝐻</ci><apply id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S4.SS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3"><times id="S4.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.2">𝑛</ci><ci id="S4.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">H\in\mathbb{R}^{n\times d}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_H ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> (<math alttext="n" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_n</annotation></semantics></math> denotes number of tokens in a batch) through fc1 layer <math alttext="W_{1}\in\mathbb{R}^{d\times d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><msub id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2.2" xref="S4.SS1.p1.4.m4.1.1.2.2.cmml">W</mi><mn id="S4.SS1.p1.4.m4.1.1.2.3" xref="S4.SS1.p1.4.m4.1.1.2.3.cmml">1</mn></msub><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.2.cmml">d</mi><mo id="S4.SS1.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.p1.4.m4.1.1.3.3.1.cmml">×</mo><msub id="S4.SS1.p1.4.m4.1.1.3.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.3.2.cmml">d</mi><mi id="S4.SS1.p1.4.m4.1.1.3.3.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.3.3.cmml">𝑓𝑓</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><in id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></in><apply id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2">𝑊</ci><cn id="S4.SS1.p1.4.m4.1.1.2.3.cmml" type="integer" xref="S4.SS1.p1.4.m4.1.1.2.3">1</cn></apply><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">ℝ</ci><apply id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"><times id="S4.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.2">𝑑</ci><apply id="S4.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.3.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3.2">𝑑</ci><ci id="S4.SS1.p1.4.m4.1.1.3.3.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3.3">𝑓𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">W_{1}\in\mathbb{R}^{d\times d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.
Then the output is passed to <math alttext="\mathit{ReLU}" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mi id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">𝑅𝑒𝐿𝑈</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><ci id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">𝑅𝑒𝐿𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\mathit{ReLU}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_ReLU</annotation></semantics></math> and the fc2 layer <math alttext="W_{2}" class="ltx_Math" display="inline" id="S4.SS1.p1.6.m6.1"><semantics id="S4.SS1.p1.6.m6.1a"><msub id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">W</mi><mn id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">𝑊</ci><cn id="S4.SS1.p1.6.m6.1.1.3.cmml" type="integer" xref="S4.SS1.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">W_{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.6.m6.1d">italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, as formalized in Eq <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S4.E2" title="In 4.1 Vanilla Feed-Forward Network ‣ 4 Neuron Specialization Training ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, with bias terms omitted.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathrm{FFN}(H)=\mathrm{ReLU}(HW_{1})\,W_{2}." class="ltx_Math" display="block" id="S4.E2.m1.2"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.2.1" xref="S4.E2.m1.2.2.1.1.cmml"><mrow id="S4.E2.m1.2.2.1.1" xref="S4.E2.m1.2.2.1.1.cmml"><mrow id="S4.E2.m1.2.2.1.1.3" xref="S4.E2.m1.2.2.1.1.3.cmml"><mi id="S4.E2.m1.2.2.1.1.3.2" xref="S4.E2.m1.2.2.1.1.3.2.cmml">FFN</mi><mo id="S4.E2.m1.2.2.1.1.3.1" xref="S4.E2.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S4.E2.m1.2.2.1.1.3.3.2" xref="S4.E2.m1.2.2.1.1.3.cmml"><mo id="S4.E2.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S4.E2.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">H</mi><mo id="S4.E2.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S4.E2.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E2.m1.2.2.1.1.2" xref="S4.E2.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E2.m1.2.2.1.1.1" xref="S4.E2.m1.2.2.1.1.1.cmml"><mi id="S4.E2.m1.2.2.1.1.1.3" xref="S4.E2.m1.2.2.1.1.1.3.cmml">ReLU</mi><mo id="S4.E2.m1.2.2.1.1.1.2" xref="S4.E2.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.2.2.1.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E2.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.2.2.1.1.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.2.2.1.1.1.1.1.1.2" xref="S4.E2.m1.2.2.1.1.1.1.1.1.2.cmml">H</mi><mo id="S4.E2.m1.2.2.1.1.1.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.1.1.1.1.cmml">⁢</mo><msub id="S4.E2.m1.2.2.1.1.1.1.1.1.3" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.2.2.1.1.1.1.1.1.3.2" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S4.E2.m1.2.2.1.1.1.1.1.1.3.3" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S4.E2.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E2.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E2.m1.2.2.1.1.1.2a" lspace="0.170em" xref="S4.E2.m1.2.2.1.1.1.2.cmml">⁢</mo><msub id="S4.E2.m1.2.2.1.1.1.4" xref="S4.E2.m1.2.2.1.1.1.4.cmml"><mi id="S4.E2.m1.2.2.1.1.1.4.2" xref="S4.E2.m1.2.2.1.1.1.4.2.cmml">W</mi><mn id="S4.E2.m1.2.2.1.1.1.4.3" xref="S4.E2.m1.2.2.1.1.1.4.3.cmml">2</mn></msub></mrow></mrow><mo id="S4.E2.m1.2.2.1.2" lspace="0em" xref="S4.E2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.2.1.1.cmml" xref="S4.E2.m1.2.2.1"><eq id="S4.E2.m1.2.2.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.2"></eq><apply id="S4.E2.m1.2.2.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.3"><times id="S4.E2.m1.2.2.1.1.3.1.cmml" xref="S4.E2.m1.2.2.1.1.3.1"></times><ci id="S4.E2.m1.2.2.1.1.3.2.cmml" xref="S4.E2.m1.2.2.1.1.3.2">FFN</ci><ci id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">𝐻</ci></apply><apply id="S4.E2.m1.2.2.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1"><times id="S4.E2.m1.2.2.1.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.1.2"></times><ci id="S4.E2.m1.2.2.1.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.1.3">ReLU</ci><apply id="S4.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1"><times id="S4.E2.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.1.1"></times><ci id="S4.E2.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.1.2">𝐻</ci><apply id="S4.E2.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.2.2.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3.2">𝑊</ci><cn id="S4.E2.m1.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.E2.m1.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S4.E2.m1.2.2.1.1.1.4.cmml" xref="S4.E2.m1.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.1.1.1.4.1.cmml" xref="S4.E2.m1.2.2.1.1.1.4">subscript</csymbol><ci id="S4.E2.m1.2.2.1.1.1.4.2.cmml" xref="S4.E2.m1.2.2.1.1.1.4.2">𝑊</ci><cn id="S4.E2.m1.2.2.1.1.1.4.3.cmml" type="integer" xref="S4.E2.m1.2.2.1.1.1.4.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">\mathrm{FFN}(H)=\mathrm{ReLU}(HW_{1})\,W_{2}.</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.2d">roman_FFN ( italic_H ) = roman_ReLU ( italic_H italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Specializing Task-Specific FFN</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.10">Next, we investigate continuous training upon a subset of specialized parameters within FFN for each task.
Given a pre-trained vanilla multilingual Transformer model with tags to identify the language pairs, e.g., <cite class="ltx_cite ltx_citemacro_citet">Johnson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib16" title="">2017</a>)</cite>, we can derive specialized neuron set <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><msubsup id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2.2" xref="S4.SS2.p1.1.m1.1.1.2.2.cmml">S</mi><mi id="S4.SS2.p1.1.m1.1.1.2.3" xref="S4.SS2.p1.1.m1.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">superscript</csymbol><apply id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.2.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2.2">𝑆</ci><ci id="S4.SS2.p1.1.m1.1.1.2.3.cmml" xref="S4.SS2.p1.1.m1.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> for each layer of a task task<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We treat each translation direction as a distinct task.</span></span></span> <math alttext="t" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mi id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_t</annotation></semantics></math> and threshold <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.3.m3.1"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.3.m3.1d">italic_k</annotation></semantics></math> following the method outlined in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1" title="3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>.
Then, we derive a boolean mask vector <span class="ltx_text" id="S4.SS2.p1.4.1"><math alttext="m_{k}^{t}\in\{0,1\}^{d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="S4.SS2.p1.4.1.m1.2"><semantics id="S4.SS2.p1.4.1.m1.2a"><mrow id="S4.SS2.p1.4.1.m1.2.3" xref="S4.SS2.p1.4.1.m1.2.3.cmml"><msubsup id="S4.SS2.p1.4.1.m1.2.3.2" xref="S4.SS2.p1.4.1.m1.2.3.2.cmml"><mi id="S4.SS2.p1.4.1.m1.2.3.2.2.2" xref="S4.SS2.p1.4.1.m1.2.3.2.2.2.cmml">m</mi><mi id="S4.SS2.p1.4.1.m1.2.3.2.2.3" xref="S4.SS2.p1.4.1.m1.2.3.2.2.3.cmml">k</mi><mi id="S4.SS2.p1.4.1.m1.2.3.2.3" xref="S4.SS2.p1.4.1.m1.2.3.2.3.cmml">t</mi></msubsup><mo id="S4.SS2.p1.4.1.m1.2.3.1" xref="S4.SS2.p1.4.1.m1.2.3.1.cmml">∈</mo><msup id="S4.SS2.p1.4.1.m1.2.3.3" xref="S4.SS2.p1.4.1.m1.2.3.3.cmml"><mrow id="S4.SS2.p1.4.1.m1.2.3.3.2.2" xref="S4.SS2.p1.4.1.m1.2.3.3.2.1.cmml"><mo id="S4.SS2.p1.4.1.m1.2.3.3.2.2.1" stretchy="false" xref="S4.SS2.p1.4.1.m1.2.3.3.2.1.cmml">{</mo><mn id="S4.SS2.p1.4.1.m1.1.1" xref="S4.SS2.p1.4.1.m1.1.1.cmml">0</mn><mo id="S4.SS2.p1.4.1.m1.2.3.3.2.2.2" xref="S4.SS2.p1.4.1.m1.2.3.3.2.1.cmml">,</mo><mn id="S4.SS2.p1.4.1.m1.2.2" xref="S4.SS2.p1.4.1.m1.2.2.cmml">1</mn><mo id="S4.SS2.p1.4.1.m1.2.3.3.2.2.3" stretchy="false" xref="S4.SS2.p1.4.1.m1.2.3.3.2.1.cmml">}</mo></mrow><msub id="S4.SS2.p1.4.1.m1.2.3.3.3" xref="S4.SS2.p1.4.1.m1.2.3.3.3.cmml"><mi id="S4.SS2.p1.4.1.m1.2.3.3.3.2" xref="S4.SS2.p1.4.1.m1.2.3.3.3.2.cmml">d</mi><mi id="S4.SS2.p1.4.1.m1.2.3.3.3.3" xref="S4.SS2.p1.4.1.m1.2.3.3.3.3.cmml">𝑓𝑓</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.1.m1.2b"><apply id="S4.SS2.p1.4.1.m1.2.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3"><in id="S4.SS2.p1.4.1.m1.2.3.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.1"></in><apply id="S4.SS2.p1.4.1.m1.2.3.2.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S4.SS2.p1.4.1.m1.2.3.2.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2">superscript</csymbol><apply id="S4.SS2.p1.4.1.m1.2.3.2.2.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S4.SS2.p1.4.1.m1.2.3.2.2.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2">subscript</csymbol><ci id="S4.SS2.p1.4.1.m1.2.3.2.2.2.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2.2.2">𝑚</ci><ci id="S4.SS2.p1.4.1.m1.2.3.2.2.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.4.1.m1.2.3.2.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3.2.3">𝑡</ci></apply><apply id="S4.SS2.p1.4.1.m1.2.3.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.4.1.m1.2.3.3.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3">superscript</csymbol><set id="S4.SS2.p1.4.1.m1.2.3.3.2.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3.2.2"><cn id="S4.SS2.p1.4.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.4.1.m1.1.1">0</cn><cn id="S4.SS2.p1.4.1.m1.2.2.cmml" type="integer" xref="S4.SS2.p1.4.1.m1.2.2">1</cn></set><apply id="S4.SS2.p1.4.1.m1.2.3.3.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.4.1.m1.2.3.3.3.1.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3.3">subscript</csymbol><ci id="S4.SS2.p1.4.1.m1.2.3.3.3.2.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3.3.2">𝑑</ci><ci id="S4.SS2.p1.4.1.m1.2.3.3.3.3.cmml" xref="S4.SS2.p1.4.1.m1.2.3.3.3.3">𝑓𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.1.m1.2c">m_{k}^{t}\in\{0,1\}^{d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.4.1.m1.2d">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></span> from <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.5.m4.1"><semantics id="S4.SS2.p1.5.m4.1a"><msubsup id="S4.SS2.p1.5.m4.1.1" xref="S4.SS2.p1.5.m4.1.1.cmml"><mi id="S4.SS2.p1.5.m4.1.1.2.2" xref="S4.SS2.p1.5.m4.1.1.2.2.cmml">S</mi><mi id="S4.SS2.p1.5.m4.1.1.2.3" xref="S4.SS2.p1.5.m4.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p1.5.m4.1.1.3" xref="S4.SS2.p1.5.m4.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m4.1b"><apply id="S4.SS2.p1.5.m4.1.1.cmml" xref="S4.SS2.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m4.1.1.1.cmml" xref="S4.SS2.p1.5.m4.1.1">superscript</csymbol><apply id="S4.SS2.p1.5.m4.1.1.2.cmml" xref="S4.SS2.p1.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m4.1.1.2.1.cmml" xref="S4.SS2.p1.5.m4.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m4.1.1.2.2.cmml" xref="S4.SS2.p1.5.m4.1.1.2.2">𝑆</ci><ci id="S4.SS2.p1.5.m4.1.1.2.3.cmml" xref="S4.SS2.p1.5.m4.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.5.m4.1.1.3.cmml" xref="S4.SS2.p1.5.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m4.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.5.m4.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, where the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p1.6.m5.1"><semantics id="S4.SS2.p1.6.m5.1a"><mi id="S4.SS2.p1.6.m5.1.1" xref="S4.SS2.p1.6.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m5.1b"><ci id="S4.SS2.p1.6.m5.1.1.cmml" xref="S4.SS2.p1.6.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.6.m5.1d">italic_i</annotation></semantics></math>-th element in <math alttext="m_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.7.m6.1"><semantics id="S4.SS2.p1.7.m6.1a"><msubsup id="S4.SS2.p1.7.m6.1.1" xref="S4.SS2.p1.7.m6.1.1.cmml"><mi id="S4.SS2.p1.7.m6.1.1.2.2" xref="S4.SS2.p1.7.m6.1.1.2.2.cmml">m</mi><mi id="S4.SS2.p1.7.m6.1.1.2.3" xref="S4.SS2.p1.7.m6.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p1.7.m6.1.1.3" xref="S4.SS2.p1.7.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.7.m6.1b"><apply id="S4.SS2.p1.7.m6.1.1.cmml" xref="S4.SS2.p1.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m6.1.1.1.cmml" xref="S4.SS2.p1.7.m6.1.1">superscript</csymbol><apply id="S4.SS2.p1.7.m6.1.1.2.cmml" xref="S4.SS2.p1.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.7.m6.1.1.2.1.cmml" xref="S4.SS2.p1.7.m6.1.1">subscript</csymbol><ci id="S4.SS2.p1.7.m6.1.1.2.2.cmml" xref="S4.SS2.p1.7.m6.1.1.2.2">𝑚</ci><ci id="S4.SS2.p1.7.m6.1.1.2.3.cmml" xref="S4.SS2.p1.7.m6.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.7.m6.1.1.3.cmml" xref="S4.SS2.p1.7.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.7.m6.1c">m_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.7.m6.1d">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is set to 1 only when <math alttext="i\in S_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.8.m7.1"><semantics id="S4.SS2.p1.8.m7.1a"><mrow id="S4.SS2.p1.8.m7.1.1" xref="S4.SS2.p1.8.m7.1.1.cmml"><mi id="S4.SS2.p1.8.m7.1.1.2" xref="S4.SS2.p1.8.m7.1.1.2.cmml">i</mi><mo id="S4.SS2.p1.8.m7.1.1.1" xref="S4.SS2.p1.8.m7.1.1.1.cmml">∈</mo><msubsup id="S4.SS2.p1.8.m7.1.1.3" xref="S4.SS2.p1.8.m7.1.1.3.cmml"><mi id="S4.SS2.p1.8.m7.1.1.3.2.2" xref="S4.SS2.p1.8.m7.1.1.3.2.2.cmml">S</mi><mi id="S4.SS2.p1.8.m7.1.1.3.2.3" xref="S4.SS2.p1.8.m7.1.1.3.2.3.cmml">k</mi><mi id="S4.SS2.p1.8.m7.1.1.3.3" xref="S4.SS2.p1.8.m7.1.1.3.3.cmml">t</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.8.m7.1b"><apply id="S4.SS2.p1.8.m7.1.1.cmml" xref="S4.SS2.p1.8.m7.1.1"><in id="S4.SS2.p1.8.m7.1.1.1.cmml" xref="S4.SS2.p1.8.m7.1.1.1"></in><ci id="S4.SS2.p1.8.m7.1.1.2.cmml" xref="S4.SS2.p1.8.m7.1.1.2">𝑖</ci><apply id="S4.SS2.p1.8.m7.1.1.3.cmml" xref="S4.SS2.p1.8.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.8.m7.1.1.3.1.cmml" xref="S4.SS2.p1.8.m7.1.1.3">superscript</csymbol><apply id="S4.SS2.p1.8.m7.1.1.3.2.cmml" xref="S4.SS2.p1.8.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.8.m7.1.1.3.2.1.cmml" xref="S4.SS2.p1.8.m7.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.8.m7.1.1.3.2.2.cmml" xref="S4.SS2.p1.8.m7.1.1.3.2.2">𝑆</ci><ci id="S4.SS2.p1.8.m7.1.1.3.2.3.cmml" xref="S4.SS2.p1.8.m7.1.1.3.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.8.m7.1.1.3.3.cmml" xref="S4.SS2.p1.8.m7.1.1.3.3">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.8.m7.1c">i\in S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.8.m7.1d">italic_i ∈ italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>, and apply it to control parameter updates.
Specifically, we broadcast <math alttext="m_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p1.9.m8.1"><semantics id="S4.SS2.p1.9.m8.1a"><msubsup id="S4.SS2.p1.9.m8.1.1" xref="S4.SS2.p1.9.m8.1.1.cmml"><mi id="S4.SS2.p1.9.m8.1.1.2.2" xref="S4.SS2.p1.9.m8.1.1.2.2.cmml">m</mi><mi id="S4.SS2.p1.9.m8.1.1.2.3" xref="S4.SS2.p1.9.m8.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p1.9.m8.1.1.3" xref="S4.SS2.p1.9.m8.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.9.m8.1b"><apply id="S4.SS2.p1.9.m8.1.1.cmml" xref="S4.SS2.p1.9.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m8.1.1.1.cmml" xref="S4.SS2.p1.9.m8.1.1">superscript</csymbol><apply id="S4.SS2.p1.9.m8.1.1.2.cmml" xref="S4.SS2.p1.9.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.9.m8.1.1.2.1.cmml" xref="S4.SS2.p1.9.m8.1.1">subscript</csymbol><ci id="S4.SS2.p1.9.m8.1.1.2.2.cmml" xref="S4.SS2.p1.9.m8.1.1.2.2">𝑚</ci><ci id="S4.SS2.p1.9.m8.1.1.2.3.cmml" xref="S4.SS2.p1.9.m8.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p1.9.m8.1.1.3.cmml" xref="S4.SS2.p1.9.m8.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.9.m8.1c">m_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.9.m8.1d">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and perform Hadamard Product with <math alttext="W_{1}" class="ltx_Math" display="inline" id="S4.SS2.p1.10.m9.1"><semantics id="S4.SS2.p1.10.m9.1a"><msub id="S4.SS2.p1.10.m9.1.1" xref="S4.SS2.p1.10.m9.1.1.cmml"><mi id="S4.SS2.p1.10.m9.1.1.2" xref="S4.SS2.p1.10.m9.1.1.2.cmml">W</mi><mn id="S4.SS2.p1.10.m9.1.1.3" xref="S4.SS2.p1.10.m9.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.10.m9.1b"><apply id="S4.SS2.p1.10.m9.1.1.cmml" xref="S4.SS2.p1.10.m9.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.10.m9.1.1.1.cmml" xref="S4.SS2.p1.10.m9.1.1">subscript</csymbol><ci id="S4.SS2.p1.10.m9.1.1.2.cmml" xref="S4.SS2.p1.10.m9.1.1.2">𝑊</ci><cn id="S4.SS2.p1.10.m9.1.1.3.cmml" type="integer" xref="S4.SS2.p1.10.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.10.m9.1c">W_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.10.m9.1d">italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> in each FFN layer as follows:</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\textit{FFN}(H)=\mathit{ReLU}(H(m_{k}^{t}\odot W_{1}))W_{2}." class="ltx_Math" display="block" id="S4.E3.m1.2"><semantics id="S4.E3.m1.2a"><mrow id="S4.E3.m1.2.2.1" xref="S4.E3.m1.2.2.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1" xref="S4.E3.m1.2.2.1.1.cmml"><mrow id="S4.E3.m1.2.2.1.1.3" xref="S4.E3.m1.2.2.1.1.3.cmml"><mtext class="ltx_mathvariant_italic" id="S4.E3.m1.2.2.1.1.3.2" xref="S4.E3.m1.2.2.1.1.3.2a.cmml">FFN</mtext><mo id="S4.E3.m1.2.2.1.1.3.1" xref="S4.E3.m1.2.2.1.1.3.1.cmml">⁢</mo><mrow id="S4.E3.m1.2.2.1.1.3.3.2" xref="S4.E3.m1.2.2.1.1.3.cmml"><mo id="S4.E3.m1.2.2.1.1.3.3.2.1" stretchy="false" xref="S4.E3.m1.2.2.1.1.3.cmml">(</mo><mi id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml">H</mi><mo id="S4.E3.m1.2.2.1.1.3.3.2.2" stretchy="false" xref="S4.E3.m1.2.2.1.1.3.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.2" xref="S4.E3.m1.2.2.1.1.2.cmml">=</mo><mrow id="S4.E3.m1.2.2.1.1.1" xref="S4.E3.m1.2.2.1.1.1.cmml"><mi id="S4.E3.m1.2.2.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.3.cmml">𝑅𝑒𝐿𝑈</mi><mo id="S4.E3.m1.2.2.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.2.2.1.1.1.1.1.2" stretchy="false" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml">H</mi><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml">m</mi><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml">k</mi><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml">t</mi></msubsup><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">⊙</mo><msub id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml">W</mi><mn id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m1.2.2.1.1.1.1.1.3" stretchy="false" xref="S4.E3.m1.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.E3.m1.2.2.1.1.1.2a" xref="S4.E3.m1.2.2.1.1.1.2.cmml">⁢</mo><msub id="S4.E3.m1.2.2.1.1.1.4" xref="S4.E3.m1.2.2.1.1.1.4.cmml"><mi id="S4.E3.m1.2.2.1.1.1.4.2" xref="S4.E3.m1.2.2.1.1.1.4.2.cmml">W</mi><mn id="S4.E3.m1.2.2.1.1.1.4.3" xref="S4.E3.m1.2.2.1.1.1.4.3.cmml">2</mn></msub></mrow></mrow><mo id="S4.E3.m1.2.2.1.2" lspace="0em" xref="S4.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.2b"><apply id="S4.E3.m1.2.2.1.1.cmml" xref="S4.E3.m1.2.2.1"><eq id="S4.E3.m1.2.2.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.2"></eq><apply id="S4.E3.m1.2.2.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.3"><times id="S4.E3.m1.2.2.1.1.3.1.cmml" xref="S4.E3.m1.2.2.1.1.3.1"></times><ci id="S4.E3.m1.2.2.1.1.3.2a.cmml" xref="S4.E3.m1.2.2.1.1.3.2"><mtext class="ltx_mathvariant_italic" id="S4.E3.m1.2.2.1.1.3.2.cmml" xref="S4.E3.m1.2.2.1.1.3.2">FFN</mtext></ci><ci id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1">𝐻</ci></apply><apply id="S4.E3.m1.2.2.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1"><times id="S4.E3.m1.2.2.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.2"></times><ci id="S4.E3.m1.2.2.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.3">𝑅𝑒𝐿𝑈</ci><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1"><times id="S4.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.2"></times><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.3">𝐻</ci><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.1">direct-product</csymbol><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.2">𝑚</ci><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.2.3">𝑘</ci></apply><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.2">𝑊</ci><cn id="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S4.E3.m1.2.2.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply><apply id="S4.E3.m1.2.2.1.1.1.4.cmml" xref="S4.E3.m1.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S4.E3.m1.2.2.1.1.1.4.1.cmml" xref="S4.E3.m1.2.2.1.1.1.4">subscript</csymbol><ci id="S4.E3.m1.2.2.1.1.1.4.2.cmml" xref="S4.E3.m1.2.2.1.1.1.4.2">𝑊</ci><cn id="S4.E3.m1.2.2.1.1.1.4.3.cmml" type="integer" xref="S4.E3.m1.2.2.1.1.1.4.3">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.2c">\textit{FFN}(H)=\mathit{ReLU}(H(m_{k}^{t}\odot W_{1}))W_{2}.</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.2d">FFN ( italic_H ) = italic_ReLU ( italic_H ( italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⊙ italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ) italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.5"><math alttext="m_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.1.m1.1"><semantics id="S4.SS2.p3.1.m1.1a"><msubsup id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.m1.1.1.2.2" xref="S4.SS2.p3.1.m1.1.1.2.2.cmml">m</mi><mi id="S4.SS2.p3.1.m1.1.1.2.3" xref="S4.SS2.p3.1.m1.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p3.1.m1.1.1.3" xref="S4.SS2.p3.1.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">superscript</csymbol><apply id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.1.m1.1.1.2.1.cmml" xref="S4.SS2.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p3.1.m1.1.1.2.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2.2">𝑚</ci><ci id="S4.SS2.p3.1.m1.1.1.2.3.cmml" xref="S4.SS2.p3.1.m1.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p3.1.m1.1.1.3.cmml" xref="S4.SS2.p3.1.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">m_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.1.m1.1d">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> plays the role of controlling parameter update, where the boolean value of <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p3.2.m2.1"><semantics id="S4.SS2.p3.2.m2.1a"><mi id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><ci id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.2.m2.1d">italic_i</annotation></semantics></math>-th element in <math alttext="m_{k}^{t}" class="ltx_Math" display="inline" id="S4.SS2.p3.3.m3.1"><semantics id="S4.SS2.p3.3.m3.1a"><msubsup id="S4.SS2.p3.3.m3.1.1" xref="S4.SS2.p3.3.m3.1.1.cmml"><mi id="S4.SS2.p3.3.m3.1.1.2.2" xref="S4.SS2.p3.3.m3.1.1.2.2.cmml">m</mi><mi id="S4.SS2.p3.3.m3.1.1.2.3" xref="S4.SS2.p3.3.m3.1.1.2.3.cmml">k</mi><mi id="S4.SS2.p3.3.m3.1.1.3" xref="S4.SS2.p3.3.m3.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m3.1b"><apply id="S4.SS2.p3.3.m3.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.1.cmml" xref="S4.SS2.p3.3.m3.1.1">superscript</csymbol><apply id="S4.SS2.p3.3.m3.1.1.2.cmml" xref="S4.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.3.m3.1.1.2.1.cmml" xref="S4.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S4.SS2.p3.3.m3.1.1.2.2.cmml" xref="S4.SS2.p3.3.m3.1.1.2.2">𝑚</ci><ci id="S4.SS2.p3.3.m3.1.1.2.3.cmml" xref="S4.SS2.p3.3.m3.1.1.2.3">𝑘</ci></apply><ci id="S4.SS2.p3.3.m3.1.1.3.cmml" xref="S4.SS2.p3.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m3.1c">m_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.3.m3.1d">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> denotes if the <math alttext="i" class="ltx_Math" display="inline" id="S4.SS2.p3.4.m4.1"><semantics id="S4.SS2.p3.4.m4.1a"><mi id="S4.SS2.p3.4.m4.1.1" xref="S4.SS2.p3.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m4.1b"><ci id="S4.SS2.p3.4.m4.1.1.cmml" xref="S4.SS2.p3.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.4.m4.1d">italic_i</annotation></semantics></math>-th row of parameters in <math alttext="W_{1}" class="ltx_Math" display="inline" id="S4.SS2.p3.5.m5.1"><semantics id="S4.SS2.p3.5.m5.1a"><msub id="S4.SS2.p3.5.m5.1.1" xref="S4.SS2.p3.5.m5.1.1.cmml"><mi id="S4.SS2.p3.5.m5.1.1.2" xref="S4.SS2.p3.5.m5.1.1.2.cmml">W</mi><mn id="S4.SS2.p3.5.m5.1.1.3" xref="S4.SS2.p3.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m5.1b"><apply id="S4.SS2.p3.5.m5.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p3.5.m5.1.1.1.cmml" xref="S4.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p3.5.m5.1.1.2.cmml" xref="S4.SS2.p3.5.m5.1.1.2">𝑊</ci><cn id="S4.SS2.p3.5.m5.1.1.3.cmml" type="integer" xref="S4.SS2.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m5.1c">W_{1}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p3.5.m5.1d">italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> can be updated or not for each layer<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that <math alttext="m_{k}^{t}" class="ltx_Math" display="inline" id="footnote3.m1.1"><semantics id="footnote3.m1.1b"><msubsup id="footnote3.m1.1.1" xref="footnote3.m1.1.1.cmml"><mi id="footnote3.m1.1.1.2.2" xref="footnote3.m1.1.1.2.2.cmml">m</mi><mi id="footnote3.m1.1.1.2.3" xref="footnote3.m1.1.1.2.3.cmml">k</mi><mi id="footnote3.m1.1.1.3" xref="footnote3.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="footnote3.m1.1c"><apply id="footnote3.m1.1.1.cmml" xref="footnote3.m1.1.1"><csymbol cd="ambiguous" id="footnote3.m1.1.1.1.cmml" xref="footnote3.m1.1.1">superscript</csymbol><apply id="footnote3.m1.1.1.2.cmml" xref="footnote3.m1.1.1"><csymbol cd="ambiguous" id="footnote3.m1.1.1.2.1.cmml" xref="footnote3.m1.1.1">subscript</csymbol><ci id="footnote3.m1.1.1.2.2.cmml" xref="footnote3.m1.1.1.2.2">𝑚</ci><ci id="footnote3.m1.1.1.2.3.cmml" xref="footnote3.m1.1.1.2.3">𝑘</ci></apply><ci id="footnote3.m1.1.1.3.cmml" xref="footnote3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote3.m1.1d">m_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="footnote3.m1.1e">italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> is layer-specified, we drop layer indexes hereon for simplicity of notation.</span></span></span> during continues training.
Broadly speaking, our approach selectively updates the first FFN (fc1) weights during back-propagation, tailoring the model more closely towards specific translation tasks and reinforcing neuron separation.
Note that while fc1 is selectively updated for specific tasks, other parameters are universally updated to maintain stability, and the same masking is applied to inference to ensure consistency.
We provide the pseudocode of our method in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS3" title="A.3 Pseudocode of Neuron Specialization ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.3</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we evaluate the capability of our proposed method on small (IWSLT) and large-scale (EC30) multilingual machine translation tasks.
More details of the datasets are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS1" title="A.1 Dataset details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">IWSLT.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">Following <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite>, we constructed an English-centric dataset with eight languages using IWSLT-14, ranging from 89k to 169k in corpus size.
We learned a 30k SentencePiece unigram <cite class="ltx_cite ltx_citemacro_cite">Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib17" title="">2018</a>)</cite> shared vocabulary and applied temperature oversampling with <math alttext="\tau=2" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S5.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">τ</mi><mo id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><ci id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝜏</ci><cn id="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S5.SS1.SSS0.Px1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p1.1.m1.1c">\tau=2</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p1.1.m1.1d">italic_τ = 2</annotation></semantics></math> to balance low-resource languages.
For a more comprehensive evaluation, we replaced the standard test set with Flores-200 <cite class="ltx_cite ltx_citemacro_cite">Costa-jussà et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib9" title="">2022</a>)</cite>, merging <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p1.1.1">devtest</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px1.p1.1.2">test</span>, which offers multiple parallel sentences per source text.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">EC30.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">We further validate our methods using the large-scale EC30 dataset <cite class="ltx_cite ltx_citemacro_cite">Tan and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib34" title="">2023</a>)</cite>, which features 61 million parallel training sentences across 30 English-centric language pairs, representing five language families and various writing systems.
We classify these language pairs into low-resource (=100k), medium-resource (=1M), and high-resource (=5M) categories. Following <cite class="ltx_cite ltx_citemacro_citet">Wu and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib39" title="">2023</a>)</cite>, we build a 128k size shared SentencePiece BPE vocabulary.
Aligning with the original EC30 setups, we use Ntrex-128 <cite class="ltx_cite ltx_citemacro_cite">Federmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib12" title="">2022</a>)</cite> as the validation set. Also, we use Flores-200 (merging <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.1">devtest</span> and <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px2.p1.1.2">test</span>) as test sets for cross-domain evaluation.</p>
</div>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.3" style="width:368.6pt;height:192.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.6pt,11.8pt) scale(0.890668163500674,0.890668163500674) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.2">Language</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.1.1.1"><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T1.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S5.T1.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo id="S5.T1.1.1.1.1.1.m1.1.1.1" xref="S5.T1.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.T1.1.1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1"><times id="S5.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1.1"></times><ci id="S5.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="S5.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.m1.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.1.m1.1d">roman_Δ italic_θ</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.3">Fa</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.4">Pl</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.5">Ar</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.6">He</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.7">Nl</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.8">De</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.9">It</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.1.1.1.10">Es</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S5.T1.1.1.1.11" rowspan="2"><span class="ltx_text" id="S5.T1.1.1.1.11.1">Avg</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.4.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.4.1.1">Size</th>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.2">89k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.3">128k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.4">139k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.5">144k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.6">153k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.7">160k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.8">167k</td>
<td class="ltx_td ltx_align_center" id="S5.T1.3.3.4.1.9">169k</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.5.2" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="11" id="S5.T1.3.3.5.2.1"><span class="ltx_text" id="S5.T1.3.3.5.2.1.1" style="background-color:#E6E6E6;">One-to-Many (O2M / En-X)</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.6.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.6.3.1">mT-small</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.6.3.2">-</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.3">14.5</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.4">9.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.5">12.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.6">13.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.7">17.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.8">20.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.9">17.3</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.10">18.3</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.6.3.11">15.4</td>
</tr>
<tr class="ltx_tr" id="S5.T1.2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.2.2.1">Adapter<sub class="ltx_sub" id="S5.T1.2.2.2.1.1"><span class="ltx_text ltx_font_italic" id="S5.T1.2.2.2.1.1.1">LP</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.2.2.2.2">+67%</th>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.3">+0.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.4">-0.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.5">+0.4</td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.6.1">+1.4</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.7.1">+0.2</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.8">+0.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.9">+0.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.10.1">+0.4</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.2.2.2.11"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.11.1">+0.4</span></td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.7.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.7.4.1">LaSS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.7.4.2">0%</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.3">-2.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.4">0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.5">+0.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.6">+0.7</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.7">-0.2</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.8"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.7.4.8.1">+0.7</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.9">-0.2</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.10">-0.4</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.7.4.11">-0.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.8.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.8.5.1">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.8.5.2">0%</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.8.5.3.1">+0.7</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.8.5.4.1">+0.1</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.5"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.8.5.5.1">+0.9</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.6">+0.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.7">+0.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.8">+0.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.9"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.8.5.9.1">+0.2</span></td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.10">-0.3</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.8.5.11">+0.3</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.9.6" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="11" id="S5.T1.3.3.9.6.1"><span class="ltx_text" id="S5.T1.3.3.9.6.1.1" style="background-color:#E6E6E6;">Many-to-One (M2O / X-En)</span></th>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.10.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.10.7.1">mT-small</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.10.7.2">-</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.3">19.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.4">19.4</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.5">25.7</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.6">30.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.7">30.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.8">28.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.9">29.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.10">34.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.10.7.11">24.7</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.3.1">Adapter<sub class="ltx_sub" id="S5.T1.3.3.3.1.1"><span class="ltx_text ltx_font_italic" id="S5.T1.3.3.3.1.1.1">LP</span></sub>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.3.2">+67%</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.3">+0.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.4">+0.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.5">+0.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.6">+1.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.7">+0.8</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.8">+1.0</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.9">+0.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.10">+0.3</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.3.11">+0.8</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.11.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.11.8.1">LaSS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="S5.T1.3.3.11.8.2">0%</th>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.3">+1.2</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.4">+0.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.5">+0.9</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.6">+1.4</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.7">+1.1</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.8">+1.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.9">+1.6</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.10">+0.8</td>
<td class="ltx_td ltx_align_right" id="S5.T1.3.3.11.8.11">+1.2</td>
</tr>
<tr class="ltx_tr" id="S5.T1.3.3.12.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.3.3.12.9.1">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T1.3.3.12.9.2">0%</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.3"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.3.1">+1.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.4"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.4.1">+1.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.5"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.5.1">+1.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.6"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.6.1">+2.0</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.7"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.7.1">+1.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.8"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.8.1">+2.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.9"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.9.1">+1.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.10"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.10.1">+1.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S5.T1.3.3.12.9.11"><span class="ltx_text ltx_font_bold" id="S5.T1.3.3.12.9.11.1">+1.7</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Average BLEU improvements over the baseline (mT-small) model on the IWSLT dataset.
<math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S5.T1.5.m1.1"><semantics id="S5.T1.5.m1.1b"><mrow id="S5.T1.5.m1.1.1" xref="S5.T1.5.m1.1.1.cmml"><mi id="S5.T1.5.m1.1.1.2" mathvariant="normal" xref="S5.T1.5.m1.1.1.2.cmml">Δ</mi><mo id="S5.T1.5.m1.1.1.1" xref="S5.T1.5.m1.1.1.1.cmml">⁢</mo><mi id="S5.T1.5.m1.1.1.3" xref="S5.T1.5.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.m1.1c"><apply id="S5.T1.5.m1.1.1.cmml" xref="S5.T1.5.m1.1.1"><times id="S5.T1.5.m1.1.1.1.cmml" xref="S5.T1.5.m1.1.1.1"></times><ci id="S5.T1.5.m1.1.1.2.cmml" xref="S5.T1.5.m1.1.1.2">Δ</ci><ci id="S5.T1.5.m1.1.1.3.cmml" xref="S5.T1.5.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.m1.1d">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.m1.1e">roman_Δ italic_θ</annotation></semantics></math> denotes the relative parameter increase over the baseline, encompassing all translation directions.
The best results are in <span class="ltx_text ltx_font_bold" id="S5.T1.7.1">bold</span>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Systems</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We compare our method with strong open-source baselines that share similar motivations in reducing interference for multilingual translation tasks.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Baselines:</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.2.1">mT-small.</span> For IWSLT, we train an mT-small model on Many-to-Many directions as per <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite>: a 6-layer Transformer with 4 attention heads, <math alttext="d" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mi id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><ci id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_d</annotation></semantics></math> = 512, <math alttext="d_{\mathit{ff}}" class="ltx_Math" display="inline" id="S5.I1.i1.p1.2.m2.1"><semantics id="S5.I1.i1.p1.2.m2.1a"><msub id="S5.I1.i1.p1.2.m2.1.1" xref="S5.I1.i1.p1.2.m2.1.1.cmml"><mi id="S5.I1.i1.p1.2.m2.1.1.2" xref="S5.I1.i1.p1.2.m2.1.1.2.cmml">d</mi><mi id="S5.I1.i1.p1.2.m2.1.1.3" xref="S5.I1.i1.p1.2.m2.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.2.m2.1b"><apply id="S5.I1.i1.p1.2.m2.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.I1.i1.p1.2.m2.1.1.1.cmml" xref="S5.I1.i1.p1.2.m2.1.1">subscript</csymbol><ci id="S5.I1.i1.p1.2.m2.1.1.2.cmml" xref="S5.I1.i1.p1.2.m2.1.1.2">𝑑</ci><ci id="S5.I1.i1.p1.2.m2.1.1.3.cmml" xref="S5.I1.i1.p1.2.m2.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.2.m2.1c">d_{\mathit{ff}}</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.2.m2.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math> = 1,024.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.2.1">mT-big.</span> For EC30, we train a mT-big on Many-to-Many directions following <cite class="ltx_cite ltx_citemacro_citet">Wu and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib39" title="">2023</a>)</cite>. It has 6 layers, with 16 attention heads, <math alttext="d" class="ltx_Math" display="inline" id="S5.I1.i2.p1.1.m1.1"><semantics id="S5.I1.i2.p1.1.m1.1a"><mi id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><ci id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.1.m1.1d">italic_d</annotation></semantics></math> = 1,024, and <math alttext="d_{\mathit{ff}}" class="ltx_Math" display="inline" id="S5.I1.i2.p1.2.m2.1"><semantics id="S5.I1.i2.p1.2.m2.1a"><msub id="S5.I1.i2.p1.2.m2.1.1" xref="S5.I1.i2.p1.2.m2.1.1.cmml"><mi id="S5.I1.i2.p1.2.m2.1.1.2" xref="S5.I1.i2.p1.2.m2.1.1.2.cmml">d</mi><mi id="S5.I1.i2.p1.2.m2.1.1.3" xref="S5.I1.i2.p1.2.m2.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.2.m2.1b"><apply id="S5.I1.i2.p1.2.m2.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.I1.i2.p1.2.m2.1.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S5.I1.i2.p1.2.m2.1.1.2.cmml" xref="S5.I1.i2.p1.2.m2.1.1.2">𝑑</ci><ci id="S5.I1.i2.p1.2.m2.1.1.3.cmml" xref="S5.I1.i2.p1.2.m2.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.2.m2.1c">d_{\mathit{ff}}</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.2.m2.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math> = 4,096.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Adapters.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.5">We employ two adapter methods: 1) Language Pair Adapter (<span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.1.1">Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1"><semantics id="S5.SS2.SSS0.Px2.p1.1.1.m1.1a"><msub id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1a" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1b"><apply id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1"><ci id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S5.SS2.SSS0.Px2.p1.1.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.1.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math></span>) and 2) Language Family Adapter (<span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px2.p1.2.2">Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1"><semantics id="S5.SS2.SSS0.Px2.p1.2.2.m1.1a"><msub id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1a" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1b"><apply id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1"><ci id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1.cmml" mathsize="70%" xref="S5.SS2.SSS0.Px2.p1.2.2.m1.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.2.2.m1.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math></span>). We omit Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.3.m1.1"><semantics id="S5.SS2.SSS0.Px2.p1.3.m1.1a"><msub id="S5.SS2.SSS0.Px2.p1.3.m1.1.1" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.3.m1.1.1a" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.3.m1.1b"><apply id="S5.SS2.SSS0.Px2.p1.3.m1.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1"><ci id="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1.cmml" mathsize="70%" xref="S5.SS2.SSS0.Px2.p1.3.m1.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.3.m1.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.3.m1.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math> for IWSLT due to its limited languages. Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.4.m2.1"><semantics id="S5.SS2.SSS0.Px2.p1.4.m2.1a"><msub id="S5.SS2.SSS0.Px2.p1.4.m2.1.1" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.4.m2.1.1a" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.4.m2.1b"><apply id="S5.SS2.SSS0.Px2.p1.4.m2.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1"><ci id="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1"><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1.cmml" mathsize="70%" xref="S5.SS2.SSS0.Px2.p1.4.m2.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.4.m2.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.4.m2.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math> inserts adapter modules based on language pairs, demonstrating strong effects in reducing interference while presenting no parameter sharing <cite class="ltx_cite ltx_citemacro_cite">Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib3" title="">2019</a>)</cite>. In contrast, Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px2.p1.5.m3.1"><semantics id="S5.SS2.SSS0.Px2.p1.5.m3.1a"><msub id="S5.SS2.SSS0.Px2.p1.5.m3.1.1" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1.cmml"><mi id="S5.SS2.SSS0.Px2.p1.5.m3.1.1a" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px2.p1.5.m3.1b"><apply id="S5.SS2.SSS0.Px2.p1.5.m3.1.1.cmml" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1"><ci id="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1a.cmml" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1"><mtext class="ltx_mathvariant_italic" id="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1.cmml" mathsize="70%" xref="S5.SS2.SSS0.Px2.p1.5.m3.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px2.p1.5.m3.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px2.p1.5.m3.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Chronopoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib7" title="">2023</a>)</cite> facilitates parameter sharing across similar languages by training modules for each language family.
Their bottleneck dimensions are 128 and 512 respectively.
See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2" title="A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.2</span></a> for more training details.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">LaSS.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite> proposed LaSS to locate language-specific sub-networks following the lottery ticket hypothesis, i.e., finetuning all translation directions from a pre-trained model and then pruning based on magnitude.
They then continually train the pre-trained model by only updating the sub-networks for each direction.
We adopt the strongest LaSS configuration by applying sub-networks for both attention and FFNs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Implementation and Evaluation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.4">We train our baseline models following the same hyper-parameter settings in <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Wu and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib39" title="">2023</a>)</cite>.
Specifically, we use the Adam optimizer (<math alttext="\beta 1=0.9" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mrow id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2.2" xref="S5.SS3.p1.1.m1.1.1.2.2.cmml">β</mi><mo id="S5.SS3.p1.1.m1.1.1.2.1" xref="S5.SS3.p1.1.m1.1.1.2.1.cmml">⁢</mo><mn id="S5.SS3.p1.1.m1.1.1.2.3" xref="S5.SS3.p1.1.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><eq id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></eq><apply id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2"><times id="S5.SS3.p1.1.m1.1.1.2.1.cmml" xref="S5.SS3.p1.1.m1.1.1.2.1"></times><ci id="S5.SS3.p1.1.m1.1.1.2.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2.2">𝛽</ci><cn id="S5.SS3.p1.1.m1.1.1.2.3.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.2.3">1</cn></apply><cn id="S5.SS3.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS3.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">\beta 1=0.9</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_β 1 = 0.9</annotation></semantics></math>, <math alttext="\beta 2=0.98" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mrow id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2.2" xref="S5.SS3.p1.2.m2.1.1.2.2.cmml">β</mi><mo id="S5.SS3.p1.2.m2.1.1.2.1" xref="S5.SS3.p1.2.m2.1.1.2.1.cmml">⁢</mo><mn id="S5.SS3.p1.2.m2.1.1.2.3" xref="S5.SS3.p1.2.m2.1.1.2.3.cmml">2</mn></mrow><mo id="S5.SS3.p1.2.m2.1.1.1" xref="S5.SS3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><eq id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1"></eq><apply id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2"><times id="S5.SS3.p1.2.m2.1.1.2.1.cmml" xref="S5.SS3.p1.2.m2.1.1.2.1"></times><ci id="S5.SS3.p1.2.m2.1.1.2.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2.2">𝛽</ci><cn id="S5.SS3.p1.2.m2.1.1.2.3.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1.2.3">2</cn></apply><cn id="S5.SS3.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS3.p1.2.m2.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">\beta 2=0.98</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">italic_β 2 = 0.98</annotation></semantics></math>, <math alttext="\epsilon" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><mi id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><ci id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">italic_ϵ</annotation></semantics></math> = <math alttext="10^{-9}" class="ltx_Math" display="inline" id="S5.SS3.p1.4.m4.1"><semantics id="S5.SS3.p1.4.m4.1a"><msup id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml"><mn id="S5.SS3.p1.4.m4.1.1.2" xref="S5.SS3.p1.4.m4.1.1.2.cmml">10</mn><mrow id="S5.SS3.p1.4.m4.1.1.3" xref="S5.SS3.p1.4.m4.1.1.3.cmml"><mo id="S5.SS3.p1.4.m4.1.1.3a" xref="S5.SS3.p1.4.m4.1.1.3.cmml">−</mo><mn id="S5.SS3.p1.4.m4.1.1.3.2" xref="S5.SS3.p1.4.m4.1.1.3.2.cmml">9</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><apply id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.4.m4.1.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">superscript</csymbol><cn id="S5.SS3.p1.4.m4.1.1.2.cmml" type="integer" xref="S5.SS3.p1.4.m4.1.1.2">10</cn><apply id="S5.SS3.p1.4.m4.1.1.3.cmml" xref="S5.SS3.p1.4.m4.1.1.3"><minus id="S5.SS3.p1.4.m4.1.1.3.1.cmml" xref="S5.SS3.p1.4.m4.1.1.3"></minus><cn id="S5.SS3.p1.4.m4.1.1.3.2.cmml" type="integer" xref="S5.SS3.p1.4.m4.1.1.3.2">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">10^{-9}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.4.m4.1d">10 start_POSTSUPERSCRIPT - 9 end_POSTSUPERSCRIPT</annotation></semantics></math>) with 5e-4 learning rate and 4k warmup steps in all experiments.
We use 4 NVIDIA A6000 (48G) GPUs to conduct most experiments and implement them based on Fairseq <cite class="ltx_cite ltx_citemacro_cite">Ott et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib24" title="">2019</a>)</cite> with FP16. We list detailed training and model specifications for all systems in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2" title="A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">We adopt the tokenized BLEU <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib25" title="">2002</a>)</cite> for the IWSLT dataset and detokenized case-sensitive SacreBLEU<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>nrefs:1—case:mixed—eff:no—tok:13a—smooth:exp—version:2.3.1</span></span></span> <cite class="ltx_cite ltx_citemacro_cite">Post (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib31" title="">2018</a>)</cite> for the EC30 dataset in our main result evaluation section. In addition, we provide ChrF++ <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib30" title="">2017</a>)</cite> and COMET <cite class="ltx_cite ltx_citemacro_cite">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib32" title="">2020</a>)</cite> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS4" title="A.4 Result Details using ChrF++ and COMET ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.4</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results and Analyses</h2>
<figure class="ltx_table" id="S6.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T2.3" style="width:433.6pt;height:145pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,17.5pt) scale(0.805376766178921,0.805376766178921) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T2.1.1.1.2" rowspan="2"><span class="ltx_text" id="S6.T2.1.1.1.2.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T2.1.1.1.1" rowspan="2"><span class="ltx_text" id="S6.T2.1.1.1.1.1"><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="S6.T2.1.1.1.1.1.m1.1"><semantics id="S6.T2.1.1.1.1.1.m1.1a"><mrow id="S6.T2.1.1.1.1.1.m1.1.1" xref="S6.T2.1.1.1.1.1.m1.1.1.cmml"><mi id="S6.T2.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S6.T2.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo id="S6.T2.1.1.1.1.1.m1.1.1.1" xref="S6.T2.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.T2.1.1.1.1.1.m1.1.1.3" xref="S6.T2.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T2.1.1.1.1.1.m1.1b"><apply id="S6.T2.1.1.1.1.1.m1.1.1.cmml" xref="S6.T2.1.1.1.1.1.m1.1.1"><times id="S6.T2.1.1.1.1.1.m1.1.1.1.cmml" xref="S6.T2.1.1.1.1.1.m1.1.1.1"></times><ci id="S6.T2.1.1.1.1.1.m1.1.1.2.cmml" xref="S6.T2.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="S6.T2.1.1.1.1.1.m1.1.1.3.cmml" xref="S6.T2.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.1.1.1.1.1.m1.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="S6.T2.1.1.1.1.1.m1.1d">roman_Δ italic_θ</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.1.1.1.3">High (5M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.1.1.1.4">Med (1M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T2.1.1.1.5">Low (100K)</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="3" id="S6.T2.1.1.1.6">All (61M)</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.1">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.2">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.3">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.4">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.5">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.6">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.7">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.8">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.9">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="S6.T2.3.3.4.1.10">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.11">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T2.3.3.4.1.12">Avg</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.3.5.2.1">mT-big</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.3.5.2.2">-</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.3">28.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.4">31.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.5">29.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.6">29.7</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.7">31.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.8">30.6</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.9">18.9</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.10">26.0</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T2.3.3.5.2.11">22.4</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.12">25.5</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.13">29.7</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.5.2.14">27.7</td>
</tr>
<tr class="ltx_tr" id="S6.T2.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.1">Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="S6.T2.2.2.2.1.m1.1"><semantics id="S6.T2.2.2.2.1.m1.1a"><msub id="S6.T2.2.2.2.1.m1.1.1" xref="S6.T2.2.2.2.1.m1.1.1.cmml"><mi id="S6.T2.2.2.2.1.m1.1.1a" xref="S6.T2.2.2.2.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S6.T2.2.2.2.1.m1.1.1.1" xref="S6.T2.2.2.2.1.m1.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T2.2.2.2.1.m1.1b"><apply id="S6.T2.2.2.2.1.m1.1.1.cmml" xref="S6.T2.2.2.2.1.m1.1.1"><ci id="S6.T2.2.2.2.1.m1.1.1.1a.cmml" xref="S6.T2.2.2.2.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S6.T2.2.2.2.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.T2.2.2.2.1.m1.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.2.2.2.1.m1.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="S6.T2.2.2.2.1.m1.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.2.2.2.2">+70%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.3">+0.7</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.4">+0.3</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.5">+0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.6">+0.7</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.7">+0.3</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.8">+0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.9">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.10">+0.5</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.2.2.2.11">+0.8</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.12">+0.8</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.13">+0.4</td>
<td class="ltx_td ltx_align_right" id="S6.T2.2.2.2.14">+0.6</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.3.1">Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="S6.T2.3.3.3.1.m1.1"><semantics id="S6.T2.3.3.3.1.m1.1a"><msub id="S6.T2.3.3.3.1.m1.1.1" xref="S6.T2.3.3.3.1.m1.1.1.cmml"><mi id="S6.T2.3.3.3.1.m1.1.1a" xref="S6.T2.3.3.3.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S6.T2.3.3.3.1.m1.1.1.1" xref="S6.T2.3.3.3.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T2.3.3.3.1.m1.1b"><apply id="S6.T2.3.3.3.1.m1.1.1.cmml" xref="S6.T2.3.3.3.1.m1.1.1"><ci id="S6.T2.3.3.3.1.m1.1.1.1a.cmml" xref="S6.T2.3.3.3.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S6.T2.3.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.T2.3.3.3.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T2.3.3.3.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="S6.T2.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.3.2">+87%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.3">+1.6</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.4">+0.6</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.5">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.6">+1.6</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.7">+0.4</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.8">+1.0</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.9">+0.4</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.10">+0.4</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.3.3.3.11">+0.4</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.12">+1.2</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.13">+0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.3.14">+0.8</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.6.3.1">LaSS</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.6.3.2">0%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.6.3.3.1">+2.3</span></td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.4">+0.8</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.5">+1.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.6"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.6.3.6.1">+1.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.7">+0.2</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.8">+1.0</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.9">-0.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.10">-1.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.3.3.6.3.11">-1.0</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.12">+1.3</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.13">-0.3</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.6.3.14">+0.5</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.7.4.1">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.7.4.2">0%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.3">+0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.4">-0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.5">+0.2</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.6">+0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.7">-0.7</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.8">-0.2</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.9">-0.3</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.10">-1.5</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.3.3.7.4.11">-0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.12">+0.5</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.13">-0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.7.4.14">-0.2</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.8.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.3.8.5.1">Ours-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T2.3.3.8.5.2">0%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.3">+1.2</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.4">+1.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.5">+1.1</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.6">+1.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.7">+1.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.8">+1.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.9">+0.7</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.10">+0.8</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S6.T2.3.3.8.5.11">+0.8</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.12">+1.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.13">+1.0</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T2.3.3.8.5.14">+1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.9.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.9.6.1">Ours-Dec</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T2.3.3.9.6.2">0%</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.3">+1.2</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.4">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.5">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.6">+0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.7">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.8">+1.0</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.9">+0.7</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.10">+1.1</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T2.3.3.9.6.11">+0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.12">+0.9</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.13">+1.1</td>
<td class="ltx_td ltx_align_right" id="S6.T2.3.3.9.6.14">+1.0</td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.10.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.3.3.10.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T2.3.3.10.7.2">0%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.3">+1.8</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.4"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.4.1">+1.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.5"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.5.1">+1.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.6">+1.4</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.7"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.7.1">+1.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.8"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.8.1">+1.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.9"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.9.1">+1.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.10"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.10.1">+0.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S6.T2.3.3.10.7.11"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.11.1">+1.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.12"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.12.1">+1.5</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.13"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.13.1">+1.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T2.3.3.10.7.14"><span class="ltx_text ltx_font_bold" id="S6.T2.3.3.10.7.14.1">+1.3</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Average SacreBLEU improvements on the EC30 dataset over the baseline (mT-big), categorized by High, Medium, and Low-resource translation directions. ’Random’ denotes continually updating the model with randomly selected task-specific neurons. ’Ours-Enc’ and ’Ours-Dec’ indicate Neuron Specialization applied solely to the Encoder and Decoder, respectively, while ’Ours’ signifies the method applied to both components.</figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Small-Scale Results on IWSLT</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We show results on IWSLT in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S5.T1" title="Table 1 ‣ EC30. ‣ 5.1 Datasets ‣ 5 Experimental Setup ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>.
For Many-to-One (M2O) directions, our method achieves an average +1.7 BLEU gain over the baseline, achieving the best performance among all approaches for all languages.
The Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><msub id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1a" xref="S6.SS1.p1.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><ci id="S6.SS1.p1.1.m1.1.1.1a.cmml" xref="S6.SS1.p1.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S6.SS1.p1.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.SS1.p1.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math>, with a 67% increase in parameters over the baseline model, shows weaker improvements (+0.8) than our method.
As for One-to-Many (O2M) directions, we observed weaker performance improvements for all methods.
While the gains are modest (averaging +0.3 BLEU), our method demonstrates consistent improvements across various languages in general.</p>
</div>
<section class="ltx_paragraph" id="S6.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Scaling up does not always reduce interference.</h5>
<div class="ltx_para" id="S6.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS1.SSS0.Px1.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Shaham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib33" title="">2023</a>); Chang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib4" title="">2023</a>)</cite> have found scaling up the model capacity reduces interference, even under low-resource settings. We then investigate the trade-off between performance and model capacity by employing mT-shallow, a shallower version of mT-small with three fewer layers (with <math alttext="\Delta\theta=-39\%" class="ltx_Math" display="inline" id="S6.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S6.SS1.SSS0.Px1.p1.1.m1.1a"><mrow id="S6.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mrow id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.2" mathvariant="normal" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">Δ</mi><mo id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.1" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.3" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">θ</mi></mrow><mo id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.1" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml"><mo id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3a" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">−</mo><mrow id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml"><mn id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.2" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml">39</mn><mo id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.1" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml">%</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1"><eq id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.1"></eq><apply id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2"><times id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.1"></times><ci id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.2">Δ</ci><ci id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.2.3">𝜃</ci></apply><apply id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3"><minus id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3"></minus><apply id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2"><csymbol cd="latexml" id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.1.cmml" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.1">percent</csymbol><cn id="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.2.cmml" type="integer" xref="S6.SS1.SSS0.Px1.p1.1.m1.1.1.3.2.2">39</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS0.Px1.p1.1.m1.1c">\Delta\theta=-39\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS0.Px1.p1.1.m1.1d">roman_Δ italic_θ = - 39 %</annotation></semantics></math> for parameters, see Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T6" title="Table 6 ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a> for details). Surprisingly, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.F3" title="Figure 3 ‣ Scaling up does not always reduce interference. ‣ 6.1 Small-Scale Results on IWSLT ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we show that reducing parameters improved Many-to-One (X-En) performance but weakened One-to-Many (En-X) results. This result indicates that scaling up the model capacity does not always reduce interference, but may show overfitting to have performance degradation. Furthermore, we show that implementing Neuron Specialization with mT-shallow enhances Many-to-One (X-En) performance in all directions while lessening the decline in One-to-Many (En-X) translation quality in general.</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="S6.F3.g1" src="extracted/2404.11201v1/imgs/shallow_new.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>BLEU gains of shallower models over mT-small on IWSLT show improved X-En performance at the expense of En-X. Applying Neuron Specialization reduces EN-X degradation and amplifies X-En gains.</figcaption>
</figure>
<figure class="ltx_table" id="S6.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T3.1" style="width:433.6pt;height:153.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.8pt,13.1pt) scale(0.854843275734674,0.854843275734674) ;">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S6.T3.1.1.1.1.1">Lang</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.2">De</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.3">Es</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.4">Cs</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.5">Hi</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.6">Ar</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.7">Lb</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.8">Ro</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.9">Sr</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.10">Gu</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.11">Am</td>
<td class="ltx_td ltx_align_right ltx_border_l ltx_border_tt" id="S6.T3.1.1.1.1.12">High</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S6.T3.1.1.1.1.13">Low</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.2.2.1">Size</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.2">5m</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.3">5m</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.4">5m</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.5">5m</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.6">5m</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.7">100k</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.8">100k</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.9">100k</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.10">100k</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.11">100k</td>
<td class="ltx_td ltx_align_right ltx_border_l" id="S6.T3.1.1.2.2.12">Avg</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.2.2.13">Avg</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.3.3" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S6.T3.1.1.3.3.1"><span class="ltx_text" id="S6.T3.1.1.3.3.1.1" style="background-color:#E6E6E6;">One-to-Many</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.4.4.1">Bilingual</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.2">36.3</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.3">24.6</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.4">28.7</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.5">43.9</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.6">23.7</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.7">5.5</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.8">16.2</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.9">17.8</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.10">12.8</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.1.4.4.11">4.1</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.12">31.8</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.4.4.13">11.3</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.5.5.1">mT-big</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.2"><span class="ltx_text" id="S6.T3.1.1.5.5.2.1" style="background-color:#E47979;">-4.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.3"><span class="ltx_text" id="S6.T3.1.1.5.5.3.1" style="background-color:#F3ADAC;">-1.5</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.4"><span class="ltx_text" id="S6.T3.1.1.5.5.4.1" style="background-color:#E47979;">-3.6</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.5"><span class="ltx_text" id="S6.T3.1.1.5.5.5.1" style="background-color:#E47979;">-4.4</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.6"><span class="ltx_text" id="S6.T3.1.1.5.5.6.1" style="background-color:#E47979;">-4.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.7"><span class="ltx_text" id="S6.T3.1.1.5.5.7.1" style="background-color:#A3C5FF;">+9.0</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.8"><span class="ltx_text" id="S6.T3.1.1.5.5.8.1" style="background-color:#A3C5FF;">+8.9</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.9"><span class="ltx_text" id="S6.T3.1.1.5.5.9.1" style="background-color:#A3C5FF;">+6.2</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.10"><span class="ltx_text" id="S6.T3.1.1.5.5.10.1" style="background-color:#A3C5FF;">+13.9</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.1.5.5.11"><span class="ltx_text" id="S6.T3.1.1.5.5.11.1" style="background-color:#A3C5FF;">+3.1</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.12"><span class="ltx_text" id="S6.T3.1.1.5.5.12.1" style="background-color:#E47979;">-3.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.5.5.13"><span class="ltx_text" id="S6.T3.1.1.5.5.13.1" style="background-color:#A3C5FF;">+8.2</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.6.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.6.6.1">Ours</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.2"><span class="ltx_text" id="S6.T3.1.1.6.6.2.1" style="background-color:#F3ADAC;">-2.0</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.3"><span class="ltx_text" id="S6.T3.1.1.6.6.3.1" style="background-color:#FDEBE8;">-0.2</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.4"><span class="ltx_text" id="S6.T3.1.1.6.6.4.1" style="background-color:#F3ADAC;">-1.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.5"><span class="ltx_text" id="S6.T3.1.1.6.6.5.1" style="background-color:#F3ADAC;">-2.4</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.6"><span class="ltx_text" id="S6.T3.1.1.6.6.6.1" style="background-color:#F3ADAC;">-3.0</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.7"><span class="ltx_text" id="S6.T3.1.1.6.6.7.1" style="background-color:#79AAFF;">+10.8</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.8"><span class="ltx_text" id="S6.T3.1.1.6.6.8.1" style="background-color:#79AAFF;">+10.0</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.9"><span class="ltx_text" id="S6.T3.1.1.6.6.9.1" style="background-color:#79AAFF;">+8.2</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.10"><span class="ltx_text" id="S6.T3.1.1.6.6.10.1" style="background-color:#79AAFF;">+16.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.1.6.6.11"><span class="ltx_text" id="S6.T3.1.1.6.6.11.1" style="background-color:#79AAFF;">+3.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.12"><span class="ltx_text" id="S6.T3.1.1.6.6.12.1" style="background-color:#F3ADAC;">-1.9</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.6.6.13"><span class="ltx_text" id="S6.T3.1.1.6.6.13.1" style="background-color:#79AAFF;">+9.8</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.7.7" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S6.T3.1.1.7.7.1"><span class="ltx_text" id="S6.T3.1.1.7.7.1.1" style="background-color:#E6E6E6;">Many-to-One</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.8.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.8.8.1">Bilingual</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.2">39.1</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.3">24.5</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.4">32.6</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.5">35.5</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.6">30.8</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.7">8.7</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.8">19.5</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.9">21.3</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.10">7.0</td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.1.8.8.11">8.7</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.12">32.7</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.8.8.13">13.0</td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.9.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T3.1.1.9.9.1">mT-big</td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.2"><span class="ltx_text" id="S6.T3.1.1.9.9.2.1" style="background-color:#F3ADAC;">-1.5</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.3"><span class="ltx_text" id="S6.T3.1.1.9.9.3.1" style="background-color:#D1E2FF;">+0.9</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.4"><span class="ltx_text" id="S6.T3.1.1.9.9.4.1" style="background-color:#D1E2FF;">+0.2</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.5"><span class="ltx_text" id="S6.T3.1.1.9.9.5.1" style="background-color:#F3ADAC;">-1.8</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.6"><span class="ltx_text" id="S6.T3.1.1.9.9.6.1" style="background-color:#F3ADAC;">-2.3</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.7"><span class="ltx_text" id="S6.T3.1.1.9.9.7.1" style="background-color:#A3C5FF;">+13.7</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.8"><span class="ltx_text" id="S6.T3.1.1.9.9.8.1" style="background-color:#A3C5FF;">+11.9</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.9"><span class="ltx_text" id="S6.T3.1.1.9.9.9.1" style="background-color:#A3C5FF;">+10.3</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.10"><span class="ltx_text" id="S6.T3.1.1.9.9.10.1" style="background-color:#A3C5FF;">+18.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S6.T3.1.1.9.9.11"><span class="ltx_text" id="S6.T3.1.1.9.9.11.1" style="background-color:#A3C5FF;">+12.5</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.12"><span class="ltx_text" id="S6.T3.1.1.9.9.12.1" style="background-color:#F3ADAC;">-1.1</span></td>
<td class="ltx_td ltx_align_right" id="S6.T3.1.1.9.9.13"><span class="ltx_text" id="S6.T3.1.1.9.9.13.1" style="background-color:#A3C5FF;">+13.3</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.1.1.10.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S6.T3.1.1.10.10.1">Ours</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.2"><span class="ltx_text" id="S6.T3.1.1.10.10.2.1" style="background-color:#FDEBE8;">-0.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.3"><span class="ltx_text" id="S6.T3.1.1.10.10.3.1" style="background-color:#A3C5FF;">+1.7</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.4"><span class="ltx_text" id="S6.T3.1.1.10.10.4.1" style="background-color:#A3C5FF;">+1.8</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.5"><span class="ltx_text" id="S6.T3.1.1.10.10.5.1" style="background-color:#FDEBE8;">-0.2</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.6"><span class="ltx_text" id="S6.T3.1.1.10.10.6.1" style="background-color:#FDEBE8;">-0.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.7"><span class="ltx_text" id="S6.T3.1.1.10.10.7.1" style="background-color:#79AAFF;">+15.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.8"><span class="ltx_text" id="S6.T3.1.1.10.10.8.1" style="background-color:#79AAFF;">+12.4</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.9"><span class="ltx_text" id="S6.T3.1.1.10.10.9.1" style="background-color:#79AAFF;">+11.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.10"><span class="ltx_text" id="S6.T3.1.1.10.10.10.1" style="background-color:#79AAFF;">+19.6</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S6.T3.1.1.10.10.11"><span class="ltx_text" id="S6.T3.1.1.10.10.11.1" style="background-color:#79AAFF;">+14.1</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.12"><span class="ltx_text" id="S6.T3.1.1.10.10.12.1" style="background-color:#D1E2FF;">+0.3</span></td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T3.1.1.10.10.13"><span class="ltx_text" id="S6.T3.1.1.10.10.13.1" style="background-color:#79AAFF;">+14.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>SacreBLEU score comparisons for Multilingual baseline and Neuron Specialization models against Bilingual ones on the EC30 dataset, limited to 5 high- and low-resource languages due to computational constraints.
<span class="ltx_text" id="S6.T3.4.1" style="background-color:#FDEBE8;">Red</span> signifies negative interference, <span class="ltx_text" id="S6.T3.5.2" style="background-color:#D1E2FF;">Blue</span> denotes positive synergy, with darker shades indicating better effects.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Large-Scale Results on EC-30</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Similar to what we observed in the small-scale setting, we find notable improvements when we scale up on the EC30 dataset.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.T2" title="Table 2 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>, we show consistent improvements across high-, medium-, and low-resource languages, with an average gain of +1.3 SacreBLEU over the baseline.
LaSS, while effective in high-resource O2M pairs, presents limitations with negative impacts (-1.0 score) on low-resource languages, highlighting difficulties in sub-network extraction for low-resource languages.
In contrast, our method achieves stable and consistent gains across all resource levels.
The Adapter<sub class="ltx_sub" id="S6.SS2.p1.1.1"><span class="ltx_text ltx_font_italic" id="S6.SS2.p1.1.1.1">LP</span></sub>, despite increasing parameters by 87% compared to the baseline, falls short of our method in boosting performance.
Additionally, we show that applying Neuron Specialization in either the encoder or decoder delivers similar gains, with both combined offering stronger performance.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.7" style="width:325.2pt;height:103.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(49.6pt,-15.8pt) scale(1.43914364791785,1.43914364791785) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T4.7.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S6.T4.3.3.3.4">Model</th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T4.1.1.1.1"><math alttext="\triangle\theta" class="ltx_Math" display="inline" id="S6.T4.1.1.1.1.m1.1"><semantics id="S6.T4.1.1.1.1.m1.1a"><mrow id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="S6.T4.1.1.1.1.m1.1.1.2.cmml">△</mi><mo id="S6.T4.1.1.1.1.m1.1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.T4.1.1.1.1.m1.1.1.3" xref="S6.T4.1.1.1.1.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><times id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1.1"></times><ci id="S6.T4.1.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.1.m1.1.1.2">△</ci><ci id="S6.T4.1.1.1.1.m1.1.1.3.cmml" xref="S6.T4.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">\triangle\theta</annotation><annotation encoding="application/x-llamapun" id="S6.T4.1.1.1.1.m1.1d">△ italic_θ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T4.2.2.2.2"><math alttext="\triangle T_{subnet}" class="ltx_Math" display="inline" id="S6.T4.2.2.2.2.m1.1"><semantics id="S6.T4.2.2.2.2.m1.1a"><mrow id="S6.T4.2.2.2.2.m1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.cmml"><mi id="S6.T4.2.2.2.2.m1.1.1.2" mathvariant="normal" xref="S6.T4.2.2.2.2.m1.1.1.2.cmml">△</mi><mo id="S6.T4.2.2.2.2.m1.1.1.1" xref="S6.T4.2.2.2.2.m1.1.1.1.cmml">⁢</mo><msub id="S6.T4.2.2.2.2.m1.1.1.3" xref="S6.T4.2.2.2.2.m1.1.1.3.cmml"><mi id="S6.T4.2.2.2.2.m1.1.1.3.2" xref="S6.T4.2.2.2.2.m1.1.1.3.2.cmml">T</mi><mrow id="S6.T4.2.2.2.2.m1.1.1.3.3" xref="S6.T4.2.2.2.2.m1.1.1.3.3.cmml"><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.2" xref="S6.T4.2.2.2.2.m1.1.1.3.3.2.cmml">s</mi><mo id="S6.T4.2.2.2.2.m1.1.1.3.3.1" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.3" xref="S6.T4.2.2.2.2.m1.1.1.3.3.3.cmml">u</mi><mo id="S6.T4.2.2.2.2.m1.1.1.3.3.1a" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.4" xref="S6.T4.2.2.2.2.m1.1.1.3.3.4.cmml">b</mi><mo id="S6.T4.2.2.2.2.m1.1.1.3.3.1b" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.5" xref="S6.T4.2.2.2.2.m1.1.1.3.3.5.cmml">n</mi><mo id="S6.T4.2.2.2.2.m1.1.1.3.3.1c" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.6" xref="S6.T4.2.2.2.2.m1.1.1.3.3.6.cmml">e</mi><mo id="S6.T4.2.2.2.2.m1.1.1.3.3.1d" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.2.2.2.2.m1.1.1.3.3.7" xref="S6.T4.2.2.2.2.m1.1.1.3.3.7.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.2.m1.1b"><apply id="S6.T4.2.2.2.2.m1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1"><times id="S6.T4.2.2.2.2.m1.1.1.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1.1"></times><ci id="S6.T4.2.2.2.2.m1.1.1.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.2">△</ci><apply id="S6.T4.2.2.2.2.m1.1.1.3.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S6.T4.2.2.2.2.m1.1.1.3.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3">subscript</csymbol><ci id="S6.T4.2.2.2.2.m1.1.1.3.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.2">𝑇</ci><apply id="S6.T4.2.2.2.2.m1.1.1.3.3.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3"><times id="S6.T4.2.2.2.2.m1.1.1.3.3.1.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.1"></times><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.2.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.2">𝑠</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.3.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.3">𝑢</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.4.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.4">𝑏</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.5.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.5">𝑛</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.6.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.6">𝑒</ci><ci id="S6.T4.2.2.2.2.m1.1.1.3.3.7.cmml" xref="S6.T4.2.2.2.2.m1.1.1.3.3.7">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.2.m1.1c">\triangle T_{subnet}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.2.2.2.2.m1.1d">△ italic_T start_POSTSUBSCRIPT italic_s italic_u italic_b italic_n italic_e italic_t end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" id="S6.T4.3.3.3.3">
<math alttext="\triangle" class="ltx_Math" display="inline" id="S6.T4.3.3.3.3.m1.1"><semantics id="S6.T4.3.3.3.3.m1.1a"><mi id="S6.T4.3.3.3.3.m1.1.1" mathvariant="normal" xref="S6.T4.3.3.3.3.m1.1.1.cmml">△</mi><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.3.m1.1b"><ci id="S6.T4.3.3.3.3.m1.1.1.cmml" xref="S6.T4.3.3.3.3.m1.1.1">△</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.3.m1.1c">\triangle</annotation><annotation encoding="application/x-llamapun" id="S6.T4.3.3.3.3.m1.1d">△</annotation></semantics></math> Memory</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S6.T4.4.4.4.1">Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="S6.T4.4.4.4.1.m1.1"><semantics id="S6.T4.4.4.4.1.m1.1a"><msub id="S6.T4.4.4.4.1.m1.1.1" xref="S6.T4.4.4.4.1.m1.1.1.cmml"><mi id="S6.T4.4.4.4.1.m1.1.1a" xref="S6.T4.4.4.4.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S6.T4.4.4.4.1.m1.1.1.1" xref="S6.T4.4.4.4.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T4.4.4.4.1.m1.1b"><apply id="S6.T4.4.4.4.1.m1.1.1.cmml" xref="S6.T4.4.4.4.1.m1.1.1"><ci id="S6.T4.4.4.4.1.m1.1.1.1a.cmml" xref="S6.T4.4.4.4.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="S6.T4.4.4.4.1.m1.1.1.1.cmml" mathsize="70%" xref="S6.T4.4.4.4.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.4.4.4.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.4.4.4.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math>
</th>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.5.5.5.2">
<math alttext="+" class="ltx_Math" display="inline" id="S6.T4.5.5.5.2.m1.1"><semantics id="S6.T4.5.5.5.2.m1.1a"><mo id="S6.T4.5.5.5.2.m1.1.1" xref="S6.T4.5.5.5.2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T4.5.5.5.2.m1.1b"><plus id="S6.T4.5.5.5.2.m1.1.1.cmml" xref="S6.T4.5.5.5.2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.5.5.5.2.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.T4.5.5.5.2.m1.1d">+</annotation></semantics></math>87%</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.5.5.5.3">n/a</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S6.T4.5.5.5.4">1.42 GB</td>
</tr>
<tr class="ltx_tr" id="S6.T4.6.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S6.T4.6.6.6.2">LaSS</th>
<td class="ltx_td ltx_align_right" id="S6.T4.6.6.6.3">0%</td>
<td class="ltx_td ltx_align_right" id="S6.T4.6.6.6.1">
<math alttext="+" class="ltx_Math" display="inline" id="S6.T4.6.6.6.1.m1.1"><semantics id="S6.T4.6.6.6.1.m1.1a"><mo id="S6.T4.6.6.6.1.m1.1.1" xref="S6.T4.6.6.6.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T4.6.6.6.1.m1.1b"><plus id="S6.T4.6.6.6.1.m1.1.1.cmml" xref="S6.T4.6.6.6.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.6.6.6.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.T4.6.6.6.1.m1.1d">+</annotation></semantics></math>33 hours</td>
<td class="ltx_td ltx_align_right" id="S6.T4.6.6.6.4">9.84 GB</td>
</tr>
<tr class="ltx_tr" id="S6.T4.7.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S6.T4.7.7.7.2">Ours</th>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T4.7.7.7.3">0%</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T4.7.7.7.1">
<math alttext="+" class="ltx_Math" display="inline" id="S6.T4.7.7.7.1.m1.1"><semantics id="S6.T4.7.7.7.1.m1.1a"><mo id="S6.T4.7.7.7.1.m1.1.1" xref="S6.T4.7.7.7.1.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S6.T4.7.7.7.1.m1.1b"><plus id="S6.T4.7.7.7.1.m1.1.1.cmml" xref="S6.T4.7.7.7.1.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.7.7.7.1.m1.1c">+</annotation><annotation encoding="application/x-llamapun" id="S6.T4.7.7.7.1.m1.1d">+</annotation></semantics></math>5 minutes</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S6.T4.7.7.7.4">3e-3 GB</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Efficiency comparison on EC30 dataset regarding extra trainable parameters (<math alttext="\triangle\theta" class="ltx_Math" display="inline" id="S6.T4.11.m1.1"><semantics id="S6.T4.11.m1.1b"><mrow id="S6.T4.11.m1.1.1" xref="S6.T4.11.m1.1.1.cmml"><mi id="S6.T4.11.m1.1.1.2" mathvariant="normal" xref="S6.T4.11.m1.1.1.2.cmml">△</mi><mo id="S6.T4.11.m1.1.1.1" xref="S6.T4.11.m1.1.1.1.cmml">⁢</mo><mi id="S6.T4.11.m1.1.1.3" xref="S6.T4.11.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.11.m1.1c"><apply id="S6.T4.11.m1.1.1.cmml" xref="S6.T4.11.m1.1.1"><times id="S6.T4.11.m1.1.1.1.cmml" xref="S6.T4.11.m1.1.1.1"></times><ci id="S6.T4.11.m1.1.1.2.cmml" xref="S6.T4.11.m1.1.1.2">△</ci><ci id="S6.T4.11.m1.1.1.3.cmml" xref="S6.T4.11.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.11.m1.1d">\triangle\theta</annotation><annotation encoding="application/x-llamapun" id="S6.T4.11.m1.1e">△ italic_θ</annotation></semantics></math>: relative increase over the baseline), extra processing time for subnet extraction (<math alttext="\triangle T_{subnet}" class="ltx_Math" display="inline" id="S6.T4.12.m2.1"><semantics id="S6.T4.12.m2.1b"><mrow id="S6.T4.12.m2.1.1" xref="S6.T4.12.m2.1.1.cmml"><mi id="S6.T4.12.m2.1.1.2" mathvariant="normal" xref="S6.T4.12.m2.1.1.2.cmml">△</mi><mo id="S6.T4.12.m2.1.1.1" xref="S6.T4.12.m2.1.1.1.cmml">⁢</mo><msub id="S6.T4.12.m2.1.1.3" xref="S6.T4.12.m2.1.1.3.cmml"><mi id="S6.T4.12.m2.1.1.3.2" xref="S6.T4.12.m2.1.1.3.2.cmml">T</mi><mrow id="S6.T4.12.m2.1.1.3.3" xref="S6.T4.12.m2.1.1.3.3.cmml"><mi id="S6.T4.12.m2.1.1.3.3.2" xref="S6.T4.12.m2.1.1.3.3.2.cmml">s</mi><mo id="S6.T4.12.m2.1.1.3.3.1" xref="S6.T4.12.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.12.m2.1.1.3.3.3" xref="S6.T4.12.m2.1.1.3.3.3.cmml">u</mi><mo id="S6.T4.12.m2.1.1.3.3.1b" xref="S6.T4.12.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.12.m2.1.1.3.3.4" xref="S6.T4.12.m2.1.1.3.3.4.cmml">b</mi><mo id="S6.T4.12.m2.1.1.3.3.1c" xref="S6.T4.12.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.12.m2.1.1.3.3.5" xref="S6.T4.12.m2.1.1.3.3.5.cmml">n</mi><mo id="S6.T4.12.m2.1.1.3.3.1d" xref="S6.T4.12.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.12.m2.1.1.3.3.6" xref="S6.T4.12.m2.1.1.3.3.6.cmml">e</mi><mo id="S6.T4.12.m2.1.1.3.3.1e" xref="S6.T4.12.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S6.T4.12.m2.1.1.3.3.7" xref="S6.T4.12.m2.1.1.3.3.7.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.T4.12.m2.1c"><apply id="S6.T4.12.m2.1.1.cmml" xref="S6.T4.12.m2.1.1"><times id="S6.T4.12.m2.1.1.1.cmml" xref="S6.T4.12.m2.1.1.1"></times><ci id="S6.T4.12.m2.1.1.2.cmml" xref="S6.T4.12.m2.1.1.2">△</ci><apply id="S6.T4.12.m2.1.1.3.cmml" xref="S6.T4.12.m2.1.1.3"><csymbol cd="ambiguous" id="S6.T4.12.m2.1.1.3.1.cmml" xref="S6.T4.12.m2.1.1.3">subscript</csymbol><ci id="S6.T4.12.m2.1.1.3.2.cmml" xref="S6.T4.12.m2.1.1.3.2">𝑇</ci><apply id="S6.T4.12.m2.1.1.3.3.cmml" xref="S6.T4.12.m2.1.1.3.3"><times id="S6.T4.12.m2.1.1.3.3.1.cmml" xref="S6.T4.12.m2.1.1.3.3.1"></times><ci id="S6.T4.12.m2.1.1.3.3.2.cmml" xref="S6.T4.12.m2.1.1.3.3.2">𝑠</ci><ci id="S6.T4.12.m2.1.1.3.3.3.cmml" xref="S6.T4.12.m2.1.1.3.3.3">𝑢</ci><ci id="S6.T4.12.m2.1.1.3.3.4.cmml" xref="S6.T4.12.m2.1.1.3.3.4">𝑏</ci><ci id="S6.T4.12.m2.1.1.3.3.5.cmml" xref="S6.T4.12.m2.1.1.3.3.5">𝑛</ci><ci id="S6.T4.12.m2.1.1.3.3.6.cmml" xref="S6.T4.12.m2.1.1.3.3.6">𝑒</ci><ci id="S6.T4.12.m2.1.1.3.3.7.cmml" xref="S6.T4.12.m2.1.1.3.3.7">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.12.m2.1d">\triangle T_{subnet}</annotation><annotation encoding="application/x-llamapun" id="S6.T4.12.m2.1e">△ italic_T start_POSTSUBSCRIPT italic_s italic_u italic_b italic_n italic_e italic_t end_POSTSUBSCRIPT</annotation></semantics></math>), and extra memory (<math alttext="\triangle" class="ltx_Math" display="inline" id="S6.T4.13.m3.1"><semantics id="S6.T4.13.m3.1b"><mi id="S6.T4.13.m3.1.1" mathvariant="normal" xref="S6.T4.13.m3.1.1.cmml">△</mi><annotation-xml encoding="MathML-Content" id="S6.T4.13.m3.1c"><ci id="S6.T4.13.m3.1.1.cmml" xref="S6.T4.13.m3.1.1">△</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.13.m3.1d">\triangle</annotation><annotation encoding="application/x-llamapun" id="S6.T4.13.m3.1e">△</annotation></semantics></math> Memory).
</figcaption>
</figure>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Efficiency Comparisons.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px1.p1.1">We compare the efficiency on three aspects (Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.T4" title="Table 4 ‣ 6.2 Large-Scale Results on EC-30 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>).
For trainable parameter increase, introducing lightweight language pair adapters accumulates a significant +87% parameter growth over the baseline.
Next, compared to LaSS, which is fine-tuned to identify sub-networks and demands substantial time (33 hours with 4 Nvidia A6000 GPUs), our approach efficiently locates specialized neurons in just 5 minutes. Considering memory costs, essential for handling numerous languages in deployment environments, our method proves more economical, primarily requiring storage of 1-bit masks for the FFN neurons instead of extensive parameters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Random Mask.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px2.p1.1">We also incorporate the experiments using random masks with Neuron Specialization Training, to validate whether our Specialized Neuron Identification process can capture useful task-specific modularity. We randomly sample 70% neurons to be task-specific and then conduct the same Neuron Specialization Training step. Our results indicate that the random masks strategy sacrifices performance on low-resource tasks (average -0.9 score) to enhance the performance of high-resource O2M directions (+0.9 score). This indicates the effectiveness of our identification method in locating intrinsic task-specific neurons.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">The role of threshold factor.</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px3.p1.2">We explore the impact of our sole hyper-parameter <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S6.SS2.SSS0.Px3.p1.1.m1.1a"><mi id="S6.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S6.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px3.p1.1.m1.1b"><ci id="S6.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS0.Px3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS0.Px3.p1.1.m1.1d">italic_k</annotation></semantics></math> (neuron selection threshold factor) on performance.
The results indicate that performance generally improves with an increase in <math alttext="k" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S6.SS2.SSS0.Px3.p1.2.m2.1a"><mi id="S6.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S6.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S6.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS0.Px3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px3.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS0.Px3.p1.2.m2.1d">italic_k</annotation></semantics></math>, up to a point of 95% (around 25% sparsity), beyond which the performance starts to drop.
See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS5" title="A.5 Sparsity versus Performance ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.5</span></a> for more detailed results.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>The Impact of Reducing Interference</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In this section, we evaluate to what extent our Neuron Specialization method mitigates interference and enhances cross-lingual transfer. Similar to <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib38" title="">2020</a>)</cite>, we train bilingual models that do not contain interference or transfers, and then compare results between bilingual models, the conventional multilingual baseline model (mT-big), and our neuron specialization (ours). We train Transformer-big and Transformer-based models for high- and low-resource tasks, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.SS2" title="A.2 Model and Training Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.T3" title="Table 3 ‣ Scaling up does not always reduce interference. ‣ 6.1 Small-Scale Results on IWSLT ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>, we show that the conventional multilingual model (mT-big) facilitates clear positive transfer for low-resource languages versus bilingual setups, leading to +8.2 (O2M) and +13.3 (M2O) score gains but incurs negative interference for high-resource languages (-3.7 and -1.1 scores).</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Our method reduces interference for high-resource settings, leading to +1.8 and +1.4 SacreBLEU gains over mT-big in O2M and M2O directions. Moreover, our Neuron Specialization enhances low-resource language performance with average gains of +1.6 (O2M) and +1.2 (M2O) SacreBLEU over the mT-big, demonstrating its ability to foster cross-lingual transfer. Despite improvements, our approach still trails behind bilingual models for most high-resource O2M directions, indicating that while interference is largely reduced, room for improvement still exists.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we have identified and leveraged <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">intrinsic task-specific modularity</span> within multilingual networks to mitigate interference. We showed that FFN neurons activate in a language-specific way, and they present structural overlaps that reflect language proximity, which progress across layers. We then introduced <span class="ltx_text ltx_font_italic" id="S7.p1.1.2">Neuron Specialization</span> to leverage these natural modularity signals to structure the network, enhancing task specificity and improving knowledge transfer. Our experimental results, spanning various resource levels, show that our method consistently outperforms strong baseline systems, with additional analyses demonstrating reduced interference and increased knowledge transfer.
Our work deepens the understanding of multilingual models by revealing their intrinsic modularity, offering insights into how multi-task models can be optimized without extensive modifications.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This study primarily focuses on Multilingual Machine Translation, a key method in multi-task learning, using it as our primary testbed.
However, the exploration of multilingual capabilities can be extended beyond translation to include a broader range of Multilingual Natural Language Processing tasks.
These areas remain unexplored in our current research and are considered promising directions for future work.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Additionally, our analysis is limited to the feed-forward network (FFN) components within the Transformer architecture, which, although they constitute a significant portion of the model’s parameters, represent only one facet of its complex structure.
Future investigations could yield valuable insights by assessing the modularity of other Transformer components, such as the attention mechanisms or layer normalization modules, to provide a more comprehensive understanding of the system’s overall functionality.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">Lastly, we conducted our identification methods of specialized neurons primarily on Feed-Forward Networks that use ReLU as the activation function.
This is because neurons after the ReLU naturally present two states: active (&gt;0) and inactive (=0), which offers a clear view of their contributions to the network outputs, thus being inherently interpretable.
Recent work on Large Language Models has also explored the binary activation states of FFN neurons, particularly focused on when neurons are activated, and their roles in aggregating information <cite class="ltx_cite ltx_citemacro_cite">Voita et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib36" title="">2023</a>)</cite>.
We leave the exploration of FFN neurons using other activation functions such as the GELU <cite class="ltx_cite ltx_citemacro_cite">Hendrycks and Gimpel (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib15" title="">2016</a>)</cite>, to future work.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Broader Impact</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">Recognizing the inherent risks of mistranslation in machine translation data, we have made efforts to prioritize the incorporation of high-quality data, such as two open-sourced Multilingual Machine Translation datasets: IWSLT and EC30. Additionally, issues of fairness emerge, meaning that the capacity to generate content may not be equitably distributed across different languages or demographic groups. This can lead to the perpetuation and amplification of existing societal prejudices, such as biases related to gender, embedded in the data.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aharoni et al. (2019)</span>
<span class="ltx_bibblock">
Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Massively multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 3874–3884.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Araabi and Monz (2020)</span>
<span class="ltx_bibblock">
Ali Araabi and Christof Monz. 2020.

</span>
<span class="ltx_bibblock">Optimizing transformer for low-resource neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 3429–3435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna and Firat (2019)</span>
<span class="ltx_bibblock">
Ankur Bapna and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Simple, scalable adaptation for neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1538–1548.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2023)</span>
<span class="ltx_bibblock">
Tyler A Chang, Catherine Arnett, Zhuowen Tu, and Benjamin K Bergen. 2023.

</span>
<span class="ltx_bibblock">When is multilinguality a curse? language modeling for 250 high-and low-resource languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2311.09205</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choenni et al. (2023a)</span>
<span class="ltx_bibblock">
Rochelle Choenni, Dan Garrette, and Ekaterina Shutova. 2023a.

</span>
<span class="ltx_bibblock">Cross-lingual transfer with language-specific subnetworks for low-resource dependency parsing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Computational Linguistics</em>, 49(3):613–641.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choenni et al. (2023b)</span>
<span class="ltx_bibblock">
Rochelle Choenni, Ekaterina Shutova, and Dan Garrette. 2023b.

</span>
<span class="ltx_bibblock">Examining modularity in multilingual lms via language-specialized subnetworks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2311.08273</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chronopoulou et al. (2023)</span>
<span class="ltx_bibblock">
Alexandra Chronopoulou, Dario Stojanovski, and Alexander Fraser. 2023.

</span>
<span class="ltx_bibblock">Language-family adapters for low-resource multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the The Sixth Workshop on Technologies for Machine Translation of Low-Resource Languages (LoResMT 2023)</em>, pages 59–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costa-jussà et al. (2022)</span>
<span class="ltx_bibblock">
Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2207.04672</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dobs et al. (2022)</span>
<span class="ltx_bibblock">
Katharina Dobs, Julio Martinez, Alexander JE Kell, and Nancy Kanwisher. 2022.

</span>
<span class="ltx_bibblock">Brain-like functional specialization emerges spontaneously in deep neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Science advances</em>, 8(11):eabl8913.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2021)</span>
<span class="ltx_bibblock">
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021.

</span>
<span class="ltx_bibblock">Beyond english-centric multilingual machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">The Journal of Machine Learning Research</em>, 22(1):4839–4886.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Federmann et al. (2022)</span>
<span class="ltx_bibblock">
Christian Federmann, Tom Kocmi, and Ying Xin. 2022.

</span>
<span class="ltx_bibblock">Ntrex-128–news test references for mt evaluation of 128 languages.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</em>, pages 21–24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frankle and Carbin (2018)</span>
<span class="ltx_bibblock">
Jonathan Frankle and Michael Carbin. 2018.

</span>
<span class="ltx_bibblock">The lottery ticket hypothesis: Finding sparse, trainable neural networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Dan He, Minh Quang Pham, Thanh-Le Ha, and Marco Turchi. 2023.

</span>
<span class="ltx_bibblock">Gradient-based gradual pruning for language-specific multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 654–670.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks and Gimpel (2016)</span>
<span class="ltx_bibblock">
Dan Hendrycks and Kevin Gimpel. 2016.

</span>
<span class="ltx_bibblock">Gaussian error linear units (gelus).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1606.08415</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, et al. 2017.

</span>
<span class="ltx_bibblock">Google’s multilingual neural machine translation system: Enabling zero-shot translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Transactions of the Association for Computational Linguistics</em>, 5:339–351.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudugunta et al. (2019)</span>
<span class="ltx_bibblock">
Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. 2019.

</span>
<span class="ltx_bibblock">Investigating multilingual nmt representations at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 1565–1575.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Scao et al. (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Gong (2021)</span>
<span class="ltx_bibblock">
Xian Li and Hongyu Gong. 2021.

</span>
<span class="ltx_bibblock">Robust optimization for multilingual translation with imbalanced data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>, 34:25086–25099.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2023a)</span>
<span class="ltx_bibblock">
Baohao Liao, Yan Meng, and Christof Monz. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.233" title="">Parameter-efficient fine-tuning without introducing new latency</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 4242–4260, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al. (2023b)</span>
<span class="ltx_bibblock">
Baohao Liao, Shaomu Tan, and Christof Monz. 2023b.

</span>
<span class="ltx_bibblock">Make pre-trained model reversible: From parameter to memory efficient fine-tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Zehui Lin, Liwei Wu, Mingxuan Wang, and Lei Li. 2021.

</span>
<span class="ltx_bibblock">Learning language specific sub-network for multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 293–305.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott et al. (2019)</span>
<span class="ltx_bibblock">
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">fairseq: A fast, extensible toolkit for sequence modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1904.01038</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2022)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe. 2022.

</span>
<span class="ltx_bibblock">Lifting the curse of multilinguality by pre-training modular transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3479–3495.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2023)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Sebastian Ruder, Ivan Vulić, and Edoardo Ponti. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=z9EkXfvxta" title="">Modular deep learning</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Transactions on Machine Learning Research</em>.

</span>
<span class="ltx_bibblock">Survey Certification.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock">How multilingual is multilingual bert?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4996–5001.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2023)</span>
<span class="ltx_bibblock">
Telmo Pires, Robin Schmidt, Yi-Hsiu Liao, and Stephan Peitz. 2023.

</span>
<span class="ltx_bibblock">Learning language-specific layers for multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14767–14783.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2017)</span>
<span class="ltx_bibblock">
Maja Popović. 2017.

</span>
<span class="ltx_bibblock">chrf++: words helping character n-grams.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the second conference on machine translation</em>, pages 612–618.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Post (2018)</span>
<span class="ltx_bibblock">
Matt Post. 2018.

</span>
<span class="ltx_bibblock">A call for clarity in reporting bleu scores.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the Third Conference on Machine Translation: Research Papers</em>, pages 186–191.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock">Comet: A neural framework for mt evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaham et al. (2023)</span>
<span class="ltx_bibblock">
Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, and Shruti Bhosale. 2023.

</span>
<span class="ltx_bibblock">Causes and cures for interference in multilingual translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Monz (2023)</span>
<span class="ltx_bibblock">
Shaomu Tan and Christof Monz. 2023.

</span>
<span class="ltx_bibblock">Towards a better understanding of variations in zero-shot neural machine translation performance.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 13553–13568.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voita et al. (2023)</span>
<span class="ltx_bibblock">
Elena Voita, Javier Ferrando, and Christoforos Nalmpantis. 2023.

</span>
<span class="ltx_bibblock">Neurons in large language models: Dead, n-gram, positional.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2309.04827</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Zhang (2022)</span>
<span class="ltx_bibblock">
Qian Wang and Jiajun Zhang. 2022.

</span>
<span class="ltx_bibblock">Parameter differentiation based multilingual neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 36, pages 11440–11448.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2020)</span>
<span class="ltx_bibblock">
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. 2020.

</span>
<span class="ltx_bibblock">Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Monz (2023)</span>
<span class="ltx_bibblock">
Di Wu and Christof Monz. 2023.

</span>
<span class="ltx_bibblock">Beyond shared vocabulary: Increasing representational word similarities across languages for multilingual machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xin et al. (2022)</span>
<span class="ltx_bibblock">
Derrick Xin, Behrooz Ghorbani, Justin Gilmer, Ankush Garg, and Orhan Firat. 2022.

</span>
<span class="ltx_bibblock">Do current multi-task optimization methods in deep learning even help?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Advances in neural information processing systems</em>, 35:13597–13609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Guangyu Robert Yang, Madhura R Joglekar, H Francis Song, William T Newsome, and Xiao-Jing Wang. 2019.

</span>
<span class="ltx_bibblock">Task representations in neural networks trained to perform many cognitive tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Nature neuroscience</em>, 22(2):297–306.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020a)</span>
<span class="ltx_bibblock">
Biao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat. 2020a.

</span>
<span class="ltx_bibblock">Share or not? learning to schedule language-specific capacity for multilingual translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020b)</span>
<span class="ltx_bibblock">
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020b.

</span>
<span class="ltx_bibblock">Improving massively multilingual neural machine translation and zero-shot translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 1628–1639.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Chaojun Xiao, Xiaozhi Wang, Xu Han, Zhiyuan Liu, Ruobing Xie, Maosong Sun, and Jie Zhou. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.250" title="">Emergent modularity in pre-trained transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 4066–4083, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>
<figure class="ltx_table" id="A1.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="A1.T5.1" style="width:624.3pt;height:145pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A1.T5.1.1"><span class="ltx_text" id="A1.T5.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T5.1.1.1.1">
<span class="ltx_thead">
<span class="ltx_tr" id="A1.T5.1.1.1.1.2.1">
<span class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A1.T5.1.1.1.1.2.1.1" style="padding:4.25pt 3.0pt;"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3" id="A1.T5.1.1.1.1.2.1.2" style="padding:4.25pt 3.0pt;">Germanic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3" id="A1.T5.1.1.1.1.2.1.3" style="padding:4.25pt 3.0pt;">Romance</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3" id="A1.T5.1.1.1.1.2.1.4" style="padding:4.25pt 3.0pt;">Slavic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_colspan ltx_colspan_3" id="A1.T5.1.1.1.1.2.1.5" style="padding:4.25pt 3.0pt;">Indo-Aryan</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_3" id="A1.T5.1.1.1.1.2.1.6" style="padding:4.25pt 3.0pt;">Afro-Asiatic</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="A1.T5.1.1.1.1.3.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_r" id="A1.T5.1.1.1.1.3.1.1" style="padding:4.25pt 3.0pt;"></span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.2" style="padding:4.25pt 3.0pt;">ISO</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.3" style="padding:4.25pt 3.0pt;">Language</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.3.1.4" style="padding:4.25pt 3.0pt;">Script</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.5" style="padding:4.25pt 3.0pt;">ISO</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.6" style="padding:4.25pt 3.0pt;">Language</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.3.1.7" style="padding:4.25pt 3.0pt;">Script</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.8" style="padding:4.25pt 3.0pt;">ISO</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.9" style="padding:4.25pt 3.0pt;">Language</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.3.1.10" style="padding:4.25pt 3.0pt;">Script</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.11" style="padding:4.25pt 3.0pt;">ISO</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.12" style="padding:4.25pt 3.0pt;">Language</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.3.1.13" style="padding:4.25pt 3.0pt;">Script</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.14" style="padding:4.25pt 3.0pt;">ISO</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.15" style="padding:4.25pt 3.0pt;">Language</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.3.1.16" style="padding:4.25pt 3.0pt;">Script</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.4.2">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="A1.T5.1.1.1.1.4.2.1" style="padding:4.25pt 3.0pt;"><span class="ltx_text" id="A1.T5.1.1.1.1.4.2.1.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.1.4.2.1.1.1">
<span class="ltx_tr" id="A1.T5.1.1.1.1.4.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.4.2.1.1.1.1.1" style="padding:4.25pt 3.0pt;">High</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.4.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.4.2.1.1.1.2.1" style="padding:4.25pt 3.0pt;">(5m)</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.2" style="padding:4.25pt 3.0pt;">de</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.3" style="padding:4.25pt 3.0pt;">German</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.4.2.4" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.5" style="padding:4.25pt 3.0pt;">fr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.6" style="padding:4.25pt 3.0pt;">French</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.4.2.7" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.8" style="padding:4.25pt 3.0pt;">ru</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.9" style="padding:4.25pt 3.0pt;">Russian</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.4.2.10" style="padding:4.25pt 3.0pt;">Cyrillic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.11" style="padding:4.25pt 3.0pt;">hi</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.12" style="padding:4.25pt 3.0pt;">Hindi</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.4.2.13" style="padding:4.25pt 3.0pt;">Devanagari</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.14" style="padding:4.25pt 3.0pt;">ar</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.15" style="padding:4.25pt 3.0pt;">Arabic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.4.2.16" style="padding:4.25pt 3.0pt;">Arabic</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.5.3">
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.1" style="padding:4.25pt 3.0pt;">nl</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.2" style="padding:4.25pt 3.0pt;">Dutch</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.5.3.3" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.4" style="padding:4.25pt 3.0pt;">es</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.5" style="padding:4.25pt 3.0pt;">Spanish</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.5.3.6" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.7" style="padding:4.25pt 3.0pt;">cs</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.8" style="padding:4.25pt 3.0pt;">Czech</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.5.3.9" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.10" style="padding:4.25pt 3.0pt;">bn</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.11" style="padding:4.25pt 3.0pt;">Bengali</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.5.3.12" style="padding:4.25pt 3.0pt;">Bengali</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.13" style="padding:4.25pt 3.0pt;">he</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.14" style="padding:4.25pt 3.0pt;">Hebrew</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.5.3.15" style="padding:4.25pt 3.0pt;">Hebrew</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.6.4">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="A1.T5.1.1.1.1.6.4.1" style="padding:4.25pt 3.0pt;"><span class="ltx_text" id="A1.T5.1.1.1.1.6.4.1.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.1.6.4.1.1.1">
<span class="ltx_tr" id="A1.T5.1.1.1.1.6.4.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.6.4.1.1.1.1.1" style="padding:4.25pt 3.0pt;">Med</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.6.4.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.6.4.1.1.1.2.1" style="padding:4.25pt 3.0pt;">(1m)</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.2" style="padding:4.25pt 3.0pt;">sv</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.3" style="padding:4.25pt 3.0pt;">Swedish</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.6.4.4" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.5" style="padding:4.25pt 3.0pt;">it</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.6" style="padding:4.25pt 3.0pt;">Italian</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.6.4.7" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.8" style="padding:4.25pt 3.0pt;">pl</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.9" style="padding:4.25pt 3.0pt;">Polish</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.6.4.10" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.11" style="padding:4.25pt 3.0pt;">kn</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.12" style="padding:4.25pt 3.0pt;">Kannada</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.6.4.13" style="padding:4.25pt 3.0pt;">Devanagari</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.14" style="padding:4.25pt 3.0pt;">mt</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.15" style="padding:4.25pt 3.0pt;">Maltese</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.6.4.16" style="padding:4.25pt 3.0pt;">Latin</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.1">
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.2" style="padding:4.25pt 3.0pt;">da</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.3" style="padding:4.25pt 3.0pt;">Danish</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.1.4" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.5" style="padding:4.25pt 3.0pt;">pt</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.6" style="padding:4.25pt 3.0pt;">Portuguese</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.1.7" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.8" style="padding:4.25pt 3.0pt;">bg</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.9" style="padding:4.25pt 3.0pt;">Bulgarian</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.1.10" style="padding:4.25pt 3.0pt;">Cyrillic</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.11" style="padding:4.25pt 3.0pt;">mr</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.12" style="padding:4.25pt 3.0pt;">Marathi</span>
<span class="ltx_td ltx_align_center ltx_border_r" id="A1.T5.1.1.1.1.1.13" style="padding:4.25pt 3.0pt;">Devanagari</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.14" style="padding:4.25pt 3.0pt;">ha</span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.1" style="padding:4.25pt 3.0pt;">Hausa<sup class="ltx_sup" id="A1.T5.1.1.1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="A1.T5.1.1.1.1.1.1.1.1">∗</span></sup></span>
<span class="ltx_td ltx_align_center" id="A1.T5.1.1.1.1.1.15" style="padding:4.25pt 3.0pt;">Latin</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.7.5">
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_bb ltx_border_r ltx_border_t ltx_rowspan ltx_rowspan_2" id="A1.T5.1.1.1.1.7.5.1" style="padding:4.25pt 3.0pt;"><span class="ltx_text" id="A1.T5.1.1.1.1.7.5.1.1">
<span class="ltx_tabular ltx_align_middle" id="A1.T5.1.1.1.1.7.5.1.1.1">
<span class="ltx_tr" id="A1.T5.1.1.1.1.7.5.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.7.5.1.1.1.1.1" style="padding:4.25pt 3.0pt;">Low</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.7.5.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A1.T5.1.1.1.1.7.5.1.1.1.2.1" style="padding:4.25pt 3.0pt;">(100k)</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.2" style="padding:4.25pt 3.0pt;">af</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.3" style="padding:4.25pt 3.0pt;">Afrikaans</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.7.5.4" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.5" style="padding:4.25pt 3.0pt;">ro</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.6" style="padding:4.25pt 3.0pt;">Romanian</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.7.5.7" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.8" style="padding:4.25pt 3.0pt;">uk</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.9" style="padding:4.25pt 3.0pt;">Ukrainian</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.7.5.10" style="padding:4.25pt 3.0pt;">Cyrillic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.11" style="padding:4.25pt 3.0pt;">sd</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.12" style="padding:4.25pt 3.0pt;">Sindhi</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T5.1.1.1.1.7.5.13" style="padding:4.25pt 3.0pt;">Arabic</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.14" style="padding:4.25pt 3.0pt;">ti</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.15" style="padding:4.25pt 3.0pt;">Tigrinya</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T5.1.1.1.1.7.5.16" style="padding:4.25pt 3.0pt;">Ethiopic</span></span>
<span class="ltx_tr" id="A1.T5.1.1.1.1.8.6">
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.1" style="padding:4.25pt 3.0pt;">lb</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.2" style="padding:4.25pt 3.0pt;">Luxembourgish</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T5.1.1.1.1.8.6.3" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.4" style="padding:4.25pt 3.0pt;">oc</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.5" style="padding:4.25pt 3.0pt;">Occitan</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T5.1.1.1.1.8.6.6" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.7" style="padding:4.25pt 3.0pt;">sr</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.8" style="padding:4.25pt 3.0pt;">Serbian</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T5.1.1.1.1.8.6.9" style="padding:4.25pt 3.0pt;">Latin</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.10" style="padding:4.25pt 3.0pt;">gu</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.11" style="padding:4.25pt 3.0pt;">Gujarati</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T5.1.1.1.1.8.6.12" style="padding:4.25pt 3.0pt;">Devanagari</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.13" style="padding:4.25pt 3.0pt;">am</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.14" style="padding:4.25pt 3.0pt;">Amharic</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="A1.T5.1.1.1.1.8.6.15" style="padding:4.25pt 3.0pt;">Ethiopic</span></span>
</span>
</span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Details of EC30 Training Dataset. Numbers in the table represent the number of sentences, for example, 5m denotes exactly 5,000,000 number of sentences. The only exception is Hausa, where its size is 334k (334,000).</figcaption>
</figure>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.1" style="width:346.9pt;height:101.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-102.8pt,30.1pt) scale(0.627835816004967,0.627835816004967) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T6.1.1.1.2" rowspan="2"><span class="ltx_text" id="A1.T6.1.1.1.2.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.3" rowspan="2"><span class="ltx_text" id="A1.T6.1.1.1.3.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.4">Num.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.5">Num.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.6">Num.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.7" rowspan="2"><span class="ltx_text" id="A1.T6.1.1.1.7.1">dim</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1" rowspan="2"><span class="ltx_text" id="A1.T6.1.1.1.1.1"><math alttext="d_{\mathit{ff}}" class="ltx_Math" display="inline" id="A1.T6.1.1.1.1.1.m1.1"><semantics id="A1.T6.1.1.1.1.1.m1.1a"><msub id="A1.T6.1.1.1.1.1.m1.1.1" xref="A1.T6.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T6.1.1.1.1.1.m1.1.1.2" xref="A1.T6.1.1.1.1.1.m1.1.1.2.cmml">d</mi><mi id="A1.T6.1.1.1.1.1.m1.1.1.3" xref="A1.T6.1.1.1.1.1.m1.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T6.1.1.1.1.1.m1.1b"><apply id="A1.T6.1.1.1.1.1.m1.1.1.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.T6.1.1.1.1.1.m1.1.1.1.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A1.T6.1.1.1.1.1.m1.1.1.2.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1.2">𝑑</ci><ci id="A1.T6.1.1.1.1.1.m1.1.1.3.cmml" xref="A1.T6.1.1.1.1.1.m1.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.1.1.1.1.1.m1.1c">d_{\mathit{ff}}</annotation><annotation encoding="application/x-llamapun" id="A1.T6.1.1.1.1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.8">max</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.9">update</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.10" rowspan="2"><span class="ltx_text" id="A1.T6.1.1.1.10.1">dropout</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.2.1">
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.2.1.1">trainable params</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.2.1.2">Layer</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.2.1.3">Attn Head</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.2.1.4">tokens</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.2.1.5">freq</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.3.2.1">mT-shallow</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.2">IWSLT</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.3">47M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.4">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.6">512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.7">1,024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.8">2,560</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.9">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.2.10">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.1.1.4.3.1">mT-small</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.2">IWSLT</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.3">76M</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.4">6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.5">8</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.6">512</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.7">1,024</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.8">2,560</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.9">4</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.4.3.10">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.5.4.1">bilingual-low</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.2">EC30</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.3">52M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.4">6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.6">512</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.7">1,024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.8">2,560</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.9">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.5.4.10">0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.1.1.6.5.1">bilingual-high</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.2">EC30</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.3">439M</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.4">6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.5">16</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.6">1,024</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.7">4096</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.8">2,560</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.9">10</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.6.5.10">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.1.1.7.6.1">mT-big</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.2">EC30</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.3">439M</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.4">6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.5">16</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.6">1,024</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.7">4,096</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.8">7,680</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.9">21</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.7.6.10">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T6.1.1.8.7.1">LaSS</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.2">EC30</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.3">439M</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.4">6</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.5">16</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.6">1,024</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.7">4,096</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.8">7,680</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.9">21</td>
<td class="ltx_td ltx_align_center" id="A1.T6.1.1.8.7.10">0.1</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T6.1.1.9.8.1">Neuron Specialization</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.2">EC30</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.3">439M</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.4">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.5">16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.6">1,024</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.7">4,096</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.8">7,680</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.9">21</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T6.1.1.9.8.10">0.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Configuration and hyper-parameter settings for all models in this paper. Num. Layer and Attn Head denote the number of layers and attention heads, respectively. dim represents the dimension of the Transformer model, <math alttext="d_{\mathit{ff}}" class="ltx_Math" display="inline" id="A1.T6.3.m1.1"><semantics id="A1.T6.3.m1.1b"><msub id="A1.T6.3.m1.1.1" xref="A1.T6.3.m1.1.1.cmml"><mi id="A1.T6.3.m1.1.1.2" xref="A1.T6.3.m1.1.1.2.cmml">d</mi><mi id="A1.T6.3.m1.1.1.3" xref="A1.T6.3.m1.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="A1.T6.3.m1.1c"><apply id="A1.T6.3.m1.1.1.cmml" xref="A1.T6.3.m1.1.1"><csymbol cd="ambiguous" id="A1.T6.3.m1.1.1.1.cmml" xref="A1.T6.3.m1.1.1">subscript</csymbol><ci id="A1.T6.3.m1.1.1.2.cmml" xref="A1.T6.3.m1.1.1.2">𝑑</ci><ci id="A1.T6.3.m1.1.1.3.cmml" xref="A1.T6.3.m1.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T6.3.m1.1d">d_{\mathit{ff}}</annotation><annotation encoding="application/x-llamapun" id="A1.T6.3.m1.1e">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math> means the dimension of the feed-forward layer. bilingual-low and -high represent the bilingual models for low and high-resource languages.</figcaption>
</figure>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Dataset details</h3>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">IWSLT</h5>
<div class="ltx_para" id="A1.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px1.p1.1">We collect and pre-processes the IWSLT-14 dataset following <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite>. We refer readers to <cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite> for more details.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">EC30</h5>
<div class="ltx_para" id="A1.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS1.SSS0.Px2.p1.1">We utilize the EC30, a subset of the EC40 dataset <cite class="ltx_cite ltx_citemacro_cite">Tan and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib34" title="">2023</a>)</cite> (with 10 extremely low-resource languages removed in our experiments) as our main dataset for most experiments and analyses. We list the Languages with their ISO and scripts in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T5" title="Table 5 ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, along with their number of sentences. In general, EC30 is an English-centric Multilingual Machine Translation dataset containing 61 million sentences covering 30 languages (excluding English). It collected data from 5 representative language families with multiple writing scripts. In addition, EC30 is well balanced at each resource level, for example, for all high-resource languages, the number of training sentences is 5 million. Note that the EC30 is already pre-processed and tokenized (with Moses tokenizer), thus we directly use it for our study.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Model and Training Details</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">We list the configurations and hyper-parameter settings of all systems for the main training setting (EC30) in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T6" title="Table 6 ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>. As for global training settings, we adopt the pre-norm and share the decoder input output embedding for all systems. We use cross entropy with label smoothing to avoid overfitting (smoothing factor=0.1) and set early stopping to 20 for all systems. Similar to <cite class="ltx_cite ltx_citemacro_citet">Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib11" title="">2021</a>)</cite>, we prepend language tags to the source and target sentences to indicate the translation directions for all multilingual translation systems.</p>
</div>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Bilingual models.</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p1.1">For bilingual models of low-resource languages, we adopt the suggested hyper-parameter settings from <cite class="ltx_cite ltx_citemacro_citet">Araabi and Monz (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib2" title="">2020</a>)</cite>, such as <math alttext="d_{\mathit{ff}}=512" class="ltx_Math" display="inline" id="A1.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="A1.SS2.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><msub id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml"><mi id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.2" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml">d</mi><mi id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.3" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml">𝑓𝑓</mi></msub><mo id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.3" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml">512</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1"><eq id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.1"></eq><apply id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.1.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2">subscript</csymbol><ci id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.2.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.2">𝑑</ci><ci id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.3.cmml" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.2.3">𝑓𝑓</ci></apply><cn id="A1.SS2.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="A1.SS2.SSS0.Px1.p1.1.m1.1.1.3">512</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.SSS0.Px1.p1.1.m1.1c">d_{\mathit{ff}}=512</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.SSS0.Px1.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT = 512</annotation></semantics></math>, number of attention head as 2, and dropout as 0.3. Furthermore, We train separate dictionaries for low-resource bilingual models to avoid potential overfitting instead of using the large 128k shared multilingual dictionary.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS0.Px1.p2">
<p class="ltx_p" id="A1.SS2.SSS0.Px1.p2.1">For bilingual models of high-resource languages, we adopt the 128k shared multilingual dictionary and train models with the Transformer-big architecture as the multilingual baseline (mT-big). The detailed configurations can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T6" title="Table 6 ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Language Pair Adapters.</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px2.p1.1">We implement Language Pair Adapters <cite class="ltx_cite ltx_citemacro_cite">Bapna and Firat (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib3" title="">2019</a>)</cite> by ourselves based on Fairseq. The Language Pair Adapter is learned depending on each pair, e.g., we learn two modules for en-de, namely en on the Encoder side and the de on the Decoder side. Note that, except for the unified pre-trained model, language pair adapters do not share any parameters with each other, preventing potential knowledge transfers. We set its bottleneck dimension as 128 for all experiments of IWSLT and EC30.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS0.Px2.p2">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">IWSLT.</span> For the IWSLT dataset that contains 8 languages with 16 language pairs/translation directions, the size mT-small base model is 76M. Language Pair Adapters insert 3.2M additional trainable parameters for one language pair, thus resulting in 51.2M added parameters for all language pairs, leading to 67% relative parameter increase over the baseline model.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">EC30.</span> For the EC30 dataset that contains 30 languages with 60 language pairs/translation directions, the size mT-big base model is 439M. Language Pair Adapters insert 6.4M extra trainable parameters for one language pair, thus resulting in 384M added parameters for all language pairs, leading to 87% relative parameter increase over the baseline model.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Language Family Adapters. </h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px3.p1.1">The Language Family Adapter <cite class="ltx_cite ltx_citemacro_cite">Chronopoulou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib7" title="">2023</a>)</cite> is learned depending on each language family, e.g., for all 6 Germanic languages in the EC30, we learn two modules for en-Germanic, namely the en adapter on the Encoder side and the Germanic adapter on the Decoder side. We set its bottleneck dimension as 512 for all experiments for the EC30.</p>
</div>
<div class="ltx_para" id="A1.SS2.SSS0.Px3.p2">
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i1.p1.1.1">EC30.</span> For the EC30 dataset that contains 30 languages with 60 language pairs/translation directions, the size mT-big base model is 439M. Language Family Adapters insert 25.3M additional trainable parameters for one family (on EN-X directions), thus resulting in 303.6M added parameters for all families on both EN-X and X-En directions, leading to 69% relative parameter increase over the baseline model.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">LaSS.</h5>
<div class="ltx_para" id="A1.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="A1.SS2.SSS0.Px4.p1.1">When reproducing LaSS <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib23" title="">2021</a>)</cite>, we adopt the code from their official Github page<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/NLP-Playground/LaSS</span></span></span> with the same hyper-parameter setting as they suggested in their paper. For the IWSLT dataset, we finetune the mT-small for each translation direction with dropout=0.3, we then identify the language-specific parameters for attention and feed-forward modules (the setting with the strongest improvements in their paper) with a pruning rate of 70%. We continue to train the sparse networks while keeping the same setting as the pre-training phase as they suggested. Note that we observed different results as they reported in the paper, even though we used the same code, hyper-parameter settings, and corresponding Python environment and package version. We also found that <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib14" title="">2023</a>)</cite> reproduced LaSS results in their paper, which shows similar improvements (around +0.6 BLUE gains) over the baseline of our reproductions. As for an improved method over LaSS proposed by <cite class="ltx_cite ltx_citemacro_citet">He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib14" title="">2023</a>)</cite>, we do not reproduce their method since no open-source code has been released.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Pseudocode of Neuron Specialization</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">We provide the pseudocode of our proposed method, <span class="ltx_text ltx_font_italic" id="A1.SS3.p1.1.1">Neuron Specialization</span>. We present the process of Specialized Neuron Identification in Algorithm. <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#alg1" title="Algorithm 1 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> and Neuron Specialization Training in Algorithm. <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#alg2" title="Algorithm 2 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Result Details using ChrF++ and COMET</h3>
<div class="ltx_para" id="A1.SS4.p1">
<p class="ltx_p" id="A1.SS4.p1.1">For our main experiments in the EC30, we further provide the ChrF++ <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib30" title="">2017</a>)</cite> and COMET <cite class="ltx_cite ltx_citemacro_cite">Rei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#bib.bib32" title="">2020</a>)</cite> scores as extra results, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T7" title="Table 7 ‣ A.4 Result Details using ChrF++ and COMET ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.T8" title="Table 8 ‣ A.4 Result Details using ChrF++ and COMET ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a>, respectively. Similar to what we observed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S6.SS2" title="6.2 Large-Scale Results on EC-30 ‣ 6 Results and Analyses ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6.2</span></a>, our Neuron Specialization presents consistent performance improvements over the baseline model while outperforming other methods such as LaSS and Adapters.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.3" style="width:433.6pt;height:145pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.4pt,17.5pt) scale(0.805376766178921,0.805376766178921) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T7.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.1.1.1.2" rowspan="2"><span class="ltx_text" id="A1.T7.1.1.1.2.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.1.1.1.1" rowspan="2"><span class="ltx_text" id="A1.T7.1.1.1.1.1"><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="A1.T7.1.1.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.1.1.m1.1a"><mrow id="A1.T7.1.1.1.1.1.m1.1.1" xref="A1.T7.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T7.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="A1.T7.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo id="A1.T7.1.1.1.1.1.m1.1.1.1" xref="A1.T7.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.T7.1.1.1.1.1.m1.1.1.3" xref="A1.T7.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.1.1.m1.1b"><apply id="A1.T7.1.1.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1"><times id="A1.T7.1.1.1.1.1.m1.1.1.1.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1.1"></times><ci id="A1.T7.1.1.1.1.1.m1.1.1.2.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="A1.T7.1.1.1.1.1.m1.1.1.3.cmml" xref="A1.T7.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.1.1.m1.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.1.1.m1.1d">roman_Δ italic_θ</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.1.1.1.3">High (5M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.1.1.1.4">Med (1M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T7.1.1.1.5">Low (100K)</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="3" id="A1.T7.1.1.1.6">All (61M)</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.1">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.2">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.3">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.4">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.5">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.6">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.7">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.8">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.9">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="A1.T7.3.3.4.1.10">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.11">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.4.1.12">Avg</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.5.2.1">mT-big</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.5.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.3">52.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.4">57.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.5">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.6">53.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.7">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.8">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.9">42.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.10">50.0</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.5.2.11">46.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.12">49.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.13">54.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.5.2.14">52.2</td>
</tr>
<tr class="ltx_tr" id="A1.T7.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.2.2.1">Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="A1.T7.2.2.2.1.m1.1"><semantics id="A1.T7.2.2.2.1.m1.1a"><msub id="A1.T7.2.2.2.1.m1.1.1" xref="A1.T7.2.2.2.1.m1.1.1.cmml"><mi id="A1.T7.2.2.2.1.m1.1.1a" xref="A1.T7.2.2.2.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="A1.T7.2.2.2.1.m1.1.1.1" xref="A1.T7.2.2.2.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.1.m1.1b"><apply id="A1.T7.2.2.2.1.m1.1.1.cmml" xref="A1.T7.2.2.2.1.m1.1.1"><ci id="A1.T7.2.2.2.1.m1.1.1.1a.cmml" xref="A1.T7.2.2.2.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="A1.T7.2.2.2.1.m1.1.1.1.cmml" mathsize="70%" xref="A1.T7.2.2.2.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.2.2.2">+87%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.3">+1.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.4">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.5">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.6">+1.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.7">+0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.8">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.9">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.10">+0.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.2.2.2.11">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.12">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.13">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.2.2.2.14">+0.5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.3.1">Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="A1.T7.3.3.3.1.m1.1"><semantics id="A1.T7.3.3.3.1.m1.1a"><msub id="A1.T7.3.3.3.1.m1.1.1" xref="A1.T7.3.3.3.1.m1.1.1.cmml"><mi id="A1.T7.3.3.3.1.m1.1.1a" xref="A1.T7.3.3.3.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="A1.T7.3.3.3.1.m1.1.1.1" xref="A1.T7.3.3.3.1.m1.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.1.m1.1b"><apply id="A1.T7.3.3.3.1.m1.1.1.cmml" xref="A1.T7.3.3.3.1.m1.1.1"><ci id="A1.T7.3.3.3.1.m1.1.1.1a.cmml" xref="A1.T7.3.3.3.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="A1.T7.3.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="A1.T7.3.3.3.1.m1.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.1.m1.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="A1.T7.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.3.2">+70%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.3">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.4">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.5">+0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.6">+0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.7">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.8">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.9">+1.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.10">+0.4</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.3.11">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.12">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.13">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.3.14">+0.5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.6.3.1">LaSS</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.6.3.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.6.3.3.1">+1.7</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.4">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.5">+1.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.6"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.6.3.6.1">+1.3</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.7">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.8">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.9">-0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.10">-1.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.6.3.11">-0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.12">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.13">-0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.6.3.14">+0.5</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.7.4.1">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.7.4.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.3">+0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.4">-0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.5">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.6">+0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.7">-0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.8">-0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.9">-0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.10">-1.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.7.4.11">-0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.12">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.13">-0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.7.4.14">-0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.8.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.8.5.1">Ours-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.8.5.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.3">+1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.4">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.5">+1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.6">+0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.7">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.8">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.9">+0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.10">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.3.3.8.5.11">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.12">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.13">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.3.3.8.5.14">+0.8</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.9.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.9.6.1">Ours-Dec</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.9.6.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.3">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.4">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.5">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.6">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.7">+1.0</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.8">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.9">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.10">+1.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.3.3.9.6.11">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.12">+0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.13">+1.0</td>
<td class="ltx_td ltx_align_center" id="A1.T7.3.3.9.6.14">+0.9</td>
</tr>
<tr class="ltx_tr" id="A1.T7.3.3.10.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.3.3.10.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.3.3.10.7.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.3">+1.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.4.1">+1.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.5"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.5.1">+1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.6">+1.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.7"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.7.1">+0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.8"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.8.1">+1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.9"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.9.1">+1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.10"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.10.1">+0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.3.3.10.7.11"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.11.1">+1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.12"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.12.1">+1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.13"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.13.1">+0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.3.3.10.7.14"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.10.7.14.1">+1.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Average <span class="ltx_text ltx_font_bold" id="A1.T7.6.1">ChrF++</span> improvements on the EC30 dataset over the baseline (mT-big), categorized by High, Medium, and Low-resource translation directions. ’Ours-Enc’ and ’Ours-Dec’ indicate neuron specialization applied solely to the Encoder and Decoder, respectively, while ’Ours’ signifies the method applied to both components. The best results are highlighted in <span class="ltx_text ltx_font_bold" id="A1.T7.7.2">bold</span>.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T8.3" style="width:433.6pt;height:144.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-53.5pt,17.8pt) scale(0.802066302110233,0.802066302110233) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T8.1.1.1.2" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.1.2.1">Methods</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T8.1.1.1.1" rowspan="2"><span class="ltx_text" id="A1.T8.1.1.1.1.1"><math alttext="\Delta\theta" class="ltx_Math" display="inline" id="A1.T8.1.1.1.1.1.m1.1"><semantics id="A1.T8.1.1.1.1.1.m1.1a"><mrow id="A1.T8.1.1.1.1.1.m1.1.1" xref="A1.T8.1.1.1.1.1.m1.1.1.cmml"><mi id="A1.T8.1.1.1.1.1.m1.1.1.2" mathvariant="normal" xref="A1.T8.1.1.1.1.1.m1.1.1.2.cmml">Δ</mi><mo id="A1.T8.1.1.1.1.1.m1.1.1.1" xref="A1.T8.1.1.1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="A1.T8.1.1.1.1.1.m1.1.1.3" xref="A1.T8.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.T8.1.1.1.1.1.m1.1b"><apply id="A1.T8.1.1.1.1.1.m1.1.1.cmml" xref="A1.T8.1.1.1.1.1.m1.1.1"><times id="A1.T8.1.1.1.1.1.m1.1.1.1.cmml" xref="A1.T8.1.1.1.1.1.m1.1.1.1"></times><ci id="A1.T8.1.1.1.1.1.m1.1.1.2.cmml" xref="A1.T8.1.1.1.1.1.m1.1.1.2">Δ</ci><ci id="A1.T8.1.1.1.1.1.m1.1.1.3.cmml" xref="A1.T8.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.1.1.1.1.1.m1.1c">\Delta\theta</annotation><annotation encoding="application/x-llamapun" id="A1.T8.1.1.1.1.1.m1.1d">roman_Δ italic_θ</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T8.1.1.1.3">High (5M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T8.1.1.1.4">Med (1M)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="A1.T8.1.1.1.5">Low (100K)</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_tt" colspan="3" id="A1.T8.1.1.1.6">All (61M)</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.4.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.1">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.2">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.3">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.4">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.5">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.6">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.7">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.8">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.9">Avg</td>
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_t" id="A1.T8.3.3.4.1.10">O2M</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.11">M2O</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.4.1.12">Avg</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.5.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.5.2.1">mT-big</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.5.2.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.3">83.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.4">83.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.5">83.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.6">81.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.7">80.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.8">80.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.9">73.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.10">73.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.5.2.11">73.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.12">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.13">79.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.5.2.14">79.1</td>
</tr>
<tr class="ltx_tr" id="A1.T8.2.2.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.2.2.2.1">Adapter<math alttext="{}_{\textit{LP}}" class="ltx_Math" display="inline" id="A1.T8.2.2.2.1.m1.1"><semantics id="A1.T8.2.2.2.1.m1.1a"><msub id="A1.T8.2.2.2.1.m1.1.1" xref="A1.T8.2.2.2.1.m1.1.1.cmml"><mi id="A1.T8.2.2.2.1.m1.1.1a" xref="A1.T8.2.2.2.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="A1.T8.2.2.2.1.m1.1.1.1" xref="A1.T8.2.2.2.1.m1.1.1.1a.cmml">LP</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T8.2.2.2.1.m1.1b"><apply id="A1.T8.2.2.2.1.m1.1.1.cmml" xref="A1.T8.2.2.2.1.m1.1.1"><ci id="A1.T8.2.2.2.1.m1.1.1.1a.cmml" xref="A1.T8.2.2.2.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="A1.T8.2.2.2.1.m1.1.1.1.cmml" mathsize="70%" xref="A1.T8.2.2.2.1.m1.1.1.1">LP</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.2.2.1.m1.1c">{}_{\textit{LP}}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.2.2.2.1.m1.1d">start_FLOATSUBSCRIPT LP end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.2.2.2.2">+87%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.3">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.4">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.5">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.6">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.7">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.8">+0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.9">0</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.10">+0.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.2.2.2.11">0</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.12">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.13">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.2.2.2.14">+0.4</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.3.1">Adapter<math alttext="{}_{\textit{Fam}}" class="ltx_Math" display="inline" id="A1.T8.3.3.3.1.m1.1"><semantics id="A1.T8.3.3.3.1.m1.1a"><msub id="A1.T8.3.3.3.1.m1.1.1" xref="A1.T8.3.3.3.1.m1.1.1.cmml"><mi id="A1.T8.3.3.3.1.m1.1.1a" xref="A1.T8.3.3.3.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="A1.T8.3.3.3.1.m1.1.1.1" xref="A1.T8.3.3.3.1.m1.1.1.1a.cmml">Fam</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T8.3.3.3.1.m1.1b"><apply id="A1.T8.3.3.3.1.m1.1.1.cmml" xref="A1.T8.3.3.3.1.m1.1.1"><ci id="A1.T8.3.3.3.1.m1.1.1.1a.cmml" xref="A1.T8.3.3.3.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" id="A1.T8.3.3.3.1.m1.1.1.1.cmml" mathsize="70%" xref="A1.T8.3.3.3.1.m1.1.1.1">Fam</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.3.3.3.1.m1.1c">{}_{\textit{Fam}}</annotation><annotation encoding="application/x-llamapun" id="A1.T8.3.3.3.1.m1.1d">start_FLOATSUBSCRIPT Fam end_FLOATSUBSCRIPT</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.3.2">+70%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.3">+0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.4">+0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.5">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.6">+0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.7">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.8">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.9">+0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.10">+0.3</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.3.11">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.12">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.13">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.3.14">+0.4</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.6.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.6.3.1">LaSS</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.6.3.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.3"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.6.3.3.1">+1.5</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.4">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.5"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.6.3.5.1">+1.2</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.6"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.6.3.6.1">+0.9</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.7">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.8"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.6.3.8.1">+0.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.9">-0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.10">-1.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.6.3.11">-0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.12">+0.7</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.13">+0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.6.3.14">+0.4</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.7.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.7.4.1">Random</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.7.4.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.3">+0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.4">-0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.5">+0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.6">-0.1</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.7">-0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.8">-0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.9">-0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.10">-0.9</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.7.4.11">-0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.12">-0.2</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.13">-0.4</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.7.4.14">-0.3</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.8.5">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.8.5.1">Ours-Enc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.8.5.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.3">+1.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.4">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.5">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.6">+0.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.7">+0.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.8">+0.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.9">+0.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.10"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.8.5.10.1">+0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.3.3.8.5.11">+0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.12">+0.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.13">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.3.3.8.5.14">+0.7</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.9.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.9.6.1">Ours-Dec</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.9.6.2">0%</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.3">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.4">+0.8</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.5">+0.9</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.6">+0.5</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.7"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.9.6.7.1">+1.0</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.8"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.9.6.8.1">+0.8</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.9">+0.3</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.10"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.9.6.10.1">+0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.3.3.9.6.11">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.12">+0.6</td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.13"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.9.6.13.1">+1.0</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.3.3.9.6.14">+0.8</td>
</tr>
<tr class="ltx_tr" id="A1.T8.3.3.10.7">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T8.3.3.10.7.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T8.3.3.10.7.2">0%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.3">+1.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.4"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.4.1">+1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.5"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.5.1">+1.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.6"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.6.1">+0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.7">+0.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.8"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.8.1">+0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.9"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.9.1">+0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.10">+0.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T8.3.3.10.7.11"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.11.1">+0.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.12"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.12.1">+1.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.13">+0.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.3.3.10.7.14"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.10.7.14.1">+0.9</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Average <span class="ltx_text ltx_font_bold" id="A1.T8.6.1">COMET</span> improvements on the EC30 dataset over the baseline (mT-big), categorized by High, Medium, and Low-resource translation directions. ’Ours-Enc’ and ’Ours-Dec’ indicate neuron specialization applied solely to the Encoder and Decoder, respectively, while ’Ours’ signifies the method applied to both components. The best results are highlighted in <span class="ltx_text ltx_font_bold" id="A1.T8.7.2">bold</span>.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="A1.F4.g1" src="extracted/2404.11201v1/imgs/k_chart.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Improvements of Neuron Specialization method over the mT-large baseline on EC30. The x-axis indicates the factor <math alttext="k" class="ltx_Math" display="inline" id="A1.F4.2.m1.1"><semantics id="A1.F4.2.m1.1b"><mi id="A1.F4.2.m1.1.1" xref="A1.F4.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.F4.2.m1.1c"><ci id="A1.F4.2.m1.1.1.cmml" xref="A1.F4.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F4.2.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="A1.F4.2.m1.1e">italic_k</annotation></semantics></math> and the dynamic sparsity of the fc1 layer, with displayed values ranging from minimum to maximum sparsity achieved. The y-axis indicates the SacreBLEU improvements over the mT-large model.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Sparsity versus Performance</h3>
<div class="ltx_para" id="A1.SS5.p1">
<p class="ltx_p" id="A1.SS5.p1.6">For the Neuron Specialization, we dynamically select specialized neurons via a cumulative activation threshold <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.1.m1.1"><semantics id="A1.SS5.p1.1.m1.1a"><mi id="A1.SS5.p1.1.m1.1.1" xref="A1.SS5.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.1.m1.1b"><ci id="A1.SS5.p1.1.m1.1.1.cmml" xref="A1.SS5.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.1.m1.1d">italic_k</annotation></semantics></math> in Equation <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.E1" title="In Neuron Selection. ‣ 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a>, which is the only hyper-parameter of our method. Here, we discuss the impact of <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.2.m2.1"><semantics id="A1.SS5.p1.2.m2.1a"><mi id="A1.SS5.p1.2.m2.1.1" xref="A1.SS5.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.2.m2.1b"><ci id="A1.SS5.p1.2.m2.1.1.cmml" xref="A1.SS5.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.2.m2.1d">italic_k</annotation></semantics></math> on the final performance and its relationship to the sparsity. As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS1" title="3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3.1</span></a>, a smaller factor <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.3.m3.1"><semantics id="A1.SS5.p1.3.m3.1a"><mi id="A1.SS5.p1.3.m3.1.1" xref="A1.SS5.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.3.m3.1b"><ci id="A1.SS5.p1.3.m3.1.1.cmml" xref="A1.SS5.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.3.m3.1d">italic_k</annotation></semantics></math> results in more sparse specialized neuron selection, which makes the fc1 weight more sparse as well in the Neuron Specialization Training process. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F4" title="Figure 4 ‣ A.4 Result Details using ChrF++ and COMET ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">4</span></a>, we show that increase <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.4.m4.1"><semantics id="A1.SS5.p1.4.m4.1a"><mi id="A1.SS5.p1.4.m4.1.1" xref="A1.SS5.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.4.m4.1b"><ci id="A1.SS5.p1.4.m4.1.1.cmml" xref="A1.SS5.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.4.m4.1d">italic_k</annotation></semantics></math> leads to higher improvements in general, and the optimal performance is about when <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.5.m5.1"><semantics id="A1.SS5.p1.5.m5.1a"><mi id="A1.SS5.p1.5.m5.1.1" xref="A1.SS5.p1.5.m5.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.5.m5.1b"><ci id="A1.SS5.p1.5.m5.1.1.cmml" xref="A1.SS5.p1.5.m5.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.5.m5.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.5.m5.1d">italic_k</annotation></semantics></math>=95%. Such observation follows the intuition since when <math alttext="k" class="ltx_Math" display="inline" id="A1.SS5.p1.6.m6.1"><semantics id="A1.SS5.p1.6.m6.1a"><mi id="A1.SS5.p1.6.m6.1.1" xref="A1.SS5.p1.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS5.p1.6.m6.1b"><ci id="A1.SS5.p1.6.m6.1.1.cmml" xref="A1.SS5.p1.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS5.p1.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="A1.SS5.p1.6.m6.1d">italic_k</annotation></semantics></math> is too low, model capacity will be largely reduced.</p>
</div>
<figure class="ltx_figure" id="A1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="A1.F5.g1" src="extracted/2404.11201v1/imgs/sparsity_k95.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Sparsity progression of Neuron Specialization when <math alttext="k=95" class="ltx_Math" display="inline" id="A1.F5.2.m1.1"><semantics id="A1.F5.2.m1.1b"><mrow id="A1.F5.2.m1.1.1" xref="A1.F5.2.m1.1.1.cmml"><mi id="A1.F5.2.m1.1.1.2" xref="A1.F5.2.m1.1.1.2.cmml">k</mi><mo id="A1.F5.2.m1.1.1.1" xref="A1.F5.2.m1.1.1.1.cmml">=</mo><mn id="A1.F5.2.m1.1.1.3" xref="A1.F5.2.m1.1.1.3.cmml">95</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F5.2.m1.1c"><apply id="A1.F5.2.m1.1.1.cmml" xref="A1.F5.2.m1.1.1"><eq id="A1.F5.2.m1.1.1.1.cmml" xref="A1.F5.2.m1.1.1.1"></eq><ci id="A1.F5.2.m1.1.1.2.cmml" xref="A1.F5.2.m1.1.1.2">𝑘</ci><cn id="A1.F5.2.m1.1.1.3.cmml" type="integer" xref="A1.F5.2.m1.1.1.3">95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F5.2.m1.1d">k=95</annotation><annotation encoding="application/x-llamapun" id="A1.F5.2.m1.1e">italic_k = 95</annotation></semantics></math> on the EC30. We observe that the sparsity becomes smaller in the Encoder and then goes up in the Decoder. Note that this figure is based on the natural signals extracted from the untouched pre-trained model, and will be leveraged later in the process of Neuron Specialization Training. This intrinsic pattern naturally follows our intuition that specialized neurons progress from language specific to agnostic the in Encoder, and vice versa in the Decoder.</figcaption>
</figure>
<div class="ltx_para" id="A1.SS5.p2">
<p class="ltx_p" id="A1.SS5.p2.1">Furthermore, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F5" title="Figure 5 ‣ A.5 Sparsity versus Performance ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">5</span></a>, we show that the sparsity of the network presents an intuitive structure: the sparsity decreases in the Encoder and increases in the Decoder. This implies the natural signal within the pre-trained multilingual model that neurons progress from language-specific to language-agnostic in the Encoder, and vice versa in the Decoder. Such observation is natural because it is reflected by the untouched network, similar to what we observed in the Progression of Neuron overlaps in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.SS2.SSS2" title="3.2.2 The Progression of Neuron Overlaps ‣ 3.2 Analysis on EC30 ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.6 </span>Visualization Details</h3>
<div class="ltx_para" id="A1.SS6.p1">
<p class="ltx_p" id="A1.SS6.p1.1">We provide the additional Pairwise Intersection over Union (IoU) scores for specialized neurons in the first Encoder layer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F6" title="Figure 6 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">6</span></a>), last Encoder layer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F7" title="Figure 7 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">7</span></a>), and last Decoder layer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#A1.F8" title="Figure 8 ‣ A.6 Visualization Details ‣ Appendix A Appendix ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">8</span></a>). The figures show that the Neurons gradually changed from language-specific to language-agnostic in the Encoder, and vice versa in the Decoder.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="A1.F6.g1" src="extracted/2404.11201v1/imgs/enc_0.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the <span class="ltx_text ltx_font_bold" id="A1.F6.2.1">first encoder</span> FFN layer across all X-En language pairs to measure the degree of overlap between language pairs. Darker cells indicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="A1.F7.g1" src="extracted/2404.11201v1/imgs/enc_5.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the <span class="ltx_text ltx_font_bold" id="A1.F7.2.1">last encoder</span> FFN layer across all One-to-Many language pairs to measure the degree of overlap between language pairs. Darker cells indicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility.</figcaption>
</figure>
<figure class="ltx_figure" id="A1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="215" id="A1.F8.g1" src="extracted/2404.11201v1/imgs/dec_5.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Pairwise Intersection over Union (IoU) scores for specialized neurons extracted from the <span class="ltx_text ltx_font_bold" id="A1.F8.2.1">last decoder</span> FFN layer across all X-En language pairs to measure the degree of overlap between language pairs. Darker cells indicate stronger overlap, with the color threshold set from 40 to 80 to improve visibility.</figcaption>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Specialized Neuron Identification</figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="algx1.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="algx1.l1.1">Input:</span> A pre-trained multi-task model <math alttext="\theta" class="ltx_Math" display="inline" id="algx1.l1.m1.1"><semantics id="algx1.l1.m1.1a"><mi id="algx1.l1.m1.1.1" xref="algx1.l1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.m1.1b"><ci id="algx1.l1.m1.1.1.cmml" xref="algx1.l1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m1.1d">italic_θ</annotation></semantics></math> with dimensions <math alttext="d" class="ltx_Math" display="inline" id="algx1.l1.m2.1"><semantics id="algx1.l1.m2.1a"><mi id="algx1.l1.m2.1.1" xref="algx1.l1.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.m2.1b"><ci id="algx1.l1.m2.1.1.cmml" xref="algx1.l1.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m2.1d">italic_d</annotation></semantics></math> and <math alttext="\mathit{d_{ff}}" class="ltx_Math" display="inline" id="algx1.l1.m3.1"><semantics id="algx1.l1.m3.1a"><msub id="algx1.l1.m3.1.1" xref="algx1.l1.m3.1.1.cmml"><mi id="algx1.l1.m3.1.1.2" xref="algx1.l1.m3.1.1.2.cmml">d</mi><mi id="algx1.l1.m3.1.1.3" xref="algx1.l1.m3.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="algx1.l1.m3.1b"><apply id="algx1.l1.m3.1.1.cmml" xref="algx1.l1.m3.1.1"><csymbol cd="ambiguous" id="algx1.l1.m3.1.1.1.cmml" xref="algx1.l1.m3.1.1">subscript</csymbol><ci id="algx1.l1.m3.1.1.2.cmml" xref="algx1.l1.m3.1.1.2">𝑑</ci><ci id="algx1.l1.m3.1.1.3.cmml" xref="algx1.l1.m3.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m3.1c">\mathit{d_{ff}}</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m3.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math>; a validation dataset <math alttext="D" class="ltx_Math" display="inline" id="algx1.l1.m4.1"><semantics id="algx1.l1.m4.1a"><mi id="algx1.l1.m4.1.1" xref="algx1.l1.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.m4.1b"><ci id="algx1.l1.m4.1.1.cmml" xref="algx1.l1.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m4.1d">italic_D</annotation></semantics></math> with <math alttext="T" class="ltx_Math" display="inline" id="algx1.l1.m5.1"><semantics id="algx1.l1.m5.1a"><mi id="algx1.l1.m5.1.1" xref="algx1.l1.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algx1.l1.m5.1b"><ci id="algx1.l1.m5.1.1.cmml" xref="algx1.l1.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m5.1d">italic_T</annotation></semantics></math> tasks, where <math alttext="D=\{D_{1},...,D_{T}\}" class="ltx_Math" display="inline" id="algx1.l1.m6.3"><semantics id="algx1.l1.m6.3a"><mrow id="algx1.l1.m6.3.3" xref="algx1.l1.m6.3.3.cmml"><mi id="algx1.l1.m6.3.3.4" xref="algx1.l1.m6.3.3.4.cmml">D</mi><mo id="algx1.l1.m6.3.3.3" xref="algx1.l1.m6.3.3.3.cmml">=</mo><mrow id="algx1.l1.m6.3.3.2.2" xref="algx1.l1.m6.3.3.2.3.cmml"><mo id="algx1.l1.m6.3.3.2.2.3" stretchy="false" xref="algx1.l1.m6.3.3.2.3.cmml">{</mo><msub id="algx1.l1.m6.2.2.1.1.1" xref="algx1.l1.m6.2.2.1.1.1.cmml"><mi id="algx1.l1.m6.2.2.1.1.1.2" xref="algx1.l1.m6.2.2.1.1.1.2.cmml">D</mi><mn id="algx1.l1.m6.2.2.1.1.1.3" xref="algx1.l1.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="algx1.l1.m6.3.3.2.2.4" xref="algx1.l1.m6.3.3.2.3.cmml">,</mo><mi id="algx1.l1.m6.1.1" mathvariant="normal" xref="algx1.l1.m6.1.1.cmml">…</mi><mo id="algx1.l1.m6.3.3.2.2.5" xref="algx1.l1.m6.3.3.2.3.cmml">,</mo><msub id="algx1.l1.m6.3.3.2.2.2" xref="algx1.l1.m6.3.3.2.2.2.cmml"><mi id="algx1.l1.m6.3.3.2.2.2.2" xref="algx1.l1.m6.3.3.2.2.2.2.cmml">D</mi><mi id="algx1.l1.m6.3.3.2.2.2.3" xref="algx1.l1.m6.3.3.2.2.2.3.cmml">T</mi></msub><mo id="algx1.l1.m6.3.3.2.2.6" stretchy="false" xref="algx1.l1.m6.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l1.m6.3b"><apply id="algx1.l1.m6.3.3.cmml" xref="algx1.l1.m6.3.3"><eq id="algx1.l1.m6.3.3.3.cmml" xref="algx1.l1.m6.3.3.3"></eq><ci id="algx1.l1.m6.3.3.4.cmml" xref="algx1.l1.m6.3.3.4">𝐷</ci><set id="algx1.l1.m6.3.3.2.3.cmml" xref="algx1.l1.m6.3.3.2.2"><apply id="algx1.l1.m6.2.2.1.1.1.cmml" xref="algx1.l1.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="algx1.l1.m6.2.2.1.1.1.1.cmml" xref="algx1.l1.m6.2.2.1.1.1">subscript</csymbol><ci id="algx1.l1.m6.2.2.1.1.1.2.cmml" xref="algx1.l1.m6.2.2.1.1.1.2">𝐷</ci><cn id="algx1.l1.m6.2.2.1.1.1.3.cmml" type="integer" xref="algx1.l1.m6.2.2.1.1.1.3">1</cn></apply><ci id="algx1.l1.m6.1.1.cmml" xref="algx1.l1.m6.1.1">…</ci><apply id="algx1.l1.m6.3.3.2.2.2.cmml" xref="algx1.l1.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="algx1.l1.m6.3.3.2.2.2.1.cmml" xref="algx1.l1.m6.3.3.2.2.2">subscript</csymbol><ci id="algx1.l1.m6.3.3.2.2.2.2.cmml" xref="algx1.l1.m6.3.3.2.2.2.2">𝐷</ci><ci id="algx1.l1.m6.3.3.2.2.2.3.cmml" xref="algx1.l1.m6.3.3.2.2.2.3">𝑇</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m6.3c">D=\{D_{1},...,D_{T}\}</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m6.3d">italic_D = { italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_D start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }</annotation></semantics></math>; and an accumulation threshold factor <math alttext="k\in[0\%,100\%]" class="ltx_Math" display="inline" id="algx1.l1.m7.2"><semantics id="algx1.l1.m7.2a"><mrow id="algx1.l1.m7.2.2" xref="algx1.l1.m7.2.2.cmml"><mi id="algx1.l1.m7.2.2.4" xref="algx1.l1.m7.2.2.4.cmml">k</mi><mo id="algx1.l1.m7.2.2.3" xref="algx1.l1.m7.2.2.3.cmml">∈</mo><mrow id="algx1.l1.m7.2.2.2.2" xref="algx1.l1.m7.2.2.2.3.cmml"><mo id="algx1.l1.m7.2.2.2.2.3" stretchy="false" xref="algx1.l1.m7.2.2.2.3.cmml">[</mo><mrow id="algx1.l1.m7.1.1.1.1.1" xref="algx1.l1.m7.1.1.1.1.1.cmml"><mn id="algx1.l1.m7.1.1.1.1.1.2" xref="algx1.l1.m7.1.1.1.1.1.2.cmml">0</mn><mo id="algx1.l1.m7.1.1.1.1.1.1" xref="algx1.l1.m7.1.1.1.1.1.1.cmml">%</mo></mrow><mo id="algx1.l1.m7.2.2.2.2.4" xref="algx1.l1.m7.2.2.2.3.cmml">,</mo><mrow id="algx1.l1.m7.2.2.2.2.2" xref="algx1.l1.m7.2.2.2.2.2.cmml"><mn id="algx1.l1.m7.2.2.2.2.2.2" xref="algx1.l1.m7.2.2.2.2.2.2.cmml">100</mn><mo id="algx1.l1.m7.2.2.2.2.2.1" xref="algx1.l1.m7.2.2.2.2.2.1.cmml">%</mo></mrow><mo id="algx1.l1.m7.2.2.2.2.5" stretchy="false" xref="algx1.l1.m7.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l1.m7.2b"><apply id="algx1.l1.m7.2.2.cmml" xref="algx1.l1.m7.2.2"><in id="algx1.l1.m7.2.2.3.cmml" xref="algx1.l1.m7.2.2.3"></in><ci id="algx1.l1.m7.2.2.4.cmml" xref="algx1.l1.m7.2.2.4">𝑘</ci><interval closure="closed" id="algx1.l1.m7.2.2.2.3.cmml" xref="algx1.l1.m7.2.2.2.2"><apply id="algx1.l1.m7.1.1.1.1.1.cmml" xref="algx1.l1.m7.1.1.1.1.1"><csymbol cd="latexml" id="algx1.l1.m7.1.1.1.1.1.1.cmml" xref="algx1.l1.m7.1.1.1.1.1.1">percent</csymbol><cn id="algx1.l1.m7.1.1.1.1.1.2.cmml" type="integer" xref="algx1.l1.m7.1.1.1.1.1.2">0</cn></apply><apply id="algx1.l1.m7.2.2.2.2.2.cmml" xref="algx1.l1.m7.2.2.2.2.2"><csymbol cd="latexml" id="algx1.l1.m7.2.2.2.2.2.1.cmml" xref="algx1.l1.m7.2.2.2.2.2.1">percent</csymbol><cn id="algx1.l1.m7.2.2.2.2.2.2.cmml" type="integer" xref="algx1.l1.m7.2.2.2.2.2.2">100</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l1.m7.2c">k\in[0\%,100\%]</annotation><annotation encoding="application/x-llamapun" id="algx1.l1.m7.2d">italic_k ∈ [ 0 % , 100 % ]</annotation></semantics></math> as the only hyper-parameter.

</div>
<div class="ltx_listingline" id="algx1.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><span class="ltx_text ltx_font_bold" id="algx1.l2.1">Output:</span> A set of selected specialized neurons <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="algx1.l2.m1.1"><semantics id="algx1.l2.m1.1a"><msubsup id="algx1.l2.m1.1.1" xref="algx1.l2.m1.1.1.cmml"><mi id="algx1.l2.m1.1.1.2.2" xref="algx1.l2.m1.1.1.2.2.cmml">S</mi><mi id="algx1.l2.m1.1.1.2.3" xref="algx1.l2.m1.1.1.2.3.cmml">k</mi><mi id="algx1.l2.m1.1.1.3" xref="algx1.l2.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx1.l2.m1.1b"><apply id="algx1.l2.m1.1.1.cmml" xref="algx1.l2.m1.1.1"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.1.cmml" xref="algx1.l2.m1.1.1">superscript</csymbol><apply id="algx1.l2.m1.1.1.2.cmml" xref="algx1.l2.m1.1.1"><csymbol cd="ambiguous" id="algx1.l2.m1.1.1.2.1.cmml" xref="algx1.l2.m1.1.1">subscript</csymbol><ci id="algx1.l2.m1.1.1.2.2.cmml" xref="algx1.l2.m1.1.1.2.2">𝑆</ci><ci id="algx1.l2.m1.1.1.2.3.cmml" xref="algx1.l2.m1.1.1.2.3">𝑘</ci></apply><ci id="algx1.l2.m1.1.1.3.cmml" xref="algx1.l2.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m1.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="algx1.l2.m1.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> for each task <math alttext="t" class="ltx_Math" display="inline" id="algx1.l2.m2.1"><semantics id="algx1.l2.m2.1a"><mi id="algx1.l2.m2.1.1" xref="algx1.l2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="algx1.l2.m2.1b"><ci id="algx1.l2.m2.1.1.cmml" xref="algx1.l2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="algx1.l2.m2.1d">italic_t</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="algx1.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span><span class="ltx_text ltx_font_bold" id="algx1.l3.1">for</span> task <math alttext="t" class="ltx_Math" display="inline" id="algx1.l3.m1.1"><semantics id="algx1.l3.m1.1a"><mi id="algx1.l3.m1.1.1" xref="algx1.l3.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="algx1.l3.m1.1b"><ci id="algx1.l3.m1.1.1.cmml" xref="algx1.l3.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="algx1.l3.m1.1d">italic_t</annotation></semantics></math> in <math alttext="T" class="ltx_Math" display="inline" id="algx1.l3.m2.1"><semantics id="algx1.l3.m2.1a"><mi id="algx1.l3.m2.1.1" xref="algx1.l3.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algx1.l3.m2.1b"><ci id="algx1.l3.m2.1.1.cmml" xref="algx1.l3.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l3.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="algx1.l3.m2.1d">italic_T</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l3.2">do</span>
</div>
<div class="ltx_listingline" id="algx1.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span>     <span class="ltx_text" id="algx1.l4.1" style="background-color:#D9D9D9;">Step 1: Activation Recording</span>
</div>
<div class="ltx_listingline" id="algx1.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     Initialize activation vector <math alttext="A_{t}=\mathbf{0}\in\mathbb{R}^{d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="algx1.l5.m1.1"><semantics id="algx1.l5.m1.1a"><mrow id="algx1.l5.m1.1.1" xref="algx1.l5.m1.1.1.cmml"><msub id="algx1.l5.m1.1.1.2" xref="algx1.l5.m1.1.1.2.cmml"><mi id="algx1.l5.m1.1.1.2.2" xref="algx1.l5.m1.1.1.2.2.cmml">A</mi><mi id="algx1.l5.m1.1.1.2.3" xref="algx1.l5.m1.1.1.2.3.cmml">t</mi></msub><mo id="algx1.l5.m1.1.1.3" xref="algx1.l5.m1.1.1.3.cmml">=</mo><mn id="algx1.l5.m1.1.1.4" xref="algx1.l5.m1.1.1.4.cmml">𝟎</mn><mo id="algx1.l5.m1.1.1.5" xref="algx1.l5.m1.1.1.5.cmml">∈</mo><msup id="algx1.l5.m1.1.1.6" xref="algx1.l5.m1.1.1.6.cmml"><mi id="algx1.l5.m1.1.1.6.2" xref="algx1.l5.m1.1.1.6.2.cmml">ℝ</mi><msub id="algx1.l5.m1.1.1.6.3" xref="algx1.l5.m1.1.1.6.3.cmml"><mi id="algx1.l5.m1.1.1.6.3.2" xref="algx1.l5.m1.1.1.6.3.2.cmml">d</mi><mi id="algx1.l5.m1.1.1.6.3.3" xref="algx1.l5.m1.1.1.6.3.3.cmml">𝑓𝑓</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="algx1.l5.m1.1b"><apply id="algx1.l5.m1.1.1.cmml" xref="algx1.l5.m1.1.1"><and id="algx1.l5.m1.1.1a.cmml" xref="algx1.l5.m1.1.1"></and><apply id="algx1.l5.m1.1.1b.cmml" xref="algx1.l5.m1.1.1"><eq id="algx1.l5.m1.1.1.3.cmml" xref="algx1.l5.m1.1.1.3"></eq><apply id="algx1.l5.m1.1.1.2.cmml" xref="algx1.l5.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l5.m1.1.1.2.1.cmml" xref="algx1.l5.m1.1.1.2">subscript</csymbol><ci id="algx1.l5.m1.1.1.2.2.cmml" xref="algx1.l5.m1.1.1.2.2">𝐴</ci><ci id="algx1.l5.m1.1.1.2.3.cmml" xref="algx1.l5.m1.1.1.2.3">𝑡</ci></apply><cn id="algx1.l5.m1.1.1.4.cmml" type="integer" xref="algx1.l5.m1.1.1.4">0</cn></apply><apply id="algx1.l5.m1.1.1c.cmml" xref="algx1.l5.m1.1.1"><in id="algx1.l5.m1.1.1.5.cmml" xref="algx1.l5.m1.1.1.5"></in><share href="https://arxiv.org/html/2404.11201v1#algx1.l5.m1.1.1.4.cmml" id="algx1.l5.m1.1.1d.cmml" xref="algx1.l5.m1.1.1"></share><apply id="algx1.l5.m1.1.1.6.cmml" xref="algx1.l5.m1.1.1.6"><csymbol cd="ambiguous" id="algx1.l5.m1.1.1.6.1.cmml" xref="algx1.l5.m1.1.1.6">superscript</csymbol><ci id="algx1.l5.m1.1.1.6.2.cmml" xref="algx1.l5.m1.1.1.6.2">ℝ</ci><apply id="algx1.l5.m1.1.1.6.3.cmml" xref="algx1.l5.m1.1.1.6.3"><csymbol cd="ambiguous" id="algx1.l5.m1.1.1.6.3.1.cmml" xref="algx1.l5.m1.1.1.6.3">subscript</csymbol><ci id="algx1.l5.m1.1.1.6.3.2.cmml" xref="algx1.l5.m1.1.1.6.3.2">𝑑</ci><ci id="algx1.l5.m1.1.1.6.3.3.cmml" xref="algx1.l5.m1.1.1.6.3.3">𝑓𝑓</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l5.m1.1c">A_{t}=\mathbf{0}\in\mathbb{R}^{d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="algx1.l5.m1.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_0 ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l6.1">for</span> sample <math alttext="x_{i}" class="ltx_Math" display="inline" id="algx1.l6.m1.1"><semantics id="algx1.l6.m1.1a"><msub id="algx1.l6.m1.1.1" xref="algx1.l6.m1.1.1.cmml"><mi id="algx1.l6.m1.1.1.2" xref="algx1.l6.m1.1.1.2.cmml">x</mi><mi id="algx1.l6.m1.1.1.3" xref="algx1.l6.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="algx1.l6.m1.1b"><apply id="algx1.l6.m1.1.1.cmml" xref="algx1.l6.m1.1.1"><csymbol cd="ambiguous" id="algx1.l6.m1.1.1.1.cmml" xref="algx1.l6.m1.1.1">subscript</csymbol><ci id="algx1.l6.m1.1.1.2.cmml" xref="algx1.l6.m1.1.1.2">𝑥</ci><ci id="algx1.l6.m1.1.1.3.cmml" xref="algx1.l6.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m1.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="algx1.l6.m1.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> in <math alttext="D_{t}" class="ltx_Math" display="inline" id="algx1.l6.m2.1"><semantics id="algx1.l6.m2.1a"><msub id="algx1.l6.m2.1.1" xref="algx1.l6.m2.1.1.cmml"><mi id="algx1.l6.m2.1.1.2" xref="algx1.l6.m2.1.1.2.cmml">D</mi><mi id="algx1.l6.m2.1.1.3" xref="algx1.l6.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="algx1.l6.m2.1b"><apply id="algx1.l6.m2.1.1.cmml" xref="algx1.l6.m2.1.1"><csymbol cd="ambiguous" id="algx1.l6.m2.1.1.1.cmml" xref="algx1.l6.m2.1.1">subscript</csymbol><ci id="algx1.l6.m2.1.1.2.cmml" xref="algx1.l6.m2.1.1.2">𝐷</ci><ci id="algx1.l6.m2.1.1.3.cmml" xref="algx1.l6.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l6.m2.1c">D_{t}</annotation><annotation encoding="application/x-llamapun" id="algx1.l6.m2.1d">italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx1.l6.2">do</span>
</div>
<div class="ltx_listingline" id="algx1.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>         Record activation state <math alttext="a^{t}_{i}\in\mathbb{R}^{d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="algx1.l7.m1.1"><semantics id="algx1.l7.m1.1a"><mrow id="algx1.l7.m1.1.1" xref="algx1.l7.m1.1.1.cmml"><msubsup id="algx1.l7.m1.1.1.2" xref="algx1.l7.m1.1.1.2.cmml"><mi id="algx1.l7.m1.1.1.2.2.2" xref="algx1.l7.m1.1.1.2.2.2.cmml">a</mi><mi id="algx1.l7.m1.1.1.2.3" xref="algx1.l7.m1.1.1.2.3.cmml">i</mi><mi id="algx1.l7.m1.1.1.2.2.3" xref="algx1.l7.m1.1.1.2.2.3.cmml">t</mi></msubsup><mo id="algx1.l7.m1.1.1.1" xref="algx1.l7.m1.1.1.1.cmml">∈</mo><msup id="algx1.l7.m1.1.1.3" xref="algx1.l7.m1.1.1.3.cmml"><mi id="algx1.l7.m1.1.1.3.2" xref="algx1.l7.m1.1.1.3.2.cmml">ℝ</mi><msub id="algx1.l7.m1.1.1.3.3" xref="algx1.l7.m1.1.1.3.3.cmml"><mi id="algx1.l7.m1.1.1.3.3.2" xref="algx1.l7.m1.1.1.3.3.2.cmml">d</mi><mi id="algx1.l7.m1.1.1.3.3.3" xref="algx1.l7.m1.1.1.3.3.3.cmml">𝑓𝑓</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="algx1.l7.m1.1b"><apply id="algx1.l7.m1.1.1.cmml" xref="algx1.l7.m1.1.1"><in id="algx1.l7.m1.1.1.1.cmml" xref="algx1.l7.m1.1.1.1"></in><apply id="algx1.l7.m1.1.1.2.cmml" xref="algx1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.2.1.cmml" xref="algx1.l7.m1.1.1.2">subscript</csymbol><apply id="algx1.l7.m1.1.1.2.2.cmml" xref="algx1.l7.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.2.2.1.cmml" xref="algx1.l7.m1.1.1.2">superscript</csymbol><ci id="algx1.l7.m1.1.1.2.2.2.cmml" xref="algx1.l7.m1.1.1.2.2.2">𝑎</ci><ci id="algx1.l7.m1.1.1.2.2.3.cmml" xref="algx1.l7.m1.1.1.2.2.3">𝑡</ci></apply><ci id="algx1.l7.m1.1.1.2.3.cmml" xref="algx1.l7.m1.1.1.2.3">𝑖</ci></apply><apply id="algx1.l7.m1.1.1.3.cmml" xref="algx1.l7.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.3.1.cmml" xref="algx1.l7.m1.1.1.3">superscript</csymbol><ci id="algx1.l7.m1.1.1.3.2.cmml" xref="algx1.l7.m1.1.1.3.2">ℝ</ci><apply id="algx1.l7.m1.1.1.3.3.cmml" xref="algx1.l7.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l7.m1.1.1.3.3.1.cmml" xref="algx1.l7.m1.1.1.3.3">subscript</csymbol><ci id="algx1.l7.m1.1.1.3.3.2.cmml" xref="algx1.l7.m1.1.1.3.3.2">𝑑</ci><ci id="algx1.l7.m1.1.1.3.3.3.cmml" xref="algx1.l7.m1.1.1.3.3.3">𝑓𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l7.m1.1c">a^{t}_{i}\in\mathbb{R}^{d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="algx1.l7.m1.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>         <math alttext="A_{t}=A_{t}+a^{t}_{i}" class="ltx_Math" display="inline" id="algx1.l8.m1.1"><semantics id="algx1.l8.m1.1a"><mrow id="algx1.l8.m1.1.1" xref="algx1.l8.m1.1.1.cmml"><msub id="algx1.l8.m1.1.1.2" xref="algx1.l8.m1.1.1.2.cmml"><mi id="algx1.l8.m1.1.1.2.2" xref="algx1.l8.m1.1.1.2.2.cmml">A</mi><mi id="algx1.l8.m1.1.1.2.3" xref="algx1.l8.m1.1.1.2.3.cmml">t</mi></msub><mo id="algx1.l8.m1.1.1.1" xref="algx1.l8.m1.1.1.1.cmml">=</mo><mrow id="algx1.l8.m1.1.1.3" xref="algx1.l8.m1.1.1.3.cmml"><msub id="algx1.l8.m1.1.1.3.2" xref="algx1.l8.m1.1.1.3.2.cmml"><mi id="algx1.l8.m1.1.1.3.2.2" xref="algx1.l8.m1.1.1.3.2.2.cmml">A</mi><mi id="algx1.l8.m1.1.1.3.2.3" xref="algx1.l8.m1.1.1.3.2.3.cmml">t</mi></msub><mo id="algx1.l8.m1.1.1.3.1" xref="algx1.l8.m1.1.1.3.1.cmml">+</mo><msubsup id="algx1.l8.m1.1.1.3.3" xref="algx1.l8.m1.1.1.3.3.cmml"><mi id="algx1.l8.m1.1.1.3.3.2.2" xref="algx1.l8.m1.1.1.3.3.2.2.cmml">a</mi><mi id="algx1.l8.m1.1.1.3.3.3" xref="algx1.l8.m1.1.1.3.3.3.cmml">i</mi><mi id="algx1.l8.m1.1.1.3.3.2.3" xref="algx1.l8.m1.1.1.3.3.2.3.cmml">t</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx1.l8.m1.1b"><apply id="algx1.l8.m1.1.1.cmml" xref="algx1.l8.m1.1.1"><eq id="algx1.l8.m1.1.1.1.cmml" xref="algx1.l8.m1.1.1.1"></eq><apply id="algx1.l8.m1.1.1.2.cmml" xref="algx1.l8.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.2.1.cmml" xref="algx1.l8.m1.1.1.2">subscript</csymbol><ci id="algx1.l8.m1.1.1.2.2.cmml" xref="algx1.l8.m1.1.1.2.2">𝐴</ci><ci id="algx1.l8.m1.1.1.2.3.cmml" xref="algx1.l8.m1.1.1.2.3">𝑡</ci></apply><apply id="algx1.l8.m1.1.1.3.cmml" xref="algx1.l8.m1.1.1.3"><plus id="algx1.l8.m1.1.1.3.1.cmml" xref="algx1.l8.m1.1.1.3.1"></plus><apply id="algx1.l8.m1.1.1.3.2.cmml" xref="algx1.l8.m1.1.1.3.2"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.2.1.cmml" xref="algx1.l8.m1.1.1.3.2">subscript</csymbol><ci id="algx1.l8.m1.1.1.3.2.2.cmml" xref="algx1.l8.m1.1.1.3.2.2">𝐴</ci><ci id="algx1.l8.m1.1.1.3.2.3.cmml" xref="algx1.l8.m1.1.1.3.2.3">𝑡</ci></apply><apply id="algx1.l8.m1.1.1.3.3.cmml" xref="algx1.l8.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.1.cmml" xref="algx1.l8.m1.1.1.3.3">subscript</csymbol><apply id="algx1.l8.m1.1.1.3.3.2.cmml" xref="algx1.l8.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx1.l8.m1.1.1.3.3.2.1.cmml" xref="algx1.l8.m1.1.1.3.3">superscript</csymbol><ci id="algx1.l8.m1.1.1.3.3.2.2.cmml" xref="algx1.l8.m1.1.1.3.3.2.2">𝑎</ci><ci id="algx1.l8.m1.1.1.3.3.2.3.cmml" xref="algx1.l8.m1.1.1.3.3.2.3">𝑡</ci></apply><ci id="algx1.l8.m1.1.1.3.3.3.cmml" xref="algx1.l8.m1.1.1.3.3.3">𝑖</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.m1.1c">A_{t}=A_{t}+a^{t}_{i}</annotation><annotation encoding="application/x-llamapun" id="algx1.l8.m1.1d">italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" id="algx1.l8.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l8.1.m1.1"><semantics id="algx1.l8.1.m1.1a"><mo id="algx1.l8.1.m1.1.1" xref="algx1.l8.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l8.1.m1.1b"><ci id="algx1.l8.1.m1.1.1.cmml" xref="algx1.l8.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l8.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l8.1.m1.1d">▷</annotation></semantics></math> Accumulate activation states
</span>
</div>
<div class="ltx_listingline" id="algx1.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l9.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l9.2">for</span>
</div>
<div class="ltx_listingline" id="algx1.l10">
<span class="ltx_tag ltx_tag_listingline">10:</span>     <math alttext="a^{t}=\frac{A_{t}}{|D_{t}|}" class="ltx_Math" display="inline" id="algx1.l10.m1.1"><semantics id="algx1.l10.m1.1a"><mrow id="algx1.l10.m1.1.2" xref="algx1.l10.m1.1.2.cmml"><msup id="algx1.l10.m1.1.2.2" xref="algx1.l10.m1.1.2.2.cmml"><mi id="algx1.l10.m1.1.2.2.2" xref="algx1.l10.m1.1.2.2.2.cmml">a</mi><mi id="algx1.l10.m1.1.2.2.3" xref="algx1.l10.m1.1.2.2.3.cmml">t</mi></msup><mo id="algx1.l10.m1.1.2.1" xref="algx1.l10.m1.1.2.1.cmml">=</mo><mfrac id="algx1.l10.m1.1.1" xref="algx1.l10.m1.1.1.cmml"><msub id="algx1.l10.m1.1.1.3" xref="algx1.l10.m1.1.1.3.cmml"><mi id="algx1.l10.m1.1.1.3.2" xref="algx1.l10.m1.1.1.3.2.cmml">A</mi><mi id="algx1.l10.m1.1.1.3.3" xref="algx1.l10.m1.1.1.3.3.cmml">t</mi></msub><mrow id="algx1.l10.m1.1.1.1.1" xref="algx1.l10.m1.1.1.1.2.cmml"><mo id="algx1.l10.m1.1.1.1.1.2" stretchy="false" xref="algx1.l10.m1.1.1.1.2.1.cmml">|</mo><msub id="algx1.l10.m1.1.1.1.1.1" xref="algx1.l10.m1.1.1.1.1.1.cmml"><mi id="algx1.l10.m1.1.1.1.1.1.2" xref="algx1.l10.m1.1.1.1.1.1.2.cmml">D</mi><mi id="algx1.l10.m1.1.1.1.1.1.3" xref="algx1.l10.m1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="algx1.l10.m1.1.1.1.1.3" stretchy="false" xref="algx1.l10.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="algx1.l10.m1.1b"><apply id="algx1.l10.m1.1.2.cmml" xref="algx1.l10.m1.1.2"><eq id="algx1.l10.m1.1.2.1.cmml" xref="algx1.l10.m1.1.2.1"></eq><apply id="algx1.l10.m1.1.2.2.cmml" xref="algx1.l10.m1.1.2.2"><csymbol cd="ambiguous" id="algx1.l10.m1.1.2.2.1.cmml" xref="algx1.l10.m1.1.2.2">superscript</csymbol><ci id="algx1.l10.m1.1.2.2.2.cmml" xref="algx1.l10.m1.1.2.2.2">𝑎</ci><ci id="algx1.l10.m1.1.2.2.3.cmml" xref="algx1.l10.m1.1.2.2.3">𝑡</ci></apply><apply id="algx1.l10.m1.1.1.cmml" xref="algx1.l10.m1.1.1"><divide id="algx1.l10.m1.1.1.2.cmml" xref="algx1.l10.m1.1.1"></divide><apply id="algx1.l10.m1.1.1.3.cmml" xref="algx1.l10.m1.1.1.3"><csymbol cd="ambiguous" id="algx1.l10.m1.1.1.3.1.cmml" xref="algx1.l10.m1.1.1.3">subscript</csymbol><ci id="algx1.l10.m1.1.1.3.2.cmml" xref="algx1.l10.m1.1.1.3.2">𝐴</ci><ci id="algx1.l10.m1.1.1.3.3.cmml" xref="algx1.l10.m1.1.1.3.3">𝑡</ci></apply><apply id="algx1.l10.m1.1.1.1.2.cmml" xref="algx1.l10.m1.1.1.1.1"><abs id="algx1.l10.m1.1.1.1.2.1.cmml" xref="algx1.l10.m1.1.1.1.1.2"></abs><apply id="algx1.l10.m1.1.1.1.1.1.cmml" xref="algx1.l10.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="algx1.l10.m1.1.1.1.1.1.1.cmml" xref="algx1.l10.m1.1.1.1.1.1">subscript</csymbol><ci id="algx1.l10.m1.1.1.1.1.1.2.cmml" xref="algx1.l10.m1.1.1.1.1.1.2">𝐷</ci><ci id="algx1.l10.m1.1.1.1.1.1.3.cmml" xref="algx1.l10.m1.1.1.1.1.1.3">𝑡</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.m1.1c">a^{t}=\frac{A_{t}}{|D_{t}|}</annotation><annotation encoding="application/x-llamapun" id="algx1.l10.m1.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = divide start_ARG italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG | italic_D start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT | end_ARG</annotation></semantics></math> <span class="ltx_text" id="algx1.l10.2" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l10.1.m1.1"><semantics id="algx1.l10.1.m1.1a"><mo id="algx1.l10.1.m1.1.1" xref="algx1.l10.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l10.1.m1.1b"><ci id="algx1.l10.1.m1.1.1.cmml" xref="algx1.l10.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l10.1.m1.1d">▷</annotation></semantics></math> Compute average activation state for task <math alttext="t" class="ltx_Math" display="inline" id="algx1.l10.2.m2.1"><semantics id="algx1.l10.2.m2.1a"><mi id="algx1.l10.2.m2.1.1" xref="algx1.l10.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="algx1.l10.2.m2.1b"><ci id="algx1.l10.2.m2.1.1.cmml" xref="algx1.l10.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l10.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="algx1.l10.2.m2.1d">italic_t</annotation></semantics></math>
</span>
</div>
<div class="ltx_listingline" id="algx1.l11">
<span class="ltx_tag ltx_tag_listingline">11:</span>     <span class="ltx_text" id="algx1.l11.1" style="background-color:#D9D9D9;">Step 2: Neuron Selection</span>
</div>
<div class="ltx_listingline" id="algx1.l12">
<span class="ltx_tag ltx_tag_listingline">12:</span>     Initialize selected neurons set <math alttext="S_{k}^{t}=\emptyset" class="ltx_Math" display="inline" id="algx1.l12.m1.1"><semantics id="algx1.l12.m1.1a"><mrow id="algx1.l12.m1.1.1" xref="algx1.l12.m1.1.1.cmml"><msubsup id="algx1.l12.m1.1.1.2" xref="algx1.l12.m1.1.1.2.cmml"><mi id="algx1.l12.m1.1.1.2.2.2" xref="algx1.l12.m1.1.1.2.2.2.cmml">S</mi><mi id="algx1.l12.m1.1.1.2.2.3" xref="algx1.l12.m1.1.1.2.2.3.cmml">k</mi><mi id="algx1.l12.m1.1.1.2.3" xref="algx1.l12.m1.1.1.2.3.cmml">t</mi></msubsup><mo id="algx1.l12.m1.1.1.1" xref="algx1.l12.m1.1.1.1.cmml">=</mo><mi id="algx1.l12.m1.1.1.3" mathvariant="normal" xref="algx1.l12.m1.1.1.3.cmml">∅</mi></mrow><annotation-xml encoding="MathML-Content" id="algx1.l12.m1.1b"><apply id="algx1.l12.m1.1.1.cmml" xref="algx1.l12.m1.1.1"><eq id="algx1.l12.m1.1.1.1.cmml" xref="algx1.l12.m1.1.1.1"></eq><apply id="algx1.l12.m1.1.1.2.cmml" xref="algx1.l12.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l12.m1.1.1.2.1.cmml" xref="algx1.l12.m1.1.1.2">superscript</csymbol><apply id="algx1.l12.m1.1.1.2.2.cmml" xref="algx1.l12.m1.1.1.2"><csymbol cd="ambiguous" id="algx1.l12.m1.1.1.2.2.1.cmml" xref="algx1.l12.m1.1.1.2">subscript</csymbol><ci id="algx1.l12.m1.1.1.2.2.2.cmml" xref="algx1.l12.m1.1.1.2.2.2">𝑆</ci><ci id="algx1.l12.m1.1.1.2.2.3.cmml" xref="algx1.l12.m1.1.1.2.2.3">𝑘</ci></apply><ci id="algx1.l12.m1.1.1.2.3.cmml" xref="algx1.l12.m1.1.1.2.3">𝑡</ci></apply><emptyset id="algx1.l12.m1.1.1.3.cmml" xref="algx1.l12.m1.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l12.m1.1c">S_{k}^{t}=\emptyset</annotation><annotation encoding="application/x-llamapun" id="algx1.l12.m1.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = ∅</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l13">
<span class="ltx_tag ltx_tag_listingline">13:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l13.2">while</span> selection condition not met <span class="ltx_text ltx_font_bold" id="algx1.l13.3">do</span> <span class="ltx_text" id="algx1.l13.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx1.l13.1.m1.1"><semantics id="algx1.l13.1.m1.1a"><mo id="algx1.l13.1.m1.1.1" xref="algx1.l13.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx1.l13.1.m1.1b"><ci id="algx1.l13.1.m1.1.1.cmml" xref="algx1.l13.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx1.l13.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx1.l13.1.m1.1d">▷</annotation></semantics></math> Refer to Eq. <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S3.E1" title="In Neuron Selection. ‣ 3.1 Identifying Specialized Neurons ‣ 3 Neuron Structural Analysis ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">1</span></a> for condition
</span>
</div>
<div class="ltx_listingline" id="algx1.l14">
<span class="ltx_tag ltx_tag_listingline">14:</span>         Select neurons based on <math alttext="a^{t}" class="ltx_Math" display="inline" id="algx1.l14.m1.1"><semantics id="algx1.l14.m1.1a"><msup id="algx1.l14.m1.1.1" xref="algx1.l14.m1.1.1.cmml"><mi id="algx1.l14.m1.1.1.2" xref="algx1.l14.m1.1.1.2.cmml">a</mi><mi id="algx1.l14.m1.1.1.3" xref="algx1.l14.m1.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="algx1.l14.m1.1b"><apply id="algx1.l14.m1.1.1.cmml" xref="algx1.l14.m1.1.1"><csymbol cd="ambiguous" id="algx1.l14.m1.1.1.1.cmml" xref="algx1.l14.m1.1.1">superscript</csymbol><ci id="algx1.l14.m1.1.1.2.cmml" xref="algx1.l14.m1.1.1.2">𝑎</ci><ci id="algx1.l14.m1.1.1.3.cmml" xref="algx1.l14.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l14.m1.1c">a^{t}</annotation><annotation encoding="application/x-llamapun" id="algx1.l14.m1.1d">italic_a start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> and add them to <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="algx1.l14.m2.1"><semantics id="algx1.l14.m2.1a"><msubsup id="algx1.l14.m2.1.1" xref="algx1.l14.m2.1.1.cmml"><mi id="algx1.l14.m2.1.1.2.2" xref="algx1.l14.m2.1.1.2.2.cmml">S</mi><mi id="algx1.l14.m2.1.1.2.3" xref="algx1.l14.m2.1.1.2.3.cmml">k</mi><mi id="algx1.l14.m2.1.1.3" xref="algx1.l14.m2.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx1.l14.m2.1b"><apply id="algx1.l14.m2.1.1.cmml" xref="algx1.l14.m2.1.1"><csymbol cd="ambiguous" id="algx1.l14.m2.1.1.1.cmml" xref="algx1.l14.m2.1.1">superscript</csymbol><apply id="algx1.l14.m2.1.1.2.cmml" xref="algx1.l14.m2.1.1"><csymbol cd="ambiguous" id="algx1.l14.m2.1.1.2.1.cmml" xref="algx1.l14.m2.1.1">subscript</csymbol><ci id="algx1.l14.m2.1.1.2.2.cmml" xref="algx1.l14.m2.1.1.2.2">𝑆</ci><ci id="algx1.l14.m2.1.1.2.3.cmml" xref="algx1.l14.m2.1.1.2.3">𝑘</ci></apply><ci id="algx1.l14.m2.1.1.3.cmml" xref="algx1.l14.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx1.l14.m2.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="algx1.l14.m2.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="algx1.l15">
<span class="ltx_tag ltx_tag_listingline">15:</span>     <span class="ltx_text ltx_font_bold" id="algx1.l15.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l15.2">while</span>
</div>
<div class="ltx_listingline" id="algx1.l16">
<span class="ltx_tag ltx_tag_listingline">16:</span><span class="ltx_text ltx_font_bold" id="algx1.l16.1">end</span> <span class="ltx_text ltx_font_bold" id="algx1.l16.2">for</span>
</div>
</div>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg2.2.1.1">Algorithm 2</span> </span> Neuron Specialization Training</figcaption>
<div class="ltx_listing ltx_listing" id="alg2.3">
<div class="ltx_listingline" id="algx2.l1">
<span class="ltx_tag ltx_tag_listingline">1:</span><span class="ltx_text ltx_font_bold" id="algx2.l1.1">Input:</span> A pre-trained multi-task model <math alttext="\theta" class="ltx_Math" display="inline" id="algx2.l1.m1.1"><semantics id="algx2.l1.m1.1a"><mi id="algx2.l1.m1.1.1" xref="algx2.l1.m1.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="algx2.l1.m1.1b"><ci id="algx2.l1.m1.1.1.cmml" xref="algx2.l1.m1.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m1.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m1.1d">italic_θ</annotation></semantics></math> with dimensions <math alttext="d" class="ltx_Math" display="inline" id="algx2.l1.m2.1"><semantics id="algx2.l1.m2.1a"><mi id="algx2.l1.m2.1.1" xref="algx2.l1.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="algx2.l1.m2.1b"><ci id="algx2.l1.m2.1.1.cmml" xref="algx2.l1.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m2.1d">italic_d</annotation></semantics></math> and <math alttext="\mathit{d_{ff}}" class="ltx_Math" display="inline" id="algx2.l1.m3.1"><semantics id="algx2.l1.m3.1a"><msub id="algx2.l1.m3.1.1" xref="algx2.l1.m3.1.1.cmml"><mi id="algx2.l1.m3.1.1.2" xref="algx2.l1.m3.1.1.2.cmml">d</mi><mi id="algx2.l1.m3.1.1.3" xref="algx2.l1.m3.1.1.3.cmml">𝑓𝑓</mi></msub><annotation-xml encoding="MathML-Content" id="algx2.l1.m3.1b"><apply id="algx2.l1.m3.1.1.cmml" xref="algx2.l1.m3.1.1"><csymbol cd="ambiguous" id="algx2.l1.m3.1.1.1.cmml" xref="algx2.l1.m3.1.1">subscript</csymbol><ci id="algx2.l1.m3.1.1.2.cmml" xref="algx2.l1.m3.1.1.2">𝑑</ci><ci id="algx2.l1.m3.1.1.3.cmml" xref="algx2.l1.m3.1.1.3">𝑓𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m3.1c">\mathit{d_{ff}}</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m3.1d">italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT</annotation></semantics></math>. Corpora data <math alttext="C" class="ltx_Math" display="inline" id="algx2.l1.m4.1"><semantics id="algx2.l1.m4.1a"><mi id="algx2.l1.m4.1.1" xref="algx2.l1.m4.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="algx2.l1.m4.1b"><ci id="algx2.l1.m4.1.1.cmml" xref="algx2.l1.m4.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m4.1c">C</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m4.1d">italic_C</annotation></semantics></math> with <math alttext="T" class="ltx_Math" display="inline" id="algx2.l1.m5.1"><semantics id="algx2.l1.m5.1a"><mi id="algx2.l1.m5.1.1" xref="algx2.l1.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algx2.l1.m5.1b"><ci id="algx2.l1.m5.1.1.cmml" xref="algx2.l1.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m5.1c">T</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m5.1d">italic_T</annotation></semantics></math> tasks that contain both training and validation data. A set of selected specialized neurons <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="algx2.l1.m6.1"><semantics id="algx2.l1.m6.1a"><msubsup id="algx2.l1.m6.1.1" xref="algx2.l1.m6.1.1.cmml"><mi id="algx2.l1.m6.1.1.2.2" xref="algx2.l1.m6.1.1.2.2.cmml">S</mi><mi id="algx2.l1.m6.1.1.2.3" xref="algx2.l1.m6.1.1.2.3.cmml">k</mi><mi id="algx2.l1.m6.1.1.3" xref="algx2.l1.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx2.l1.m6.1b"><apply id="algx2.l1.m6.1.1.cmml" xref="algx2.l1.m6.1.1"><csymbol cd="ambiguous" id="algx2.l1.m6.1.1.1.cmml" xref="algx2.l1.m6.1.1">superscript</csymbol><apply id="algx2.l1.m6.1.1.2.cmml" xref="algx2.l1.m6.1.1"><csymbol cd="ambiguous" id="algx2.l1.m6.1.1.2.1.cmml" xref="algx2.l1.m6.1.1">subscript</csymbol><ci id="algx2.l1.m6.1.1.2.2.cmml" xref="algx2.l1.m6.1.1.2.2">𝑆</ci><ci id="algx2.l1.m6.1.1.2.3.cmml" xref="algx2.l1.m6.1.1.2.3">𝑘</ci></apply><ci id="algx2.l1.m6.1.1.3.cmml" xref="algx2.l1.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m6.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m6.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> for each task <math alttext="t" class="ltx_Math" display="inline" id="algx2.l1.m7.1"><semantics id="algx2.l1.m7.1a"><mi id="algx2.l1.m7.1.1" xref="algx2.l1.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="algx2.l1.m7.1b"><ci id="algx2.l1.m7.1.1.cmml" xref="algx2.l1.m7.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l1.m7.1c">t</annotation><annotation encoding="application/x-llamapun" id="algx2.l1.m7.1d">italic_t</annotation></semantics></math>.

</div>
<div class="ltx_listingline" id="algx2.l2">
<span class="ltx_tag ltx_tag_listingline">2:</span><span class="ltx_text ltx_font_bold" id="algx2.l2.1">Output:</span> A new specialized network <math alttext="\theta^{new}" class="ltx_Math" display="inline" id="algx2.l2.m1.1"><semantics id="algx2.l2.m1.1a"><msup id="algx2.l2.m1.1.1" xref="algx2.l2.m1.1.1.cmml"><mi id="algx2.l2.m1.1.1.2" xref="algx2.l2.m1.1.1.2.cmml">θ</mi><mrow id="algx2.l2.m1.1.1.3" xref="algx2.l2.m1.1.1.3.cmml"><mi id="algx2.l2.m1.1.1.3.2" xref="algx2.l2.m1.1.1.3.2.cmml">n</mi><mo id="algx2.l2.m1.1.1.3.1" xref="algx2.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l2.m1.1.1.3.3" xref="algx2.l2.m1.1.1.3.3.cmml">e</mi><mo id="algx2.l2.m1.1.1.3.1a" xref="algx2.l2.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l2.m1.1.1.3.4" xref="algx2.l2.m1.1.1.3.4.cmml">w</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="algx2.l2.m1.1b"><apply id="algx2.l2.m1.1.1.cmml" xref="algx2.l2.m1.1.1"><csymbol cd="ambiguous" id="algx2.l2.m1.1.1.1.cmml" xref="algx2.l2.m1.1.1">superscript</csymbol><ci id="algx2.l2.m1.1.1.2.cmml" xref="algx2.l2.m1.1.1.2">𝜃</ci><apply id="algx2.l2.m1.1.1.3.cmml" xref="algx2.l2.m1.1.1.3"><times id="algx2.l2.m1.1.1.3.1.cmml" xref="algx2.l2.m1.1.1.3.1"></times><ci id="algx2.l2.m1.1.1.3.2.cmml" xref="algx2.l2.m1.1.1.3.2">𝑛</ci><ci id="algx2.l2.m1.1.1.3.3.cmml" xref="algx2.l2.m1.1.1.3.3">𝑒</ci><ci id="algx2.l2.m1.1.1.3.4.cmml" xref="algx2.l2.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l2.m1.1c">\theta^{new}</annotation><annotation encoding="application/x-llamapun" id="algx2.l2.m1.1d">italic_θ start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT</annotation></semantics></math>. Note that only the fc1 weight matrix will be trained task-specifically, the other parameters are shared across tasks. In addition, <math alttext="\theta^{new}" class="ltx_Math" display="inline" id="algx2.l2.m2.1"><semantics id="algx2.l2.m2.1a"><msup id="algx2.l2.m2.1.1" xref="algx2.l2.m2.1.1.cmml"><mi id="algx2.l2.m2.1.1.2" xref="algx2.l2.m2.1.1.2.cmml">θ</mi><mrow id="algx2.l2.m2.1.1.3" xref="algx2.l2.m2.1.1.3.cmml"><mi id="algx2.l2.m2.1.1.3.2" xref="algx2.l2.m2.1.1.3.2.cmml">n</mi><mo id="algx2.l2.m2.1.1.3.1" xref="algx2.l2.m2.1.1.3.1.cmml">⁢</mo><mi id="algx2.l2.m2.1.1.3.3" xref="algx2.l2.m2.1.1.3.3.cmml">e</mi><mo id="algx2.l2.m2.1.1.3.1a" xref="algx2.l2.m2.1.1.3.1.cmml">⁢</mo><mi id="algx2.l2.m2.1.1.3.4" xref="algx2.l2.m2.1.1.3.4.cmml">w</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="algx2.l2.m2.1b"><apply id="algx2.l2.m2.1.1.cmml" xref="algx2.l2.m2.1.1"><csymbol cd="ambiguous" id="algx2.l2.m2.1.1.1.cmml" xref="algx2.l2.m2.1.1">superscript</csymbol><ci id="algx2.l2.m2.1.1.2.cmml" xref="algx2.l2.m2.1.1.2">𝜃</ci><apply id="algx2.l2.m2.1.1.3.cmml" xref="algx2.l2.m2.1.1.3"><times id="algx2.l2.m2.1.1.3.1.cmml" xref="algx2.l2.m2.1.1.3.1"></times><ci id="algx2.l2.m2.1.1.3.2.cmml" xref="algx2.l2.m2.1.1.3.2">𝑛</ci><ci id="algx2.l2.m2.1.1.3.3.cmml" xref="algx2.l2.m2.1.1.3.3">𝑒</ci><ci id="algx2.l2.m2.1.1.3.4.cmml" xref="algx2.l2.m2.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l2.m2.1c">\theta^{new}</annotation><annotation encoding="application/x-llamapun" id="algx2.l2.m2.1d">italic_θ start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> does not contain more trainable parameters than <math alttext="\theta" class="ltx_Math" display="inline" id="algx2.l2.m3.1"><semantics id="algx2.l2.m3.1a"><mi id="algx2.l2.m3.1.1" xref="algx2.l2.m3.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="algx2.l2.m3.1b"><ci id="algx2.l2.m3.1.1.cmml" xref="algx2.l2.m3.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l2.m3.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="algx2.l2.m3.1d">italic_θ</annotation></semantics></math> due to the sparse network feature.

</div>
<div class="ltx_listingline" id="algx2.l3">
<span class="ltx_tag ltx_tag_listingline">3:</span>Derive boolean mask <span class="ltx_text" id="algx2.l3.1"><math alttext="m^{t}\in\{0,1\}^{d_{\mathit{ff}}}" class="ltx_Math" display="inline" id="algx2.l3.1.m1.2"><semantics id="algx2.l3.1.m1.2a"><mrow id="algx2.l3.1.m1.2.3" xref="algx2.l3.1.m1.2.3.cmml"><msup id="algx2.l3.1.m1.2.3.2" xref="algx2.l3.1.m1.2.3.2.cmml"><mi id="algx2.l3.1.m1.2.3.2.2" xref="algx2.l3.1.m1.2.3.2.2.cmml">m</mi><mi id="algx2.l3.1.m1.2.3.2.3" xref="algx2.l3.1.m1.2.3.2.3.cmml">t</mi></msup><mo id="algx2.l3.1.m1.2.3.1" xref="algx2.l3.1.m1.2.3.1.cmml">∈</mo><msup id="algx2.l3.1.m1.2.3.3" xref="algx2.l3.1.m1.2.3.3.cmml"><mrow id="algx2.l3.1.m1.2.3.3.2.2" xref="algx2.l3.1.m1.2.3.3.2.1.cmml"><mo id="algx2.l3.1.m1.2.3.3.2.2.1" stretchy="false" xref="algx2.l3.1.m1.2.3.3.2.1.cmml">{</mo><mn id="algx2.l3.1.m1.1.1" xref="algx2.l3.1.m1.1.1.cmml">0</mn><mo id="algx2.l3.1.m1.2.3.3.2.2.2" xref="algx2.l3.1.m1.2.3.3.2.1.cmml">,</mo><mn id="algx2.l3.1.m1.2.2" xref="algx2.l3.1.m1.2.2.cmml">1</mn><mo id="algx2.l3.1.m1.2.3.3.2.2.3" stretchy="false" xref="algx2.l3.1.m1.2.3.3.2.1.cmml">}</mo></mrow><msub id="algx2.l3.1.m1.2.3.3.3" xref="algx2.l3.1.m1.2.3.3.3.cmml"><mi id="algx2.l3.1.m1.2.3.3.3.2" xref="algx2.l3.1.m1.2.3.3.3.2.cmml">d</mi><mi id="algx2.l3.1.m1.2.3.3.3.3" xref="algx2.l3.1.m1.2.3.3.3.3.cmml">𝑓𝑓</mi></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="algx2.l3.1.m1.2b"><apply id="algx2.l3.1.m1.2.3.cmml" xref="algx2.l3.1.m1.2.3"><in id="algx2.l3.1.m1.2.3.1.cmml" xref="algx2.l3.1.m1.2.3.1"></in><apply id="algx2.l3.1.m1.2.3.2.cmml" xref="algx2.l3.1.m1.2.3.2"><csymbol cd="ambiguous" id="algx2.l3.1.m1.2.3.2.1.cmml" xref="algx2.l3.1.m1.2.3.2">superscript</csymbol><ci id="algx2.l3.1.m1.2.3.2.2.cmml" xref="algx2.l3.1.m1.2.3.2.2">𝑚</ci><ci id="algx2.l3.1.m1.2.3.2.3.cmml" xref="algx2.l3.1.m1.2.3.2.3">𝑡</ci></apply><apply id="algx2.l3.1.m1.2.3.3.cmml" xref="algx2.l3.1.m1.2.3.3"><csymbol cd="ambiguous" id="algx2.l3.1.m1.2.3.3.1.cmml" xref="algx2.l3.1.m1.2.3.3">superscript</csymbol><set id="algx2.l3.1.m1.2.3.3.2.1.cmml" xref="algx2.l3.1.m1.2.3.3.2.2"><cn id="algx2.l3.1.m1.1.1.cmml" type="integer" xref="algx2.l3.1.m1.1.1">0</cn><cn id="algx2.l3.1.m1.2.2.cmml" type="integer" xref="algx2.l3.1.m1.2.2">1</cn></set><apply id="algx2.l3.1.m1.2.3.3.3.cmml" xref="algx2.l3.1.m1.2.3.3.3"><csymbol cd="ambiguous" id="algx2.l3.1.m1.2.3.3.3.1.cmml" xref="algx2.l3.1.m1.2.3.3.3">subscript</csymbol><ci id="algx2.l3.1.m1.2.3.3.3.2.cmml" xref="algx2.l3.1.m1.2.3.3.3.2">𝑑</ci><ci id="algx2.l3.1.m1.2.3.3.3.3.cmml" xref="algx2.l3.1.m1.2.3.3.3.3">𝑓𝑓</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l3.1.m1.2c">m^{t}\in\{0,1\}^{d_{\mathit{ff}}}</annotation><annotation encoding="application/x-llamapun" id="algx2.l3.1.m1.2d">italic_m start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ∈ { 0 , 1 } start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_ff end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></span> from <math alttext="S_{k}^{t}" class="ltx_Math" display="inline" id="algx2.l3.m1.1"><semantics id="algx2.l3.m1.1a"><msubsup id="algx2.l3.m1.1.1" xref="algx2.l3.m1.1.1.cmml"><mi id="algx2.l3.m1.1.1.2.2" xref="algx2.l3.m1.1.1.2.2.cmml">S</mi><mi id="algx2.l3.m1.1.1.2.3" xref="algx2.l3.m1.1.1.2.3.cmml">k</mi><mi id="algx2.l3.m1.1.1.3" xref="algx2.l3.m1.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="algx2.l3.m1.1b"><apply id="algx2.l3.m1.1.1.cmml" xref="algx2.l3.m1.1.1"><csymbol cd="ambiguous" id="algx2.l3.m1.1.1.1.cmml" xref="algx2.l3.m1.1.1">superscript</csymbol><apply id="algx2.l3.m1.1.1.2.cmml" xref="algx2.l3.m1.1.1"><csymbol cd="ambiguous" id="algx2.l3.m1.1.1.2.1.cmml" xref="algx2.l3.m1.1.1">subscript</csymbol><ci id="algx2.l3.m1.1.1.2.2.cmml" xref="algx2.l3.m1.1.1.2.2">𝑆</ci><ci id="algx2.l3.m1.1.1.2.3.cmml" xref="algx2.l3.m1.1.1.2.3">𝑘</ci></apply><ci id="algx2.l3.m1.1.1.3.cmml" xref="algx2.l3.m1.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l3.m1.1c">S_{k}^{t}</annotation><annotation encoding="application/x-llamapun" id="algx2.l3.m1.1d">italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> for each layer

</div>
<div class="ltx_listingline" id="algx2.l4">
<span class="ltx_tag ltx_tag_listingline">4:</span><span class="ltx_text ltx_font_bold" id="algx2.l4.1">while</span> <math alttext="\theta^{new}" class="ltx_Math" display="inline" id="algx2.l4.m1.1"><semantics id="algx2.l4.m1.1a"><msup id="algx2.l4.m1.1.1" xref="algx2.l4.m1.1.1.cmml"><mi id="algx2.l4.m1.1.1.2" xref="algx2.l4.m1.1.1.2.cmml">θ</mi><mrow id="algx2.l4.m1.1.1.3" xref="algx2.l4.m1.1.1.3.cmml"><mi id="algx2.l4.m1.1.1.3.2" xref="algx2.l4.m1.1.1.3.2.cmml">n</mi><mo id="algx2.l4.m1.1.1.3.1" xref="algx2.l4.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l4.m1.1.1.3.3" xref="algx2.l4.m1.1.1.3.3.cmml">e</mi><mo id="algx2.l4.m1.1.1.3.1a" xref="algx2.l4.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l4.m1.1.1.3.4" xref="algx2.l4.m1.1.1.3.4.cmml">w</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="algx2.l4.m1.1b"><apply id="algx2.l4.m1.1.1.cmml" xref="algx2.l4.m1.1.1"><csymbol cd="ambiguous" id="algx2.l4.m1.1.1.1.cmml" xref="algx2.l4.m1.1.1">superscript</csymbol><ci id="algx2.l4.m1.1.1.2.cmml" xref="algx2.l4.m1.1.1.2">𝜃</ci><apply id="algx2.l4.m1.1.1.3.cmml" xref="algx2.l4.m1.1.1.3"><times id="algx2.l4.m1.1.1.3.1.cmml" xref="algx2.l4.m1.1.1.3.1"></times><ci id="algx2.l4.m1.1.1.3.2.cmml" xref="algx2.l4.m1.1.1.3.2">𝑛</ci><ci id="algx2.l4.m1.1.1.3.3.cmml" xref="algx2.l4.m1.1.1.3.3">𝑒</ci><ci id="algx2.l4.m1.1.1.3.4.cmml" xref="algx2.l4.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l4.m1.1c">\theta^{new}</annotation><annotation encoding="application/x-llamapun" id="algx2.l4.m1.1d">italic_θ start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> not converge <span class="ltx_text ltx_font_bold" id="algx2.l4.2">do</span>
</div>
<div class="ltx_listingline" id="algx2.l5">
<span class="ltx_tag ltx_tag_listingline">5:</span>     <span class="ltx_text ltx_font_bold" id="algx2.l5.1">for</span> task <math alttext="t" class="ltx_Math" display="inline" id="algx2.l5.m1.1"><semantics id="algx2.l5.m1.1a"><mi id="algx2.l5.m1.1.1" xref="algx2.l5.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="algx2.l5.m1.1b"><ci id="algx2.l5.m1.1.1.cmml" xref="algx2.l5.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l5.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="algx2.l5.m1.1d">italic_t</annotation></semantics></math> in <math alttext="T" class="ltx_Math" display="inline" id="algx2.l5.m2.1"><semantics id="algx2.l5.m2.1a"><mi id="algx2.l5.m2.1.1" xref="algx2.l5.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="algx2.l5.m2.1b"><ci id="algx2.l5.m2.1.1.cmml" xref="algx2.l5.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l5.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="algx2.l5.m2.1d">italic_T</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="algx2.l5.2">do</span>
</div>
<div class="ltx_listingline" id="algx2.l6">
<span class="ltx_tag ltx_tag_listingline">6:</span>         <math alttext="W^{T}_{1}=m^{t}\cdot W^{\theta}_{1}" class="ltx_Math" display="inline" id="algx2.l6.m1.1"><semantics id="algx2.l6.m1.1a"><mrow id="algx2.l6.m1.1.1" xref="algx2.l6.m1.1.1.cmml"><msubsup id="algx2.l6.m1.1.1.2" xref="algx2.l6.m1.1.1.2.cmml"><mi id="algx2.l6.m1.1.1.2.2.2" xref="algx2.l6.m1.1.1.2.2.2.cmml">W</mi><mn id="algx2.l6.m1.1.1.2.3" xref="algx2.l6.m1.1.1.2.3.cmml">1</mn><mi id="algx2.l6.m1.1.1.2.2.3" xref="algx2.l6.m1.1.1.2.2.3.cmml">T</mi></msubsup><mo id="algx2.l6.m1.1.1.1" xref="algx2.l6.m1.1.1.1.cmml">=</mo><mrow id="algx2.l6.m1.1.1.3" xref="algx2.l6.m1.1.1.3.cmml"><msup id="algx2.l6.m1.1.1.3.2" xref="algx2.l6.m1.1.1.3.2.cmml"><mi id="algx2.l6.m1.1.1.3.2.2" xref="algx2.l6.m1.1.1.3.2.2.cmml">m</mi><mi id="algx2.l6.m1.1.1.3.2.3" xref="algx2.l6.m1.1.1.3.2.3.cmml">t</mi></msup><mo id="algx2.l6.m1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="algx2.l6.m1.1.1.3.1.cmml">⋅</mo><msubsup id="algx2.l6.m1.1.1.3.3" xref="algx2.l6.m1.1.1.3.3.cmml"><mi id="algx2.l6.m1.1.1.3.3.2.2" xref="algx2.l6.m1.1.1.3.3.2.2.cmml">W</mi><mn id="algx2.l6.m1.1.1.3.3.3" xref="algx2.l6.m1.1.1.3.3.3.cmml">1</mn><mi id="algx2.l6.m1.1.1.3.3.2.3" xref="algx2.l6.m1.1.1.3.3.2.3.cmml">θ</mi></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="algx2.l6.m1.1b"><apply id="algx2.l6.m1.1.1.cmml" xref="algx2.l6.m1.1.1"><eq id="algx2.l6.m1.1.1.1.cmml" xref="algx2.l6.m1.1.1.1"></eq><apply id="algx2.l6.m1.1.1.2.cmml" xref="algx2.l6.m1.1.1.2"><csymbol cd="ambiguous" id="algx2.l6.m1.1.1.2.1.cmml" xref="algx2.l6.m1.1.1.2">subscript</csymbol><apply id="algx2.l6.m1.1.1.2.2.cmml" xref="algx2.l6.m1.1.1.2"><csymbol cd="ambiguous" id="algx2.l6.m1.1.1.2.2.1.cmml" xref="algx2.l6.m1.1.1.2">superscript</csymbol><ci id="algx2.l6.m1.1.1.2.2.2.cmml" xref="algx2.l6.m1.1.1.2.2.2">𝑊</ci><ci id="algx2.l6.m1.1.1.2.2.3.cmml" xref="algx2.l6.m1.1.1.2.2.3">𝑇</ci></apply><cn id="algx2.l6.m1.1.1.2.3.cmml" type="integer" xref="algx2.l6.m1.1.1.2.3">1</cn></apply><apply id="algx2.l6.m1.1.1.3.cmml" xref="algx2.l6.m1.1.1.3"><ci id="algx2.l6.m1.1.1.3.1.cmml" xref="algx2.l6.m1.1.1.3.1">⋅</ci><apply id="algx2.l6.m1.1.1.3.2.cmml" xref="algx2.l6.m1.1.1.3.2"><csymbol cd="ambiguous" id="algx2.l6.m1.1.1.3.2.1.cmml" xref="algx2.l6.m1.1.1.3.2">superscript</csymbol><ci id="algx2.l6.m1.1.1.3.2.2.cmml" xref="algx2.l6.m1.1.1.3.2.2">𝑚</ci><ci id="algx2.l6.m1.1.1.3.2.3.cmml" xref="algx2.l6.m1.1.1.3.2.3">𝑡</ci></apply><apply id="algx2.l6.m1.1.1.3.3.cmml" xref="algx2.l6.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx2.l6.m1.1.1.3.3.1.cmml" xref="algx2.l6.m1.1.1.3.3">subscript</csymbol><apply id="algx2.l6.m1.1.1.3.3.2.cmml" xref="algx2.l6.m1.1.1.3.3"><csymbol cd="ambiguous" id="algx2.l6.m1.1.1.3.3.2.1.cmml" xref="algx2.l6.m1.1.1.3.3">superscript</csymbol><ci id="algx2.l6.m1.1.1.3.3.2.2.cmml" xref="algx2.l6.m1.1.1.3.3.2.2">𝑊</ci><ci id="algx2.l6.m1.1.1.3.3.2.3.cmml" xref="algx2.l6.m1.1.1.3.3.2.3">𝜃</ci></apply><cn id="algx2.l6.m1.1.1.3.3.3.cmml" type="integer" xref="algx2.l6.m1.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l6.m1.1c">W^{T}_{1}=m^{t}\cdot W^{\theta}_{1}</annotation><annotation encoding="application/x-llamapun" id="algx2.l6.m1.1d">italic_W start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = italic_m start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⋅ italic_W start_POSTSUPERSCRIPT italic_θ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text" id="algx2.l6.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx2.l6.1.m1.1"><semantics id="algx2.l6.1.m1.1a"><mo id="algx2.l6.1.m1.1.1" xref="algx2.l6.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx2.l6.1.m1.1b"><ci id="algx2.l6.1.m1.1.1.cmml" xref="algx2.l6.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l6.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx2.l6.1.m1.1d">▷</annotation></semantics></math> We perform this for all layers, refer to EQ. <a class="ltx_ref" href="https://arxiv.org/html/2404.11201v1#S4.E3" title="In 4.2 Specializing Task-Specific FFN ‣ 4 Neuron Specialization Training ‣ Neuron Specialization: Leveraging Intrinsic Task Modularity for Multilingual Machine Translation"><span class="ltx_text ltx_ref_tag">3</span></a>
</span>
</div>
<div class="ltx_listingline" id="algx2.l7">
<span class="ltx_tag ltx_tag_listingline">7:</span>         Train <math alttext="\theta^{new}" class="ltx_Math" display="inline" id="algx2.l7.m1.1"><semantics id="algx2.l7.m1.1a"><msup id="algx2.l7.m1.1.1" xref="algx2.l7.m1.1.1.cmml"><mi id="algx2.l7.m1.1.1.2" xref="algx2.l7.m1.1.1.2.cmml">θ</mi><mrow id="algx2.l7.m1.1.1.3" xref="algx2.l7.m1.1.1.3.cmml"><mi id="algx2.l7.m1.1.1.3.2" xref="algx2.l7.m1.1.1.3.2.cmml">n</mi><mo id="algx2.l7.m1.1.1.3.1" xref="algx2.l7.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l7.m1.1.1.3.3" xref="algx2.l7.m1.1.1.3.3.cmml">e</mi><mo id="algx2.l7.m1.1.1.3.1a" xref="algx2.l7.m1.1.1.3.1.cmml">⁢</mo><mi id="algx2.l7.m1.1.1.3.4" xref="algx2.l7.m1.1.1.3.4.cmml">w</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="algx2.l7.m1.1b"><apply id="algx2.l7.m1.1.1.cmml" xref="algx2.l7.m1.1.1"><csymbol cd="ambiguous" id="algx2.l7.m1.1.1.1.cmml" xref="algx2.l7.m1.1.1">superscript</csymbol><ci id="algx2.l7.m1.1.1.2.cmml" xref="algx2.l7.m1.1.1.2">𝜃</ci><apply id="algx2.l7.m1.1.1.3.cmml" xref="algx2.l7.m1.1.1.3"><times id="algx2.l7.m1.1.1.3.1.cmml" xref="algx2.l7.m1.1.1.3.1"></times><ci id="algx2.l7.m1.1.1.3.2.cmml" xref="algx2.l7.m1.1.1.3.2">𝑛</ci><ci id="algx2.l7.m1.1.1.3.3.cmml" xref="algx2.l7.m1.1.1.3.3">𝑒</ci><ci id="algx2.l7.m1.1.1.3.4.cmml" xref="algx2.l7.m1.1.1.3.4">𝑤</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l7.m1.1c">\theta^{new}</annotation><annotation encoding="application/x-llamapun" id="algx2.l7.m1.1d">italic_θ start_POSTSUPERSCRIPT italic_n italic_e italic_w end_POSTSUPERSCRIPT</annotation></semantics></math> using <math alttext="C^{t}" class="ltx_Math" display="inline" id="algx2.l7.m2.1"><semantics id="algx2.l7.m2.1a"><msup id="algx2.l7.m2.1.1" xref="algx2.l7.m2.1.1.cmml"><mi id="algx2.l7.m2.1.1.2" xref="algx2.l7.m2.1.1.2.cmml">C</mi><mi id="algx2.l7.m2.1.1.3" xref="algx2.l7.m2.1.1.3.cmml">t</mi></msup><annotation-xml encoding="MathML-Content" id="algx2.l7.m2.1b"><apply id="algx2.l7.m2.1.1.cmml" xref="algx2.l7.m2.1.1"><csymbol cd="ambiguous" id="algx2.l7.m2.1.1.1.cmml" xref="algx2.l7.m2.1.1">superscript</csymbol><ci id="algx2.l7.m2.1.1.2.cmml" xref="algx2.l7.m2.1.1.2">𝐶</ci><ci id="algx2.l7.m2.1.1.3.cmml" xref="algx2.l7.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="algx2.l7.m2.1c">C^{t}</annotation><annotation encoding="application/x-llamapun" id="algx2.l7.m2.1d">italic_C start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> <span class="ltx_text" id="algx2.l7.1" style="float:right;"><math alttext="\triangleright" class="ltx_Math" display="inline" id="algx2.l7.1.m1.1"><semantics id="algx2.l7.1.m1.1a"><mo id="algx2.l7.1.m1.1.1" xref="algx2.l7.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="algx2.l7.1.m1.1b"><ci id="algx2.l7.1.m1.1.1.cmml" xref="algx2.l7.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="algx2.l7.1.m1.1c">\triangleright</annotation><annotation encoding="application/x-llamapun" id="algx2.l7.1.m1.1d">▷</annotation></semantics></math> All parameters will be updated, yet fc1 layers are task specific
</span>
</div>
<div class="ltx_listingline" id="algx2.l8">
<span class="ltx_tag ltx_tag_listingline">8:</span>     <span class="ltx_text ltx_font_bold" id="algx2.l8.1">end</span> <span class="ltx_text ltx_font_bold" id="algx2.l8.2">for</span>
</div>
<div class="ltx_listingline" id="algx2.l9">
<span class="ltx_tag ltx_tag_listingline">9:</span><span class="ltx_text ltx_font_bold" id="algx2.l9.1">end</span> <span class="ltx_text ltx_font_bold" id="algx2.l9.2">while</span>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed May  1 15:38:04 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
