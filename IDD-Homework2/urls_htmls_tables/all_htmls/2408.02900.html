<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</title>
<!--Generated on Mon Aug  5 19:16:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.02900v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S1" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S2" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>MedTrinity-25M Dataset</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS1" title="In 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data Triplet</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS1.SSS0.Px1" title="In 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Images.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS1.SSS0.Px2" title="In 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">ROIs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS1.SSS0.Px3" title="In 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Textual Descriptions.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2" title="In 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Construction Pipeline</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS1" title="In 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Data Processing</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS1.Px1" title="In 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Coarse Caption Generation via Metadata Integration.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS1.Px2" title="In 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">ROI Locating.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS1.Px3" title="In 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Medical Knowledge Retrieval.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS2" title="In 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Generation of Multigranular Text Description</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS2.Px1" title="In 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Choice of MLLMs</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3" title="In 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Dataset Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3.SSS0.Px1" title="In 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Diversity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3.SSS0.Px2" title="In 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Scale</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3.SSS0.Px3" title="In 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Diseases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3.SSS0.Px4" title="In 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Richness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS3.SSS0.Px5" title="In 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title">Alignment with human</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S4" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>LLaVA-Med++: Experimental Training with MedTrinity-25M</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S5" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A1" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Data Source</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A2" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Quantitative Comparison of LLaVA-Med++ with GPT-4V</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A3" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Examples of ROIs for Normal Regions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A4" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>List of Expert models to locate ROIs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A5" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Evaluation Prompt of Alignment to Human Annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A6" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Prompt Template for Generation of Multigranular Text Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A7" title="In MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Datasheet for MedTrinity-25M</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id16.16.16">Yunfei Xie<sup class="ltx_sup" id="id16.16.16.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id16.16.16.1.1">1,∗</span></sup>   Ce Zhou<sup class="ltx_sup" id="id16.16.16.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id16.16.16.2.1">1,∗</span></sup>   Lang Gao<sup class="ltx_sup" id="id16.16.16.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id16.16.16.3.1">1,∗</span></sup>   Juncheng Wu<sup class="ltx_sup" id="id16.16.16.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id16.16.16.4.1">2,∗</span></sup>   Xianhang Li<sup class="ltx_sup" id="id16.16.16.5"><span class="ltx_text ltx_font_medium" id="id16.16.16.5.1">2</span></sup>
<br class="ltx_break"/>Hong-Yu Zhou<sup class="ltx_sup" id="id16.16.16.6"><span class="ltx_text ltx_font_medium" id="id16.16.16.6.1">3</span></sup>   Sheng Liu<sup class="ltx_sup" id="id16.16.16.7"><span class="ltx_text ltx_font_medium" id="id16.16.16.7.1">4</span></sup>   Lei Xing<sup class="ltx_sup" id="id16.16.16.8"><span class="ltx_text ltx_font_medium" id="id16.16.16.8.1">4</span></sup>   James Zou<sup class="ltx_sup" id="id16.16.16.9"><span class="ltx_text ltx_font_medium" id="id16.16.16.9.1">4</span></sup>   Cihang Xie<sup class="ltx_sup" id="id16.16.16.10"><span class="ltx_text ltx_font_medium" id="id16.16.16.10.1">2</span></sup>   Yuyin Zhou<sup class="ltx_sup" id="id16.16.16.11"><span class="ltx_text ltx_font_medium" id="id16.16.16.11.1">2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.16.12"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id16.16.16.12.1">⋆</span></sup>equal technical contribution 
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.16.13"><span class="ltx_text ltx_font_medium" id="id16.16.16.13.1">1</span></sup>Huazhong University of Science and Technology   <sup class="ltx_sup" id="id16.16.16.14"><span class="ltx_text ltx_font_medium" id="id16.16.16.14.1">2</span></sup>UC Santa Cruz 
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.16.15"><span class="ltx_text ltx_font_medium" id="id16.16.16.15.1">3</span></sup>Harvard University   <sup class="ltx_sup" id="id16.16.16.16"><span class="ltx_text ltx_font_medium" id="id16.16.16.16.1">4</span></sup>Stanford University
</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions.
We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular texual descriptions.
Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation.
Pretraining on MedTrinity-25M, our model achieves state-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal large language models and other representative SoTA approaches.
This dataset can also be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain.
The dataset is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://yunfeixie233.github.io/MedTrinity-25M/" title="">https://yunfeixie233.github.io/MedTrinity-25M/</a>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large-scale multimodal foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib5" title="">5</a>]</cite> have demonstrated remarkable success across various domains due to their ability to understand complex visual patterns in conjunction with natural language. This success has sparked significant interest in applying such models to medical vision-language tasks.
Much progress has been made to improve the medical capacity of general domain multimodal foundation models by constructing medical datasets with image-text pairs and fine-tuning general domain models on these datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, current medical datasets have several limitations.
Firstly, these datasets lack <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">multigranular</span> annotations that reveal the correlation between local and global information within medical images.
Medical images often contain detailed cues, such as regional abnormal textures or structures, which may indicate specific types of lesions. Therefore, multimodal models need the ability to infer global information, such as disease or lesion type, from local details. The absence of such data limits the models’ capacity to comprehensively understand medical images.
Moreover, current dataset construction methods heavily rely on medical images paired with reports or captions, which restricts their scalability.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we address the above challenges by proposing an automated data construction pipeline using multimodal large language models (MLLMss) without relying on paired text descriptions.
To address the lack of comprehensive medical knowledge in general-purpose MLLMs, we leverage domain-specific expert grounding models and retrieval-augmented generation (RAG) to extract relevant medical knowledge. We then prompt MLLMs to generate multigranular visual and textual annotations enriched with this knowledge based on identified regions of interest (ROIs).
We utilize this pipeline to transform the collected data, including large-scale unpaired images, into image-ROI-description triplets.
These triplets provide multigranular annotations that encompass both global textual information, such as disease/lesion type, modality, and inter-regional relationships, as well as detailed local annotations for ROIs, including bounding boxes, segmentation masks, and region-specific textual descriptions.
Using the proposed pipeline, we create a large-scale multimodal multigranular medical dataset containing over 25 million triplets, named <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">MedTrinity-25M</span>. To our best knowledge, this is the largest multimodal dataset in medicine to date.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Initially, we assemble a large amount of medical data from over 90 online resources such as
<a class="ltx_ref ltx_href" href="https://www.cancerimagingarchive.net/" title="">TCIA</a>, <a class="ltx_ref ltx_href" href="https://www.kaggle.com/" title="">Kaggle</a>, <a class="ltx_ref ltx_href" href="https://zenodo.org/" title="">Zenodo</a>, <a class="ltx_ref ltx_href" href="https://www.synapse.org/" title="">Synapse</a>, etc.
In addition to images with a small amount of high-quality paired manual reports, this assembled data also includes two types of coarse medical data: 1) Image data with segmentation masks, lesion bounding boxes, or only disease types but lacking detailed textual descriptions, and 2) Images paired with coarse captions that describe only global modality or disease information, but lack detailed descriptions of local regions.
To generate multigranular annotations from the massive coarse medical data, we first identify ROIs that contain disease or lesion patterns by applying expert grounding models. We then build a comprehensive knowledge base from online corpora (e.g., PubMed) and retrieve image-related medical knowledge. Finally, we prompt MLLMs to integrate medical knowledge with guidance of identified ROIs to generate multigranular textual descriptions.

</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Medical Multimodal Foundation Models. </span>
Due to the effectiveness of multimodal foundation models in understanding visual features, adapting these models to perform medical vision-language tasks has garnered increasing attention in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib5" title="">5</a>]</cite>.
Several papers attempt to adapt general domain multimodal foundation models with varying architecture to medical domain through end-to-end training on medical datasets.
For example, Med-Flamingo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib11" title="">11</a>]</cite> enhances the medical capacity of OpenFlamingo-9B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib13" title="">13</a>]</cite> by fine-tuning it with 0.8M interleaved and 1.6M paired medical image-text data.
While Med-PalM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib12" title="">12</a>]</cite> adapts PaLM-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib14" title="">14</a>]</cite> to medical domain using approximately 1M medical data points, demonstrating competitive or surpassing performance compared to state-of-the-art models.
Additionally, LLaVA-Med <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>]</cite> employs end-to-end visual instruction tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib1" title="">1</a>]</cite> with two stages, achieving remarkable results in medical Visual Question Answering (VQA) tasks.
Similarly, Med-Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib15" title="">15</a>]</cite> employs a long-form question answering dataset to enhance the multimodal and long-context capabilities of baseline Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib16" title="">16</a>]</cite>.
Although these models have achieved remarkable performance, they are still limited by the scale of training data.
Prior research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib17" title="">17</a>]</cite> has shown that scaling up the training data improves the performance of large multimodal foundation models.
In this paper, we aim to build a large-scale medical dataset to facilitate the development of more powerful medical multimodal foundation models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Multimodal Datasets for medicine. </span>
The significance of construting comprehensive medical multimodal datasets has garnered considerable attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>]</cite>.
Several works attempt to collect images and paired clinical reports prepared by pathology specialist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>]</cite>, which provide comprehensive descriptions of images, including disease types and corresponding reasoning.
For example, MIMIC-CXR<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>]</cite> comprises 227,835 images for 65,379 patients, containing pathological findings and impressions in reports paired with each images.
However, manually constructing such reports is both time-consuming and expensive, thereby limiting the scale of these datasets.
PMC-OA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib20" title="">20</a>]</cite> aims to expand the dataset scale by extracting a large number of image-caption pairs from medical papers, increasing the number of data samples to 1.65 million. However, the extracted captions are less detailed compared to manual clinical reports, resulting in a lack of multigranular annotations. RadGenome-Chest CT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib19" title="">19</a>]</cite> includes more detailed annotations, such as segmentation masks and medical reports generated by MLLMs. Nonetheless, its construction method still relies on paired image-text data, which limits its scalability.
Unlike these existing methods, we devise the first automated data construction pipeline to generate multigranular annotations for unpaired images, achieving a comprehensive multigranular dataset with 25 million data samples.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>MedTrinity-25M Dataset</h2>
<figure class="ltx_figure" id="S3.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="203" id="S3.F1.sf1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Qualitative Comparison with sample in radiology report of chest x-rays dataset MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib21" title="">21</a>]</cite>.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="184" id="S3.F1.sf2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Qualitative Comparison with sample in visual QA dataset SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite>.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="184" id="S3.F1.sf3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Qualitative Comparison with sample in radiology objects caption dataset ROCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib18" title="">18</a>]</cite>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Qualitative comparison with different types of dataset.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Triplet</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our dataset comprises triplets of <math alttext="\{\texttt{image},\texttt{ROI},\texttt{description}\}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.3"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.4.2" xref="S3.SS1.p1.1.m1.3.4.1.cmml"><mo id="S3.SS1.p1.1.m1.3.4.2.1" stretchy="false" xref="S3.SS1.p1.1.m1.3.4.1.cmml">{</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1a.cmml">image</mtext><mo id="S3.SS1.p1.1.m1.3.4.2.2" xref="S3.SS1.p1.1.m1.3.4.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.2.2" xref="S3.SS1.p1.1.m1.2.2a.cmml">ROI</mtext><mo id="S3.SS1.p1.1.m1.3.4.2.3" xref="S3.SS1.p1.1.m1.3.4.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.3.3" xref="S3.SS1.p1.1.m1.3.3a.cmml">description</mtext><mo id="S3.SS1.p1.1.m1.3.4.2.4" stretchy="false" xref="S3.SS1.p1.1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><set id="S3.SS1.p1.1.m1.3.4.1.cmml" xref="S3.SS1.p1.1.m1.3.4.2"><ci id="S3.SS1.p1.1.m1.1.1a.cmml" xref="S3.SS1.p1.1.m1.1.1"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">image</mtext></ci><ci id="S3.SS1.p1.1.m1.2.2a.cmml" xref="S3.SS1.p1.1.m1.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">ROI</mtext></ci><ci id="S3.SS1.p1.1.m1.3.3a.cmml" xref="S3.SS1.p1.1.m1.3.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS1.p1.1.m1.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3">description</mtext></ci></set></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">\{\texttt{image},\texttt{ROI},\texttt{description}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.3d">{ image , ROI , description }</annotation></semantics></math>. Each <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">ROI</span> is associated with an abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant region within the <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">image</span>. For each image, we provide a multigranular textual <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.3">description</span>, which includes the disease/lesion type, modality, region-specific description, and inter-regional relationships as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F2" title="In Textual Descriptions. ‣ 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Images.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">We use the original medical image in the source dataset, we extensively collected medical datasets from the following sources: (1) online resources such as <a class="ltx_ref ltx_href" href="https://www.cancerimagingarchive.net/" title="">TCIA</a>, <a class="ltx_ref ltx_href" href="https://www.kaggle.com/" title="">Kaggle</a>, <a class="ltx_ref ltx_href" href="https://zenodo.org/" title="">Zenodo</a>, <a class="ltx_ref ltx_href" href="https://www.synapse.org/" title="">Synapse</a>, <a class="ltx_ref ltx_href" href="https://huggingface.co/" title="">Hugging Face</a>,<a class="ltx_ref ltx_href" href="https://grand-challenge.org/" title="">Grand Challenge</a> , <a class="ltx_ref ltx_href" href="https://github.com/" title="">GitHub</a>, etc. (2) relevant medical dataset research, such as CheXpert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>]</cite> and DeepLesion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib23" title="">23</a>]</cite>. These datasets were first categorized into two types: (1) datasets containing local annotations, such as MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>]</cite> with corresponding radiology reports, and PMC-OA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib24" title="">24</a>]</cite> with corresponding captions, where the reports or captions provide analysis of specific local conditions in the images; another example is the 3D image segmentation dataset BraTS2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib25" title="">25</a>]</cite>, which marks the tumor regions in CT scans with masks. (2) datasets containing global annotations: such as image classification datasets ISIC2019 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib26" title="">26</a>]</cite> and ISIC2020 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib27" title="">27</a>]</cite>, whose classification labels reflect the overall pathological condition of tissue sections; another example is the CheXpert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>]</cite> dataset, which provides detailed classification of disease types for each chest X-ray. We collect 25,001,668 samples spanning 10 modalities and
over 65 diseases. For 3D volumetric
images stored in DICOM or NIfTI formats, we converted
each 2D slice to PNG format.
Additional caption and annotations like masks and bounding boxes from these
datasets were utilized to construct ROIs and corresponding textual descriptions as below.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">ROIs.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">For each image, ROIs are highlighted using segmentation masks or bounding boxes. These ROIs mostly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities.
In the few cases without abnormalities, the ROIs generally indicate the primary object or organ in the image, as shown in examples in the supplementary material.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Textual Descriptions.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">The textual descriptions for each image are provided with detailed information across various aspects. Unlike the unstructured free-text descriptions found in previous medical report datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib6" title="">6</a>]</cite> or simple short sentences in visual QA dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite> and caption dataset<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib24" title="">24</a>]</cite>, our textual descriptions are multigranular and structured. General attributes related to the image are described first, including the image modality, the specific organ depicted, and the type of disease presented. Subsequently, ROI-related information is provided, including their locations and the abnormal characteristics within them that indicate underlying pathology, such as distinctive color and texture. Additionally, comparisons between the ROIs and surrounding regions are presented to highlight differences in features and the extent of disease progression.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">We also demonstrate the multigranular textual descriptions in our dataset with those in other common forms. As illustrated in  <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F1" title="In 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib21" title="">21</a>]</cite>, visual QA dataset SLAKE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite> and radiology objects caption dataset ROCO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="402" id="S3.F2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
<span class="ltx_text ltx_font_bold" id="S3.F2.5.1">Data construction pipeline.</span> 1) Data processing: extracting essential information from collected data, including <span class="ltx_text ltx_font_bold" id="S3.F2.6.2">metadata integration</span> to generate coarse caption, <span class="ltx_text ltx_font_bold" id="S3.F2.7.3">ROI locating</span>, and <span class="ltx_text ltx_font_bold" id="S3.F2.8.4">medical knowledge collection</span>. 2) Multigranular textual description generation: using this information to prompt MLLMs to generate fine-grained captions.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S3.F3.g1" src="x5.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
<span class="ltx_text ltx_font_bold" id="S3.F3.2.1">A qualitative comparison example of generated textual description with and without coarse caption.</span> Without a coarse caption, MLLMs fails to detect diseases. On the contrary, providing a caption mentioning “COVID-19” allows MLLMs to identify and categorize the disease, facilitating further analysis.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S3.F4.g1" src="x6.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
<span class="ltx_text ltx_font_bold" id="S3.F4.2.1">A qualitative comparison example of generated textual description with and without locating ROIs.</span> Without ROIs, the caption offers only a brief global analysis; with ROIs, MLLMs conducts detailed local analysis and assesses the impact of lesion ROIs on adjacent normal regions.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="344" id="S3.F5.g1" src="x7.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
<span class="ltx_text ltx_font_bold" id="S3.F5.2.1">A qualitative comparison example of generated textual description with and without external medical knowledge.</span>
MLLMs can standardize medical terminology in its expressions and refine its diagnosis based on disease progressions detailed in medical literature.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Construction Pipeline</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Given a medical image, we aim to generate corresponding multigranular visual and texual annotations by leveraging MLLMs.
Specifically, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F2" title="In Textual Descriptions. ‣ 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, our pipeline can be decomposed into two stages - <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Data Processing</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.2">Generation of Multigranular Text Description</span>.
In the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.3">Data Processing</span> stage (<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS1" title="3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.1</span></a>), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps:
1) <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.4">Metadata Integration</span> to produce coarse captions encapsulating fundamental image information such as modality and disease types;
2) <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.5">ROI Locating</span> to identify regions of abnormalities;
and 3) <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.6">Medical Knowledge Retrieval</span> to extract relevant fine-grained medical details.
Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.SS2.SSS2" title="3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Data Processing</h4>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Coarse Caption Generation via Metadata Integration.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">We aim to generate coarse captions that provide fundamental information for a given image, including modality, organ labels, disease types, and optionally, camera views and equipment information.
Instead of extracting features directly from the images, we generate these captions by integrating dataset metadata. We first extract metadata from the datasets and then apply a fixed rule to integrate this information into coarse captions.
For example, for an image from the QaTa-COV19 dataset<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.kaggle.com/aysendegerli/qatacov19-dataset." title="">https://www.kaggle.com/aysendegerli/qatacov19-dataset.</a></span></span></span>, we derive metadata from the dataset’s accompanying paper or documentation, indicating that it consists of COVID-19 chest X-ray images.
Next, we construct coarse captions like “A chest X-ray image with COVID-19 in the lungs” highlighting the modality, organ types, and disease labels.
If the image contains additional textual information like radiological findings, this is also integrated to enhance the richness of the caption.
The effectiveness of adding coarse captions when generating fine-grained captions is illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F3" title="In Textual Descriptions. ‣ 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
In contrast to the scenario without a coarse caption where MLLMs fails to recognize the disease, providing MLLMs with a coarse caption that includes the disease type “COVID-19” enables it to identify and categorize the disease, thereby laying the foundation for further analysis.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">ROI Locating.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">We employ various strategies to locate Regions of Interest (ROIs) in images. For datasets that already include localization annotations, such as segmentation masks or bounding boxes, we derive the ROIs from these existing annotations. Specifically, bounding boxes are directly used as the ROIs, while segmentation masks are converted to ROIs by creating the smallest bounding box that covers the mask. When such localization annotations are not available, we apply different pretrained expert models listed in the Appendix to generate ROIs. For text-prompt driven grounding model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib29" title="">29</a>]</cite>, we use disease and organ information in coarse captions as text prompts to guide the model in segmenting specific parts.
Examples of generated ROIs from various modalities with different models are demonstrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F6" title="In ROI Locating. ‣ 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>. It is important to note that for modalities such as X-ray and MRI scans viewed from the z-axis, our ROI localization employs a coordinate system relative to the human body, resulting in a left-right reversal in the image representation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p2.1">Without ROIs, the original description is limited to a brief global analysis of the image. However, with ROIs, MLLMs  can perform a more detailed local analysis of the ROIs and assess the impact of lesion ROIs on the surrounding normal regions, as demonstrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F4" title="In Textual Descriptions. ‣ 3.1 Data Triplet ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S3.F6.sf1.g1" src="x8.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Example of locating ROI via SAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib29" title="">29</a>]</cite>.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S3.F6.sf2.g1" src="x9.png" width="829"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Example of locating ROI via BA-Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib30" title="">30</a>]</cite>.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F6.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S3.F6.sf3.g1" src="x10.png" width="829"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Example of locating ROI via Chexmask <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib31" title="">31</a>]</cite>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example of ROIs and their corresponding textual descriptions.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Medical Knowledge Retrieval.</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px3.p1.1">General-purpose MLLMs often produce content that lacks specialized medical terminology and professional expression.
To address this issue,
we build a medical knowledge database following the approach in MedRAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib32" title="">32</a>]</cite>. We collect three main corpora: PubMed<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubmed.ncbi.nlm.nih.gov/" title="">https://pubmed.ncbi.nlm.nih.gov/</a></span></span></span> for biomedical knowledge, StatPearls<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.statpearls.com/" title="">https://www.statpearls.com/</a></span></span></span> for clinical decision support, and medical textbooks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib33" title="">33</a>]</cite> for domain-specific knowledge.
We segment these corpora into short snippets and encode them into high-dimensional vectors using the text encoder from Med-CPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib34" title="">34</a>]</cite>. These vectors are then indexed into a specialized vector knowledge base using Faiss<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib35" title="">35</a>]</cite>, optimized for efficient retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px3.p2.1">For a given image, we retrieve relevant medical knowledge by using its coarse caption, which is generated through metadata integration. Specifically, we encode the coarse captions, including disease and organ classifications, into vectors using the Med-CPT text encoder. We then perform a vector similarity search in the medical vector database, retrieving the top eight medical knowledge snippets that semantically match the query. These snippets provide the external medical knowledge paired with the image.
A qualitative example demonstrating the effectiveness of incorporating external medical knowledge is shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F7" title="In Medical Knowledge Retrieval. ‣ 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>. With access to COVID-19-related medical knowledge, MLLMs can standardize medical terminology and refine diagnoses based on the disease progressions outlined in medical literature.</p>
</div>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="S3.F7.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span><span class="ltx_text ltx_font_bold" id="S3.F7.2.1">An example of the Top-8 retrieval results.</span> By leveraging COVID-19-related medical knowledge, MLLMs can standardize medical terminology and enhance diagnoses according to the disease progressions described in medical literature.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Generation of Multigranular Text Description</h4>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">After data processing, a comprehensive prompt is utilized to guide the MLLMs in generating multi-granular descriptions.
The prompt template consists of a three-level hierarchical framework with questions to instruct MLLMs:
(1) a global description that captures all details of the image
(2) a local-focused analysis of specific ROIs that potentially are unusual;
and (3) a local-global examination of the interaction between local and global attributes to understand the impact of local abnormalities on the entire organ.
Detailed prompt template is presented in supplementary materials.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">To ensure that the MLLMs are guided by relevant medical information not inherently present in their training data, we incorporate the processed data (coarse captions, ROIs, and retrieved medical knowledge) into the prompts.
Specifically, for global information, coarse captions are directly integrated into the prompt. For local information, ROIs on images are converted into textual descriptions based on their coordinates and area ratio within the images. Examples of these textual descriptions are shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F6" title="In ROI Locating. ‣ 3.2.1 Data Processing ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, using terms such as “left-center” and “area ratio: 1.2%”.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">To refine terminology and diagnosis within ROIs, relevant medical knowledge about specific diseases is incorporated into the prompt. Instead of merely inserting this knowledge, we instruct MLLMs to identify and align the relevant knowledge to ROIs that require analysis.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Choice of MLLMs </h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p1.1">We first prompt GPT-4V with the provided medical coarse captions, ROIs, and medical knowledge to generate a subset of 200,000 samples, maintaining a similar modality and organ distribution to our full 25 million dataset.
The goal of curating this subset is to calibrate a medical knowledge-guided MLLM to adhere to the formatting instructions specified for our text. Subsequently, we employ our model, LLaVA-Med Captioner, which is based on LLAVA-Med <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>]</cite>, the state-of-the-art medical MLLM. To further improve this model, we leverage the latest LLaMA3<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib36" title="">36</a>]</cite> to enhance its linguistic capabilities, and incorporate multi-scale feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib37" title="">37</a>]</cite> to improve its vision capabilities.
LLaVA-Med Captioner undergoes continuous training on medical multimodal data and is fine-tuned using our multigranular annotations, resulting in a specialized medical model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS2.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS2.Px1.p2.1">After fine-tuning, we then use this specialized model to generate the multigranular text descriptions on our entire dataset, resulting in 25 million image-ROI-description triplets. The fine-tuning process leverages the advanced language organization capabilities of GPT-4V, providing an effective template for fine-grained captions, which our model uses to learn the formatting of fine-grained captions. As a result, our model generates more detailed descriptions compared to GPT-4V, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F8" title="In Choice of MLLMs ‣ 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>. We also show a detailed quantitative comparison in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A2" title="Appendix B Quantitative Comparison of LLaVA-Med++ with GPT-4V ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a> in the supplementary material.</p>
</div>
<figure class="ltx_figure" id="S3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S3.F8.g1" src="x12.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
<span class="ltx_text ltx_font_bold" id="S3.F8.2.1">Qualitative Comparison with sample generated by GPT-4V.</span> Compared to GPT-4V, our model generate more detailed caption.
</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="618" id="S3.F9.sf1.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Modality distribution in MedTrinity-25M.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="498" id="S3.F9.sf2.g1" src="x14.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Anatomical and biological structures in MedTrinity-25M.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="562" id="S3.F9.sf3.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(c) </span>Data size comparison.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F9.sf4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="831" id="S3.F9.sf4.g1" src="x16.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(d) </span>Wordcloud of disease statistic.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Statistical overview of MedTrinity-25M.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Dataset Analysis</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Diversity</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">Our dataset encompasses a wide range of 10 imaging modalties, with more than 65 diseases across various anatomical structures in human. The distribution of Anatomical and biological structures in MedTrinity-25M is shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F9.sf2" title="In Figure 9 ‣ Choice of MLLMs ‣ 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9(b)</span></a>. Meanwhile, the number of samples in the dataset for each modality are shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F9.sf1" title="In Figure 9 ‣ Choice of MLLMs ‣ 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9(a)</span></a>, spanning from common ones with over 1 million samples each (CT, MRI, X-ray) to rare modalities(ultrasound, dermoscopy), demonstrating a much more balanced distribution compared to other large-scale dataset like SA-Med2D-20M<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib38" title="">38</a>]</cite>, which only contain thousands of ultrasound and dermoscopy samples.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Scale</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F9.sf3" title="In Figure 9 ‣ Choice of MLLMs ‣ 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9(c)</span></a> shows the amount of our dataset, which is significantly larger than previous datasets. To the best of our knowledge, this is the largest open-source, multi-modal multigranular medical dataset to date.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Diseases</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">The datasets involved in constructing MedTrinity-25M primarily focus on disease diagnosis and medical discovery.
In MedTrinity-25M, diseases are given in the free-form text. The same disease may be referred to using different terms, allowing for elaborate identification and analysis. <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F9.sf4" title="In Figure 9 ‣ Choice of MLLMs ‣ 3.2.2 Generation of Multigranular Text Description ‣ 3.2 Data Construction Pipeline ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9(d)</span></a> illustrates the frequently used words related to diseases in our dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Richness</h5>
<figure class="ltx_figure" id="S3.SS3.SSS0.Px4.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1" style="width:190.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1.1">
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;">Dataset</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;">Modality</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.1"></span> <span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2.1">
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;">Lesion</span></span>
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.2.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">Type</span></span>
</span></span><span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.3.3"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.1"></span> <span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2.1">
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;">Lesion</span></span>
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.2.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">BBox/Mask</span></span>
</span></span><span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.4.3"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.1"></span> <span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2">
<span class="ltx_tabular ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2.1">
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;">Color Texture</span></span>
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.2.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">Description</span></span>
</span></span><span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.5.3"></span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6" style="padding-left:0.0pt;padding-right:0.0pt;">
<span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.1"></span> <span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2">
<span class="ltx_tabular ltx_align_middle" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2.1">
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;">Region</span></span>
<span class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.2.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">Relationship</span></span>
</span></span><span class="ltx_text" id="S3.SS3.SSS0.Px4.1.fig1.1.1.6.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">MedMNIST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib39" title="">39</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.2" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.4" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.5" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S3.SS3.SSS0.Px4.1.fig1.1.2.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.1" style="padding-left:0.0pt;padding-right:0.0pt;">DeepLesion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.3" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.4" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.5" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.3.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.1" style="padding-left:0.0pt;padding-right:0.0pt;">BraTS 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib41" title="">41</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.3" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.4" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.5" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.4.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.1" style="padding-left:0.0pt;padding-right:0.0pt;">MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib21" title="">21</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.4" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.5" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.5.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.1" style="padding-left:0.0pt;padding-right:0.0pt;">Quilt-1M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.4" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.5" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.6.6" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.1" style="padding-left:0.0pt;padding-right:0.0pt;">VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib42" title="">42</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.4" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.5" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.7.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.1" style="padding-left:0.0pt;padding-right:0.0pt;">CRC100K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib43" title="">43</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.4" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.5" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.8.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.1" style="padding-left:0.0pt;padding-right:0.0pt;">SA-Med2D-20M <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib44" title="">44</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.4" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.5" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S3.SS3.SSS0.Px4.1.fig1.1.9.6" style="padding-left:0.0pt;padding-right:0.0pt;">✗</td>
</tr>
<tr class="ltx_tr" id="S3.SS3.SSS0.Px4.1.fig1.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px4.1.fig1.1.10.1.1">MedTrinity-25M(Ours)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.2" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.3" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.4" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.5" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.SS3.SSS0.Px4.1.fig1.1.10.6" style="padding-left:0.0pt;padding-right:0.0pt;">✓</td>
</tr>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of dataset types based on provided attributes of annotations.</figcaption>
</figure>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_middle" id="S3.F10" style="width:151.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="501" id="S3.SS3.SSS0.Px4.2.1.g1" src="x17.png" width="830"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><span class="ltx_text ltx_font_bold" id="S3.F10.2.1">Comparison of the average word count of text descriptions.</span> </figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px4.p1.1">We provide both quantitative analysis and qualitative examples to show the richness of our generated multigranular compare to other medical dataset. Qualitative examples are shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F1" title="In 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>, our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib21" title="">21</a>]</cite>, visual QA dataset SLAKE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite> and radiology objects caption dataset ROCO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib18" title="">18</a>]</cite>. To demonstrate the multi-granularity of our data, we compared the average word count of text descriptions in our dataset, MedTrinity-25M, with those in other medical datasets, as illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F10" title="In Richness ‣ 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>. The word count in our dataset is significantly higher, indicating greater richness.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Alignment with human</h5>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px5.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px5.p1.1">To evaluate the validity and quality of the generated multigranular annotations, we compared them with their original human annotations to assess the degree of alignment (for samples with human annotations).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px5.p2">
<p class="ltx_p" id="S3.SS3.SSS0.Px5.p2.1">Since the generated fine-grained captions  contains structured descriptions that may significantly differ from free-text radiology reports and question-answering pairs, we leveraged GPT-4V’s vision and language understanding capabilities.
Rather than focusing on the exact alignment of sentence structure or organization, GPT-4V assessed the alignment based on the accuracy of medical facts and diagnoses.
Specifically, the structure of the generated fine-grained captions  consists of five key attributes that characterize a medical image: modality, structure detection, ROI analysis, lesion texture, and local-global relation. To evaluate the generated data, we had GPT-4V perform a detailed comparison with human annotations based on these five attributes. Each attribute was scored on a scale from 0 to 2 points, with a maximum possible total score of 10 points.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of alignment scores between our generated fine-grained captionsand human annotations.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:341.7pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-5.9pt,4.7pt) scale(0.973357092641608,0.973357092641608) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.2">
<td class="ltx_td ltx_align_center" id="S3.T2.1.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
</tr>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S3.T2.st1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(a) </span>Alignment Scores on SLAKE</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.st1.1">
<tr class="ltx_tr" id="S3.T2.st1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.st1.1.1.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S3.T2.st1.1.1.1.1">Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S3.T2.st1.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">SLAKE</td>
</tr>
<tr class="ltx_tr" id="S3.T2.st1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Overall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st1.1.2.2.1"></span> <span class="ltx_text" id="S3.T2.st1.1.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st1.1.2.2.2.1">
<span class="ltx_tr" id="S3.T2.st1.1.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.2.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Modality</span></span>
</span></span><span class="ltx_text" id="S3.T2.st1.1.2.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st1.1.2.3.1"></span> <span class="ltx_text" id="S3.T2.st1.1.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st1.1.2.3.2.1">
<span class="ltx_tr" id="S3.T2.st1.1.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.3.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Structure</span></span>
<span class="ltx_tr" id="S3.T2.st1.1.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.3.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Detection</span></span>
</span></span><span class="ltx_text" id="S3.T2.st1.1.2.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st1.1.2.4.1"></span> <span class="ltx_text" id="S3.T2.st1.1.2.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st1.1.2.4.2.1">
<span class="ltx_tr" id="S3.T2.st1.1.2.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.4.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">ROI</span></span>
<span class="ltx_tr" id="S3.T2.st1.1.2.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.4.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Analysis</span></span>
</span></span><span class="ltx_text" id="S3.T2.st1.1.2.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st1.1.2.5.1"></span> <span class="ltx_text" id="S3.T2.st1.1.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st1.1.2.5.2.1">
<span class="ltx_tr" id="S3.T2.st1.1.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.5.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Lesion</span></span>
<span class="ltx_tr" id="S3.T2.st1.1.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.5.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Texture</span></span>
</span></span><span class="ltx_text" id="S3.T2.st1.1.2.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st1.1.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st1.1.2.6.1"></span> <span class="ltx_text" id="S3.T2.st1.1.2.6.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st1.1.2.6.2.1">
<span class="ltx_tr" id="S3.T2.st1.1.2.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.6.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Local-Global</span></span>
<span class="ltx_tr" id="S3.T2.st1.1.2.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st1.1.2.6.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Relation</span></span>
</span></span><span class="ltx_text" id="S3.T2.st1.1.2.6.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.st1.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">8.2/10.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">2.0/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">1.7/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">1.8/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">1.6/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st1.1.3.7" style="padding-left:4.3pt;padding-right:4.3pt;">1.1/2.0</td>
</tr>
</table>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel ltx_align_center" id="S3.T2.st2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">(b) </span>Alignment Scores on MIMIC-CXR</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.st2.1">
<tr class="ltx_tr" id="S3.T2.st2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.st2.1.1.1" rowspan="2" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_text" id="S3.T2.st2.1.1.1.1">Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6" id="S3.T2.st2.1.1.2" style="padding-left:4.3pt;padding-right:4.3pt;">MIMIC-CXR</td>
</tr>
<tr class="ltx_tr" id="S3.T2.st2.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Overall</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.2" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st2.1.2.2.1"></span> <span class="ltx_text" id="S3.T2.st2.1.2.2.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st2.1.2.2.2.1">
<span class="ltx_tr" id="S3.T2.st2.1.2.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.2.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Modality</span></span>
</span></span><span class="ltx_text" id="S3.T2.st2.1.2.2.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.3" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st2.1.2.3.1"></span> <span class="ltx_text" id="S3.T2.st2.1.2.3.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st2.1.2.3.2.1">
<span class="ltx_tr" id="S3.T2.st2.1.2.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.3.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Structure</span></span>
<span class="ltx_tr" id="S3.T2.st2.1.2.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.3.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Detection</span></span>
</span></span><span class="ltx_text" id="S3.T2.st2.1.2.3.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.4" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st2.1.2.4.1"></span> <span class="ltx_text" id="S3.T2.st2.1.2.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st2.1.2.4.2.1">
<span class="ltx_tr" id="S3.T2.st2.1.2.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.4.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">ROI</span></span>
<span class="ltx_tr" id="S3.T2.st2.1.2.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.4.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Analysis</span></span>
</span></span><span class="ltx_text" id="S3.T2.st2.1.2.4.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.5" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st2.1.2.5.1"></span> <span class="ltx_text" id="S3.T2.st2.1.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st2.1.2.5.2.1">
<span class="ltx_tr" id="S3.T2.st2.1.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.5.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Lesion</span></span>
<span class="ltx_tr" id="S3.T2.st2.1.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.5.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Texture</span></span>
</span></span><span class="ltx_text" id="S3.T2.st2.1.2.5.3"></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.st2.1.2.6" style="padding-left:4.3pt;padding-right:4.3pt;">
<span class="ltx_text" id="S3.T2.st2.1.2.6.1"></span> <span class="ltx_text" id="S3.T2.st2.1.2.6.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T2.st2.1.2.6.2.1">
<span class="ltx_tr" id="S3.T2.st2.1.2.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.6.2.1.1.1" style="padding-left:4.3pt;padding-right:4.3pt;">Local-Global</span></span>
<span class="ltx_tr" id="S3.T2.st2.1.2.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T2.st2.1.2.6.2.1.2.1" style="padding-left:4.3pt;padding-right:4.3pt;">Relation</span></span>
</span></span><span class="ltx_text" id="S3.T2.st2.1.2.6.3"></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.st2.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.1" style="padding-left:4.3pt;padding-right:4.3pt;">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.2" style="padding-left:4.3pt;padding-right:4.3pt;">8.9/10.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.3" style="padding-left:4.3pt;padding-right:4.3pt;">2.0/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.4" style="padding-left:4.3pt;padding-right:4.3pt;">1.9/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.5" style="padding-left:4.3pt;padding-right:4.3pt;">1.8/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.6" style="padding-left:4.3pt;padding-right:4.3pt;">1.6/2.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T2.st2.1.3.7" style="padding-left:4.3pt;padding-right:4.3pt;">1.6/2.0</td>
</tr>
</table>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S3.F11">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span><span class="ltx_text ltx_font_bold" id="S3.F11.2.1">An example of a perfect score result evaluated by GPT-4V.</span> GPT-4V assesses five criteria, each fully aligned with human annotations, resulting in perfect scores.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="591" id="S3.F11.g1" src="x18.png" width="830"/>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px5.p3">
<p class="ltx_p" id="S3.SS3.SSS0.Px5.p3.1">We conducted an alignment study on SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite> and MIMIC-CXR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib21" title="">21</a>]</cite>, randomly selecting 50 samples to compare with fine-grained captions for evaluating alignment scores against human annotations. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.T2.st2" title="In Table 2 ‣ Alignment with human ‣ 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2(b)</span></a>, the alignment scores were 8.2 and 8.9 for SLAKE and MIMIC-CXR, respectively. The criteria of modality, structure detection, and ROI analysis nearly achieved perfect scores, demonstrating the validity and accuracy of the generated data compared to human annotations. An example of perfect alignment score results evaluated by GPT-4V is shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S3.F11" title="In Alignment with human ‣ 3.3 Dataset Analysis ‣ 3 MedTrinity-25M Dataset ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>. In these examples, GPT-4V fully aligned with human annotations across all five criteria, resulting in perfect alignment scores.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS0.Px5.p4">
<p class="ltx_p" id="S3.SS3.SSS0.Px5.p4.1">The prompt used to query GPT-4V for evaluating the alignment score is shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A5.F14" title="In Appendix E Evaluation Prompt of Alignment to Human Annotations ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a> of supplementary.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>LLaVA-Med++: Experimental Training with MedTrinity-25M</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To further demonstrate the validity of our dataset, we compare the performance of LLaVA-Med++ with and without training on our dataset.
We select Visual Question Answering (VQA) as the evaluation task, which requires models to learn detailed visual and language representations. We assessed the performance of our model on three biomedical VQA datasets: VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib42" title="">42</a>]</cite>, SLAKE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite> and PathVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We initially pretrained LLaVA-Med++  using the methodology of LLaVA-Med <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>]</cite> as our baseline. Subsequently, for each VQA dataset evaluation, we further pretrained our model on the corresponding MedTrinity-25M  subset to achieve multigranular alignment. The model was then fine-tuned on VQA datasets for three epochs, with performance results presented in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S4.T3" title="In 4 LLaVA-Med++: Experimental Training with MedTrinity-25M ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a>. A comparative experiment was conducted without pretraining on MedTrinity-25M , maintaining all other settings. Results clearly demonstrate that LLaVA-Med++ achieves state-of-the-art performance in two of the three VQA benchmarks and ranks third in the remaining one.
Pretraining on MedTrinity-25M exhibits performance improvements of approximately 10.75% on VQA-RAD, 6.1% on SLAKE, and 13.25% on PathVQA compared to the model trained without pretraining on it. This enhancement underscores the efficacy of pretraining on MedTrinity-25M for downstream multimodal medical tasks, particularly in visual question answering.</p>
</div>
<figure class="ltx_table ltx_align_center" id="S4.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:209.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-62.8pt,30.3pt) scale(0.775396469622034,0.775396469622034) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td" id="S4.T3.1.1.1.1"></td>
<td class="ltx_td ltx_border_r" id="S4.T3.1.1.1.2"></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S4.T3.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1">VQA-RAD</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" colspan="3" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.4.1">SLAKE</span></td>
<td class="ltx_td ltx_align_center" colspan="3" id="S4.T3.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1">PathVQA</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2">
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.2.1">Method</td>
<td class="ltx_td ltx_border_r" id="S4.T3.1.1.2.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.2.3">Ref</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.2.4">Open</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.2.5">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.2.6">Ref</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.2.7">Open</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.2.8">Closed</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.2.9">Ref</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.2.10">Open</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.2.11">Closed</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="11" id="S4.T3.1.1.3.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.3.1.1">Supervised finet-tuning results with our own experiment runs</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.4.1">GPT-4V <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib2" title="">2</a>]</cite>
</td>
<td class="ltx_td" id="S4.T3.1.1.4.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.3">39.5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.4.4">78.9</td>
<td class="ltx_td" id="S4.T3.1.1.4.5"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.6">33.6</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.4.7">43.6</td>
<td class="ltx_td" id="S4.T3.1.1.4.8"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.9">-</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.4.10">-</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.5.1">LLaVA</td>
<td class="ltx_td" id="S4.T3.1.1.5.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.5.3">50.0</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.5.4">65.1</td>
<td class="ltx_td" id="S4.T3.1.1.5.5"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.5.6">78.2</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.5.7">63.2</td>
<td class="ltx_td" id="S4.T3.1.1.5.8"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.5.9">7.7</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.5.10">63.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.6.1">LLaVA-Med</td>
<td class="ltx_td" id="S4.T3.1.1.6.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.6.3">55.5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.6.4">66.5</td>
<td class="ltx_td" id="S4.T3.1.1.6.5"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.6.6">70.6</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.6.7">54.5</td>
<td class="ltx_td" id="S4.T3.1.1.6.8"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.6.9">35.9</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.6.10">89.2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.7.1">LLaVA-Med++<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.1.1">(Ours, w/o)</span>
</td>
<td class="ltx_td" id="S4.T3.1.1.7.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.7.3">64.6</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.7.4">77.0</td>
<td class="ltx_td" id="S4.T3.1.1.7.5"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.7.6">79.3</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.7.7">84.0</td>
<td class="ltx_td" id="S4.T3.1.1.7.8"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.7.9">55.0</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.7.10">94.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.8.1">LLaVA-Med++<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.1.1">(Ours, w/)</span>
</td>
<td class="ltx_td" id="S4.T3.1.1.8.2"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.8.3">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.3.1">77.1</span> (<span class="ltx_text" id="S4.T3.1.1.8.3.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.3.2.1" style="position:relative; bottom:1.0pt;">+</span> 12.5)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.8.4">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.4.1">86.0</span> (<span class="ltx_text" id="S4.T3.1.1.8.4.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.4.2.1" style="position:relative; bottom:1.0pt;">+</span> 9.0)</span>
</td>
<td class="ltx_td" id="S4.T3.1.1.8.5"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.8.6">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.6.1">86.2</span> (<span class="ltx_text" id="S4.T3.1.1.8.6.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.6.2.1" style="position:relative; bottom:1.0pt;">+</span> 6.9)</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.8.7">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.7.1">89.3</span> (<span class="ltx_text" id="S4.T3.1.1.8.7.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.7.2.1" style="position:relative; bottom:1.0pt;">+</span> 5.3)</span>
</td>
<td class="ltx_td" id="S4.T3.1.1.8.8"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.8.9">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.9.1">66.5</span> (<span class="ltx_text" id="S4.T3.1.1.8.9.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.9.2.1" style="position:relative; bottom:1.0pt;">+</span> 11.5)</span>
</td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.8.10">
<span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.10.1">99.0</span> (<span class="ltx_text" id="S4.T3.1.1.8.10.2" style="color:#009901;"> <span class="ltx_text" id="S4.T3.1.1.8.10.2.1" style="position:relative; bottom:1.0pt;">+</span> 5.0)</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9">
<td class="ltx_td ltx_align_left ltx_border_t" colspan="11" id="S4.T3.1.1.9.1"><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.9.1.1">Representative &amp; SoTA methods with numbers reported in the literature</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.10.1">VL Encoder–Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib46" title="">46</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.2">71.5</td>
<td class="ltx_td" id="S4.T3.1.1.10.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.10.4">82.5</td>
<td class="ltx_td" id="S4.T3.1.1.10.5"></td>
<td class="ltx_td" id="S4.T3.1.1.10.6"></td>
<td class="ltx_td ltx_border_r" id="S4.T3.1.1.10.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.10.8">71.5</td>
<td class="ltx_td" id="S4.T3.1.1.10.9"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.10.10">85.6</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.11.1">Q2ATransformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib47" title="">47</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.2">79.2</td>
<td class="ltx_td" id="S4.T3.1.1.11.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.11.4">81.2</td>
<td class="ltx_td" id="S4.T3.1.1.11.5"></td>
<td class="ltx_td" id="S4.T3.1.1.11.6"></td>
<td class="ltx_td ltx_border_r" id="S4.T3.1.1.11.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.11.8">54.9</td>
<td class="ltx_td" id="S4.T3.1.1.11.9"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.11.10">88.9</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.12">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.12.1">Prefix T. Medical LM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib48" title="">48</a>]</cite>
</td>
<td class="ltx_td" id="S4.T3.1.1.12.2"></td>
<td class="ltx_td" id="S4.T3.1.1.12.3"></td>
<td class="ltx_td ltx_border_r" id="S4.T3.1.1.12.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.5">84.3</td>
<td class="ltx_td" id="S4.T3.1.1.12.6"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.12.7">82.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.12.8">40.0</td>
<td class="ltx_td" id="S4.T3.1.1.12.9"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.12.10">87.0</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.13">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.13.1">PubMedCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib49" title="">49</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.13.2">60.1</td>
<td class="ltx_td" id="S4.T3.1.1.13.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.13.4">80.0</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.13.5">78.4</td>
<td class="ltx_td" id="S4.T3.1.1.13.6"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.13.7">82.5</td>
<td class="ltx_td" id="S4.T3.1.1.13.8"></td>
<td class="ltx_td" id="S4.T3.1.1.13.9"></td>
<td class="ltx_td" id="S4.T3.1.1.13.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.14">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.14.1">BiomedCLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib50" title="">50</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.14.2">67.6</td>
<td class="ltx_td" id="S4.T3.1.1.14.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.14.4">79.8</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.14.5">82.1</td>
<td class="ltx_td" id="S4.T3.1.1.14.6"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.14.7">89.7</td>
<td class="ltx_td" id="S4.T3.1.1.14.8"></td>
<td class="ltx_td" id="S4.T3.1.1.14.9"></td>
<td class="ltx_td" id="S4.T3.1.1.14.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.15">
<td class="ltx_td ltx_align_left ltx_border_r" colspan="2" id="S4.T3.1.1.15.1">M2I2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib51" title="">51</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.15.2">66.5</td>
<td class="ltx_td" id="S4.T3.1.1.15.3"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.15.4">83.5</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.15.5">74.7</td>
<td class="ltx_td" id="S4.T3.1.1.15.6"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.1.1.15.7">91.1</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.15.8">36.3</td>
<td class="ltx_td" id="S4.T3.1.1.15.9"></td>
<td class="ltx_td ltx_align_left" id="S4.T3.1.1.15.10">88.0</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span class="ltx_text ltx_font_bold" id="S4.T3.3.1">Comparison with Existing Supervised Methods.The notation w/ and w/o indicate models with and without pretraining on MedTrinity-25M, respectively.</span>
Employing multigranular alignment pretraining on MedTrinity-25M, LLaVA-Med++achieves state-of-the-art performance in two of the three VQA benchmarks and ranks third in the remaining one. Our model surpasses both multimodal large language models and other representative SoTA approaches.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This paper introduces MedTrinity-25M, a large-scale multimodal medical dataset comprising over 25 million image-ROI-description triplets sourced from more than 90 online resources, spanning 10 modalities and covering over 65 diseases. Unlike existing dataset construction methods that rely on image-text pairs, we have developed the first automated pipeline to scale up multimodal data by generating multigranular visual and textual annotations from unpaired image inputs, leveraging expert grounding models, retrieval-augmented generation techniques, and advanced MLLMs. MedTrinity-25M’s enriched annotations have the potential to support a wide range of multimodal tasks, such as captioning, report generation, classification, and segmentation, as well as facilitate the large-scale pre-training of multimodal medical AI models.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the Microsoft Accelerate Foundation Models Research Program, the OpenAI Researcher Access Program, TPU Research Cloud (TRC) program, Google Cloud Research Credits program, AWS Cloud Credit for Research program, and Lambda Cloud for supporting our computing needs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al.

</span>
<span class="ltx_bibblock">Towards generalist biomedical ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">NEJM AI</span>, 1(3):AIoa2300138, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Hong-Yu Zhou, Subathra Adithan, Julián Nicolás Acosta, Eric J Topol, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">A generalist learner for multifaceted medical image interpretation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2405.07988</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas, and Maria De La Iglesia-Vaya.

</span>
<span class="ltx_bibblock">Padchest: A large chest x-ray image dataset with multi-label annotated reports.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Medical image analysis</span>, 66:101797, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al.

</span>
<span class="ltx_bibblock">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Proceedings of the AAAI conference on artificial intelligence</span>, volume 33, pages 590–597, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:1901.07042</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro.

</span>
<span class="ltx_bibblock">Quilt-1m: One million image-text pairs for histopathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">Med-flamingo: a multimodal medical few-shot learner.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Machine Learning for Health (ML4H)</span>, pages 353–367. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al.

</span>
<span class="ltx_bibblock">Towards generalist biomedical ai.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">NEJM AI</span>, 1(3):AIoa2300138, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al.

</span>
<span class="ltx_bibblock">Openflamingo: An open-source framework for training large autoregressive vision-language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2308.01390</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.

</span>
<span class="ltx_bibblock">Palm-e: An embodied multimodal language model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">International Conference on Machine Learning</span>, pages 8469–8488. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al.

</span>
<span class="ltx_bibblock">Capabilities of gemini models in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2404.18416</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2312.11805</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2001.08361</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, and Christoph M Friedrich.

</span>
<span class="ltx_bibblock">Radiology objects in context (roco): a multimodal image dataset.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Intravascular Imaging and Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis: 7th Joint International Workshop, CVII-STENT 2018 and Third International Workshop, LABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3</span>, pages 180–189. Springer, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Radgenome-chest ct: A grounded vision-language dataset for chest ct analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2404.16754</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Pmc-clip: Contrastive language-image pre-training using biomedical documents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 525–536. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
AlistairEW Johnson, TomJ Pollard, SethJ Berkowitz, NathanielR Greenbaum, MatthewP Lungren, Chih-ying Deng, RogerG Mark, and Steven Horng.

</span>
<span class="ltx_bibblock">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Scientific data</span>, 6(1):317, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.

</span>
<span class="ltx_bibblock">Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)</span>, pages 1650–1654. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ke Yan, Xiaosong Wang, Le Lu, and Ronald M Summers.

</span>
<span class="ltx_bibblock">Deeplesion: Automated deep mining, categorization and detection of significant radiology image findings using large-scale clinical lesion annotations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1710.01766</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
axiong/pmc_oa datasets at hugging face.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/axiong/pmc_oa" title="">https://huggingface.co/datasets/axiong/pmc_oa</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Alexandros Karargyris, Renato Umeton, Micah J Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, et al.

</span>
<span class="ltx_bibblock">Federated benchmarking of medical artificial intelligence with medperf.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">Nature Machine Intelligence</span>, 5(7):799–810, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al.

</span>
<span class="ltx_bibblock">Skin lesion analysis toward melanoma detection: A challenge at the 2017 international symposium on biomedical imaging (isbi), hosted by the international skin imaging collaboration (isic).

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018)</span>, pages 168–172. IEEE, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Veronica Rotemberg, Nicholas Kurtansky, Brigid Betz-Stablein, Liam Caffery, Emmanouil Chousakos, Noel Codella, Marc Combalia, Stephen Dusza, Pascale Guitera, David Gutman, et al.

</span>
<span class="ltx_bibblock">A patient-centric dataset of images and metadata for identifying melanomas using clinical context.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Scientific data</span>, 8(1):34, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Pmc-vqa: Visual instruction tuning for medical visual question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2305.10415</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">One model to rule them all: Towards universal segmentation for medical images with text prompts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2312.17183</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jiacheng Wang, Lan Wei, Liansheng Wang, Qichao Zhou, Lei Zhu, and Jing Qin.

</span>
<span class="ltx_bibblock">Boundary-aware transformers for skin lesion segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24</span>, pages 206–216. Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Nicolas Gaggion, Lucas Mansilla, Candelaria Mosquera, Diego H. Milone, and Enzo Ferrante.

</span>
<span class="ltx_bibblock">Improving anatomical plausibility in medical image segmentation via hybrid graph neural networks: applications to chest x-ray analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">IEEE Transactions on Medical Imaging</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang.

</span>
<span class="ltx_bibblock">Benchmarking retrieval-augmented generation for medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2402.13178</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">Applied Sciences</span>, 11(14):6421, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John Wilbur, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Bioinformatics</span>, 39(11):btad651, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">IEEE Transactions on Big Data</span>, 7(3):535–547, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Meta LLaMA Team.

</span>
<span class="ltx_bibblock">Introducing meta llama 3: The most capable openly available llm to date.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ai.meta.com/blog/meta-llama-3/" title="">https://ai.meta.com/blog/meta-llama-3/</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell.

</span>
<span class="ltx_bibblock">When do we not need larger vision models?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2403.13043</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al.

</span>
<span class="ltx_bibblock">Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2311.11969</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni.

</span>
<span class="ltx_bibblock">Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">Scientific Data</span>, 10(1):41, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Ke Yan, Xiaosong Wang, Le Lu, and Ronald M Summers.

</span>
<span class="ltx_bibblock">Deeplesion: Automated deep mining, categorization and detection of significant radiology image findings using large-scale clinical lesion annotations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:1710.01766</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Maria Correia de Verdier, Rachit Saluja, Louis Gagnon, Dominic LaBella, Ujjwall Baid, Nourel Hoda Tahon, Martha Foltyn-Dumitru, Jikai Zhang, Maram Alafif, Saif Baig, et al.

</span>
<span class="ltx_bibblock">The 2024 brain tumor segmentation (brats) challenge: Glioma segmentation on post-treatment mri.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2405.18368</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions and answers about radiology images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Scientific data</span>, 5(1):1–10, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Jakob Nikolas Kather, Niels Halama, and Alexander Marx.

</span>
<span class="ltx_bibblock">100,000 histological images of human colorectal cancer and healthy tissue.

</span>
<span class="ltx_bibblock">https://doi.org/10.5281/zenodo.1214456.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al.

</span>
<span class="ltx_bibblock">Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2311.11969</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.

</span>
<span class="ltx_bibblock">Pathvqa: 30000+ questions for medical visual question answering.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">arXiv preprint arXiv:2003.10286</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Laila Bashmal, and Mansour Zuair.

</span>
<span class="ltx_bibblock">Vision–language model for visual question answering in medical imagery.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Bioengineering</span>, 10(3):380, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yunyi Liu, Zhanyu Wang, Dong Xu, and Luping Zhou.

</span>
<span class="ltx_bibblock">Q2atransformer: Improving medical vqa via an answer querying decoder.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">International Conference on Information Processing in Medical Imaging</span>, pages 445–456. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Tom Van Sonsbeek, Mohammad Mahdi Derakhshani, Ivona Najdenkoska, Cees GM Snoek, and Marcel Worring.

</span>
<span class="ltx_bibblock">Open-ended medical visual question answering through prefix tuning of language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">International Conference on Medical Image Computing and Computer-Assisted Intervention</span>, pages 726–736. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Sedigheh Eslami, Christoph Meinel, and Gerard De Melo.

</span>
<span class="ltx_bibblock">Pubmedclip: How much does clip benefit visual question answering in the medical domain?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">Findings of the Association for Computational Linguistics: EACL 2023</span>, pages 1181–1193, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al.

</span>
<span class="ltx_bibblock">Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2303.00915</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Pengfei Li, Gang Liu, Lin Tan, Jinying Liao, and Shenjun Zhong.

</span>
<span class="ltx_bibblock">Self-supervised vision-language pretraining for medial visual question answering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)</span>, pages 1–5. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford.

</span>
<span class="ltx_bibblock">Datasheets for datasets.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Communications of the ACM</span>, 64(12):86–92, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Eduardo Pontes Reis, Felipe Nascimento, Mateus Aranha, Fernando Mainetti Secol, Birajara Machado, Marcelo Felix, Anouk Stein, and Edson Amaro.

</span>
<span class="ltx_bibblock">Brain hemorrhage extended (bhx): Bounding box extrapolation from thick to thin slice ct images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">PhysioNet</span>, 101(23):e215–20, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Maria Correia de Verdier, Rachit Saluja, Louis Gagnon, Dominic LaBella, Ujjwall Baid, Nourel Hoda Tahon, Martha Foltyn-Dumitru, Jikai Zhang, Maram Alafif, Saif Baig, et al.

</span>
<span class="ltx_bibblock">The 2024 brain tumor segmentation (brats) challenge: Glioma segmentation on post-treatment mri.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2405.18368</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Alexandros Karargyris, Renato Umeton, Micah J Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, et al.

</span>
<span class="ltx_bibblock">Federated benchmarking of medical artificial intelligence with medperf.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Nature Machine Intelligence</span>, 5(7):799–810, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Andrew Janowczyk and Anant Madabhushi.

</span>
<span class="ltx_bibblock">Deep learning for digital pathology image analysis: A comprehensive tutorial with selected use cases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Journal of pathology informatics</span>, 7(1):29, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Angel Cruz-Roa, Ajay Basavanhally, Fabio González, Hannah Gilmore, Michael Feldman, Shridar Ganesan, Natalie Shih, John Tomaszewski, and Anant Madabhushi.

</span>
<span class="ltx_bibblock">Automatic detection of invasive ductal carcinoma in whole slide images with convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">Medical Imaging 2014: Digital Pathology</span>. SPIE, March 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Kexin Ding, Mu Zhou, He Wang, Olivier Gevaert, Dimitris Metaxas, and Shaoting Zhang.

</span>
<span class="ltx_bibblock">A large-scale synthetic pathological dataset for deep learning-enabled segmentation of breast cancer.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Scientific Data</span>, 10(1):231, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Jevgenij Gamper, Navid Alemi Koohbanani, Simon Graham, Mostafa Jahanifar, Ksenija Benet, Syed Ali Khurram, Ayesha Azam, Katherine Hewitt, and Nasir Rajpoot.

</span>
<span class="ltx_bibblock">Pannuke dataset extension, insights and baselines.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2003.10778</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Patrick Wagner, Maximilian Springenberg, Marius Kröger, Rose KC Moritz, Johannes Schleusener, Martina C Meinke, and Jackie Ma.

</span>
<span class="ltx_bibblock">Semantic modeling of cell damage prediction: a machine learning approach at human-level performance in dermatology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">Scientific Reports</span>, 13(1):8336, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, et al.

</span>
<span class="ltx_bibblock">A foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">arXiv preprint arXiv:2403.17834</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Jun Ma and Bo Wang.

</span>
<span class="ltx_bibblock">Miccai flare23: Fast, low-resource, and accurate organ and pan-cancer segmentation in abdomen ct, Apr 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Amir Akbarnejad, Nilanjan Ray, Penny J Barnes, and Gilbert Bigras.

</span>
<span class="ltx_bibblock">Predicting ki67, er, pr, and her2 statuses from h&amp;e-stained breast cancer images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2308.01982</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Pengfei Shao, Lijun Tang, Pu Li, Yi Xu, Chao Qin, Qiang Cao, Xiaobing Ju, Xiaoxin Meng, Qiang Lv, Jie Li, Wei Zhang, and Changjun Yin.

</span>
<span class="ltx_bibblock">Precise segmental renal artery clamping under the guidance of dual-source computed tomography angiography during laparoscopic partial nephrectomy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">Eur. Urol.</span>, 62(6):1001–1008, December 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Pengfei Shao, Chao Qin, Changjun Yin, Xiaoxin Meng, Xiaobing Ju, Jie Li, Qiang Lv, Wei Zhang, and Zhengquan Xu.

</span>
<span class="ltx_bibblock">Laparoscopic partial nephrectomy with segmental renal artery clamping: technique and clinical outcomes.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">Eur. Urol.</span>, 59(5):849–855, May 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Y He, G Yang, J Yang, Y Chen, Y Kong, J Wu, L Tang, X Zhu, JL Dillenseger, P Shao, S Zhang, H Shu, JL Coatrieux, and S Li.

</span>
<span class="ltx_bibblock">Dense biased networks with deep priori anatomy and hard region adaptation: Semisupervised learning for fine renal artery segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Medical Image Analysis</span>, 63, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Yuting He, Guanyu Yang, Jian Yang, Rongjun Ge, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Huazhong Shu, Jean-Louis Dillenseger, Jean-Louis Coatrieux, and Shuo Li.

</span>
<span class="ltx_bibblock">Meta grayscale adaptive network for 3D integrated renal structures segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">Med. Image Anal.</span>, 71:102055, July 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Llava-med: Training a large language-and-vision assistant for biomedicine in one day.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">Advances in Neural Information Processing Systems</span>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang, and Yizhou Yu.

</span>
<span class="ltx_bibblock">Sdr-former: A siamese dual-resolution transformer for liver lesion classification using 3d multi-phase imaging.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2402.17246</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Lidia Garrucho, Claire-Anne Reidel, Kaisar Kushibar, Smriti Joshi, Richard Osuala, Apostolia Tsirikoglou, Maciej Bobowicz, Javier del Riego, Alessandro Catanese, Katarzyna Gwoździewicz, et al.

</span>
<span class="ltx_bibblock">Mama-mia: A large-scale multi-center breast cancer dce-mri benchmark dataset with expert segmentations.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2406.13844</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.

</span>
<span class="ltx_bibblock">ChestX-Ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. IEEE, July 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.

</span>
<span class="ltx_bibblock">ChestX-Ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span>. IEEE, July 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.

</span>
<span class="ltx_bibblock">ChestX-ray: Hospital-scale chest x-ray database and benchmarks on weakly supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics</span>, Advances in computer vision and pattern recognition, pages 369–392. Springer International Publishing, Cham, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Masayuki Tsuneki and Fahdi Kanavati.

</span>
<span class="ltx_bibblock">Inference of captions from histopathological patches.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">International Conference on Medical Imaging with Deep Learning</span>, pages 1235–1250. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Masakata Kawai, Noriaki Ota, and Shinsuke Yamaoka.

</span>
<span class="ltx_bibblock">Large-scale pretraining on pathological images for fine-tuning of small pathological benchmarks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">Workshop on Medical Image Learning with Limited and Noisy Data</span>, pages 257–267. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Yuri Tolkach, Lisa Marie Wolgast, Alexander Damanakis, Alexey Pryalukhin, Simon Schallenberg, Wolfgang Hulla, Marie-Lisa Eich, Wolfgang Schroeder, Anirban Mukhopadhyay, Moritz Fuchs, et al.

</span>
<span class="ltx_bibblock">Artificial intelligence for tumour tissue detection and histological regression grading in oesophageal adenocarcinomas: a retrospective algorithm development and validation study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">The Lancet Digital Health</span>, 5(5):e265–e275, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions and answers about radiology images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">Scientific data</span>, 5(1):1–10, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Carsen Stringer and Marius Pachitariu.

</span>
<span class="ltx_bibblock">Cellpose3: one-click image restoration for improved cellular segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">bioRxiv</span>, pages 2024–02, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Proceedings of the IEEE/CVF international conference on computer vision</span>, pages 9650–9660, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
N. Gaggion, C. Mosquera, M. Aineseder, L. Mansilla, D. Milone, and E. Ferrante.

</span>
<span class="ltx_bibblock">CheXmask Database: a large-scale dataset of anatomical segmentation masks for chest x-ray images (version 0.1).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.13026/dx54-8351" title="">https://doi.org/10.13026/dx54-8351</a>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
</section>
<section class="ltx_appendix" id="Ax2">
<h2 class="ltx_title ltx_title_appendix">Supplementary material</h2>
<div class="ltx_para ltx_noindent" id="Ax2.p1">
<p class="ltx_p" id="Ax2.p1.1">We present the following items in the supplementary material section:</p>
<ol class="ltx_enumerate" id="Ax2.I1">
<li class="ltx_item" id="Ax2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="Ax2.I1.i1.p1">
<p class="ltx_p" id="Ax2.I1.i1.p1.1">Data source about MedTrinity-25M. (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A1" title="Appendix A Data Source ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">A</span></a>)</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="Ax2.I1.i2.p1">
<p class="ltx_p" id="Ax2.I1.i2.p1.1">Quantitative comparison between GPT-4V and LLaVA-Med Captioner (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A2" title="Appendix B Quantitative Comparison of LLaVA-Med++ with GPT-4V ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="Ax2.I1.i3.p1">
<p class="ltx_p" id="Ax2.I1.i3.p1.1">Example of ROI for normal regions (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A3" title="Appendix C Examples of ROIs for Normal Regions ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="Ax2.I1.i4.p1">
<p class="ltx_p" id="Ax2.I1.i4.p1.1">The list of expert ROI models (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A4" title="Appendix D List of Expert models to locate ROIs ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">D</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="Ax2.I1.i5.p1">
<p class="ltx_p" id="Ax2.I1.i5.p1.1">Prompt for evaluating MedTrinity-25M alignment with human annotations (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A5" title="Appendix E Evaluation Prompt of Alignment to Human Annotations ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">E</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para" id="Ax2.I1.i6.p1">
<p class="ltx_p" id="Ax2.I1.i6.p1.1">Prompt for generating MedTrinity-25M. (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A6" title="Appendix F Prompt Template for Generation of Multigranular Text Description ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">F</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="Ax2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para ltx_noindent" id="Ax2.I1.i7.p1">
<p class="ltx_p" id="Ax2.I1.i7.p1.1">A Datasheet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib52" title="">52</a>]</cite> for MedTrinity-25M (Section <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A7" title="Appendix G Datasheet for MedTrinity-25M ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">G</span></a>).</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Data Source</h2>
<figure class="ltx_table" id="A1.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Data sources for MedTrinity-25M from various medical image datasets, detailing their modalities, biological structures, quantities, and annotations.</figcaption>
<table class="ltx_tabular" id="A1.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T4.1.1.1">
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.1.1">Dataset Name</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.2" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.2.1">Modality</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1.1.3.1">
<tr class="ltx_tr" id="A1.T4.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.1.1.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1.1.1.1">Biological</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.1.1.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.3.1.2.1.1">Structures</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T4.1.1.1.4" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.4.1">Quantity</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T4.1.1.1.5" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.5.1">Text</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T4.1.1.1.6" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.1.1.6.1">
<tr class="ltx_tr" id="A1.T4.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.1.1.6.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.6.1.1.1.1">Disease</span></td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.1.1.6.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.6.1.2.1.1">Type</span></td>
</tr>
</table>
</th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T4.1.1.1.7" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.7.1">BBox</span></th>
<th class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T4.1.1.1.8" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.1.1.8.1">Mask</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T4.1.2.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="A1.T4.1.2.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">BHX<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib53" title="">53</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="A1.T4.1.2.1.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="A1.T4.1.2.1.3" style="padding-left:0.5pt;padding-right:0.5pt;">brain</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" id="A1.T4.1.2.1.4" style="padding-left:0.5pt;padding-right:0.5pt;">973908</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="A1.T4.1.2.1.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="A1.T4.1.2.1.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" id="A1.T4.1.2.1.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T4.1.2.1.8" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.3.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.3.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">BRATS24-MICCAI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib54" title="">54</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.3.2.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.3.2.3" style="padding-left:0.5pt;padding-right:0.5pt;">brain</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.3.2.4" style="padding-left:0.5pt;padding-right:0.5pt;">2535132</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.3.2.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.3.2.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.3.2.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.3.2.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.4.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.4.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">BRATS-ISBI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib55" title="">55</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.4.3.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.4.3.3" style="padding-left:0.5pt;padding-right:0.5pt;">brain</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.4.3.4" style="padding-left:0.5pt;padding-right:0.5pt;">987340</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.4.3.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.4.3.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.4.3.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.4.3.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.5.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.5.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">breast histopathology<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib57" title="">57</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.5.4.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.5.4.3" style="padding-left:0.5pt;padding-right:0.5pt;">breast</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.5.4.4" style="padding-left:0.5pt;padding-right:0.5pt;">547403</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.5.4.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.5.4.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.5.4.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.5.4.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.6.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.6.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">BreastCancer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib58" title="">58</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.6.5.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.6.5.3" style="padding-left:0.5pt;padding-right:0.5pt;">breast</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.6.5.4" style="padding-left:0.5pt;padding-right:0.5pt;">1824</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.6.5.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.6.5.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.6.5.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.6.5.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.7.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.7.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">CheXpert<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib7" title="">7</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.7.6.2" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.7.6.3" style="padding-left:0.5pt;padding-right:0.5pt;">lung</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.7.6.4" style="padding-left:0.5pt;padding-right:0.5pt;">183242</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.7.6.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.7.6.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.7.6.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.7.6.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.8.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">CISC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib59" title="">59</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.8.7.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.8.7.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.8.7.3.1">
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">Adrenal,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">Bile duct,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">Bladder,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">Breast,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Colon,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">Cervix,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">Esophagus</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">Kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.8.7.3.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.8.7.3.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">Liver,etc</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.8.7.4" style="padding-left:0.5pt;padding-right:0.5pt;">16285</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.8.7.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.8.7.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.8.7.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.8.7.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.9.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.9.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">CPD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib60" title="">60</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.9.8.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.9.8.3" style="padding-left:0.5pt;padding-right:0.5pt;">skin</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.9.8.4" style="padding-left:0.5pt;padding-right:0.5pt;">204</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.9.8.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.9.8.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.9.8.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.9.8.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.10.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT-RATE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib61" title="">61</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.10.9.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.10.9.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.10.9.3.1">
<tr class="ltx_tr" id="A1.T4.1.10.9.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.10.9.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.10.9.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.10.9.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">mediastinum,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.10.9.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.10.9.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.10.9.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">heart, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.10.9.4" style="padding-left:0.5pt;padding-right:0.5pt;">3869640</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.10.9.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.10.9.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.10.9.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.10.9.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.11.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">DeepLesion<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib40" title="">40</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.11.10.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.11.10.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.11.10.3.1">
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">bone,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">abdomen,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">mediastinum,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">soft tissue,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.11.10.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.11.10.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">pelvis</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.11.10.4" style="padding-left:0.5pt;padding-right:0.5pt;">2889672</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.11.10.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.11.10.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.11.10.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.11.10.8" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.12.11.1" style="padding-left:0.5pt;padding-right:0.5pt;">FLARE23<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib62" title="">62</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.12.11.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.12.11.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.12.11.3.1">
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">Liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">spleen,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">pancreas,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Aorta,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">adrenal gland,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">Gallbladder,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">esophagus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">stomach,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.12.11.3.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.12.11.3.1.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">duodenum,etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.12.11.4" style="padding-left:0.5pt;padding-right:0.5pt;">13770</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.12.11.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.12.11.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.12.11.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.12.11.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.13.12">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.13.12.1" style="padding-left:0.5pt;padding-right:0.5pt;">ihc4bc<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib63" title="">63</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.13.12.2" style="padding-left:0.5pt;padding-right:0.5pt;">Microscopy</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.13.12.3" style="padding-left:0.5pt;padding-right:0.5pt;">cell</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.13.12.4" style="padding-left:0.5pt;padding-right:0.5pt;">102535</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.13.12.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.13.12.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.13.12.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.13.12.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.14.13">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.14.13.1" style="padding-left:0.5pt;padding-right:0.5pt;">KIPA22<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib67" title="">67</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.14.13.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.14.13.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.14.13.3.1">
<tr class="ltx_tr" id="A1.T4.1.14.13.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.14.13.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.14.13.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.14.13.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">cervix</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.14.13.4" style="padding-left:0.5pt;padding-right:0.5pt;">26878</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.14.13.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.14.13.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.14.13.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.14.13.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.15.14.1" style="padding-left:0.5pt;padding-right:0.5pt;">LLaVA-Med<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib68" title="">68</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.15.14.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.15.14.2.1">
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">MR,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">Endoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Ultrasound,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">Dermoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">Microscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">Fundus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.2.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.2.1.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">PET</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.15.14.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.15.14.3.1">
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">cell, rib,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">tissue,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">face,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">vascular,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">bone,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.15.14.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.15.14.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">lymph, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.15.14.4" style="padding-left:0.5pt;padding-right:0.5pt;">22550</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.15.14.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.15.14.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.15.14.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.15.14.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.16.15">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.16.15.1" style="padding-left:0.5pt;padding-right:0.5pt;">LLD-MMRI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib69" title="">69</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.16.15.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.16.15.3" style="padding-left:0.5pt;padding-right:0.5pt;">liver</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.16.15.4" style="padding-left:0.5pt;padding-right:0.5pt;">21523</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.16.15.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.16.15.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.16.15.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.16.15.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.17.16">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.17.16.1" style="padding-left:0.5pt;padding-right:0.5pt;">MAMA-MIA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib70" title="">70</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.17.16.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.17.16.3" style="padding-left:0.5pt;padding-right:0.5pt;">breast</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.17.16.4" style="padding-left:0.5pt;padding-right:0.5pt;">316113</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.17.16.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.17.16.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.17.16.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.17.16.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.18.17">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.18.17.1" style="padding-left:0.5pt;padding-right:0.5pt;">MIMIC-CXR-JPG<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib8" title="">8</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.18.17.2" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.18.17.3" style="padding-left:0.5pt;padding-right:0.5pt;">lung</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.18.17.4" style="padding-left:0.5pt;padding-right:0.5pt;">240506</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.18.17.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.18.17.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.18.17.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.18.17.8" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.19.18">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.19.18.1" style="padding-left:0.5pt;padding-right:0.5pt;">NCT-CRC-HE-100K<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib43" title="">43</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.19.18.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.19.18.3" style="padding-left:0.5pt;padding-right:0.5pt;">colon</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.19.18.4" style="padding-left:0.5pt;padding-right:0.5pt;">100361</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.19.18.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.19.18.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.19.18.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.19.18.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.20.19">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.20.19.1" style="padding-left:0.5pt;padding-right:0.5pt;">NIH-CXR<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib73" title="">73</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.20.19.2" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.20.19.3" style="padding-left:0.5pt;padding-right:0.5pt;">lung</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.20.19.4" style="padding-left:0.5pt;padding-right:0.5pt;">986</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.20.19.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.20.19.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.20.19.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.20.19.8" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.21.20">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.21.20.1" style="padding-left:0.5pt;padding-right:0.5pt;">PadChest<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib6" title="">6</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.21.20.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.21.20.3" style="padding-left:0.5pt;padding-right:0.5pt;">lung</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.21.20.4" style="padding-left:0.5pt;padding-right:0.5pt;">96284</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.21.20.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.21.20.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.21.20.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.21.20.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.22.21">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.22.21.1" style="padding-left:0.5pt;padding-right:0.5pt;">PatchGastricADC22<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib74" title="">74</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.22.21.2" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.22.21.3" style="padding-left:0.5pt;padding-right:0.5pt;">brain</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.22.21.4" style="padding-left:0.5pt;padding-right:0.5pt;">98399</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.22.21.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.22.21.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.22.21.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.22.21.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.23.22.1" style="padding-left:0.5pt;padding-right:0.5pt;">Path-VQA training<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib45" title="">45</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.23.22.2" style="padding-left:0.5pt;padding-right:0.5pt;">Pathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.23.22.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.23.22.3.1">
<tr class="ltx_tr" id="A1.T4.1.23.22.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.23.22.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">gastrointestinal,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.23.22.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">colon,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.23.22.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">appendix,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.23.22.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.23.22.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">pinworm,etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.23.22.4" style="padding-left:0.5pt;padding-right:0.5pt;">13375</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.23.22.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.23.22.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.23.22.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.23.22.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.24.23.1" style="padding-left:0.5pt;padding-right:0.5pt;">PMC-OA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.24.23.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.24.23.2.1">
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">MR,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">Endoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Ultrasound,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">Dermoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">Microscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">Fundus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.2.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.2.1.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">PET</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.24.23.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.24.23.3.1">
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">cell,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">tissue,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">vascular,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">bone,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">lymph,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">eye,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.24.23.3.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.24.23.3.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">epithelium, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.24.23.4" style="padding-left:0.5pt;padding-right:0.5pt;">856999</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.24.23.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.24.23.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.24.23.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.24.23.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.25.24.1" style="padding-left:0.5pt;padding-right:0.5pt;">PMC-VQA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib28" title="">28</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.25.24.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.25.24.2.1">
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">MR,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">Endoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Ultrasound,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">Dermoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">Microscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">Fundus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.2.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.2.1.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">PET</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.25.24.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.25.24.3.1">
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">cell,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">tissue,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">artery,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">bone,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">face,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">rib,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">vascular,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.9">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.9.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.25.24.3.1.10">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.25.24.3.1.10.1" style="padding-left:0.5pt;padding-right:0.5pt;">eye, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.25.24.4" style="padding-left:0.5pt;padding-right:0.5pt;">144999</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.25.24.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.25.24.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.25.24.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.25.24.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.26.25.1" style="padding-left:0.5pt;padding-right:0.5pt;">PTCGA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib75" title="">75</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.26.25.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.26.25.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.26.25.3.1">
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">breast,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">uterine corpus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.26.25.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.26.25.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">thyroid</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.26.25.4" style="padding-left:0.5pt;padding-right:0.5pt;">3293965</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.26.25.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.26.25.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.26.25.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.26.25.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.27.26.1" style="padding-left:0.5pt;padding-right:0.5pt;">Quilt-1M<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.27.26.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.27.26.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.27.26.3.1">
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">skin,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">soft tissue,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">blood,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.27.26.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.27.26.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">bone, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.27.26.4" style="padding-left:0.5pt;padding-right:0.5pt;">643819</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.27.26.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.27.26.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.27.26.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.27.26.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.28.27.1" style="padding-left:0.5pt;padding-right:0.5pt;">SAMMed-20M<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib44" title="">44</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.28.27.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.28.27.2.1">
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">PET,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">MR,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">Endoscopy,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.2.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.2.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">dermoscopy</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.28.27.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.28.27.3.1">
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">pancreas,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">pulmonary,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">hepatic,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.28.27.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.28.27.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">skin, etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.28.27.4" style="padding-left:0.5pt;padding-right:0.5pt;">5491274</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.28.27.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.28.27.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.28.27.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.28.27.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.29.28.1" style="padding-left:0.5pt;padding-right:0.5pt;">SLAKE training<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.29.28.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.29.28.2.1">
<tr class="ltx_tr" id="A1.T4.1.29.28.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">CT,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">MRI,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.2.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.2.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.29.28.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.29.28.3.1">
<tr class="ltx_tr" id="A1.T4.1.29.28.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">liver,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">pelvic,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.29.28.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.29.28.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.29.28.4" style="padding-left:0.5pt;padding-right:0.5pt;">646</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.29.28.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.29.28.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.29.28.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.29.28.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.30.29">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.30.29.1" style="padding-left:0.5pt;padding-right:0.5pt;">TCGA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib75" title="">75</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.30.29.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.30.29.3" style="padding-left:0.5pt;padding-right:0.5pt;">tissue</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.30.29.4" style="padding-left:0.5pt;padding-right:0.5pt;">1142221</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.30.29.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.30.29.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.30.29.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.30.29.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.31.30.1" style="padding-left:0.5pt;padding-right:0.5pt;">ULS23</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.31.30.2" style="padding-left:0.5pt;padding-right:0.5pt;">CT</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.31.30.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.31.30.3.1">
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">lymph nodes,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">bladder,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.4.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.5.1" style="padding-left:0.5pt;padding-right:0.5pt;">colon,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.6.1" style="padding-left:0.5pt;padding-right:0.5pt;">kidney,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.7">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.7.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.31.30.3.1.8">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.31.30.3.1.8.1" style="padding-left:0.5pt;padding-right:0.5pt;">pancreas.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.31.30.4" style="padding-left:0.5pt;padding-right:0.5pt;">105669</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.31.30.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.31.30.6" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.31.30.7" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.31.30.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.32.31">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.32.31.1" style="padding-left:0.5pt;padding-right:0.5pt;">VALSET<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib76" title="">76</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.32.31.2" style="padding-left:0.5pt;padding-right:0.5pt;">Histopathology</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.32.31.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.32.31.3.1">
<tr class="ltx_tr" id="A1.T4.1.32.31.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.32.31.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">oesophagus,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.32.31.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.32.31.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">stomach</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.32.31.4" style="padding-left:0.5pt;padding-right:0.5pt;">277565</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.32.31.5" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.32.31.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.32.31.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.32.31.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.33.32">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.33.32.1" style="padding-left:0.5pt;padding-right:0.5pt;">VQA-RAD training<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib77" title="">77</a>]</cite>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.33.32.2" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.33.32.2.1">
<tr class="ltx_tr" id="A1.T4.1.33.32.2.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.33.32.2.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">X-Ray,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.33.32.2.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.33.32.2.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">MRI</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T4.1.33.32.3" style="padding-left:0.5pt;padding-right:0.5pt;">
<table class="ltx_tabular ltx_align_middle" id="A1.T4.1.33.32.3.1">
<tr class="ltx_tr" id="A1.T4.1.33.32.3.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.33.32.3.1.1.1" style="padding-left:0.5pt;padding-right:0.5pt;">brain,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.33.32.3.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.33.32.3.1.2.1" style="padding-left:0.5pt;padding-right:0.5pt;">lung,</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.33.32.3.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T4.1.33.32.3.1.3.1" style="padding-left:0.5pt;padding-right:0.5pt;">abdomen,etc.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" id="A1.T4.1.33.32.4" style="padding-left:0.5pt;padding-right:0.5pt;">1758</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.33.32.5" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.33.32.6" style="padding-left:0.5pt;padding-right:0.5pt;">✓</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="A1.T4.1.33.32.7" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T4.1.33.32.8" style="padding-left:0.5pt;padding-right:0.5pt;">✗</td>
</tr>
<tr class="ltx_tr" id="A1.T4.1.34.33">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_b ltx_border_tt" id="A1.T4.1.34.33.1" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.34.33.1.1">Total</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_tt" id="A1.T4.1.34.33.2" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_tt" id="A1.T4.1.34.33.3" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_b ltx_border_r ltx_border_tt" id="A1.T4.1.34.33.4" style="padding-left:0.5pt;padding-right:0.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T4.1.34.33.4.1">25016845</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_r ltx_border_tt" id="A1.T4.1.34.33.5" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_r ltx_border_tt" id="A1.T4.1.34.33.6" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_r ltx_border_tt" id="A1.T4.1.34.33.7" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_bb ltx_border_b ltx_border_tt" id="A1.T4.1.34.33.8" style="padding-left:0.5pt;padding-right:0.5pt;"></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Quantitative Comparison of LLaVA-Med++ with GPT-4V</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">As detailed in Section 3.2.2 of the main paper, we developed an enhanced version of LLaVA-Med <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib9" title="">9</a>]</cite>, called LLaVA-Med++. This enhancement leverages the latest LLaMA3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib36" title="">36</a>]</cite> to boost linguistic capabilities and incorporates multi-scale feature extraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib37" title="">37</a>]</cite> to improve vision capabilities.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">To justify the selection of our specialized medical model, LLaVA-Med++, over GPT-4V for generating textual descriptions, we conducted a quantitative comparison of the outputs generated by both models. We assessed the level of detail by comparing the average word count of text descriptions generated for the same sample. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A2.F12" title="In Appendix B Quantitative Comparison of LLaVA-Med++ with GPT-4V ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>, LLaVA-Med++, after task-specific fine-tuning, outperformed GPT-4V by 3.6% in word count, indicating that the descriptions generated by LLaVA-Med++ are more detailed.
Based on these findings, we selected LLaVA-Med++  to generate fine-grained captions  for our entire MedTrinity-25M.</p>
</div>
<figure class="ltx_figure" id="A2.F12">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Qualitative comparison of the relative average word count of samples generated by LLaVA-Med++ and GPT-4V.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="63" id="A2.F12.g1" src="extracted/5775656/figures_file/length_gpt.png" width="419"/>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Examples of ROIs for Normal Regions</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">As detailed in Section 3.1 of the main paper, the regions of interest (ROIs) identified using expert grounding models predominantly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few instances where no abnormalities are present, the ROIs typically highlight the primary object or organ in the image. Examples of ROIs without abnormalities are shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A3.F13.sf2" title="Figure 13(b) ‣ Figure 13 ‣ Appendix C Examples of ROIs for Normal Regions ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">13(b)</span></a>.</p>
</div>
<figure class="ltx_figure" id="A3.F13">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Examples of ROIs for normal regions.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F13.sf1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>A no infection sample from MIMIC-CXR. The ROIs highlight the left and right lungs.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="359" id="A3.F13.sf1.g1" src="extracted/5775656/figures_file/normal_roi.png" width="359"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A3.F13.sf2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>A healthy sample from SLAKE. The ROI points out the liver.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="359" id="A3.F13.sf2.g1" src="extracted/5775656/figures_file/normal_roi_2.png" width="359"/>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>List of Expert models to locate ROIs</h2>
<figure class="ltx_table" id="A4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>List of expert models used to generate ROIs for different datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.T5.1">
<tr class="ltx_tr" id="A4.T5.1.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A4.T5.1.1.1" style="padding-left:12.0pt;padding-right:12.0pt;">ID</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A4.T5.1.1.2" style="padding-left:12.0pt;padding-right:12.0pt;">Dataset Name</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A4.T5.1.1.3" style="padding-left:12.0pt;padding-right:12.0pt;">Model</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T5.1.2.1" style="padding-left:12.0pt;padding-right:12.0pt;">1</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T5.1.2.2" style="padding-left:12.0pt;padding-right:12.0pt;">breast histopathology</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.1.2.3" rowspan="10" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text" id="A4.T5.1.2.3.1">HoverNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib78" title="">78</a>]</cite></span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.3.1" style="padding-left:12.0pt;padding-right:12.0pt;">2</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.3.2" style="padding-left:12.0pt;padding-right:12.0pt;">BreastCancer</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.4.1" style="padding-left:12.0pt;padding-right:12.0pt;">3</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.4.2" style="padding-left:12.0pt;padding-right:12.0pt;">CISC</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.5.1" style="padding-left:12.0pt;padding-right:12.0pt;">4</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.5.2" style="padding-left:12.0pt;padding-right:12.0pt;">CPD</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.6.1" style="padding-left:12.0pt;padding-right:12.0pt;">5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.6.2" style="padding-left:12.0pt;padding-right:12.0pt;">NCT-CRC-HE-100K</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.7.1" style="padding-left:12.0pt;padding-right:12.0pt;">6</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.7.2" style="padding-left:12.0pt;padding-right:12.0pt;">PTCGA</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.8.1" style="padding-left:12.0pt;padding-right:12.0pt;">7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.8.2" style="padding-left:12.0pt;padding-right:12.0pt;">TCGA</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.9.1" style="padding-left:12.0pt;padding-right:12.0pt;">8</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.9.2" style="padding-left:12.0pt;padding-right:12.0pt;">VALSET</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.10.1" style="padding-left:12.0pt;padding-right:12.0pt;">9</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.10.2" style="padding-left:12.0pt;padding-right:12.0pt;">ihc4bc</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.11.1" style="padding-left:12.0pt;padding-right:12.0pt;">10</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.11.2" style="padding-left:12.0pt;padding-right:12.0pt;">Quilt-1M</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T5.1.12.1" style="padding-left:12.0pt;padding-right:12.0pt;">11</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T5.1.12.2" style="padding-left:12.0pt;padding-right:12.0pt;">CT-RATE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.1.12.3" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text" id="A4.T5.1.12.3.1">SAT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib29" title="">29</a>]</cite></span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.13">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T5.1.13.1" style="padding-left:12.0pt;padding-right:12.0pt;">12</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T5.1.13.2" style="padding-left:12.0pt;padding-right:12.0pt;">PMC-OA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T5.1.13.3" rowspan="4" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text" id="A4.T5.1.13.3.1">DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib79" title="">79</a>]</cite></span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.14.1" style="padding-left:12.0pt;padding-right:12.0pt;">13</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.14.2" style="padding-left:12.0pt;padding-right:12.0pt;">PMC-VQA</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.15.1" style="padding-left:12.0pt;padding-right:12.0pt;">14</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.15.2" style="padding-left:12.0pt;padding-right:12.0pt;">LLaVA-Med</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.16.1" style="padding-left:12.0pt;padding-right:12.0pt;">15</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.16.2" style="padding-left:12.0pt;padding-right:12.0pt;">Path-VQA training</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.17">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T5.1.17.1" style="padding-left:12.0pt;padding-right:12.0pt;">16</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T5.1.17.2" style="padding-left:12.0pt;padding-right:12.0pt;">PadChest</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A4.T5.1.17.3" rowspan="3" style="padding-left:12.0pt;padding-right:12.0pt;"><span class="ltx_text" id="A4.T5.1.17.3.1">CheXmask <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib80" title="">80</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib31" title="">31</a>]</cite></span></td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T5.1.18.1" style="padding-left:12.0pt;padding-right:12.0pt;">17</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="A4.T5.1.18.2" style="padding-left:12.0pt;padding-right:12.0pt;">MIMIC-CXR-JPG</td>
</tr>
<tr class="ltx_tr" id="A4.T5.1.19">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A4.T5.1.19.1" style="padding-left:12.0pt;padding-right:12.0pt;">18</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A4.T5.1.19.2" style="padding-left:12.0pt;padding-right:12.0pt;">CheXpert</td>
</tr>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">As detailed in Section 3.2.1 of the main paper, for datasets lacking localization information such as segmentation masks and bounding boxes, we employ various pretrained expert models to identify the ROIs. The specific expert models used for each dataset are listed in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A4.T5" title="In Appendix D List of Expert models to locate ROIs ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Evaluation Prompt of Alignment to Human Annotations</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">The prompt used to query GPT-4V for evaluating the alignment score is shown in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A5.F14" title="In Appendix E Evaluation Prompt of Alignment to Human Annotations ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure class="ltx_figure" id="A5.F14">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Prompt used to evaluate the alignment of generated fine-grained captions.</figcaption><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1191" id="A5.F14.g1" src="x19.png" width="748"/>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Prompt Template for Generation of Multigranular Text Description</h2>
<figure class="ltx_figure" id="A6.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="1409" id="A6.1.g1" src="x20.png" width="747"/>
</figure>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">To generate multigranular textual descriptions, we design a multi-task prompting approach, breaking down this task into several smaller descriptive tasks. The model’s responses to these different tasks collectively form the final fine-grained text description.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p2">
<p class="ltx_p" id="A6.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#A6" title="Appendix F Prompt Template for Generation of Multigranular Text Description ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">appendix</span> <span class="ltx_text ltx_ref_tag">F</span></a> illustrates our prompt template consisting of a three-level hierarchical framework with questions to instruct MLLMs:</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p3">
<p class="ltx_p" id="A6.p3.1">Step 1 - Global Understanding: Instruct MLLMs to provide a comprehensive description of the image, detailing all modalities, identified anatomical structures, and their approximate locations. This step ensures that MLLMs gains an overarching understanding and basic information about the image.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p4">
<p class="ltx_p" id="A6.p4.1">Step 2 - Local Analysis: Instruct MLLMs to conduct a detailed analysis of the regions of interest (ROI), including their locations, abnormalities, and textures. This step guides MLLMs to focus on specific lesions for a thorough assessment.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p5">
<p class="ltx_p" id="A6.p5.1">Step 3 - Local-Global Relationship: Instruct MLLMs to examine the relationship between local and global regions and predict how the surrounding areas will be affected by the lesions in the ROI. This step aims to understand the interaction between local and global attributes, assessing the impact of local abnormalities on the entire organ for accurate disease diagnosis.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Datasheet for MedTrinity-25M</h2>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">In this section, we present a DataSheet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#bib.bib52" title="">52</a>]</cite> for MedTrinity-25M, synthesizing many of the other analyses we performed in this paper.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p2">
<ol class="ltx_enumerate" id="A7.I1">
<li class="ltx_item" id="A7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i1.p1.1">Motivation For Datasheet Creation</p>
<ul class="ltx_itemize" id="A7.I1.i1.I1">
<li class="ltx_item" id="A7.I1.i1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i1.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i1.I1.i1.p1.1.1">Why was the dataset created?</span>
The dataset was created to provide a large-scale, multimodal, multigranular medical dataset to support a wide range of multimodal tasks such as captioning, report generation, classification, and segmentation. It aims to facilitate large-scale pre-training of multimodal medical AI models by providing enriched annotations from unpaired image inputs.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i1.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i1.I1.i2.p1.1.1">Has the dataset been used already?</span>
Yes. Multigranular annotations enable a wide range of tasks like Medical Visual Question Answering, which we discuss in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S4" title="4 LLaVA-Med++: Experimental Training with MedTrinity-25M ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i1.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i1.I1.i3.p1.1.1">What (other) tasks could the dataset be used for?</span>
The MedTrinity-25M dataset could be used for multiple medical imaging tasks such as classification, segmentation, detection, and medical report generation. Its extensive and detailed annotations make it suitable for training and evaluating machine learning models across these tasks.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i1.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i1.I1.i4.p1.1.1">Who funded dataset creation?</span>
This work is partially supported by the OpenAI Researcher Access Program, AWS Cloud Credit for Research Program, TPU Research Cloud (TRC) program and Google Cloud Research Credits program.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="A7.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i2.p1.1">Data composition</p>
<ul class="ltx_itemize" id="A7.I1.i2.I1">
<li class="ltx_item" id="A7.I1.i2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i1.p1.1.1">What are the instances?</span>
Each instance in the dataset is a triplet consisting of an image, a Region of Interest (ROI), and a multigranular textual description. The ROI is associated with abnormalities and represented by bounding boxes or segmentation masks.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i2.p1.1.1">How many instances are there?</span>
The dataset comprises over 25 million image-ROI-description triplets sourced from more than 90 online resources, spanning 10 modalities and covering over 65 diseases.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i3.p1.1.1">What data does each instance consist of?</span>
Each instance consists of a medical image, a corresponding ROI (highlighting abnormalities within the image), and a detailed, multigranular textual description that includes disease/lesion type, modality, region-specific description, and inter-regional relationships.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i4.p1.1.1">Is there a label or target associated with each instance?</span>
Yes, the textual description serves as a detailed label or target, providing information about the disease or lesion type, as well as other relevant medical details.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i2.I1.i5.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i5.p1.1.1">Is any information missing from individual instances?</span>
No.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i6.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i6.p1.1.1">Are relationships between individual instances made explicit?</span>
Not applicable – we do not study relationships between disparate medical samples.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i7.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i7.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i7.p1.1.1">Does the dataset contain all possible instances or is it a sample?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i7.p2">
<p class="ltx_p" id="A7.I1.i2.I1.i7.p2.1">Our generation pipeline includes all instances collected from available medical data sources. However, the current list of medical dataset sources is not exhaustive, indicating a high probability of collecting additional instances in the future.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i8.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i8.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i8.p1.1.1">Are there recommended data splits (e.g., training, development/validation, testing)?</span>
There are no recommended data splits, as this data was curated mainly for pretraining rather than evaluation.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A7.I1.i2.I1.i9.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i9.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i9.p1.1.1">Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</span>
Yes. Despite multiple efforts to minimize errors using coarse captions and external medical knowledge, the textual descriptions generated by MLLMsmay still contain inaccuracies.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i2.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i2.I1.i10.p1">
<p class="ltx_p" id="A7.I1.i2.I1.i10.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i2.I1.i10.p1.1.1">Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?</span>
The dataset is largely self-contained. However, it was constructed using data from over 90 online resources such as TCIA, Kaggle, Zenodo, and Synapse. The images and related data were collected from these sources, but the dataset itself does not rely on external resources like websites or tweets for its primary functionality once compiled.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="A7.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i3.p1.1">Collection Process</p>
<ul class="ltx_itemize" id="A7.I1.i3.I1">
<li class="ltx_item" id="A7.I1.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.I1.i1.p1.1.1">What mechanisms or procedures were used to collect the data?</span>
The data collection involved an automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations from unpaired images. Data was collected from over 90 different sources, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.I1.i2.p1.1.1">How was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i2.p2">
<p class="ltx_p" id="A7.I1.i3.I1.i2.p2.1">The data associated with each instance was indirectly inferred and derived from the collected images using domain-specific expert models and multimodal large language models (MLLMs). The images were annotated with bounding boxes, segmentation masks, and textual descriptions, transforming them into image-ROI-description triplets.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.I1.i3.p1.1.1">If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)?</span>
The dataset is not a sample from a larger set but an extensive collection aggregated from multiple datasets and online sources. The strategy was to include as many diverse images and annotations as possible from a wide range of medical datasets.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.I1.i4.p1.1.1">Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?</span>
Data collection was primarily done by the co-authors of this paper.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i3.I1.i5.p1">
<p class="ltx_p" id="A7.I1.i3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i3.I1.i5.p1.1.1">Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in which the data associated with the instances was created.</span>
The data was collected from April 2024 to June 2024.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i4.p1.1">Data Preprocessing</p>
<ul class="ltx_itemize" id="A7.I1.i4.I1">
<li class="ltx_item" id="A7.I1.i4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i4.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i4.I1.i1.p1.1.1">Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)?</span>
Extensive preprocessing and annotation were performed, including segmentation, bounding box creation, and generating multigranular textual descriptions. The preprocessing also involved integrating metadata and knowledge retrieval from sources like PubMed to create comprehensive descriptions.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i4.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i4.I1.i2.p1.1.1">Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the ‘raw’ data.</span>
The raw data was saved, but at this time we do not plan to release it directly due to copyright and privacy concerns.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i4.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i4.I1.i3.p1.1.1">Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.</span>
The software for preprocessing and labeling, including the automated pipeline and MLLMs, is available at  <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yunfeixie233/DataProcessingSystem" title="">https://github.com/yunfeixie233/DataProcessingSystem</a>.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i4.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i4.I1.i4.p1.1.1">Does this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet? If not, what are the limitations?</span>
Yes. The preprocessing and collection procedures align with the motivation of creating a comprehensive, large-scale multimodal dataset to support the development of advanced medical AI models. The dataset’s multigranular annotations enable a wide range of tasks like Medical Visual Question Answering, which we discuss in <a class="ltx_ref" href="https://arxiv.org/html/2408.02900v1#S4" title="4 LLaVA-Med++: Experimental Training with MedTrinity-25M ‣ MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="A7.I1.i5.p1">
<p class="ltx_p" id="A7.I1.i5.p1.1">Dataset Distribution</p>
<ul class="ltx_itemize" id="A7.I1.i5.I1">
<li class="ltx_item" id="A7.I1.i5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i5.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i5.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i5.I1.i1.p1.1.1">How will the dataset be distributed?</span>
The dataset is publicly available and can be accessed via the provided link: MedTrinity-25M <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://yunfeixie233.github.io/MedTrinity-25M/" title="">https://yunfeixie233.github.io/MedTrinity-25M/</a>.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i5.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i5.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i5.I1.i2.p1.1.1">When will the dataset be released/first distributed? What license (if any) is it distributed under?</span>
We will release it as soon as possible, using a permissible license for research-based use.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i5.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i5.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i5.I1.i3.p1.1.1">Are there any copyrights on the data?</span>
We believe our use is ‘fair use,’ however, due to an abundance of caution, we will not be releasing any of the videos themselves.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i5.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i5.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i5.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i5.I1.i4.p1.1.1">Are there any fees or access restrictions?</span>
No.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i6.p1">
<p class="ltx_p" id="A7.I1.i6.p1.1">Dataset Maintenance</p>
<ul class="ltx_itemize" id="A7.I1.i6.I1">
<li class="ltx_item" id="A7.I1.i6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i6.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i6.I1.i1.p1.1.1">Who is supporting/hosting/maintaining the dataset?</span>
The first authors of this paper.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i6.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i6.I1.i2.p1.1.1">Will the dataset be updated? If so, how often and by whom?</span>
We do not plan to update it at this time.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i6.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i6.I1.i3.p1.1.1">Is there a repository to link to any/all papers/systems that use this dataset?</span>
Not right now, but we encourage anyone who uses the dataset to cite our paper so it can be easily found.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i6.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i6.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i6.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i6.I1.i4.p1.1.1">If others want to extend/augment/build on this dataset, is there a mechanism for them to do so?</span>
Not at this time.</p>
</div>
</li>
</ul>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">7.</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.p1">
<p class="ltx_p" id="A7.I1.i7.p1.1">Legal and Ethical Considerations</p>
<ul class="ltx_itemize" id="A7.I1.i7.I1">
<li class="ltx_item" id="A7.I1.i7.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i1.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i1.p1.1.1">Were any ethical review processes conducted (e.g., by an institutional review board)?</span>
No official processes were done, as our research is not on human subjects, however, because the dataset is in the medical domain we had significant internal discussions and deliberations when choosing the scraping strategy.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i2.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i2.p1.1.1">Does the dataset contain data that might be considered confidential?</span>
The dataset does not contain data that might be considered confidential, as it uses publicly available sources and anonymized medical data.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i3.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i3.p1.1.1">Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why?</span>
The dataset does not contain data that might be offensive, insulting, threatening, or anxiety-inducing. It consists of medical images and associated annotations for clinical and research use.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i4.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i4.p1.1.1">Does the dataset relate to people?</span>
The dataset relates to people as it involves medical images and data. However, it is anonymized and does not include identifiable information.</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i5.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i5.p1.1.1">Does the dataset identify any subpopulations (e.g., by age, gender)?</span>
Not explicitly (e.g. through labels)</p>
</div>
</li>
<li class="ltx_item" id="A7.I1.i7.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A7.I1.i7.I1.i6.p1">
<p class="ltx_p" id="A7.I1.i7.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="A7.I1.i7.I1.i6.p1.1.1">Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset?</span>
The dataset does not identify specific subpopulations directly in the provided description. Additionally, it is not possible to identify individuals from the dataset as it is anonymized and compiled from various sources.</p>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Aug  5 19:16:05 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
