<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2110.05607] Partial Variable Training for Efficient On-Device Federated Learning</title><meta property="og:description" content="This paper aims to address the major challenges of Federated Learning (FL) on edge devices: limited memory and expensive communication. We propose a novel method, called Partial Variable Training (PVT), that only trainâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Partial Variable Training for Efficient On-Device Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Partial Variable Training for Efficient On-Device Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2110.05607">

<!--Generated on Tue Mar 19 14:38:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Partial Variable Training for Efficient On-Device Federated Learning</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.2" class="ltx_p">This paper aims to address the major challenges of Federated Learning (FL) on edge devices: limited memory and expensive communication. We propose a novel method, called <em id="id2.2.1" class="ltx_emph ltx_font_italic">Partial Variable Training (PVT)</em>, that only trains a small subset of variables on edge devices to reduce memory usage and communication cost. With PVT, we show that network accuracy can be maintained by utilizing more local training steps and devices, which is favorable for FL involving a large population of devices. According to our experiments on two state-of-the-art neural networks for speech recognition and two different datasets, PVT can reduce memory usage by up to 1.9<math id="id1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation></semantics></math> and communication cost by up to 593<math id="id2.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation></semantics></math> while attaining comparable accuracy when compared with full network training.</p>
</div>
<div id="p1" class="ltx_para">
<svg id="p1.pic1" class="ltx_picture" height="52.27" overflow="visible" version="1.1" width="705.31"><g transform="translate(0,52.27) matrix(1 0 0 -1 0 0) translate(-72.35,0) translate(0,-14.11) matrix(1.0 0.0 0.0 1.0 76.96 35.41)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><foreignObject width="696.08" height="43.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="p1.pic1.1.1.1.1" class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" style="width:503.1pt;">
<span id="p1.pic1.1.1.1.1.1" class="ltx_p"><span id="p1.pic1.1.1.1.1.1.1" class="ltx_text" style="font-size:80%;">Â©2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span>
</span></foreignObject></g></svg>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p2.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰</span></span>
federated learning, speech recognition</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Neural Networks (NNs) have become an indispensable backbone of many artificial intelligence applications, such as automatic speech recognitionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, that enrich our daily life. How well neural networks can serve users largely depends on the quality of data available for training. Edge devices generate a large amount of data constantly. However, the private nature of such data prevents them from being uploaded to servers to improve NNs and, hence, user experience.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> is a promising solution to this dilemma by keeping data always on edge devices (referred to as <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">clients</em>). FL starts from broadcasting a server network to clients and trains it on clients with local data. The resultant network changes are then sent back to the server and aggregated to update the server network. One set of these steps comprises a <em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">federated round</em>, and multiple rounds are carried out until the server network converges.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Performing FL on edge devices is challenging. In addition to the challenges the general FL facesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, such as not independent and identically distributed (non-IID) data, on-device FL needs to address the efficiency problem. First, the memory is usually highly limited on edge devices. Edge devices can have as low as a few megabytes available for training. Second, communication is expensive. Communication can be orders of magnitude slower than local computationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we propose <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Partial Variable Training (PVT)</em> to address the above challenges of on-device FL. On a client, the standard All-Variable Training (AVT) trains all the variables (e.g., weights and biases) and returns all the changes to the server. In contrast, PVT trains only a subset of variables and freezes the remaining per federated round. Only the changes corresponding to the trained variables are returned. The following summarizes the benefits of PVT:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Reducing memory usage:</span> Because memory usage is dominated by buffered activations for backward passes, freezing some variables prevents buffering the corresponding activations, hence it reduces memory usage.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Reducing communication cost:</span> Frozen variables are unchanged during training and do not need to be updated, thus reduced communication cost.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">No need of modifying architectures:</span> PVT does not require inserting special layers or operations into networks and allows using the original architectures as is.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">No requirement of network-specific knowledge:</span> PVT does not assume any special properties of a network or its layers, which makes PVT applicable to various networks.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p"><span id="S1.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Suiting large-scale federated learning:</span> We observe that the loss in accuracy when we freeze a large number of variables can be compensated by increasing the numbers of local training steps and clients. This trade-off is favorable for large-scale FL, where memory usage and communication cost are the hard constraints while there are many available devices.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.10" class="ltx_p">Partial Variable Training (PVT) trains only a subset of variables and sends back the corresponding changes to a server. Fig.Â <a href="#S2.F1" title="Figure 1 â€£ 2 Methodology â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates one federated round of PVT. This simple example trains a two-layer network with two clients. On <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="Client\_1" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1a" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.p1.1.m1.1.1.4" xref="S2.p1.1.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1b" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.p1.1.m1.1.1.5" xref="S2.p1.1.m1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1c" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.p1.1.m1.1.1.6" xref="S2.p1.1.m1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1d" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi id="S2.p1.1.m1.1.1.7" xref="S2.p1.1.m1.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1e" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.1.m1.1.1.8" xref="S2.p1.1.m1.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.1f" xref="S2.p1.1.m1.1.1.1.cmml">â€‹</mo><mn id="S2.p1.1.m1.1.1.9" xref="S2.p1.1.m1.1.1.9.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><times id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></times><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ğ¶</ci><ci id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3">ğ‘™</ci><ci id="S2.p1.1.m1.1.1.4.cmml" xref="S2.p1.1.m1.1.1.4">ğ‘–</ci><ci id="S2.p1.1.m1.1.1.5.cmml" xref="S2.p1.1.m1.1.1.5">ğ‘’</ci><ci id="S2.p1.1.m1.1.1.6.cmml" xref="S2.p1.1.m1.1.1.6">ğ‘›</ci><ci id="S2.p1.1.m1.1.1.7.cmml" xref="S2.p1.1.m1.1.1.7">ğ‘¡</ci><ci id="S2.p1.1.m1.1.1.8.cmml" xref="S2.p1.1.m1.1.1.8">_</ci><cn type="integer" id="S2.p1.1.m1.1.1.9.cmml" xref="S2.p1.1.m1.1.1.9">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">Client\_1</annotation></semantics></math>, the variables in <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="Layer\_1" display="inline"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1a" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.p1.2.m2.1.1.4" xref="S2.p1.2.m2.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1b" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.p1.2.m2.1.1.5" xref="S2.p1.2.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1c" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi id="S2.p1.2.m2.1.1.6" xref="S2.p1.2.m2.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1d" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.2.m2.1.1.7" xref="S2.p1.2.m2.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.2.m2.1.1.1e" xref="S2.p1.2.m2.1.1.1.cmml">â€‹</mo><mn id="S2.p1.2.m2.1.1.8" xref="S2.p1.2.m2.1.1.8.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><times id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1"></times><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ğ¿</ci><ci id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3">ğ‘</ci><ci id="S2.p1.2.m2.1.1.4.cmml" xref="S2.p1.2.m2.1.1.4">ğ‘¦</ci><ci id="S2.p1.2.m2.1.1.5.cmml" xref="S2.p1.2.m2.1.1.5">ğ‘’</ci><ci id="S2.p1.2.m2.1.1.6.cmml" xref="S2.p1.2.m2.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.2.m2.1.1.7.cmml" xref="S2.p1.2.m2.1.1.7">_</ci><cn type="integer" id="S2.p1.2.m2.1.1.8.cmml" xref="S2.p1.2.m2.1.1.8">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">Layer\_1</annotation></semantics></math> are trained and those in <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="Layer\_2" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1a" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.1.1.4" xref="S2.p1.3.m3.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1b" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.1.1.5" xref="S2.p1.3.m3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1c" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi id="S2.p1.3.m3.1.1.6" xref="S2.p1.3.m3.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1d" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.3.m3.1.1.7" xref="S2.p1.3.m3.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.3.m3.1.1.1e" xref="S2.p1.3.m3.1.1.1.cmml">â€‹</mo><mn id="S2.p1.3.m3.1.1.8" xref="S2.p1.3.m3.1.1.8.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><times id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1"></times><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">ğ¿</ci><ci id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3">ğ‘</ci><ci id="S2.p1.3.m3.1.1.4.cmml" xref="S2.p1.3.m3.1.1.4">ğ‘¦</ci><ci id="S2.p1.3.m3.1.1.5.cmml" xref="S2.p1.3.m3.1.1.5">ğ‘’</ci><ci id="S2.p1.3.m3.1.1.6.cmml" xref="S2.p1.3.m3.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.3.m3.1.1.7.cmml" xref="S2.p1.3.m3.1.1.7">_</ci><cn type="integer" id="S2.p1.3.m3.1.1.8.cmml" xref="S2.p1.3.m3.1.1.8">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">Layer\_2</annotation></semantics></math> are frozen. On <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="Client\_2" display="inline"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1a" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.4" xref="S2.p1.4.m4.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1b" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.5" xref="S2.p1.4.m4.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1c" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.6" xref="S2.p1.4.m4.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1d" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi id="S2.p1.4.m4.1.1.7" xref="S2.p1.4.m4.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1e" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.4.m4.1.1.8" xref="S2.p1.4.m4.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.4.m4.1.1.1f" xref="S2.p1.4.m4.1.1.1.cmml">â€‹</mo><mn id="S2.p1.4.m4.1.1.9" xref="S2.p1.4.m4.1.1.9.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><times id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></times><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">ğ¶</ci><ci id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3">ğ‘™</ci><ci id="S2.p1.4.m4.1.1.4.cmml" xref="S2.p1.4.m4.1.1.4">ğ‘–</ci><ci id="S2.p1.4.m4.1.1.5.cmml" xref="S2.p1.4.m4.1.1.5">ğ‘’</ci><ci id="S2.p1.4.m4.1.1.6.cmml" xref="S2.p1.4.m4.1.1.6">ğ‘›</ci><ci id="S2.p1.4.m4.1.1.7.cmml" xref="S2.p1.4.m4.1.1.7">ğ‘¡</ci><ci id="S2.p1.4.m4.1.1.8.cmml" xref="S2.p1.4.m4.1.1.8">_</ci><cn type="integer" id="S2.p1.4.m4.1.1.9.cmml" xref="S2.p1.4.m4.1.1.9">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">Client\_2</annotation></semantics></math>, the variables in <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="Layer\_2" display="inline"><semantics id="S2.p1.5.m5.1a"><mrow id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1a" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S2.p1.5.m5.1.1.4" xref="S2.p1.5.m5.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1b" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S2.p1.5.m5.1.1.5" xref="S2.p1.5.m5.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1c" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mi id="S2.p1.5.m5.1.1.6" xref="S2.p1.5.m5.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1d" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.5.m5.1.1.7" xref="S2.p1.5.m5.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.5.m5.1.1.1e" xref="S2.p1.5.m5.1.1.1.cmml">â€‹</mo><mn id="S2.p1.5.m5.1.1.8" xref="S2.p1.5.m5.1.1.8.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><times id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1.1"></times><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">ğ¿</ci><ci id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3">ğ‘</ci><ci id="S2.p1.5.m5.1.1.4.cmml" xref="S2.p1.5.m5.1.1.4">ğ‘¦</ci><ci id="S2.p1.5.m5.1.1.5.cmml" xref="S2.p1.5.m5.1.1.5">ğ‘’</ci><ci id="S2.p1.5.m5.1.1.6.cmml" xref="S2.p1.5.m5.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.5.m5.1.1.7.cmml" xref="S2.p1.5.m5.1.1.7">_</ci><cn type="integer" id="S2.p1.5.m5.1.1.8.cmml" xref="S2.p1.5.m5.1.1.8">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">Layer\_2</annotation></semantics></math> are trained and those in <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="Layer\_1" display="inline"><semantics id="S2.p1.6.m6.1a"><mrow id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1a" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S2.p1.6.m6.1.1.4" xref="S2.p1.6.m6.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1b" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S2.p1.6.m6.1.1.5" xref="S2.p1.6.m6.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1c" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mi id="S2.p1.6.m6.1.1.6" xref="S2.p1.6.m6.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1d" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.6.m6.1.1.7" xref="S2.p1.6.m6.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.6.m6.1.1.1e" xref="S2.p1.6.m6.1.1.1.cmml">â€‹</mo><mn id="S2.p1.6.m6.1.1.8" xref="S2.p1.6.m6.1.1.8.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><times id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1"></times><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">ğ¿</ci><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">ğ‘</ci><ci id="S2.p1.6.m6.1.1.4.cmml" xref="S2.p1.6.m6.1.1.4">ğ‘¦</ci><ci id="S2.p1.6.m6.1.1.5.cmml" xref="S2.p1.6.m6.1.1.5">ğ‘’</ci><ci id="S2.p1.6.m6.1.1.6.cmml" xref="S2.p1.6.m6.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.6.m6.1.1.7.cmml" xref="S2.p1.6.m6.1.1.7">_</ci><cn type="integer" id="S2.p1.6.m6.1.1.8.cmml" xref="S2.p1.6.m6.1.1.8">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">Layer\_1</annotation></semantics></math> are frozen. The server then uses the change of <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="Layer\_1" display="inline"><semantics id="S2.p1.7.m7.1a"><mrow id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml"><mi id="S2.p1.7.m7.1.1.2" xref="S2.p1.7.m7.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S2.p1.7.m7.1.1.3" xref="S2.p1.7.m7.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1a" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S2.p1.7.m7.1.1.4" xref="S2.p1.7.m7.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1b" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S2.p1.7.m7.1.1.5" xref="S2.p1.7.m7.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1c" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi id="S2.p1.7.m7.1.1.6" xref="S2.p1.7.m7.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1d" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.7.m7.1.1.7" xref="S2.p1.7.m7.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.7.m7.1.1.1e" xref="S2.p1.7.m7.1.1.1.cmml">â€‹</mo><mn id="S2.p1.7.m7.1.1.8" xref="S2.p1.7.m7.1.1.8.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><apply id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1"><times id="S2.p1.7.m7.1.1.1.cmml" xref="S2.p1.7.m7.1.1.1"></times><ci id="S2.p1.7.m7.1.1.2.cmml" xref="S2.p1.7.m7.1.1.2">ğ¿</ci><ci id="S2.p1.7.m7.1.1.3.cmml" xref="S2.p1.7.m7.1.1.3">ğ‘</ci><ci id="S2.p1.7.m7.1.1.4.cmml" xref="S2.p1.7.m7.1.1.4">ğ‘¦</ci><ci id="S2.p1.7.m7.1.1.5.cmml" xref="S2.p1.7.m7.1.1.5">ğ‘’</ci><ci id="S2.p1.7.m7.1.1.6.cmml" xref="S2.p1.7.m7.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.7.m7.1.1.7.cmml" xref="S2.p1.7.m7.1.1.7">_</ci><cn type="integer" id="S2.p1.7.m7.1.1.8.cmml" xref="S2.p1.7.m7.1.1.8">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">Layer\_1</annotation></semantics></math> from <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="Client\_1" display="inline"><semantics id="S2.p1.8.m8.1a"><mrow id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml"><mi id="S2.p1.8.m8.1.1.2" xref="S2.p1.8.m8.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S2.p1.8.m8.1.1.3" xref="S2.p1.8.m8.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1a" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S2.p1.8.m8.1.1.4" xref="S2.p1.8.m8.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1b" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S2.p1.8.m8.1.1.5" xref="S2.p1.8.m8.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1c" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S2.p1.8.m8.1.1.6" xref="S2.p1.8.m8.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1d" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi id="S2.p1.8.m8.1.1.7" xref="S2.p1.8.m8.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1e" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.8.m8.1.1.8" xref="S2.p1.8.m8.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.8.m8.1.1.1f" xref="S2.p1.8.m8.1.1.1.cmml">â€‹</mo><mn id="S2.p1.8.m8.1.1.9" xref="S2.p1.8.m8.1.1.9.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><apply id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1"><times id="S2.p1.8.m8.1.1.1.cmml" xref="S2.p1.8.m8.1.1.1"></times><ci id="S2.p1.8.m8.1.1.2.cmml" xref="S2.p1.8.m8.1.1.2">ğ¶</ci><ci id="S2.p1.8.m8.1.1.3.cmml" xref="S2.p1.8.m8.1.1.3">ğ‘™</ci><ci id="S2.p1.8.m8.1.1.4.cmml" xref="S2.p1.8.m8.1.1.4">ğ‘–</ci><ci id="S2.p1.8.m8.1.1.5.cmml" xref="S2.p1.8.m8.1.1.5">ğ‘’</ci><ci id="S2.p1.8.m8.1.1.6.cmml" xref="S2.p1.8.m8.1.1.6">ğ‘›</ci><ci id="S2.p1.8.m8.1.1.7.cmml" xref="S2.p1.8.m8.1.1.7">ğ‘¡</ci><ci id="S2.p1.8.m8.1.1.8.cmml" xref="S2.p1.8.m8.1.1.8">_</ci><cn type="integer" id="S2.p1.8.m8.1.1.9.cmml" xref="S2.p1.8.m8.1.1.9">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">Client\_1</annotation></semantics></math> and that of <math id="S2.p1.9.m9.1" class="ltx_Math" alttext="Layer\_2" display="inline"><semantics id="S2.p1.9.m9.1a"><mrow id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml"><mi id="S2.p1.9.m9.1.1.2" xref="S2.p1.9.m9.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S2.p1.9.m9.1.1.3" xref="S2.p1.9.m9.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1a" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S2.p1.9.m9.1.1.4" xref="S2.p1.9.m9.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1b" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S2.p1.9.m9.1.1.5" xref="S2.p1.9.m9.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1c" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi id="S2.p1.9.m9.1.1.6" xref="S2.p1.9.m9.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1d" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.9.m9.1.1.7" xref="S2.p1.9.m9.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.9.m9.1.1.1e" xref="S2.p1.9.m9.1.1.1.cmml">â€‹</mo><mn id="S2.p1.9.m9.1.1.8" xref="S2.p1.9.m9.1.1.8.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><apply id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1"><times id="S2.p1.9.m9.1.1.1.cmml" xref="S2.p1.9.m9.1.1.1"></times><ci id="S2.p1.9.m9.1.1.2.cmml" xref="S2.p1.9.m9.1.1.2">ğ¿</ci><ci id="S2.p1.9.m9.1.1.3.cmml" xref="S2.p1.9.m9.1.1.3">ğ‘</ci><ci id="S2.p1.9.m9.1.1.4.cmml" xref="S2.p1.9.m9.1.1.4">ğ‘¦</ci><ci id="S2.p1.9.m9.1.1.5.cmml" xref="S2.p1.9.m9.1.1.5">ğ‘’</ci><ci id="S2.p1.9.m9.1.1.6.cmml" xref="S2.p1.9.m9.1.1.6">ğ‘Ÿ</ci><ci id="S2.p1.9.m9.1.1.7.cmml" xref="S2.p1.9.m9.1.1.7">_</ci><cn type="integer" id="S2.p1.9.m9.1.1.8.cmml" xref="S2.p1.9.m9.1.1.8">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">Layer\_2</annotation></semantics></math> from <math id="S2.p1.10.m10.1" class="ltx_Math" alttext="Client\_2" display="inline"><semantics id="S2.p1.10.m10.1a"><mrow id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml"><mi id="S2.p1.10.m10.1.1.2" xref="S2.p1.10.m10.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S2.p1.10.m10.1.1.3" xref="S2.p1.10.m10.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1a" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S2.p1.10.m10.1.1.4" xref="S2.p1.10.m10.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1b" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S2.p1.10.m10.1.1.5" xref="S2.p1.10.m10.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1c" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S2.p1.10.m10.1.1.6" xref="S2.p1.10.m10.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1d" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi id="S2.p1.10.m10.1.1.7" xref="S2.p1.10.m10.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1e" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.p1.10.m10.1.1.8" xref="S2.p1.10.m10.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.p1.10.m10.1.1.1f" xref="S2.p1.10.m10.1.1.1.cmml">â€‹</mo><mn id="S2.p1.10.m10.1.1.9" xref="S2.p1.10.m10.1.1.9.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><apply id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1"><times id="S2.p1.10.m10.1.1.1.cmml" xref="S2.p1.10.m10.1.1.1"></times><ci id="S2.p1.10.m10.1.1.2.cmml" xref="S2.p1.10.m10.1.1.2">ğ¶</ci><ci id="S2.p1.10.m10.1.1.3.cmml" xref="S2.p1.10.m10.1.1.3">ğ‘™</ci><ci id="S2.p1.10.m10.1.1.4.cmml" xref="S2.p1.10.m10.1.1.4">ğ‘–</ci><ci id="S2.p1.10.m10.1.1.5.cmml" xref="S2.p1.10.m10.1.1.5">ğ‘’</ci><ci id="S2.p1.10.m10.1.1.6.cmml" xref="S2.p1.10.m10.1.1.6">ğ‘›</ci><ci id="S2.p1.10.m10.1.1.7.cmml" xref="S2.p1.10.m10.1.1.7">ğ‘¡</ci><ci id="S2.p1.10.m10.1.1.8.cmml" xref="S2.p1.10.m10.1.1.8">_</ci><cn type="integer" id="S2.p1.10.m10.1.1.9.cmml" xref="S2.p1.10.m10.1.1.9">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">Client\_2</annotation></semantics></math> to update both layers of the server network. There are three main design decisions for PVT: what to freeze, how to freeze, and how many to freeze.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2110.05607/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="188" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.10.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>The illustration of the proposed partial variable training. In this example, <math id="S2.F1.5.m1.1" class="ltx_Math" alttext="Client\_1" display="inline"><semantics id="S2.F1.5.m1.1b"><mrow id="S2.F1.5.m1.1.1" xref="S2.F1.5.m1.1.1.cmml"><mi id="S2.F1.5.m1.1.1.2" xref="S2.F1.5.m1.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi id="S2.F1.5.m1.1.1.3" xref="S2.F1.5.m1.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1b" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi id="S2.F1.5.m1.1.1.4" xref="S2.F1.5.m1.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1c" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi id="S2.F1.5.m1.1.1.5" xref="S2.F1.5.m1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1d" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi id="S2.F1.5.m1.1.1.6" xref="S2.F1.5.m1.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1e" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi id="S2.F1.5.m1.1.1.7" xref="S2.F1.5.m1.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1f" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.F1.5.m1.1.1.8" xref="S2.F1.5.m1.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.F1.5.m1.1.1.1g" xref="S2.F1.5.m1.1.1.1.cmml">â€‹</mo><mn id="S2.F1.5.m1.1.1.9" xref="S2.F1.5.m1.1.1.9.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.5.m1.1c"><apply id="S2.F1.5.m1.1.1.cmml" xref="S2.F1.5.m1.1.1"><times id="S2.F1.5.m1.1.1.1.cmml" xref="S2.F1.5.m1.1.1.1"></times><ci id="S2.F1.5.m1.1.1.2.cmml" xref="S2.F1.5.m1.1.1.2">ğ¶</ci><ci id="S2.F1.5.m1.1.1.3.cmml" xref="S2.F1.5.m1.1.1.3">ğ‘™</ci><ci id="S2.F1.5.m1.1.1.4.cmml" xref="S2.F1.5.m1.1.1.4">ğ‘–</ci><ci id="S2.F1.5.m1.1.1.5.cmml" xref="S2.F1.5.m1.1.1.5">ğ‘’</ci><ci id="S2.F1.5.m1.1.1.6.cmml" xref="S2.F1.5.m1.1.1.6">ğ‘›</ci><ci id="S2.F1.5.m1.1.1.7.cmml" xref="S2.F1.5.m1.1.1.7">ğ‘¡</ci><ci id="S2.F1.5.m1.1.1.8.cmml" xref="S2.F1.5.m1.1.1.8">_</ci><cn type="integer" id="S2.F1.5.m1.1.1.9.cmml" xref="S2.F1.5.m1.1.1.9">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.5.m1.1d">Client\_1</annotation></semantics></math> only trains <math id="S2.F1.6.m2.1" class="ltx_Math" alttext="Layer\_1" display="inline"><semantics id="S2.F1.6.m2.1b"><mrow id="S2.F1.6.m2.1.1" xref="S2.F1.6.m2.1.1.cmml"><mi id="S2.F1.6.m2.1.1.2" xref="S2.F1.6.m2.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S2.F1.6.m2.1.1.3" xref="S2.F1.6.m2.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1b" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S2.F1.6.m2.1.1.4" xref="S2.F1.6.m2.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1c" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S2.F1.6.m2.1.1.5" xref="S2.F1.6.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1d" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mi id="S2.F1.6.m2.1.1.6" xref="S2.F1.6.m2.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1e" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.F1.6.m2.1.1.7" xref="S2.F1.6.m2.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.F1.6.m2.1.1.1f" xref="S2.F1.6.m2.1.1.1.cmml">â€‹</mo><mn id="S2.F1.6.m2.1.1.8" xref="S2.F1.6.m2.1.1.8.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.6.m2.1c"><apply id="S2.F1.6.m2.1.1.cmml" xref="S2.F1.6.m2.1.1"><times id="S2.F1.6.m2.1.1.1.cmml" xref="S2.F1.6.m2.1.1.1"></times><ci id="S2.F1.6.m2.1.1.2.cmml" xref="S2.F1.6.m2.1.1.2">ğ¿</ci><ci id="S2.F1.6.m2.1.1.3.cmml" xref="S2.F1.6.m2.1.1.3">ğ‘</ci><ci id="S2.F1.6.m2.1.1.4.cmml" xref="S2.F1.6.m2.1.1.4">ğ‘¦</ci><ci id="S2.F1.6.m2.1.1.5.cmml" xref="S2.F1.6.m2.1.1.5">ğ‘’</ci><ci id="S2.F1.6.m2.1.1.6.cmml" xref="S2.F1.6.m2.1.1.6">ğ‘Ÿ</ci><ci id="S2.F1.6.m2.1.1.7.cmml" xref="S2.F1.6.m2.1.1.7">_</ci><cn type="integer" id="S2.F1.6.m2.1.1.8.cmml" xref="S2.F1.6.m2.1.1.8">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.6.m2.1d">Layer\_1</annotation></semantics></math> and sends back its change. Similarly, <math id="S2.F1.7.m3.1" class="ltx_Math" alttext="Client\_2" display="inline"><semantics id="S2.F1.7.m3.1b"><mrow id="S2.F1.7.m3.1.1" xref="S2.F1.7.m3.1.1.cmml"><mi id="S2.F1.7.m3.1.1.2" xref="S2.F1.7.m3.1.1.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi id="S2.F1.7.m3.1.1.3" xref="S2.F1.7.m3.1.1.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1b" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi id="S2.F1.7.m3.1.1.4" xref="S2.F1.7.m3.1.1.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1c" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi id="S2.F1.7.m3.1.1.5" xref="S2.F1.7.m3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1d" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi id="S2.F1.7.m3.1.1.6" xref="S2.F1.7.m3.1.1.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1e" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi id="S2.F1.7.m3.1.1.7" xref="S2.F1.7.m3.1.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1f" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.F1.7.m3.1.1.8" xref="S2.F1.7.m3.1.1.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.F1.7.m3.1.1.1g" xref="S2.F1.7.m3.1.1.1.cmml">â€‹</mo><mn id="S2.F1.7.m3.1.1.9" xref="S2.F1.7.m3.1.1.9.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.7.m3.1c"><apply id="S2.F1.7.m3.1.1.cmml" xref="S2.F1.7.m3.1.1"><times id="S2.F1.7.m3.1.1.1.cmml" xref="S2.F1.7.m3.1.1.1"></times><ci id="S2.F1.7.m3.1.1.2.cmml" xref="S2.F1.7.m3.1.1.2">ğ¶</ci><ci id="S2.F1.7.m3.1.1.3.cmml" xref="S2.F1.7.m3.1.1.3">ğ‘™</ci><ci id="S2.F1.7.m3.1.1.4.cmml" xref="S2.F1.7.m3.1.1.4">ğ‘–</ci><ci id="S2.F1.7.m3.1.1.5.cmml" xref="S2.F1.7.m3.1.1.5">ğ‘’</ci><ci id="S2.F1.7.m3.1.1.6.cmml" xref="S2.F1.7.m3.1.1.6">ğ‘›</ci><ci id="S2.F1.7.m3.1.1.7.cmml" xref="S2.F1.7.m3.1.1.7">ğ‘¡</ci><ci id="S2.F1.7.m3.1.1.8.cmml" xref="S2.F1.7.m3.1.1.8">_</ci><cn type="integer" id="S2.F1.7.m3.1.1.9.cmml" xref="S2.F1.7.m3.1.1.9">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.7.m3.1d">Client\_2</annotation></semantics></math> only trains <math id="S2.F1.8.m4.1" class="ltx_Math" alttext="Layer\_2" display="inline"><semantics id="S2.F1.8.m4.1b"><mrow id="S2.F1.8.m4.1.1" xref="S2.F1.8.m4.1.1.cmml"><mi id="S2.F1.8.m4.1.1.2" xref="S2.F1.8.m4.1.1.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mi id="S2.F1.8.m4.1.1.3" xref="S2.F1.8.m4.1.1.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1b" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mi id="S2.F1.8.m4.1.1.4" xref="S2.F1.8.m4.1.1.4.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1c" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mi id="S2.F1.8.m4.1.1.5" xref="S2.F1.8.m4.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1d" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mi id="S2.F1.8.m4.1.1.6" xref="S2.F1.8.m4.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1e" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.F1.8.m4.1.1.7" xref="S2.F1.8.m4.1.1.7.cmml">_</mi><mo lspace="0em" rspace="0em" id="S2.F1.8.m4.1.1.1f" xref="S2.F1.8.m4.1.1.1.cmml">â€‹</mo><mn id="S2.F1.8.m4.1.1.8" xref="S2.F1.8.m4.1.1.8.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.8.m4.1c"><apply id="S2.F1.8.m4.1.1.cmml" xref="S2.F1.8.m4.1.1"><times id="S2.F1.8.m4.1.1.1.cmml" xref="S2.F1.8.m4.1.1.1"></times><ci id="S2.F1.8.m4.1.1.2.cmml" xref="S2.F1.8.m4.1.1.2">ğ¿</ci><ci id="S2.F1.8.m4.1.1.3.cmml" xref="S2.F1.8.m4.1.1.3">ğ‘</ci><ci id="S2.F1.8.m4.1.1.4.cmml" xref="S2.F1.8.m4.1.1.4">ğ‘¦</ci><ci id="S2.F1.8.m4.1.1.5.cmml" xref="S2.F1.8.m4.1.1.5">ğ‘’</ci><ci id="S2.F1.8.m4.1.1.6.cmml" xref="S2.F1.8.m4.1.1.6">ğ‘Ÿ</ci><ci id="S2.F1.8.m4.1.1.7.cmml" xref="S2.F1.8.m4.1.1.7">_</ci><cn type="integer" id="S2.F1.8.m4.1.1.8.cmml" xref="S2.F1.8.m4.1.1.8">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.8.m4.1d">Layer\_2</annotation></semantics></math> and sends back its change. The server updates both layers after receiving the changes from both clients.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>What to Freeze</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">PVT categorizes variables into two groups: freezable variables and non-freezable variables. PVT always trains the non-freezable variables and freezes a subset of freezable variables. To avoid the need for prior knowledge about the target network, the variable grouping is in the granularity of variables based on variable types. We define three types of variables:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.5" class="ltx_p"><span id="S2.I1.i1.p1.5.1" class="ltx_text ltx_font_bold">Additive vectors:</span> An additive vector adds a constant to each channel of input activations. Examples are the biases in convolutional layers and the offset factors in normalization layers. Additive vectors usually have a negligible cost in communication because they typically account for only a small portion of a network. Moreover, they also use less memory than the other variable types since fewer activations need to be buffered. In the example of updating biases (<math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="b_{i}" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><msub id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml">b</mi><mi id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">ğ‘</ci><ci id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">b_{i}</annotation></semantics></math>) by gradients in a linear layer <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="a_{i+1}=a_{i}+b_{i}" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><mrow id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml"><msub id="S2.I1.i1.p1.2.m2.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.2.2" xref="S2.I1.i1.p1.2.m2.1.1.2.2.cmml">a</mi><mrow id="S2.I1.i1.p1.2.m2.1.1.2.3" xref="S2.I1.i1.p1.2.m2.1.1.2.3.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.2.3.2" xref="S2.I1.i1.p1.2.m2.1.1.2.3.2.cmml">i</mi><mo id="S2.I1.i1.p1.2.m2.1.1.2.3.1" xref="S2.I1.i1.p1.2.m2.1.1.2.3.1.cmml">+</mo><mn id="S2.I1.i1.p1.2.m2.1.1.2.3.3" xref="S2.I1.i1.p1.2.m2.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.I1.i1.p1.2.m2.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S2.I1.i1.p1.2.m2.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.3.cmml"><msub id="S2.I1.i1.p1.2.m2.1.1.3.2" xref="S2.I1.i1.p1.2.m2.1.1.3.2.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.3.2.2" xref="S2.I1.i1.p1.2.m2.1.1.3.2.2.cmml">a</mi><mi id="S2.I1.i1.p1.2.m2.1.1.3.2.3" xref="S2.I1.i1.p1.2.m2.1.1.3.2.3.cmml">i</mi></msub><mo id="S2.I1.i1.p1.2.m2.1.1.3.1" xref="S2.I1.i1.p1.2.m2.1.1.3.1.cmml">+</mo><msub id="S2.I1.i1.p1.2.m2.1.1.3.3" xref="S2.I1.i1.p1.2.m2.1.1.3.3.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.3.3.2" xref="S2.I1.i1.p1.2.m2.1.1.3.3.2.cmml">b</mi><mi id="S2.I1.i1.p1.2.m2.1.1.3.3.3" xref="S2.I1.i1.p1.2.m2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><apply id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1"><eq id="S2.I1.i1.p1.2.m2.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1"></eq><apply id="S2.I1.i1.p1.2.m2.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.2">ğ‘</ci><apply id="S2.I1.i1.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.3"><plus id="S2.I1.i1.p1.2.m2.1.1.2.3.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.3.1"></plus><ci id="S2.I1.i1.p1.2.m2.1.1.2.3.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S2.I1.i1.p1.2.m2.1.1.2.3.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S2.I1.i1.p1.2.m2.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3"><plus id="S2.I1.i1.p1.2.m2.1.1.3.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.1"></plus><apply id="S2.I1.i1.p1.2.m2.1.1.3.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.3.2.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.2">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.1.1.3.2.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.2.2">ğ‘</ci><ci id="S2.I1.i1.p1.2.m2.1.1.3.2.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.I1.i1.p1.2.m2.1.1.3.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.3.3.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.3">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.1.1.3.3.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.3.2">ğ‘</ci><ci id="S2.I1.i1.p1.2.m2.1.1.3.3.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">a_{i+1}=a_{i}+b_{i}</annotation></semantics></math>, where <math id="S2.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="a_{*}" display="inline"><semantics id="S2.I1.i1.p1.3.m3.1a"><msub id="S2.I1.i1.p1.3.m3.1.1" xref="S2.I1.i1.p1.3.m3.1.1.cmml"><mi id="S2.I1.i1.p1.3.m3.1.1.2" xref="S2.I1.i1.p1.3.m3.1.1.2.cmml">a</mi><mo id="S2.I1.i1.p1.3.m3.1.1.3" xref="S2.I1.i1.p1.3.m3.1.1.3.cmml">âˆ—</mo></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m3.1b"><apply id="S2.I1.i1.p1.3.m3.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.3.m3.1.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.3.m3.1.1.2.cmml" xref="S2.I1.i1.p1.3.m3.1.1.2">ğ‘</ci><times id="S2.I1.i1.p1.3.m3.1.1.3.cmml" xref="S2.I1.i1.p1.3.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m3.1c">a_{*}</annotation></semantics></math> represent activations, <math id="S2.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="\frac{\partial a_{i+1}}{\partial b_{i}}" display="inline"><semantics id="S2.I1.i1.p1.4.m4.1a"><mfrac id="S2.I1.i1.p1.4.m4.1.1" xref="S2.I1.i1.p1.4.m4.1.1.cmml"><mrow id="S2.I1.i1.p1.4.m4.1.1.2" xref="S2.I1.i1.p1.4.m4.1.1.2.cmml"><mo rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.2.1" xref="S2.I1.i1.p1.4.m4.1.1.2.1.cmml">âˆ‚</mo><msub id="S2.I1.i1.p1.4.m4.1.1.2.2" xref="S2.I1.i1.p1.4.m4.1.1.2.2.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.2.2.2" xref="S2.I1.i1.p1.4.m4.1.1.2.2.2.cmml">a</mi><mrow id="S2.I1.i1.p1.4.m4.1.1.2.2.3" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.2.2.3.2" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.2.cmml">i</mi><mo id="S2.I1.i1.p1.4.m4.1.1.2.2.3.1" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.1.cmml">+</mo><mn id="S2.I1.i1.p1.4.m4.1.1.2.2.3.3" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.3.cmml">1</mn></mrow></msub></mrow><mrow id="S2.I1.i1.p1.4.m4.1.1.3" xref="S2.I1.i1.p1.4.m4.1.1.3.cmml"><mo rspace="0em" id="S2.I1.i1.p1.4.m4.1.1.3.1" xref="S2.I1.i1.p1.4.m4.1.1.3.1.cmml">âˆ‚</mo><msub id="S2.I1.i1.p1.4.m4.1.1.3.2" xref="S2.I1.i1.p1.4.m4.1.1.3.2.cmml"><mi id="S2.I1.i1.p1.4.m4.1.1.3.2.2" xref="S2.I1.i1.p1.4.m4.1.1.3.2.2.cmml">b</mi><mi id="S2.I1.i1.p1.4.m4.1.1.3.2.3" xref="S2.I1.i1.p1.4.m4.1.1.3.2.3.cmml">i</mi></msub></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.4.m4.1b"><apply id="S2.I1.i1.p1.4.m4.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1"><divide id="S2.I1.i1.p1.4.m4.1.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1"></divide><apply id="S2.I1.i1.p1.4.m4.1.1.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2"><partialdiff id="S2.I1.i1.p1.4.m4.1.1.2.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.1"></partialdiff><apply id="S2.I1.i1.p1.4.m4.1.1.2.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.4.m4.1.1.2.2.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2">subscript</csymbol><ci id="S2.I1.i1.p1.4.m4.1.1.2.2.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2.2">ğ‘</ci><apply id="S2.I1.i1.p1.4.m4.1.1.2.2.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3"><plus id="S2.I1.i1.p1.4.m4.1.1.2.2.3.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.1"></plus><ci id="S2.I1.i1.p1.4.m4.1.1.2.2.3.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.I1.i1.p1.4.m4.1.1.2.2.3.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.2.2.3.3">1</cn></apply></apply></apply><apply id="S2.I1.i1.p1.4.m4.1.1.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3"><partialdiff id="S2.I1.i1.p1.4.m4.1.1.3.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.1"></partialdiff><apply id="S2.I1.i1.p1.4.m4.1.1.3.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.4.m4.1.1.3.2.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.2">subscript</csymbol><ci id="S2.I1.i1.p1.4.m4.1.1.3.2.2.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.2.2">ğ‘</ci><ci id="S2.I1.i1.p1.4.m4.1.1.3.2.3.cmml" xref="S2.I1.i1.p1.4.m4.1.1.3.2.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.4.m4.1c">\frac{\partial a_{i+1}}{\partial b_{i}}</annotation></semantics></math> does not depend on <math id="S2.I1.i1.p1.5.m5.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.I1.i1.p1.5.m5.1a"><msub id="S2.I1.i1.p1.5.m5.1.1" xref="S2.I1.i1.p1.5.m5.1.1.cmml"><mi id="S2.I1.i1.p1.5.m5.1.1.2" xref="S2.I1.i1.p1.5.m5.1.1.2.cmml">a</mi><mi id="S2.I1.i1.p1.5.m5.1.1.3" xref="S2.I1.i1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.5.m5.1b"><apply id="S2.I1.i1.p1.5.m5.1.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.5.m5.1.1.1.cmml" xref="S2.I1.i1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.5.m5.1.1.2.cmml" xref="S2.I1.i1.p1.5.m5.1.1.2">ğ‘</ci><ci id="S2.I1.i1.p1.5.m5.1.1.3.cmml" xref="S2.I1.i1.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.5.m5.1c">a_{i}</annotation></semantics></math>.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.5" class="ltx_p"><span id="S2.I1.i2.p1.5.1" class="ltx_text ltx_font_bold">Multiplicative vectors:</span> A multiplicative vector scales each channel of input activations by a constant. Examples are the scaling factors in normalization layers. Similar to additive vectors, multiplicative vectors usually account for only a small portion of a network and have a negligible cost in communication. However, they require buffering activations for gradient computation, hence they consume more memory than additive vectors. In the example of updating scaling factors (<math id="S2.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="s_{i}" display="inline"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">s</mi><mi id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">ğ‘ </ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">s_{i}</annotation></semantics></math>) by gradients in a linear layer <math id="S2.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="a_{i+1}=s_{i}*a_{i}" display="inline"><semantics id="S2.I1.i2.p1.2.m2.1a"><mrow id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><msub id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.2" xref="S2.I1.i2.p1.2.m2.1.1.2.2.cmml">a</mi><mrow id="S2.I1.i2.p1.2.m2.1.1.2.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2.3.2" xref="S2.I1.i2.p1.2.m2.1.1.2.3.2.cmml">i</mi><mo id="S2.I1.i2.p1.2.m2.1.1.2.3.1" xref="S2.I1.i2.p1.2.m2.1.1.2.3.1.cmml">+</mo><mn id="S2.I1.i2.p1.2.m2.1.1.2.3.3" xref="S2.I1.i2.p1.2.m2.1.1.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.I1.i2.p1.2.m2.1.1.1" xref="S2.I1.i2.p1.2.m2.1.1.1.cmml">=</mo><mrow id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml"><msub id="S2.I1.i2.p1.2.m2.1.1.3.2" xref="S2.I1.i2.p1.2.m2.1.1.3.2.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.3.2.2" xref="S2.I1.i2.p1.2.m2.1.1.3.2.2.cmml">s</mi><mi id="S2.I1.i2.p1.2.m2.1.1.3.2.3" xref="S2.I1.i2.p1.2.m2.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.I1.i2.p1.2.m2.1.1.3.1" xref="S2.I1.i2.p1.2.m2.1.1.3.1.cmml">âˆ—</mo><msub id="S2.I1.i2.p1.2.m2.1.1.3.3" xref="S2.I1.i2.p1.2.m2.1.1.3.3.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.3.3.2" xref="S2.I1.i2.p1.2.m2.1.1.3.3.2.cmml">a</mi><mi id="S2.I1.i2.p1.2.m2.1.1.3.3.3" xref="S2.I1.i2.p1.2.m2.1.1.3.3.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><eq id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.1"></eq><apply id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.2">ğ‘</ci><apply id="S2.I1.i2.p1.2.m2.1.1.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3"><plus id="S2.I1.i2.p1.2.m2.1.1.2.3.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3.1"></plus><ci id="S2.I1.i2.p1.2.m2.1.1.2.3.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S2.I1.i2.p1.2.m2.1.1.2.3.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2.3.3">1</cn></apply></apply><apply id="S2.I1.i2.p1.2.m2.1.1.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3"><times id="S2.I1.i2.p1.2.m2.1.1.3.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.1"></times><apply id="S2.I1.i2.p1.2.m2.1.1.3.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.3.2.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.2">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.3.2.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.2.2">ğ‘ </ci><ci id="S2.I1.i2.p1.2.m2.1.1.3.2.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.2.3">ğ‘–</ci></apply><apply id="S2.I1.i2.p1.2.m2.1.1.3.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.3.3.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.3">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.3.3.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.3.2">ğ‘</ci><ci id="S2.I1.i2.p1.2.m2.1.1.3.3.3.cmml" xref="S2.I1.i2.p1.2.m2.1.1.3.3.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">a_{i+1}=s_{i}*a_{i}</annotation></semantics></math>, <math id="S2.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\frac{\partial a_{i+1}}{\partial s_{i}}" display="inline"><semantics id="S2.I1.i2.p1.3.m3.1a"><mfrac id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mrow id="S2.I1.i2.p1.3.m3.1.1.2" xref="S2.I1.i2.p1.3.m3.1.1.2.cmml"><mo rspace="0em" id="S2.I1.i2.p1.3.m3.1.1.2.1" xref="S2.I1.i2.p1.3.m3.1.1.2.1.cmml">âˆ‚</mo><msub id="S2.I1.i2.p1.3.m3.1.1.2.2" xref="S2.I1.i2.p1.3.m3.1.1.2.2.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2.2.2" xref="S2.I1.i2.p1.3.m3.1.1.2.2.2.cmml">a</mi><mrow id="S2.I1.i2.p1.3.m3.1.1.2.2.3" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2.2.3.2" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.2.cmml">i</mi><mo id="S2.I1.i2.p1.3.m3.1.1.2.2.3.1" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.1.cmml">+</mo><mn id="S2.I1.i2.p1.3.m3.1.1.2.2.3.3" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.3.cmml">1</mn></mrow></msub></mrow><mrow id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml"><mo rspace="0em" id="S2.I1.i2.p1.3.m3.1.1.3.1" xref="S2.I1.i2.p1.3.m3.1.1.3.1.cmml">âˆ‚</mo><msub id="S2.I1.i2.p1.3.m3.1.1.3.2" xref="S2.I1.i2.p1.3.m3.1.1.3.2.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.3.2.2" xref="S2.I1.i2.p1.3.m3.1.1.3.2.2.cmml">s</mi><mi id="S2.I1.i2.p1.3.m3.1.1.3.2.3" xref="S2.I1.i2.p1.3.m3.1.1.3.2.3.cmml">i</mi></msub></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><divide id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"></divide><apply id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2"><partialdiff id="S2.I1.i2.p1.3.m3.1.1.2.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.1"></partialdiff><apply id="S2.I1.i2.p1.3.m3.1.1.2.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.2.2.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2">subscript</csymbol><ci id="S2.I1.i2.p1.3.m3.1.1.2.2.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2.2">ğ‘</ci><apply id="S2.I1.i2.p1.3.m3.1.1.2.2.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3"><plus id="S2.I1.i2.p1.3.m3.1.1.2.2.3.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.1"></plus><ci id="S2.I1.i2.p1.3.m3.1.1.2.2.3.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.I1.i2.p1.3.m3.1.1.2.2.3.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2.2.3.3">1</cn></apply></apply></apply><apply id="S2.I1.i2.p1.3.m3.1.1.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3"><partialdiff id="S2.I1.i2.p1.3.m3.1.1.3.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3.1"></partialdiff><apply id="S2.I1.i2.p1.3.m3.1.1.3.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.3.2.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3.2">subscript</csymbol><ci id="S2.I1.i2.p1.3.m3.1.1.3.2.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3.2.2">ğ‘ </ci><ci id="S2.I1.i2.p1.3.m3.1.1.3.2.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3.2.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">\frac{\partial a_{i+1}}{\partial s_{i}}</annotation></semantics></math> is equal to <math id="S2.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.I1.i2.p1.4.m4.1a"><msub id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml"><mi id="S2.I1.i2.p1.4.m4.1.1.2" xref="S2.I1.i2.p1.4.m4.1.1.2.cmml">a</mi><mi id="S2.I1.i2.p1.4.m4.1.1.3" xref="S2.I1.i2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.1b"><apply id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.4.m4.1.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.4.m4.1.1.2.cmml" xref="S2.I1.i2.p1.4.m4.1.1.2">ğ‘</ci><ci id="S2.I1.i2.p1.4.m4.1.1.3.cmml" xref="S2.I1.i2.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.1c">a_{i}</annotation></semantics></math>, so <math id="S2.I1.i2.p1.5.m5.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.I1.i2.p1.5.m5.1a"><msub id="S2.I1.i2.p1.5.m5.1.1" xref="S2.I1.i2.p1.5.m5.1.1.cmml"><mi id="S2.I1.i2.p1.5.m5.1.1.2" xref="S2.I1.i2.p1.5.m5.1.1.2.cmml">a</mi><mi id="S2.I1.i2.p1.5.m5.1.1.3" xref="S2.I1.i2.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.5.m5.1b"><apply id="S2.I1.i2.p1.5.m5.1.1.cmml" xref="S2.I1.i2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.5.m5.1.1.1.cmml" xref="S2.I1.i2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.5.m5.1.1.2.cmml" xref="S2.I1.i2.p1.5.m5.1.1.2">ğ‘</ci><ci id="S2.I1.i2.p1.5.m5.1.1.3.cmml" xref="S2.I1.i2.p1.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.5.m5.1c">a_{i}</annotation></semantics></math> needs to be buffered.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Multiplicative matrices:</span> An input activation matrix is multiplied by a multiplicative matrix. Examples are the weight matrices in convolutional and feed-forward layers. They usually dominate the network size and require buffering activations for gradient computation. Hence, multiplicative matrices are the most expensive in terms of memory usage and communication cost.</p>
</div>
</li>
</ul>
<p id="S2.SS1.p1.2" class="ltx_p">TableÂ <a href="#S2.T1" title="Table 1 â€£ 2.1 What to Freeze â€£ 2 Methodology â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the memory usage and the communication cost of these three variable types. It is intuitive to freeze multiplicative matrices first, multiplicative vectors second, and additive vectors third. Moreover, prior knowledge on the target network, such as layer ambienceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, can be incorporated to further improve the effectiveness of PVT.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:224.8pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.5pt,3.6pt) scale(0.9,0.9) ;">
<table id="S2.T1.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"></th>
<th id="S2.T1.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Memory</th>
<th id="S2.T1.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Communication</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.1.2.1" class="ltx_tr">
<th id="S2.T1.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Additive Vectors</th>
<td id="S2.T1.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">Low</td>
<td id="S2.T1.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">Low</td>
</tr>
<tr id="S2.T1.2.1.3.2" class="ltx_tr">
<th id="S2.T1.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Multiplicative Vectors</th>
<td id="S2.T1.2.1.3.2.2" class="ltx_td ltx_align_center">High</td>
<td id="S2.T1.2.1.3.2.3" class="ltx_td ltx_align_center">Low</td>
</tr>
<tr id="S2.T1.2.1.4.3" class="ltx_tr">
<th id="S2.T1.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">Multiplicative Matrices</th>
<td id="S2.T1.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">High</td>
<td id="S2.T1.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">High</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>The memory usage and the communication cost of the three variable types.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>How to Freeze</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The choice about which variables to freeze can</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p"><span id="S2.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Be fixed (fixed):</span> The same set of variables are chosen by all clients in all rounds.</p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p"><span id="S2.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Vary per round (PR):</span> The chosen variables vary from round to round, but all clients in a given round choose the same set of variables.</p>
</div>
</li>
<li id="S2.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S2.I2.i3.p1" class="ltx_para">
<p id="S2.I2.i3.p1.1" class="ltx_p"><span id="S2.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Vary per client per round (PCPR):</span> The chosen variables vary from round to round and from client to client.</p>
</div>
</li>
</ul>
<p id="S2.SS2.p1.2" class="ltx_p">PCPR generally attains the highest accuracy given the same number of rounds and works the best for from-scratch training because it updates all the variables in each round. Compared with PCPR, PR is easier to implement because all the clients share the same network graph at the cost of longer convergence time. The fixed scheme can be useful when we have prior knowledge about the target network.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>How Many to Freeze</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Freezing more variables enables higher memory and communication savings at the cost of accuracy degradation. Fortunately, we observe that almost all the loss in accuracy can be compensated by utilizing more local training steps and more clients. Therefore, by carefully choosing the hyperparameters (e.g., number of frozen variables, local training steps, and clients), PVT can improve FL efficiency while providing highly accurate networks.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">We suggest the following steps to determine these hyper-parameters systematically. We first increase the number of frozen variables until the memory and communication constraints are satisfied. Then, we increase the number of local training steps to what is allowed on devices. Finally, we increase the number of clients until the accuracy is restored.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Settings</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Two networks are adopted in the experiments to cover both non-streaming and streaming use cases. The first network is the largest ConformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> (referred to as <em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">non-streaming Conformer</em>) with Batch Normalization replaced by Group Normalization, which is more suitable to federated learningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> but at a small cost of accuracy. The second network is our production-grade Conformer variantÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> (referred to as <em id="S3.SS1.p1.1.2" class="ltx_emph ltx_font_italic">streaming Conformer</em>), which supports streaming use cases and contains approximately 137M trainable parameters.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">In addition to two networks, we adopt two datasets with different partition methods to cover different data distributions and from-scratch training and domain adaptation scenarios. The first dataset is LibrispeechÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">IID Librispeech</em> is generated by randomly partitioning Librispeech into multiple small datasets to simulate IID clientsâ€™ local datasets. <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">Non-IID Librispeech</em> is generated by partitioning Librispeech by speakers to simulate non-IID clientsâ€™ local datasets. The two different partition methods help evaluate PVT under both IID and non-IID data. For experiments on Librispeech, we train networks from scratch with the training set and evaluate them on the test set.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The second dataset is an anonymized <em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Multi-Domain (MD) dataset</em> containing approximately 400K hours of utterances from domains such as search, farfield, telephony, and YouTubeÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. We withhold one domain (<em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">Medium Form (MF)</em> in this paper) from the MD dataset to evaluate PVT under domain adaptation scenarios. The MF domain has approximately 26K hours of utterance with an average duration of 10.4 seconds. For experiments on the MD dataset, we first train networks on the training set with the MF domain withheld, refining them on the MF domain, and then evaluate them on a disjoint test set from the MF domain.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Unless otherwise specified, the non-freezable variables are all the additive vectors, and the freezable variables are all the multiplicative vectors and matrices. We randomly freeze 90% of freezable variables with the PCPR scheme. There are 1024 clients, and each client trains a network with 5 local steps. The batch size is 16 per client. For resource consumption, we report the peak memory usage with gradient recomputationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and the client-to-server (CtoS) communication cost. For Word Error Rates (WERs) on Librispeech, we report them in the format of <em id="S3.SS1.p4.1.1" class="ltx_emph ltx_font_italic">dev/dev-other/test/test-other</em>, where each item corresponds to the WER of the dev, dev-other, test, and test-other set from left to right.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Non-Streaming Conformer on Librispeech</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.2" class="ltx_p">TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.2 Non-Streaming Conformer on Librispeech â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the results of non-streaming Conformer on IID Librispeech. Compared with all-variable training (AVT), PVT can achieve similar WERs with much lower memory usage and CtoS communication cost. We observe that running forward passes alone requires 761MB, and AVT requires extra 611MB to allow running backward passes. In contrast, PVT only requires extra 313MB, which is 1.9<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation></semantics></math> lower than that of AVT. Moreover, PVT reduces the CtoS communication cost by 9.8<math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mo id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><times id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\times</annotation></semantics></math>.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:239.4pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.3pt,4.5pt) scale(0.9,0.9) ;">
<table id="S3.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<th id="S3.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T2.2.1.1.1.2.1" class="ltx_text">WERs</span></th>
<th id="S3.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Resource</th>
</tr>
<tr id="S3.T2.2.1.2.2" class="ltx_tr">
<th id="S3.T2.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Peak Memory</th>
<th id="S3.T2.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CtoS Comm.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.3.1" class="ltx_tr">
<th id="S3.T2.2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">FProp</th>
<th id="S3.T2.2.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="S3.T2.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">761MB</td>
<td id="S3.T2.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T2.2.1.4.2" class="ltx_tr">
<th id="S3.T2.2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AVT</th>
<th id="S3.T2.2.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2.1/4.6/2.2/4.8</th>
<td id="S3.T2.2.1.4.2.3" class="ltx_td ltx_align_center">1372MB</td>
<td id="S3.T2.2.1.4.2.4" class="ltx_td ltx_align_center">452MB</td>
</tr>
<tr id="S3.T2.2.1.5.3" class="ltx_tr">
<th id="S3.T2.2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T2.2.1.5.3.1.1" class="ltx_text ltx_font_bold">PVT</span></th>
<th id="S3.T2.2.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T2.2.1.5.3.2.1" class="ltx_text ltx_font_bold">2.1/4.9/2.3/4.8</span></th>
<td id="S3.T2.2.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.3.3.1" class="ltx_text ltx_font_bold">1074MB</span></td>
<td id="S3.T2.2.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.1.5.3.4.1" class="ltx_text ltx_font_bold">46MB</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.4.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>The results of non-streaming Conformer on IID Librispeech. <em id="S3.T2.5.2" class="ltx_emph ltx_font_italic">FProp</em> refers to performing only forward passes.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">TableÂ <a href="#S3.T3" title="Table 3 â€£ 3.2 Non-Streaming Conformer on Librispeech â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes the WERs of non-streaming Conformer on non-IID Librispeech. Even with non-IID data, PVT can still attain comparable WERs to AVT. The reduction in memory usage and CtoS communication cost is the same as the previous IID experiment and, hence, omitted in TableÂ <a href="#S3.T3" title="Table 3 â€£ 3.2 Non-Streaming Conformer on Librispeech â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. These experiments show the versatility of PVT to work well with both IID and non-IID data.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:179.4pt;height:32.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.0pt,1.8pt) scale(0.9,0.9) ;">
<table id="S3.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T3.2.1.1.1" class="ltx_tr">
<th id="S3.T3.2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt"></th>
<td id="S3.T3.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">AVT</td>
<td id="S3.T3.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S3.T3.2.1.1.1.3.1" class="ltx_text ltx_font_bold">PVT</span></td>
</tr>
<tr id="S3.T3.2.1.2.2" class="ltx_tr">
<th id="S3.T3.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t">WER</th>
<td id="S3.T3.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">2.0/4.7/2.2/4.9</td>
<td id="S3.T3.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T3.2.1.2.2.3.1" class="ltx_text ltx_font_bold">2.1/4.8/2.2/5.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.3.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>The WERs of non-streaming Conformer on non-IID Librispeech.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Streaming Conformer on Multi-Domain Dataset</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">We observe that domain adaptation may allow training fewer variables than from-scratch training. In this experiment, only training biases (i.e., freezing all freezable variables) is enough for providing high accuracy. TableÂ <a href="#S3.T4" title="Table 4 â€£ 3.3 Streaming Conformer on Multi-Domain Dataset â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the results of streaming Conformer on the multi-domain dataset. Compared to AVT, PVT can achieve similar WERs with 1.3<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mo id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><times id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\times</annotation></semantics></math> reduction in extra memory usage on top of running forward passes and 593<math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mo id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><times id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\times</annotation></semantics></math> reduction in communication cost. This experiment demonstrates the effectiveness of PVT on domain adaptation and large-scale datasets.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:249.6pt;height:97.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.9pt,5.4pt) scale(0.9,0.9) ;">
<table id="S3.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.1.1.1" class="ltx_tr">
<th id="S3.T4.2.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"></th>
<th id="S3.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" rowspan="2"><span id="S3.T4.2.1.1.1.2.1" class="ltx_text">WER</span></th>
<th id="S3.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Resource</th>
</tr>
<tr id="S3.T4.2.1.2.2" class="ltx_tr">
<th id="S3.T4.2.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Peak Memory</th>
<th id="S3.T4.2.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">CtoS Comm.</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.1.3.1" class="ltx_tr">
<th id="S3.T4.2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Before Refinement</th>
<th id="S3.T4.2.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">6.9</th>
<td id="S3.T4.2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S3.T4.2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S3.T4.2.1.4.2" class="ltx_tr">
<th id="S3.T4.2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">FProp</th>
<th id="S3.T4.2.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<td id="S3.T4.2.1.4.2.3" class="ltx_td ltx_align_center">836MB</td>
<td id="S3.T4.2.1.4.2.4" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S3.T4.2.1.5.3" class="ltx_tr">
<th id="S3.T4.2.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">AVT</th>
<th id="S3.T4.2.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">4.5</th>
<td id="S3.T4.2.1.5.3.3" class="ltx_td ltx_align_center">1606MB</td>
<td id="S3.T4.2.1.5.3.4" class="ltx_td ltx_align_center">522MB</td>
</tr>
<tr id="S3.T4.2.1.6.4" class="ltx_tr">
<th id="S3.T4.2.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T4.2.1.6.4.1.1" class="ltx_text ltx_font_bold">PVT</span></th>
<th id="S3.T4.2.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r"><span id="S3.T4.2.1.6.4.2.1" class="ltx_text ltx_font_bold">4.6</span></th>
<td id="S3.T4.2.1.6.4.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.6.4.3.1" class="ltx_text ltx_font_bold">1448MB</span></td>
<td id="S3.T4.2.1.6.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.2.1.6.4.4.1" class="ltx_text ltx_font_bold">0.9MB</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.4.1.1" class="ltx_text ltx_font_bold">Table 4</span>: </span>The results of streaming Conformer on the multi-domain dataset. The WER is on the MF domain. <em id="S3.T4.5.2" class="ltx_emph ltx_font_italic">FProp</em> refers to performing only forward passes.</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Ablation Study</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We use non-streaming Conformer on IID Librispeech to study the influence of different design decisions. We use 1 local training step per round and 128 clients in this section.</p>
</div>
<section id="S3.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Per-Round Scheme vs. Per-Client-Per-Round Scheme</h4>

<div id="S3.SS4.SSS1.p1" class="ltx_para">
<p id="S3.SS4.SSS1.p1.1" class="ltx_p">We observe that per-round (PR) scheme is unable to converge to reasonable WERs within 200K rounds while the per-client-per-round (PCPR) scheme attains 2.4/5.7/2.5/5.7, which are close to that of AVT. Our hypothesis is that PR only updates 10% of the freezable variables per round, so needs much more rounds than PCPR to properly update the entire network.</p>
</div>
</section>
<section id="S3.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Number of Local Training Steps</h4>

<div id="S3.SS4.SSS2.p1" class="ltx_para">
<p id="S3.SS4.SSS2.p1.1" class="ltx_p">TableÂ <a href="#S3.T5" title="Table 5 â€£ 3.4.2 Number of Local Training Steps â€£ 3.4 Ablation Study â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> compares the WERs and the required numbers of rounds when various numbers of local training steps are used. We observe that using more local training steps improves WERs and speeds up convergence. Moreover, in this experiment, PVT allows as many as 128 local training steps and still provides converged results.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:263.1pt;height:64.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-14.6pt,3.6pt) scale(0.9,0.9) ;">
<table id="S3.T5.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T5.2.1.1.1" class="ltx_tr">
<th id="S3.T5.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Number of Local Steps</th>
<th id="S3.T5.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">WER</th>
<th id="S3.T5.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of Rounds</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T5.2.1.2.1" class="ltx_tr">
<th id="S3.T5.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">1</th>
<td id="S3.T5.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.2/5.5/2.4/5.4</td>
<td id="S3.T5.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">350K</td>
</tr>
<tr id="S3.T5.2.1.3.2" class="ltx_tr">
<th id="S3.T5.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">5</th>
<td id="S3.T5.2.1.3.2.2" class="ltx_td ltx_align_center">2.1/4.9/2.2/4.9</td>
<td id="S3.T5.2.1.3.2.3" class="ltx_td ltx_align_center">155K</td>
</tr>
<tr id="S3.T5.2.1.4.3" class="ltx_tr">
<th id="S3.T5.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">128</th>
<td id="S3.T5.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_bb">2.0/4.8/2.2/4.6</td>
<td id="S3.T5.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_bb">15K</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.3.1.1" class="ltx_text ltx_font_bold">Table 5</span>: </span>The comparison among various numbers of local steps with non-streaming Conformer on IID Librispeech.</figcaption>
</figure>
</section>
<section id="S3.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Number of Clients</h4>

<div id="S3.SS4.SSS3.p1" class="ltx_para">
<p id="S3.SS4.SSS3.p1.1" class="ltx_p">TableÂ <a href="#S3.T6" title="Table 6 â€£ 3.4.3 Number of Clients â€£ 3.4 Ablation Study â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> compares the WERs and the required numbers of rounds when various numbers of clients are used. Similar to numbers of local training steps, WERs and convergence speed improve as we use more clients. This study shows that the loss of WERs and convergence speed caused by freezing variables can be compensated by using more clients, which is a favorable property for large-scale federated learning.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<div id="S3.T6.2" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:244.9pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-13.6pt,4.5pt) scale(0.9,0.9) ;">
<table id="S3.T6.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.2.1.1.1" class="ltx_tr">
<th id="S3.T6.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Number of Clients</th>
<th id="S3.T6.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">WER</th>
<th id="S3.T6.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Number of Rounds</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.2.1.2.1" class="ltx_tr">
<th id="S3.T6.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">128</th>
<td id="S3.T6.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2.2/5.5/2.4/5.4</td>
<td id="S3.T6.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">350K</td>
</tr>
<tr id="S3.T6.2.1.3.2" class="ltx_tr">
<th id="S3.T6.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">1024</th>
<td id="S3.T6.2.1.3.2.2" class="ltx_td ltx_align_center">2.1/5.0/2.3/4.9</td>
<td id="S3.T6.2.1.3.2.3" class="ltx_td ltx_align_center">205K</td>
</tr>
<tr id="S3.T6.2.1.4.3" class="ltx_tr">
<th id="S3.T6.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">2048</th>
<td id="S3.T6.2.1.4.3.2" class="ltx_td ltx_align_center">2.1/5.1/2.2/5.0</td>
<td id="S3.T6.2.1.4.3.3" class="ltx_td ltx_align_center">90K</td>
</tr>
<tr id="S3.T6.2.1.5.4" class="ltx_tr">
<th id="S3.T6.2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r">4096</th>
<td id="S3.T6.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">2.1/5.1/2.2/5.0</td>
<td id="S3.T6.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">60K</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T6.3.1.1" class="ltx_text ltx_font_bold">Table 6</span>: </span>The comparison among various numbers of clients with non-streaming Conformer on IID Librispeech.</figcaption>
</figure>
</section>
<section id="S3.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.4 </span>WER and Convergence Speed Improvement</h4>

<div id="S3.SS4.SSS4.p1" class="ltx_para">
<p id="S3.SS4.SSS4.p1.1" class="ltx_p">Fig.Â <a href="#S3.F2" title="Figure 2 â€£ 3.4.4 WER and Convergence Speed Improvement â€£ 3.4 Ablation Study â€£ 3 Experimental Results â€£ Partial Variable Training for Efficient On-Device Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows how WERs and convergence speed improve while we sequentially apply what we learned from the analysis and ablation study above. When PCPR scheme, not-freezing additive vectors, 5 local training steps, and 1024 clients are sequentially applied, the training curve of PVT gradually becomes comparable to that of AVT.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2110.05607/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="242" height="96" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>The WER and convergence speed improvement while we sequentially apply PCPR scheme, not freezing additive vectors (NFAV), 5 local steps (5LS), 1024 clients (1024CL) to train non-streaming Conformer on IID Librispeech.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Works</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">There are strong efforts to tackle similar challenges as PVT in the literature. Model transport compressionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and gradient transport compressionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> apply compression techniques to decrease the server-to-client and the client-to-server communication cost respectively, but the memory usage on devices is unchanged. Network compression reduces both memory usage and communication cost by simplifying networks, including manual designÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, pruningÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, quantizationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and neural architecture searchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. However, reducing network complexity may degrade the ability of federated learning to benefit from a large amount of data, which requires high-complexity networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Federated dropout Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> addresses this issue by using simplified server networks on clients, so the server network can be complex without burdening clients. As server and client networks differ, federated dropout needs additional infrastructure to maintain a mapping of client networks to the full server network. Similar to federate dropout, group knowledge transferÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> also uses different networks on a server and clients. The simpler client networks are used to extract the features for training the server network on a server, which decreases client loading at the cost of increased server loading. Compared to the above methods, PVT can reduce both memory usage and communication cost without their downsides and can be further combined with them to achieve even better efficiency.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we proposed Partial Variable Training that significantly reduces memory usage and communication cost for Federated Learning (FL) with a negligible impact on accuracy. It does not require modifying architectures and network-specific knowledge and suits large-scale FL. We hope this technique will help bring FL onto edge devices and run in a large scale to greatly improve user experience.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Acknowledgements</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We thank Yonghui Xiao, Petr Zadrazil, and Changwan Ryu for supporting memory measurement in this paper.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton,

</span>
<span class="ltx_bibblock">â€œSpeech recognition with deep recurrent neural networks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</span>, 2013, pp. 6645â€“6649.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Peter Kairouz, HÂ BrendanÂ McMahan, Brendan Avent, etÂ al.,

</span>
<span class="ltx_bibblock">â€œAdvances and open problems in federated learning,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.04977</span>, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jianyu Wang, Zachary Charles, Zheng Xu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œA field guide to federated optimization,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.06917</span>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Junxian Huang, Feng Qian, Yihua Guo, etÂ al.,

</span>
<span class="ltx_bibblock">â€œAn in-depth study of lte: Effect of network protocol and
application behavior on performance,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM SIGCOMM Conference on SIGCOMM</span>, 2013.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
C.Â Zhang, S.Â Bengio, and Y.Â Singer,

</span>
<span class="ltx_bibblock">â€œAre all layers created equal?,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1902.01996</span>, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Anmol Gulati, James Qin, Chung-Cheng Chiu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œConformer: Convolution-augmented transformer for speech
recognition,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Conference of the International Speech Communication
Association (INTERSPEECH)</span>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and PhillipÂ B. Gibbons,

</span>
<span class="ltx_bibblock">â€œThe non-iid data quagmire of decentralized machine learning,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.00189</span>, 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
BoÂ Li, Anmol Gulati, Jiahui Yu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œA better and faster end-to-end model for streaming asr,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur,

</span>
<span class="ltx_bibblock">â€œLibrispeech: An asr corpus based on public domain audio books,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</span>, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Arun Narayanan, Rohit Prabhavalkar, Chung-Cheng Chiu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œRecognizing long-form speech using streaming end-to-end models,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">IEEE Automatic Speech Recognition and Understanding
Workshop</span>, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ananya Misra, Dongseong Hwang, Zhouyuan Huo, etÂ al.,

</span>
<span class="ltx_bibblock">â€œA Comparison of Supervised and Unsupervised Pre-Training of
End-to-End Models,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Conference of the International Speech Communication
Association (INTERSPEECH)</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin,

</span>
<span class="ltx_bibblock">â€œTraining deep nets with sublinear memory cost,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1604.06174</span>, 2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
SÃ©lim Chraibi, Ahmed Khaled, Dmitry Kovalev, etÂ al.,

</span>
<span class="ltx_bibblock">â€œDistributed fixed point methods with compressed iterates,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1912.09925</span>, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jakub KoneÄnÃ½, H.Â Brendan McMahan, FelixÂ X. Yu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œFederated learning: Strategies for improving communication
efficiency,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)
Workshop on Private Multi-Party Machine Learning</span>, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, etÂ al.,

</span>
<span class="ltx_bibblock">â€œMobilenetv2: Inverted residuals and linear bottlenecks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze,

</span>
<span class="ltx_bibblock">â€œDesigning energy-efficient convolutional neural networks using
energy-aware pruning,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">IEEE Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi,

</span>
<span class="ltx_bibblock">â€œXnor-net: Imagenet classification using binary convolutional neural
networks,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision (ECCV)</span>, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tien-Ju Yang, Yi-Lun Liao, and Vivienne Sze,

</span>
<span class="ltx_bibblock">â€œNetadaptv2: Efficient neural architecture search with fast
super-network training and architecture optimization,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, etÂ al.,

</span>
<span class="ltx_bibblock">â€œLanguage models are few-shot learners,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</span>,
2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Jakub KoneÄnÃ½, H.Â Brendan McMahan, and Ameet
Talwalkar,

</span>
<span class="ltx_bibblock">â€œExpanding the reach of federated learning by reducing client
resource requirements,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1812.07210</span>, 2018.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr,

</span>
<span class="ltx_bibblock">â€œGroup knowledge transfer: Federated learning of large cnns at the
edge,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems (NeurIPS)</span>,
2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2110.05606" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2110.05607" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2110.05607">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2110.05607" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2110.05608" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar 19 14:38:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
