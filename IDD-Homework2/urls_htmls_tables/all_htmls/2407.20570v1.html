<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</title>
<!--Generated on Tue Jul 30 05:52:33 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2407.20570v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S1" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S2" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S2.SS1" title="In 2 Related Work ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>LLM-based Visualization System for Domain Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S2.SS2" title="In 2 Related Work ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Fine-tuning LLMs for Domain Specific Applications</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S2.SS3" title="In 2 Related Work ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Intelligent Tutorial System and Educational Agent</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Fine-Tuned LLM for Visualization System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS1" title="In 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Problem Definition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS2" title="In 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Framework</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS3" title="In 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Application of the Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Requirement Analysis of Tailor-Mind</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS1" title="In 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Interviews and Surveys</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS2" title="In 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Challenges for Self-Regulated Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS3" title="In 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Requirements for Tailor-Mind</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Tailor-Mind</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS1" title="In 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Workflow and Self-Regulated Learning Process</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS2" title="In 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Fine-tuned LLM for Self-Regulated Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3" title="In 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>User Interface</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3.SSS1" title="In 5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.1 </span>Chat View</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3.SSS2" title="In 5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.2 </span>File Preview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3.SSS3" title="In 5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.3 </span>Question Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3.SSS4" title="In 5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.4 </span>Learning Path</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3.SSS5" title="In 5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3.5 </span>Knowledge MindMap</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS1" title="In 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Model Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS2" title="In 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Usage Scenario</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS2.SSS1" title="In 6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Integrating Knowledge and Deepening Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS2.SSS2" title="In 6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Stimulating Interest and Exploratory Learning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS3" title="In 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>User Study</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS3.SSS1" title="In 6.3 User Study ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Experimental Set-up</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS3.SSS2" title="In 6.3 User Study ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Results and Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S7" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S7.SS1" title="In 7 Discussion ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Discussion on Tailor-Mind</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S7.SS2" title="In 7 Discussion ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Discussion on General Framework</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S8" title="In Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\onlineid</span>
<p class="ltx_p" id="p1.2">1096
<span class="ltx_ERROR undefined" id="p1.2.1">\vgtccategory</span>Research
<span class="ltx_ERROR undefined" id="p1.2.2">\vgtcpapertype</span>applicaition


<span class="ltx_ERROR undefined" id="p1.2.3">\authorfooter</span>
Lin Gao, Jing Lu, Zekai Shao, Ziyue Lin, Shengbin Yue, Chiokit Ieong, Yi Sun, Zhongyu Wei, and Siming Chen are with the School of Data Science at Fudan University. S. Chen is also with the Shanghai Key Laboratory of Data Science. S. Chen is the corresponding author.
E-mail: lingao23@m.fudan.edu.cn, simingchen@fudan.edu.cn
Rory James Zauner is with the Faculty of Computer Science, University of Vienna, Vienna, Austria.

</p>
</div>
<h1 class="ltx_title ltx_title_document">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_ERROR undefined" id="1.1.1">\authororcid</span>Lin Gao0009-0004-1613-1774
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jing Lu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_ERROR undefined" id="2.1.1">\authororcid</span>Zekai Shao0000-0003-2014-5293
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ziyue Lin
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_ERROR undefined" id="3.1.1">\authororcid</span>Shengbin Yue0000-0002-6764-1756
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Chiokit Ieong
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yi Sun
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break"/>Rory James Zauner
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_ERROR undefined" id="4.1.1">\authororcid</span>Zhongyu Wei0000-0003-3789-8507 and
<span class="ltx_ERROR undefined" id="5.2.2">\authororcid</span>Siming Chen0000-0002-2690-3588
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="6.1">Large Language Models (LLMs) have shown great potential in intelligent visualization systems, especially for domain-specific applications. Integrating LLMs into visualization systems presents challenges, and we categorize these challenges into three alignments: domain problems with LLMs, visualization with LLMs, and interaction with LLMs. To achieve these alignments, we propose a framework and outline a workflow to guide the application of fine-tuned LLMs to enhance visual interactions for domain-specific tasks. These alignment challenges are critical in education because of the need for an intelligent visualization system to support beginners’ self-regulated learning. Therefore, we apply the framework to education and introduce Tailor-Mind, an interactive visualization system designed to facilitate self-regulated learning for artificial intelligence beginners. Drawing on insights from a preliminary study, we identify self-regulated learning tasks and fine-tuning objectives to guide visualization design and tuning data construction. Our focus on aligning visualization with fine-tuned LLM makes Tailor-Mind more like a personalized tutor. Tailor-Mind also supports interactive recommendations to help beginners better achieve their learning goals. Model performance evaluations and user studies confirm that Tailor-Mind improves the self-regulated learning experience, effectively validating the proposed framework.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Fine-tuned large language model, visualization system, self-regulated learning, intelligent tutorial system
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\teaser</span><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="467" id="p2.g1" src="x1.png" width="830"/>
<p class="ltx_p ltx_align_center" id="p2.2"><span class="ltx_text ltx_caption" id="p2.2.1">The overview of <span class="ltx_text ltx_font_italic" id="p2.2.1.1">Tailor-Mind.</span> The Chat View (A) enables users to engage in conversations and receive recommended questions (A1), enhancing their knowledge exploration. Intelligent responses provide in-depth explanations (A2) and guide users to discover new knowledge connections (A3). The File Preview (B) displays learning materials (B1), knowledge structures in a tree widget (B2), and knowledge cards (B3). The Knowledge MindMap (C) visualizes the network of knowledge based on the learning levels (C1), allowing users to select different layouts (C2). Knowledge nodes display users’ notes (C3), support recommended questions, and allow for the modification of learning goals (C4). Users are also encouraged to take notes in a rich text editor (D). The Question Recommendation (E) offers questions about extracted knowledge points on various learning levels. The Learning Path (F) helps users understand their learning plans and delve into each detailed learning objective.
</span>
</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancement of large language models (LLMs) has captured significant attention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib47" title="">47</a>]</cite> and opened new avenues for tackling specialized domain problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib26" title="">26</a>]</cite>. Increasingly, researchers are applying these general models to specific areas, with supervised fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib54" title="">54</a>]</cite> emerging as a practical approach. By training on domain-specific datasets, this method enhances the model’s performance in fields such as healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib46" title="">46</a>]</cite>, legal <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib62" title="">62</a>]</cite>, and education <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib7" title="">7</a>]</cite>. Domain-specific LLMs, fine-tuned in this way, demonstrate improved decision-making, knowledge integration, retrieval, and logical reasoning, addressing issues of instability and inaccuracy commonly found in general models when applied to domain-specific tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib66" title="">66</a>]</cite>.
Visualization systems are also commonly used to solve domain-specific problems because they reveal data insights and provide a good user experience through interactions. Some researchers have specifically proposed general frameworks in the visualization community to guide analysts in designing and implementing visualization workflows for domain tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib1" title="">1</a>]</cite>. However, traditional visualization systems that incorporate small-scale models have limited capabilities for intelligent analysis. Due to the intelligence of LLMs, many researchers are considering integrating LLMs into the visual interaction processes of domain-specific problems.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, integrating LLMs and visualization systems to address domain-specific problems encounters some difficulties, owing to their distinct capabilities, which complicate the optimal leveraging of their respective strengths.
<span class="ltx_text" id="S1.p2.1.1">The first challenge is the adaptation of LLMs to domain-specific problems. For the strong domain barriers in these problems, general LLMs mostly lack the ability to handle specific, complex domain knowledge and therefore cannot effectively solve domain problems.<span class="ltx_text" id="S1.p2.1.1.1">
<span class="ltx_text" id="S1.p2.1.1.1.1">The second challenge is synchronizing visualization with LLMs. LLMs lack knowledge of visualization and an understanding of visualization systems. To empower visualization systems with LLM capabilities, it is crucial to ensure that models internalize the visualization process and visual mappings as knowledge and actions.<span class="ltx_text" id="S1.p2.1.1.1.1.1">
<span class="ltx_text" id="S1.p2.1.1.1.1.1.1">The third challenge involves understanding and enhancing interactions with LLMs. LLMs need to understand various types of interactions and how to provide personalized interactive recommendations. Ensuring LLMs accurately interpret user intentions from different interactions is vital for achieving effective and personalized interactive experiences.<span class="ltx_text" id="S1.p2.1.1.1.1.1.1.1"></span></span></span></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1">Existing LLM-empowered visualization systems can intelligently and automatically solve problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib52" title="">52</a>]</cite>. However, they merely embed LLM into the system without enhancing the model’s knowledge and behavior for complex, domain-specific knowledge.<span class="ltx_text" id="S1.p3.1.1.1">
<span class="ltx_text" id="S1.p3.1.1.1.1">Additionally, some efforts address domain-specific problems by fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib15" title="">15</a>]</cite>. However, these fine-tuned models are constructed in isolation from the visualization process and do not consider the tasks and requirements involved in visualization.<span class="ltx_text" id="S1.p3.1.1.1.1.1">
<span class="ltx_text" id="S1.p3.1.1.1.1.1.1">Therefore, there is a lack of work that integrates domain knowledge, visualization, and interaction to leverage LLMs for domain-specific problems.<span class="ltx_text" id="S1.p3.1.1.1.1.1.1.1"></span></span></span></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, <span class="ltx_text" id="S1.p4.1.1">we analyze the relationships and patterns among LLMs, domain knowledge, visualization, and interaction, identifying three alignment objectives: domain problems with LLMs, visualizations with LLMs, and interactions with LLMs. Based on these alignments, we propose a conceptual framework to guide the tuning of LLMs for domain-specific tasks in visualization systems.<span class="ltx_text" id="S1.p4.1.1.1">
<span class="ltx_text" id="S1.p4.1.1.1.1">We apply our proposed framework to the educational domain, introducing Tailor-Mind for artificial intelligence (AI) beginners. A preliminary study with students and teachers reveals a need for intelligent interactive systems in Self-Regulated Learning (SRL), which our approach aims to address.<span class="ltx_text" id="S1.p4.1.1.1.1.1"> <span class="ltx_text" id="S1.p4.1.1.1.1.1.1">By integrating expert insights with user needs, learning tasks are defined, leading to the establishment of design requirements and fine-tuning objectives for the domain model. This optimization ensures the model fits the learning tasks and maintains interactive functionality between visualization and students’ learning process.<span class="ltx_text" id="S1.p4.1.1.1.1.1.1.1"> To our knowledge, this work presents the first conceptual framework that combines fine-tuned models and visualization systems to tackle domain-specific problems.
The contributions are as follows:</span></span></span></span></span></span></p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">A conceptual framework</span> that integrates fine-tuned LLMs into interactive visualization systems, alongside <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.2">a workflow</span> of applying the framework to different domains.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Applying the framework to the education domain, we introduce <span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Tailor-Mind</span>, an interactive visualization system for artificial intelligence beginners supported by a fine-tuned LLM. The system supports intelligent exploration of knowledge and personalized recommendations during the self-regulated learning process.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The evaluation of model performance, alongside findings from usage scenarios and user study, validates Tailor-Mind’s effectiveness in facilitating SRL experiences. This substantiates the framework’s rationality and feasibility and offers the educational domain valuable insights for promoting active, iterative learning.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section reviews related work on fine-tuned LLMs for domain-specific applications and LLM-empowered visualization systems, especially in the education domain.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>LLM-based Visualization System for Domain Tasks</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1">The exceptional performance of LLMs has facilitated their integration into visual interactions. Currently, their power can be leveraged in several ways: external knowledge bases, prompt engineering, agent settings, and data fine-tuning.<span class="ltx_text" id="S2.SS1.p1.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">External knowledge bases.<span class="ltx_text ltx_font_medium" id="S2.SS1.p2.1.1.1"> External knowledge bases enhance LLMs by providing vast repositories of structured information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib23" title="">23</a>]</cite>.<span class="ltx_text" id="S2.SS1.p2.1.1.1.1"> DocFlow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib38" title="">38</a>]</cite> intelligently classifies documents by incorporating document information retrieval techniques based on user questions. Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib36" title="">36</a>]</cite> proposed an LLM-AUGMENTER system that augments LLM with plug-and-play modules based on external knowledge.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Prompt engineering.</span> Through prompt engineering, LEVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib64" title="">64</a>]</cite> enables LLMs to generate the declarative syntax of a visual analytics system, understand view relationships, and interpret diagram information to provide analysis tasks and interaction recommendations. <span class="ltx_text" id="S2.SS1.p3.1.2">Works tailored to specific scenarios, such as interior design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib12" title="">12</a>]</cite> and virtual museum tours <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib52" title="">52</a>]</cite>, enhance user experiences by interpreting inputs and adapting outputs, thereby facilitating intelligent and personalized solutions. Simultaneously, there is work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib44" title="">44</a>]</cite> that focuses on visualizing prompt performance and methods for iterative optimization.<span class="ltx_text" id="S2.SS1.p3.1.2.1"></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Agent setting.<span class="ltx_text ltx_font_medium" id="S2.SS1.p4.1.1.1"> A unique application of prompt projects lies in agent design, where the LLM-based conversational agent enhances user immersion in conjunction with interaction design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib28" title="">28</a>]</cite>.
The sandbox is another scenario where LLM-based agents are used to simulate social behaviors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib35" title="">35</a>]</cite>. AgentLens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib29" title="">29</a>]</cite> illustrates the evolution of LLM-based autonomous systems through hierarchical temporal visualization.<span class="ltx_text" id="S2.SS1.p4.1.1.1.1"></span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">Data fine-tuning.<span class="ltx_text ltx_font_medium" id="S2.SS1.p5.1.1.1"> Several systems employ data fine-tuning to personalize models, primarily focusing on addressing natural language questions.<span class="ltx_text" id="S2.SS1.p5.1.1.1.1">
CommonsenseVIS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib50" title="">50</a>]</cite> adds concept and relation alignments to improve model behavior contextualization and question-answering ability.
PlantoGraphy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib15" title="">15</a>]</cite> uses a fine-tuned model to transform garden scene layouts into realistic landscape renderings.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">The above work demonstrates that integrating LLMs into visualization systems improves user experience. LLM performance in domain tasks is becoming increasingly critical, and the challenge of enhancing domain task completion through the LLM-empowered visualization system remains a concern. Our work focuses on aligning model performance with visualization and interaction to assist domain task-solving.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fine-tuning LLMs for Domain Specific Applications</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The advent of LLMs like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib31" title="">31</a>]</cite> and LLaMa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib47" title="">47</a>]</cite> has catalyzed research into leveraging their formidable powers across diverse professional domains. Wei et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib54" title="">54</a>]</cite> introduced an innovative instruction fine-tuning technique to bolster the models’ adaptability to domain tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib67" title="">67</a>]</cite>. In the judiciary, projects like DISC-LawLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib62" title="">62</a>]</cite> has made significant headway in addressing intricate tasks such as legal element identification, case sorting, and judgment forecasting. In the financial sector, FinGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib60" title="">60</a>]</cite> stands as a testament to the potential of large models developed through a thorough analysis of financial narratives, social discourse, and fiscal reports. In healthcare, BenTsao <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib48" title="">48</a>]</cite> has improved models’ question-answering capabilities and proposed a fine-tuned dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib8" title="">8</a>]</cite> leveraging technologies like knowledge graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib49" title="">49</a>]</cite>. Initiatives such as EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib7" title="">7</a>]</cite> and Taoli Llama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib61" title="">61</a>]</cite> have enhanced the utility of large models, meeting the growing call for accessible models in the educational sphere <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib2" title="">2</a>]</cite>. Moreover, in specific disciplines, Yue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib63" title="">63</a>]</cite> trained the MAmmoTH series of models with enhanced mathematical reasoning ability by mixing Chain of Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib55" title="">55</a>]</cite> and Programming of Thought (PoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib4" title="">4</a>]</cite>. PromptProtein <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib53" title="">53</a>]</cite> focused on protein sequence prediction, demonstrating the value of discipline-specific LLMs.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Instruction fine-tuning is acknowledged for enhancing model efficacy and adaptability in targeted domains. Yet, developing fine-tuning datasets is intricate and laborious, with the dataset’s quality and volume being pivotal to the model’s domain-specific outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib40" title="">40</a>]</cite>. For many developers, constructing datasets for fine-tuning presents a formidable and time-intensive challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib39" title="">39</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Intelligent Tutorial System and Educational Agent</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Since the illustrating workflow pertains to education, it is essential to analyze how previous studies utilized LLM alongside visualization to augment the pedagogical landscape <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib17" title="">17</a>]</cite>. A crucial aspect of an intelligent tutorial system (ITS) is interactive visualization, offering an intuitive and engaging educational journey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib6" title="">6</a>]</cite>. Wang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib69" title="">69</a>]</cite> have innovated by automating slide generation from Jupyter Notebooks. TransforLearn <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib10" title="">10</a>]</cite> provides an interactive way to understand the Transformer model and data flows. Kabdo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib5" title="">5</a>]</cite> have proposed AlgoSolve, a tool aiding learners in algorithmic problems. The advancement of LLMs has given rise to LLM-empowered ITS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib20" title="">20</a>]</cite>. Storyfier<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib37" title="">37</a>]</cite> utilizes LLMs in language practice, generating contexts that encompass target words. UKP-SQuARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib9" title="">9</a>]</cite> offers a platform to operate, assess, and analyze various QA models. Another critical area in ITS is using educational agents to instruct, motivate, and engage learners <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib57" title="">57</a>]</cite>. HypoCompass <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib30" title="">30</a>]</cite> proposes a learning-by-teaching approach, where one agent functions as a student, while a novice oversees the debugging process. Ruffle &amp; Riley <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib41" title="">41</a>]</cite> demonstrate student and teacher roles, executing tutorial scripts from textbooks.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">LLM-enhanced approaches offer novel learning solutions yet often fail to meet domain-specific standards, with effectiveness tied to the model’s capabilities. Moreover, these methods primarily act as chatbots, lacking comprehensive learner guidance. In educational theory, fostering students’ SRL capabilities is paramount <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib34" title="">34</a>]</cite>. Consequently, we integrate the proposed framework within the educational context, constructing instructional datasets for fine-tuned domain models and merging the model with a visualization system to assist users in accomplishing SRL tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Fine-Tuned LLM for Visualization System</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS1" title="3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3.1</span></a>, the challenges of integrating LLMs into visualization systems are summarized in three alignments. To address these challenges, we discuss the proposed framework (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS2" title="3.2 Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3.2</span></a>) and analyze how it can be applied to solve domain problems (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS3" title="3.3 Application of the Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Problem Definition</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Given user preferences for visual interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib27" title="">27</a>]</cite>, enhancing the intelligence of visualization systems becomes crucial.
However, integrating LLMs into visualization systems presents multiple challenges. To tackle these, we propose three alignments.
<span class="ltx_text" id="S3.SS1.p1.1.1">We define “alignment” as the mutual adaptation and coordination between LLMs and various elements such as visualization systems, domain knowledge, and user interactions, aimed at meeting multifaceted requirements and optimizing performance for models’ targeted behaviors.<span class="ltx_text" id="S3.SS1.p1.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A1</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Alignment between domain problems and LLMs.</span>
It is critical to integrate domain-specific knowledge (e.g., terminology and concepts), experiences, and insights into the LLM and match the linguistic patterns. This helps LLMs to skillfully solve complex tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib11" title="">11</a>]</cite> that require a deep understanding of domain specificity.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A2</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Alignment between visualizations and LLMs.</span> It’s essential to account for the context in which visualization operates, including the specific data features or insights. The visualization design and its alignment with model outputs are paramount. This design extends beyond visual representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib42" title="">42</a>]</cite> to encompass the system’s pipeline to reflect domain-specific problem-solving.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">A3</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Alignment between interactions and LLMs.</span> LLMs should interpret user intents behind interactions, including natural language, non-verbal cues, and visual elements. Furthermore, since user interactions can be aimless and uncertain, LLMs need to adapt to exploratory queries and offer guided hints.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Framework</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Based on the traditional framework of visual analytics processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib1" title="">1</a>]</cite>, we think of the impact of the LLM’s addition and propose the conceptual framework, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.F1" title="Figure 1 ‣ 3.2 Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">1</span></a>. In this framework, we use domain tasks and data as inputs to provide people with task solution results and experiences through an intelligent visualization system based on the fine-tuned LLM.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="230" id="S3.F1.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S3.F1.3.2" style="font-size:90%;">Framework of integrating fine-tuned LLM into visualization system. The black lines indicate relationships in traditional processes, and the red lines highlight connections that are introduced or altered by the involvement of fine-tuned LLM.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Node Description.</span> <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">Task</span> possesses strong domain-specific traits and is typically a complex domain problem consisting of multiple sub-tasks. <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.3">Data</span> consists of the data provided by users and the data needed to be transformed into domain knowledge. <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.4">Human</span> is the subject involved in the whole process. Additionally, humans are the end users of the fine-tuned LLM and visualization system, and they interact with them to accomplish the task. <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.5">Visualization</span> refers to the visualization system, which provides interaction and visual perception. Humans use the visualization system as a platform to complete tasks. <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.6">Fine-tuned LLM</span> outperforms the small-scale model and excels at handling domain tasks than the generalized LLM.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p3.1.1">Fine-tuning<span class="ltx_text ltx_font_upright" id="S3.SS2.p3.1.1.1"> aligns LLM with domain requirements.</span></span>
Fine-tuning requires that models learn to “know” domain knowledge and “apply” knowledge to solve problems.
Converting domain data into high-quality knowledge is a key aspect of fine-tuning. High-quality data for fine-tuning can help models distill insights and knowledge from it.
Moreover, fine-tuning involves teaching LLM the processing flow and expertise from domain-specific tasks. The model needs to develop behaviors for handling domain-specific problems by comprehending the logic and sequence between sub-tasks rather than processing a task in isolation.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p4.1.1">Integration<span class="ltx_text ltx_font_upright" id="S3.SS2.p4.1.1.1"> and </span>refinement<span class="ltx_text ltx_font_upright" id="S3.SS2.p4.1.1.2"> of LLMs with visualization systems.</span></span>
In the process of integration, the fine-tuned model needs to understand the original intent of the visualization design, including the design purposes, data features, and communications of relevant insights. The model also needs to learn the contextual logic required for the insights to align the visualization.
Refinement strategies can be extracted from visual designs and system workflows, and they can be applied to facilitate model handling of domain problems in the visualization system.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Intelligent <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1.1">assistance</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p5.1.1.2">interaction</span> from LLM align with user requirements.</span>
The fine-tuned LLM aligns with human preferences, habits, and behavioral patterns. This adaptation enables the LLM to seamlessly deliver intelligent assistance during user interactions, whether through natural language processing or visual interface engagement.
Users can facilitate problem-solving through guided interactions from fine-tuned LLM, ensuring that aimless queries are supported in a coherent and purposeful manner.
</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Application of the Framework</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We can derive insights for implementing intelligent visualization interaction processes from the conceptual framework.
<span class="ltx_text" id="S3.SS3.p1.1.1">Therefore, we propose a general workflow (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.F2" title="Figure 2 ‣ 3.3 Application of the Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">2</span></a>) along with corresponding guidelines and provide a detailed explanation based on its application to an educational problem. We focus on SRL, a process where students plan, monitor, and evaluate their own learning, as intelligent and online learning platforms have shifted students toward greater self-initiative rather than traditional teacher guidance.<span class="ltx_text" id="S3.SS3.p1.1.1.1">
<span class="ltx_text" id="S3.SS3.p1.1.1.1.1">The workflow process undergoes three phases, with guidelines using “<math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.SS3.p1.1.1.1.1.m1.1"><semantics id="S3.SS3.p1.1.1.1.1.m1.1a"><mo id="S3.SS3.p1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.SS3.p1.1.1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.1.1.1.m1.1b"><ci id="S3.SS3.p1.1.1.1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.1.1.1.m1.1d">⇒</annotation></semantics></math>” to represent the influence or guidance of one aspect on another.<span class="ltx_text" id="S3.SS3.p1.1.1.1.1.1"></span></span></span></span></p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S3.F2.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.3.2" style="font-size:90%;">Workflow for applying the framework. All three phases of the workflow are designed to achieve the alignment challenges.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Task Identification.</span>
<span class="ltx_text" id="S3.SS3.p2.1.2">In a traditional visualization workflow, the first step is to refine <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2.1">design requirements</span> based on the needs of domain experts and target users.<span class="ltx_text" id="S3.SS3.p2.1.2.2">
<span class="ltx_text" id="S3.SS3.p2.1.2.2.1">Based on the supported user needs and the existing LLM’s gaps in domain knowledge and capabilities (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i1" title="Item A1 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A1</span></a>), we derive targeted <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2.2.1.1">tuning tasks</span> to enhance the model’s domain knowledge and performance.<span class="ltx_text" id="S3.SS3.p2.1.2.2.1.2"></span></span></span></span></p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text" id="S3.I2.i1.p1.1.1">(Design Requirements <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I2.i1.p1.1.1.m1.1"><semantics id="S3.I2.i1.p1.1.1.m1.1a"><mo id="S3.I2.i1.p1.1.1.m1.1.1" stretchy="false" xref="S3.I2.i1.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p1.1.1.m1.1b"><ci id="S3.I2.i1.p1.1.1.m1.1.1.cmml" xref="S3.I2.i1.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i1.p1.1.1.m1.1d">⇒</annotation></semantics></math> Tuning Tasks) Extract tasks that require enhanced intelligence from the requirements.<span class="ltx_text" id="S3.I2.i1.p1.1.1.1"></span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text" id="S3.I2.i2.p1.1.1">(Tuning Tasks <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I2.i2.p1.1.1.m1.1"><semantics id="S3.I2.i2.p1.1.1.m1.1a"><mo id="S3.I2.i2.p1.1.1.m1.1.1" stretchy="false" xref="S3.I2.i2.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I2.i2.p1.1.1.m1.1b"><ci id="S3.I2.i2.p1.1.1.m1.1.1.cmml" xref="S3.I2.i2.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i2.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I2.i2.p1.1.1.m1.1d">⇒</annotation></semantics></math> Design Requirements) Constructing usage scenarios helps to validate and iterate on the needs of potential users.<span class="ltx_text" id="S3.I2.i2.p1.1.1.1"></span></span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text" id="S3.SS3.p3.1.1">Regarding its application in education, we conducted a preliminary study on SRL to identify the design requirements for helping beginners deeply understand knowledge and personalize their learning process. From this, we distilled key tasks for fine-tuning, such as optimizing domain-specific Q&amp;A to enhance knowledge comprehension and providing personalized recommendations to tailor the learning experience. We refined these functionalities through continuous user engagement to enhance the intelligent SRL process.<span class="ltx_text" id="S3.SS3.p3.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Design Mapping.</span>
<span class="ltx_text" id="S3.SS3.p4.1.2">Based on the design requirements, we further developed <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2.1">visualization</span> views and summarized the visual exploration <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2.2">pipeline</span>.<span class="ltx_text" id="S3.SS3.p4.1.2.3">
<span class="ltx_text" id="S3.SS3.p4.1.2.3.1">By integrating LLMs, the entire visualization process becomes more intelligent and interactive. We need to collect and construct <span class="ltx_text ltx_font_italic" id="S3.SS3.p4.1.2.3.1.1">tuning data</span> to adapt to the visualization system (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i2" title="Item A2 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A2</span></a>).<span class="ltx_text" id="S3.SS3.p4.1.2.3.1.2"></span></span></span></span></p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1"><span class="ltx_text" id="S3.I3.i1.p1.1.1">(Pipeline <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I3.i1.p1.1.1.m1.1"><semantics id="S3.I3.i1.p1.1.1.m1.1a"><mo id="S3.I3.i1.p1.1.1.m1.1.1" stretchy="false" xref="S3.I3.i1.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.1.1.m1.1b"><ci id="S3.I3.i1.p1.1.1.m1.1.1.cmml" xref="S3.I3.i1.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.1.1.m1.1d">⇒</annotation></semantics></math> Tuning Data) We need to summarize patterns in the process of solving problems through visual interactions, guiding model behavior using multi-turn dialogues or CoT approaches.<span class="ltx_text" id="S3.I3.i1.p1.1.1.1">
</span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1"><span class="ltx_text" id="S3.I3.i2.p1.1.1">(Visualization <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I3.i2.p1.1.1.m1.1"><semantics id="S3.I3.i2.p1.1.1.m1.1a"><mo id="S3.I3.i2.p1.1.1.m1.1.1" stretchy="false" xref="S3.I3.i2.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I3.i2.p1.1.1.m1.1b"><ci id="S3.I3.i2.p1.1.1.m1.1.1.cmml" xref="S3.I3.i2.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i2.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i2.p1.1.1.m1.1d">⇒</annotation></semantics></math> Tuning Data) To support the functionalities of different visualization views, we need to construct data structures that align with their designs. This includes understanding encoding methods, data source characteristics, and forms of data insights.
Additionally, the mapping relationships in visualization standardize the data structure and instruction format.<span class="ltx_text" id="S3.I3.i2.p1.1.1.1">
</span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.1"><span class="ltx_text" id="S3.I3.i3.p1.1.1">(Tuning Data <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I3.i3.p1.1.1.m1.1"><semantics id="S3.I3.i3.p1.1.1.m1.1a"><mo id="S3.I3.i3.p1.1.1.m1.1.1" stretchy="false" xref="S3.I3.i3.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I3.i3.p1.1.1.m1.1b"><ci id="S3.I3.i3.p1.1.1.m1.1.1.cmml" xref="S3.I3.i3.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i3.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i3.p1.1.1.m1.1d">⇒</annotation></semantics></math> Pipeline) Concrete tuning data supplements and optimizes the pipeline design, integrating intelligent interaction into the exploration process.<span class="ltx_text" id="S3.I3.i3.p1.1.1.1">
</span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i4.p1">
<p class="ltx_p" id="S3.I3.i4.p1.1"><span class="ltx_text" id="S3.I3.i4.p1.1.1">(Tuning Data <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I3.i4.p1.1.1.m1.1"><semantics id="S3.I3.i4.p1.1.1.m1.1a"><mo id="S3.I3.i4.p1.1.1.m1.1.1" stretchy="false" xref="S3.I3.i4.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I3.i4.p1.1.1.m1.1b"><ci id="S3.I3.i4.p1.1.1.m1.1.1.cmml" xref="S3.I3.i4.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i4.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i4.p1.1.1.m1.1d">⇒</annotation></semantics></math> Visualization) Based on the tuning data, the model generates data that aligns with the expected visualization views, improving the accuracy of the results. In-depth data analysis provides more visualization design options, enriching the representation of the data.<span class="ltx_text" id="S3.I3.i4.p1.1.1.1"></span></span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1"><span class="ltx_text" id="S3.SS3.p6.1.1">In the context of SRL, based on the detailed pipeline process segmented into forethought, performance, and self-reflection phases, we constructed data for four scenarios in alignment with fine-tuning tasks. For example, to align with the interactive process of self-assessment in the last phase, we constructed fine-tuning data in the form of multi-turn dialogues encompassing question recommendations, answers, and explanations. Additionally, we developed supported instruction fine-tuning data for various visualization views. For instance, summarizing knowledge points involved constructing a knowledge graph with a node-link network view to facilitate the recommendation of multiple related knowledge points. This also provided relational guidance and automatic addition functionalities for the node-link network view.<span class="ltx_text" id="S3.SS3.p6.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.1">User Alignment.</span>
A prototype of the <span class="ltx_text ltx_font_italic" id="S3.SS3.p7.1.2">visualization</span> is used to obtain user suggestions for iterative optimization to capture user intent (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i3" title="Item A3 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A3</span></a>).
From user feedback, tuning data should be refined to improve the performance of <span class="ltx_text ltx_font_italic" id="S3.SS3.p7.1.3">fine-tuned LLM</span>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<ul class="ltx_itemize" id="S3.I4">
<li class="ltx_item" id="S3.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i1.p1">
<p class="ltx_p" id="S3.I4.i1.p1.1"><span class="ltx_text" id="S3.I4.i1.p1.1.1">(Visualization System <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I4.i1.p1.1.1.m1.1"><semantics id="S3.I4.i1.p1.1.1.m1.1a"><mo id="S3.I4.i1.p1.1.1.m1.1.1" stretchy="false" xref="S3.I4.i1.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I4.i1.p1.1.1.m1.1b"><ci id="S3.I4.i1.p1.1.1.m1.1.1.cmml" xref="S3.I4.i1.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i1.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i1.p1.1.1.m1.1d">⇒</annotation></semantics></math> Fine-tuned LLM) The system’s user experience optimizes the model’s recommendation capabilities, enhancing interactive recommendations and intent inferences. By analyzing interaction records, we identify user characteristics and interaction patterns, enabling targeted and personalized improvements to the model’s output.<span class="ltx_text" id="S3.I4.i1.p1.1.1.1">
</span></span></p>
</div>
</li>
<li class="ltx_item" id="S3.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I4.i2.p1">
<p class="ltx_p" id="S3.I4.i2.p1.1"><span class="ltx_text" id="S3.I4.i2.p1.1.1">(Fine-tuned LLM <math alttext="\Rightarrow" class="ltx_Math" display="inline" id="S3.I4.i2.p1.1.1.m1.1"><semantics id="S3.I4.i2.p1.1.1.m1.1a"><mo id="S3.I4.i2.p1.1.1.m1.1.1" stretchy="false" xref="S3.I4.i2.p1.1.1.m1.1.1.cmml">⇒</mo><annotation-xml encoding="MathML-Content" id="S3.I4.i2.p1.1.1.m1.1b"><ci id="S3.I4.i2.p1.1.1.m1.1.1.cmml" xref="S3.I4.i2.p1.1.1.m1.1.1">⇒</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I4.i2.p1.1.1.m1.1c">\Rightarrow</annotation><annotation encoding="application/x-llamapun" id="S3.I4.i2.p1.1.1.m1.1d">⇒</annotation></semantics></math> Visualization System) Integrating the fine-tuned model into each system module provides more intelligent interaction methods to support the initial domain problem.<span class="ltx_text" id="S3.I4.i2.p1.1.1.1"></span></span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p9">
<p class="ltx_p" id="S3.SS3.p9.1"><span class="ltx_text" id="S3.SS3.p9.1.1">In education, we combined the fine-tuned model with the visualization system to form the final system, Tailor-Mind, for prototype user experience collection. Based on user feedback, we encoded user group (students) characteristics into the model to optimize its capabilities. For instance, considering beginners’ traits, we provided question recommendations to simplify the questioning process and encourage divergent thinking. By treating model answers as reference material, we further tuned the model to intelligently extract key information and structured data, helping students grasp essential points.<span class="ltx_text" id="S3.SS3.p9.1.1.1"></span></span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Requirement Analysis of Tailor-Mind</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text" id="S4.p1.1.1">Recognizing the need for effective tools to support SRL,<span class="ltx_text" id="S4.p1.1.1.1"> we conducted a preliminary study to identify the design requirements of the visualization system and specific fine-tuning tasks for intelligent SRL.</span></span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Interviews and Surveys</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We interviewed two domain experts and investigated how students adopt SRL in their studies.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Our research involved two thirty-minute interviews with a distinguished education researcher (E1), who studies students’ learning behaviors, and a university lecturer (E2) who teaches AI. E1 advocated for Zimmerman’s SRL model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib72" title="">72</a>]</cite> to guide our study, emphasizing the need to cultivate students’ initiative and enthusiasm. E2, from a teaching perspective, noted the difficulty students face in linking various knowledge areas. E2 suggested that an ideal ITS should reduce cognitive load, enhance learner engagement, and offer personalized knowledge representation. Both experts concurred on the significance of promoting SRL over passive or task-specific learning methods.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We also recruited students with experience in machine learning and deep learning courses and received 16 responses (7 female, 9 male). This group included 3 Ph.D. students, 8 M.S. students, and 5 undergraduates, all from the Computer Science and Data Science disciplines. The results showed that all participants had attempted SRL but were confused about practical implementations. Their main challenges and needs are depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.F3" title="Figure 3 ‣ 4.1 Interviews and Surveys ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3</span></a>. Notably, almost everyone desired personalized and timely assessments of their learning outcomes.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="203" id="S4.F3.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S4.F3.3.2" style="font-size:90%;">Results of student surveys. The left chart indicates the difficulties encountered by beginners in SRL. The right side demonstrates their need for a visualization system that supports intelligent aids to SRL.
</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Challenges for Self-Regulated Learning</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Through interviews with experts and student surveys, we identified several main challenges during the SRL process.</p>
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C1</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.1.1">Limited knowledge of SRL.</span> E1 indicated that the primary challenge for many students in engaging with self-regulated scientific and effective learning stems from a lack of awareness. This observation aligns with findings from our student surveys.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C2</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Lack of motivation and guidance.</span> E1 mentioned that maintaining enthusiasm and focus is difficult, especially in self-learning. She emphasized that appropriate goal-setting and guidance are crucial for sustaining students’ self-motivation and self-discipline throughout the typical SRL process.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C3</span>
<div class="ltx_para" id="S4.I1.i3.p1">
<p class="ltx_p" id="S4.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.1">Complex and esoteric knowledge.</span> E2 highlighted that a major barrier for many students is the complex organization of knowledge. It is a great challenge to understand, apply, and interconnect different concepts independently. This complexity often overwhelms students, hindering their ability to achieve satisfactory learning outcomes through SRL.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">C4</span>
<div class="ltx_para" id="S4.I1.i4.p1">
<p class="ltx_p" id="S4.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i4.p1.1.1">Lack of immediate feedback.</span> The cost and accessibility of personalized tutoring present significant barriers. Existing methods fall short of providing personalized feedback as well. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.F3" title="Figure 3 ‣ 4.1 Interviews and Surveys ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3</span></a>, assessing their learning progress is challenging.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Requirements for Tailor-Mind</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In response to the challenges outlined in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS2" title="4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4.2</span></a>, we present the following requirements for the intelligent assistance of SRL.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">R1</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i1.p1.1.1">Comprehensive SRL guidance and awareness building.</span> Tailor-Mind should facilitate users in adhering to the SRL process (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i1" title="Item C1 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C1</span></a>) to foster active learning. Leveraging insights from educational models and theories, it’s critical to specify and encode detailed sub-tasks within the foundational SRL framework. We should also educate users about the significance of setting goals, implementing effective learning strategies, and reflecting (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i2" title="Item C2 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C2</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">R2</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i2.p1.1.1">Optimization of learning depth and efficiency.</span> <span class="ltx_text" id="S4.I2.i2.p1.1.2">We must align our teaching goals to provide clear and well-organized explanations.<span class="ltx_text" id="S4.I2.i2.p1.1.2.1"> To support beginners in applying and transferring knowledge while reducing cognitive load, presenting knowledge in a structured and simplified way is crucial (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i3" title="Item C3 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C3</span></a>). Visual representations are important in breaking down complex information and illustrating the relationships between knowledge points.</span></span></p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">R3</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i3.p1.1.1">Personalized learning and adaptive assessments.</span> Tailoring the learning journey to individual needs is crucial. Offering users intelligent recommendations for learning objectives, paths, and content can significantly enhance personalized learning experiences. By analyzing learning performance across various objectives, Tailor-Mind must deliver targeted and suitable feedback to facilitate dynamic and iterative learning, empowering students to progress effectively and adaptively (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i4" title="Item C4 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C4</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">R4</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I2.i4.p1.1.1">Engaging and interactive learning environments.</span> To keep students’ enthusiasm and focus during SRL (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i2" title="Item C2 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C2</span></a>), Tailor-Mind should provide visual guidance throughout the learning process via interactions. Simultaneously, the explanation of intricate knowledge points should be captivating and engaging (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I1.i3" title="Item C3 ‣ 4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">C3</span></a>).</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Tailor-Mind</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we follow the workflow (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.F2" title="Figure 2 ‣ 3.3 Application of the Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">2</span></a>) of the proposed conceptual framework in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.SS2" title="3.2 Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">3.2</span></a> to facilitate the SRL process, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>. Specifically, Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS1" title="5.1 Workflow and Self-Regulated Learning Process ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5.1</span></a> details the learning process and identifies sub-tasks under each stage. In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS2" title="5.2 Fine-tuned LLM for Self-Regulated Learning ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5.2</span></a>, we introduce the fine-tuning tasks and corresponding tuning datasets to align the domain requirements with LLMs (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i1" title="Item A1 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A1</span></a>). We purposely analyze some forms of data construction that consider visualization (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i2" title="Item A2 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A2</span></a>) and user interaction alignment (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i3" title="Item A3 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A3</span></a>). The user interface of Tailor-Mind is illustrated in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS3" title="5.3 User Interface ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5.3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="250" id="S5.F4.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F4.8.4.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S5.F4.6.3" style="font-size:90%;">In applying workflow to SRL in education, we outline the process in three phases. Phase <span class="ltx_text" id="S5.F4.4.1.1" style="position:relative; bottom:-0.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="17" id="S5.F4.4.1.1.g1" src="x9.png" width="17"/></span> involves establishing a fundamental understanding of the SRL task (A1) and collecting data on artificial intelligence (A2). The design requirements (B) align with those outlined in Sec.<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS3" title="4.3 Requirements for Tailor-Mind ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4.3</span></a> from which we derive the tuning tasks. Phase <span class="ltx_text" id="S5.F4.5.2.2" style="position:relative; bottom:-0.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="17" id="S5.F4.5.2.2.g1" src="x10.png" width="17"/></span> details the SRL pipeline sub-tasks and visualizations (C1), leading to the creation of fine-tuning data (C2). In phase <span class="ltx_text" id="S5.F4.6.3.3" style="position:relative; bottom:-0.9pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="17" id="S5.F4.6.3.3.g1" src="x11.png" width="17"/></span>, we enhance the fine-tuning effects and visualization interactions by integrating user feedback within the visualization system.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Workflow and Self-Regulated Learning Process</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Guided by the conceptual framework proposed in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.F1" title="Figure 1 ‣ 3.2 Framework ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">1</span></a>, we have integrated the fine-tuned LLMs into a visualization system that supports intelligent SRL for beginners. The specific implementation workflow is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>. Starting with the domain tasks (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>A1), we conducted a detailed requirement analysis (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>B). To help AI beginners with a comprehensive SRL process (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I2.i1" title="Item R1 ‣ 4.3 Requirements for Tailor-Mind ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">R1</span></a>), we propose a detailed SRL pipeline (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>C1) based on Zimmerman’s model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib72" title="">72</a>]</cite>. The sub-tasks in the pipeline serve as the main objectives for visualization design and fine-tuning work. Finely segmenting the domain space aids in better aligning the model with domain problems. The pipeline consists of three stages: forethought, performance, and self-reflection.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Forethought involves planning and goal-setting. Analyzing user-uploaded learning materials, we recommend personalized learning paths (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I2.i3" title="Item R3 ‣ 4.3 Requirements for Tailor-Mind ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">R3</span></a>), helping beginners organize resources and set achievable goals, thereby enhancing motivation (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I2.i4" title="Item R4 ‣ 4.3 Requirements for Tailor-Mind ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">R4</span></a>).
During the performance stage, beginners employ strategies from the forethought stage. Besides acquiring knowledge from the LLM, we facilitate the application of this knowledge, for instance, through notepads and knowledge mind-maps, enabling learners to track their progress.
Self-reflection, identified as the most challenging stage for beginners, involves synthesizing learning into structured notes. Based on evaluations from tests aligned with set goals, we provide feedback and help students dynamically adjust learning paths for continuous learning experiences (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.I2.i3" title="Item R3 ‣ 4.3 Requirements for Tailor-Mind ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">R3</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Fine-tuned LLM for Self-Regulated Learning</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Through the task identification in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>-<span class="ltx_text" id="S5.SS2.p1.1.1" style="position:relative; bottom:-1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="17" id="S5.SS2.p1.1.1.g1" src="x12.png" width="17"/></span>, we have collected raw data for AI teaching (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>A2), including unlabeled data and conversation data related to the topic.
<span class="ltx_text" id="S5.SS2.p1.1.2">We iterated on the fine-tuning tasks and the specific data formats, prompted by adjustments made after collecting user feedback with the prototype model and system. The initial fine-tuning tasks focused solely on augmenting domain knowledge, aimed at enhancing and expanding knowledge, which did not incorporate visualization design mapping and user alignment. Consequently, we refined the tuning tasks (T1-T3) based on design requirements and raw data (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>B) and established four scenarios.<span class="ltx_text" id="S5.SS2.p1.1.2.1"></span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">T1 Comprehensive and structured knowledge explanations.<span class="ltx_text ltx_font_medium" id="S5.SS2.p2.1.1.1"> Initially, multi-turn dialogues were extracted from raw data. Focusing on learning efficiency and cognitive depth, we integrated Bloom’s Taxonomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib22" title="">22</a>]</cite> into the model’s thought chain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib55" title="">55</a>]</cite> in <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.1.1.1">Open-ended QA</span> (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F5" title="Figure 5 ‣ 5.2 Fine-tuned LLM for Self-Regulated Learning ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5</span></a>A). User feedback analysis has led to incorporating visual suggestions, such as highlighting, in the responses.<span class="ltx_text" id="S5.SS2.p2.1.1.1.2"></span></span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">T2 Extract relationships between different knowledge points.</span> The <span class="ltx_text ltx_font_italic" id="S5.SS2.p3.1.2">Relation Extraction</span> reflects the networked structural data of disciplinary knowledge (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F5" title="Figure 5 ‣ 5.2 Fine-tuned LLM for Self-Regulated Learning ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5</span></a>B). <span class="ltx_text" id="S5.SS2.p3.1.3">After the initial iteration, to assist users in linking paragraph text to network visualization, we expanded this task to arbitrary texts, aiding users in obtaining personalized relationship recommendations based on model responses as references.<span class="ltx_text" id="S5.SS2.p3.1.3.1"> Fine-tuning for data features helps bridge the gap between model output and data visualization, thus enhancing user comprehension and interaction.</span></span></p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">T3 Intelligent recommendations.<span class="ltx_text ltx_font_medium" id="S5.SS2.p4.1.1.1"> Intelligent recommendations primarily involve suggesting <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.1.1.1">Tests &amp; Answers</span> and <span class="ltx_text ltx_font_italic" id="S5.SS2.p4.1.1.1.2">Question Recommendation</span>. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F5" title="Figure 5 ‣ 5.2 Fine-tuned LLM for Self-Regulated Learning ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5</span></a>C, aligned with the self-reflection stage of the pipeline, we utilized a multi-turn dialogue format. This strategy provides the model with a contextual environment conducive to timely feedback, incorporating phases of question recommendation, answering, and detailed explanations.<span class="ltx_text" id="S5.SS2.p4.1.1.1.3"> Through the visualization system, the model recommends questions at different learning levels (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F5" title="Figure 5 ‣ 5.2 Fine-tuned LLM for Self-Regulated Learning ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5</span></a>D), guiding beginners in a structured and comprehensive learning approach.</span></span></span></p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S5.F5.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S5.F5.4.2" style="font-size:90%;">Refined fine-tuning datasets with examples, where different highlights indicate various alignments. Data construction for tuning tasks (A for T1, B for T2, C and D for T3) has undergone one iteration.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Based on the described tasks and scenarios, we have compiled a total of 74,932 fine-tuning data entries. The construction of tuning data involves extracting multi-turn dialogues from reference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib59" title="">59</a>]</cite> and invoking terms. The fine-tuning process is developed on the open-source LLM Baichuan2-7B-chat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib58" title="">58</a>]</cite>, which is trained on a high-quality corpus of 2.6 trillion tokens, achieving a high level of performance in both English and Chinese. We conducted supervised low-rank adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib13" title="">13</a>]</cite> fine-tuning on the constructed tuning dataset, which endows the model with knowledge reasoning and educational behavior patterns. The training process had a learning rate of 5e-5, underwent 3 training epochs, and was completed on 4×4090 GPUs. We describe the entire data construction and fine-tuning process in Supplementary Materials.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>User Interface</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">We design a visual interface including the views and interactions representative of the SRL pipeline outlined in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.SS1" title="5.1 Workflow and Self-Regulated Learning Process ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">5.1</span></a>. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S5.F4" title="Figure 4 ‣ 5 Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4</span></a>-<span class="ltx_text" id="S5.SS3.p1.1.1" style="position:relative; bottom:-1.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="17" id="S5.SS3.p1.1.1.g1" src="x14.png" width="17"/></span>, we consider the alignment of the user interaction with the fine-tuned model.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.1 </span>Chat View</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">The Chat View (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A) serves as a primary gateway, supporting the upload of learning materials and providing a guided introduction to SRL. We consider the presentation of LLM outputs in the Chat View as a visual representation of knowledge, summarizing the multi-turn dialogues between users and LLMs as interactions. <span class="ltx_text" id="S5.SS3.SSS1.p1.1.1">Therefore, to reduce cognitive load, we will emphasize logical connectives in responses such as <span class="ltx_text ltx_font_italic" id="S5.SS3.SSS1.p1.1.1.1">For instance</span> (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A2). In general, the responses will be carried out according to the following steps: interpreting the question, explaining key knowledge points, giving examples, and summarizing.<span class="ltx_text" id="S5.SS3.SSS1.p1.1.1.2"> The visualization of knowledge extends beyond text dialogues to include rich text formats, such as button selections (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A1). As illustrated in Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A3, selecting some buttons about knowledge relationships can trigger the addition of new nodes and edges in the Knowledge MindMap.</span></span></p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.2 </span>File Preview</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">The File Preview (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>B) provides users with a preview of the uploaded learning materials (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>B1). Given the fine-tuned model’s limitations in multimodal processing, ChatGPT is employed to parse the learning materials, thereby ensuring the precision of the subsequent outcomes. The knowledge structure tree widget (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>B2) is constructed based on the relationships of knowledge points within the file. Users can interact through clicks to view the corresponding knowledge cards (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>B3). These cards shed light on the significance or application of the respective knowledge points in the context of the material. Each knowledge card features the ability to copy and ask questions, which facilitates a seamless transition to the Chat View for in-depth explanation and reduces aimless exploration for beginners.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.3 </span>Question Recommendation</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">Integrating Bloom’s Taxonomy with educational objectives, we classify eight learning levels from basic to advanced. The fine-tuned model recommends questions based on the knowledge points extracted from the file across the eight learning levels. Additionally, each question supports content copying and resetting (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>E).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.4 </span>Learning Path</h4>
<div class="ltx_para" id="S5.SS3.SSS4.p1">
<p class="ltx_p" id="S5.SS3.SSS4.p1.1">We depict key knowledge points along the learning path as milestones, visualized as small flags in Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>F. Each flag’s color corresponds to the learning level required for that knowledge point, and flag height represents the importance. Larger flags signify greater significance. Beneath each flag, specific expressions of knowledge are encoded as colored dots, with colors matching those used system-wide for learning levels and sizes denoting importance. Hovering over these flags reveals detailed expressions derived from the model. Numerical analysis is facilitated by stacked bar charts that tally these colored dots, allowing for comparison of learning paths before and after the self-reflection stage. The timeline on which these milestones are placed uses a relative scale to represent time spent between them, as we lack direct access to the users’ actual learning abilities.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS4.p2">
<p class="ltx_p" id="S5.SS3.SSS4.p2.1">Incorporating the importance and relevance of knowledge points, the system offers personalized recommendations. Users can customize their learning path within the Knowledge MindMap using reset, edit, and submit buttons to adjust milestone data as needed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.3.5 </span>Knowledge MindMap</h4>
<div class="ltx_para" id="S5.SS3.SSS5.p1">
<p class="ltx_p" id="S5.SS3.SSS5.p1.1"><span class="ltx_text" id="S5.SS3.SSS5.p1.1.1">A knowledge point is a fundamental concept or piece of information that serves as a building block, enabling students to incrementally understand complex AI systems.<span class="ltx_text" id="S5.SS3.SSS5.p1.1.1.1"> Based on this, we represent knowledge point entities as nodes and illustrate the logical relationships between them as edges. The nodes’ color and size mirror the design mappings used for milestone flags in the Learning Path. At the same time, tutors suggest that we correspond the relationships between knowledge points to learning levels, ensuring logical consistency and reducing cognitive load for users. To display the structural features of knowledge points, we support various layouts for the network structure (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>C1), including the “dagre” layout to show hierarchical relationships and the “concentric” layout to highlight core knowledge points. Each node supports recommended questions, setting goals, and taking notes (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>C3). Questions recommended regarding this knowledge point can be discussed directly in Chat View (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A1). After completing the note-taking, the model processes it into a word cloud returned on the selected node (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>C2), which is convenient for preview and can serve as a learning marker. Users can also customize the network structure based on their understanding of knowledge points, performing operations such as adding, deleting, and editing nodes or edges. In addition to taking notes on a particular knowledge point node, users can also record any discoveries in Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>D.</span></span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we evaluate the fine-tuned LLM (SFT-2.0) by comparing the performances of the other four models on the test datasets through both human and OpenAI GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib32" title="">32</a>]</cite> (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS1" title="6.1 Model Performance ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6.1</span></a>). Two usage scenarios are narrated to illustrate how Tailor-Mind can help the SRL process in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS2" title="6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6.2</span></a>. We further conduct an in-person study and interviews with participants and analyze the results in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.SS3" title="6.3 User Study ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6.3</span></a>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Model Performance</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">There is a lack of authoritative benchmark datasets for evaluation in AI education. The generic benchmark dataset, such as AGIEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib71" title="">71</a>]</cite> and C-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib16" title="">16</a>]</cite>, is intended to general models and is not suitable to test a specific model’s output with a standardized structure. Therefore, we consider constructing a dedicated test dataset, setting subjective evaluation criteria, and comparing the performances among several models. The evaluation results are shown in Table. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.T1" title="Table 1 ‣ 6.1 Model Performance ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">1</span></a>. Detailed evaluation results and analyses are available in the Supplementary Materials.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Settings.</span>
<span class="ltx_text" id="S6.SS1.p2.1.2">We generated a dataset comprising 280 test data entries for seven fine-tuning tasks across eight AI subdomains using GPT-4, aiming to cover a wide range of issues within the domain as comprehensively as possible. Each task included five questions of varying difficulties, each accompanied by an optimal answer for subsequent evaluation reference. This dataset assesses LLMs’ accuracy and depth in understanding and addressing domain-specific issues, thereby reflecting the models’ ability to assimilate and apply domain knowledge (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i1" title="Item A1 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A1</span></a>).<span class="ltx_text" id="S6.SS1.p2.1.2.1"> Throughout this process, we conducted manual reviews and engaged in self-reflection with GPT-4 to ensure the dataset’s accuracy<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib18" title="">18</a>]</cite>.
We selected the Base-model, EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib7" title="">7</a>]</cite>, OpenAI GPT-3.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib31" title="">31</a>]</cite> and the SFT-1.0 model (without user alignment and visualization alignment), to compare with the final fine-tuned model SFT-2.0. <span class="ltx_text" id="S6.SS1.p2.1.2.1.1">These models were chosen to facilitate a multifaceted comparison, including domain expertise, output consistency, and alignment requirements, as detailed in the Supplementary Material.<span class="ltx_text" id="S6.SS1.p2.1.2.1.1.1"> To ensure fairness and objectivity, model information was kept undisclosed to referees.</span></span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Methods.</span>
<span class="ltx_text" id="S6.SS1.p3.1.2">To assess model performances and their alignment with user perceptions (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i3" title="Item A3 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A3</span></a>), we introduced a referee model to simulate real-world scenarios.<span class="ltx_text" id="S6.SS1.p3.1.2.1"> Leveraging GPT-4, known for its alignment with controlled and crowdsourced human preferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib70" title="">70</a>]</cite>, we employed it to evaluate outputs based on <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.1">Accuracy</span>, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.2">Completeness</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.3">Clarity</span>, scoring each criterion from 0 to 5. <span class="ltx_text" id="S6.SS1.p3.1.2.1.4">These criteria facilitate the evaluation of model outputs for consistency with design intentions, data characteristics, and domain insights (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i2" title="Item A2 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A2</span></a>), with interpretations varying slightly across different tasks.<span class="ltx_text" id="S6.SS1.p3.1.2.1.4.1"> Specifically, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.4.1.1">Accuracy</span> assesses alignment with the reference answer in content, semantics, and structure, <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.4.1.2">Completeness</span> ensures no detail is overlooked, and <span class="ltx_text ltx_font_italic" id="S6.SS1.p3.1.2.1.4.1.3">Clarity</span> evaluates logical coherence and clear expression. To mitigate bias, ground truth is provided. <span class="ltx_text" id="S6.SS1.p3.1.2.1.4.1.4">Scores are averaged over multiple rounds to capture different dimensions and simulate user interactions, addressing uncertainties and the exploratory nature of model feedback (<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S3.I1.i3" title="Item A3 ‣ 3.1 Problem Definition ‣ 3 Fine-Tuned LLM for Visualization System ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">A3</span></a>).<span class="ltx_text" id="S6.SS1.p3.1.2.1.4.1.4.1"> Additionally, seven AI experts manually rated these criteria to enrich the evaluation process.</span></span></span></span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Results.</span>
From Table. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.T1" title="Table 1 ‣ 6.1 Model Performance ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">1</span></a>, the SFT-2.0 model outperforms the other models in all aspects of the evaluation by humans. The following findings are drawn from the results:
(1) The SFT-2.0 model’s responses are accurate and follow the logic of knowledge presentation. Experts generally indicated that the model output was highly structured and could be aligned with the subsequent visualization design. However, other models, even when given a detailed prompt, still did not fulfill all the requirements.
(2) The SFT-2.0 model exhibits the most stable performance with the smallest variance across the three criteria, primarily due to the benefits and effects of fine-tuning. The stability of model outputs is particularly important for user interaction and presentation in the visualization system.
(3) The SFT-2.0 model is more in line with user preferences. Although we emphasized that the referee model should not be influenced by response length when scoring completeness, it still tended to judge longer responses as better. This issue particularly existed in the Question Recommendation task, leading to the result in Table. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.T1" title="Table 1 ‣ 6.1 Model Performance ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">1</span></a> that considered GPT-3.5’s answers more complete. Experts corrected this by pointing out that <span class="ltx_text ltx_font_italic" id="S6.SS1.p4.1.2">"GPT-3.5’s answers are redundant and not conducive to direct understanding, and the results from the SFT-2.0 model are more suitable for beginners"</span>.</p>
</div>
<figure class="ltx_table" id="S6.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S6.T1.30">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T1.30.31.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S6.T1.30.31.1.1" rowspan="2"><span class="ltx_text" id="S6.T1.30.31.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4" id="S6.T1.30.31.1.2">Evaluation by Human</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4" id="S6.T1.30.31.1.3">Evaluation by GPT-4</td>
</tr>
<tr class="ltx_tr" id="S6.T1.30.32.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.1">ACC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.2">CPL</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.3">CLR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.4">Average</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.5">ACC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.6">CPL</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.7">CLR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.30.32.2.8">Average</td>
</tr>
<tr class="ltx_tr" id="S6.T1.6.6">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S6.T1.6.6.7">Base-model</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.1.1.1">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.1.1.1.1">3.68</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.1.1.1.m1.1"><semantics id="S6.T1.1.1.1.m1.1a"><mo id="S6.T1.1.1.1.m1.1.1" xref="S6.T1.1.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.1.1.1.m1.1b"><csymbol cd="latexml" id="S6.T1.1.1.1.m1.1.1.cmml" xref="S6.T1.1.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.1.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.1.1.1.m1.1d">±</annotation></semantics></math>1.22)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.2.2.2">3.71 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.2.2.2.m1.1"><semantics id="S6.T1.2.2.2.m1.1a"><mo id="S6.T1.2.2.2.m1.1.1" xref="S6.T1.2.2.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.2.2.2.m1.1b"><csymbol cd="latexml" id="S6.T1.2.2.2.m1.1.1.cmml" xref="S6.T1.2.2.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.2.2.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.2.2.2.m1.1d">±</annotation></semantics></math>1.22)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.3.3.3">3.28 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.3.3.3.m1.1"><semantics id="S6.T1.3.3.3.m1.1a"><mo id="S6.T1.3.3.3.m1.1.1" xref="S6.T1.3.3.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.3.3.3.m1.1b"><csymbol cd="latexml" id="S6.T1.3.3.3.m1.1.1.cmml" xref="S6.T1.3.3.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.3.3.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.3.3.3.m1.1d">±</annotation></semantics></math>1.80)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.6.6.8">3.55</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.4.4.4">3.54 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.4.4.4.m1.1"><semantics id="S6.T1.4.4.4.m1.1a"><mo id="S6.T1.4.4.4.m1.1.1" xref="S6.T1.4.4.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.4.4.4.m1.1b"><csymbol cd="latexml" id="S6.T1.4.4.4.m1.1.1.cmml" xref="S6.T1.4.4.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.4.4.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.4.4.4.m1.1d">±</annotation></semantics></math>1.91)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.5.5.5">3.86 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.5.5.5.m1.1"><semantics id="S6.T1.5.5.5.m1.1a"><mo id="S6.T1.5.5.5.m1.1.1" xref="S6.T1.5.5.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.5.5.5.m1.1b"><csymbol cd="latexml" id="S6.T1.5.5.5.m1.1.1.cmml" xref="S6.T1.5.5.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.5.5.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.5.5.5.m1.1d">±</annotation></semantics></math>1.77)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.6.6.6">3.75 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.6.6.6.m1.1"><semantics id="S6.T1.6.6.6.m1.1a"><mo id="S6.T1.6.6.6.m1.1.1" xref="S6.T1.6.6.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.6.6.6.m1.1b"><csymbol cd="latexml" id="S6.T1.6.6.6.m1.1.1.cmml" xref="S6.T1.6.6.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.6.6.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.6.6.6.m1.1d">±</annotation></semantics></math>1.54)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S6.T1.6.6.9">3.72</td>
</tr>
<tr class="ltx_tr" id="S6.T1.12.12">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S6.T1.12.12.7">EduChat</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.7.7.1">3.45 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.7.7.1.m1.1"><semantics id="S6.T1.7.7.1.m1.1a"><mo id="S6.T1.7.7.1.m1.1.1" xref="S6.T1.7.7.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.7.7.1.m1.1b"><csymbol cd="latexml" id="S6.T1.7.7.1.m1.1.1.cmml" xref="S6.T1.7.7.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.7.7.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.7.7.1.m1.1d">±</annotation></semantics></math>2.33)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.8.8.2">3.42 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.8.8.2.m1.1"><semantics id="S6.T1.8.8.2.m1.1a"><mo id="S6.T1.8.8.2.m1.1.1" xref="S6.T1.8.8.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.8.8.2.m1.1b"><csymbol cd="latexml" id="S6.T1.8.8.2.m1.1.1.cmml" xref="S6.T1.8.8.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.8.8.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.8.8.2.m1.1d">±</annotation></semantics></math>2.32)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.9.9.3">2.96 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.9.9.3.m1.1"><semantics id="S6.T1.9.9.3.m1.1a"><mo id="S6.T1.9.9.3.m1.1.1" xref="S6.T1.9.9.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.9.9.3.m1.1b"><csymbol cd="latexml" id="S6.T1.9.9.3.m1.1.1.cmml" xref="S6.T1.9.9.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.9.9.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.9.9.3.m1.1d">±</annotation></semantics></math>2.65)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.12.12.8">3.28</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.10.10.4">3.11 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.10.10.4.m1.1"><semantics id="S6.T1.10.10.4.m1.1a"><mo id="S6.T1.10.10.4.m1.1.1" xref="S6.T1.10.10.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.10.10.4.m1.1b"><csymbol cd="latexml" id="S6.T1.10.10.4.m1.1.1.cmml" xref="S6.T1.10.10.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.10.10.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.10.10.4.m1.1d">±</annotation></semantics></math>2.98)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.11.11.5">3.45 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.11.11.5.m1.1"><semantics id="S6.T1.11.11.5.m1.1a"><mo id="S6.T1.11.11.5.m1.1.1" xref="S6.T1.11.11.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.11.11.5.m1.1b"><csymbol cd="latexml" id="S6.T1.11.11.5.m1.1.1.cmml" xref="S6.T1.11.11.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.11.11.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.11.11.5.m1.1d">±</annotation></semantics></math>2.91)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.12.12.6">3.32 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.12.12.6.m1.1"><semantics id="S6.T1.12.12.6.m1.1a"><mo id="S6.T1.12.12.6.m1.1.1" xref="S6.T1.12.12.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.12.12.6.m1.1b"><csymbol cd="latexml" id="S6.T1.12.12.6.m1.1.1.cmml" xref="S6.T1.12.12.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.12.12.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.12.12.6.m1.1d">±</annotation></semantics></math>2.04)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.12.12.9">3.30</td>
</tr>
<tr class="ltx_tr" id="S6.T1.18.18">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S6.T1.18.18.7">GPT-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.13.13.1">4.11 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.13.13.1.m1.1"><semantics id="S6.T1.13.13.1.m1.1a"><mo id="S6.T1.13.13.1.m1.1.1" xref="S6.T1.13.13.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.13.13.1.m1.1b"><csymbol cd="latexml" id="S6.T1.13.13.1.m1.1.1.cmml" xref="S6.T1.13.13.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.13.13.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.13.13.1.m1.1d">±</annotation></semantics></math>0.60)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.14.14.2">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.14.14.2.1">3.93</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.14.14.2.m1.1"><semantics id="S6.T1.14.14.2.m1.1a"><mo id="S6.T1.14.14.2.m1.1.1" xref="S6.T1.14.14.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.14.14.2.m1.1b"><csymbol cd="latexml" id="S6.T1.14.14.2.m1.1.1.cmml" xref="S6.T1.14.14.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.14.14.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.14.14.2.m1.1d">±</annotation></semantics></math>0.58)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.15.15.3">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.15.15.3.1">3.79</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.15.15.3.m1.1"><semantics id="S6.T1.15.15.3.m1.1a"><mo id="S6.T1.15.15.3.m1.1.1" xref="S6.T1.15.15.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.15.15.3.m1.1b"><csymbol cd="latexml" id="S6.T1.15.15.3.m1.1.1.cmml" xref="S6.T1.15.15.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.15.15.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.15.15.3.m1.1d">±</annotation></semantics></math>1.44)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.18.18.8"><span class="ltx_text ltx_ulem_uline" id="S6.T1.18.18.8.1">3.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.16.16.4">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.16.16.4.1">4.09</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.16.16.4.m1.1"><semantics id="S6.T1.16.16.4.m1.1a"><mo id="S6.T1.16.16.4.m1.1.1" xref="S6.T1.16.16.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.16.16.4.m1.1b"><csymbol cd="latexml" id="S6.T1.16.16.4.m1.1.1.cmml" xref="S6.T1.16.16.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.16.16.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.16.16.4.m1.1d">±</annotation></semantics></math>1.34)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.17.17.5">
<span class="ltx_text ltx_font_bold" id="S6.T1.17.17.5.1">4.09</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.17.17.5.m1.1"><semantics id="S6.T1.17.17.5.m1.1a"><mo id="S6.T1.17.17.5.m1.1.1" xref="S6.T1.17.17.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.17.17.5.m1.1b"><csymbol cd="latexml" id="S6.T1.17.17.5.m1.1.1.cmml" xref="S6.T1.17.17.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.17.17.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.17.17.5.m1.1d">±</annotation></semantics></math>1.21)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.18.18.6">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.18.18.6.1">3.98</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.18.18.6.m1.1"><semantics id="S6.T1.18.18.6.m1.1a"><mo id="S6.T1.18.18.6.m1.1.1" xref="S6.T1.18.18.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.18.18.6.m1.1b"><csymbol cd="latexml" id="S6.T1.18.18.6.m1.1.1.cmml" xref="S6.T1.18.18.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.18.18.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.18.18.6.m1.1d">±</annotation></semantics></math>0.66)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.18.18.9"><span class="ltx_text ltx_ulem_uline" id="S6.T1.18.18.9.1">4.05</span></td>
</tr>
<tr class="ltx_tr" id="S6.T1.24.24">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r" id="S6.T1.24.24.7">SFT-1.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.19.19.1">3.48 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.19.19.1.m1.1"><semantics id="S6.T1.19.19.1.m1.1a"><mo id="S6.T1.19.19.1.m1.1.1" xref="S6.T1.19.19.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.19.19.1.m1.1b"><csymbol cd="latexml" id="S6.T1.19.19.1.m1.1.1.cmml" xref="S6.T1.19.19.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.19.19.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.19.19.1.m1.1d">±</annotation></semantics></math>1.09)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.20.20.2">3.29 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.20.20.2.m1.1"><semantics id="S6.T1.20.20.2.m1.1a"><mo id="S6.T1.20.20.2.m1.1.1" xref="S6.T1.20.20.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.20.20.2.m1.1b"><csymbol cd="latexml" id="S6.T1.20.20.2.m1.1.1.cmml" xref="S6.T1.20.20.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.20.20.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.20.20.2.m1.1d">±</annotation></semantics></math>1.17)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.21.21.3">3.22 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.21.21.3.m1.1"><semantics id="S6.T1.21.21.3.m1.1a"><mo id="S6.T1.21.21.3.m1.1.1" xref="S6.T1.21.21.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.21.21.3.m1.1b"><csymbol cd="latexml" id="S6.T1.21.21.3.m1.1.1.cmml" xref="S6.T1.21.21.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.21.21.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.21.21.3.m1.1d">±</annotation></semantics></math>1.21)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.24.24.8">3.33</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.22.22.4">2.97 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.22.22.4.m1.1"><semantics id="S6.T1.22.22.4.m1.1a"><mo id="S6.T1.22.22.4.m1.1.1" xref="S6.T1.22.22.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.22.22.4.m1.1b"><csymbol cd="latexml" id="S6.T1.22.22.4.m1.1.1.cmml" xref="S6.T1.22.22.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.22.22.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.22.22.4.m1.1d">±</annotation></semantics></math>2.29)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.23.23.5">3.00 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.23.23.5.m1.1"><semantics id="S6.T1.23.23.5.m1.1a"><mo id="S6.T1.23.23.5.m1.1.1" xref="S6.T1.23.23.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.23.23.5.m1.1b"><csymbol cd="latexml" id="S6.T1.23.23.5.m1.1.1.cmml" xref="S6.T1.23.23.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.23.23.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.23.23.5.m1.1d">±</annotation></semantics></math>2.03)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.24.24.6">3.35 (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.24.24.6.m1.1"><semantics id="S6.T1.24.24.6.m1.1a"><mo id="S6.T1.24.24.6.m1.1.1" xref="S6.T1.24.24.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.24.24.6.m1.1b"><csymbol cd="latexml" id="S6.T1.24.24.6.m1.1.1.cmml" xref="S6.T1.24.24.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.24.24.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.24.24.6.m1.1d">±</annotation></semantics></math>1.73)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S6.T1.24.24.9">3.10</td>
</tr>
<tr class="ltx_tr" id="S6.T1.30.30">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r" id="S6.T1.30.30.7">SFT-2.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.25.25.1">
<span class="ltx_text ltx_font_bold" id="S6.T1.25.25.1.1">4.40</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.25.25.1.m1.1"><semantics id="S6.T1.25.25.1.m1.1a"><mo id="S6.T1.25.25.1.m1.1.1" xref="S6.T1.25.25.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.25.25.1.m1.1b"><csymbol cd="latexml" id="S6.T1.25.25.1.m1.1.1.cmml" xref="S6.T1.25.25.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.25.25.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.25.25.1.m1.1d">±</annotation></semantics></math>0.51)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.26.26.2">
<span class="ltx_text ltx_font_bold" id="S6.T1.26.26.2.1">4.03</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.26.26.2.m1.1"><semantics id="S6.T1.26.26.2.m1.1a"><mo id="S6.T1.26.26.2.m1.1.1" xref="S6.T1.26.26.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.26.26.2.m1.1b"><csymbol cd="latexml" id="S6.T1.26.26.2.m1.1.1.cmml" xref="S6.T1.26.26.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.26.26.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.26.26.2.m1.1d">±</annotation></semantics></math>0.58)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.27.27.3">
<span class="ltx_text ltx_font_bold" id="S6.T1.27.27.3.1">4.46</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.27.27.3.m1.1"><semantics id="S6.T1.27.27.3.m1.1a"><mo id="S6.T1.27.27.3.m1.1.1" xref="S6.T1.27.27.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.27.27.3.m1.1b"><csymbol cd="latexml" id="S6.T1.27.27.3.m1.1.1.cmml" xref="S6.T1.27.27.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.27.27.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.27.27.3.m1.1d">±</annotation></semantics></math>0.54)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.30.30.8"><span class="ltx_text ltx_font_bold" id="S6.T1.30.30.8.1">4.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.28.28.4">
<span class="ltx_text ltx_font_bold" id="S6.T1.28.28.4.1">4.15</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.28.28.4.m1.1"><semantics id="S6.T1.28.28.4.m1.1a"><mo id="S6.T1.28.28.4.m1.1.1" xref="S6.T1.28.28.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.28.28.4.m1.1b"><csymbol cd="latexml" id="S6.T1.28.28.4.m1.1.1.cmml" xref="S6.T1.28.28.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.28.28.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.28.28.4.m1.1d">±</annotation></semantics></math>0.98)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.29.29.5">
<span class="ltx_text ltx_ulem_uline" id="S6.T1.29.29.5.1">4.06</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.29.29.5.m1.1"><semantics id="S6.T1.29.29.5.m1.1a"><mo id="S6.T1.29.29.5.m1.1.1" xref="S6.T1.29.29.5.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.29.29.5.m1.1b"><csymbol cd="latexml" id="S6.T1.29.29.5.m1.1.1.cmml" xref="S6.T1.29.29.5.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.29.29.5.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.29.29.5.m1.1d">±</annotation></semantics></math>0.97)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.30.30.6">
<span class="ltx_text ltx_font_bold" id="S6.T1.30.30.6.1">4.39</span> (<math alttext="\pm" class="ltx_Math" display="inline" id="S6.T1.30.30.6.m1.1"><semantics id="S6.T1.30.30.6.m1.1a"><mo id="S6.T1.30.30.6.m1.1.1" xref="S6.T1.30.30.6.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S6.T1.30.30.6.m1.1b"><csymbol cd="latexml" id="S6.T1.30.30.6.m1.1.1.cmml" xref="S6.T1.30.30.6.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.T1.30.30.6.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S6.T1.30.30.6.m1.1d">±</annotation></semantics></math>0.58)</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S6.T1.30.30.9"><span class="ltx_text ltx_font_bold" id="S6.T1.30.30.9.1">4.20</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S6.T1.34.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S6.T1.35.2" style="font-size:90%;">Evaluation of model performance using metrics ACC (Accuracy), CPL (Completeness), and CLR (Clarity), where <span class="ltx_text ltx_font_bold" id="S6.T1.35.2.1">bold</span> indicates the best result and <span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T1.35.2.2">underline</span> the second best. Our model (SFT-2.0) performs well in both human and GPT-4 assessments.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Usage Scenario</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We illustrate usage scenarios with Tailor-Mind from two perspectives: a beginner’s enhanced understanding of the Transformer model and a beginner’s preparatory journey in Reinforcement Learning (RL).</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Integrating Knowledge and Deepening Understanding</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Evelyn is a data analyst who needs to make sequence predictions for her current work. <span class="ltx_text" id="S6.SS2.SSS1.p1.1.1">Therefore, she uploads the authoritative learning material that she found on a website, hoping to understand the Transformer better and determine whether it meets her work requirements.<span class="ltx_text" id="S6.SS2.SSS1.p1.1.1.1"> In the forethought phase, she discovers that the knowledge points listed in the File Preview (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>B2) seemed familiar, but there was significant forgetfulness and a lack of understanding of how they are related to each other. After understanding the file structure and learning path, Evelyn begins her study of model components. By the time she reaches the final milestone, she locates the “Transformer Encoder” node in the Knowledge MindMap (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>C4) and continues the study based on the recommended questions. She selects the first candidate question (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A1) and intends to understand the encoder’s composition against the corresponding part in the material. The clear and structured response in Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A2 satisfies her, and she says, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.1.1.1">"This example shows me that such a structure represents an encoder block, and it takes multiple encoder blocks to make up an encoder layer"</span>.</span></span></p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">The multiple relationships suggested at the bottom of the response catch her attention, and she chooses the last button to add to her customized MindMap (Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education</span></span>A3) as she has previously learned that the advent of the Transformer replaces many of the scenarios in which Recurrent Neural Networks (RNNs) are used. While editing the added RNNs node, she notices that the only node connected from the “Transformer Encoder” is “Parallel”, which is highly recommended in terms of importance. Therefore, she continues to ask for the recommended questions related to “Parallel” and selects the option “What is parallel processing, and how does it differ from sequential processing?”. After understanding the answer, she says, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p2.1.1">"I always knew that the Transformer was superior to RNNs, but I never understood the specific reasons. Now I realize that it’s the Transformer’s capability for parallelization that makes it better at handling long sequence data, which aligns well with my upcoming work requirements"</span>.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p3">
<p class="ltx_p" id="S6.SS2.SSS1.p3.1">In the self-reflection stage, she is asked to answer the question about why Transformer is superior to RNNs (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.F6" title="Figure 6 ‣ 6.2.2 Stimulating Interest and Exploratory Learning ‣ 6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6</span></a>A). She easily chooses the correct answer and gains a deeper understanding of the point. Moreover, Evelyn expresses that this process allows her to integrate many fragmented pieces of knowledge, enriching her knowledge network.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Stimulating Interest and Exploratory Learning</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">Rex, a senior undergraduate student, is asked to do a preview of the course material about RL. As a result, he seeks the help of Tailor-Mind to sort through the material highlights and lighten his class load. After uploading the material, he follows the recommended learning path for question-driven learning in RL concepts, problems, and core components. While exploring the Knowledge MindMap, he is attracted to the “Game Playing” node and triggers the question recommendation function (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.F6" title="Figure 6 ‣ 6.2.2 Stimulating Interest and Exploratory Learning ‣ 6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6</span></a>B). After understanding the “Game Playing” by RL, he becomes more interested in the upcoming course. Rex says, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.1.1">"Without the intuitive navigation of the RL application provided by Knowledge MindMap, I might regard RL as a somewhat boring topic"</span>.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">Rex’s careful and diligent study during the performance stage helps him successfully complete most of the test questions, which filled him with anticipation for upcoming lessons. However, he encounters a mistake due to a vague understanding when answering questions about the “Reward Function”. The learning path reminds him to reinforce his understanding of the reward function, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.F6" title="Figure 6 ‣ 6.2.2 Stimulating Interest and Exploratory Learning ‣ 6.2 Usage Scenario ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">6</span></a>C. Simultaneously, he also brings this question into the classroom.</p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S6.F6.g1" src="x15.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S6.F6.3.2" style="font-size:90%;">Usage Scenarios for Tailor-Mind. (A) is about the scenario of consolidating understanding. (B) and (C) are intermediate processes in exploratory learning.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>User Study</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">We demonstrated the effectiveness of Tailor-Mind through a user study.
We designed a comparative experiment to facilitate learning of the Transformer model, while the control group employed solely GPT-4.
With challenges in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S4.SS2" title="4.2 Challenges for Self-Regulated Learning ‣ 4 Requirement Analysis of Tailor-Mind ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">4.2</span></a> and expert recommendation, we observed 7 metrics of participants’ behaviors throughout the process.
They are study duration, the number of questions attempted, question level, study plan adoption, study plan completion rate, note-taking practice, and engagement in self-reflection.
Detailed procedures and experimental records can be found in our Supplementary Materials.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1 </span>Experimental Set-up</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1"><span class="ltx_text" id="S6.SS3.SSS1.p1.1.1">We recruited 24 participants with a background in computer science who have not previously studied the Transformer model. Among them, 8 were undergraduate students, 16 were postgraduate students, and 14 identified themselves as male, 10 as female. We assessed the participants’ understanding of SRL and their usual study habits through a pre-study questionnaire and accordingly divided them into two groups, Group T and Group C, with 12 members in each. Group T (T1-T12) utilized the Tailor-Mind to complete SRL tasks, and Group C (C1-C12) used the state-of-the-art LLM GPT-4.<span class="ltx_text" id="S6.SS3.SSS1.p1.1.1.1"></span></span></p>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p2">
<p class="ltx_p" id="S6.SS3.SSS1.p2.1">We conducted an in-person observational experiment for each participant. The session began with a briefing on the purpose of our user study and an explanation of the whole process.
After a concise 3-5 minute tutorial introducing the participants to SRL’s background knowledge, concepts, and procedures, we provided an introduction and tutorial on the visual and interactive features of Tailor-Mind for Group T. An observational study was conducted with all participants to assess their learning experience with specified material on the Transformer model. Throughout this process, the 7 metrics were systematically recorded. <span class="ltx_text" id="S6.SS3.SSS1.p2.1.1">The results of these observational metrics are documented in the Supplementary Material.<span class="ltx_text" id="S6.SS3.SSS1.p2.1.1.1">
Upon completing the SRL tasks, participants answered objective questions to assess their learning performance. Additionally, face-to-face interviews were conducted to explore insights and issues observed. Group T participants also responded to supplementary subjective questions regarding their use of Tailor-Mind.</span></span></p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="944" id="S6.F7.g1" src="x16.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="S6.F7.3.2" style="font-size:90%;">User study results. The box-plot (A) displays the performance outcomes of two groups on objective experimental questions. The number of asterisks (*) in the upper indicates the significance level of the test (*, **, *** for p &lt;0.05, 0.01, and 0.005, respectively). (B) presents the detailed objective experimental questions and the corresponding distribution of satisfaction levels. Q1 through 3 pertain to Usability; Q4 to 6 focus on Effectiveness; Q7 indicates Customization; and Q8 and Q9 are about Recommendation. The results show that Tailor-Mind improves learning performance and receives good user feedback.</span></figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2 </span>Results and Analysis</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.SSS2.p1.1.1">Enhanced efficiency in self-regulated learning.</span>
In addition to better performance on all objective questions, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.F7" title="Figure 7 ‣ 6.3.1 Experimental Set-up ‣ 6.3 User Study ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">7</span></a>A, we have observational results and a summary of interviews that corroborate this finding.
First, Tailor-Mind’s responses are so precise and comprehensive that they minimize cognitive load while elaborating key concepts, making it particularly beneficial for novices.
Participants T2, T7, and T10 expressed appreciation for examples provided in explanations that aided in understanding abstract concepts.
T5 favored the inclusion of summaries with each answer, facilitating note-taking.
T8 discussed Tailor-Mind in clarifying concepts such as layer normalization over batch normalization, <span class="ltx_text ltx_font_italic" id="S6.SS3.SSS2.p1.1.2">"The answers were neither too detailed nor too vague for beginners."</span>
In contrast, participants in Group C posed more generalized questions, which resulted in information-laden answers and challenged beginners’ comprehension.
C3 and C6 noted that although responses were comprehensive, areas of confusion persisted, as ChatGPT seemed to assume they possessed prior knowledge.
<span class="ltx_text" id="S6.SS3.SSS2.p1.1.3">C10 remained skeptical about the answers provided by ChatGPT and is constantly concerned that it may make mistakes.<span class="ltx_text" id="S6.SS3.SSS2.p1.1.3.1">
We also observed that C2 and C12 twice sought simpler explanations of previous responses in more accessible language.
Meanwhile, visualization also plays a crucial role in comprehension.
T8 initially found network structures complex but observed enhanced hierarchical understanding after switching to the “dagre” layout.
Second, questions level, one of the observational metrics, revealed that Tailor-Mind facilitated participants in delving into more profound questions.
T2 was impressed by the quality of the recommended questions, highlighting the model’s ability to prompt multi-faceted questions and deeper exploration of concepts and their interrelations.
T4 commented that <span class="ltx_text ltx_font_italic" id="S6.SS3.SSS2.p1.1.3.1.1">"I felt inspired by recommended questions to initiate spontaneous questions."</span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p2">
<p class="ltx_p" id="S6.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.SSS2.p2.1.1">Improved learning habits and promotion of iterative learning.</span>
On one hand, Tailor-Mind guided participants toward a more structured learning habit.
Group T was compelled to understand and establish a learning plan, in contrast to Group C, where only four participants spontaneously engaged in planning.
Interview results indicated that understanding key and difficult points aided in setting objectives.
T4 and T6 found it easier to grasp key points from File Preview.
T2 appreciated the categorization and basic segmentation of knowledge points, <span class="ltx_text ltx_font_italic" id="S6.SS3.SSS2.p2.1.2">"The system recommended a learning sequence that follows a logical progression, which could mitigate issues of missing prerequisite knowledge and was recognized only after I completed all the tasks."</span>.
Conversely, Group C largely depended on the learning material to question and determine their learning needs, and they were often unclear about what needed to be studied.
Their questions were typically raised in response to confusion about GPT’s prior answers, guiding subsequent inquiry.
On the other hand, the whole process encouraged participants in Group T to reflect not only in the self-reflection stage.
T2, T7, and T8 spontaneously conducted self-assessments, validating their knowledge through recommended complex questions before the self-reflection stage.
T6 appreciated the final test, which highlighted areas of misunderstanding despite their initial confidence in comprehension.
<span class="ltx_text ltx_font_italic" id="S6.SS3.SSS2.p2.1.3">"Many test questions are designed from a speculative perspective, presenting issues that may be classified as ’partially true’."</span>
<span class="ltx_text" id="S6.SS3.SSS2.p2.1.4">The self-reflection phase also facilitated participants’ planning for further learning. Both T10 and T12 developed clear plans for their upcoming learning goals.<span class="ltx_text" id="S6.SS3.SSS2.p2.1.4.1">
T7, a participant with good self-learning habits, used spontaneous questions to verify whether he understood the knowledge correctly, thus further consolidating or correcting his understanding of knowledge points.
<span class="ltx_text ltx_font_italic" id="S6.SS3.SSS2.p2.1.4.1.1">"Through self-assessment and new learning paths, I found mistakes in questions about Layer Normalization. I need to consolidate its principles and functions further."</span> <span class="ltx_text" id="S6.SS3.SSS2.p2.1.4.1.2">In contrast, five participants in Group C were involved in self-reflection.<span class="ltx_text" id="S6.SS3.SSS2.p2.1.4.1.2.1"></span></span></span></span></p>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p3">
<p class="ltx_p" id="S6.SS3.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.SSS2.p3.1.1">Enhanced user engagement and facilitation of active learning.</span>
The exit questionnaire, depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#S6.F7" title="Figure 7 ‣ 6.3.1 Experimental Set-up ‣ 6.3 User Study ‣ 6 Evaluation ‣ Fine-Tuned Large Language Model for Visualization System: A Study on Self-Regulated Learning in Education"><span class="ltx_text ltx_ref_tag">7</span></a>B, consisted of nine evaluation questions from four aspects.
Tailor-Mind received high ratings in these four aspects. Participants expressed willingness to continue learning with Tailor-Mind and recommended it to other beginners.
At the same time, we found that the process of users formulating questions became simplified.
An automatic array of recommended questions rendered it more user-friendly and encouraged participants to engage more readily.
This not only strengthened their curiosity about exploring these questions but also propelled them to seek out and delve into novel knowledge areas independently.
T4 shared a sense of accomplishment upon encountering and accurately responding to a question about the differences between “positional encoding” and “one-hot encoding” in the self-reflection stage.
T7 acknowledged an initial reluctance towards adopting new methodologies.
For him, determining where to start and sustaining motivation posed considerable challenges.
However, Tailor-Mind made the learning journey more seamless and rewarding, significantly enhancing T7’s enthusiasm for delving deeper into uncharted knowledge.
This shift not only reflected T7’s growing competence in navigating the system but also underscored the system’s role in nurturing an enduring passion for learning.
<span class="ltx_text" id="S6.SS3.SSS2.p3.1.2">Additionally, we observed that most participants in Group C complained about the slow response of GPT, and many did not know how to pose questions effectively.<span class="ltx_text" id="S6.SS3.SSS2.p3.1.2.1"></span></span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Discussion on Tailor-Mind</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p1.1.1">Generalizability of Tailor-Mind.</span>
Tailor-Mind can generalize to self-learning in all disciplines with structured knowledge.
For any discipline and learner, maintaining refining knowledge structures is a general requirement of scientific self-learning, which is ensured by our visualization system and tuning methods.
Currently, LLMs have uncertain capabilities in extracting semantic and structural information from images, audio, and video materials.
Hence, Tailor-Mind does not support learning from these types of content <span class="ltx_text" id="S7.SS1.p1.1.2">and is not connected to web resources.<span class="ltx_text" id="S7.SS1.p1.1.2.1">
However, with rapid advancements in multimodal large model capabilities, Tailor-Mind’s framework and underlying fine-tuning mechanisms can be flexibly extended by converting non-text data into text format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib19" title="">19</a>]</cite> <span class="ltx_text" id="S7.SS1.p1.1.2.1.1">and utilizing online data<span class="ltx_text" id="S7.SS1.p1.1.2.1.1.1">.</span></span></span></span></p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Enhancement for pipeline.</span>
The SRL pipeline’s three stages offer potential for technical enhancements.
In the forethought phase, future integration with web searches could complement question list recommendations, broadening the spectrum of learning resources.
The performance phase could see the automatic optimization of human-recorded notes and their incorporation into a knowledge map.
Meanwhile, the self-reflection stage might expand to include various self-assessment forms, such as error correction during learning.
Additionally, employing multiple agents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib35" title="">35</a>]</cite> throughout the whole SRL process could enrich the learning experience, with each agent assuming specific roles like tutoring, concentration monitoring, and incentives for learning.
The single-person SRL process can be further extended to community learning to obtain a wider range of learning experiences and effects.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span>Discussion on General Framework</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1">Generalizability of the framework.</span>
Our frameworks involves specific domain, visualization system, user interactions, and LLMs.
<span class="ltx_text" id="S7.SS2.p1.1.2">(1) Specific domain:
Our framework is well-suited for domains that are highly specialized, information-dense, and characterized by broad and deep knowledge. These domains are also highly structured, with well-defined rules and standards, and their tasks are problem-solving or decision-making oriented, requiring a user-centered approach.
Therefore, the limitations of our framework lie in handling tasks that are highly subjective, lack clear norms, and have strong emotional dependencies, such as artistic creativity and open-ended experimental research.<span class="ltx_text" id="S7.SS2.p1.1.2.1">
<span class="ltx_text" id="S7.SS2.p1.1.2.1.1">(2) Visualization systems:
The well-defined functions of views in visualization systems ensure compatibility with our framework. In this work, we designed the visualization system to prioritize user interaction.<span class="ltx_text" id="S7.SS2.p1.1.2.1.1.1"> We believe that visual analytic (VA) systems focused on data analysis can also benefit from our framework. VA systems designed for text data can easily standardize requirements and domain knowledge into text-based fine-tuning tasks. For VA systems dealing with non-textual data, our framework could adapt as tuning methods evolve to teach models to analyze data, such as invoking numerical analysis APIs.
<span class="ltx_text" id="S7.SS2.p1.1.2.1.1.1.1">(3) LLMs:<span class="ltx_text" id="S7.SS2.p1.1.2.1.1.1.1.1">
Although we verified the process on a relatively small open-source model, the entire process is also applicable to stronger closed-source models, such as GPT-4, since it can be fine-tuned.
In specific applications of the framework, it is crucial to consider the trade-offs between model performance and access costs, ensuring the selection of the most suitable model for the given task.</span></span></span></span></span></span></p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">Iterative and customized fine-tuning.</span>
Considering the variations in users with different levels of knowledge and behavioral preferences, we find that personalized and customized fine-tuning could be beneficial.
However, this process requires an initial automatic assessment of the user’s knowledge level regarding the material, followed by adjusting the fine-tuning tasks based on that knowledge level, making automation challenging and costly.
We believe that current fine-tuning based on the behavioral patterns of the majority of users can meet most user needs.
Although the information may be slightly overwhelming for beginners, interactive guidance in the interface can mitigate this issue.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.1">Performances of fine-tuned LLM.<span class="ltx_text ltx_font_medium" id="S7.SS2.p3.1.1.1">
Fine-tuning standardizes model outputs through instruction data. It aligns LLM outputs with latent behavioral norms embedded in the data, resulting in more controllable, stable, and consistent outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib3" title="">3</a>]</cite>.
However, as we cannot consider all possible data scenarios, and given the inherent randomness in generative models, the model still produces seemingly plausible but nonsensical responses, known as “hallucinations” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib14" title="">14</a>]</cite>.
Fully training a specialized LLM for every domain is impractical and resource-intensive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib68" title="">68</a>]</cite>.
To reduce erroneous outputs, future work should include not only a self-reflection mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib43" title="">43</a>]</cite> but also an error reporting system to enhance trustworthiness. This requires accurately and promptly identifying LLM errors and providing effective solutions. In addition to logging erroneous data, multi-agent collaborative supervision is also a viable solution.
Additionally, alignment with user interactions can be further improved. Beyond current efforts in aligning with human preferences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2407.20570v1#bib.bib33" title="">33</a>]</cite>, users should participate in constructing tuning data by supporting feedback and correction of model errors.<span class="ltx_text" id="S7.SS2.p3.1.1.1.1"></span></span></span></p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.1">Effectiveness and comprehensiveness of evaluation.</span>
Our evaluation includes multiple effects and comprehensive methods, yet may still involve limitations.
For the quantitative evaluation of a model, a common approach in the NLP community for fine-tuning tasks in specialized domains, where GPT’s output is considered as ground truth, is comparing it with open-source models.
We adopt this experimental setup and extend the comparison to GPT-3.5, demonstrating the rigor of our evaluation.
In evaluating the system, we noted differences in users’ knowledge levels but did not conduct grouped experiments to assess the impact of different user profiles on system effectiveness.
Additionally, we did not examine the differences in system usage before and after fine-tuning by a large pool of users, considering the cost associated with controlling all variables for comparative experiments.
However, comparing the fine-tuned model and system against GPT-4 (a strong baseline) showcases our system’s superiority.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">To summarize, we proposed a framework integrating fine-tuned LLMs into visualization systems to achieve intelligent and interactive domain problem-solving.
Based on this framework, we summarize a workflow for solving the three alignments among domain knowledge, visualization, interaction, and LLMs.
To demonstrate the application of our framework, we introduce Tailor-Mind, an interactive intelligent visualization system.
Following a detailed SRL pipeline, we designed fine-tuning data to improve intelligent decision-making and personalized recommendations.
Through two usage scenarios, we illustrated that Tailor-Mind is suitable for beginners and aids in knowledge consolidation.
Model performance evaluations and user studies confirmed that Tailor-Mind is effective in promoting the scientific, active, and iterative SRL, which also validates the proposed framework and workflow.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>The authors wish to thank Professor Yan Ding and her team from the Institute of Higher Education, Fudan University, for valuable feedback on this project.
This work is supported by Natural Science Foundation of China
(NSFC No.62202105, 62102323) and Shanghai Municipal Science and Technology Major Project (2021SHZDZX0103).



</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
N. Andrienko, T. Lammarsch, G. Andrienko, G. Fuchs, D. Keim, S. Miksch, and A. Rind.

</span>
<span class="ltx_bibblock">Viewing Visual Analytics as Model Building.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Computer Graphics Forum</span>, 37(6):275–299, 2018. <a class="ltx_ref ltx_href" href="https://doi.org/10.1111/cgf.13324" title="">doi: 10 . 1111/cgf . 13324</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Baladón, I. Sastre, L. Chiruzzo, and A. Rosá.

</span>
<span class="ltx_bibblock">RETUYT-InCo at BEA 2023 shared task: Tuning open-source LLMs for generating teacher responses.

</span>
<span class="ltx_bibblock">In E. Kochmar, J. Burstein, A. Horbach, R. Laarmann-Quante, N. Madnani, A. Tack, V. Yaneva, Z. Yuan, and T. Zesch, eds., <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)</span>, pp. 756–765. Association for Computational Linguistics, Toronto, Canada, July 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.bea-1.61" title="">doi: 10 . 18653/v1/2023 . bea-1 . 61</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Chen, Y. Zhang, Q. Zhang, H. Yang, X. Hu, X. Ma, Y. Yanggong, and J. Zhao.

</span>
<span class="ltx_bibblock">Maybe only 0.5% data is needed: A preliminary exploration of low training data instruction tuning, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.09246" title="">doi: 10 . 48550/arXiv . 2305 . 09246</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
W. Chen, X. Ma, X. Wang, and W. W. Cohen.

</span>
<span class="ltx_bibblock">Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2211.12588" title="">doi: 10 . 48550/arXiv . 2211 . 12588</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
K. Choi, H. Shin, M. Xia, and J. Kim.

</span>
<span class="ltx_bibblock">Algosolve: Supporting subgoal learning in algorithmic problem-solving with learnersourced microtasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</span>, pp. 1–16, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3491102.3501917" title="">doi: 10 . 1145/3491102 . 3501917</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
T. Crow, A. Luxton-Reilly, and B. Wuensche.

</span>
<span class="ltx_bibblock">Intelligent tutoring systems for programming education: a systematic review.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 20th Australasian Computing Education Conference</span>, pp. 53–62, 2018. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3160489.3160492" title="">doi: 10 . 1145/3160489 . 3160492</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Dan, Z. Lei, Y. Gu, Y. Li, J. Yin, J. Lin, L. Ye, Z. Tie, Y. Zhou, Y. Wang, A. Zhou, Z. Zhou, Q. Chen, J. Zhou, L. He, and X. Qiu.

</span>
<span class="ltx_bibblock">Educhat: A large-scale language model-based chatbot system for intelligent education, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.02773" title="">doi: 10 . 48550/arXiv . 2308 . 02773</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Du, S. Zhao, Y. Chen, R. Bai, J. Liu, H. Wu, H. Wang, and B. Qin.

</span>
<span class="ltx_bibblock">The calla dataset: Probing llms’ interactive knowledge acquisition from chinese medical literature, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.04198" title="">doi: 10 . 48550/arXiv . 2309 . 04198</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H. Fang, H. Puerto, and I. Gurevych.

</span>
<span class="ltx_bibblock">UKP-SQuARE: An interactive tool for teaching question answering.

</span>
<span class="ltx_bibblock">In E. Kochmar, J. Burstein, A. Horbach, R. Laarmann-Quante, N. Madnani, A. Tack, V. Yaneva, Z. Yuan, and T. Zesch, eds., <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)</span>, pp. 195–204. Association for Computational Linguistics, Toronto, Canada, July 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.bea-1.17" title="">doi: 10 . 18653/v1/2023 . bea-1 . 17</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
L. Gao, Z. Shao, Z. Luo, H. Hu, C. Turkay, and S. Chen.

</span>
<span class="ltx_bibblock">Transforlearn: Interactive visual tutorial for the transformer model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(1):891–901, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3327353" title="">doi: 10 . 1109/TVCG . 2023 . 3327353</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q. He, J. Zeng, W. Huang, L. Chen, J. Xiao, Q. He, X. Zhou, J. Liang, and Y. Xiao.

</span>
<span class="ltx_bibblock">Can large language models understand real-world complex instructions?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 38(16):18188–18196, Mar. 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1609/aaai.v38i16.29777" title="">doi: 10 . 1609/aaai . v38i16 . 29777</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Hou, M. Yang, H. Cui, L. Wang, J. Xu, and W. Zeng.

</span>
<span class="ltx_bibblock">C2ideas: Supporting creative interior color design ideation with a large language model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</span>, CHI ’24. Association for Computing Machinery, New York, NY, USA, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3613904.3642224" title="">doi: 10 . 1145/3613904 . 3642224</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</span>, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2106.09685" title="">doi: 10 . 48550/arXiv . 2106 . 09685</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">ArXiv</span>, abs/2311.05232, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2311.05232" title="">doi: 10 . 48550/arXiv . 2311 . 05232</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
R. Huang, H. Lin, C. Chen, K. Zhang, and W. Zeng.

</span>
<span class="ltx_bibblock">Plantography: Incorporating iterative design process into generative artificial intelligence for landscape rendering.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</span>, CHI ’24. Association for Computing Machinery, New York, NY, USA, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3613904.3642824" title="">doi: 10 . 1145/3613904 . 3642824</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He.

</span>
<span class="ltx_bibblock">C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Advances in Neural Information Processing Systems</span>, 36:62991–63010, Dec. 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.08322" title="">doi: 10 . 48550/arXiv . 2305 . 08322</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
G.-J. Hwang, H. Xie, B. W. Wah, and D. Gašević.

</span>
<span class="ltx_bibblock">Vision, challenges, roles and research issues of artificial intelligence in education, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.caeai.2020.100001" title="">doi: 10 . 1016/j . caeai . 2020 . 100001</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Z. Ji, T. Yu, Y. Xu, N. Lee, E. Ishii, and P. Fung.

</span>
<span class="ltx_bibblock">Towards mitigating LLM hallucination via self reflection.

</span>
<span class="ltx_bibblock">In H. Bouamor, J. Pino, and K. Bali, eds., <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pp. 1827–1843. Association for Computational Linguistics, Singapore, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.123" title="">doi: 10 . 18653/v1/2023 . findings-emnlp . 123</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Y. Jiang, X. Yan, G.-P. Ji, K. Fu, M. Sun, H. Xiong, D.-P. Fan, and F. Khan.

</span>
<span class="ltx_bibblock">Effectiveness assessment of recent large vision-language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Visual Intelligence</span>, 2, 06 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s44267-024-00050-1" title="">doi: 10 . 1007/s44267-024-00050-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer, U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier, et al.

</span>
<span class="ltx_bibblock">Chatgpt for good? on opportunities and challenges of large language models for education.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Learning and individual differences</span>, 103:102274, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.lindif.2023.102274" title="">doi: 10 . 1016/j . lindif . 2023 . 102274</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Keim, G. Andrienko, J.-D. Fekete, C. Görg, J. Kohlhammer, and G. Melançon.

</span>
<span class="ltx_bibblock">Visual Analytics: Definition, Process, and Challenges.

</span>
<span class="ltx_bibblock">In A. Kerren, J. T. Stasko, J.-D. Fekete, and C. North, eds., <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Information Visualization</span>, vol. 4950, pp. 154–175. Springer Berlin Heidelberg, Berlin, Heidelberg, 2008. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-540-70956-5_7" title="">doi: 10 . 1007/978-3-540-70956-5_7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D. R. Krathwohl.

</span>
<span class="ltx_bibblock">A revision of bloom’s taxonomy: An overview.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Theory Into Practice</span>, 41(4):212–218, 2002. <a class="ltx_ref ltx_href" href="https://doi.org/10.1207/s15430421tip4104_2" title="">doi: 10 . 1207/s15430421tip4104_2</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</span>, NIPS ’20. Curran Associates Inc., Red Hook, NY, USA, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2005.11401" title="">doi: 10 . 48550/arXiv . 2005 . 11401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Li, Y. Zhang, Z. Li, J. Chen, L. Chen, N. Cheng, J. Wang, T. Zhou, and J. Xiao.

</span>
<span class="ltx_bibblock">From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.12032" title="">doi: 10 . 48550/arXiv . 2308 . 12032</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
C. Ling and et al.

</span>
<span class="ltx_bibblock">Domain specialization as the key to make large language models disruptive: A comprehensive survey, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2305.18703" title="">doi: 10 . 48550/arXiv . 2305 . 18703</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
D. Y. Liu, L. M. Xu, X. M. Lin, X. Wei, W. J. Yu, Y. Wang, and Z. M. Wei.

</span>
<span class="ltx_bibblock">Machine learning for semiconductors.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">Chip</span>, 1(4):100033, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.chip.2022.100033" title="">doi: 10 . 1016/j . chip . 2022 . 100033</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
J. Liu, T. Dwyer, G. Tack, S. Gratzl, and K. Marriott.

</span>
<span class="ltx_bibblock">Supporting the problem-solving loop: Designing highly interactive optimisation systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 27(2):1764–1774, 2020. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2020.3030364" title="">doi: 10 . 1109/TVCG . 2020 . 3030364</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y. Liu, S. Chen, H. Cheng, M. Yu, X. Ran, A. Mo, Y. Tang, and Y. Huang.

</span>
<span class="ltx_bibblock">How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</span>, CHI ’24. Association for Computing Machinery, New York, NY, USA, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3613904.3642698" title="">doi: 10 . 1145/3613904 . 3642698</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Lu, B. Pan, J. Chen, Y. Feng, J. Hu, Y. Peng, and W. Chen.

</span>
<span class="ltx_bibblock">Agentlens: Visual analysis for agent behaviors in llm-based autonomous systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, pp. 1–17, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2024.3394053" title="">doi: 10 . 1109/TVCG . 2024 . 3394053</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Q. Ma, H. Shen, K. Koedinger, and S. T. Wu.

</span>
<span class="ltx_bibblock">How to teach programming in the ai era? using llms as a teachable agent for debugging, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-031-64302-6_19" title="">doi: 10 . 1007/978-3-031-64302-6_19</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.openai.com/chatgpt" title="">https://www.openai.com/chatgpt</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/papers/gpt-4.pdf" title="">https://cdn.openai.com/papers/gpt-4.pdf</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
L. Ouyang and et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2203.02155" title="">doi: 10 . 48550/arXiv . 2203 . 02155</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
E. Panadero.

</span>
<span class="ltx_bibblock">A review of self-regulated learning: Six models and four directions for research.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Frontiers in psychology</span>, 8:422, 2017. <a class="ltx_ref ltx_href" href="https://doi.org/10.3389/fpsyg.2017.00422" title="">doi: 10 . 3389/fpsyg . 2017 . 00422</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. S. Park, J. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</span>, UIST ’23. Association for Computing Machinery, New York, NY, USA, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3586183.3606763" title="">doi: 10 . 1145/3586183 . 3606763</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
B. Peng, M. Galley, P. He, H. Cheng, Y. Xie, Y. Hu, Q. Huang, L. Liden, Z. Yu, W. Chen, and J. Gao.

</span>
<span class="ltx_bibblock">Check your facts and try again: Improving large language models with external knowledge and automated feedback, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.12813" title="">doi: 10 . 48550/arXiv . 2302 . 12813</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Z. Peng, X. Wang, Q. Han, J. Zhu, X. Ma, and H. Qu.

</span>
<span class="ltx_bibblock">Storyfier: Exploring vocabulary learning support with text generation models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</span>, UIST ’23. Association for Computing Machinery, New York, NY, USA, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3586183.3606786" title="">doi: 10 . 1145/3586183 . 3606786</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
R. Qiu, Y. Tu, Y.-S. Wang, P.-Y. Yen, and H.-W. Shen.

</span>
<span class="ltx_bibblock">Docflow: A visual analytics system for question-based document retrieval and categorization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(2):1533–1548, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3219762" title="">doi: 10 . 1109/TVCG . 2022 . 3219762</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom.

</span>
<span class="ltx_bibblock">Toolformer: Language models can teach themselves to use tools, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.04761" title="">doi: 10 . 48550/arXiv . 2302 . 04761</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T. Schick and H. Schütze.

</span>
<span class="ltx_bibblock">Generating datasets with pretrained language models, Nov. 2021. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.555" title="">doi: 10 . 18653/v1/2021 . emnlp-main . 555</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
R. Schmucker, M. Xia, A. Azaria, and T. Mitchell.

</span>
<span class="ltx_bibblock">Ruffle&amp;riley: Towards the automated induction of conversational tutoring systems, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.01420" title="">doi: 10 . 48550/arXiv . 2310 . 01420</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
D. Shi, W. Cui, D. Huang, H. Zhang, and N. Cao.

</span>
<span class="ltx_bibblock">Reverse-engineering information presentations: recovering hierarchical grouping from layouts of visual elements.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Visual Intelligence</span>, 1, 06 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s44267-023-00010-1" title="">doi: 10 . 1007/s44267-023-00010-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
N. Shinn, B. Labash, and A. Gopinath.

</span>
<span class="ltx_bibblock">Reflexion: an autonomous agent with dynamic memory and self-reflection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">ArXiv</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2303.11366" title="">doi: 10 . 48550/arXiv . 2303 . 11366</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and A. M. Rush.

</span>
<span class="ltx_bibblock">Interactive and visual prompt engineering for ad-hoc task adaptation with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 29(1):1146–1156, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2022.3209479" title="">doi: 10 . 1109/TVCG . 2022 . 3209479</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Tack, E. Kochmar, Z. Yuan, S. Bibauw, and C. Piech.

</span>
<span class="ltx_bibblock">The BEA 2023 shared task on generating AI teacher responses in educational dialogues.

</span>
<span class="ltx_bibblock">In E. Kochmar, J. Burstein, A. Horbach, R. Laarmann-Quante, N. Madnani, A. Tack, V. Yaneva, Z. Yuan, and T. Zesch, eds., <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)</span>, pp. 785–795. Association for Computational Linguistics, Toronto, Canada, July 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.bea-1.64" title="">doi: 10 . 18653/v1/2023 . bea-1 . 64</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, and D. S. W. Ting.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Nature Medicine</span>, 29(8):1930–1940, Aug. 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1038/s41591-023-02448-8" title="">doi: 10 . 1038/s41591-023-02448-8</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2302.13971" title="">doi: 10 . 48550/arXiv . 2302 . 13971</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, and T. Liu.

</span>
<span class="ltx_bibblock">Huatuo: Tuning llama model with chinese medical knowledge, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.06975" title="">doi: 10 . 48550/arXiv . 2304 . 06975</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
H. Wang, S. Zhao, Z. Qiang, Z. Li, N. Xi, Y. Du, M. Cai, H. Guo, Y. Chen, H. Xu, B. Qin, and T. Liu.

</span>
<span class="ltx_bibblock">Knowledge-tuning large language models with structured medical knowledge bases for reliable response generation in chinese, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.04175" title="">doi: 10 . 48550/arXiv . 2309 . 04175</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
X. Wang, R. Huang, Z. Jin, T. Fang, and H. Qu.

</span>
<span class="ltx_bibblock">Commonsensevis: Visualizing and understanding commonsense reasoning capabilities of natural language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, p. 1–11, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/tvcg.2023.3327153" title="">doi: 10 . 1109/tvcg . 2023 . 3327153</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language models with self-generated instructions, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2212.10560" title="">doi: 10 . 48550/arXiv . 2212 . 10560</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Z. Wang, L.-P. Yuan, L. Wang, B. Jiang, and W. Zeng.

</span>
<span class="ltx_bibblock">Virtuwander: Enhancing multi-modal interaction for virtual tour guidance through large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</span>, CHI ’24. Association for Computing Machinery, New York, NY, USA, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3613904.3642235" title="">doi: 10 . 1145/3613904 . 3642235</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Z. Wang, Q. Zhang, S.-W. HU, H. Yu, X. Jin, Z. Gong, and H. Chen.

</span>
<span class="ltx_bibblock">Multi-level protein structure pre-training via prompt learning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">The Eleventh International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.

</span>
<span class="ltx_bibblock">Finetuned language models are zero-shot learners, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2109.01652" title="">doi: 10 . 48550/arXiv . 2109 . 01652</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 36th International Conference on Neural Information Processing Systems</span>, NIPS ’22. Curran Associates Inc., Red Hook, NY, USA, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2201.11903" title="">doi: 10 . 48550/arXiv . 2201 . 11903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
G. Wu, S. Guo, J. Hoffswell, G. Y.-Y. Chan, R. A. Rossi, and E. Koh.

</span>
<span class="ltx_bibblock">Socrates: Data story generation via adaptive machine-guided elicitation of user feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">IEEE Transactions on Visualization and Computer Graphics</span>, 30(1):131–141, 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2023.3327363" title="">doi: 10 . 1109/TVCG . 2023 . 3327363</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Z. Xi and et al.

</span>
<span class="ltx_bibblock">The rise and potential of large language model based agents: A survey, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.07864" title="">doi: 10 . 48550/arXiv . 2309 . 07864</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
A. Yang and et al.

</span>
<span class="ltx_bibblock">Baichuan 2: Open large-scale language models, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.10305" title="">doi: 10 . 48550/arXiv . 2309 . 10305</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
D. Yang, R. Yuan, Y. Fan, Y. Yang, Z. Wang, S. Wang, and H. Zhao.

</span>
<span class="ltx_bibblock">RefGPT: Dialogue generation of GPT, by GPT, and for GPT.

</span>
<span class="ltx_bibblock">In H. Bouamor, J. Pino, and K. Bali, eds., <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</span>, pp. 2511–2535. Association for Computational Linguistics, Singapore, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.165" title="">doi: 10 . 18653/v1/2023 . findings-emnlp . 165</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
H. Yang, X.-Y. Liu, and C. D. Wang.

</span>
<span class="ltx_bibblock">Fingpt: Open-source financial large language models, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.06031" title="">doi: 10 . 48550/arXiv . 2306 . 06031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
J. Yu, J. Zhu, Y. Wang, Y. Liu, H. Chang, J. Nie, C. Kong, R. Chong, XinLiu, J. An, L. Lu, M. Fang, and L. Zhu.

</span>
<span class="ltx_bibblock">Taoli llama.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/blcuicall/taoli" title="">https://github.com/blcuicall/taoli</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou, Y. Xiao, S. Yun, X. Huang, and Z. Wei.

</span>
<span class="ltx_bibblock">Disc-lawllm: Fine-tuning large language models for intelligent legal services, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.11325" title="">doi: 10 . 48550/arXiv . 2309 . 11325</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen.

</span>
<span class="ltx_bibblock">Mammoth: Building math generalist models through hybrid instruction tuning, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2309.05653" title="">doi: 10 . 48550/arXiv . 2309 . 05653</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Z. Yuheng, Y. Zhang, Y. Zhang, X. Zhao, J. Wang, Z. Shao, C. Turkay, and S. Chen.

</span>
<span class="ltx_bibblock">Leva: Using large language models to enhance visual analytics.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">IEEE transactions on visualization and computer graphics</span>, PP, 03 2024. <a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TVCG.2024.3368060" title="">doi: 10 . 1109/TVCG . 2024 . 3368060</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
A. Zeng and et al.

</span>
<span class="ltx_bibblock">GLM-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">The Eleventh International Conference on Learning Representations</span>, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2210.02414" title="">doi: 10 . 48550/arXiv . 2210 . 02414</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
H. Zhang, G. Li, J. Li, Z. Zhang, Y. ZHU, and Z. Jin.

</span>
<span class="ltx_bibblock">Fine-Tuning Pre-Trained Language Models Effectively by Optimizing Subnetworks Adaptively.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">Advances in Neural Information Processing Systems</span>, vol. 35, pp. 21442–21454. Curran Associates, Inc., 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2211.01642" title="">doi: 10 . 48550/arXiv . 2211 . 01642</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang, J. Li, R. Hu, T. Zhang, F. Wu, and G. Wang.

</span>
<span class="ltx_bibblock">Instruction tuning for large language models: A survey, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2308.10792" title="">doi: 10 . 48550/arXiv . 2308 . 10792</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Z. Zhang, C. Zheng, D. Tang, K. Sun, Y. Ma, Y. Bu, X. Zhou, and L. Zhao.

</span>
<span class="ltx_bibblock">Balancing specialized and general skills in llms: The impact of modern tuning and data strategy.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.1.1">ArXiv</span>, abs/2310.04945, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2310.04945" title="">doi: 10 . 48550/arXiv . 2310 . 04945</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
C. Zheng, D. Wang, A. Y. Wang, and X. Ma.

</span>
<span class="ltx_bibblock">Telling stories from computational notebooks: Ai-assisted presentation slides creation for presenting data science work.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib69.1.1">Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems</span>, CHI ’22. Association for Computing Machinery, New York, NY, USA, 2022. <a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3491102.3517615" title="">doi: 10 . 1145/3491102 . 3517615</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib70.1.1">Advances in Neural Information Processing Systems</span>, vol. 36, pp. 46595–46623. Curran Associates, Inc., 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2306.05685" title="">doi: 10 . 48550/arXiv . 2306 . 05685</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
W. Zhong, R. Cui, Y. Guo, Y. Liang, S. Lu, Y. Wang, A. Saied, W. Chen, and N. Duan.

</span>
<span class="ltx_bibblock">Agieval: A human-centric benchmark for evaluating foundation models, 2023. <a class="ltx_ref ltx_href" href="https://doi.org/10.48550/arXiv.2304.06364" title="">doi: 10 . 48550/arXiv . 2304 . 06364</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
B. J. Zimmerman.

</span>
<span class="ltx_bibblock">Chapter 2 - attaining self-regulation: A social cognitive perspective.

</span>
<span class="ltx_bibblock">In M. Boekaerts, P. R. Pintrich, and M. Zeidner, eds., <span class="ltx_text ltx_font_italic" id="bib.bib72.1.1">Handbook of Self-Regulation</span>, pp. 13–39. Academic Press, San Diego, 2000. <a class="ltx_ref ltx_href" href="https://doi.org/10.1016/B978-012109890-2/50031-7" title="">doi: 10 . 1016/B978-012109890-2/50031-7</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Jul 30 05:52:33 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
