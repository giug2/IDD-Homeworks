<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>ACER: Automatic Language Model Context Extension via Retrieval</title>
<!--Generated on Fri Oct 11 18:00:20 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.09141v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S1" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S2" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">ACER</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S2.SS1" title="In 2 ACER â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Automatic Data Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S2.SS2" title="In 2 ACER â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Fine-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS1" title="In 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS2" title="In 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Data Synthesis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS2.SSS0.Px1" title="In 3.2 Data Synthesis â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title">Ranker Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS2.SSS0.Px2" title="In 3.2 Data Synthesis â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title">Generator Model</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS3" title="In 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Fine-tuning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS3.SSS1" title="In 3.3 Fine-tuning â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Implementation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS3.SSS2" title="In 3.3 Fine-tuning â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Infrastructure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S3.SS4" title="In 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Compared Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S4" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S5" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S5.SS1" title="In 5 Analysis â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparing Llama-3.1 and <span class="ltx_text ltx_font_smallcaps">ACER</span> at Different Context Sizes</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S5.SS2" title="In 5 Analysis â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Using an Unsupervised Retriever</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S6" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S6.SS0.SSS0.Px1" title="In 6 Related Works â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title">Language Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S6.SS0.SSS0.Px2" title="In 6 Related Works â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title">Long Context Modeling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S6.SS0.SSS0.Px3" title="In 6 Related Works â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title">Retrieval Augmented Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#S7" title="In ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ACER: Automatic Language Model Context Extension via Retrieval</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luyu Gao 
<br class="ltx_break"/>Language Technologies Institute
<br class="ltx_break"/>Carnegie Mellon University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">luyug@cs.cmu.edu</span>
<br class="ltx_break"/>&amp;Yunyi Zhang 
<br class="ltx_break"/>Department of Computer Science 
<br class="ltx_break"/>University of Illinois Urbana-Champaign

<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">yzhan238@illinois.edu</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Jamie Callan 
<br class="ltx_break"/>Language Technologies Institute
<br class="ltx_break"/>Carnegie Mellon University
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">callan@cs.cmu.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Long-context modeling is one of the critical capabilities of language AI for digesting and reasoning over complex information pieces. In practice, long-context capabilities are typically built into a pre-trained language modelÂ (LM) through a carefully designed context extension stage, with the goal of producing generalist long-context capabilities. In our preliminary experiments, however, we discovered that the current open-weight generalist long-context models are still lacking in practical long-context processing tasks. While this means perfectly effective long-context modeling demands task-specific data, the cost can be prohibitive. In this paper, we draw inspiration from how humans process a large body of information: a lossy <span class="ltx_text ltx_font_bold" id="id5.id1.1">retrieval</span> stage ranks a large set of documents while the reader ends up reading deeply only the top candidates. We build an <span class="ltx_text ltx_font_bold" id="id5.id1.2">automatic</span> data synthesis pipeline that mimics this process using short-context LMs. The short-context LMs are further tuned using these self-generated data to obtain task-specific long-context capabilities. Similar to how pre-training learns from imperfect data, we hypothesize and further demonstrate that the short-context model can bootstrap over the synthetic data, outperforming not only long-context generalist models but also the retrieval and read pipeline used to synthesize the training data in real-world tasks such as long-context retrieval augmented generation.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The field of Artificial IntelligenceÂ (AI) and Natural Language ProcessingÂ (NLP) have made substantial progress in building and teaching neural language modelsÂ (LMs) to understand and generate languageÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib48" title="">2019</a>; Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib8" title="">2020</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib43" title="">2023</a>; Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib2" title="">2024</a>; Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib58" title="">2023a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib59" title="">b</a>; MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>. Large-scale deep learning has enabled large LMs to learn from massive amounts of <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p1.1.1">human-generated</span> textÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib48" title="">2019</a>; Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib8" title="">2020</a>)</cite>. However, new challenges emerge as researchers consider building capabilities beyond those of humans. One popular example, also the focus of this paper, is understanding long-contexts of textÂ <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib12" title="">2019</a>; Su etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib55" title="">2024</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib68" title="">2023</a>)</cite>. Despite the advancements in modelingÂ <cite class="ltx_cite ltx_citemacro_citep">(Dao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib14" title="">2022</a>; Su etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib55" title="">2024</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib68" title="">2023</a>)</cite>, data keeps being lacking simply because humans no longer produce them naturally. Specifically, while longer contexts give more degrees of freedom in forming possible language sequences and long-interaction/complex-information tasks, the existing long texts are limited to organized and compact ones like novels and codeÂ <cite class="ltx_cite ltx_citemacro_citep">(Fu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib18" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">A common methodology in building long-context LM is to incorporate a context extension phase into the model building cycle after the general pre-training phase and before the task-aware post-training phaseÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib68" title="">2023</a>; RoziÃ¨re etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib53" title="">2024</a>; MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>. As the standard pre-training stage builds into the model the general capabilities over diverse text distributions, the long-context extension phase is designed with the hope to extend generalist capabilities further to long-context patterns. The subsequent post-training is supposed to activate and align these extra long-context capabilities to task instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the exciting promise of this context extension scheme, the limited portion of model training dedicated to long-context contradicts the aforementioned increasingly more complex space language and task patterns when the text gets longerÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib68" title="">2023</a>; MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>. In other words, using a context extension phase to cover all long-context understanding tasks can be mathematically intractable, and consequently, building a long-context generalist may not succeed. To investigate this, in this paper, we conduct experiments and demonstrate empirically that practical tasks like long-context retrieval augmented generation can easily break existing long-context models.
While creating domain-specific supervised training may help fix this. Unlike short-context training data that can be collected from everyday peopleÂ <cite class="ltx_cite ltx_citemacro_citep">(Kopf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib34" title="">2023</a>)</cite>, long-context training can be much harder to curate. Reading long pieces of text is inherently hard for humans, which could make the annotation process not only costly but also very demanding and, therefore, less reliable.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, to alleviate this problem and provide an intermediate solution, we propose a new approach which <span class="ltx_text ltx_font_bold" id="S1.p4.1.1">A</span>utomates <span class="ltx_text ltx_font_bold" id="S1.p4.1.2">C</span>ontext <span class="ltx_text ltx_font_bold" id="S1.p4.1.3">E</span>xtension via <span class="ltx_text ltx_font_bold" id="S1.p4.1.4">R</span>etrievalÂ (<span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.5">ACER</span>; <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S2.F1" title="Figure 1 â€£ 2 ACER â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a>). Overall, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.6">ACER</span> is a two-stage method. We start with synthesizing <em class="ltx_emph ltx_font_italic" id="S1.p4.1.7">imperfect</em> data by combining retrieval with an LM <em class="ltx_emph ltx_font_italic" id="S1.p4.1.8">excels in short context</em>. In the subsequent stage, we will fine-tune a large LM to bootstrap over this data. <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.9">ACER</span> will start with some pairs of question and its long context, while no labeling is required. In the synthesis pipeline, the long context is broken into chunks and a retrieval model will score and rank the chunks. A small set of top-ranked chunks will be fed into the short-context LM to produce answer <em class="ltx_emph ltx_font_italic" id="S1.p4.1.10">with chain-of-thought</em>Â (CoT; <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib61" title="">2022</a>)</cite>) reasoning. Then, in the fine-tuning stage, we train the model using the context, question, and the CoT. On the other hand, the retrieval-based data synthesis process will be hidden from the model. We desire that the deep model will learn a generic long-context understanding function by fitting over the full context and the CoT reasoning. We hypothesize that the model may discover a better latent function that transcends the original ranking mechanism used in the first stage. This also shares some spirit with LM pre-training. Whereas typically pre-training bootstrap from existing human data, due to the apparent data scarcity, <span class="ltx_text ltx_font_smallcaps" id="S1.p4.1.11">ACER</span> will bootstrap from synthetic long-context data generated using retrieval.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our experiments demonstrated the effectiveness of <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.1">ACER</span>. We found model trained with <span class="ltx_text ltx_font_smallcaps" id="S1.p5.1.2">ACER</span> without supervision can outperform contemporary generalist long-context models. It also outperforms its own retrieval-based answering pipeline if applied to the test sets.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">ACER</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we will give an overview of <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">ACER</span>. The <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.2">ACER</span> method consists of two major stages: 1) automatic data synthesis, and 2) self training, as illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S2.F1" title="Figure 1 â€£ 2 ACER â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 1</span></a>.
In this chapter, we will describe how each of these stages work.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="288" id="S2.F1.g1" src="extracted/5920405/fig/main.drawio.curr.png" width="548"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The full process of <span class="ltx_text ltx_font_smallcaps" id="S2.F1.2.1">ACER</span> involves a data synthesis stage and a fine-tuning stage. (top) The data synthesis stage splits and retrieves a set of relevant text chunks for a problem and use a short-context model to generate an answer with CoTÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib61" title="">2022</a>)</cite>. (bottom) The fine-tuning stage use the original long-context data and the synthetic CoT answer to fine-tune a long-context model.</figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Automatic Data Synthesis</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Our data synthesis process combines a heuristic-based retrieval pipeline and a short-context generator. We use the following ingredients,</p>
<ol class="ltx_enumerate" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.I1.i1.1.1.m1.1"><semantics id="S2.I1.i1.1.1.m1.1b"><mo id="S2.I1.i1.1.1.m1.1.1" xref="S2.I1.i1.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i1.1.1.m1.1c"><ci id="S2.I1.i1.1.1.m1.1.1.cmml" xref="S2.I1.i1.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.2.1">Prompts</span>: a set of prompts/problems, <math alttext="\{p_{1},p_{2},..,p_{N}\}" class="ltx_math_unparsed" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><mrow id="S2.I1.i1.p1.1.m1.1b"><mo id="S2.I1.i1.p1.1.m1.1.1" stretchy="false">{</mo><msub id="S2.I1.i1.p1.1.m1.1.2"><mi id="S2.I1.i1.p1.1.m1.1.2.2">p</mi><mn id="S2.I1.i1.p1.1.m1.1.2.3">1</mn></msub><mo id="S2.I1.i1.p1.1.m1.1.3">,</mo><msub id="S2.I1.i1.p1.1.m1.1.4"><mi id="S2.I1.i1.p1.1.m1.1.4.2">p</mi><mn id="S2.I1.i1.p1.1.m1.1.4.3">2</mn></msub><mo id="S2.I1.i1.p1.1.m1.1.5">,</mo><mo id="S2.I1.i1.p1.1.m1.1.6" lspace="0em" rspace="0.0835em">.</mo><mo id="S2.I1.i1.p1.1.m1.1.7" lspace="0.0835em" rspace="0.167em">.</mo><mo id="S2.I1.i1.p1.1.m1.1.8">,</mo><msub id="S2.I1.i1.p1.1.m1.1.9"><mi id="S2.I1.i1.p1.1.m1.1.9.2">p</mi><mi id="S2.I1.i1.p1.1.m1.1.9.3">N</mi></msub><mo id="S2.I1.i1.p1.1.m1.1.10" stretchy="false">}</mo></mrow><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">\{p_{1},p_{2},..,p_{N}\}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">{ italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , . . , italic_p start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math>, each of which consists of a pair of context and question <math alttext="p_{i}=(c_{i},q_{i})" class="ltx_Math" display="inline" id="S2.I1.i1.p1.2.m2.2"><semantics id="S2.I1.i1.p1.2.m2.2a"><mrow id="S2.I1.i1.p1.2.m2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.cmml"><msub id="S2.I1.i1.p1.2.m2.2.2.4" xref="S2.I1.i1.p1.2.m2.2.2.4.cmml"><mi id="S2.I1.i1.p1.2.m2.2.2.4.2" xref="S2.I1.i1.p1.2.m2.2.2.4.2.cmml">p</mi><mi id="S2.I1.i1.p1.2.m2.2.2.4.3" xref="S2.I1.i1.p1.2.m2.2.2.4.3.cmml">i</mi></msub><mo id="S2.I1.i1.p1.2.m2.2.2.3" xref="S2.I1.i1.p1.2.m2.2.2.3.cmml">=</mo><mrow id="S2.I1.i1.p1.2.m2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml"><mo id="S2.I1.i1.p1.2.m2.2.2.2.2.3" stretchy="false" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml">(</mo><msub id="S2.I1.i1.p1.2.m2.1.1.1.1.1" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml"><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml">c</mi><mi id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.I1.i1.p1.2.m2.2.2.2.2.4" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml">,</mo><msub id="S2.I1.i1.p1.2.m2.2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.cmml"><mi id="S2.I1.i1.p1.2.m2.2.2.2.2.2.2" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml">q</mi><mi id="S2.I1.i1.p1.2.m2.2.2.2.2.2.3" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.I1.i1.p1.2.m2.2.2.2.2.5" stretchy="false" xref="S2.I1.i1.p1.2.m2.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.2b"><apply id="S2.I1.i1.p1.2.m2.2.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2"><eq id="S2.I1.i1.p1.2.m2.2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.3"></eq><apply id="S2.I1.i1.p1.2.m2.2.2.4.cmml" xref="S2.I1.i1.p1.2.m2.2.2.4"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.2.2.4.1.cmml" xref="S2.I1.i1.p1.2.m2.2.2.4">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.2.2.4.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2.4.2">ğ‘</ci><ci id="S2.I1.i1.p1.2.m2.2.2.4.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.4.3">ğ‘–</ci></apply><interval closure="open" id="S2.I1.i1.p1.2.m2.2.2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2"><apply id="S2.I1.i1.p1.2.m2.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.1.1.1.1.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.2.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.2">ğ‘</ci><ci id="S2.I1.i1.p1.2.m2.1.1.1.1.1.3.cmml" xref="S2.I1.i1.p1.2.m2.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.I1.i1.p1.2.m2.2.2.2.2.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.2.m2.2.2.2.2.2.1.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S2.I1.i1.p1.2.m2.2.2.2.2.2.2.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.2">ğ‘</ci><ci id="S2.I1.i1.p1.2.m2.2.2.2.2.2.3.cmml" xref="S2.I1.i1.p1.2.m2.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.2c">p_{i}=(c_{i},q_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.2.m2.2d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.I1.i2.1.1.m1.1"><semantics id="S2.I1.i2.1.1.m1.1b"><mo id="S2.I1.i2.1.1.m1.1.1" xref="S2.I1.i2.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i2.1.1.m1.1c"><ci id="S2.I1.i2.1.1.m1.1.1.cmml" xref="S2.I1.i2.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.5"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.5.1">Short-Context Ranker</span>: A ranker model <math alttext="r(q,t)\rightarrow\mathbb{R}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.2"><semantics id="S2.I1.i2.p1.1.m1.2a"><mrow id="S2.I1.i2.p1.1.m1.2.3" xref="S2.I1.i2.p1.1.m1.2.3.cmml"><mrow id="S2.I1.i2.p1.1.m1.2.3.2" xref="S2.I1.i2.p1.1.m1.2.3.2.cmml"><mi id="S2.I1.i2.p1.1.m1.2.3.2.2" xref="S2.I1.i2.p1.1.m1.2.3.2.2.cmml">r</mi><mo id="S2.I1.i2.p1.1.m1.2.3.2.1" xref="S2.I1.i2.p1.1.m1.2.3.2.1.cmml">â¢</mo><mrow id="S2.I1.i2.p1.1.m1.2.3.2.3.2" xref="S2.I1.i2.p1.1.m1.2.3.2.3.1.cmml"><mo id="S2.I1.i2.p1.1.m1.2.3.2.3.2.1" stretchy="false" xref="S2.I1.i2.p1.1.m1.2.3.2.3.1.cmml">(</mo><mi id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml">q</mi><mo id="S2.I1.i2.p1.1.m1.2.3.2.3.2.2" xref="S2.I1.i2.p1.1.m1.2.3.2.3.1.cmml">,</mo><mi id="S2.I1.i2.p1.1.m1.2.2" xref="S2.I1.i2.p1.1.m1.2.2.cmml">t</mi><mo id="S2.I1.i2.p1.1.m1.2.3.2.3.2.3" stretchy="false" xref="S2.I1.i2.p1.1.m1.2.3.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.I1.i2.p1.1.m1.2.3.1" stretchy="false" xref="S2.I1.i2.p1.1.m1.2.3.1.cmml">â†’</mo><mi id="S2.I1.i2.p1.1.m1.2.3.3" xref="S2.I1.i2.p1.1.m1.2.3.3.cmml">â„</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.2b"><apply id="S2.I1.i2.p1.1.m1.2.3.cmml" xref="S2.I1.i2.p1.1.m1.2.3"><ci id="S2.I1.i2.p1.1.m1.2.3.1.cmml" xref="S2.I1.i2.p1.1.m1.2.3.1">â†’</ci><apply id="S2.I1.i2.p1.1.m1.2.3.2.cmml" xref="S2.I1.i2.p1.1.m1.2.3.2"><times id="S2.I1.i2.p1.1.m1.2.3.2.1.cmml" xref="S2.I1.i2.p1.1.m1.2.3.2.1"></times><ci id="S2.I1.i2.p1.1.m1.2.3.2.2.cmml" xref="S2.I1.i2.p1.1.m1.2.3.2.2">ğ‘Ÿ</ci><interval closure="open" id="S2.I1.i2.p1.1.m1.2.3.2.3.1.cmml" xref="S2.I1.i2.p1.1.m1.2.3.2.3.2"><ci id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">ğ‘</ci><ci id="S2.I1.i2.p1.1.m1.2.2.cmml" xref="S2.I1.i2.p1.1.m1.2.2">ğ‘¡</ci></interval></apply><ci id="S2.I1.i2.p1.1.m1.2.3.3.cmml" xref="S2.I1.i2.p1.1.m1.2.3.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.2c">r(q,t)\rightarrow\mathbb{R}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.2d">italic_r ( italic_q , italic_t ) â†’ blackboard_R</annotation></semantics></math> which takes a question <math alttext="q" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><mi id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><ci id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_q</annotation></semantics></math> and a piece of <em class="ltx_emph ltx_font_italic" id="S2.I1.i2.p1.5.2">short</em> text <math alttext="t" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3.1"><semantics id="S2.I1.i2.p1.3.m3.1a"><mi id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><ci id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.1d">italic_t</annotation></semantics></math>. This ranker determines a relevance score corresponding to how helpful the text <math alttext="t" class="ltx_Math" display="inline" id="S2.I1.i2.p1.4.m4.1"><semantics id="S2.I1.i2.p1.4.m4.1a"><mi id="S2.I1.i2.p1.4.m4.1.1" xref="S2.I1.i2.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.4.m4.1b"><ci id="S2.I1.i2.p1.4.m4.1.1.cmml" xref="S2.I1.i2.p1.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.4.m4.1d">italic_t</annotation></semantics></math> in answering the question <math alttext="q" class="ltx_Math" display="inline" id="S2.I1.i2.p1.5.m5.1"><semantics id="S2.I1.i2.p1.5.m5.1a"><mi id="S2.I1.i2.p1.5.m5.1.1" xref="S2.I1.i2.p1.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.5.m5.1b"><ci id="S2.I1.i2.p1.5.m5.1.1.cmml" xref="S2.I1.i2.p1.5.m5.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.5.m5.1d">italic_q</annotation></semantics></math>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S2.I1.i3.1.1.m1.1"><semantics id="S2.I1.i3.1.1.m1.1b"><mo id="S2.I1.i3.1.1.m1.1.1" xref="S2.I1.i3.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i3.1.1.m1.1c"><ci id="S2.I1.i3.1.1.m1.1.1.cmml" xref="S2.I1.i3.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.3"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.3.1">Short-Context Generator</span>: A generator model <math alttext="g(x)\rightarrow a" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.2" xref="S2.I1.i3.p1.1.m1.1.2.cmml"><mrow id="S2.I1.i3.p1.1.m1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.2.2.cmml"><mi id="S2.I1.i3.p1.1.m1.1.2.2.2" xref="S2.I1.i3.p1.1.m1.1.2.2.2.cmml">g</mi><mo id="S2.I1.i3.p1.1.m1.1.2.2.1" xref="S2.I1.i3.p1.1.m1.1.2.2.1.cmml">â¢</mo><mrow id="S2.I1.i3.p1.1.m1.1.2.2.3.2" xref="S2.I1.i3.p1.1.m1.1.2.2.cmml"><mo id="S2.I1.i3.p1.1.m1.1.2.2.3.2.1" stretchy="false" xref="S2.I1.i3.p1.1.m1.1.2.2.cmml">(</mo><mi id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml">x</mi><mo id="S2.I1.i3.p1.1.m1.1.2.2.3.2.2" stretchy="false" xref="S2.I1.i3.p1.1.m1.1.2.2.cmml">)</mo></mrow></mrow><mo id="S2.I1.i3.p1.1.m1.1.2.1" stretchy="false" xref="S2.I1.i3.p1.1.m1.1.2.1.cmml">â†’</mo><mi id="S2.I1.i3.p1.1.m1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.2.3.cmml">a</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.2"><ci id="S2.I1.i3.p1.1.m1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.2.1">â†’</ci><apply id="S2.I1.i3.p1.1.m1.1.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.2.2"><times id="S2.I1.i3.p1.1.m1.1.2.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.2.2.1"></times><ci id="S2.I1.i3.p1.1.m1.1.2.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.2.2.2">ğ‘”</ci><ci id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">ğ‘¥</ci></apply><ci id="S2.I1.i3.p1.1.m1.1.2.3.cmml" xref="S2.I1.i3.p1.1.m1.1.2.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">g(x)\rightarrow a</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_g ( italic_x ) â†’ italic_a</annotation></semantics></math> that takes some short prompt <math alttext="x" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><mi id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><ci id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">italic_x</annotation></semantics></math> and returns an answer <math alttext="a" class="ltx_Math" display="inline" id="S2.I1.i3.p1.3.m3.1"><semantics id="S2.I1.i3.p1.3.m3.1a"><mi id="S2.I1.i3.p1.3.m3.1.1" xref="S2.I1.i3.p1.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.3.m3.1b"><ci id="S2.I1.i3.p1.3.m3.1.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.3.m3.1c">a</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.3.m3.1d">italic_a</annotation></semantics></math>. This will be an aligned instruction-tuned model like many existing contemporary LM.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.6">We start the data synthesis process from the set of prompts. For one prompt <math alttext="p_{i}=(c_{i},q_{i})" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.2"><semantics id="S2.SS1.p2.1.m1.2a"><mrow id="S2.SS1.p2.1.m1.2.2" xref="S2.SS1.p2.1.m1.2.2.cmml"><msub id="S2.SS1.p2.1.m1.2.2.4" xref="S2.SS1.p2.1.m1.2.2.4.cmml"><mi id="S2.SS1.p2.1.m1.2.2.4.2" xref="S2.SS1.p2.1.m1.2.2.4.2.cmml">p</mi><mi id="S2.SS1.p2.1.m1.2.2.4.3" xref="S2.SS1.p2.1.m1.2.2.4.3.cmml">i</mi></msub><mo id="S2.SS1.p2.1.m1.2.2.3" xref="S2.SS1.p2.1.m1.2.2.3.cmml">=</mo><mrow id="S2.SS1.p2.1.m1.2.2.2.2" xref="S2.SS1.p2.1.m1.2.2.2.3.cmml"><mo id="S2.SS1.p2.1.m1.2.2.2.2.3" stretchy="false" xref="S2.SS1.p2.1.m1.2.2.2.3.cmml">(</mo><msub id="S2.SS1.p2.1.m1.1.1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.1.1.1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.1.1.1.2.cmml">c</mi><mi id="S2.SS1.p2.1.m1.1.1.1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p2.1.m1.2.2.2.2.4" xref="S2.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><msub id="S2.SS1.p2.1.m1.2.2.2.2.2" xref="S2.SS1.p2.1.m1.2.2.2.2.2.cmml"><mi id="S2.SS1.p2.1.m1.2.2.2.2.2.2" xref="S2.SS1.p2.1.m1.2.2.2.2.2.2.cmml">q</mi><mi id="S2.SS1.p2.1.m1.2.2.2.2.2.3" xref="S2.SS1.p2.1.m1.2.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.SS1.p2.1.m1.2.2.2.2.5" stretchy="false" xref="S2.SS1.p2.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.2b"><apply id="S2.SS1.p2.1.m1.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2"><eq id="S2.SS1.p2.1.m1.2.2.3.cmml" xref="S2.SS1.p2.1.m1.2.2.3"></eq><apply id="S2.SS1.p2.1.m1.2.2.4.cmml" xref="S2.SS1.p2.1.m1.2.2.4"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.2.2.4.1.cmml" xref="S2.SS1.p2.1.m1.2.2.4">subscript</csymbol><ci id="S2.SS1.p2.1.m1.2.2.4.2.cmml" xref="S2.SS1.p2.1.m1.2.2.4.2">ğ‘</ci><ci id="S2.SS1.p2.1.m1.2.2.4.3.cmml" xref="S2.SS1.p2.1.m1.2.2.4.3">ğ‘–</ci></apply><interval closure="open" id="S2.SS1.p2.1.m1.2.2.2.3.cmml" xref="S2.SS1.p2.1.m1.2.2.2.2"><apply id="S2.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.SS1.p2.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.SS1.p2.1.m1.2.2.2.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.2.2.2.2.2.1.cmml" xref="S2.SS1.p2.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p2.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS1.p2.1.m1.2.2.2.2.2.2">ğ‘</ci><ci id="S2.SS1.p2.1.m1.2.2.2.2.2.3.cmml" xref="S2.SS1.p2.1.m1.2.2.2.2.2.3">ğ‘–</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.2c">p_{i}=(c_{i},q_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.2d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math>, we break the context into a set of text chunks <math alttext="\{t_{i1},t_{i2},t_{i3},...\}" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.4"><semantics id="S2.SS1.p2.2.m2.4a"><mrow id="S2.SS1.p2.2.m2.4.4.3" xref="S2.SS1.p2.2.m2.4.4.4.cmml"><mo id="S2.SS1.p2.2.m2.4.4.3.4" stretchy="false" xref="S2.SS1.p2.2.m2.4.4.4.cmml">{</mo><msub id="S2.SS1.p2.2.m2.2.2.1.1" xref="S2.SS1.p2.2.m2.2.2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.2.2.1.1.2" xref="S2.SS1.p2.2.m2.2.2.1.1.2.cmml">t</mi><mrow id="S2.SS1.p2.2.m2.2.2.1.1.3" xref="S2.SS1.p2.2.m2.2.2.1.1.3.cmml"><mi id="S2.SS1.p2.2.m2.2.2.1.1.3.2" xref="S2.SS1.p2.2.m2.2.2.1.1.3.2.cmml">i</mi><mo id="S2.SS1.p2.2.m2.2.2.1.1.3.1" xref="S2.SS1.p2.2.m2.2.2.1.1.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.2.m2.2.2.1.1.3.3" xref="S2.SS1.p2.2.m2.2.2.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS1.p2.2.m2.4.4.3.5" xref="S2.SS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.2.m2.3.3.2.2" xref="S2.SS1.p2.2.m2.3.3.2.2.cmml"><mi id="S2.SS1.p2.2.m2.3.3.2.2.2" xref="S2.SS1.p2.2.m2.3.3.2.2.2.cmml">t</mi><mrow id="S2.SS1.p2.2.m2.3.3.2.2.3" xref="S2.SS1.p2.2.m2.3.3.2.2.3.cmml"><mi id="S2.SS1.p2.2.m2.3.3.2.2.3.2" xref="S2.SS1.p2.2.m2.3.3.2.2.3.2.cmml">i</mi><mo id="S2.SS1.p2.2.m2.3.3.2.2.3.1" xref="S2.SS1.p2.2.m2.3.3.2.2.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.2.m2.3.3.2.2.3.3" xref="S2.SS1.p2.2.m2.3.3.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S2.SS1.p2.2.m2.4.4.3.6" xref="S2.SS1.p2.2.m2.4.4.4.cmml">,</mo><msub id="S2.SS1.p2.2.m2.4.4.3.3" xref="S2.SS1.p2.2.m2.4.4.3.3.cmml"><mi id="S2.SS1.p2.2.m2.4.4.3.3.2" xref="S2.SS1.p2.2.m2.4.4.3.3.2.cmml">t</mi><mrow id="S2.SS1.p2.2.m2.4.4.3.3.3" xref="S2.SS1.p2.2.m2.4.4.3.3.3.cmml"><mi id="S2.SS1.p2.2.m2.4.4.3.3.3.2" xref="S2.SS1.p2.2.m2.4.4.3.3.3.2.cmml">i</mi><mo id="S2.SS1.p2.2.m2.4.4.3.3.3.1" xref="S2.SS1.p2.2.m2.4.4.3.3.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.2.m2.4.4.3.3.3.3" xref="S2.SS1.p2.2.m2.4.4.3.3.3.3.cmml">3</mn></mrow></msub><mo id="S2.SS1.p2.2.m2.4.4.3.7" xref="S2.SS1.p2.2.m2.4.4.4.cmml">,</mo><mi id="S2.SS1.p2.2.m2.1.1" mathvariant="normal" xref="S2.SS1.p2.2.m2.1.1.cmml">â€¦</mi><mo id="S2.SS1.p2.2.m2.4.4.3.8" stretchy="false" xref="S2.SS1.p2.2.m2.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.4b"><set id="S2.SS1.p2.2.m2.4.4.4.cmml" xref="S2.SS1.p2.2.m2.4.4.3"><apply id="S2.SS1.p2.2.m2.2.2.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.2.2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS1.p2.2.m2.2.2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.2">ğ‘¡</ci><apply id="S2.SS1.p2.2.m2.2.2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.3"><times id="S2.SS1.p2.2.m2.2.2.1.1.3.1.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.3.1"></times><ci id="S2.SS1.p2.2.m2.2.2.1.1.3.2.cmml" xref="S2.SS1.p2.2.m2.2.2.1.1.3.2">ğ‘–</ci><cn id="S2.SS1.p2.2.m2.2.2.1.1.3.3.cmml" type="integer" xref="S2.SS1.p2.2.m2.2.2.1.1.3.3">1</cn></apply></apply><apply id="S2.SS1.p2.2.m2.3.3.2.2.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.3.3.2.2.1.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS1.p2.2.m2.3.3.2.2.2.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2.2">ğ‘¡</ci><apply id="S2.SS1.p2.2.m2.3.3.2.2.3.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2.3"><times id="S2.SS1.p2.2.m2.3.3.2.2.3.1.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2.3.1"></times><ci id="S2.SS1.p2.2.m2.3.3.2.2.3.2.cmml" xref="S2.SS1.p2.2.m2.3.3.2.2.3.2">ğ‘–</ci><cn id="S2.SS1.p2.2.m2.3.3.2.2.3.3.cmml" type="integer" xref="S2.SS1.p2.2.m2.3.3.2.2.3.3">2</cn></apply></apply><apply id="S2.SS1.p2.2.m2.4.4.3.3.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.2.m2.4.4.3.3.1.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3">subscript</csymbol><ci id="S2.SS1.p2.2.m2.4.4.3.3.2.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3.2">ğ‘¡</ci><apply id="S2.SS1.p2.2.m2.4.4.3.3.3.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3.3"><times id="S2.SS1.p2.2.m2.4.4.3.3.3.1.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3.3.1"></times><ci id="S2.SS1.p2.2.m2.4.4.3.3.3.2.cmml" xref="S2.SS1.p2.2.m2.4.4.3.3.3.2">ğ‘–</ci><cn id="S2.SS1.p2.2.m2.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS1.p2.2.m2.4.4.3.3.3.3">3</cn></apply></apply><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">â€¦</ci></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.4c">\{t_{i1},t_{i2},t_{i3},...\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.4d">{ italic_t start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i 3 end_POSTSUBSCRIPT , â€¦ }</annotation></semantics></math>. The ranking model will assign chunk <math alttext="t_{ij}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><msub id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml"><mi id="S2.SS1.p2.3.m3.1.1.2" xref="S2.SS1.p2.3.m3.1.1.2.cmml">t</mi><mrow id="S2.SS1.p2.3.m3.1.1.3" xref="S2.SS1.p2.3.m3.1.1.3.cmml"><mi id="S2.SS1.p2.3.m3.1.1.3.2" xref="S2.SS1.p2.3.m3.1.1.3.2.cmml">i</mi><mo id="S2.SS1.p2.3.m3.1.1.3.1" xref="S2.SS1.p2.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S2.SS1.p2.3.m3.1.1.3.3" xref="S2.SS1.p2.3.m3.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><apply id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.3.m3.1.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p2.3.m3.1.1.2.cmml" xref="S2.SS1.p2.3.m3.1.1.2">ğ‘¡</ci><apply id="S2.SS1.p2.3.m3.1.1.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3"><times id="S2.SS1.p2.3.m3.1.1.3.1.cmml" xref="S2.SS1.p2.3.m3.1.1.3.1"></times><ci id="S2.SS1.p2.3.m3.1.1.3.2.cmml" xref="S2.SS1.p2.3.m3.1.1.3.2">ğ‘–</ci><ci id="S2.SS1.p2.3.m3.1.1.3.3.cmml" xref="S2.SS1.p2.3.m3.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">t_{ij}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">italic_t start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> a relevance score <math alttext="s_{ij}=r(q_{i},t_{ij})" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.2"><semantics id="S2.SS1.p2.4.m4.2a"><mrow id="S2.SS1.p2.4.m4.2.2" xref="S2.SS1.p2.4.m4.2.2.cmml"><msub id="S2.SS1.p2.4.m4.2.2.4" xref="S2.SS1.p2.4.m4.2.2.4.cmml"><mi id="S2.SS1.p2.4.m4.2.2.4.2" xref="S2.SS1.p2.4.m4.2.2.4.2.cmml">s</mi><mrow id="S2.SS1.p2.4.m4.2.2.4.3" xref="S2.SS1.p2.4.m4.2.2.4.3.cmml"><mi id="S2.SS1.p2.4.m4.2.2.4.3.2" xref="S2.SS1.p2.4.m4.2.2.4.3.2.cmml">i</mi><mo id="S2.SS1.p2.4.m4.2.2.4.3.1" xref="S2.SS1.p2.4.m4.2.2.4.3.1.cmml">â¢</mo><mi id="S2.SS1.p2.4.m4.2.2.4.3.3" xref="S2.SS1.p2.4.m4.2.2.4.3.3.cmml">j</mi></mrow></msub><mo id="S2.SS1.p2.4.m4.2.2.3" xref="S2.SS1.p2.4.m4.2.2.3.cmml">=</mo><mrow id="S2.SS1.p2.4.m4.2.2.2" xref="S2.SS1.p2.4.m4.2.2.2.cmml"><mi id="S2.SS1.p2.4.m4.2.2.2.4" xref="S2.SS1.p2.4.m4.2.2.2.4.cmml">r</mi><mo id="S2.SS1.p2.4.m4.2.2.2.3" xref="S2.SS1.p2.4.m4.2.2.2.3.cmml">â¢</mo><mrow id="S2.SS1.p2.4.m4.2.2.2.2.2" xref="S2.SS1.p2.4.m4.2.2.2.2.3.cmml"><mo id="S2.SS1.p2.4.m4.2.2.2.2.2.3" stretchy="false" xref="S2.SS1.p2.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S2.SS1.p2.4.m4.1.1.1.1.1.1" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p2.4.m4.1.1.1.1.1.1.2" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1.2.cmml">q</mi><mi id="S2.SS1.p2.4.m4.1.1.1.1.1.1.3" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS1.p2.4.m4.2.2.2.2.2.4" xref="S2.SS1.p2.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S2.SS1.p2.4.m4.2.2.2.2.2.2" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p2.4.m4.2.2.2.2.2.2.2" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.2.cmml">t</mi><mrow id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.cmml"><mi id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.2" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.2.cmml">i</mi><mo id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.1" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.1.cmml">â¢</mo><mi id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.3" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.SS1.p2.4.m4.2.2.2.2.2.5" stretchy="false" xref="S2.SS1.p2.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.2b"><apply id="S2.SS1.p2.4.m4.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2"><eq id="S2.SS1.p2.4.m4.2.2.3.cmml" xref="S2.SS1.p2.4.m4.2.2.3"></eq><apply id="S2.SS1.p2.4.m4.2.2.4.cmml" xref="S2.SS1.p2.4.m4.2.2.4"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.2.2.4.1.cmml" xref="S2.SS1.p2.4.m4.2.2.4">subscript</csymbol><ci id="S2.SS1.p2.4.m4.2.2.4.2.cmml" xref="S2.SS1.p2.4.m4.2.2.4.2">ğ‘ </ci><apply id="S2.SS1.p2.4.m4.2.2.4.3.cmml" xref="S2.SS1.p2.4.m4.2.2.4.3"><times id="S2.SS1.p2.4.m4.2.2.4.3.1.cmml" xref="S2.SS1.p2.4.m4.2.2.4.3.1"></times><ci id="S2.SS1.p2.4.m4.2.2.4.3.2.cmml" xref="S2.SS1.p2.4.m4.2.2.4.3.2">ğ‘–</ci><ci id="S2.SS1.p2.4.m4.2.2.4.3.3.cmml" xref="S2.SS1.p2.4.m4.2.2.4.3.3">ğ‘—</ci></apply></apply><apply id="S2.SS1.p2.4.m4.2.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2.2"><times id="S2.SS1.p2.4.m4.2.2.2.3.cmml" xref="S2.SS1.p2.4.m4.2.2.2.3"></times><ci id="S2.SS1.p2.4.m4.2.2.2.4.cmml" xref="S2.SS1.p2.4.m4.2.2.2.4">ğ‘Ÿ</ci><interval closure="open" id="S2.SS1.p2.4.m4.2.2.2.2.3.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2"><apply id="S2.SS1.p2.4.m4.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.4.m4.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.SS1.p2.4.m4.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p2.4.m4.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.SS1.p2.4.m4.2.2.2.2.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.4.m4.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p2.4.m4.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.2">ğ‘¡</ci><apply id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3"><times id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.1.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.1"></times><ci id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.2.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.2">ğ‘–</ci><ci id="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.3.cmml" xref="S2.SS1.p2.4.m4.2.2.2.2.2.2.3.3">ğ‘—</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.2c">s_{ij}=r(q_{i},t_{ij})</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.2d">italic_s start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = italic_r ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT )</annotation></semantics></math>. Based on these scores, we can produce a ranking of the text chunk indices using the estimated helpfulness, [<math alttext="r_{i1},r_{i2},r_{i3}" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.3"><semantics id="S2.SS1.p2.5.m5.3a"><mrow id="S2.SS1.p2.5.m5.3.3.3" xref="S2.SS1.p2.5.m5.3.3.4.cmml"><msub id="S2.SS1.p2.5.m5.1.1.1.1" xref="S2.SS1.p2.5.m5.1.1.1.1.cmml"><mi id="S2.SS1.p2.5.m5.1.1.1.1.2" xref="S2.SS1.p2.5.m5.1.1.1.1.2.cmml">r</mi><mrow id="S2.SS1.p2.5.m5.1.1.1.1.3" xref="S2.SS1.p2.5.m5.1.1.1.1.3.cmml"><mi id="S2.SS1.p2.5.m5.1.1.1.1.3.2" xref="S2.SS1.p2.5.m5.1.1.1.1.3.2.cmml">i</mi><mo id="S2.SS1.p2.5.m5.1.1.1.1.3.1" xref="S2.SS1.p2.5.m5.1.1.1.1.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.5.m5.1.1.1.1.3.3" xref="S2.SS1.p2.5.m5.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS1.p2.5.m5.3.3.3.4" xref="S2.SS1.p2.5.m5.3.3.4.cmml">,</mo><msub id="S2.SS1.p2.5.m5.2.2.2.2" xref="S2.SS1.p2.5.m5.2.2.2.2.cmml"><mi id="S2.SS1.p2.5.m5.2.2.2.2.2" xref="S2.SS1.p2.5.m5.2.2.2.2.2.cmml">r</mi><mrow id="S2.SS1.p2.5.m5.2.2.2.2.3" xref="S2.SS1.p2.5.m5.2.2.2.2.3.cmml"><mi id="S2.SS1.p2.5.m5.2.2.2.2.3.2" xref="S2.SS1.p2.5.m5.2.2.2.2.3.2.cmml">i</mi><mo id="S2.SS1.p2.5.m5.2.2.2.2.3.1" xref="S2.SS1.p2.5.m5.2.2.2.2.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.5.m5.2.2.2.2.3.3" xref="S2.SS1.p2.5.m5.2.2.2.2.3.3.cmml">2</mn></mrow></msub><mo id="S2.SS1.p2.5.m5.3.3.3.5" xref="S2.SS1.p2.5.m5.3.3.4.cmml">,</mo><msub id="S2.SS1.p2.5.m5.3.3.3.3" xref="S2.SS1.p2.5.m5.3.3.3.3.cmml"><mi id="S2.SS1.p2.5.m5.3.3.3.3.2" xref="S2.SS1.p2.5.m5.3.3.3.3.2.cmml">r</mi><mrow id="S2.SS1.p2.5.m5.3.3.3.3.3" xref="S2.SS1.p2.5.m5.3.3.3.3.3.cmml"><mi id="S2.SS1.p2.5.m5.3.3.3.3.3.2" xref="S2.SS1.p2.5.m5.3.3.3.3.3.2.cmml">i</mi><mo id="S2.SS1.p2.5.m5.3.3.3.3.3.1" xref="S2.SS1.p2.5.m5.3.3.3.3.3.1.cmml">â¢</mo><mn id="S2.SS1.p2.5.m5.3.3.3.3.3.3" xref="S2.SS1.p2.5.m5.3.3.3.3.3.3.cmml">3</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.3b"><list id="S2.SS1.p2.5.m5.3.3.4.cmml" xref="S2.SS1.p2.5.m5.3.3.3"><apply id="S2.SS1.p2.5.m5.1.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.1.1.1.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.5.m5.1.1.1.1.2.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1.2">ğ‘Ÿ</ci><apply id="S2.SS1.p2.5.m5.1.1.1.1.3.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1.3"><times id="S2.SS1.p2.5.m5.1.1.1.1.3.1.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1.3.1"></times><ci id="S2.SS1.p2.5.m5.1.1.1.1.3.2.cmml" xref="S2.SS1.p2.5.m5.1.1.1.1.3.2">ğ‘–</ci><cn id="S2.SS1.p2.5.m5.1.1.1.1.3.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.1.1.1.1.3.3">1</cn></apply></apply><apply id="S2.SS1.p2.5.m5.2.2.2.2.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.2.2.2.2.1.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p2.5.m5.2.2.2.2.2.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2.2">ğ‘Ÿ</ci><apply id="S2.SS1.p2.5.m5.2.2.2.2.3.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2.3"><times id="S2.SS1.p2.5.m5.2.2.2.2.3.1.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2.3.1"></times><ci id="S2.SS1.p2.5.m5.2.2.2.2.3.2.cmml" xref="S2.SS1.p2.5.m5.2.2.2.2.3.2">ğ‘–</ci><cn id="S2.SS1.p2.5.m5.2.2.2.2.3.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.2.2.2.2.3.3">2</cn></apply></apply><apply id="S2.SS1.p2.5.m5.3.3.3.3.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.5.m5.3.3.3.3.1.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p2.5.m5.3.3.3.3.2.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3.2">ğ‘Ÿ</ci><apply id="S2.SS1.p2.5.m5.3.3.3.3.3.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3.3"><times id="S2.SS1.p2.5.m5.3.3.3.3.3.1.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3.3.1"></times><ci id="S2.SS1.p2.5.m5.3.3.3.3.3.2.cmml" xref="S2.SS1.p2.5.m5.3.3.3.3.3.2">ğ‘–</ci><cn id="S2.SS1.p2.5.m5.3.3.3.3.3.3.cmml" type="integer" xref="S2.SS1.p2.5.m5.3.3.3.3.3.3">3</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.3c">r_{i1},r_{i2},r_{i3}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.3d">italic_r start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT italic_i 3 end_POSTSUBSCRIPT</annotation></semantics></math>, â€¦]. We collect the top <math alttext="M" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">M</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">italic_M</annotation></semantics></math> ranked text chunks while making sure that their concatenation can still fit in the short generator. These â€œmost helpfulâ€ chunks will together be fed into a generator to produce an answer,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{a}_{i}=g(\texttt{INST}\circ q_{i}\circ t_{r_{i1}}\circ t_{r_{i2}}\circ..%
\circ t_{r_{iM}})" class="ltx_math_unparsed" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1b"><msub id="S2.E1.m1.1.1"><mover accent="true" id="S2.E1.m1.1.1.2"><mi id="S2.E1.m1.1.1.2.2">a</mi><mo id="S2.E1.m1.1.1.2.1">^</mo></mover><mi id="S2.E1.m1.1.1.3">i</mi></msub><mo id="S2.E1.m1.1.2">=</mo><mi id="S2.E1.m1.1.3">g</mi><mrow id="S2.E1.m1.1.4"><mo id="S2.E1.m1.1.4.1" stretchy="false">(</mo><mtext class="ltx_mathvariant_monospace" id="S2.E1.m1.1.4.2">INST</mtext><mo id="S2.E1.m1.1.4.3" lspace="0.222em" rspace="0.222em">âˆ˜</mo><msub id="S2.E1.m1.1.4.4"><mi id="S2.E1.m1.1.4.4.2">q</mi><mi id="S2.E1.m1.1.4.4.3">i</mi></msub><mo id="S2.E1.m1.1.4.5" lspace="0.222em" rspace="0.222em">âˆ˜</mo><msub id="S2.E1.m1.1.4.6"><mi id="S2.E1.m1.1.4.6.2">t</mi><msub id="S2.E1.m1.1.4.6.3"><mi id="S2.E1.m1.1.4.6.3.2">r</mi><mrow id="S2.E1.m1.1.4.6.3.3"><mi id="S2.E1.m1.1.4.6.3.3.2">i</mi><mo id="S2.E1.m1.1.4.6.3.3.1">â¢</mo><mn id="S2.E1.m1.1.4.6.3.3.3">1</mn></mrow></msub></msub><mo id="S2.E1.m1.1.4.7" lspace="0.222em" rspace="0.222em">âˆ˜</mo><msub id="S2.E1.m1.1.4.8"><mi id="S2.E1.m1.1.4.8.2">t</mi><msub id="S2.E1.m1.1.4.8.3"><mi id="S2.E1.m1.1.4.8.3.2">r</mi><mrow id="S2.E1.m1.1.4.8.3.3"><mi id="S2.E1.m1.1.4.8.3.3.2">i</mi><mo id="S2.E1.m1.1.4.8.3.3.1">â¢</mo><mn id="S2.E1.m1.1.4.8.3.3.3">2</mn></mrow></msub></msub><mo id="S2.E1.m1.1.4.9" lspace="0.222em" rspace="0em">âˆ˜</mo><mo id="S2.E1.m1.1.4.10" lspace="0em" rspace="0.0835em">.</mo><mo id="S2.E1.m1.1.4.11" lspace="0.0835em" rspace="0em">.</mo><mo id="S2.E1.m1.1.4.12" lspace="0em" rspace="0.222em">âˆ˜</mo><msub id="S2.E1.m1.1.4.13"><mi id="S2.E1.m1.1.4.13.2">t</mi><msub id="S2.E1.m1.1.4.13.3"><mi id="S2.E1.m1.1.4.13.3.2">r</mi><mrow id="S2.E1.m1.1.4.13.3.3"><mi id="S2.E1.m1.1.4.13.3.3.2">i</mi><mo id="S2.E1.m1.1.4.13.3.3.1">â¢</mo><mi id="S2.E1.m1.1.4.13.3.3.3">M</mi></mrow></msub></msub><mo id="S2.E1.m1.1.4.14" stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\hat{a}_{i}=g(\texttt{INST}\circ q_{i}\circ t_{r_{i1}}\circ t_{r_{i2}}\circ..%
\circ t_{r_{iM}})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">over^ start_ARG italic_a end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_g ( INST âˆ˜ italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT âˆ˜ italic_t start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_i 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ˜ italic_t start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_i 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆ˜ . . âˆ˜ italic_t start_POSTSUBSCRIPT italic_r start_POSTSUBSCRIPT italic_i italic_M end_POSTSUBSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Here <span class="ltx_text ltx_font_typewriter" id="S2.SS1.p3.1.1">INST</span> denotes an instruction to the model to elicit an explicit reasoning in a chain-of-thought like form. This makes sure that the model can describe how it compares the information in the provided text pieces to identify the most useful ones, as well as how it combines them to arrive at the final answer. This reasoning process will be used in the next stage to help train the model in the fine-tuning stage.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">This data synthesis process leverages two critical capabilities in the original LM, relevance/usefulness analysis and understanding of short pieces of text. We combine them heuristically to build a surrogate long-context pipeline. Readers familiar with the concept of map-reduce may recognize our data synthesis as such a process.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.2">During data synthesis, we produce for each prompt, a full document ranking, a small set of helpful documents and an answer. For fine-tuning, we discard the ranking and the document set and use only the generated answer, because we do not want our model to be exposed to and learn from the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.2.1">lossy</em> retrieval pipeline. We fine-tune an LM <math alttext="f_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><msub id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2" xref="S2.SS2.p1.1.m1.1.1.2.cmml">f</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">Î¸</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">ğœƒ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">f_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT</annotation></semantics></math> to produce the CoT answer <math alttext="\hat{a_{i}}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mover accent="true" id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><msub id="S2.SS2.p1.2.m2.1.1.2" xref="S2.SS2.p1.2.m2.1.1.2.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2.2" xref="S2.SS2.p1.2.m2.1.1.2.2.cmml">a</mi><mi id="S2.SS2.p1.2.m2.1.1.2.3" xref="S2.SS2.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS2.p1.2.m2.1.1.1" xref="S2.SS2.p1.2.m2.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><ci id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1.1">^</ci><apply id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.2.1.cmml" xref="S2.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2.2">ğ‘</ci><ci id="S2.SS2.p1.2.m2.1.1.2.3.cmml" xref="S2.SS2.p1.2.m2.1.1.2.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\hat{a_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">over^ start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>, i.e.,</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\theta}=\text{argmin}_{\theta}\sum_{i}f(\hat{a_{i}}|(c_{i},q_{i}))" class="ltx_Math" display="block" id="S2.E2.m1.1"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mover accent="true" id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml">Î¸</mi><mo id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">^</mo></mover><mo id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml"><mtext id="S2.E2.m1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.3.2a.cmml">argmin</mtext><mi id="S2.E2.m1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.3.3.cmml">Î¸</mi></msub><mo id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.cmml"><munder id="S2.E2.m1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.2.cmml"><mo id="S2.E2.m1.1.1.1.1.2.2" movablelimits="false" xref="S2.E2.m1.1.1.1.1.2.2.cmml">âˆ‘</mo><mi id="S2.E2.m1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.2.3.cmml">i</mi></munder><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.3.cmml">f</mi><mo id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.2.cmml">â¢</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S2.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S2.E2.m1.1.1.1.1.1.1.1.1.4" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.2.cmml">a</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.4.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.1.cmml">^</mo></mover><mo fence="false" id="S2.E2.m1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.3" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><msub id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">c</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.4" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">,</mo><msub id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml">q</mi><mi id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.5" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S2.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2"></eq><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><ci id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1">^</ci><ci id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2">ğœƒ</ci></apply><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><apply id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.3.2a.cmml" xref="S2.E2.m1.1.1.1.3.2"><mtext id="S2.E2.m1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.3.2">argmin</mtext></ci><ci id="S2.E2.m1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.3.3">ğœƒ</ci></apply><apply id="S2.E2.m1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1"><apply id="S2.E2.m1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.2">subscript</csymbol><sum id="S2.E2.m1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.2.2"></sum><ci id="S2.E2.m1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><times id="S2.E2.m1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.3">ğ‘“</ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.3">conditional</csymbol><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4"><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.1">^</ci><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.4.2.3">ğ‘–</ci></apply></apply><interval closure="open" id="S2.E2.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2"><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1.1.2.2.2.3">ğ‘–</ci></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\hat{\theta}=\text{argmin}_{\theta}\sum_{i}f(\hat{a_{i}}|(c_{i},q_{i}))</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.1d">over^ start_ARG italic_Î¸ end_ARG = argmin start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f ( over^ start_ARG italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG | ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In this setup, we provide the model a lossless, unfiltered access to the full context. In addition, we pair it with an answer with extra training signals, a CoT describing an information extraction process of picking up useful pieces of information and sorting through. As with any other deep learning application, we desire that the large over-parametrized LM can learn during the optimization process to fit a long-context understanding function that will lead to similar reasoning and answer. To keep it simple, we fine-tune the model with teacher forcing using a log likelihood loss. Only the answer tokens participate in loss computation with the context token loss masked out during training.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We consider two realistic tasks, long-context retrieval augmented generationÂ (<cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib26" title="">2024</a>)</cite>) and long-context reading comprehension. We pick long-context RAG as our major evaluation since the task, while being very useful, has only very recently be carefully studied. This is more aligned with our desired setup, distinct yet useful long context tasks with little supervision.
We consider the following two datasets:
<span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Natural QuestionÂ (NQ)</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib35" title="">2019</a>)</cite>
and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.2">TriviaQAÂ (TQA)</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Joshi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib27" title="">2017</a>)</cite>
We use Wikipedia as the knowledge source,
and BGEÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib65" title="">2024b</a>)</cite> which is a dense retrieverÂ <cite class="ltx_cite ltx_citemacro_cite">Karpukhin etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib30" title="">2020</a>)</cite>.
We report the exact matchÂ (EM) metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">For long-context reading comprehension, we used the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">NarrativeQA</span> datasetÂ <cite class="ltx_cite ltx_citemacro_cite">KoÄiskÃ½ etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib33" title="">2018</a>)</cite>. The dataset has been out for several years at the time of writing and has since been a standard evaluation for long context. We expect many long-context models being trained on some form of its train set. Nevertheless, we still use it as reference to understand how our self-supervised approach compares to the supervised ones. We used the curated test set from LongBenchÂ <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib4" title="">2023</a>)</cite>. We report the token-level F1 metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">While these datasets have short gold answer, modern LMs tend to produce long answers. In order to evaluate EM or F1 scores, we employ an additional short answer extraction step by few-shot prompting a Llama-3-8B-Instruct model using the prompt introduced by <cite class="ltx_cite ltx_citemacro_cite">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib26" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Synthesis</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">For data synthesis, in order to keep it simple, we implement the ranker and the short context generator by prompting the same LM, Llama-3-8B-Instruct. This is the short-context version of the latest generation of Llama modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>. We use the 8B variant instead of the 70B or 405B to demonstrate the applicability of <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">ACER</span> in a cost efficient setup.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Ranker Model</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We show the prompt of the ranker model in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S3.F2" title="Figure 2 â€£ Ranker Model â€£ 3.2 Data Synthesis â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 2</span></a>. We instruct the model to read the question and context chunkÂ (referred to as passage in the prompt) and <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px1.p1.1.1">think step-by-step</em> to decide the helpfulness of the passage for answering the question. We formulate it as a multiple choice question where the model is instructed to choose from 5 options from the most a) â€œproviding exact answerâ€ to e) â€œnot relatedâ€.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><svg class="ltx_picture ltx_centering" height="246.62" id="S3.F2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,246.62) matrix(1 0 0 -1 0 0)"><g fill="#606060" fill-opacity="1.0"><path d="M 0 9.69 L 0 226.21 C 0 231.56 4.34 235.89 9.69 235.89 L 590.31 235.89 C 595.66 235.89 600 231.56 600 226.21 L 600 9.69 C 600 4.34 595.66 0 590.31 0 L 9.69 0 C 4.34 0 0 4.34 0 9.69 Z" style="stroke:none"></path></g><g fill="#ECECEC" fill-opacity="1.0"><path d="M 0.69 10.38 L 0.69 225.52 C 0.69 230.87 5.03 235.2 10.38 235.2 L 589.62 235.2 C 594.97 235.2 599.31 230.87 599.31 225.52 L 599.31 10.38 C 599.31 5.03 594.97 0.69 589.62 0.69 L 10.38 0.69 C 5.03 0.69 0.69 5.03 0.69 10.38 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 241.52 224.08)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 3.15 L 0 19.39 C 0 21.13 1.41 22.54 3.15 22.54 L 109.21 22.54 C 110.95 22.54 112.36 21.13 112.36 19.39 L 112.36 3.15 C 112.36 1.41 110.95 0 109.21 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#CCCCCC" fill-opacity="1.0"><path d="M 1.18 3.15 L 1.18 19.39 C 1.18 20.47 2.06 21.35 3.15 21.35 L 109.21 21.35 C 110.29 21.35 111.17 20.47 111.17 19.39 L 111.17 3.15 C 111.17 2.06 110.29 1.18 109.21 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="94.25"><span class="ltx_text ltx_font_bold" id="S3.F2.pic1.1.1.1.1.1.1.1">Ranker Prompt</span></foreignobject></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 18.47 18.47)"><foreignobject color="#000000" height="195.02" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="563.07">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F2.pic1.2.2.2.1.1" style="width:406.9pt;">
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.1">Letâ€™s think step by step: decide if the given question can be answered by the given passage.

<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.2">Choose from the following options:</span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.3">a) The passage provides the exact answer to the given question.</span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.4">b) The passage provides a partial answer to the question.</span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.5">c) The passage provides an answer to a similar but different question.</span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.6">d) The passage is only tangentially related to the question.</span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.7">e) The passage is not related to the question.</span>
<br class="ltx_break"/>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.8">The last line of your answer should be:
Answer: [a)/b)/c)/d)/e)].

<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.9">Question:
<span class="ltx_text" id="S3.F2.pic1.2.2.2.1.1.9.1" style="background-color:#FFBFBF;">{question}</span>
<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F2.pic1.2.2.2.1.1.10">Passage:
<span class="ltx_text" id="S3.F2.pic1.2.2.2.1.1.10.1" style="background-color:#FFBFBF;">{passages}</span></span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Prompts given to the LM to produce relevance judgement.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Generator Model</h5>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">We show the prompt for the generator model in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S3.F3" title="Figure 3 â€£ Generator Model â€£ 3.2 Data Synthesis â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a>. The model is instructed to read the given passages and extract <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS0.Px2.p1.1.1">several</em> pieces of potentially helpful information before it makes decision on what evidence to use and then use it to answer the question.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><svg class="ltx_picture ltx_centering" height="234.75" id="S3.F3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,234.75) matrix(1 0 0 -1 0 0)"><g fill="#606060" fill-opacity="1.0"><path d="M 0 6.92 L 0 213.32 C 0 217.14 3.1 220.24 6.92 220.24 L 593.08 220.24 C 596.9 220.24 600 217.14 600 213.32 L 600 6.92 C 600 3.1 596.9 0 593.08 0 L 6.92 0 C 3.1 0 0 3.1 0 6.92 Z" style="stroke:none"></path></g><g fill="#ECECEC" fill-opacity="1.0"><path d="M 0.55 7.47 L 0.55 212.77 C 0.55 216.59 3.65 219.69 7.47 219.69 L 592.53 219.69 C 596.35 219.69 599.45 216.59 599.45 212.77 L 599.45 7.47 C 599.45 3.65 596.35 0.55 592.53 0.55 L 7.47 0.55 C 3.65 0.55 0.55 3.65 0.55 7.47 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 232.7 212.37)"><g class="ltx_nestedsvg" fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="matrix(1 0 0 1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 3.15 L 0 19.23 C 0 20.97 1.41 22.38 3.15 22.38 L 126.83 22.38 C 128.57 22.38 129.98 20.97 129.98 19.23 L 129.98 3.15 C 129.98 1.41 128.57 0 126.83 0 L 3.15 0 C 1.41 0 0 1.41 0 3.15 Z" style="stroke:none"></path></g><g fill="#CCCCCC" fill-opacity="1.0"><path d="M 1.18 3.15 L 1.18 19.23 C 1.18 20.32 2.06 21.2 3.15 21.2 L 126.83 21.2 C 127.92 21.2 128.8 20.32 128.8 19.23 L 128.8 3.15 C 128.8 2.06 127.92 1.18 126.83 1.18 L 3.15 1.18 C 2.06 1.18 1.18 2.06 1.18 3.15 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 9.06 7.81)"><foreignobject color="#000000" height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="111.87"><span class="ltx_text ltx_font_bold" id="S3.F3.pic1.3.3.3.1.1.1.1">Generator Prompt</span></foreignobject></g></g></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 11.41 11.41)"><foreignobject color="#000000" height="193.49" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="577.18">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:417.1pt;">
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.3">Answer the following question by reading the given passages.

<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4">Question: <span class="ltx_text" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.4.1" style="background-color:#FFBFBF;">{question}</span>
<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.5">Passages:</span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6"><span class="ltx_text" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.6.1" style="background-color:#FFBFBF;">{passages}</span>
<br class="ltx_break"/>
<br class="ltx_break"/></span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.7">Think carefully and start your answer by first extracting a few pieces of related information from the passages.</span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.8">Then identify the most useful piece of information and use it to answer the question.</span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">On the last line, end your response with a complete full sentence answer of the following format: Answer: <math alttext="&lt;" class="ltx_Math" display="inline" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><lt id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">&lt;</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">&lt;</annotation></semantics></math>complete answer<math alttext="&gt;" class="ltx_Math" display="inline" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1"><semantics id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><gt id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1d">&gt;</annotation></semantics></math></span>
<span class="ltx_p" id="S3.F3.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.9">Answer â€œNo answer was foundâ€ if you cannot find the answer in the given passages.</span>
</span></foreignobject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompts given to the LM to produce the final CoT answer.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">We use prompts in the corresponding datasetâ€™s training set. For RAG tasks, we obtain the context by using a dense retriever to retrieve 100 passages from Wikipedia. We use the DPR version of Wikipedia dump, where the documents are splited into chunks of 100 words.
For NarrativeQA, the context is simply the original full book. We do not make any additional edit over it.
We apply an addition data augmentation technique to the RAG data: we perform a sliding window shuffle of candidates passages. We use window size of 30 and a stride of 20. This is used to efficiently emulate a ranking noise where the absolute change in position decays with the change size. One copy of the noised data is used as well as one with original ranking.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p3.1">For the creation of the final data, we reuse the generator prompt templateÂ (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S3.F3" title="Figure 3 â€£ Generator Model â€£ 3.2 Data Synthesis â€£ 3 Experimental Setup â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 3</span></a>) with the full context as question. We combine it with the generated answer using a chat template.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We directly fine-tune the <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p1.1.1">ACER</span> models over long context without approximation. This section describes the training details as well as contemporary training technologies/techniques we adopted.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Implementation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">We train over the synthesized data for 1 epoch with a batch size of 128 examples. We use a max sequence length of 32k for RAG and 64k for reading comprehension, and therefore this amounts up to 4M tokens and 8M tokens batch, respectively. These lengths are picked to cover most of the test lengths while at the same time keeping training on our accelerators viable and efficient. At the test time, RoPE extrapolationÂ <cite class="ltx_cite ltx_citemacro_citep">(Su etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib55" title="">2024</a>)</cite> is utilized for sequences of longer length. We use a <math alttext="3e-6" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.1"><semantics id="S3.SS3.SSS1.p1.1.m1.1a"><mrow id="S3.SS3.SSS1.p1.1.m1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.cmml"><mrow id="S3.SS3.SSS1.p1.1.m1.1.1.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml"><mn id="S3.SS3.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.2.cmml">3</mn><mo id="S3.SS3.SSS1.p1.1.m1.1.1.2.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.1.cmml">â¢</mo><mi id="S3.SS3.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S3.SS3.SSS1.p1.1.m1.1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S3.SS3.SSS1.p1.1.m1.1.1.3" xref="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.1b"><apply id="S3.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1"><minus id="S3.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.1"></minus><apply id="S3.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2"><times id="S3.SS3.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.1"></times><cn id="S3.SS3.SSS1.p1.1.m1.1.1.2.2.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.2">3</cn><ci id="S3.SS3.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.2.3">ğ‘’</ci></apply><cn id="S3.SS3.SSS1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS3.SSS1.p1.1.m1.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.1c">3e-6</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.1d">3 italic_e - 6</annotation></semantics></math> learning rate with Adam optimizerÂ <cite class="ltx_cite ltx_citemacro_citep">(Kingma &amp; Ba, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib32" title="">2015</a>)</cite>. For model initialization, we still require a model that has a sufficiently long context length. This can be achieved unsupervisedly by training on long-context corpora. To save cost, we borrow Llama-3-8B-ProLong-Base modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib19" title="">2024</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="princeton-nlp/Llama-3-8B-ProLong-512k-Base" title="">princeton-nlp/Llama-3-8B-ProLong-512k-Base</a></span></span></span>, an open model unsupervisedly context-extended from our data synthesizer model Llama-3-8B-Instruct (The authors made decision to build a base model from an instruction tuned version.)</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Infrastructure</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">We train our models using JAXÂ <cite class="ltx_cite ltx_citemacro_citep">(Bradbury etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib7" title="">2018</a>)</cite> on cloud TPUv4sÂ <cite class="ltx_cite ltx_citemacro_citep">(Jouppi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib28" title="">2023</a>)</cite> provided by TPU Research Cloud. These 4th generation TPUs are built with relatively small-size 32GB HBM per chip while interconnected with high bandwidth fibers. We therefore opt to partition computation and perform our training with 3D parallelism of data parallelism, sequence parallelism, and tensor parallelism. We train on 32 TPUv4 chips in a <math alttext="(2,4,4)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.3"><semantics id="S3.SS3.SSS2.p1.1.m1.3a"><mrow id="S3.SS3.SSS2.p1.1.m1.3.4.2" xref="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.3.4.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml">(</mo><mn id="S3.SS3.SSS2.p1.1.m1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.cmml">2</mn><mo id="S3.SS3.SSS2.p1.1.m1.3.4.2.2" xref="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml">,</mo><mn id="S3.SS3.SSS2.p1.1.m1.2.2" xref="S3.SS3.SSS2.p1.1.m1.2.2.cmml">4</mn><mo id="S3.SS3.SSS2.p1.1.m1.3.4.2.3" xref="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml">,</mo><mn id="S3.SS3.SSS2.p1.1.m1.3.3" xref="S3.SS3.SSS2.p1.1.m1.3.3.cmml">4</mn><mo id="S3.SS3.SSS2.p1.1.m1.3.4.2.4" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.3b"><vector id="S3.SS3.SSS2.p1.1.m1.3.4.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.3.4.2"><cn id="S3.SS3.SSS2.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.1.1">2</cn><cn id="S3.SS3.SSS2.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.2.2">4</cn><cn id="S3.SS3.SSS2.p1.1.m1.3.3.cmml" type="integer" xref="S3.SS3.SSS2.p1.1.m1.3.3">4</cn></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.3c">(2,4,4)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.3d">( 2 , 4 , 4 )</annotation></semantics></math> configuration mesh and map these axes to data, sequence, and tensor parallelism axes respectively.
Optimizer states as well as the full-precision copy of model weights are kept fully sharded over the mesh, over the entire course of training. We manually shard model, optimizer, and critical activation in attention and feed-forward network. We use JAXâ€™s <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS2.p1.1.1">shard_map</span> to shard flash attentionÂ <cite class="ltx_cite ltx_citemacro_citep">(Dao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib14" title="">2022</a>; Dao, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib13" title="">2024</a>)</cite> TPU kernels<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py" title="">https://github.com/jax-ml/jax/blob/main/jax/experimental/pallas/ops/tpu/flash_attention.py</a></span></span></span> across model axis by heads. The rest of the shardings are decided by the XLA SPMD compilerÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib69" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">We compute loss only over the answer, with the long context receiving only implicit gradients. Taking advantage of this to save memory and flops, on top of using a loss mask, we (dynamically) slice the last layer of hidden states at each batch instance into a smaller answer tensor. Only this smaller tensor is passed to the LM head for out-projection and loss computation. In the backward pass, this will translate into a (dynamic) update of the gradient tensor.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Compared Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">For comparison, we consider models that are relatively open and closely related to the base model we use for <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p1.1.1">ACER</span>, Llama-3.1-Instruct. We acknowledge the effectiveness of proprietary models such as GPT4Â <cite class="ltx_cite ltx_citemacro_cite">OpenAI (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib43" title="">2023</a>)</cite>. We do not include them here, because our focus is to evaluate a method of data synthesis and the models and data they use are not directly comparable.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Specifically, we compare <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p2.1.1">ACER</span> with long-context LMs fine-tuned on open data as well as those fine-tuned on closed data. Specifically, we consider the following,</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i1.1.1.m1.1"><semantics id="S3.I1.i1.1.1.m1.1b"><mo id="S3.I1.i1.1.1.m1.1.1" xref="S3.I1.i1.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i1.1.1.m1.1c"><ci id="S3.I1.i1.1.1.m1.1.1.cmml" xref="S3.I1.i1.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Together-Llama-2-7B-32K-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(Together, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib57" title="">2023</a>)</cite></span>: A model by TogetherAI by extending Llama2 to a 32k context size. It is fine-tuned on an open mixture of long-context question answering and summarization data.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i2.1.1.m1.1"><semantics id="S3.I1.i2.1.1.m1.1b"><mo id="S3.I1.i2.1.1.m1.1.1" xref="S3.I1.i2.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i2.1.1.m1.1c"><ci id="S3.I1.i2.1.1.m1.1.1.cmml" xref="S3.I1.i2.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Llama-3-8B-ProLong-512k-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib19" title="">2024</a>)</cite></span>: This is a Llama3-8B-Instruct context extended on open long context dataset. It is then fine-tuned on UltraChatÂ <cite class="ltx_cite ltx_citemacro_citep">(Ding etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib16" title="">2023</a>)</cite>, a large fine-tuning dataset generated by GPT-4. The creator found it most helpful amongst open instruction-tuning datasets. This model closely relates to ours and share the same base model, offering a straightforward comparison.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i3.1.1.m1.1"><semantics id="S3.I1.i3.1.1.m1.1b"><mo id="S3.I1.i3.1.1.m1.1.1" xref="S3.I1.i3.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i3.1.1.m1.1c"><ci id="S3.I1.i3.1.1.m1.1.1.cmml" xref="S3.I1.i3.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">Llama3-8B-Instruct (Truncation)Â <cite class="ltx_cite ltx_citemacro_citep">(MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite></span>: Llama3-8B-Instruct is a 8k context model pre-trained and instruction-tuned by MetaAI with closed data. In this setup, we truncate the input to fit it into the context.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i4.1.1.m1.1"><semantics id="S3.I1.i4.1.1.m1.1b"><mo id="S3.I1.i4.1.1.m1.1.1" xref="S3.I1.i4.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i4.1.1.m1.1c"><ci id="S3.I1.i4.1.1.m1.1.1.cmml" xref="S3.I1.i4.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i4.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i4.p1.1.1">Llama3-8B-Instruct (RAG)Â <cite class="ltx_cite ltx_citemacro_citep">(MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite></span>: This is Llama3-8B-Instruct reading a small set of input chunks generated with the <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.I1.i4.p1.1.2">same</span> retrieval pipeline used in the <span class="ltx_text ltx_font_smallcaps" id="S3.I1.i4.p1.1.3">ACER</span> data synthesis.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i5.1.1.m1.1"><semantics id="S3.I1.i5.1.1.m1.1b"><mo id="S3.I1.i5.1.1.m1.1.1" xref="S3.I1.i5.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i5.1.1.m1.1c"><ci id="S3.I1.i5.1.1.m1.1.1.cmml" xref="S3.I1.i5.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i5.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i5.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i5.p1.1.1">Llama3.1-8B-InstructÂ <cite class="ltx_cite ltx_citemacro_citep">(MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite></span>: This is the second iteration of the Llama3 model by MetaAI. Its context is systematically extended by Meta GenAI team.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S3.I1.i6.1.1.m1.1"><semantics id="S3.I1.i6.1.1.m1.1b"><mo id="S3.I1.i6.1.1.m1.1.1" xref="S3.I1.i6.1.1.m1.1.1.cmml">âˆ™</mo><annotation-xml encoding="MathML-Content" id="S3.I1.i6.1.1.m1.1c"><ci id="S3.I1.i6.1.1.m1.1.1.cmml" xref="S3.I1.i6.1.1.m1.1.1">âˆ™</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i6.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i6.1.1.m1.1e">âˆ™</annotation></semantics></math></span>
<div class="ltx_para ltx_noindent" id="S3.I1.i6.p1">
<p class="ltx_p" id="S3.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i6.p1.1.1">Mistral-Nemo-Instruct-2407Â <cite class="ltx_cite ltx_citemacro_citep">(MistralAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib42" title="">2024</a>)</cite></span>: A larger 12B model trained by MistralAI and Nvidia using closed data. It has a native 128k context length.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">We perform inference with vLLMÂ <cite class="ltx_cite ltx_citemacro_citep">(Kwon etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib36" title="">2023</a>)</cite> with greedy decoding. Models always read the full context and extrapolate when reading longer context than the original training-time length except in Llama3-8B-Instruct (Truncation).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Results</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S4.T1" title="Table 1 â€£ 4 Experimental Results â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">TableÂ 1</span></a>, we show the performance of the compared systems as well as <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">ACER</span> on the evaluation datasets. We see a general trend that <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">ACER</span> outperforms the compared systems with decent margins especially on the novel long-context RAG tasks. We observe a general trend that the closed data model performing better than models trained on open data. Specifically, the Together-Llama-2 model, which is based on the previous generation Llama model, significantly underperforms all other model. This is likely due to two facts that the model essentially stems from an earlier generation, and its shorter context requires it to do more extrapolation.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T1.1.2.1.1" rowspan="2">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.2.1.1.1">
<span class="ltx_p" id="S4.T1.1.2.1.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.1.1.1.1">Model</span></span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.2.1">NQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.3.1">TQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.2.1.4.1">NarrativeQA</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.1">(EM)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">(EM)</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">(F1)</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.4.3.1.1">
<span class="ltx_p" id="S4.T1.1.4.3.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.1.4.3.1.1.1.1">Supervised Open Data</span></span>
</span>
</td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.4.3.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.4.3.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.4.3.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.5.4.1.1">
<span class="ltx_p" id="S4.T1.1.5.4.1.1.1" style="width:170.7pt;">Together-Llama-2-7B-32K-Instruct</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">0.172</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">0.299</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4">0.013</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.6.5.1.1">
<span class="ltx_p" id="S4.T1.1.6.5.1.1.1" style="width:170.7pt;">Llama-3-8B-ProLong-512k-Instruct</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2">0.260</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">0.457</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.4">0.150</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.7.6.1.1">
<span class="ltx_p" id="S4.T1.1.7.6.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.1.7.6.1.1.1.1">Supervised Closed Data</span></span>
</span>
</td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.7.6.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.7.6.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.7.6.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.8.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.8.7.1.1">
<span class="ltx_p" id="S4.T1.1.8.7.1.1.1" style="width:170.7pt;">Llama3-8B-Instruct (Truncation)</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.2">0.388</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.3">0.567</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.4">0.076</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.9.8.1.1">
<span class="ltx_p" id="S4.T1.1.9.8.1.1.1" style="width:170.7pt;">Llama3-8B-Instruct (RAG)</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.2">0.408</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.3">0.611</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.4">0.161</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.10.9.1.1">
<span class="ltx_p" id="S4.T1.1.10.9.1.1.1" style="width:170.7pt;">Llama3.1-8B-Instruct</span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.2">0.312</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.3">0.518</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.4"><span class="ltx_text ltx_font_bold" id="S4.T1.1.10.9.4.1">0.217</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.1.1.1">
<span class="ltx_p" id="S4.T1.1.1.1.1.1" style="width:170.7pt;">Mistral-Nemo-Instruct-2407<math alttext="{}^{\text{12B}}" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.1.m1.1a"><msup id="S4.T1.1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S4.T1.1.1.1.1.1.m1.1.1a" xref="S4.T1.1.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S4.T1.1.1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.1.1.m1.1.1.1a.cmml">12B</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1"><ci id="S4.T1.1.1.1.1.1.m1.1.1.1a.cmml" xref="S4.T1.1.1.1.1.1.m1.1.1.1"><mtext id="S4.T1.1.1.1.1.1.m1.1.1.1.cmml" mathsize="70%" xref="S4.T1.1.1.1.1.1.m1.1.1.1">12B</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.1.m1.1c">{}^{\text{12B}}</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.1.m1.1d">start_FLOATSUPERSCRIPT 12B end_FLOATSUPERSCRIPT</annotation></semantics></math></span>
</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.2">0.365</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.3">0.534</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4">0.072</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T1.1.11.10.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.11.10.1.1">
<span class="ltx_p" id="S4.T1.1.11.10.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_italic" id="S4.T1.1.11.10.1.1.1.1">Self-Supervised</span></span>
</span>
</td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.11.10.2"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.11.10.3"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.1.11.10.4"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T1.1.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T1.1.12.11.1.1">
<span class="ltx_p" id="S4.T1.1.12.11.1.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_smallcaps" id="S4.T1.1.12.11.1.1.1.1">ACER</span> (ours)</span>
</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.12.11.2"><span class="ltx_text ltx_font_bold" id="S4.T1.1.12.11.2.1">0.446</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.12.11.3"><span class="ltx_text ltx_font_bold" id="S4.T1.1.12.11.3.1">0.648</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.12.11.4">0.189</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Performance comparison of <span class="ltx_text ltx_font_smallcaps" id="S4.T1.3.1">ACER</span> and baselines on Natural Question (NQ), TriviaQA (TQA), and NarrativeQA. The best performance on each dataset is boldfaced.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Now, to remind our reader, the rest of the 8B models, all came from the same base model Llama-3-8B-Base. Despite this fact, we see vastly different performance. The ProLong model performs decently on NarrativeQA but falls behind on the RAG tasks. This is not surprising as the UltraChat supervision it uses relates more closely to standard-form question answering but can be very different from tasks in the wild like long-context RAG here. On the other hand, we see with the more involved long-context extension done by MetaAI, the Llama-3.1 model significantly outperforms Prolong. It actually also attains the best performance on NarrativeQA, likely because the data it uses may contain long-context QA data. Our <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.1">ACER</span> model is the second best on NarrativeQA, still out-performing all other systems, suggesting that our self-supervised method is still competitive when no specific supervised data is present. Nevertheless, <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.2">ACER</span> is the best-performing on the RAG datasets, again confirming the usefulness of our self-supervised method.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">We do also note that the 12B Mistral-Nemo does not always show decisive advantage over the 8B models. This demonstrates further that the model size and capability advantage do not always translate into long-context performance. As we discussed before, long context means more task diversity and a modelâ€™s task fitness becomes as critical; here we see the Mistral model performs decently on RAG but is lacking on Narrative QA. This again shows the usefulness of <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.1">ACER</span> which is able to self-supervisedly teach a model a new task.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">When compared with Llama3-8B-Instruct (RAG), which uses Llama-3 with the <span class="ltx_text ltx_font_smallcaps" id="S4.p4.1.1">ACER</span> retrieval pipeline in a test-time ad-hoc manner, we still see a decent performance boost in <span class="ltx_text ltx_font_smallcaps" id="S4.p4.1.2">ACER</span>. This suggests that learning and bootstrapping from <span class="ltx_text ltx_font_smallcaps" id="S4.p4.1.3">ACER</span> generated data can transcend the original data synthesis pipeline, as we desired.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Analysis</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparing Llama-3.1 and <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.1.1">ACER</span> at Different Context Sizes</h3>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S5.F4.3" style="width:397.5pt;height:230.7pt;vertical-align:-230.7pt;"><span class="ltx_transformed_inner" style="transform:translate(3.0pt,0.0pt) scale(1.01507427366747,1.01507427366747) ;">
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.1"><svg class="ltx_picture" height="246.8" id="S5.F4.1.pic1" overflow="visible" version="1.1" width="252.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,246.8) matrix(1 0 0 -1 0 0) translate(36.41,0) translate(0,37.37) matrix(1.0 0.0 0.0 1.0 -36.41 -37.37)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(36.41,0) translate(0,37.37)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 9.81 0 L 9.81 179.33 M 19.62 0 L 19.62 179.33 M 39.25 0 L 39.25 179.33 M 98.12 0 L 98.12 179.33 M 196.23 0 L 196.23 179.33" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 17.93 L 215.86 17.93 M 0 35.87 L 215.86 35.87 M 0 53.8 L 215.86 53.8 M 0 71.73 L 215.86 71.73 M 0 89.66 L 215.86 89.66 M 0 107.6 L 215.86 107.6 M 0 125.53 L 215.86 125.53 M 0 143.46 L 215.86 143.46" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 9.81 0 L 9.81 5.91 M 19.62 0 L 19.62 5.91 M 39.25 0 L 39.25 5.91 M 98.12 0 L 98.12 5.91 M 196.23 0 L 196.23 5.91 M 9.81 179.33 L 9.81 173.42 M 19.62 179.33 L 19.62 173.42 M 39.25 179.33 L 39.25 173.42 M 98.12 179.33 L 98.12 173.42 M 196.23 179.33 L 196.23 173.42" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 17.93 L 5.91 17.93 M 0 35.87 L 5.91 35.87 M 0 53.8 L 5.91 53.8 M 0 71.73 L 5.91 71.73 M 0 89.66 L 5.91 89.66 M 0 107.6 L 5.91 107.6 M 0 125.53 L 5.91 125.53 M 0 143.46 L 5.91 143.46 M 215.86 17.93 L 209.95 17.93 M 215.86 35.87 L 209.95 35.87 M 215.86 53.8 L 209.95 53.8 M 215.86 71.73 L 209.95 71.73 M 215.86 89.66 L 209.95 89.66 M 215.86 107.6 L 209.95 107.6 M 215.86 125.53 L 209.95 125.53 M 215.86 143.46 L 209.95 143.46" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 179.33 L 215.86 179.33 L 215.86 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 6.35 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><math alttext="5" class="ltx_Math" display="inline" id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">5</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 12.7 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="10" class="ltx_Math" display="inline" id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">10</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.33 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="20" class="ltx_Math" display="inline" id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">20</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 91.2 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="50" class="ltx_Math" display="inline" id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">50</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 185.86 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="20.76"><math alttext="100" class="ltx_Math" display="inline" id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1d">100</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 13.47)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.32" class="ltx_Math" display="inline" id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.32</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">0.32</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">0.32</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1d">0.32</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 31.41)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.34" class="ltx_Math" display="inline" id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.34</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">0.34</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">0.34</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1d">0.34</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 49.34)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.36" class="ltx_Math" display="inline" id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.36</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">0.36</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">0.36</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1d">0.36</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 67.27)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.38" class="ltx_Math" display="inline" id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.38</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">0.38</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">0.38</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1d">0.38</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -24.88 85.2)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.4" class="ltx_Math" display="inline" id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">0.4</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1d">0.4</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 103.14)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.42" class="ltx_Math" display="inline" id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.42</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">0.42</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">0.42</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1d">0.42</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 121.07)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.44" class="ltx_Math" display="inline" id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.44</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">0.44</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">0.44</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1d">0.44</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 139)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.46" class="ltx_Math" display="inline" id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.46</mn><annotation-xml encoding="MathML-Content" id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">0.46</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1c">0.46</annotation><annotation encoding="application/x-llamapun" id="S5.F4.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1d">0.46</annotation></semantics></math></foreignobject></g><clippath id="pgfcp13"><path d="M 0 0 L 215.86 0 L 215.86 179.33 L 0 179.33 Z"></path></clippath><g clip-path="url(#pgfcp13)"><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt"><path d="M 9.81 77.11 L 19.62 74.42 L 39.25 46.62 L 98.12 37.66 L 196.23 18.83" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt"><path d="M 9.81 82.96 L 19.62 107.05 L 39.25 116.98 L 98.12 122.7 L 196.23 131.64" style="fill:none"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt"><path d="M 7.04 74.34 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 16.86 71.65 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 36.48 43.86 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 95.35 34.89 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 193.47 16.06 h 5.53 v 5.53 h -5.53 Z"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt"><path d="M 12.58 82.96 C 12.58 84.49 11.34 85.72 9.81 85.72 C 8.28 85.72 7.04 84.49 7.04 82.96 C 7.04 81.43 8.28 80.19 9.81 80.19 C 11.34 80.19 12.58 81.43 12.58 82.96 Z M 9.81 82.96" style="fill:none"></path><path d="M 22.39 107.05 C 22.39 108.58 21.15 109.82 19.62 109.82 C 18.09 109.82 16.86 108.58 16.86 107.05 C 16.86 105.52 18.09 104.28 19.62 104.28 C 21.15 104.28 22.39 105.52 22.39 107.05 Z M 19.62 107.05" style="fill:none"></path><path d="M 42.01 116.98 C 42.01 118.51 40.78 119.75 39.25 119.75 C 37.72 119.75 36.48 118.51 36.48 116.98 C 36.48 115.46 37.72 114.22 39.25 114.22 C 40.78 114.22 42.01 115.46 42.01 116.98 Z M 39.25 116.98" style="fill:none"></path><path d="M 100.88 122.7 C 100.88 124.23 99.65 125.46 98.12 125.46 C 96.59 125.46 95.35 124.23 95.35 122.7 C 95.35 121.17 96.59 119.93 98.12 119.93 C 99.65 119.93 100.88 121.17 100.88 122.7 Z M 98.12 122.7" style="fill:none"></path><path d="M 199 131.64 C 199 133.17 197.76 134.41 196.23 134.41 C 194.71 134.41 193.47 133.17 193.47 131.64 C 193.47 130.11 194.71 128.87 196.23 128.87 C 197.76 128.87 199 130.11 199 131.64 Z M 196.23 131.64" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 69.49 -32.76)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.87"><span class="ltx_text" id="S5.F4.1.pic1.14.14.14.14.14.1.1">Context Size</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 55.41 195.21)"><foreignobject height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="105.05"><span class="ltx_text" id="S5.F4.1.pic1.15.15.15.15.15.1.1">Natural Question</span></foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 30.1 150.1)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 24.73 0) translate(30.54,0) matrix(1.0 0.0 0.0 1.0 -27.77 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.54"><span class="ltx_text" id="S5.F4.1.pic1.16.16.16.16.16.1.1.1.1.1">Llama3.1</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" transform="matrix(1 0 0 -1 85.8 0) translate(0.55,0)"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 110.53 0) translate(22.56,0) matrix(1.0 0.0 0.0 1.0 -19.79 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="39.59"><span class="ltx_text" id="S5.F4.1.pic1.17.17.17.17.17.2.2.2.1.1">ACER</span></foreignobject></g></g></g></g></g></g></g></svg>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F4.2"><svg class="ltx_picture" height="246.65" id="S5.F4.2.pic1" overflow="visible" version="1.1" width="252.54"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,246.65) matrix(1 0 0 -1 0 0) translate(36.41,0) translate(0,37.37) matrix(1.0 0.0 0.0 1.0 -36.41 -37.37)"><g class="ltx_nestedsvg" transform="matrix(1 0 0 1 0 0) translate(36.41,0) translate(0,37.37)"><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 9.81 0 L 9.81 179.33 M 19.62 0 L 19.62 179.33 M 39.25 0 L 39.25 179.33 M 98.12 0 L 98.12 179.33 M 196.23 0 L 196.23 179.33" style="fill:none"></path></g><g color="#BFBFBF" fill="#BFBFBF" stroke="#BFBFBF" stroke-width="0.4pt"><path d="M 0 0 L 215.86 0 M 0 17.93 L 215.86 17.93 M 0 35.87 L 215.86 35.87 M 0 53.8 L 215.86 53.8 M 0 71.73 L 215.86 71.73 M 0 89.66 L 215.86 89.66 M 0 107.6 L 215.86 107.6 M 0 125.53 L 215.86 125.53 M 0 143.46 L 215.86 143.46" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 9.81 0 L 9.81 5.91 M 19.62 0 L 19.62 5.91 M 39.25 0 L 39.25 5.91 M 98.12 0 L 98.12 5.91 M 196.23 0 L 196.23 5.91 M 9.81 179.33 L 9.81 173.42 M 19.62 179.33 L 19.62 173.42 M 39.25 179.33 L 39.25 173.42 M 98.12 179.33 L 98.12 173.42 M 196.23 179.33 L 196.23 173.42" style="fill:none"></path></g><g color="#808080" fill="#808080" stroke="#808080" stroke-width="0.2pt"><path d="M 0 0 L 5.91 0 M 0 17.93 L 5.91 17.93 M 0 35.87 L 5.91 35.87 M 0 53.8 L 5.91 53.8 M 0 71.73 L 5.91 71.73 M 0 89.66 L 5.91 89.66 M 0 107.6 L 5.91 107.6 M 0 125.53 L 5.91 125.53 M 0 143.46 L 5.91 143.46 M 215.86 0 L 209.95 0 M 215.86 17.93 L 209.95 17.93 M 215.86 35.87 L 209.95 35.87 M 215.86 53.8 L 209.95 53.8 M 215.86 71.73 L 209.95 71.73 M 215.86 89.66 L 209.95 89.66 M 215.86 107.6 L 209.95 107.6 M 215.86 125.53 L 209.95 125.53 M 215.86 143.46 L 209.95 143.46" style="fill:none"></path></g><g fill="#000000" stroke="#000000" stroke-width="0.4pt"><path d="M 0 0 L 0 179.33 L 215.86 179.33 L 215.86 0 L 0 0 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 6.35 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.92"><math alttext="5" class="ltx_Math" display="inline" id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">5</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 12.7 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="10" class="ltx_Math" display="inline" id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1d">10</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 32.33 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="20" class="ltx_Math" display="inline" id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">20</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1d">20</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 91.2 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="13.84"><math alttext="50" class="ltx_Math" display="inline" id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">50</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1d">50</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 185.86 -13.81)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="20.76"><math alttext="100" class="ltx_Math" display="inline" id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">100</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1d">100</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -24.88 -4.46)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.5" class="ltx_Math" display="inline" id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">0.5</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1d">0.5</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 13.47)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.52" class="ltx_Math" display="inline" id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.52</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">0.52</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">0.52</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1d">0.52</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 31.41)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.54" class="ltx_Math" display="inline" id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.54</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">0.54</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">0.54</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1d">0.54</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 49.34)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.56" class="ltx_Math" display="inline" id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.56</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">0.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">0.56</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1d">0.56</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 67.27)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.58" class="ltx_Math" display="inline" id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.58</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">0.58</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">0.58</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1d">0.58</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -24.88 85.2)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="19.99"><math alttext="0.6" class="ltx_Math" display="inline" id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.6</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">0.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">0.6</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1d">0.6</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 103.14)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.62" class="ltx_Math" display="inline" id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.62</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">0.62</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">0.62</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1d">0.62</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 121.07)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.64" class="ltx_Math" display="inline" id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.64</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">0.64</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1c">0.64</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1d">0.64</annotation></semantics></math></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -31.79 139)"><foreignobject height="8.92" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="26.91"><math alttext="0.66" class="ltx_Math" display="inline" id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1a"><mn id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">0.66</mn><annotation-xml encoding="MathML-Content" id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1b"><cn id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" type="float" xref="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1">0.66</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1c">0.66</annotation><annotation encoding="application/x-llamapun" id="S5.F4.2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1d">0.66</annotation></semantics></math></foreignobject></g><clippath id="pgfcp14"><path d="M 0 0 L 215.86 0 L 215.86 179.33 L 0 179.33 Z"></path></clippath><g clip-path="url(#pgfcp14)"><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt"><path d="M 9.81 39.45 L 19.62 35.87 L 39.25 41.25 L 98.12 15.24 L 196.23 6.28" style="fill:none"></path></g><g></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt"><path d="M 9.81 60.07 L 19.62 86.08 L 39.25 107.6 L 98.12 104.01 L 196.23 108.49" style="fill:none"></path></g><g></g></g><g color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt"><path d="M 7.04 36.68 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 16.86 33.1 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 36.48 38.48 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 95.35 12.48 h 5.53 v 5.53 h -5.53 Z"></path><path d="M 193.47 3.51 h 5.53 v 5.53 h -5.53 Z"></path></g><g color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt"><path d="M 12.58 60.07 C 12.58 61.6 11.34 62.84 9.81 62.84 C 8.28 62.84 7.04 61.6 7.04 60.07 C 7.04 58.55 8.28 57.31 9.81 57.31 C 11.34 57.31 12.58 58.55 12.58 60.07 Z M 9.81 60.07" style="fill:none"></path><path d="M 22.39 86.08 C 22.39 87.6 21.15 88.84 19.62 88.84 C 18.09 88.84 16.86 87.6 16.86 86.08 C 16.86 84.55 18.09 83.31 19.62 83.31 C 21.15 83.31 22.39 84.55 22.39 86.08 Z M 19.62 86.08" style="fill:none"></path><path d="M 42.01 107.6 C 42.01 109.12 40.78 110.36 39.25 110.36 C 37.72 110.36 36.48 109.12 36.48 107.6 C 36.48 106.07 37.72 104.83 39.25 104.83 C 40.78 104.83 42.01 106.07 42.01 107.6 Z M 39.25 107.6" style="fill:none"></path><path d="M 100.88 104.01 C 100.88 105.54 99.65 106.78 98.12 106.78 C 96.59 106.78 95.35 105.54 95.35 104.01 C 95.35 102.48 96.59 101.24 98.12 101.24 C 99.65 101.24 100.88 102.48 100.88 104.01 Z M 98.12 104.01" style="fill:none"></path><path d="M 199 108.49 C 199 110.02 197.76 111.26 196.23 111.26 C 194.71 111.26 193.47 110.02 193.47 108.49 C 193.47 106.96 194.71 105.72 196.23 105.72 C 197.76 105.72 199 106.96 199 108.49 Z M 196.23 108.49" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 69.49 -32.76)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="76.87"><span class="ltx_text" id="S5.F4.2.pic1.15.15.15.15.15.1.1">Context Size</span></foreignobject></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 79.27 195.21)"><foreignobject height="12.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="57.31"><span class="ltx_text" id="S5.F4.2.pic1.16.16.16.16.16.1.1">TriviaQA</span></foreignobject></g><g fill="#FFFFFF" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 30.1 150.1)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 8.61)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 8.61)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#0000FF" fill="#0000FF" stroke="#0000FF" stroke-width="0.8pt" transform="matrix(1 0 0 -1 0 0) translate(0.55,0)"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 9.04 -2.77 h 5.53 v 5.53 h -5.53 Z"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 24.73 0) translate(30.54,0) matrix(1.0 0.0 0.0 1.0 -27.77 -3.77)"><foreignobject height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="55.54"><span class="ltx_text" id="S5.F4.2.pic1.17.17.17.17.17.1.1.1.1.1">Llama3.1</span></foreignobject></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" color="#FF0000" fill="#FF0000" stroke="#FF0000" stroke-width="0.8pt" transform="matrix(1 0 0 -1 85.8 0) translate(0.55,0)"><path d="M 0 0 L 11.81 0 L 23.62 0" style="fill:none"></path><path d="M 14.58 0 C 14.58 1.53 13.34 2.77 11.81 2.77 C 10.28 2.77 9.04 1.53 9.04 0 C 9.04 -1.53 10.28 -2.77 11.81 -2.77 C 13.34 -2.77 14.58 -1.53 14.58 0 Z M 11.81 0" style="fill:none"></path></g><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" fill="#000000" stroke="#000000" transform="matrix(1 0 0 -1 110.53 0) translate(22.56,0) matrix(1.0 0.0 0.0 1.0 -19.79 -3.69)"><foreignobject height="9.46" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="39.59"><span class="ltx_text" id="S5.F4.2.pic1.18.18.18.18.18.2.2.2.1.1">ACER</span></foreignobject></g></g></g></g></g></g></g></svg>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance comparison between Llama3.1 and ACER when reading different context sizes on the Natural Question and Trivia QA datasets.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">To get a more fine-grained understanding of how context extension affects each model, in this section, we compare models at a variety of context lengths.
Specifically, we varies the number of retrieved passages fed into the model for generating the final answer.
Recall that Llama-3.1 and <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.1.1">ACER</span> both derive from the same pre-trained Llama-3 base model using different context extension processes.
In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S5.F4" title="Figure 4 â€£ 5.1 Comparing Llama-3.1 and ACER at Different Context Sizes â€£ 5 Analysis â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">FigureÂ 4</span></a>, we plot the two modelsâ€™ performance against number of passages on both Natural Question and TriviaQAÂ (1k subset to reduce inference cost.)</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">We observed a very interesting yet intuitive phenomenon. Here the two models perform very similarly when reading a small context. This means they possesses similar capability and alignment behavior at short context, as <em class="ltx_emph ltx_font_italic" id="S5.SS1.p2.1.1">sibling models</em>. However, as context increases, the performance keeps diverge. While <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p2.1.2">ACER</span> can keep digesting more useful information from the context, Llama-3.1 seemingly suffers from the longer contexts with performance decaying. This shows one other example that <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p2.1.3">ACER</span> achieved the goal we set for it and successfully <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.4">extended</span> the modelâ€™s capability presented in a shorter context onto a much longer context.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Using an Unsupervised Retriever</h3>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.1" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1">Retriever</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.2.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.3.1">NQ</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.4.1">TQA</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.1">(EM)</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.2.2.2">(EM)</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.3.1" rowspan="4"><span class="ltx_text" id="S5.T2.1.3.3.1.1">BM25</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.3.3.2">Llama3 (Truncation)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.3">0.260</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.3.3.4">0.542</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.4.4.1">Llama3 (RAG)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.2">0.354</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.4.4.3">0.576</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.5.5.1">Llama3.1</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.2">0.305</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.5.5.3">0.507</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.6.6.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.6.6.1.1">ACER</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.6.2.1">0.383</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.6.6.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.6.6.3.1">0.632</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T2.1.7.7.1" rowspan="4"><span class="ltx_text" id="S5.T2.1.7.7.1.1">Dense</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.1.7.7.2">Llama3 (Truncation)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.7.7.3">0.388</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.1.7.7.4">0.567</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.8.8.1">Llama3 (RAG)</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.8.8.2">0.408</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.8.8.3">0.611</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.9.9.1">Llama3.1</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.2">0.312</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.9.9.3">0.518</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.1.10.10.1"><span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.10.10.1.1">ACER</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.10.10.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.10.10.2.1">0.446</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.1.10.10.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.10.10.3.1">0.648</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of <span class="ltx_text ltx_font_smallcaps" id="S5.T2.3.1">ACER</span> and baselines with different base retrievers.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The previously discussed <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.1">ACER</span> pipeline for RAG tasks adopts a supervisedly trained dense retriever. In certain situations where even retrieval data is scarce, obtaining such a retriever may not even be possible. People instead need to fall back to the classical BM25 retrievers. In this section, we consider such a situation by running <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.2">ACER</span> and some of the baseline systems with BM25. This means the top-100 candidates will be different and of lower quality. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2410.09141v1#S5.T2" title="Table 2 â€£ 5.2 Using an Unsupervised Retriever â€£ 5 Analysis â€£ ACER: Automatic Language Model Context Extension via Retrieval"><span class="ltx_text ltx_ref_tag">TableÂ 2</span></a>, we show the results of this BM25 setup compared with the original dense setup. While using BM25, all systems take a hit in performance because of lower-quality candidates, the general performance order of the systems remain the same. Our <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.3">ACER</span> method still outperforms the other systems by decent margins. In fact, when looking at the the absolute numbers, we can observe that <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.4">ACER</span> with BM25 attains close or even better performance than best-performing baseline systems. The results suggest that <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.1.5">ACER</span> is relatively agnostic to the retriever used and attains good performance to warm-start a system without requiring extra supervision.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Works</h2>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Language Modeling</h5>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">Recent advancements of large language models have shown strong language understanding ability and ace a wide range of natural language processing tasks. Based on the Transformer structureÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib60" title="">2017</a>)</cite>, LMs can be broadly categorized as decoder-only models (e.g., GPTÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib47" title="">2018</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib48" title="">2019</a>)</cite>), encoder-only models (e.g., BERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib15" title="">2019</a>)</cite> and RoBERTaÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib40" title="">2019</a>)</cite>), and encoder-decoder models (e.g., BARTÂ <cite class="ltx_cite ltx_citemacro_cite">Lewis etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib37" title="">2020a</a>)</cite> and T5Â <cite class="ltx_cite ltx_citemacro_cite">Raffel etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib50" title="">2020</a>)</cite>). Most recently, the large language models such as the GPT familyÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib8" title="">2020</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib43" title="">2023</a>)</cite>, GeminiÂ <cite class="ltx_cite ltx_citemacro_citep">(GeminiTeam etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib20" title="">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib21" title="">b</a>)</cite>, LlamaÂ <cite class="ltx_cite ltx_citemacro_cite">Touvron etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib58" title="">2023a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib59" title="">b</a>); MetaAI etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>, and ClaudeÂ <cite class="ltx_cite ltx_citemacro_cite">Anthropic (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib1" title="">2023</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib2" title="">2024</a>)</cite> have attract great attention by achieving human-level performance on various tasks and showing structure-following ability. These models are typically trained in several stages, including pre-training on web-scale text data with auto-regressive language modeling, supervised fine-tuning on specific applications, and human preference alignment. Going beyond the scope of this paper, for production models, the last step plays the pivotal role to steer the LMs as a dialogue system to answer humanâ€™s instruction and generate responses that are in desired quality and style, safe, and ethical. Some recent LM alignment methods include reinforcement learning from human feedback (RLHF)Â <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib44" title="">2022</a>)</cite>, proximal policy optimizationÂ (PPO)Â <cite class="ltx_cite ltx_citemacro_citep">(Schulman etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib54" title="">2017</a>)</cite>, direct policy optimization (DPO)Â <cite class="ltx_cite ltx_citemacro_citep">(Rafailov etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib49" title="">2023</a>)</cite>, and Kahneman-Tversky optimization (KTO)Â <cite class="ltx_cite ltx_citemacro_citep">(Ethayarajh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib17" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Long Context Modeling</h5>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px2.p1.1">TransformerÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib60" title="">2017</a>)</cite> is the foundation architecture for recent advancements in language models. However, its quadratic temporal and computation complexity to the sequence length poses great challenge to scale to long input sequence, and the lack of robust position embeddings also degrades the performance of long context understanding. Therefore, tremendous efforts are made to improve the long-context modeling of language models from different aspects, including more efficient attention mechanism for long contextÂ <cite class="ltx_cite ltx_citemacro_cite">Beltagy etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib5" title="">2020</a>); Kwon etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib36" title="">2023</a>); Dao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib14" title="">2022</a>); Dao (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib13" title="">2024</a>); Xiao etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib64" title="">2024a</a>)</cite>, a (recurrent) internal or external memory bankÂ <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib12" title="">2019</a>; Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib62" title="">2022a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib63" title="">b</a>)</cite>, and length extrapolation through positional encodingÂ <cite class="ltx_cite ltx_citemacro_citep">(Press etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib46" title="">2022</a>; Su etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib55" title="">2024</a>; Peng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib45" title="">2024</a>; Zhu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib72" title="">2024</a>)</cite> and adaptationÂ <cite class="ltx_cite ltx_citemacro_citep">(Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib68" title="">2023</a>; Fu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib18" title="">2024</a>; RoziÃ¨re etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib53" title="">2024</a>; MetaAI etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib41" title="">2024</a>)</cite>. To evaluate the long-context modeling ability of LLMs, several synthetic benchmarks are proposed, such as Lost in the MiddleÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib39" title="">2024</a>)</cite>, Needle in a HaystackÂ <cite class="ltx_cite ltx_citemacro_citep">(Kamradt, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib29" title="">2023</a>)</cite>, and RulerÂ <cite class="ltx_cite ltx_citemacro_citep">(Hsieh etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib23" title="">2024</a>)</cite>. Because we are proposing new data synthesis method for long-context LM training, we do not include the synthetic benchmarks.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Retrieval Augmented Generation</h5>
<div class="ltx_para ltx_noindent" id="S6.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px3.p1.1">Retrieval-augmented generation, or RAG, enhances LLMsâ€™ ability in knowledge intensive tasks. Originated from open-domain question answeringÂ <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib10" title="">2017</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib67" title="">2021b</a>)</cite>, RAG will enrich the prompt (i.e., a question) with additional relevant context retrieved from an external corpus to help a reader better answer the questionÂ <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib38" title="">2020b</a>; Guu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib22" title="">2020</a>; Izacard etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib25" title="">2024</a>)</cite>. The retrieval methods range from sparse featuresÂ <cite class="ltx_cite ltx_citemacro_citep">(Robertson &amp; Zaragoza, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib52" title="">2009</a>; Roberts etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib51" title="">2020</a>)</cite> and deep-learning based dense representationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib30" title="">2020</a>; Lewis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib38" title="">2020b</a>)</cite> to direct generation by LLMsÂ <cite class="ltx_cite ltx_citemacro_citep">(Sun etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib56" title="">2023</a>; Yu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib71" title="">2023</a>)</cite>. Recent studies have explored various ways to enhance RAG, such as better query understandingÂ <cite class="ltx_cite ltx_citemacro_citep">(Kim etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib31" title="">2023</a>; Chan etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib9" title="">2024</a>)</cite>, a better retrieverÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib30" title="">2020</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib66" title="">2021a</a>; Yao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib70" title="">2023</a>)</cite>, and a better reading modelÂ <cite class="ltx_cite ltx_citemacro_citep">(Izacard &amp; Grave, <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib24" title="">2021</a>; Cheng etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib11" title="">2021</a>; Borgeaud etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib6" title="">2021</a>)</cite>. Specifically, Self-RAGÂ <cite class="ltx_cite ltx_citemacro_citep">(Asai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib3" title="">2024</a>)</cite> trains a critique model that reflects the retrieved passages and the generation to adaptively decide whether the retrieval is necessary. <cite class="ltx_cite ltx_citemacro_citet">Jiang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.09141v1#bib.bib26" title="">2024</a>)</cite> explored the granularity of retrieval units and usage of long-context for better retrieval.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this paper, we argue that building long-context multi-task generalist model can be prohibitively hard due to the diverse space of possible tasks. To avoid curating data for each long-context task of interest,
we propose <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.1">ACER</span>, a method for automatically extending language model capabilities to longer contexts without using human generated supervision. <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.2">ACER</span> pivots through a retrieval pipeline to help a short-context model to heuristically process long pieces of text and produce a synthetic <em class="ltx_emph ltx_font_italic" id="S7.p1.1.3">imperfect</em> answer. We demonstrate that this model can then bootstrap over this synthetic answer to gain even stronger long context capabilities, often outperforms carefully built long-context generalist models. We believe <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.4">ACER</span> demonstrate an effective unsupervised approach to extend short context capabilities into longer contexts. It can be a useful tool to improve specific long-context task performance where little training data exists. Users only need to write a few prompts to run the pipeline.
Broadly, we introduce automating capabilities extension onto longer context as a new research problem; future works may consider better data synthesis processes and paths to produce better extension results.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS0.SSSx1">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>
<div class="ltx_para ltx_noindent" id="S7.SS0.SSSx1.p1">
<p class="ltx_p" id="S7.SS0.SSSx1.p1.1">The compute resource in this research is provided partly by the TPU Research CloudÂ (TRC) program, for which the authors are very grateful.
The authors would like to thank Akari Asai, Minyang Tian and Xueguang Ma for helpful discussions.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing 100K Context Windows, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/index/100k-context-windows" title="">https://www.anthropic.com/index/100k-context-windows</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2024)</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Introducing the next generation of claude, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.anthropic.com/news/claude-3-family" title="">https://www.anthropic.com/news/claude-3-family</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai etÂ al. (2024)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=hSyW5go0v8" title="">https://openreview.net/forum?id=hSyW5go0v8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li.

</span>
<span class="ltx_bibblock">Longbench: A bilingual, multitask benchmark for long context understanding, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy etÂ al. (2020)</span>
<span class="ltx_bibblock">
IzÂ Beltagy, MatthewÂ E. Peters, and Arman Cohan.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv:2004.05150</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud etÂ al. (2021)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George vanÂ den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego deÂ LasÂ Casas, Aurelia Guy, Jacob Menick, Roman Ring, T.Â W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, JackÂ W. Rae, Erich Elsen, and L.Â Sifre.

</span>
<span class="ltx_bibblock">Improving language models by retrieving from trillions of tokens.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">International Conference on Machine Learning</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:244954723" title="">https://api.semanticscholar.org/CorpusID:244954723</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bradbury etÂ al. (2018)</span>
<span class="ltx_bibblock">
James Bradbury, Roy Frostig, Peter Hawkins, MatthewÂ James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang.

</span>
<span class="ltx_bibblock">JAX: composable transformations of Python+NumPy programs, 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://github.com/jax-ml/jax" title="">http://github.com/jax-ml/jax</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
TomÂ B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, DanielÂ M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">NeurIPS</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan etÂ al. (2024)</span>
<span class="ltx_bibblock">
Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu.

</span>
<span class="ltx_bibblock">RQ-RAG: Learning to refine queries for retrieval augmented generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">First Conference on Language Modeling</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=tzE7VqsaJ4" title="">https://openreview.net/forum?id=tzE7VqsaJ4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes.

</span>
<span class="ltx_bibblock">Reading Wikipedia to answer open-domain questions.

</span>
<span class="ltx_bibblock">In Regina Barzilay and Min-Yen Kan (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.Â  1870â€“1879, Vancouver, Canada, July 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P17-1171</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P17-1171" title="">https://aclanthology.org/P17-1171</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng etÂ al. (2021)</span>
<span class="ltx_bibblock">
Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">UnitedQA: A hybrid approach for open domain question answering.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pp.Â  3080â€“3090, Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.acl-long.240</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.240" title="">https://aclanthology.org/2021.acl-long.240</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2019)</span>
<span class="ltx_bibblock">
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.

</span>
<span class="ltx_bibblock">Transformer-XL: Attentive language models beyond a fixed-length context.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and LluÃ­s MÃ rquez (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pp.Â  2978â€“2988, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1285</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1285" title="">https://aclanthology.org/P19-1285</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao (2024)</span>
<span class="ltx_bibblock">
Tri Dao.

</span>
<span class="ltx_bibblock">FlashAttention-2: Faster attention with better parallelism and work partitioning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations (ICLR)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dao etÂ al. (2022)</span>
<span class="ltx_bibblock">
Tri Dao, DanielÂ Y. Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©.

</span>
<span class="ltx_bibblock">FlashAttention: Fast and memory-efficient exact attention with IO-awareness.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">NAACL-HLT</em>, pp.Â  4171â€“4186, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2305.14233</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ethayarajh etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Model alignment as prospect theoretic optimization.

</span>
<span class="ltx_bibblock">In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, volume 235 of <em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">Proceedings of Machine Learning Research</em>, pp.Â  12634â€“12651. PMLR, 21â€“27 Jul 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.mlr.press/v235/ethayarajh24a.html" title="">https://proceedings.mlr.press/v235/ethayarajh24a.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hanna Hajishirzi, Yoon Kim, and Hao Peng.

</span>
<span class="ltx_bibblock">Data engineering for scaling language models to 128k context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ArXiv</em>, abs/2402.10171, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:267682361" title="">https://api.semanticscholar.org/CorpusID:267682361</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen.

</span>
<span class="ltx_bibblock">Prolong long-context language model series, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-64k-Instruct" title="">https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-64k-Instruct</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GeminiTeam etÂ al. (2024a)</span>
<span class="ltx_bibblock">
GeminiTeam, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, AndrewÂ M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, PaulÂ R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, EdÂ Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian GÃ¼ra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ãgoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff
Pitman, Rushin Shah, Emanuel Taropa, MajdÂ Al Merey, Martin Baeuml, Zhifeng Chen, LaurentÂ El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, YiÂ Luan, XiÂ Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, JackÂ W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, GauravÂ Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, IÃ±aki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang,
Jordan Grimstad, AleÂ Jakse Hartman, Xavier Garcia, ThanumalayanÂ Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego deÂ LasÂ Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, AdriÃ Â PuigdomÃ¨nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, SÃ©bastien M.Â R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry
Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, TomÂ Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika RogoziÅ„ska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, ClaraÂ Huiyi Hu, Raoul deÂ Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, ReinaldÂ Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai GimÃ©nez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran
Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George vanÂ den Driessche, Tao Wang, Fan Yang, Shuo yiin Chang, Paul Komarek, Ross McIlroy, Mario LuÄiÄ‡, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, PaulÂ Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell,
Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, RaphaÃ«lÂ Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, CharlineÂ Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, JaimeÂ Alonso Lorenzo, LarsÂ Lowe SjÃ¶sund, SÃ©bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, LÃ©onard Hussenot, LivioÂ Baldini Soares, Kate Baumli, MichaelÂ B. Chang, AdriÃ  Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic,
Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, VÃ­ctorÂ Campos Campos, Alex Tomala, Yunhao Tang, DaliaÂ El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, CeÂ Zheng, Phoebe Thacker, Ã‡aÄŸlar ÃœnlÃ¼, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, LisaÂ Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, SayedÂ Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, LeÂ Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van
Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja RakiÄ‡eviÄ‡, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, NicolaÂ De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz KÄ™pa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri GayatriÂ Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad
Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, HafeezulÂ Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Ã„lgmyr, TimothÃ©e Lottaz, QiÂ Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, PamÂ G Rabinovitch, Pavan KumarÂ Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, IdanÂ Heimlich Shtacher, Shachi Paul, Oscar Akerlund, FranÃ§ois-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora
Marinescu, Martin BÃ¶lle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, LamÂ Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, MalcolmÂ Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-WeiÂ "Louis" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, NoahÂ Ã“ Donnaile, SÃ©bastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJÂ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris
Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, SrividyaÂ Pranavi Potharaju, Eileen Oâ€™Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, NiccolÃ²Â Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ã„hdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, JiÂ Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou,
Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, JohnÂ Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur BraÅ¾inskas, Andrei Sozanschi, Matthew Hayes, HÃ©ctorÂ FernÃ¡ndez Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante KÃ¤rrman, PaweÅ‚ Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal
Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, YeÂ Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, JaumeÂ Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, YuÂ Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso CastaÃ±o, Irene Giannoumis, Wooyeol Kim, MikoÅ‚aj RybiÅ„ski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, DianaÂ Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le,
LuÂ Li, Chimezie Iwuanyanwu, LuÂ Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, ElenaÂ Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom vanÂ der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, DucÂ Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, MariaÂ Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, RÃ©mi Leblond, Shirley Chung, Harry Askham, LuisÂ C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin,
Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, DanielÂ J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, TianÂ Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant
Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, AmÃ©lie HÃ©liou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, SoheilÂ Hassas Yeganeh, Siim PÃµder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, ChristopherÂ A. Choquette-Choo, Yunjie Li, TJÂ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona
Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane RiviÃ¨re, Alanna Walton, ClÃ©ment Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia vanÂ der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-PluciÅ„ska, David Bridson, Dario deÂ Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, ShreyasÂ Rammohan Belle, Lei Wang, Chetan Tekur, MihirÂ Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, YiÂ Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, ManishÂ Reddy Vuyyuru, John Aslanides, Nidhi Vyas,
Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, XiÂ Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, JiÂ Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MKÂ Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua
Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, KeÂ Ye, JeanÂ Michel Sarr, MelanieÂ Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, VÃ­t ListÃ­k, Mathias Carlen, Jan vanÂ de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul MÃ¼ller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, XingyuÂ Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, BoÂ Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia,
Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, ChaitanyaÂ Krishna Lanka, Derik Clive, YiÂ Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, TuÂ Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Gemini: A family of highly capable multimodal models, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.11805" title="">https://arxiv.org/abs/2312.11805</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GeminiTeam etÂ al. (2024b)</span>
<span class="ltx_bibblock">
GeminiTeam, Petko Georgiev, VingÂ Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh Mariooryad, Yifan Ding, Xinyang Geng, Fred Alcober, Roy Frostig, Mark Omernick, Lexi Walker, Cosmin Paduraru, Christina Sorokin, Andrea Tacchetti, Colin Gaffney, Samira Daruki, Olcan Sercinoglu, Zach Gleicher, Juliette Love, Paul Voigtlaender, Rohan Jain, Gabriela Surita, Kareem Mohamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Kornraphop Kawintiranon, Orhan Firat, Yiming Gu, Yujing Zhang, Matthew Rahtz, Manaal Faruqui, Natalie Clay, Justin Gilmer, JDÂ Co-Reyes, Ivo Penchev, Rui Zhu, Nobuyuki Morioka, Kevin Hui, Krishna Haridasan, Victor Campos, Mahdis Mahdieh, Mandy Guo, Samer Hassan, Kevin Kilgour, Arpi Vezer, Heng-Tze Cheng, Raoul deÂ Liedekerke, Siddharth Goyal, Paul Barham, DJÂ Strouse, Seb Noury, Jonas Adler, Mukund Sundararajan, Sharad Vikram, Dmitry Lepikhin, Michela Paganini, Xavier Garcia, Fan Yang, Dasha Valter, Maja Trebacz, Kiran Vodrahalli, Chulayuth
Asawaroengchai, Roman Ring, Norbert Kalb, LivioÂ Baldini Soares, Siddhartha Brahma, David Steiner, Tianhe Yu, Fabian Mentzer, Antoine He, Lucas Gonzalez, Bibo Xu, RaphaelÂ Lopez Kaufman, LaurentÂ El Shafey, Junhyuk Oh, Tom Hennigan, George vanÂ den Driessche, Seth Odoom, Mario Lucic, Becca Roelofs, Sid Lall, Amit Marathe, Betty Chan, Santiago Ontanon, Luheng He, Denis Teplyashin, Jonathan Lai, Phil Crone, Bogdan Damoc, Lewis Ho, Sebastian Riedel, Karel Lenc, Chih-Kuan Yeh, Aakanksha Chowdhery, Yang Xu, Mehran Kazemi, Ehsan Amid, Anastasia Petrushkina, Kevin Swersky, Ali Khodaei, Gowoon Chen, Chris Larkin, Mario Pinto, Geng Yan, AdriaÂ Puigdomenech Badia, Piyush Patil, Steven Hansen, Dave Orr, Sebastien M.Â R. Arnold, Jordan Grimstad, Andrew Dai, Sholto Douglas, Rishika Sinha, Vikas Yadav, XiÂ Chen, Elena Gribovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel, Paul Komarek, Sophia Austin, Sebastian Borgeaud, Linda Friso, Abhimanyu Goyal, Ben Caine, Kris Cao, Da-Woon Chung, Matthew Lamm, Gabe Barth-Maron, Thais
Kagohara, Kate Olszewska, Mia Chen, Kaushik Shivakumar, Rishabh Agarwal, Harshal Godhia, Ravi Rajwar, Javier Snaider, Xerxes Dotiwalla, Yuan Liu, Aditya Barua, Victor Ungureanu, Yuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth, James Qin, Ivo Danihelka, Tulsee Doshi, Martin Chadwick, Jilin Chen, Sanil Jain, Quoc Le, Arjun Kar, Madhu Gurumurthy, Cheng Li, Ruoxin Sang, Fangyu Liu, Lampros Lamprou, Rich Munoz, Nathan Lintz, Harsh Mehta, Heidi Howard, Malcolm Reynolds, Lora Aroyo, Quan Wang, Lorenzo Blanco, Albin Cassirer, Jordan Griffith, Dipanjan Das, Stephan Lee, Jakub Sygnowski, Zach Fisher, James Besley, Richard Powell, Zafarali Ahmed, Dominik Paulus, David Reitter, Zalan Borsos, Rishabh Joshi, Aedan Pope, Steven Hand, Vittorio Selo, Vihan Jain, Nikhil Sethi, Megha Goel, Takaki Makino, Rhys May, Zhen Yang, Johan Schalkwyk, Christina Butterfield, Anja Hauth, Alex Goldin, Will Hawkins, Evan Senter, Sergey Brin, Oliver Woodman, Marvin Ritter, Eric Noland, Minh Giang, Vijay Bolina, Lisa Lee, Tim Blyth, Ian
Mackinnon, Machel Reid, Obaid Sarvana, David Silver, Alexander Chen, Lily Wang, Loren Maggiore, Oscar Chang, Nithya Attaluri, Gregory Thornton, Chung-Cheng Chiu, Oskar Bunyan, Nir Levine, Timothy Chung, Evgenii Eltyshev, Xiance Si, Timothy Lillicrap, Demetra Brady, Vaibhav Aggarwal, Boxi Wu, Yuanzhong Xu, Ross McIlroy, Kartikeya Badola, Paramjit Sandhu, Erica Moreira, Wojciech Stokowiec, Ross Hemsley, Dong Li, Alex Tudor, Pranav Shyam, Elahe Rahimtoroghi, Salem Haykal, Pablo Sprechmann, Xiang Zhou, Diana Mincu, Yujia Li, Ravi Addanki, Kalpesh Krishna, Xiao Wu, Alexandre Frechette, Matan Eyal, Allan Dafoe, Dave Lacey, Jay Whang, Thi Avrahami, YeÂ Zhang, Emanuel Taropa, Hanzhao Lin, Daniel Toyama, Eliza Rutherford, Motoki Sano, HyunJeong Choe, Alex Tomala, Chalence Safranek-Shrader, Nora Kassner, Mantas Pajarskas, Matt Harvey, Sean Sechrist, Meire Fortunato, Christina Lyu, Gamaleldin Elsayed, Chenkai Kuang, James Lottes, Eric Chu, Chao Jia, Chih-Wei Chen, Peter Humphreys, Kate Baumli, Connie Tao, Rajkumar
Samuel, CiceroÂ Nogueira dos Santos, Anders Andreassen, Nemanja RakiÄ‡eviÄ‡, Dominik Grewe, Aviral Kumar, Stephanie Winkler, Jonathan Caton, Andrew Brock, Sid Dalmia, Hannah Sheahan, Iain Barr, Yingjie Miao, Paul Natsev, Jacob Devlin, Feryal Behbahani, Flavien Prost, Yanhua Sun, Artiom Myaskovsky, ThanumalayanÂ Sankaranarayana Pillai, Dan Hurt, Angeliki Lazaridou, XiÂ Xiong, CeÂ Zheng, Fabio Pardo, Xiaowei Li, Dan Horgan, Joe Stanton, Moran Ambar, Fei Xia, Alejandro Lince, Mingqiu Wang, Basil Mustafa, Albert Webson, Hyo Lee, Rohan Anil, Martin Wicke, Timothy Dozat, Abhishek Sinha, Enrique Piqueras, Elahe Dabir, Shyam Upadhyay, Anudhyan Boral, LisaÂ Anne Hendricks, Corey Fry, Josip Djolonga, YiÂ Su, Jake Walker, Jane Labanowski, Ronny Huang, Vedant Misra, Jeremy Chen, RJÂ Skerry-Ryan, Avi Singh, Shruti Rijhwani, Dian Yu, Alex Castro-Ros, Beer Changpinyo, Romina Datta, Sumit Bagri, ArnarÂ Mar Hrafnkelsson, Marcello Maggioni, Daniel Zheng, Yury Sulsky, Shaobo Hou, TomÂ Le Paine, Antoine Yang, Jason Riesa, Dominika
Rogozinska, Dror Marcus, DaliaÂ El Badawy, Qiao Zhang, Luyu Wang, Helen Miller, Jeremy Greer, LarsÂ Lowe Sjos, Azade Nova, Heiga Zen, Rahma Chaabouni, Mihaela Rosca, Jiepu Jiang, Charlie Chen, Ruibo Liu, Tara Sainath, Maxim Krikun, Alex Polozov, Jean-Baptiste Lespiau, Josh Newlan, Zeyncep Cankara, Soo Kwak, Yunhan Xu, Phil Chen, Andy Coenen, Clemens Meyer, Katerina Tsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, Chenjie Gu, Jin Miao, Christian Frank, Zeynep Cankara, Sanjay Ganapathy, Ishita Dasgupta, Steph Hughes-Fitt, Heng Chen, David Reid, Keran Rong, Hongmin Fan, Joost van Amersfoort, Vincent Zhuang, Aaron Cohen, ShixiangÂ Shane Gu, Anhad Mohananey, Anastasija Ilic, Taylor Tobin, John Wieting, Anna Bortsova, Phoebe Thacker, Emma Wang, Emily Caveness, Justin Chiu, Eren Sezener, Alex Kaskasoli, Steven Baker, Katie Millican, Mohamed Elhawaty, Kostas Aisopos, Carl Lebsack, Nathan Byrd, Hanjun Dai, Wenhao Jia, Matthew Wiethoff, Elnaz Davoodi, Albert Weston, Lakshman Yagati, Arun Ahuja, Isabel Gao, Golan Pundak,
Susan Zhang, Michael Azzam, KheÂ Chai Sim, Sergi Caelles, James Keeling, Abhanshu Sharma, Andy Swing, YaGuang Li, Chenxi Liu, CarrieÂ Grimes Bostock, Yamini Bansal, Zachary Nado, Ankesh Anand, Josh Lipschultz, Abhijit Karmarkar, Lev Proleev, Abe Ittycheriah, SoheilÂ Hassas Yeganeh, George Polovets, Aleksandra Faust, Jiao Sun, Alban Rrustemi, Pen Li, Rakesh Shivanna, Jeremiah Liu, Chris Welty, Federico Lebron, Anirudh Baddepudi, Sebastian Krause, Emilio Parisotto, Radu Soricut, Zheng Xu, Dawn Bloxwich, Melvin Johnson, Behnam Neyshabur, Justin Mao-Jones, Renshen Wang, Vinay Ramasesh, Zaheer Abbas, Arthur Guez, Constant Segal, DucÂ Dung Nguyen, James Svensson, LeÂ Hou, Sarah York, Kieran Milan, Sophie Bridgers, Wiktor Gworek, Marco Tagliasacchi, James Lee-Thorp, Michael Chang, Alexey Guseynov, AleÂ Jakse Hartman, Michael Kwong, Ruizhe Zhao, Sheleem Kashem, Elizabeth Cole, Antoine Miech, Richard Tanburn, Mary Phuong, Filip Pavetic, Sebastien Cevey, Ramona Comanescu, Richard Ives, Sherry Yang, Cosmo Du, BoÂ Li, Zizhao
Zhang, Mariko Iinuma, ClaraÂ Huiyi Hu, Aurko Roy, Shaan Bijwadia, Zhenkai Zhu, Danilo Martins, Rachel Saputro, Anita Gergely, Steven Zheng, Dawei Jia, Ioannis Antonoglou, Adam Sadovsky, Shane Gu, Yingying Bi, Alek Andreev, Sina Samangooei, Mina Khan, Tomas Kocisky, Angelos Filos, Chintu Kumar, Colton Bishop, Adams Yu, Sarah Hodkinson, Sid Mittal, Premal Shah, Alexandre Moufarek, Yong Cheng, Adam Bloniarz, Jaehoon Lee, Pedram Pejman, Paul Michel, Stephen Spencer, Vladimir Feinberg, Xuehan Xiong, Nikolay Savinov, Charlotte Smith, Siamak Shakeri, Dustin Tran, Mary Chesus, Bernd Bohnet, George Tucker, Tamara von Glehn, Carrie Muir, Yiran Mao, Hideto Kazawa, Ambrose Slone, Kedar Soparkar, Disha Shrivastava, James Cobon-Kerr, Michael Sharman, Jay Pavagadhi, Carlos Araya, Karolis Misiunas, Nimesh Ghelani, Michael Laskin, David Barker, Qiujia Li, Anton Briukhov, Neil Houlsby, Mia Glaese, Balaji Lakshminarayanan, Nathan Schucher, Yunhao Tang, Eli Collins, Hyeontaek Lim, Fangxiaoyu Feng, Adria Recasens, Guangda Lai,
Alberto Magni, NicolaÂ De Cao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay, Mostafa Dehghani, Jenny Brennan, Yifan He, Kelvin Xu, Yang Gao, Carl Saroufim, James Molloy, Xinyi Wu, Seb Arnold, Solomon Chang, Julian Schrittwieser, Elena Buchatskaya, Soroush Radpour, Martin Polacek, Skye Giordano, Ankur Bapna, Simon Tokumine, Vincent Hellendoorn, Thibault Sottiaux, Sarah Cogan, Aliaksei Severyn, Mohammad Saleh, Shantanu Thakoor, Laurent Shefey, Siyuan Qiao, Meenu Gaba, Shuo yiin Chang, Craig Swanson, Biao Zhang, Benjamin Lee, PaulÂ Kishan Rubenstein, Gan Song, Tom Kwiatkowski, Anna Koop, Ajay Kannan, David Kao, Parker Schuh, Axel Stjerngren, Golnaz Ghiasi, Gena Gibson, Luke Vilnis, YeÂ Yuan, FelipeÂ Tiengo Ferreira, Aishwarya Kamath, Ted Klimenko, Ken Franko, Kefan Xiao, Indro Bhattacharya, Miteyan Patel, Rui Wang, Alex Morris, Robin Strudel, Vivek Sharma, Peter Choy, SayedÂ Hadi Hashemi, Jessica Landon, Mara Finkelstein, Priya Jhakra, Justin Frye, Megan Barnes, Matthew Mauger, Dennis Daun, Khuslen Baatarsukh, Matthew
Tung, Wael Farhan, Henryk Michalewski, Fabio Viola, Felix deÂ ChaumontÂ Quitry, CharlineÂ Le Lan, Tom Hudson, Qingze Wang, Felix Fischer, Ivy Zheng, Elspeth White, Anca Dragan, Jean baptiste Alayrac, Eric Ni, Alexander Pritzel, Adam Iwanicki, Michael Isard, Anna Bulanova, Lukas Zilka, Ethan Dyer, Devendra Sachan, Srivatsan Srinivasan, Hannah Muckenhirn, Honglong Cai, Amol Mandhane, Mukarram Tariq, JackÂ W. Rae, Gary Wang, Kareem Ayoub, Nicholas FitzGerald, Yao Zhao, Woohyun Han, Chris Alberti, Dan Garrette, Kashyap Krishnakumar, Mai Gimenez, Anselm Levskaya, Daniel Sohn, Josip Matak, Inaki Iturrate, MichaelÂ B. Chang, Jackie Xiang, Yuan Cao, Nishant Ranka, Geoff Brown, Adrian Hutter, Vahab Mirrokni, Nanxin Chen, Kaisheng Yao, Zoltan Egyed, Francois Galilee, Tyler Liechty, Praveen Kallakuri, Evan Palmer, Sanjay Ghemawat, Jasmine Liu, David Tao, Chloe Thornton, Tim Green, Mimi Jasarevic, Sharon Lin, Victor Cotruta, Yi-Xuan Tan, Noah Fiedel, Hongkun Yu, EdÂ Chi, Alexander Neitz, Jens Heitkaemper, Anu Sinha, Denny
Zhou, YiÂ Sun, Charbel Kaed, Brice Hulse, Swaroop Mishra, Maria Georgaki, Sneha Kudugunta, Clement Farabet, Izhak Shafran, Daniel Vlasic, Anton Tsitsulin, Rajagopal Ananthanarayanan, Alen Carin, Guolong Su, Pei Sun, Shashank V, Gabriel Carvajal, Josef Broder, Iulia Comsa, Alena Repina, William Wong, WarrenÂ Weilun Chen, Peter Hawkins, Egor Filonov, Lucia Loher, Christoph Hirnschall, Weiyi Wang, Jingchen Ye, Andrea Burns, Hardie Cate, DianaÂ Gage Wright, Federico Piccinini, Lei Zhang, Chu-Cheng Lin, Ionel Gog, Yana Kulizhskaya, Ashwin Sreevatsa, Shuang Song, LuisÂ C. Cobo, Anand Iyer, Chetan Tekur, Guillermo Garrido, Zhuyun Xiao, Rupert Kemp, HuaixiuÂ Steven Zheng, Hui Li, Ananth Agarwal, Christel Ngani, Kati Goshvadi, Rebeca Santamaria-Fernandez, Wojciech Fica, Xinyun Chen, Chris Gorgolewski, Sean Sun, Roopal Garg, Xinyu Ye, S.Â M.Â Ali Eslami, Nan Hua, Jon Simon, Pratik Joshi, Yelin Kim, Ian Tenney, Sahitya Potluri, LamÂ Nguyen Thiet, Quan Yuan, Florian Luisier, Alexandra Chronopoulou, Salvatore Scellato, Praveen
Srinivasan, Minmin Chen, Vinod Koverkathu, Valentin Dalibard, Yaming Xu, Brennan Saeta, Keith Anderson, Thibault Sellam, Nick Fernando, Fantine Huot, Junehyuk Jung, Mani Varadarajan, Michael Quinn, Amit Raul, Maigo Le, Ruslan Habalov, Jon Clark, Komal Jalan, Kalesha Bullard, Achintya Singhal, Thang Luong, Boyu Wang, Sujeevan Rajayogam, Julian Eisenschlos, Johnson Jia, Daniel Finchelstein, Alex Yakubovich, Daniel Balle, Michael Fink, Sameer Agarwal, Jing Li, DjÂ Dvijotham, Shalini Pal, Kai Kang, Jaclyn Konzelmann, Jennifer Beattie, Olivier Dousse, Diane Wu, Remi Crocker, Chen Elkind, SiddharthaÂ Reddy Jonnalagadda, Jong Lee, Dan Holtmann-Rice, Krystal Kallarackal, Rosanne Liu, Denis Vnukov, Neera Vats, Luca Invernizzi, Mohsen Jafari, Huanjie Zhou, Lilly Taylor, Jennifer Prendki, Marcus Wu, Tom Eccles, Tianqi Liu, Kavya Kopparapu, Francoise Beaufays, Christof Angermueller, Andreea Marzoca, Shourya Sarcar, Hilal Dib, Jeff Stanway, Frank Perbet, Nejc Trdin, Rachel Sterneck, Andrey Khorlin, Dinghua Li, Xihui Wu,
Sonam Goenka, David Madras, Sasha Goldshtein, Willi Gierke, Tong Zhou, Yaxin Liu, Yannie Liang, Anais White, Yunjie Li, Shreya Singh, Sanaz Bahargam, Mark Epstein, Sujoy Basu, LiÂ Lao, Adnan Ozturel, Carl Crous, Alex Zhai, Han Lu, Zora Tung, Neeraj Gaur, Alanna Walton, Lucas Dixon, Ming Zhang, Amir Globerson, Grant Uy, Andrew Bolt, Olivia Wiles, Milad Nasr, Ilia Shumailov, Marco Selvi, Francesco Piccinno, Ricardo Aguilar, Sara McCarthy, Misha Khalman, Mrinal Shukla, Vlado Galic, John Carpenter, Kevin Villela, Haibin Zhang, Harry Richardson, James Martens, Matko Bosnjak, ShreyasÂ Rammohan Belle, Jeff Seibert, Mahmoud Alnahlawi, Brian McWilliams, Sankalp Singh, Annie Louis, Wen Ding, Dan Popovici, Lenin Simicich, Laura Knight, Pulkit Mehta, Nishesh Gupta, Chongyang Shi, Saaber Fatehi, Jovana Mitrovic, Alex Grills, Joseph Pagadora, Dessie Petrova, Danielle Eisenbud, Zhishuai Zhang, Damion Yates, Bhavishya Mittal, Nilesh Tripuraneni, Yannis Assael, Thomas Brovelli, Prateek Jain, Mihajlo Velimirovic, Canfer
Akbulut, Jiaqi Mu, Wolfgang Macherey, Ravin Kumar, Jun Xu, Haroon Qureshi, Gheorghe Comanici, Jeremy Wiesner, Zhitao Gong, Anton Ruddock, Matthias Bauer, Nick Felt, Anirudh GP, Anurag Arnab, Dustin Zelle, Jonas Rothfuss, Bill Rosgen, Ashish Shenoy, Bryan Seybold, Xinjian Li, Jayaram Mudigonda, Goker Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi, YiÂ Yao, Christopher Yew, Steven Kan, Isaac Caswell, Carey Radebaugh, Andre Elisseeff, Pedro Valenzuela, Kay McKinney, Kim Paterson, Albert Cui, Eri Latorre-Chimoto, Solomon Kim, William Zeng, Ken Durden, Priya Ponnapalli, Tiberiu Sosea, ChristopherÂ A. Choquette-Choo, James Manyika, Brona Robenek, Harsha Vashisht, Sebastien Pereira, Hoi Lam, Marko Velic, Denese Owusu-Afriyie, Katherine Lee, Tolga Bolukbasi, Alicia Parrish, Shawn Lu, Jane Park, Balaji Venkatraman, Alice Talbert, Lambert Rosique, Yuchung Cheng, Andrei Sozanschi, Adam Paszke, Praveen Kumar, Jessica Austin, LuÂ Li, Khalid Salama, Wooyeol Kim, Nandita Dukkipati, Anthony Baryshnikov, Christos Kaplanis,
XiangHai Sheng, Yuri Chervonyi, Caglar Unlu, Diego deÂ LasÂ Casas, Harry Askham, Kathryn Tunyasuvunakool, Felix Gimeno, Siim Poder, Chester Kwak, Matt Miecnikowski, Vahab Mirrokni, Alek Dimitriev, Aaron Parisi, Dangyi Liu, Tomy Tsai, Toby Shevlane, Christina Kouridi, Drew Garmon, Adrian Goedeckemeyer, AdamÂ R. Brown, Anitha Vijayakumar, Ali Elqursh, Sadegh Jazayeri, Jin Huang, SaraÂ Mc Carthy, Jay Hoover, Lucy Kim, Sandeep Kumar, Wei Chen, Courtney Biles, Garrett Bingham, Evan Rosen, Lisa Wang, Qijun Tan, David Engel, Francesco Pongetti, Dario deÂ Cesare, Dongseong Hwang, Lily Yu, Jennifer Pullman, Srini Narayanan, Kyle Levin, Siddharth Gopal, Megan Li, Asaf Aharoni, Trieu Trinh, Jessica Lo, Norman Casagrande, Roopali Vij, Loic Matthey, Bramandia Ramadhana, Austin Matthews, CJÂ Carey, Matthew Johnson, Kremena Goranova, Rohin Shah, Shereen Ashraf, Kingshuk Dasgupta, Rasmus Larsen, Yicheng Wang, ManishÂ Reddy Vuyyuru, Chong Jiang, Joana Ijazi, Kazuki Osawa, Celine Smith, RamyaÂ Sree Boppana, Taylan Bilal, Yuma
Koizumi, Ying Xu, Yasemin Altun, Nir Shabat, Ben Bariach, Alex Korchemniy, Kiam Choo, Olaf Ronneberger, Chimezie Iwuanyanwu, Shubin Zhao, David Soergel, Cho-Jui Hsieh, Irene Cai, Shariq Iqbal, Martin Sundermeyer, Zhe Chen, Elie Bursztein, Chaitanya Malaviya, Fadi Biadsy, Prakash Shroff, Inderjit Dhillon, Tejasi Latkar, Chris Dyer, Hannah Forbes, Massimo Nicosia, Vitaly Nikolaev, Somer Greene, Marin Georgiev, Pidong Wang, Nina Martin, Hanie Sedghi, John Zhang, Praseem Banzal, Doug Fritz, Vikram Rao, Xuezhi Wang, Jiageng Zhang, Viorica Patraucean, Dayou Du, Igor Mordatch, Ivan Jurin, Lewis Liu, Ayush Dubey, Abhi Mohan, Janek Nowakowski, Vlad-Doru Ion, Nan Wei, Reiko Tojo, MariaÂ Abi Raad, DrewÂ A. Hudson, Vaishakh Keshava, Shubham Agrawal, Kevin Ramirez, Zhichun Wu, Hoang Nguyen, JiÂ Liu, Madhavi Sewak, Bryce Petrini, DongHyun Choi, Ivan Philips, Ziyue Wang, Ioana Bica, Ankush Garg, Jarek Wilkiewicz, Priyanka Agrawal, Xiaowei Li, Danhao Guo, Emily Xue, Naseer Shaik, Andrew Leach, SadhÂ MNM Khan, Julia Wiesinger,
Sammy Jerome, Abhishek Chakladar, AlekÂ Wenjiao Wang, Tina Ornduff, Folake Abu, Alireza Ghaffarkhah, Marcus Wainwright, Mario Cortes, Frederick Liu, Joshua Maynez, Andreas Terzis, Pouya Samangouei, Riham Mansour, Tomasz KÄ™pa, FranÃ§ois-Xavier Aubet, Anton Algymr, Dan Banica, Agoston Weisz, Andras Orban, Alexandre Senges, Ewa Andrejczuk, Mark Geller, NiccoloÂ Dal Santo, Valentin Anklin, MajdÂ Al Merey, Martin Baeuml, Trevor Strohman, Junwen Bai, Slav Petrov, Yonghui Wu, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05530" title="">https://arxiv.org/abs/2403.05530</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Realm: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, ICMLâ€™20. JMLR.org, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hsieh etÂ al. (2024)</span>
<span class="ltx_bibblock">
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg.

</span>
<span class="ltx_bibblock">Ruler: Whatâ€™s the real context size of your long-context language models?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2404.06654</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard &amp; Grave (2021)</span>
<span class="ltx_bibblock">
Gautier Izacard and Edouard Grave.

</span>
<span class="ltx_bibblock">Leveraging passage retrieval with generative models for open domain question answering.

</span>
<span class="ltx_bibblock">In Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</em>, pp.Â  874â€“880, Online, April 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.eacl-main.74</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.eacl-main.74" title="">https://aclanthology.org/2021.eacl-main.74</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard etÂ al. (2024)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave.

</span>
<span class="ltx_bibblock">Atlas: few-shot learning with retrieval augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">J. Mach. Learn. Res.</em>, 24(1), March 2024.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Longrag: Enhancing retrieval-augmented generation with long-context llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">ArXiv</em>, abs/2406.15319, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:270688725" title="">https://api.semanticscholar.org/CorpusID:270688725</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi etÂ al. (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension.

</span>
<span class="ltx_bibblock">In Regina Barzilay and Min-Yen Kan (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.Â  1601â€“1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P17-1147</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P17-1147" title="">https://aclanthology.org/P17-1147</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jouppi etÂ al. (2023)</span>
<span class="ltx_bibblock">
NormanÂ P. Jouppi, George Kurian, Sheng Li, PeterÂ C. Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, Cliff Young, Xiaoping Zhou, Zongwei Zhou, and DavidÂ A. Patterson.

</span>
<span class="ltx_bibblock">Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 50th Annual International Symposium on Computer Architecture</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257921908" title="">https://api.semanticscholar.org/CorpusID:257921908</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamradt (2023)</span>
<span class="ltx_bibblock">
Gregory Kamradt.

</span>
<span class="ltx_bibblock">Needle in a haystack - pressure testing llms, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main" title="">https://github.com/gkamradt/LLMTest_NeedleInAHaystack/tree/main</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pp.Â  6769â€“6781, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.550</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.550" title="">https://aclanthology.org/2020.emnlp-main.550</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. (2023)</span>
<span class="ltx_bibblock">
Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.Â  996â€“1009, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.63</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.63" title="">https://aclanthology.org/2023.emnlp-main.63</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma &amp; Ba (2015)</span>
<span class="ltx_bibblock">
DiederikÂ P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A method for stochastic optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">ICLR (Poster)</em>, 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1412.6980" title="">http://arxiv.org/abs/1412.6980</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KoÄiskÃ½ etÂ al. (2018)</span>
<span class="ltx_bibblock">
TomÃ¡Å¡ KoÄiskÃ½, Jonathan Schwarz, Phil Blunsom, Chris Dyer, KarlÂ Moritz Hermann, GÃ¡bor Melis, and Edward Grefenstette.

</span>
<span class="ltx_bibblock">The NarrativeQA reading comprehension challenge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Transactions of the Association for Computational Linguistics</em>, 6:317â€“328, 2018.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl_a_00023</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/Q18-1023" title="">https://aclanthology.org/Q18-1023</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kopf etÂ al. (2023)</span>
<span class="ltx_bibblock">
Andreas Kopf, Yannic Kilcher, Dimitri von Rutte, Sotiris Anagnostidis, ZhiÂ Rui Tam, Keith Stevens, Abdullah Barhoum, NguyenÂ Minh Duc, Oliver Stanley, Richâ€™ard Nagyfi, ESÂ Shahul, Sameer Suri, David Glushkov, Arnav Dantuluri, Andrew Maguire, Christoph Schuhmann, Huu Nguyen, and Alexander Mattick.

</span>
<span class="ltx_bibblock">Openassistant conversations - democratizing large language model alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">ArXiv</em>, abs/2304.07327, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258179434" title="">https://api.semanticscholar.org/CorpusID:258179434</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski etÂ al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, AndrewÂ M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.

</span>
<span class="ltx_bibblock">Natural questions: A benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Transactions of the Association for Computational Linguistics</em>, 7:452â€“466, 2019.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl_a_00276</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/Q19-1026" title="">https://aclanthology.org/Q19-1026</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, CodyÂ Hao Yu, JosephÂ E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020a)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">ACL</em>, pp.Â  7871â€“7880, 2020a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020b)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 34th International Conference on Neural Information Processing Systems</em>, NIPS â€™20, Red Hook, NY, USA, 2020b. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781713829546.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2024)</span>
<span class="ltx_bibblock">
NelsonÂ F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.

</span>
<span class="ltx_bibblock">Lost in the middle: How language models use long contexts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Transactions of the Association for Computational Linguistics</em>, 12:157â€“173, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl_a_00638</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.tacl-1.9" title="">https://aclanthology.org/2024.tacl-1.9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized BERT pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MetaAI etÂ al. (2024)</span>
<span class="ltx_bibblock">
MetaAI, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, CristianÂ Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, EricÂ Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, GeorgiaÂ Lewis Anderson, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, HuÂ Xu, Hugo Touvron, Iliyan
Zarov, ImanolÂ Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer vanÂ der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, KalyanÂ Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, KeÂ Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Lauren Rantala-Yeary, Laurens vanÂ der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke deÂ Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, MiteshÂ Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Olivier Duchenne, Onur Ã‡elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, PunitÂ Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, RicardoÂ Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, SeohyunÂ Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vladan Petrovic,
Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, XiaoqingÂ Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, YiÂ Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, ZacharieÂ Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aaron Grattafiori, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alex Vaughan, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Franco, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, BetoÂ De Paola, Bhargavi Paranjape, Bing Liu, BoÂ Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo,
Carl Parker, Carly Burton, Catalina Mejia, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, Danny Wyatt, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat Ozgenel, Francesco Caggioni, Francisco GuzmÃ¡n, Frank Kanayet, Frank Seide, GabrielaÂ Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Govind Thattai, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, KamÂ Hou U, Karan Saxena, Karthik Prasad, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kun Huang, Kunal Chawla, Kushal Lakhotia, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Maria Tsimpoukelli, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, MichaelÂ L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, MiquelÂ Jubert Hermoso, MoÂ Metanat, Mohammad Rastegari, Munish Bansal, Nandhini
Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, NikolayÂ Pavlovich Laptev, Ning Dong, Ning Zhang, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Rohan Maheswari, Russ Howes, Ruty Rinott, SaiÂ Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, ShengxinÂ Cindy Zha, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta,
Sungmin Cho, Sunny Virk, Suraj Subramanian, SyÂ Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Kohler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, VinayÂ Satish Kumar, Vishal Mangla, VÃ­tor Albiero, Vlad Ionescu, Vlad Poenaru, VladÂ Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaofang Wang, Xiaojian Wu, Xiaolan Wang, Xide Xia, Xilun Wu, Xinbo Gao, Yanjun Chen, YeÂ Hu, YeÂ Jia, YeÂ Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yuchen Hao, Yundi Qian, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, and Zhiwei Zhao.

</span>
<span class="ltx_bibblock">The llama 3 herd of models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MistralAI (2024)</span>
<span class="ltx_bibblock">
MistralAI.

</span>
<span class="ltx_bibblock">Mistral nemo, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mistral.ai/news/mistral-nemo/" title="">https://mistral.ai/news/mistral-nemo/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, XuÂ Jiang, Diogo Almeida, CarrollÂ L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, PaulÂ F. Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng etÂ al. (2024)</span>
<span class="ltx_bibblock">
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.

</span>
<span class="ltx_bibblock">YaRN: Efficient context window extension of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=wHBfxhZu1u" title="">https://openreview.net/forum?id=wHBfxhZu1u</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ofir Press, Noah Smith, and Mike Lewis.

</span>
<span class="ltx_bibblock">Train short, test long: Attention with linear biases enables input length extrapolation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=R8sQPpGCv0" title="">https://openreview.net/forum?id=R8sQPpGCv0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, etÂ al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, etÂ al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">OpenAI blog</em>, 1(8):9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, ChristopherÂ D. Manning, Stefano Ermon, and Chelsea Finn.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel etÂ al. (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and PeterÂ J. Liu.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">J. Mach. Learn. Res.</em>, 21:140:1â€“140:67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts etÂ al. (2020)</span>
<span class="ltx_bibblock">
Adam Roberts, Colin Raffel, and Noam Shazeer.

</span>
<span class="ltx_bibblock">How much knowledge can you pack into the parameters of a language model?

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pp.Â  5418â€“5426, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.437</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.437" title="">https://aclanthology.org/2020.emnlp-main.437</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson &amp; Zaragoza (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson and Hugo Zaragoza.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: Bm25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Found. Trends Inf. Retr.</em>, 3(4):333â€“389, apr 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">RoziÃ¨re etÂ al. (2024)</span>
<span class="ltx_bibblock">
Baptiste RoziÃ¨re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, XiaoqingÂ Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, JÃ©rÃ©my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, CristianÂ Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre DÃ©fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.12950" title="">https://arxiv.org/abs/2308.12950</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman etÂ al. (2017)</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:1707.06347</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jianlin Su, Murtadha Ahmed, YuÂ Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.

</span>
<span class="ltx_bibblock">Roformer: Enhanced transformer with rotary position embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Neurocomput.</em>, 568(C), March 2024.

</span>
<span class="ltx_bibblock">ISSN 0925-2312.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.neucom.2023.127063</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2023.127063" title="">https://doi.org/10.1016/j.neucom.2023.127063</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhiqing Sun, Xuezhi Wang, YiÂ Tay, Yiming Yang, and Denny Zhou.

</span>
<span class="ltx_bibblock">Recitation-augmented language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=-cqvvvb-NkI" title="">https://openreview.net/forum?id=-cqvvvb-NkI</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Together (2023)</span>
<span class="ltx_bibblock">
Together.

</span>
<span class="ltx_bibblock">OpenChatKit: An Open Toolkit and Base Model for Dialogue-style Applications, 3 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/togethercomputer/OpenChatKit" title="">https://github.com/togethercomputer/OpenChatKit</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">ArXiv</em>, abs/2302.13971, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257219404" title="">https://api.semanticscholar.org/CorpusID:257219404</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, KevinÂ R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, DanielÂ M. Bikel, Lukas Blecher, CristianÂ CantÃ³n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, AnthonyÂ S. Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, IsabelÂ M. Kloumann, A.Â V. Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, R.Â Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and
Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">ArXiv</em>, abs/2307.09288, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:259950998" title="">https://api.semanticscholar.org/CorpusID:259950998</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanÂ N. Gomez, Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of the 31st International Conference on Neural Information Processing Systems</em>, NIPSâ€™17, pp.Â  6000â€“6010, Red Hook, NY, USA, 2017. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781510860964.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, EdÂ H. Chi, QuocÂ V. Le, and Denny Zhou.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">NeurIPS</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2022a)</span>
<span class="ltx_bibblock">
Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, Alborz Geramifard, and Zhou Yu.

</span>
<span class="ltx_bibblock">Memformer: A memory-augmented transformer for sequence modeling.

</span>
<span class="ltx_bibblock">In Yulan He, Heng Ji, Sujian Li, Yang Liu, and Chua-Hui Chang (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022</em>, pp.Â  308â€“318, Online only, November 2022a. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.findings-aacl.29" title="">https://aclanthology.org/2022.findings-aacl.29</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2022b)</span>
<span class="ltx_bibblock">
Yuhuai Wu, MarkusÂ Norman Rabe, DeLesley Hutchins, and Christian Szegedy.

</span>
<span class="ltx_bibblock">Memorizing transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">International Conference on Learning Representations</em>, 2022b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=TrjbxzRcnf-" title="">https://openreview.net/forum?id=TrjbxzRcnf-</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis.

</span>
<span class="ltx_bibblock">Efficient streaming language models with attention sinks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">The Twelfth International Conference on Learning Representations</em>, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=NG7sS51zVF" title="">https://openreview.net/forum?id=NG7sS51zVF</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie.

</span>
<span class="ltx_bibblock">C-pack: Packed resources for general chinese embeddings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, SIGIR â€™24, pp.Â  641â€“649, New York, NY, USA, 2024b. Association for Computing Machinery.

</span>
<span class="ltx_bibblock">ISBN 9798400704314.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3626772.3657878</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3626772.3657878" title="">https://doi.org/10.1145/3626772.3657878</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, YeÂ Li, Kwok-Fung Tang, Jialin Liu, PaulÂ N. Bennett, Junaid Ahmed, and Arnold Overwijk.

</span>
<span class="ltx_bibblock">Approximate nearest neighbor negative contrastive learning for dense text retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">International Conference on Learning Representations</em>, 2021a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=zeFrfgyZln" title="">https://openreview.net/forum?id=zeFrfgyZln</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Wenhan Xiong, XiangÂ Lorraine Li, Srinivasan Iyer, Jingfei Du, Patrick Lewis, WilliamÂ Yang Wang, Yashar Mehdad, Wen-tau Yih, Sebastian Riedel, Douwe Kiela, and Barlas OÄŸuz.

</span>
<span class="ltx_bibblock">Answering complex open-domain questions with multi-hop dense retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">International Conference on Learning Representations (ICLR)</em>, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, KarthikÂ Abinav Sankararaman, Barlas OÄŸuz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, and Hao Ma.

</span>
<span class="ltx_bibblock">Effective long-context scaling of foundation models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">North American Chapter of the Association for Computational Linguistics</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:263134982" title="">https://api.semanticscholar.org/CorpusID:263134982</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, BlakeÂ A. Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, NoamÂ M. Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, and Zhifeng Chen.

</span>
<span class="ltx_bibblock">Gspmd: General and scalable parallelization for ml computation graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">ArXiv</em>, abs/2105.04663, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:234357958" title="">https://api.semanticscholar.org/CorpusID:234357958</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.

</span>
<span class="ltx_bibblock">ReAct: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">International Conference on Learning Representations (ICLR)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang.

</span>
<span class="ltx_bibblock">Generate rather than retrieve: Large language models are strong context generators.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=fB0hRu9GZUS" title="">https://openreview.net/forum?id=fB0hRu9GZUS</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wenhao Wu, Furu Wei, and Sujian Li.

</span>
<span class="ltx_bibblock">PoSE: Efficient context window extension of LLMs via positional skip-wise training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">The Twelfth International Conference on Learning Representations</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=3Z1gxuAQrA" title="">https://openreview.net/forum?id=3Z1gxuAQrA</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Oct 11 18:00:20 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
