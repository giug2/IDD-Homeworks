<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2112.06888] Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection</title><meta property="og:description" content="Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2112.06888">

<!--Generated on Fri Mar  1 15:53:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Diego Garcia-Olano, Yasumasa Onoe, Joydeep Ghosh

</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Knowledge-Based Visual Question Answering (KBVQA) is a bi-modal task requiring external world knowledge in order to correctly answer a text question and associated image. Recent single modality text work has shown knowledge injection into pre-trained language models, specifically entity enhanced knowledge graph embeddings, can improve performance on downstream entity-centric tasks. In this work, we empirically study how and whether such methods, applied in a bi-modal setting, can improve an existing VQA systemâ€™s performance on the KBVQA task. We experiment with two large publicly available VQA datasets, (1) KVQA which contains mostly rare Wikipedia entities and (2) OKVQA which is less entity-centric and more aligned with common sense reasoning. Both lack explicit entity spans and we study the effect of different weakly supervised and manual methods for obtaining them. Additionally we analyze how recently proposed bi-modal and single modal attention explanations are affected by the incorporation of such entity enhanced representations. Our results show substantial improved performance on the KBVQA task without the need for additional costly pre-training and we provide insights for when entity knowledge injection helps improve a modelâ€™s understanding.
We provide code and enhanced datasets for reproducibility.</p>
</div>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">Visual Question Answering (VQA) is a multi-modal task that involves correctly answering a text question pertaining to an associated image without the explicit need for external world knowledge (facts about history, geography, etc). In addition to datasets involving the need for commonsense reasoning <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2019</a>; Wang etÂ al. <a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite>, recent work on <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">knowledge-based VQA</span> <cite class="ltx_cite ltx_citemacro_citep">(SanketÂ Shah and Talukdar <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> involve questions whose answers explicitly require external knowledge about named entities within an image.
Common amongst state of the art VQA solutions <cite class="ltx_cite ltx_citemacro_citep">(Singh etÂ al. <a href="#bib.bib28" title="" class="ltx_ref">2020</a>)</cite> is the need for large amounts of computational resources and supervised question-image pairs in order to pretrain models that generalize well. Recent work E-BERT <cite class="ltx_cite ltx_citemacro_citep">(Poerner, Waltinger, and SchÃ¼tze <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> gives improved performance on single modality, entity-centric text tasks by using efficient external knowledge injection into pre-trained Transformer language models (LMs). Although there has been quite a bit of work studying whether LMs can be used as knowledge bases<cite class="ltx_cite ltx_citemacro_citep">(Petroni etÂ al. <a href="#bib.bib18" title="" class="ltx_ref">2019</a>; Roberts, Raffel, and Shazeer <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; Poerner, Waltinger, and SchÃ¼tze <a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> there has been less attention on how this affects vision-language models. Additionally, while research on interpretability methods for single modalities is abundant <cite class="ltx_cite ltx_citemacro_citep">(Sundararajan, Taly, and Yan <a href="#bib.bib32" title="" class="ltx_ref">2017</a>; Lundberg and Lee <a href="#bib.bib14" title="" class="ltx_ref">2017</a>; Pruthi etÂ al. <a href="#bib.bib21" title="" class="ltx_ref">2020</a>)</cite>, saliency maps for images or feature attribution methods for text for instance, only recently are there methods explicitly targeted for bi-modal tasks like VQA, namely the bi-modal generic attention explainability method BM-GAE <cite class="ltx_cite ltx_citemacro_citep">(Chefer, Gur, and Wolf <a href="#bib.bib2" title="" class="ltx_ref">2021a</a>)</cite> which provides a promising method by which to understand image and text explanations jointly. 
<br class="ltx_break">In this work we analyze how knowledge injection via E-BERT affects the performance on an existing visual-linguistic model LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> on the relatively unexplored task of knowledge-based VQA (KBVQA) both in terms of accuracy and explainability via BM-GAE.
We experiment using two large publicly available VQA datasets: (i)KVQA <cite class="ltx_cite ltx_citemacro_citep">(SanketÂ Shah and Talukdar <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> that is explicitly tied to Wikipedia and rich in rare entities and (ii) OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> that is less entity-centric and more aligned with common sense reasoning. Both datasets lack explicit entity spans and we show how using different entity sets resulting from either weakly supervised methods or manual human annotation affects knowledge injection on task performance. 
<br class="ltx_break">Our analysis shows improved performance on the entity rich KVQA dataset, 2.5% top 1 accuracy, and a smaller improvement on the OKVQA dataset. Both come without the need for any additional costly pre-training; for a given dataset, simply fine tune LXMERT using knowledge injection via E-BERT and do inference on its test set. In addition to error analysis, we assess the effect of E-BERT on the explanations generated by BM-GAE and provide insights for when entity injection helps improve a modelâ€™s understanding for knowledge aware VQA. Our weakly and manually enhanced datasets and code are made available at <span id="Sx1.p1.1.2" class="ltx_text ltx_font_typewriter">anonymous.4open.science/r/kbvqa_xai/</span>.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Background and Related Work</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">In this section we provide some background on E-BERT, bi-modal generic attention explainability for VQA and LXMERT along with related works.</p>
</div>
<section id="Sx2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">E-BERT</h3>

<div id="Sx2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px1.p1.7" class="ltx_p">Wikipedia2Vec <cite class="ltx_cite ltx_citemacro_citep">(Yamada etÂ al. <a href="#bib.bib37" title="" class="ltx_ref">2016</a>)</cite> embeds
words and entities (Wikipedia URLs) into a common space. Given a vocabulary of words <math id="Sx2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\mathbb{L}_{\mathrm{Word}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">Word</mi></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.1.m1.1.1.3">Word</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.1.m1.1c">\mathbb{L}_{\mathrm{Word}}</annotation></semantics></math>
and a vocabulary of entities <math id="Sx2.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="\mathbb{L}_{\mathrm{Ent}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.2.m2.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">Ent</mi></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.2.m2.1.1.3">Ent</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.2.m2.1c">\mathbb{L}_{\mathrm{Ent}}</annotation></semantics></math>, it learns a lookup
embedding function <math id="Sx2.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{E}_{\mathrm{Wikipedia}}:\mathbb{L}_{\mathrm{Word}}\cup\mathbb{L}_{\mathrm{Ent}}\rightarrow R^{d_{\mathrm{Wikipedia}}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.2.cmml">â„°</mi><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.3.cmml">Wikipedia</mi></msub><mo lspace="0.278em" rspace="0.278em" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml">:</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml"><mrow id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.3.cmml">Word</mi></msub><mo id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.1" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.1.cmml">âˆª</mo><msub id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.3.cmml">Ent</mi></msub></mrow><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.1" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml">â†’</mo><msup id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.2.cmml">R</mi><msub id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.2.cmml">d</mi><mi id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.3.cmml">Wikipedia</mi></msub></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1"><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.1">:</ci><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.2">â„°</ci><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.2.3">Wikipedia</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3"><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.1">â†’</ci><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2"><union id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.1"></union><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.2.3">Word</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.2.3.3">Ent</ci></apply></apply><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3">superscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.2">ğ‘…</ci><apply id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.2">ğ‘‘</ci><ci id="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.3.m3.1.1.3.3.3.3">Wikipedia</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.3.m3.1c">\mathcal{E}_{\mathrm{Wikipedia}}:\mathbb{L}_{\mathrm{Word}}\cup\mathbb{L}_{\mathrm{Ent}}\rightarrow R^{d_{\mathrm{Wikipedia}}}</annotation></semantics></math> .The E-BERT authors <cite class="ltx_cite ltx_citemacro_citep">(Poerner, Waltinger, and SchÃ¼tze <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> align Wikipedia2Vec entity vectors <math id="Sx2.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{E}_{\mathrm{Wikipedia}}[\mathbb{L}_{\mathrm{Ent}}]" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.4.m4.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2.cmml">â„°</mi><mi id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.cmml">Wikipedia</mi></msub><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.2.1.cmml">[</mo><msub id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.3.cmml">Ent</mi></msub><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1"><times id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.2"></times><apply id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.2">â„°</ci><ci id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.3.3">Wikipedia</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1"><csymbol cd="latexml" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.4.m4.1.1.1.1.1.3">Ent</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.4.m4.1c">\mathcal{E}_{\mathrm{Wikipedia}}[\mathbb{L}_{\mathrm{Ent}}]</annotation></semantics></math> with BERTâ€™s wordpiece vector space <math id="Sx2.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{E}_{\mathrm{BERT}}[\mathbb{L}_{\mathrm{WP}}]" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.5.m5.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2.cmml">â„°</mi><mi id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3.cmml">BERT</mi></msub><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml">â€‹</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.2.cmml"><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.2.1.cmml">[</mo><msub id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml">WP</mi></msub><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1"><times id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.2"></times><apply id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.2">â„°</ci><ci id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.3.3">BERT</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1"><csymbol cd="latexml" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.2">delimited-[]</csymbol><apply id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.5.m5.1.1.1.1.1.3">WP</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.5.m5.1c">\mathcal{E}_{\mathrm{BERT}}[\mathbb{L}_{\mathrm{WP}}]</annotation></semantics></math> and then feed these aligned vectors into BERT as if they came from BERTâ€™s native wordpiece space. This procedure allows E-BERT to inject knowledge into BERT without making any changes to the BERT encoder itself or doing any additional pretraining. Specifically given an <math id="Sx2.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="x\in\mathbb{L}_{\mathrm{WP}}\cap\mathbb{L}_{\mathrm{Word}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.6.m6.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">x</mi><mo id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml">âˆˆ</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.3" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.3.cmml">WP</mi></msub><mo id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.1" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml">âˆ©</mo><msub id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.3.cmml">Word</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1"><in id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.1"></in><ci id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.2">ğ‘¥</ci><apply id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3"><intersect id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.1"></intersect><apply id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.2.3">WP</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.6.m6.1.1.3.3.3">Word</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.6.m6.1c">x\in\mathbb{L}_{\mathrm{WP}}\cap\mathbb{L}_{\mathrm{Word}}</annotation></semantics></math> they learn an unconstrained linear mapping <math id="Sx2.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{W}\in\mathbb{R}^{d_{\mathrm{BERT}}\times d_{\mathrm{Wikipedia}}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.7.m7.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml">ğ–</mi><mo id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.2.cmml">â„</mi><mrow id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.2.cmml">d</mi><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.3" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.3.cmml">BERT</mi></msub><mo lspace="0.222em" rspace="0.222em" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.1" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.1.cmml">Ã—</mo><msub id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.2.cmml">d</mi><mi id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.3.cmml">Wikipedia</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1"><in id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.1"></in><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.2">ğ–</ci><apply id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3">superscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.2">â„</ci><apply id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3"><times id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.1"></times><apply id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.2">ğ‘‘</ci><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.2.3">BERT</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.2">ğ‘‘</ci><ci id="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.7.m7.1.1.3.3.3.3">Wikipedia</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.7.m7.1c">\mathbf{W}\in\mathbb{R}^{d_{\mathrm{BERT}}\times d_{\mathrm{Wikipedia}}}</annotation></semantics></math> that seeks to minimize</p>
<table id="Sx2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="Sx2.E1.m1.3" class="ltx_Math" alttext="\sum_{x\in\mathbb{L}_{\mathrm{WP}}\cap\mathbb{L}_{\mathrm{Word}}}||\mathbf{W}{\mathcal{E}}_{\mathrm{Wikipedia}}(x)-{\mathcal{E}}_{\mathrm{BERT}}(x){||}_{2}^{2}" display="block"><semantics id="Sx2.E1.m1.3a"><mrow id="Sx2.E1.m1.3.3" xref="Sx2.E1.m1.3.3.cmml"><munder id="Sx2.E1.m1.3.3.2" xref="Sx2.E1.m1.3.3.2.cmml"><mo movablelimits="false" id="Sx2.E1.m1.3.3.2.2" xref="Sx2.E1.m1.3.3.2.2.cmml">âˆ‘</mo><mrow id="Sx2.E1.m1.3.3.2.3" xref="Sx2.E1.m1.3.3.2.3.cmml"><mi id="Sx2.E1.m1.3.3.2.3.2" xref="Sx2.E1.m1.3.3.2.3.2.cmml">x</mi><mo id="Sx2.E1.m1.3.3.2.3.1" xref="Sx2.E1.m1.3.3.2.3.1.cmml">âˆˆ</mo><mrow id="Sx2.E1.m1.3.3.2.3.3" xref="Sx2.E1.m1.3.3.2.3.3.cmml"><msub id="Sx2.E1.m1.3.3.2.3.3.2" xref="Sx2.E1.m1.3.3.2.3.3.2.cmml"><mi id="Sx2.E1.m1.3.3.2.3.3.2.2" xref="Sx2.E1.m1.3.3.2.3.3.2.2.cmml">ğ•ƒ</mi><mi id="Sx2.E1.m1.3.3.2.3.3.2.3" xref="Sx2.E1.m1.3.3.2.3.3.2.3.cmml">WP</mi></msub><mo id="Sx2.E1.m1.3.3.2.3.3.1" xref="Sx2.E1.m1.3.3.2.3.3.1.cmml">âˆ©</mo><msub id="Sx2.E1.m1.3.3.2.3.3.3" xref="Sx2.E1.m1.3.3.2.3.3.3.cmml"><mi id="Sx2.E1.m1.3.3.2.3.3.3.2" xref="Sx2.E1.m1.3.3.2.3.3.3.2.cmml">ğ•ƒ</mi><mi id="Sx2.E1.m1.3.3.2.3.3.3.3" xref="Sx2.E1.m1.3.3.2.3.3.3.3.cmml">Word</mi></msub></mrow></mrow></munder><msubsup id="Sx2.E1.m1.3.3.1" xref="Sx2.E1.m1.3.3.1.cmml"><mrow id="Sx2.E1.m1.3.3.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.2.cmml"><mo lspace="0em" stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.2" xref="Sx2.E1.m1.3.3.1.1.1.2.1.cmml">â€–</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.cmml"><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.cmml"><mi id="Sx2.E1.m1.3.3.1.1.1.1.1.2.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.2.cmml">ğ–</mi><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.1.cmml">â€‹</mo><msub id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.2.cmml">â„°</mi><mi id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.3" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.3.cmml">Wikipedia</mi></msub><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.1a" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.1.cmml">â€‹</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.2.4.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.4.2.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="Sx2.E1.m1.1.1" xref="Sx2.E1.m1.1.1.cmml">x</mi><mo stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.4.2.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="Sx2.E1.m1.3.3.1.1.1.1.1.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml">âˆ’</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.3" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.cmml"><msub id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml">â„°</mi><mi id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.3" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.3.cmml">BERT</mi></msub><mo lspace="0em" rspace="0em" id="Sx2.E1.m1.3.3.1.1.1.1.1.3.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.1.cmml">â€‹</mo><mrow id="Sx2.E1.m1.3.3.1.1.1.1.1.3.3.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.cmml"><mo stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.1.3.3.2.1" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.cmml">(</mo><mi id="Sx2.E1.m1.2.2" xref="Sx2.E1.m1.2.2.cmml">x</mi><mo stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.1.3.3.2.2" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="Sx2.E1.m1.3.3.1.1.1.1.3" xref="Sx2.E1.m1.3.3.1.1.1.2.1.cmml">â€–</mo></mrow><mn id="Sx2.E1.m1.3.3.1.1.3" xref="Sx2.E1.m1.3.3.1.1.3.cmml">2</mn><mn id="Sx2.E1.m1.3.3.1.3" xref="Sx2.E1.m1.3.3.1.3.cmml">2</mn></msubsup></mrow><annotation-xml encoding="MathML-Content" id="Sx2.E1.m1.3b"><apply id="Sx2.E1.m1.3.3.cmml" xref="Sx2.E1.m1.3.3"><apply id="Sx2.E1.m1.3.3.2.cmml" xref="Sx2.E1.m1.3.3.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.2.1.cmml" xref="Sx2.E1.m1.3.3.2">subscript</csymbol><sum id="Sx2.E1.m1.3.3.2.2.cmml" xref="Sx2.E1.m1.3.3.2.2"></sum><apply id="Sx2.E1.m1.3.3.2.3.cmml" xref="Sx2.E1.m1.3.3.2.3"><in id="Sx2.E1.m1.3.3.2.3.1.cmml" xref="Sx2.E1.m1.3.3.2.3.1"></in><ci id="Sx2.E1.m1.3.3.2.3.2.cmml" xref="Sx2.E1.m1.3.3.2.3.2">ğ‘¥</ci><apply id="Sx2.E1.m1.3.3.2.3.3.cmml" xref="Sx2.E1.m1.3.3.2.3.3"><intersect id="Sx2.E1.m1.3.3.2.3.3.1.cmml" xref="Sx2.E1.m1.3.3.2.3.3.1"></intersect><apply id="Sx2.E1.m1.3.3.2.3.3.2.cmml" xref="Sx2.E1.m1.3.3.2.3.3.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.2.3.3.2.1.cmml" xref="Sx2.E1.m1.3.3.2.3.3.2">subscript</csymbol><ci id="Sx2.E1.m1.3.3.2.3.3.2.2.cmml" xref="Sx2.E1.m1.3.3.2.3.3.2.2">ğ•ƒ</ci><ci id="Sx2.E1.m1.3.3.2.3.3.2.3.cmml" xref="Sx2.E1.m1.3.3.2.3.3.2.3">WP</ci></apply><apply id="Sx2.E1.m1.3.3.2.3.3.3.cmml" xref="Sx2.E1.m1.3.3.2.3.3.3"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.2.3.3.3.1.cmml" xref="Sx2.E1.m1.3.3.2.3.3.3">subscript</csymbol><ci id="Sx2.E1.m1.3.3.2.3.3.3.2.cmml" xref="Sx2.E1.m1.3.3.2.3.3.3.2">ğ•ƒ</ci><ci id="Sx2.E1.m1.3.3.2.3.3.3.3.cmml" xref="Sx2.E1.m1.3.3.2.3.3.3.3">Word</ci></apply></apply></apply></apply><apply id="Sx2.E1.m1.3.3.1.cmml" xref="Sx2.E1.m1.3.3.1"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.1.2.cmml" xref="Sx2.E1.m1.3.3.1">superscript</csymbol><apply id="Sx2.E1.m1.3.3.1.1.cmml" xref="Sx2.E1.m1.3.3.1"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.1.1.2.cmml" xref="Sx2.E1.m1.3.3.1">subscript</csymbol><apply id="Sx2.E1.m1.3.3.1.1.1.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="Sx2.E1.m1.3.3.1.1.1.2.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.2">norm</csymbol><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1"><minus id="Sx2.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.1"></minus><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2"><times id="Sx2.E1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.1"></times><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.2">ğ–</ci><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.2">â„°</ci><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.3.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.2.3.3">Wikipedia</ci></apply><ci id="Sx2.E1.m1.1.1.cmml" xref="Sx2.E1.m1.1.1">ğ‘¥</ci></apply><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.3.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3"><times id="Sx2.E1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.1"></times><apply id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.1.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2">subscript</csymbol><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.2.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.2">â„°</ci><ci id="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.3.cmml" xref="Sx2.E1.m1.3.3.1.1.1.1.1.3.2.3">BERT</ci></apply><ci id="Sx2.E1.m1.2.2.cmml" xref="Sx2.E1.m1.2.2">ğ‘¥</ci></apply></apply></apply><cn type="integer" id="Sx2.E1.m1.3.3.1.1.3.cmml" xref="Sx2.E1.m1.3.3.1.1.3">2</cn></apply><cn type="integer" id="Sx2.E1.m1.3.3.1.3.cmml" xref="Sx2.E1.m1.3.3.1.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.E1.m1.3c">\sum_{x\in\mathbb{L}_{\mathrm{WP}}\cap\mathbb{L}_{\mathrm{Word}}}||\mathbf{W}{\mathcal{E}}_{\mathrm{Wikipedia}}(x)-{\mathcal{E}}_{\mathrm{BERT}}(x){||}_{2}^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="Sx2.SS0.SSS0.Px1.p1.18" class="ltx_p">Since Wikipedia2Vec embeds <math id="Sx2.SS0.SSS0.Px1.p1.8.m1.1" class="ltx_Math" alttext="\mathbb{L}_{\mathrm{Word}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.8.m1.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.3.cmml">Word</mi></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.8.m1.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.8.m1.1.1.3">Word</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.8.m1.1c">\mathbb{L}_{\mathrm{Word}}</annotation></semantics></math> and <math id="Sx2.SS0.SSS0.Px1.p1.9.m2.1" class="ltx_Math" alttext="\mathbb{L}_{\mathrm{Ent}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.9.m2.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.3.cmml">Ent</mi></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.9.m2.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.9.m2.1.1.3">Ent</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.9.m2.1c">\mathbb{L}_{\mathrm{Ent}}</annotation></semantics></math> into
the same space, <math id="Sx2.SS0.SSS0.Px1.p1.10.m3.1" class="ltx_Math" alttext="W" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.10.m3.1a"><mi id="Sx2.SS0.SSS0.Px1.p1.10.m3.1.1" xref="Sx2.SS0.SSS0.Px1.p1.10.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.10.m3.1b"><ci id="Sx2.SS0.SSS0.Px1.p1.10.m3.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.10.m3.1.1">ğ‘Š</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.10.m3.1c">W</annotation></semantics></math> can be applied
to <math id="Sx2.SS0.SSS0.Px1.p1.11.m4.1" class="ltx_Math" alttext="L_{\mathrm{Ent}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.11.m4.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.2.cmml">L</mi><mi id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.3.cmml">Ent</mi></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.11.m4.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.2">ğ¿</ci><ci id="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.11.m4.1.1.3">Ent</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.11.m4.1c">L_{\mathrm{Ent}}</annotation></semantics></math> as well. Thus at inference time they simply use <math id="Sx2.SS0.SSS0.Px1.p1.12.m5.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.12.m5.1a"><mi id="Sx2.SS0.SSS0.Px1.p1.12.m5.1.1" xref="Sx2.SS0.SSS0.Px1.p1.12.m5.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.12.m5.1b"><ci id="Sx2.SS0.SSS0.Px1.p1.12.m5.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.12.m5.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.12.m5.1c">\mathbf{W}</annotation></semantics></math> to construct an <math id="Sx2.SS0.SSS0.Px1.p1.13.m6.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.13.m6.1a"><msub id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.2.cmml">â„°</mi><mrow id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.cmml"><mi mathvariant="normal" id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.2.cmml">E</mi><mo id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.1" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.1.cmml">âˆ’</mo><mi id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.3.cmml">BERT</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.13.m6.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.2">â„°</ci><apply id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3"><minus id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.1"></minus><ci id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.2">E</ci><ci id="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.13.m6.1.1.3.3">BERT</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.13.m6.1c">{\mathcal{E}}_{\mathrm{E-BERT}}</annotation></semantics></math> entity embeddings via <math id="Sx2.SS0.SSS0.Px1.p1.14.m7.2" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}(a)={\mathbf{W}{E}_{\mathrm{Wikipedia}}(a)}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.14.m7.2a"><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.cmml"><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.2.cmml">â„°</mi><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.cmml"><mi mathvariant="normal" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.2.cmml">E</mi><mo id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.1.cmml">âˆ’</mo><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.3.cmml">BERT</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.1.cmml">â€‹</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.cmml"><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.3.2.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.cmml">(</mo><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.1.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.1.1.cmml">a</mi><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.cmml">)</mo></mrow></mrow><mo id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.1.cmml">=</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.2.cmml">ğ–</mi><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1.cmml">â€‹</mo><msub id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.2.cmml">E</mi><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.3.cmml">Wikipedia</mi></msub><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1a" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1.cmml">â€‹</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.4.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.cmml"><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.4.2.1" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.cmml">(</mo><mi id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.2.cmml">a</mi><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.4.2.2" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2b"><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3"><eq id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.1"></eq><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2"><times id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.1"></times><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.2">â„°</ci><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3"><minus id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.1"></minus><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.2">E</ci><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.2.2.3.3">BERT</ci></apply></apply><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.1.1">ğ‘</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3"><times id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.1"></times><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.2">ğ–</ci><apply id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.2">ğ¸</ci><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.3.3.3.3">Wikipedia</ci></apply><ci id="Sx2.SS0.SSS0.Px1.p1.14.m7.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.14.m7.2.2">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.14.m7.2c">{\mathcal{E}}_{\mathrm{E-BERT}}(a)={\mathbf{W}{E}_{\mathrm{Wikipedia}}(a)}</annotation></semantics></math> for an entity <math id="Sx2.SS0.SSS0.Px1.p1.15.m8.1" class="ltx_Math" alttext="a" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.15.m8.1a"><mi id="Sx2.SS0.SSS0.Px1.p1.15.m8.1.1" xref="Sx2.SS0.SSS0.Px1.p1.15.m8.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.15.m8.1b"><ci id="Sx2.SS0.SSS0.Px1.p1.15.m8.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.15.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.15.m8.1c">a</annotation></semantics></math> where <math id="Sx2.SS0.SSS0.Px1.p1.16.m9.1" class="ltx_Math" alttext="\mathcal{E}_{\mathrm{E-BERT}}:\mathbb{L}_{\mathrm{Ent}}\rightarrow R^{d_{\mathrm{BERT}}}" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.16.m9.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.2.cmml">â„°</mi><mrow id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.cmml"><mi mathvariant="normal" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.2.cmml">E</mi><mo id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.1" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.1.cmml">âˆ’</mo><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.3.cmml">BERT</mi></mrow></msub><mo lspace="0.278em" rspace="0.278em" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.1" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.1.cmml">:</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.2.cmml">ğ•ƒ</mi><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.3.cmml">Ent</mi></msub><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.1" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.1.cmml">â†’</mo><msup id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.2.cmml">R</mi><msub id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.cmml"><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.2" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.2.cmml">d</mi><mi id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.3" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.3.cmml">BERT</mi></msub></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1"><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.1">:</ci><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.2">â„°</ci><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3"><minus id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.1"></minus><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.2">E</ci><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.2.3.3">BERT</ci></apply></apply><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3"><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.1">â†’</ci><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.2">ğ•ƒ</ci><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.2.3">Ent</ci></apply><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3">superscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.2">ğ‘…</ci><apply id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.2">ğ‘‘</ci><ci id="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.16.m9.1.1.3.3.3.3">BERT</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.16.m9.1c">\mathcal{E}_{\mathrm{E-BERT}}:\mathbb{L}_{\mathrm{Ent}}\rightarrow R^{d_{\mathrm{BERT}}}</annotation></semantics></math> . They then prepend <math id="Sx2.SS0.SSS0.Px1.p1.17.m10.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}(a)" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.17.m10.1a"><mrow id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.cmml"><msub id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.2.cmml">â„°</mi><mrow id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.cmml"><mi mathvariant="normal" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.2.cmml">E</mi><mo id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.1" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.1.cmml">âˆ’</mo><mi id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.3" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.3.cmml">BERT</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.1" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.1.cmml">â€‹</mo><mrow id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.3.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.cmml"><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.3.2.1" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.cmml">(</mo><mi id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.1" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.1.cmml">a</mi><mo stretchy="false" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.3.2.2" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1b"><apply id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2"><times id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.1"></times><apply id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2"><csymbol cd="ambiguous" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2">subscript</csymbol><ci id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.2">â„°</ci><apply id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3"><minus id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.1"></minus><ci id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.2.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.2">E</ci><ci id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.3.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.2.2.3.3">BERT</ci></apply></apply><ci id="Sx2.SS0.SSS0.Px1.p1.17.m10.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.17.m10.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.17.m10.1c">{\mathcal{E}}_{\mathrm{E-BERT}}(a)</annotation></semantics></math> to the BERT embedding of <math id="Sx2.SS0.SSS0.Px1.p1.18.m11.1" class="ltx_Math" alttext="a" display="inline"><semantics id="Sx2.SS0.SSS0.Px1.p1.18.m11.1a"><mi id="Sx2.SS0.SSS0.Px1.p1.18.m11.1.1" xref="Sx2.SS0.SSS0.Px1.p1.18.m11.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="Sx2.SS0.SSS0.Px1.p1.18.m11.1b"><ci id="Sx2.SS0.SSS0.Px1.p1.18.m11.1.1.cmml" xref="Sx2.SS0.SSS0.Px1.p1.18.m11.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SS0.SSS0.Px1.p1.18.m11.1c">a</annotation></semantics></math> ( with a slash â€œ/" between them ). They finally feed this updated input directly into their task classifier, and show improved accuracy and robustness measures for QA and other tasks.</p>
</div>
</section>
<section id="Sx2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Learning Cross-Modality Encoder Representations from Transformers</h3>

<div id="Sx2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px2.p1.1" class="ltx_p">LXMERT <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal <a href="#bib.bib33" title="" class="ltx_ref">2019</a>)</cite> is a large-scale Transformer vision-language model consisting of three encoders: a visual object relationship encoder that leverages Fast R-CNN features, a language encoder (BERT base), and a cross-modality encoder that incorporates the prior visual and language encoder features. The vision and language encoders leverage many self attention layers while the final cross modality uses co-attention over both modalities. It is pre-trained with large amounts of image-and-sentence pairs from 5 vision-language datasets via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering.</p>
</div>
</section>
<section id="Sx2.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Bi-modal Generic Attention and VQA Explainations</h3>

<div id="Sx2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px3.p1.1" class="ltx_p">In a recent work<cite class="ltx_cite ltx_citemacro_citep">(Chefer, Gur, and Wolf <a href="#bib.bib2" title="" class="ltx_ref">2021a</a>)</cite> the authors propose BM-GAE, the first method to explain predictions by any Transformer-based architecture, particularly bi-modal Transformers that utilize co-attention like LXMERT, with word token feature importance and visual region saliency maps. They show BM-GAE to be superior to all existing methods which are adapted from single modality methods via perturbation tests; ie, they identify important text tokens/regions via BM-GAE on the inference set and show removing the most important of them and re-doing inference has the most negative impact on accuracy compared with other methods. The method uses the modelâ€™s attention layers to produce relevancy maps for each of the interactions between the input modalities in the network and is a generalization of TRF <cite class="ltx_cite ltx_citemacro_citep">(Chefer, Gur, and Wolf <a href="#bib.bib3" title="" class="ltx_ref">2021b</a>)</cite> without Layer-wise Relevance Propagation <cite class="ltx_cite ltx_citemacro_citep">(Bach etÂ al. <a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite> which itself was shown to be effective on single-modality Transformers that utilize self-attention such as VilBERT<cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="Sx2.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px3.p2.1" class="ltx_p">Additional VQA explainability methods which focus on models that generate rationales for predicted answers <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al. <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Wu, Chen, and Mooney <a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite> or as part of the inner process of a model <cite class="ltx_cite ltx_citemacro_citep">(NagarajÂ Rao etÂ al. <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite> are of interest and can provide faithful explanations at the expense of additional input data supervision. For this work we focus on methods which can be used with LXMERT as is without needing to re-train LXMERT with additional loss objectives or train auxiliary models to generate rationales that could be used in conjunction with LXMERT.</p>
</div>
</section>
<section id="Sx2.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Knowledge-Based VQA</h3>

<div id="Sx2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="Sx2.SS0.SSS0.Px4.p1.1" class="ltx_p">While there are quite a bit of methods for VQA and VQA requiring commonsense reasoning <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al. <a href="#bib.bib39" title="" class="ltx_ref">2020</a>; Gan etÂ al. <a href="#bib.bib5" title="" class="ltx_ref">2020</a>; Lu etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2019</a>; Su etÂ al. <a href="#bib.bib31" title="" class="ltx_ref">2020</a>; Messina etÂ al. <a href="#bib.bib16" title="" class="ltx_ref">2020</a>; Shi etÂ al. <a href="#bib.bib27" title="" class="ltx_ref">2020</a>; Gao etÂ al. <a href="#bib.bib6" title="" class="ltx_ref">2019</a>; Sampat, Yang, and Baral <a href="#bib.bib24" title="" class="ltx_ref">2020</a>)</cite>, there are fewer knowledge-based VQA ones <cite class="ltx_cite ltx_citemacro_citep">(Li, Wang, and Zhu <a href="#bib.bib11" title="" class="ltx_ref">2020</a>; Singh etÂ al. <a href="#bib.bib29" title="" class="ltx_ref">2019</a>; GardÃ¨res etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>; Ziaeefard and Lecue <a href="#bib.bib41" title="" class="ltx_ref">2020</a>; Song etÂ al. <a href="#bib.bib30" title="" class="ltx_ref">2020</a>; Shevchenko etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> and they follow a pattern of using visual, text and knowledge base embeddings separately or jointly for learning to produce answers. The two works most similar to ours either inject knowledge as a separate input into the cross attention module of LXMERT ( changing it to allow interaction with the text and a knowledge graph embedding input) <cite class="ltx_cite ltx_citemacro_citep">(GardÃ¨res etÂ al. <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite> or as an additional pre-training step that both identifies entity spans in a question and adds an additional objective pushing those knowledge graph representations towards their BERT entity representations <cite class="ltx_cite ltx_citemacro_citep">(Shevchenko etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>. Our work differs from these in that both of their solutions require pretraining of LXMERT whereas our proposal can be simply plugged in and used with LXMERTâ€™s existing pretrained weights and fine tuning for downstream tasks.</p>
</div>
<figure id="Sx2.F1" class="ltx_figure"><img src="/html/2112.06888/assets/x1.png" id="Sx2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="442" height="231" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>E-BERT knowledge injection into pre-trained LXMERT for knowledge aware VQA during fine-tuning. The example above is from the KVQA dataset where image captions are provided. The use of such optional captions is studied here.</figcaption>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Knowledge Injection via E-BERT for KBVQA</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.6" class="ltx_p">Figure <a href="#Sx2.F1" title="Figure 1 â€£ Knowledge-Based VQA â€£ Background and Related Work â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the manner in which we utilize E-BERT for knowledge injection into LXMERTâ€™s BERT model for knowledge aware VQA. We first need to learn to map the 5.8 million Wikipedia entity embedding matrix the authors provide<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://wikipedia2vec.github.io/wikipedia2vec/</span></span></span> to that of the pre-trained LXMERT BERT space. We utilize the code available in <cite class="ltx_cite ltx_citemacro_citep">(Poerner, Waltinger, and SchÃ¼tze <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> to learn that linear mapping <math id="Sx3.p1.1.m1.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}" display="inline"><semantics id="Sx3.p1.1.m1.1a"><msub id="Sx3.p1.1.m1.1.1" xref="Sx3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.p1.1.m1.1.1.2" xref="Sx3.p1.1.m1.1.1.2.cmml">â„°</mi><mrow id="Sx3.p1.1.m1.1.1.3" xref="Sx3.p1.1.m1.1.1.3.cmml"><mi mathvariant="normal" id="Sx3.p1.1.m1.1.1.3.2" xref="Sx3.p1.1.m1.1.1.3.2.cmml">E</mi><mo id="Sx3.p1.1.m1.1.1.3.1" xref="Sx3.p1.1.m1.1.1.3.1.cmml">âˆ’</mo><mi id="Sx3.p1.1.m1.1.1.3.3" xref="Sx3.p1.1.m1.1.1.3.3.cmml">BERT</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.p1.1.m1.1b"><apply id="Sx3.p1.1.m1.1.1.cmml" xref="Sx3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="Sx3.p1.1.m1.1.1.1.cmml" xref="Sx3.p1.1.m1.1.1">subscript</csymbol><ci id="Sx3.p1.1.m1.1.1.2.cmml" xref="Sx3.p1.1.m1.1.1.2">â„°</ci><apply id="Sx3.p1.1.m1.1.1.3.cmml" xref="Sx3.p1.1.m1.1.1.3"><minus id="Sx3.p1.1.m1.1.1.3.1.cmml" xref="Sx3.p1.1.m1.1.1.3.1"></minus><ci id="Sx3.p1.1.m1.1.1.3.2.cmml" xref="Sx3.p1.1.m1.1.1.3.2">E</ci><ci id="Sx3.p1.1.m1.1.1.3.3.cmml" xref="Sx3.p1.1.m1.1.1.3.3">BERT</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.1.m1.1c">{\mathcal{E}}_{\mathrm{E-BERT}}</annotation></semantics></math>.
We then adapt LXMERT <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://github.com/airsplay/lxmert</span></span></span> to add E-BERT representations for each entity span in a given input question sequence during the tokenization phase as follows: We first check if an entity span <math id="Sx3.p1.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="Sx3.p1.2.m2.1a"><mi id="Sx3.p1.2.m2.1.1" xref="Sx3.p1.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="Sx3.p1.2.m2.1b"><ci id="Sx3.p1.2.m2.1.1.cmml" xref="Sx3.p1.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.2.m2.1c">a</annotation></semantics></math> exists in the WikipediaVec matrix via a direct map lookup and if so, generate the entity wikipedia2vec embedding using <math id="Sx3.p1.3.m3.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}(a)" display="inline"><semantics id="Sx3.p1.3.m3.1a"><mrow id="Sx3.p1.3.m3.1.2" xref="Sx3.p1.3.m3.1.2.cmml"><msub id="Sx3.p1.3.m3.1.2.2" xref="Sx3.p1.3.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.p1.3.m3.1.2.2.2" xref="Sx3.p1.3.m3.1.2.2.2.cmml">â„°</mi><mrow id="Sx3.p1.3.m3.1.2.2.3" xref="Sx3.p1.3.m3.1.2.2.3.cmml"><mi mathvariant="normal" id="Sx3.p1.3.m3.1.2.2.3.2" xref="Sx3.p1.3.m3.1.2.2.3.2.cmml">E</mi><mo id="Sx3.p1.3.m3.1.2.2.3.1" xref="Sx3.p1.3.m3.1.2.2.3.1.cmml">âˆ’</mo><mi id="Sx3.p1.3.m3.1.2.2.3.3" xref="Sx3.p1.3.m3.1.2.2.3.3.cmml">BERT</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx3.p1.3.m3.1.2.1" xref="Sx3.p1.3.m3.1.2.1.cmml">â€‹</mo><mrow id="Sx3.p1.3.m3.1.2.3.2" xref="Sx3.p1.3.m3.1.2.cmml"><mo stretchy="false" id="Sx3.p1.3.m3.1.2.3.2.1" xref="Sx3.p1.3.m3.1.2.cmml">(</mo><mi id="Sx3.p1.3.m3.1.1" xref="Sx3.p1.3.m3.1.1.cmml">a</mi><mo stretchy="false" id="Sx3.p1.3.m3.1.2.3.2.2" xref="Sx3.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p1.3.m3.1b"><apply id="Sx3.p1.3.m3.1.2.cmml" xref="Sx3.p1.3.m3.1.2"><times id="Sx3.p1.3.m3.1.2.1.cmml" xref="Sx3.p1.3.m3.1.2.1"></times><apply id="Sx3.p1.3.m3.1.2.2.cmml" xref="Sx3.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="Sx3.p1.3.m3.1.2.2.1.cmml" xref="Sx3.p1.3.m3.1.2.2">subscript</csymbol><ci id="Sx3.p1.3.m3.1.2.2.2.cmml" xref="Sx3.p1.3.m3.1.2.2.2">â„°</ci><apply id="Sx3.p1.3.m3.1.2.2.3.cmml" xref="Sx3.p1.3.m3.1.2.2.3"><minus id="Sx3.p1.3.m3.1.2.2.3.1.cmml" xref="Sx3.p1.3.m3.1.2.2.3.1"></minus><ci id="Sx3.p1.3.m3.1.2.2.3.2.cmml" xref="Sx3.p1.3.m3.1.2.2.3.2">E</ci><ci id="Sx3.p1.3.m3.1.2.2.3.3.cmml" xref="Sx3.p1.3.m3.1.2.2.3.3">BERT</ci></apply></apply><ci id="Sx3.p1.3.m3.1.1.cmml" xref="Sx3.p1.3.m3.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.3.m3.1c">{\mathcal{E}}_{\mathrm{E-BERT}}(a)</annotation></semantics></math> to map that to the LXMERT BERT space. We finally append a BERT slash embedding followed by the BERT embedding of the entity <math id="Sx3.p1.4.m4.1" class="ltx_Math" alttext="a" display="inline"><semantics id="Sx3.p1.4.m4.1a"><mi id="Sx3.p1.4.m4.1.1" xref="Sx3.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="Sx3.p1.4.m4.1b"><ci id="Sx3.p1.4.m4.1.1.cmml" xref="Sx3.p1.4.m4.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.4.m4.1c">a</annotation></semantics></math> to <math id="Sx3.p1.5.m5.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}(a)" display="inline"><semantics id="Sx3.p1.5.m5.1a"><mrow id="Sx3.p1.5.m5.1.2" xref="Sx3.p1.5.m5.1.2.cmml"><msub id="Sx3.p1.5.m5.1.2.2" xref="Sx3.p1.5.m5.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.p1.5.m5.1.2.2.2" xref="Sx3.p1.5.m5.1.2.2.2.cmml">â„°</mi><mrow id="Sx3.p1.5.m5.1.2.2.3" xref="Sx3.p1.5.m5.1.2.2.3.cmml"><mi mathvariant="normal" id="Sx3.p1.5.m5.1.2.2.3.2" xref="Sx3.p1.5.m5.1.2.2.3.2.cmml">E</mi><mo id="Sx3.p1.5.m5.1.2.2.3.1" xref="Sx3.p1.5.m5.1.2.2.3.1.cmml">âˆ’</mo><mi id="Sx3.p1.5.m5.1.2.2.3.3" xref="Sx3.p1.5.m5.1.2.2.3.3.cmml">BERT</mi></mrow></msub><mo lspace="0em" rspace="0em" id="Sx3.p1.5.m5.1.2.1" xref="Sx3.p1.5.m5.1.2.1.cmml">â€‹</mo><mrow id="Sx3.p1.5.m5.1.2.3.2" xref="Sx3.p1.5.m5.1.2.cmml"><mo stretchy="false" id="Sx3.p1.5.m5.1.2.3.2.1" xref="Sx3.p1.5.m5.1.2.cmml">(</mo><mi id="Sx3.p1.5.m5.1.1" xref="Sx3.p1.5.m5.1.1.cmml">a</mi><mo stretchy="false" id="Sx3.p1.5.m5.1.2.3.2.2" xref="Sx3.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Sx3.p1.5.m5.1b"><apply id="Sx3.p1.5.m5.1.2.cmml" xref="Sx3.p1.5.m5.1.2"><times id="Sx3.p1.5.m5.1.2.1.cmml" xref="Sx3.p1.5.m5.1.2.1"></times><apply id="Sx3.p1.5.m5.1.2.2.cmml" xref="Sx3.p1.5.m5.1.2.2"><csymbol cd="ambiguous" id="Sx3.p1.5.m5.1.2.2.1.cmml" xref="Sx3.p1.5.m5.1.2.2">subscript</csymbol><ci id="Sx3.p1.5.m5.1.2.2.2.cmml" xref="Sx3.p1.5.m5.1.2.2.2">â„°</ci><apply id="Sx3.p1.5.m5.1.2.2.3.cmml" xref="Sx3.p1.5.m5.1.2.2.3"><minus id="Sx3.p1.5.m5.1.2.2.3.1.cmml" xref="Sx3.p1.5.m5.1.2.2.3.1"></minus><ci id="Sx3.p1.5.m5.1.2.2.3.2.cmml" xref="Sx3.p1.5.m5.1.2.2.3.2">E</ci><ci id="Sx3.p1.5.m5.1.2.2.3.3.cmml" xref="Sx3.p1.5.m5.1.2.2.3.3">BERT</ci></apply></apply><ci id="Sx3.p1.5.m5.1.1.cmml" xref="Sx3.p1.5.m5.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.5.m5.1c">{\mathcal{E}}_{\mathrm{E-BERT}}(a)</annotation></semantics></math>. As an example if â€œBarack Obama" is an entity that is found in the WikipediaVec matrix, the output for it while creating embeddings becomes <math id="Sx3.p1.6.m6.1" class="ltx_Math" alttext="{\mathcal{E}}_{\mathrm{E-BERT}}" display="inline"><semantics id="Sx3.p1.6.m6.1a"><msub id="Sx3.p1.6.m6.1.1" xref="Sx3.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="Sx3.p1.6.m6.1.1.2" xref="Sx3.p1.6.m6.1.1.2.cmml">â„°</mi><mrow id="Sx3.p1.6.m6.1.1.3" xref="Sx3.p1.6.m6.1.1.3.cmml"><mi mathvariant="normal" id="Sx3.p1.6.m6.1.1.3.2" xref="Sx3.p1.6.m6.1.1.3.2.cmml">E</mi><mo id="Sx3.p1.6.m6.1.1.3.1" xref="Sx3.p1.6.m6.1.1.3.1.cmml">âˆ’</mo><mi id="Sx3.p1.6.m6.1.1.3.3" xref="Sx3.p1.6.m6.1.1.3.3.cmml">BERT</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="Sx3.p1.6.m6.1b"><apply id="Sx3.p1.6.m6.1.1.cmml" xref="Sx3.p1.6.m6.1.1"><csymbol cd="ambiguous" id="Sx3.p1.6.m6.1.1.1.cmml" xref="Sx3.p1.6.m6.1.1">subscript</csymbol><ci id="Sx3.p1.6.m6.1.1.2.cmml" xref="Sx3.p1.6.m6.1.1.2">â„°</ci><apply id="Sx3.p1.6.m6.1.1.3.cmml" xref="Sx3.p1.6.m6.1.1.3"><minus id="Sx3.p1.6.m6.1.1.3.1.cmml" xref="Sx3.p1.6.m6.1.1.3.1"></minus><ci id="Sx3.p1.6.m6.1.1.3.2.cmml" xref="Sx3.p1.6.m6.1.1.3.2">E</ci><ci id="Sx3.p1.6.m6.1.1.3.3.cmml" xref="Sx3.p1.6.m6.1.1.3.3">BERT</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx3.p1.6.m6.1c">{\mathcal{E}}_{\mathrm{E-BERT}}</annotation></semantics></math>(â€œBarack Obama")) + BERT(â€œ/") + BERT(â€œBarack Obama"). As WikipediaVec was trained using a cased tokenizer and LXMERT BERT is uncased by default, we need to titlecase the entity input string to WikipediaVec before hand as well.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">In the case that entity sets are not explicitly given with a downstream knowledge aware VQA task, we propose and study the important effect different methods for obtaining them affect downstream task performance. In the following section we discuss the datasets we use for our experiments and the methods we utilize to extract entity spans.</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Experiments</h2>

<section id="Sx4.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">KVQA dataset</h3>

<div id="Sx4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px1.p1.1" class="ltx_p">The KVQA dataset <cite class="ltx_cite ltx_citemacro_citep">(SanketÂ Shah and Talukdar <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> contains 24K images with text captions, 183K image/question QA pairs over 5 splits of the data ( median 7 questions per image), and associated metadata for the 18.8K unique Wikipedia entities displayed in those images (QID and Wikipage title). The dataset has a large amount of QA pairs compared with other VQA datasets and its explicit Wiki supervision is unique. Of the 18.8K entities that exist in KVQA many are rare; only 65% of the entities exist in the top 1 million most occurring entities in Wikipedia and only 91% of them were found in WikipediaVecâ€™s entity matrix.</p>
</div>
<div id="Sx4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px1.p2.1" class="ltx_p">The baseline model proposed in the KVQA dataset paper is composed of both visual and text based entity linking methods to Wikidata from which entity facts over 18 pre-determined relationship types are extracted. These facts are then encoded along with spatial coordinates and text questions via memNet or Bi-LSTM and fed into a multi-layer perceptron followed by a softmax classifier to predict the final answer from among all 20k possible answers.</p>
</div>
<div id="Sx4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px1.p3.1" class="ltx_p">We note that around 25% of the images in the main dataset, 6K out of 24K, are found directly in the 69K image reference dataset for face identification making visual entity linking relatively simple and unrealistic. Additionally the model only provides results for a closed world experiment where only 18 types of relations are considered from 18.8k Wiki entity pages. In contrast our method does not rely on a reference set of images for entity identification nor a subset of relations for use as answer possibilities.</p>
</div>
</section>
<section id="Sx4.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">KVQA entity span construction</h3>

<div id="Sx4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px2.p1.1" class="ltx_p">KVQA does not provide gold entity spans and we examine three methods for generating them, the first two which involve using the SpaCY named entity recognizer <span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We use â€œweb-smâ€ from http://spacy.io</span></span></span>. In total, we have 5 different variants of the data we try in our experiments as input where the first two â€œQuestion" and â€œ+ Caption" are baselines which do not utilize knowledge injection.</p>
<ol id="Sx4.I1" class="ltx_enumerate">
<li id="Sx4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx4.I1.i1.p1" class="ltx_para">
<p id="Sx4.I1.i1.p1.1" class="ltx_p">â€œQuestion" - only the question is used</p>
</div>
</li>
<li id="Sx4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx4.I1.i2.p1" class="ltx_para">
<p id="Sx4.I1.i2.p1.1" class="ltx_p">â€œ+ Caption", the question with the image caption is used</p>
</div>
</li>
<li id="Sx4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx4.I1.i3.p1" class="ltx_para">
<p id="Sx4.I1.i3.p1.1" class="ltx_p">â€œNERper" uses SpaCY to first identify named entity spans and then filters to only include those whose labels are associated with people.</p>
</div>
</li>
<li id="Sx4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="Sx4.I1.i4.p1" class="ltx_para">
<p id="Sx4.I1.i4.p1.1" class="ltx_p">â€œNERagro" uses SpaCY with an additional noun phrase chunking mechanism and no filtering to aggressively catch more spans.</p>
</div>
</li>
<li id="Sx4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="Sx4.I1.i5.p1" class="ltx_para">
<p id="Sx4.I1.i5.p1.1" class="ltx_p">â€œKVQAmeta" uses the wiki entity names provided in the metadata of each question and uses string matching to search for them over the question. In cases where entities are not present in the question, we prepend them to the caption.</p>
</div>
</li>
</ol>
<p id="Sx4.SS0.SSS0.Px2.p1.2" class="ltx_p">The KVQAmeta procedure allows for the possibility of knowledge injection of all entities present in a question, while NERper and NERagro allow for additional entities that might be present in the question and image caption to be introduced. In all three cases the image caption is concatenated to the end of the question prior to running the entity span detection methods. For these three sets, we also try three methods to improve entity linking.</p>
<ol id="Sx4.I2" class="ltx_enumerate">
<li id="Sx4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx4.I2.i1.p1" class="ltx_para">
<p id="Sx4.I2.i1.p1.1" class="ltx_p">â€œas is" which utilize the entity sets as is,</p>
</div>
</li>
<li id="Sx4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx4.I2.i2.p1" class="ltx_para">
<p id="Sx4.I2.i2.p1.1" class="ltx_p">â€œlinks" which filters the entity sets to only include those with verified links to wikipedia and</p>
</div>
</li>
<li id="Sx4.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx4.I2.i3.p1" class="ltx_para">
<p id="Sx4.I2.i3.p1.1" class="ltx_p">â€œnoisy" which leverages a Wikipedia API<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/goldsmith/Wikipedia</span></span></span> to search for the most likely wikipedia link for spans missing them.</p>
</div>
</li>
</ol>
<p id="Sx4.SS0.SSS0.Px2.p1.3" class="ltx_p">Table <a href="#Sx4.T1" title="Table 1 â€£ KVQA entity span construction â€£ Experiments â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the entity spans per question, E-BERT injected entities per question (since only those with values in WikipediaVec can be mapped) and the percent of questions with E-BERT injections for split 1. We note that KVQAmeta has the least E-BERT injected entities per question and the highest percent of questions with E-BERT injected entities ( 99% for KVQAmeta "noisy" for instance), while NERagro has the most E-BERT injected entities per question and NERper has the lowest percent per question.</p>
</div>
<figure id="Sx4.T1" class="ltx_table">
<table id="Sx4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T1.1.1.1" class="ltx_tr">
<th id="Sx4.T1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="Sx4.T1.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T1.1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">ents</td>
<td id="Sx4.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">eberts</td>
<td id="Sx4.T1.1.1.1.6" class="ltx_td ltx_align_center ltx_border_tt">Qs w/</td>
</tr>
<tr id="Sx4.T1.1.2.2" class="ltx_tr">
<th id="Sx4.T1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<td id="Sx4.T1.1.2.2.2" class="ltx_td ltx_align_center">Type</td>
<td id="Sx4.T1.1.2.2.3" class="ltx_td ltx_align_center">Acc</td>
<td id="Sx4.T1.1.2.2.4" class="ltx_td ltx_align_center">per Q</td>
<td id="Sx4.T1.1.2.2.5" class="ltx_td ltx_align_center">per Q</td>
<td id="Sx4.T1.1.2.2.6" class="ltx_td ltx_align_center">eberts</td>
</tr>
<tr id="Sx4.T1.1.3.3" class="ltx_tr">
<th id="Sx4.T1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Shah 2019</th>
<td id="Sx4.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">49.50</td>
<td id="Sx4.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx4.T1.1.4.4" class="ltx_tr">
<th id="Sx4.T1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ Caption</th>
<td id="Sx4.T1.1.4.4.2" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.4.4.3" class="ltx_td ltx_align_center">50.20</td>
<td id="Sx4.T1.1.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.4.4.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.4.4.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx4.T1.1.5.5" class="ltx_tr">
<th id="Sx4.T1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Question</th>
<td id="Sx4.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">47.54</td>
<td id="Sx4.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx4.T1.1.6.6" class="ltx_tr">
<th id="Sx4.T1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ Caption</th>
<td id="Sx4.T1.1.6.6.2" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.6.6.3" class="ltx_td ltx_align_center">50.25</td>
<td id="Sx4.T1.1.6.6.4" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.6.6.5" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T1.1.6.6.6" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx4.T1.1.7.7" class="ltx_tr">
<th id="Sx4.T1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERper</th>
<td id="Sx4.T1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_t">as is</td>
<td id="Sx4.T1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_t">50.37</td>
<td id="Sx4.T1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_t">2.5</td>
<td id="Sx4.T1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_t">1.5</td>
<td id="Sx4.T1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">.78</td>
</tr>
<tr id="Sx4.T1.1.8.8" class="ltx_tr">
<th id="Sx4.T1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<td id="Sx4.T1.1.8.8.2" class="ltx_td ltx_align_center">links</td>
<td id="Sx4.T1.1.8.8.3" class="ltx_td ltx_align_center">50.42</td>
<td id="Sx4.T1.1.8.8.4" class="ltx_td ltx_align_center">1.8</td>
<td id="Sx4.T1.1.8.8.5" class="ltx_td ltx_align_center">1.5</td>
<td id="Sx4.T1.1.8.8.6" class="ltx_td ltx_align_center">.79</td>
</tr>
<tr id="Sx4.T1.1.9.9" class="ltx_tr">
<th id="Sx4.T1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<td id="Sx4.T1.1.9.9.2" class="ltx_td ltx_align_center">noisy</td>
<td id="Sx4.T1.1.9.9.3" class="ltx_td ltx_align_center">50.69</td>
<td id="Sx4.T1.1.9.9.4" class="ltx_td ltx_align_center">2.5</td>
<td id="Sx4.T1.1.9.9.5" class="ltx_td ltx_align_center">2.3</td>
<td id="Sx4.T1.1.9.9.6" class="ltx_td ltx_align_center">.94</td>
</tr>
<tr id="Sx4.T1.1.10.10" class="ltx_tr">
<th id="Sx4.T1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERagro</th>
<td id="Sx4.T1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_t">as is</td>
<td id="Sx4.T1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_t">50.26</td>
<td id="Sx4.T1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx4.T1.1.10.10.4.1" class="ltx_text ltx_font_bold">4.0</span></td>
<td id="Sx4.T1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_t">2.6</td>
<td id="Sx4.T1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_t">.91</td>
</tr>
<tr id="Sx4.T1.1.11.11" class="ltx_tr">
<th id="Sx4.T1.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<td id="Sx4.T1.1.11.11.2" class="ltx_td ltx_align_center">links</td>
<td id="Sx4.T1.1.11.11.3" class="ltx_td ltx_align_center">50.33</td>
<td id="Sx4.T1.1.11.11.4" class="ltx_td ltx_align_center">2.2</td>
<td id="Sx4.T1.1.11.11.5" class="ltx_td ltx_align_center">2.2</td>
<td id="Sx4.T1.1.11.11.6" class="ltx_td ltx_align_center">.97</td>
</tr>
<tr id="Sx4.T1.1.12.12" class="ltx_tr">
<th id="Sx4.T1.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<td id="Sx4.T1.1.12.12.2" class="ltx_td ltx_align_center">noisy</td>
<td id="Sx4.T1.1.12.12.3" class="ltx_td ltx_align_center">50.77</td>
<td id="Sx4.T1.1.12.12.4" class="ltx_td ltx_align_center">3.3</td>
<td id="Sx4.T1.1.12.12.5" class="ltx_td ltx_align_center"><span id="Sx4.T1.1.12.12.5.1" class="ltx_text ltx_font_bold">3.2</span></td>
<td id="Sx4.T1.1.12.12.6" class="ltx_td ltx_align_center">.97</td>
</tr>
<tr id="Sx4.T1.1.13.13" class="ltx_tr">
<th id="Sx4.T1.1.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">KVQAmeta</th>
<td id="Sx4.T1.1.13.13.2" class="ltx_td ltx_align_center ltx_border_t">as is</td>
<td id="Sx4.T1.1.13.13.3" class="ltx_td ltx_align_center ltx_border_t">52.65</td>
<td id="Sx4.T1.1.13.13.4" class="ltx_td ltx_align_center ltx_border_t">1.4</td>
<td id="Sx4.T1.1.13.13.5" class="ltx_td ltx_align_center ltx_border_t">1.2</td>
<td id="Sx4.T1.1.13.13.6" class="ltx_td ltx_align_center ltx_border_t">.87</td>
</tr>
<tr id="Sx4.T1.1.14.14" class="ltx_tr">
<th id="Sx4.T1.1.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KVQAmeta</th>
<td id="Sx4.T1.1.14.14.2" class="ltx_td ltx_align_center">links</td>
<td id="Sx4.T1.1.14.14.3" class="ltx_td ltx_align_center">52.68</td>
<td id="Sx4.T1.1.14.14.4" class="ltx_td ltx_align_center">1.4</td>
<td id="Sx4.T1.1.14.14.5" class="ltx_td ltx_align_center">1.3</td>
<td id="Sx4.T1.1.14.14.6" class="ltx_td ltx_align_center">.95</td>
</tr>
<tr id="Sx4.T1.1.15.15" class="ltx_tr">
<th id="Sx4.T1.1.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">KVQAmeta</th>
<td id="Sx4.T1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_bb">noisy</td>
<td id="Sx4.T1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx4.T1.1.15.15.3.1" class="ltx_text ltx_font_bold">52.83</span></td>
<td id="Sx4.T1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_bb">1.4</td>
<td id="Sx4.T1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_bb">1.4</td>
<td id="Sx4.T1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx4.T1.1.15.15.6.1" class="ltx_text ltx_font_bold">.99</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>KVQA overall accuracy results over 5 splits and entity spans per question (ents per Q), E-BERT representations injected per question (eberts per Q) and the percent of questions with E-BERT injections (Qs w/ eberts) for split 1</figcaption>
</figure>
</section>
<section id="Sx4.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">OKVQA dataset</h3>

<div id="Sx4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px3.p1.1" class="ltx_p">The OKVQA dataset <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> from AllenNLP contains 14k image/question pairs which is less than KVQA and is less entity based since its objective is to test commonsense reasoning. It does however provided around 10 answers per question which is more robust from an answer set perspective than KVQA which only includes one labelerâ€™s answer per question since slight variations of a labelerâ€™s answer would cause incorrect answers.</p>
</div>
</section>
<section id="Sx4.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">OKVQA entity span construction</h3>

<div id="Sx4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="Sx4.SS0.SSS0.Px4.p1.1" class="ltx_p">As OKVQA does not provide entity spans, we again leverage SpaCY to get 3 different entity span set variants. These three sets represent progressively less noisy versions of entity sets obtained which we hypothesized would be beneficial given OKVQA is less entity-centric overall compared with KVQA.</p>
<ol id="Sx4.I3" class="ltx_enumerate">
<li id="Sx4.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="Sx4.I3.i1.p1" class="ltx_para">
<p id="Sx4.I3.i1.p1.1" class="ltx_p">â€œ13K" we use SpaCY with no filtering to obtain entity spans for 13K QA pairs (92.8% of questions).</p>
</div>
</li>
<li id="Sx4.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="Sx4.I3.i2.p1" class="ltx_para">
<p id="Sx4.I3.i2.p1.1" class="ltx_p">â€œ4K" we use a semi-automated rules based technique to identify poor candidate spans (ie too general, etc) which filters the set to â€œ4K" (28.6% of questions).</p>
</div>
</li>
<li id="Sx4.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="Sx4.I3.i3.p1" class="ltx_para">
<p id="Sx4.I3.i3.p1.1" class="ltx_p">â€œ2.5K" we did manual filtering over unique entity spans to filter it down to â€œ2.5K" (17.8% of questions).</p>
</div>
</li>
</ol>
</div>
<figure id="Sx4.T2" class="ltx_table">
<table id="Sx4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T2.1.1.1" class="ltx_tr">
<th id="Sx4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt">Model</th>
<td id="Sx4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Mean</td>
<td id="Sx4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Std</td>
<td id="Sx4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt">Max</td>
<td id="Sx4.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">Median</td>
</tr>
<tr id="Sx4.T2.1.2.2" class="ltx_tr">
<th id="Sx4.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">OKVQA best</th>
<td id="Sx4.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">27.84</td>
<td id="Sx4.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="Sx4.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx4.T2.1.3.3" class="ltx_tr">
<th id="Sx4.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Shevchenko 21</th>
<td id="Sx4.T2.1.3.3.2" class="ltx_td ltx_align_center">39.04</td>
<td id="Sx4.T2.1.3.3.3" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.3.3.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx4.T2.1.4.4" class="ltx_tr">
<th id="Sx4.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Wu et al 21</th>
<td id="Sx4.T2.1.4.4.2" class="ltx_td ltx_align_center">40.50</td>
<td id="Sx4.T2.1.4.4.3" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.4.4.4" class="ltx_td ltx_align_center">-</td>
<td id="Sx4.T2.1.4.4.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="Sx4.T2.1.5.5" class="ltx_tr">
<th id="Sx4.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">LXMERT Plain</th>
<td id="Sx4.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t">43.51</td>
<td id="Sx4.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td id="Sx4.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">43.87</td>
<td id="Sx4.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">43.34</td>
</tr>
<tr id="Sx4.T2.1.6.6" class="ltx_tr">
<th id="Sx4.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ EBERT 13K</th>
<td id="Sx4.T2.1.6.6.2" class="ltx_td ltx_align_center">40.59</td>
<td id="Sx4.T2.1.6.6.3" class="ltx_td ltx_align_center">0.09</td>
<td id="Sx4.T2.1.6.6.4" class="ltx_td ltx_align_center">40.69</td>
<td id="Sx4.T2.1.6.6.5" class="ltx_td ltx_align_center">40.59</td>
</tr>
<tr id="Sx4.T2.1.7.7" class="ltx_tr">
<th id="Sx4.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">+ EBERT 4K</th>
<td id="Sx4.T2.1.7.7.2" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.7.7.2.1" class="ltx_text ltx_font_bold">43.67</span></td>
<td id="Sx4.T2.1.7.7.3" class="ltx_td ltx_align_center">0.13</td>
<td id="Sx4.T2.1.7.7.4" class="ltx_td ltx_align_center">43.88</td>
<td id="Sx4.T2.1.7.7.5" class="ltx_td ltx_align_center"><span id="Sx4.T2.1.7.7.5.1" class="ltx_text ltx_font_bold">43.66</span></td>
</tr>
<tr id="Sx4.T2.1.8.8" class="ltx_tr">
<th id="Sx4.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">+ EBERT 2.5K</th>
<td id="Sx4.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_bb">43.61</td>
<td id="Sx4.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_bb">0.36</td>
<td id="Sx4.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx4.T2.1.8.8.4.1" class="ltx_text ltx_font_bold">44.10</span></td>
<td id="Sx4.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_bb">43.34</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>OKVQA model results over 5 runs</figcaption>
</figure>
<figure id="Sx4.T3" class="ltx_table">
<table id="Sx4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx4.T3.1.1.1" class="ltx_tr">
<td id="Sx4.T3.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.2" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.3" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">multi</td>
<td id="Sx4.T3.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">multi</td>
<td id="Sx4.T3.1.1.1.6" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.7" class="ltx_td ltx_align_right ltx_border_tt">multi</td>
<td id="Sx4.T3.1.1.1.8" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.9" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.10" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.11" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.12" class="ltx_td ltx_border_tt"></td>
<td id="Sx4.T3.1.1.1.13" class="ltx_td ltx_align_center ltx_border_tt">Acc /</td>
</tr>
<tr id="Sx4.T3.1.2.2" class="ltx_tr">
<td id="Sx4.T3.1.2.2.1" class="ltx_td ltx_align_left">Model</td>
<td id="Sx4.T3.1.2.2.2" class="ltx_td ltx_align_left">Type</td>
<td id="Sx4.T3.1.2.2.3" class="ltx_td ltx_align_right">1-hop</td>
<td id="Sx4.T3.1.2.2.4" class="ltx_td ltx_align_right">hop</td>
<td id="Sx4.T3.1.2.2.5" class="ltx_td ltx_align_right">rel</td>
<td id="Sx4.T3.1.2.2.6" class="ltx_td ltx_align_right">bool</td>
<td id="Sx4.T3.1.2.2.7" class="ltx_td ltx_align_right">entity</td>
<td id="Sx4.T3.1.2.2.8" class="ltx_td ltx_align_right">cmp</td>
<td id="Sx4.T3.1.2.2.9" class="ltx_td ltx_align_right">spatial</td>
<td id="Sx4.T3.1.2.2.10" class="ltx_td ltx_align_right">subtr</td>
<td id="Sx4.T3.1.2.2.11" class="ltx_td ltx_align_right">count</td>
<td id="Sx4.T3.1.2.2.12" class="ltx_td ltx_align_right">inter</td>
<td id="Sx4.T3.1.2.2.13" class="ltx_td ltx_align_center">Conf</td>
</tr>
<tr id="Sx4.T3.1.3.3" class="ltx_tr">
<td id="Sx4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Percent</td>
<td id="Sx4.T3.1.3.3.2" class="ltx_td ltx_align_left ltx_border_t">with</td>
<td id="Sx4.T3.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t">81.80</td>
<td id="Sx4.T3.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">18.20</td>
<td id="Sx4.T3.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">53.58</td>
<td id="Sx4.T3.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">24.63</td>
<td id="Sx4.T3.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">24.96</td>
<td id="Sx4.T3.1.3.3.8" class="ltx_td ltx_align_right ltx_border_t">16.81</td>
<td id="Sx4.T3.1.3.3.9" class="ltx_td ltx_align_right ltx_border_t">15.22</td>
<td id="Sx4.T3.1.3.3.10" class="ltx_td ltx_align_right ltx_border_t">12.07</td>
<td id="Sx4.T3.1.3.3.11" class="ltx_td ltx_align_right ltx_border_t">7.89</td>
<td id="Sx4.T3.1.3.3.12" class="ltx_td ltx_align_right ltx_border_t">1.82</td>
<td id="Sx4.T3.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="Sx4.T3.1.4.4" class="ltx_tr">
<td id="Sx4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">Question</td>
<td id="Sx4.T3.1.4.4.2" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="Sx4.T3.1.4.4.3" class="ltx_td ltx_align_right ltx_border_t">44.89</td>
<td id="Sx4.T3.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t">57.98</td>
<td id="Sx4.T3.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t">47.40</td>
<td id="Sx4.T3.1.4.4.6" class="ltx_td ltx_align_right ltx_border_t">86.37</td>
<td id="Sx4.T3.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t">72.14</td>
<td id="Sx4.T3.1.4.4.8" class="ltx_td ltx_align_right ltx_border_t">81.67</td>
<td id="Sx4.T3.1.4.4.9" class="ltx_td ltx_align_right ltx_border_t">28.12</td>
<td id="Sx4.T3.1.4.4.10" class="ltx_td ltx_align_right ltx_border_t">19.68</td>
<td id="Sx4.T3.1.4.4.11" class="ltx_td ltx_align_right ltx_border_t">84.62</td>
<td id="Sx4.T3.1.4.4.12" class="ltx_td ltx_align_right ltx_border_t">65.00</td>
<td id="Sx4.T3.1.4.4.13" class="ltx_td ltx_align_center ltx_border_t">47.27</td>
</tr>
<tr id="Sx4.T3.1.5.5" class="ltx_tr">
<td id="Sx4.T3.1.5.5.1" class="ltx_td ltx_align_left">+ Caption</td>
<td id="Sx4.T3.1.5.5.2" class="ltx_td ltx_align_left">-</td>
<td id="Sx4.T3.1.5.5.3" class="ltx_td ltx_align_right">46.36</td>
<td id="Sx4.T3.1.5.5.4" class="ltx_td ltx_align_right">65.47</td>
<td id="Sx4.T3.1.5.5.5" class="ltx_td ltx_align_right">51.57</td>
<td id="Sx4.T3.1.5.5.6" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.5.5.6.1" class="ltx_text ltx_font_bold">87.21</span></td>
<td id="Sx4.T3.1.5.5.7" class="ltx_td ltx_align_right">72.46</td>
<td id="Sx4.T3.1.5.5.8" class="ltx_td ltx_align_right">80.91</td>
<td id="Sx4.T3.1.5.5.9" class="ltx_td ltx_align_right">29.17</td>
<td id="Sx4.T3.1.5.5.10" class="ltx_td ltx_align_right">19.33</td>
<td id="Sx4.T3.1.5.5.11" class="ltx_td ltx_align_right">85.03</td>
<td id="Sx4.T3.1.5.5.12" class="ltx_td ltx_align_right">70.29</td>
<td id="Sx4.T3.1.5.5.13" class="ltx_td ltx_align_center">49.84</td>
</tr>
<tr id="Sx4.T3.1.6.6" class="ltx_tr">
<td id="Sx4.T3.1.6.6.1" class="ltx_td ltx_align_left">KVQAmeta</td>
<td id="Sx4.T3.1.6.6.2" class="ltx_td ltx_align_left">links</td>
<td id="Sx4.T3.1.6.6.3" class="ltx_td ltx_align_right">48.87</td>
<td id="Sx4.T3.1.6.6.4" class="ltx_td ltx_align_right">70.61</td>
<td id="Sx4.T3.1.6.6.5" class="ltx_td ltx_align_right">55.43</td>
<td id="Sx4.T3.1.6.6.6" class="ltx_td ltx_align_right">86.69</td>
<td id="Sx4.T3.1.6.6.7" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.6.6.7.1" class="ltx_text ltx_font_bold">73.68</span></td>
<td id="Sx4.T3.1.6.6.8" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.6.6.8.1" class="ltx_text ltx_font_bold">82.50</span></td>
<td id="Sx4.T3.1.6.6.9" class="ltx_td ltx_align_right">31.14</td>
<td id="Sx4.T3.1.6.6.10" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.6.6.10.1" class="ltx_text ltx_font_bold">22.21</span></td>
<td id="Sx4.T3.1.6.6.11" class="ltx_td ltx_align_right">84.82</td>
<td id="Sx4.T3.1.6.6.12" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.6.6.12.1" class="ltx_text ltx_font_bold">71.47</span></td>
<td id="Sx4.T3.1.6.6.13" class="ltx_td ltx_align_center">52.83</td>
</tr>
<tr id="Sx4.T3.1.7.7" class="ltx_tr">
<td id="Sx4.T3.1.7.7.1" class="ltx_td ltx_align_left">KVQAmeta</td>
<td id="Sx4.T3.1.7.7.2" class="ltx_td ltx_align_left">noisy</td>
<td id="Sx4.T3.1.7.7.3" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.7.7.3.1" class="ltx_text ltx_font_bold">48.88</span></td>
<td id="Sx4.T3.1.7.7.4" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.7.7.4.1" class="ltx_text ltx_font_bold">71.55</span></td>
<td id="Sx4.T3.1.7.7.5" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.7.7.5.1" class="ltx_text ltx_font_bold">56.14</span></td>
<td id="Sx4.T3.1.7.7.6" class="ltx_td ltx_align_right">86.63</td>
<td id="Sx4.T3.1.7.7.7" class="ltx_td ltx_align_right">73.57</td>
<td id="Sx4.T3.1.7.7.8" class="ltx_td ltx_align_right">82.15</td>
<td id="Sx4.T3.1.7.7.9" class="ltx_td ltx_align_right">31.14</td>
<td id="Sx4.T3.1.7.7.10" class="ltx_td ltx_align_right">21.23</td>
<td id="Sx4.T3.1.7.7.11" class="ltx_td ltx_align_right"><span id="Sx4.T3.1.7.7.11.1" class="ltx_text ltx_font_bold">85.70</span></td>
<td id="Sx4.T3.1.7.7.12" class="ltx_td ltx_align_right">70.00</td>
<td id="Sx4.T3.1.7.7.13" class="ltx_td ltx_align_center"><span id="Sx4.T3.1.7.7.13.1" class="ltx_text ltx_font_bold">53.01</span></td>
</tr>
<tr id="Sx4.T3.1.8.8" class="ltx_tr">
<td id="Sx4.T3.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t">Average</td>
<td id="Sx4.T3.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t">E-BERT</td>
<td id="Sx4.T3.1.8.8.3" class="ltx_td ltx_align_right ltx_border_t">47.38</td>
<td id="Sx4.T3.1.8.8.4" class="ltx_td ltx_align_right ltx_border_t">67.48</td>
<td id="Sx4.T3.1.8.8.5" class="ltx_td ltx_align_right ltx_border_t">53.04</td>
<td id="Sx4.T3.1.8.8.6" class="ltx_td ltx_align_right ltx_border_t">86.24</td>
<td id="Sx4.T3.1.8.8.7" class="ltx_td ltx_align_right ltx_border_t">72.98</td>
<td id="Sx4.T3.1.8.8.8" class="ltx_td ltx_align_right ltx_border_t">81.85</td>
<td id="Sx4.T3.1.8.8.9" class="ltx_td ltx_align_right ltx_border_t">30.48</td>
<td id="Sx4.T3.1.8.8.10" class="ltx_td ltx_align_right ltx_border_t">20.58</td>
<td id="Sx4.T3.1.8.8.11" class="ltx_td ltx_align_right ltx_border_t">85.15</td>
<td id="Sx4.T3.1.8.8.12" class="ltx_td ltx_align_right ltx_border_t">68.46</td>
<td id="Sx4.T3.1.8.8.13" class="ltx_td ltx_align_center ltx_border_t">51.04</td>
</tr>
<tr id="Sx4.T3.1.9.9" class="ltx_tr">
<td id="Sx4.T3.1.9.9.1" class="ltx_td ltx_align_left">Best E-BERT</td>
<td id="Sx4.T3.1.9.9.2" class="ltx_td ltx_align_left">- Caption</td>
<td id="Sx4.T3.1.9.9.3" class="ltx_td ltx_align_right">2.52</td>
<td id="Sx4.T3.1.9.9.4" class="ltx_td ltx_align_right">6.08</td>
<td id="Sx4.T3.1.9.9.5" class="ltx_td ltx_align_right">4.57</td>
<td id="Sx4.T3.1.9.9.6" class="ltx_td ltx_align_right">-0.13</td>
<td id="Sx4.T3.1.9.9.7" class="ltx_td ltx_align_right">1.22</td>
<td id="Sx4.T3.1.9.9.8" class="ltx_td ltx_align_right">1.59</td>
<td id="Sx4.T3.1.9.9.9" class="ltx_td ltx_align_right">2.25</td>
<td id="Sx4.T3.1.9.9.10" class="ltx_td ltx_align_right">2.88</td>
<td id="Sx4.T3.1.9.9.11" class="ltx_td ltx_align_right">0.67</td>
<td id="Sx4.T3.1.9.9.12" class="ltx_td ltx_align_right">1.18</td>
<td id="Sx4.T3.1.9.9.13" class="ltx_td ltx_align_center">3.17</td>
</tr>
<tr id="Sx4.T3.1.10.10" class="ltx_tr">
<td id="Sx4.T3.1.10.10.1" class="ltx_td ltx_align_left ltx_border_t">Question</td>
<td id="Sx4.T3.1.10.10.2" class="ltx_td ltx_align_left ltx_border_t">-</td>
<td id="Sx4.T3.1.10.10.3" class="ltx_td ltx_align_right ltx_border_t">-0.01</td>
<td id="Sx4.T3.1.10.10.4" class="ltx_td ltx_align_right ltx_border_t">1.32</td>
<td id="Sx4.T3.1.10.10.5" class="ltx_td ltx_align_right ltx_border_t">0.05</td>
<td id="Sx4.T3.1.10.10.6" class="ltx_td ltx_align_right ltx_border_t">3.20</td>
<td id="Sx4.T3.1.10.10.7" class="ltx_td ltx_align_right ltx_border_t">2.21</td>
<td id="Sx4.T3.1.10.10.8" class="ltx_td ltx_align_right ltx_border_t">2.89</td>
<td id="Sx4.T3.1.10.10.9" class="ltx_td ltx_align_right ltx_border_t">-1.69</td>
<td id="Sx4.T3.1.10.10.10" class="ltx_td ltx_align_right ltx_border_t">-1.79</td>
<td id="Sx4.T3.1.10.10.11" class="ltx_td ltx_align_right ltx_border_t">5.57</td>
<td id="Sx4.T3.1.10.10.12" class="ltx_td ltx_align_right ltx_border_t">1.76</td>
<td id="Sx4.T3.1.10.10.13" class="ltx_td ltx_align_center ltx_border_t">0.23</td>
</tr>
<tr id="Sx4.T3.1.11.11" class="ltx_tr">
<td id="Sx4.T3.1.11.11.1" class="ltx_td ltx_align_left">+ Caption</td>
<td id="Sx4.T3.1.11.11.2" class="ltx_td ltx_align_left">-</td>
<td id="Sx4.T3.1.11.11.3" class="ltx_td ltx_align_right">0.50</td>
<td id="Sx4.T3.1.11.11.4" class="ltx_td ltx_align_right">2.70</td>
<td id="Sx4.T3.1.11.11.5" class="ltx_td ltx_align_right">1.00</td>
<td id="Sx4.T3.1.11.11.6" class="ltx_td ltx_align_right">4.26</td>
<td id="Sx4.T3.1.11.11.7" class="ltx_td ltx_align_right">3.15</td>
<td id="Sx4.T3.1.11.11.8" class="ltx_td ltx_align_right">3.85</td>
<td id="Sx4.T3.1.11.11.9" class="ltx_td ltx_align_right">-1.18</td>
<td id="Sx4.T3.1.11.11.10" class="ltx_td ltx_align_right">-1.83</td>
<td id="Sx4.T3.1.11.11.11" class="ltx_td ltx_align_right">5.97</td>
<td id="Sx4.T3.1.11.11.12" class="ltx_td ltx_align_right">3.52</td>
<td id="Sx4.T3.1.11.11.13" class="ltx_td ltx_align_center">0.90</td>
</tr>
<tr id="Sx4.T3.1.12.12" class="ltx_tr">
<td id="Sx4.T3.1.12.12.1" class="ltx_td ltx_align_left">KVQAmeta</td>
<td id="Sx4.T3.1.12.12.2" class="ltx_td ltx_align_left">links</td>
<td id="Sx4.T3.1.12.12.3" class="ltx_td ltx_align_right">1.08</td>
<td id="Sx4.T3.1.12.12.4" class="ltx_td ltx_align_right">4.26</td>
<td id="Sx4.T3.1.12.12.5" class="ltx_td ltx_align_right">1.99</td>
<td id="Sx4.T3.1.12.12.6" class="ltx_td ltx_align_right">4.65</td>
<td id="Sx4.T3.1.12.12.7" class="ltx_td ltx_align_right">3.54</td>
<td id="Sx4.T3.1.12.12.8" class="ltx_td ltx_align_right">4.16</td>
<td id="Sx4.T3.1.12.12.9" class="ltx_td ltx_align_right">-0.71</td>
<td id="Sx4.T3.1.12.12.10" class="ltx_td ltx_align_right">-1.52</td>
<td id="Sx4.T3.1.12.12.11" class="ltx_td ltx_align_right">6.86</td>
<td id="Sx4.T3.1.12.12.12" class="ltx_td ltx_align_right">3.54</td>
<td id="Sx4.T3.1.12.12.13" class="ltx_td ltx_align_center">1.66</td>
</tr>
<tr id="Sx4.T3.1.13.13" class="ltx_tr">
<td id="Sx4.T3.1.13.13.1" class="ltx_td ltx_align_left ltx_border_bb">KVQAmeta</td>
<td id="Sx4.T3.1.13.13.2" class="ltx_td ltx_align_left ltx_border_bb">noisy</td>
<td id="Sx4.T3.1.13.13.3" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.3.1" class="ltx_text ltx_font_bold">1.52</span></td>
<td id="Sx4.T3.1.13.13.4" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.4.1" class="ltx_text ltx_font_bold">4.84</span></td>
<td id="Sx4.T3.1.13.13.5" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.5.1" class="ltx_text ltx_font_bold">2.48</span></td>
<td id="Sx4.T3.1.13.13.6" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.6.1" class="ltx_text ltx_font_bold">5.87</span></td>
<td id="Sx4.T3.1.13.13.7" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.7.1" class="ltx_text ltx_font_bold">4.34</span></td>
<td id="Sx4.T3.1.13.13.8" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.8.1" class="ltx_text ltx_font_bold">5.02</span></td>
<td id="Sx4.T3.1.13.13.9" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.9.1" class="ltx_text ltx_font_bold">-0.44</span></td>
<td id="Sx4.T3.1.13.13.10" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.10.1" class="ltx_text ltx_font_bold">-1.51</span></td>
<td id="Sx4.T3.1.13.13.11" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.11.1" class="ltx_text ltx_font_bold">7.31</span></td>
<td id="Sx4.T3.1.13.13.12" class="ltx_td ltx_align_right ltx_border_bb"><span id="Sx4.T3.1.13.13.12.1" class="ltx_text ltx_font_bold">5.24</span></td>
<td id="Sx4.T3.1.13.13.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="Sx4.T3.1.13.13.13.1" class="ltx_text ltx_font_bold">2.12</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>KVQA results by question type accuracy (top half) and confidence ( bottom 4 rows of unconstrained logits). Not shown NERper has highest accuracy for spatial question types (31.42). Average E-BERT refers to averages over NERper, NERagro and KVQAmeta for each link type (as is, links, noisy) </figcaption>
</figure>
</section>
</section>
<section id="Sx5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Results</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">Table <a href="#Sx4.T1" title="Table 1 â€£ KVQA entity span construction â€£ Experiments â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows the average results over the 5 KVQA splits<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>See Appendix Table <a href="#A1.T5" title="Table 5 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> for per split results</span></span></span> and Table <a href="#Sx4.T2" title="Table 2 â€£ OKVQA entity span construction â€£ Experiments â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results over 5 runs with random seeds for OKVQA. In following we highlight observations of interest as they pertain to knowledge injection for VQA.</p>
</div>
<section id="Sx5.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">How E-BERT effects task accuracy</h3>

<div id="Sx5.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px1.p1.1" class="ltx_p">For KVQA, we see the KVQAmeta noisy entity set based model provides the best results ( 52.83 ) compared with the results of feeding the same question + caption text into LXMERT without knowledge injection ( 50.25 ). Additionally using E-BERT on the NERper and NERagro noisy search entity spans gives 0.5 accuracy improvement confirming the utility and efficiency of our method for KBVQA in those cases as well.</p>
</div>
<div id="Sx5.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px1.p2.1" class="ltx_p">The importance of entity span quality is evidenced by the variation in results between NERper, NERagro and KVQAmeta and in all 3 cases using the "noisy" search mechanism to find entity links to Wikipedia provides the best results. We finally note that using LXMERT with questions + captions outperforms the KVQA paperâ€™s baseline model <cite class="ltx_cite ltx_citemacro_citep">(SanketÂ Shah and Talukdar <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> which relies on a closed world subset of Wikipedia setting that only considers facts from 18 relations as candidate answers and a simplified face identification entity linking step, none of which are used in our setup.</p>
</div>
<div id="Sx5.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px1.p3.1" class="ltx_p">For OKVQA, we see that adding E-BERT to LXMERT only slightly improves results compared with using LXMERT without knowledge injection and only when the entity sets provided ( 4K and 2.5K ) are less noisy than those provided by SpACY outright (13K). The OKVQA data is less entity centric and does not contain image captions so retrieval or generation methods for captions could be useful.</p>
</div>
<div id="Sx5.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px1.p4.1" class="ltx_p">We note that LXMERT Plain, which does not use knowledge injection, already does better than the results from the OKVQA paper baseline model <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> (43.51 vs 27.84) and the two highest performing models <cite class="ltx_cite ltx_citemacro_citep">(Shevchenko etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Wu etÂ al. <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> when our experiments were run<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://okvqa.allenai.org/leaderboard.html</span></span></span>. The authors in <cite class="ltx_cite ltx_citemacro_citep">(Shevchenko etÂ al. <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> redo the costly step of pre-training of LXMERT with their form of knowledge injection and get 39.04 accuracy over 3 runs whereas our technique does not require rerunning pre-training. As noted in <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al. <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite>, the OKVQA test images are a subset of COCO validation images which are used to pre-train most of transformer-based vision and language models including LXMERT and VilBERT <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al. <a href="#bib.bib13" title="" class="ltx_ref">2019</a>)</cite>. Although the test questions never appear in the pre-training process, other questions on the test images may help the system understand the image better, leading to a higher performance. In <cite class="ltx_cite ltx_citemacro_citep">(Wu etÂ al. <a href="#bib.bib36" title="" class="ltx_ref">2021</a>)</cite> that amounts to a 1.1 accuracy difference ( from 40.5 to 39.4) when redoing pretraining ViLBERT with the OKVQA test images that appear in COCO removed. At the time when our experiments were run we obtained the state of the art for OKVQA though with the aforementioned data leakage pre-training issue.</p>
</div>
</section>
<section id="Sx5.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">When E-BERT effects task accuracy</h3>

<div id="Sx5.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px2.p1.1" class="ltx_p">To better understand when E-BERT knowledge effects task accuracy, we show accuracy and confidence results by question type for split 1 of KVQA in Tables <a href="#Sx4.T3" title="Table 3 â€£ OKVQA entity span construction â€£ Experiments â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We see all models perform poorly at questions of type â€œsubtraction" and â€œspatial"<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>See Appendix Figures <a href="#A1.F2" title="Figure 2 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and <a href="#A1.F4" title="Figure 4 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> for examples of question types.</span></span></span> which represent 12% and 15%, of questions with combined E-BERT average accuracies of 20.6 and 30.5. We see that both types of questions are quite challenging and almost entirely based on visual entity identification. Adding image captions to the question gives a slight improvement in â€œspatial" questions and worse performance in â€œsubtraction" ones. In both cases E-BERT results using â€œNERper noisy" and â€œKVQAmeta links" give 2.2 and 2.8 point improvements.</p>
</div>
<div id="Sx5.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px2.p2.1" class="ltx_p">For question types where LXMERT already performs strongly, â€œboolean", â€œcounting" and â€œcomparison" with average accuracies of 86.3, 85.1 and 81.8, we see the improvements provided via the best E-BERT results ( -.01, .7 and 1.6 ) are generally smaller than those for other question types such as â€œmulti-hop", â€œmulti-relation", and â€œsubtraction" which get improvements of 6.1, 4.6 and 2.9 points. We see that across question types the models that utilize E-BERT are more confident than the models which do not use knowledge injection though the level of over-confidence is inline with widely known calibration issues affecting neural nets. <cite class="ltx_cite ltx_citemacro_citep">(Desai and Durrett <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Jiang etÂ al. <a href="#bib.bib8" title="" class="ltx_ref">2021</a>)</cite></p>
</div>
</section>
<section id="Sx5.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">E-BERTs effect on VQA explainability</h3>

<div id="Sx5.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px3.p1.1" class="ltx_p">We extract visual and text explanations using BM-GAE and TRF on our KVQA models and Table <a href="#Sx5.T4" title="Table 4 â€£ E-BERTs effect on VQA explainability â€£ Results â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows accuracy results for when these explanation methods find E-BERT enhanced entities to be in the top 5 most important tokens leading to a given answer prediction<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>Appendix Table <a href="#A1.T6" title="Table 6 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the percent breakdown of how many entities appear in the top 1, top 5 and top 10 important tokens for each explanation type over our entity sets along with the percent of questions with E-BERT entity injection for each.</span></span></span>. We see that for 7 of the 9 models, questions which include E-BERT entities amongst their top 5 using BM-GAE provide better accuracy than those using the TRF method. Averaging over the models, weâ€™d achieve 59.74% accuracy with the BM-GAE model and 58.33% using the TRF method compared with 51.04% average accuracy amongst all E-BERT injection models as seen in Table <a href="#Sx4.T3" title="Table 3 â€£ OKVQA entity span construction â€£ Experiments â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. This finding suggests that when using either method, an entity appearing in the top 5 most important tokens allows for improved accuracy which is in agreement with the perturbation testing results in <cite class="ltx_cite ltx_citemacro_citep">(Chefer, Gur, and Wolf <a href="#bib.bib2" title="" class="ltx_ref">2021a</a>)</cite>.</p>
</div>
<div id="Sx5.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px3.p2.1" class="ltx_p">Over all models E-BERT entities appear in the top 5 most important tokens using TRF more than BM-GAE (10.35 vs 8.59 %) though for 3 of the models, the BM-GAE method finds more.
Interestingly using the explaination methods on the KVQAmeta noisy entity sets model, which obtains the best task accuracy results, leads to worse results compared with using the â€œas is" and links version of the KVQAmeta entity set which suggest that for these questions the â€œnoisy" use of the wikipedia search API to link possible entity spans to entity pages had an adverse effect which is not present when considering all questions, as opposed to just those where E-BERT entities are in the top 5 important tokens.</p>
</div>
<div id="Sx5.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px3.p3.1" class="ltx_p">We explore qualitative trends in these explanations to see where knowledge injection is helpful, specifically where the KVQAmeta model predicts correctly while the â€œ+ Caption" does not. <span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Two such examples are found in Appendix Figures <a href="#A1.F3" title="Figure 3 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#A1.F5" title="Figure 5 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a></span></span></span>. In these cases injecting entity knowledge focuses the model on these entities which is evidenced by the top tokens shown for the KVQAmeta model. In the first case, only KVQAmeta injects â€œKnute Nelson" which appears as the 2nd most important token and leads to the only correct prediction (Europe) amongst the models. The token explanations show knowledge injection lessens the importance of question specific words like â€œwhich" andâ€œhow", which are in top 5 tokens for the Question only model. This behavior makes sense since LXMERT was not trained with captions ( knowledge injected or not ) and fine tuning with this extra context shifts the domain a bit.</p>
</div>
<div id="Sx5.SS0.SSS0.Px3.p4" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px3.p4.1" class="ltx_p">We also explore examples where E-BERT hurts model performance<span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>See Appendix Figures <a href="#A1.F6" title="Figure 6 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> and <a href="#A1.F7" title="Figure 7 â€£ Appendix A Appendix â€£ Improving and Diagnosing Knowledge-Based Visual Question Answering via Entity Enhanced Knowledge Injection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> for negative examples</span></span></span>. In the first example, while the Question only and â€œ+ Caption" models answer correctly, the entityâ€œTop Gun" is injected for the NERper and NERagro models leading the model to incorrectly predict the entity â€œDuke Cunningham" was born after WW2, possibly due to Top Gun the 1986 film, and not the US Navy school, being erroneously injected. In the KVQAmeta model, injecting the true entity â€œDuke Cunningham" causes the model to change its prediction incorrectly which reflects a true error use case. The second example on the other hand shows a label error as the entity Fisher Morgan<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>https://en.wikipedia.org/wiki/Fisher_Morgan</span></span></span> is in fact both a singer and actor, the later which KVQAmeta is actually the only model to predict correctly. A way to identify similar dataset label errors would be to inspecting test cases where KVQAmeta injects entities not found by the other E-BERT models and gives a unique prediction marked as incorrect.</p>
</div>
<figure id="Sx5.T4" class="ltx_table">
<table id="Sx5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx5.T4.1.1.1" class="ltx_tr">
<th id="Sx5.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="Sx5.T4.1.1.1.2" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="Sx5.T4.1.1.1.3" class="ltx_td ltx_align_right ltx_border_tt">BM</td>
<td id="Sx5.T4.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt">BM</td>
<td id="Sx5.T4.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt">TRF</td>
<td id="Sx5.T4.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt">TRF</td>
</tr>
<tr id="Sx5.T4.1.2.2" class="ltx_tr">
<th id="Sx5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Model</th>
<th id="Sx5.T4.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">Type</th>
<td id="Sx5.T4.1.2.2.3" class="ltx_td ltx_align_right">ACC</td>
<td id="Sx5.T4.1.2.2.4" class="ltx_td ltx_align_right">Qs</td>
<td id="Sx5.T4.1.2.2.5" class="ltx_td ltx_align_right">Acc</td>
<td id="Sx5.T4.1.2.2.6" class="ltx_td ltx_align_right">Qs</td>
</tr>
<tr id="Sx5.T4.1.3.3" class="ltx_tr">
<th id="Sx5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERper</th>
<th id="Sx5.T4.1.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">as is</th>
<td id="Sx5.T4.1.3.3.3" class="ltx_td ltx_align_right ltx_border_t"><span id="Sx5.T4.1.3.3.3.1" class="ltx_text ltx_font_bold">58.25</span></td>
<td id="Sx5.T4.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">11.48</td>
<td id="Sx5.T4.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">56.11</td>
<td id="Sx5.T4.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">6.13</td>
</tr>
<tr id="Sx5.T4.1.4.4" class="ltx_tr">
<th id="Sx5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<th id="Sx5.T4.1.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">links</th>
<td id="Sx5.T4.1.4.4.3" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.4.4.3.1" class="ltx_text ltx_font_bold">62.18</span></td>
<td id="Sx5.T4.1.4.4.4" class="ltx_td ltx_align_right">8.67</td>
<td id="Sx5.T4.1.4.4.5" class="ltx_td ltx_align_right">56.28</td>
<td id="Sx5.T4.1.4.4.6" class="ltx_td ltx_align_right">6.90</td>
</tr>
<tr id="Sx5.T4.1.5.5" class="ltx_tr">
<th id="Sx5.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<th id="Sx5.T4.1.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">noisy</th>
<td id="Sx5.T4.1.5.5.3" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.5.5.3.1" class="ltx_text ltx_font_bold">69.85</span></td>
<td id="Sx5.T4.1.5.5.4" class="ltx_td ltx_align_right">4.75</td>
<td id="Sx5.T4.1.5.5.5" class="ltx_td ltx_align_right">68.17</td>
<td id="Sx5.T4.1.5.5.6" class="ltx_td ltx_align_right">7.11</td>
</tr>
<tr id="Sx5.T4.1.6.6" class="ltx_tr">
<th id="Sx5.T4.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERagro</th>
<th id="Sx5.T4.1.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">as is</th>
<td id="Sx5.T4.1.6.6.3" class="ltx_td ltx_align_right ltx_border_t"><span id="Sx5.T4.1.6.6.3.1" class="ltx_text ltx_font_bold">65.91</span></td>
<td id="Sx5.T4.1.6.6.4" class="ltx_td ltx_align_right ltx_border_t">4.93</td>
<td id="Sx5.T4.1.6.6.5" class="ltx_td ltx_align_right ltx_border_t">62.41</td>
<td id="Sx5.T4.1.6.6.6" class="ltx_td ltx_align_right ltx_border_t">7.41</td>
</tr>
<tr id="Sx5.T4.1.7.7" class="ltx_tr">
<th id="Sx5.T4.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<th id="Sx5.T4.1.7.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">links</th>
<td id="Sx5.T4.1.7.7.3" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.7.7.3.1" class="ltx_text ltx_font_bold">52.74</span></td>
<td id="Sx5.T4.1.7.7.4" class="ltx_td ltx_align_right">14.75</td>
<td id="Sx5.T4.1.7.7.5" class="ltx_td ltx_align_right">49.31</td>
<td id="Sx5.T4.1.7.7.6" class="ltx_td ltx_align_right">18.52</td>
</tr>
<tr id="Sx5.T4.1.8.8" class="ltx_tr">
<th id="Sx5.T4.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<th id="Sx5.T4.1.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">noisy</th>
<td id="Sx5.T4.1.8.8.3" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.8.8.3.1" class="ltx_text ltx_font_bold">56.07</span></td>
<td id="Sx5.T4.1.8.8.4" class="ltx_td ltx_align_right">20.53</td>
<td id="Sx5.T4.1.8.8.5" class="ltx_td ltx_align_right">43.31</td>
<td id="Sx5.T4.1.8.8.6" class="ltx_td ltx_align_right">18.23</td>
</tr>
<tr id="Sx5.T4.1.9.9" class="ltx_tr">
<th id="Sx5.T4.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">KVQAmeta</th>
<th id="Sx5.T4.1.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">as is</th>
<td id="Sx5.T4.1.9.9.3" class="ltx_td ltx_align_right ltx_border_t">61.00</td>
<td id="Sx5.T4.1.9.9.4" class="ltx_td ltx_align_right ltx_border_t">2.77</td>
<td id="Sx5.T4.1.9.9.5" class="ltx_td ltx_align_right ltx_border_t"><span id="Sx5.T4.1.9.9.5.1" class="ltx_text ltx_font_bold">70.03</span></td>
<td id="Sx5.T4.1.9.9.6" class="ltx_td ltx_align_right ltx_border_t">6.30</td>
</tr>
<tr id="Sx5.T4.1.10.10" class="ltx_tr">
<th id="Sx5.T4.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KVQAmeta</th>
<th id="Sx5.T4.1.10.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">links</th>
<td id="Sx5.T4.1.10.10.3" class="ltx_td ltx_align_right">68.97</td>
<td id="Sx5.T4.1.10.10.4" class="ltx_td ltx_align_right">4.26</td>
<td id="Sx5.T4.1.10.10.5" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.10.10.5.1" class="ltx_text ltx_font_bold">79.67</span></td>
<td id="Sx5.T4.1.10.10.6" class="ltx_td ltx_align_right">12.57</td>
</tr>
<tr id="Sx5.T4.1.11.11" class="ltx_tr">
<th id="Sx5.T4.1.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KVQAmeta</th>
<th id="Sx5.T4.1.11.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row">noisy</th>
<td id="Sx5.T4.1.11.11.3" class="ltx_td ltx_align_right"><span id="Sx5.T4.1.11.11.3.1" class="ltx_text ltx_font_bold">42.72</span></td>
<td id="Sx5.T4.1.11.11.4" class="ltx_td ltx_align_right">5.15</td>
<td id="Sx5.T4.1.11.11.5" class="ltx_td ltx_align_right">39.65</td>
<td id="Sx5.T4.1.11.11.6" class="ltx_td ltx_align_right">10.02</td>
</tr>
<tr id="Sx5.T4.1.12.12" class="ltx_tr">
<th id="Sx5.T4.1.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Average</th>
<th id="Sx5.T4.1.12.12.2" class="ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_t"></th>
<td id="Sx5.T4.1.12.12.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="Sx5.T4.1.12.12.3.1" class="ltx_text ltx_font_bold">59.74</span></td>
<td id="Sx5.T4.1.12.12.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">8.59</td>
<td id="Sx5.T4.1.12.12.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">58.33</td>
<td id="Sx5.T4.1.12.12.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">10.35</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>KVQA Bi-modal (BM) and Transformer attention (TRF) explaination results for Questions where an E-BERT injected entity is in top 5 most important tokens.</figcaption>
</figure>
</section>
<section id="Sx5.SS0.SSS0.Px4" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Effect of additional types of knowledge injection</h3>

<div id="Sx5.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="Sx5.SS0.SSS0.Px4.p1.1" class="ltx_p">In an effort to see whether other retrieval augmented, nearest neighbor or confidence based methods, all applied without redoing pre-training, would improve upon the results of our knowledge enhanced model using the â€œKVQAmeta" derived entity sets for the KVQA data, we conducted initial experiments using Dense Passage Retrieval (DPR) for Open Domain QA <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al. <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>, kNN language models (knn LMs) <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal etÂ al. <a href="#bib.bib10" title="" class="ltx_ref">2020</a>; Rajani etÂ al. <a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> and simple confidence thresholding where E-BERT was only utilized when the modelâ€™s confidence was above a given threshold determined on a hold out validation set. Although we do see slight improvements with the final confidence method, all within a point accuracy, we note that retrieving additional text ( 3 lines ) per question with DPR or using nearest neighbor semantic similarity lookup (knn LMs) over the training set of the downstream task did not lead to any sizeable improvement ( usually within 0.1 accuracy ). It seems likely that in both the later cases, these methods would need to redo pre-training of LXMERT in order to see gains and as our focus is on studying the effects of simple, efficient knowledge injection during fine-tuning we leave that as future work.</p>
</div>
</section>
</section>
<section id="Sx6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Conclusion</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">In this work we analyze how efficient knowledge injection via E-BERT applied during fine tuning affects the performance of an existing visual-linguistic model LXMERT on the relatively unexplored task of knowledge-based VQA (KBVQA) both in terms of accuracy and explainability via BM-GAE. We experiment using two large publicly available VQA datasets: (i)KVQA <cite class="ltx_cite ltx_citemacro_citep">(SanketÂ Shah and Talukdar <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> that is explicitly tied to Wikipedia and rich in rare entities and (ii) OKVQA <cite class="ltx_cite ltx_citemacro_citep">(Marino etÂ al. <a href="#bib.bib15" title="" class="ltx_ref">2019</a>)</cite> that is less entity-centric and more aligned with common sense reasoning. Both datasets lack explicit entity spans and we show how using different entity sets resulting from either weakly supervised methods or manual human annotation affects knowledge injection on task performance. Our analysis shows improved performance on the entity rich KVQA data, 2.5% top 1 accuracy, and a smaller improvement on OKVQA, both without the need to redo any costly pre-training.</p>
</div>
<div id="Sx6.p2" class="ltx_para">
<p id="Sx6.p2.1" class="ltx_p">In the future we can study how and if such knowledge injection techniques benefit other VQA models including recent work on prompt tuning GPT3<cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al. <a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> which shows improved results on OKVQA, though there the model size, in terms of model parameters, increases from 228 million for LXMERT to 175 billion for GPT3. The authors use of VinVl <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al. <a href="#bib.bib40" title="" class="ltx_ref">2021</a>)</cite> and COCO for generated and gold image captions could improve our results on OKVQA.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bach etÂ al. (2015)</span>
<span class="ltx_bibblock">
Bach, S.; Binder, A.; Montavon, G.; Klauschen, F.; MÃ¼ller, K.-R.; and Samek,
W. 2015.

</span>
<span class="ltx_bibblock">On Pixel-Wise Explanations for Non-Linear Classifier Decisions by
Layer-Wise Relevance Propagation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">PLoS One</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer, Gur, and Wolf (2021a)</span>
<span class="ltx_bibblock">
Chefer, H.; Gur, S.; and Wolf, L. 2021a.

</span>
<span class="ltx_bibblock">Generic Attention-model Explainability for Interpreting Bi-Modal and
Encoder-Decoder Transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chefer, Gur, and Wolf (2021b)</span>
<span class="ltx_bibblock">
Chefer, H.; Gur, S.; and Wolf, L. 2021b.

</span>
<span class="ltx_bibblock">Transformer Interpretability Beyond Attention Visualization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, 782â€“791.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai and Durrett (2020)</span>
<span class="ltx_bibblock">
Desai, S.; and Durrett, G. 2020.

</span>
<span class="ltx_bibblock">Calibration of Pre-trained Transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gan etÂ al. (2020)</span>
<span class="ltx_bibblock">
Gan, Z.; Chen, Y.; Li, L.; Zhu, C.; Cheng, Y.; and Liu, J. 2020.

</span>
<span class="ltx_bibblock">Large-Scale Adversarial Training for Vision-and-Language
Representation Learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of Advances in Neural Information Processing
Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2019)</span>
<span class="ltx_bibblock">
Gao, D.; Wang, R.; Shan, S.; and Chen, X. 2019.

</span>
<span class="ltx_bibblock">From Two Graphs to N Questions: A VQA Dataset for Compositional
Reasoning on Vision and Commonsense.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1908.02962.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GardÃ¨res etÂ al. (2020)</span>
<span class="ltx_bibblock">
GardÃ¨res, F.; Ziaeefard, M.; Abeloos, B.; and Lecue, F. 2020.

</span>
<span class="ltx_bibblock">ConceptBert: Concept-Aware Representation for Visual Question
Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jiang, Z.; Araki, J.; Ding, H.; and Neubig, G. 2021.

</span>
<span class="ltx_bibblock">How Can We Know When Language Models Know? On the Calibration of
Language Models for Question Answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9: 962â€“977.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Karpukhin, V.; Oguz, B.; Min, S.; Lewis, P.; Wu, L.; Edunov, S.; Chen, D.; and
Yih, W.-t. 2020.

</span>
<span class="ltx_bibblock">Dense Passage Retrieval for Open-Domain Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal etÂ al. (2020)</span>
<span class="ltx_bibblock">
Khandelwal, U.; Levy, O.; Jurafsky, D.; Zettlemoyer, L.; and Lewis, M. 2020.

</span>
<span class="ltx_bibblock">Generalization through Memorization: Nearest Neighbor Language
Models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li, Wang, and Zhu (2020)</span>
<span class="ltx_bibblock">
Li, G.; Wang, X.; and Zhu, W. 2020.

</span>
<span class="ltx_bibblock">Boosting Visual Question Answering with Context-Aware Knowledge
Aggregation.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2018)</span>
<span class="ltx_bibblock">
Li, Q.; Fu, J.; Yu, D.; Mei, T.; and Luo, J. 2018.

</span>
<span class="ltx_bibblock">Tell-and-Answer: Towards Explainable Visual Question Answering using
Attributes and Captions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>. Brussels, Belgium: Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Lu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019.

</span>
<span class="ltx_bibblock">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations
for Vision-and-Language Tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of Advances in Neural Information Processing
Systems (NeurIPS)</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundberg and Lee (2017)</span>
<span class="ltx_bibblock">
Lundberg, S.Â M.; and Lee, S.-I. 2017.

</span>
<span class="ltx_bibblock">A Unified Approach to Interpreting Model Predictions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">NeurIPS</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino etÂ al. (2019)</span>
<span class="ltx_bibblock">
Marino, K.; Rastegari, M.; Farhadi, A.; and Mottaghi, R. 2019.

</span>
<span class="ltx_bibblock">OK-VQA: A Visual Question Answering Benchmark Requiring External
Knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Messina etÂ al. (2020)</span>
<span class="ltx_bibblock">
Messina, N.; Amato, G.; Esuli, A.; Falchi, F.; Gennaro, C.; and
Marchand-Maillet, S. 2020.

</span>
<span class="ltx_bibblock">Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using
Transformer Encoders.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2008.05231</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NagarajÂ Rao etÂ al. (2021)</span>
<span class="ltx_bibblock">
NagarajÂ Rao, V.; Zhen, X.; Hovsepian, K.; and Shen, M. 2021.

</span>
<span class="ltx_bibblock">A First Look: Towards Explainable TextVQA Models via Visual and
Textual Explanations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Third Workshop on Multimodal Artificial
Intelligence</em>. Mexico City, Mexico: Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni etÂ al. (2019)</span>
<span class="ltx_bibblock">
Petroni, F.; RocktÃ¤schel, T.; Riedel, S.; Lewis, P.; Bakhtin, A.; Wu, Y.;
and Miller, A. 2019.

</span>
<span class="ltx_bibblock">Language Models as Knowledge Bases?

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poerner, Waltinger, and SchÃ¼tze (2019)</span>
<span class="ltx_bibblock">
Poerner, N.; Waltinger, U.; and SchÃ¼tze, H. 2019.

</span>
<span class="ltx_bibblock">BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based
Reasoning in Unsupervised QA.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1911.03681.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poerner, Waltinger, and SchÃ¼tze (2020)</span>
<span class="ltx_bibblock">
Poerner, N.; Waltinger, U.; and SchÃ¼tze, H. 2020.

</span>
<span class="ltx_bibblock">E-BERT: Efficient-Yet-Effective Entity Embeddings for BERT.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pruthi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Pruthi, G.; Liu, F.; Kale, S.; and Sundararajan, M. 2020.

</span>
<span class="ltx_bibblock">Estimating Training Data Influence by Tracing Gradient Descent.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajani etÂ al. (2020)</span>
<span class="ltx_bibblock">
Rajani, N.Â F.; Krause, B.; Yin, W.; Niu, T.; Socher, R.; and Xiong, C. 2020.

</span>
<span class="ltx_bibblock">Explaining and Improving Model Behavior with k Nearest Neighbor
Representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">https://arxiv.org/abs/2010.09030</em>, volume abs/2010.09030.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts, Raffel, and Shazeer (2020)</span>
<span class="ltx_bibblock">
Roberts, A.; Raffel, C.; and Shazeer, N. 2020.

</span>
<span class="ltx_bibblock">How Much Knowledge Can You Pack Into the Parameters of a Language
Model?

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2002.08910.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sampat, Yang, and Baral (2020)</span>
<span class="ltx_bibblock">
Sampat, S.; Yang, Y.; and Baral, C. 2020.

</span>
<span class="ltx_bibblock">Visuo-Linguistic Question Answering (VLQA) Challenge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP</em>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SanketÂ Shah and Talukdar (2019)</span>
<span class="ltx_bibblock">
SanketÂ Shah, N.Â Y., AnandÂ Mishra; and Talukdar, P.Â P. 2019.

</span>
<span class="ltx_bibblock">KVQA: Knowledge-Aware Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shevchenko etÂ al. (2021)</span>
<span class="ltx_bibblock">
Shevchenko, V.; Teney, D.; Dick, A.; and vanÂ den Hengel, A. 2021.

</span>
<span class="ltx_bibblock">Reasoning over Vision and Language: Exploring the Benefits of
Supplemental Knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">European Chapter of the Association for Computational
Linguistics (EACL)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2020)</span>
<span class="ltx_bibblock">
Shi, L.; Shuang, K.; Geng, S.; Su, P.; Jiang, Z.; Gao, P.; Fu, Z.; deÂ Melo, G.;
and Su, S. 2020.

</span>
<span class="ltx_bibblock">Contrastive Visual-Linguistic Pretraining.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2007.13135.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2020)</span>
<span class="ltx_bibblock">
Singh, A.; Goswami, V.; Natarajan, V.; Jiang, Y.; Chen, X.; Shah, M.; Rohrbach,
M.; Batra, D.; and Parikh, D. 2020.

</span>
<span class="ltx_bibblock">MMF: A multimodal framework for vision and language research.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/facebookresearch/mmf</span>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh etÂ al. (2019)</span>
<span class="ltx_bibblock">
Singh, A.Â K.; Mishra, A.; Shekhar, S.; and Chakraborty, A. 2019.

</span>
<span class="ltx_bibblock">From Strings to Things: Knowledge-Enabled VQA Model That Can Read and
Reason.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song etÂ al. (2020)</span>
<span class="ltx_bibblock">
Song, D.; Ma, S.; Sun, Z.; Yang, S.; and Liao, L. 2020.

</span>
<span class="ltx_bibblock">KVL-BERT: Knowledge Enhanced Visual-and-Linguistic BERT for Visual
Commonsense Reasoning.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, abs/2012.07000.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su etÂ al. (2020)</span>
<span class="ltx_bibblock">
Su, W.; Zhu, X.; Cao, Y.; Li, B.; Lu, L.; Wei, F.; and Dai, J. 2020.

</span>
<span class="ltx_bibblock">VL-BERT: Pre-training of Generic Visual-Linguistic Representations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundararajan, Taly, and Yan (2017)</span>
<span class="ltx_bibblock">
Sundararajan, M.; Taly, A.; and Yan, Q. 2017.

</span>
<span class="ltx_bibblock">Axiomatic Attribution for Deep Networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">ICLR</em>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2019)</span>
<span class="ltx_bibblock">
Tan, H.; and Bansal, M. 2019.

</span>
<span class="ltx_bibblock">LXMERT: Learning Cross-Modality Encoder Representations from
Transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2018)</span>
<span class="ltx_bibblock">
Wang, P.; Wu, Q.; Shen, C.; Dick, A.; and vanÂ den Hengel, A. 2018.

</span>
<span class="ltx_bibblock">FVQA: Fact-Based Visual Question Answering.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Pattern Anal. Mach. Intell.</em>

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu, Chen, and Mooney (2021)</span>
<span class="ltx_bibblock">
Wu, J.; Chen, L.; and Mooney, R.Â J. 2021.

</span>
<span class="ltx_bibblock">Improving VQA and its Explanations by Comparing Competing
Explanations.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">The AAAI Conference on Artificial Intelligence (AAAI),
Explainable Agency in Artificial Intelligence Workshop</em>, volume
arXiv:2006.15631.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Wu, J.; Lu, J.; Sabharwal, A.; and Mottaghi, R. 2021.

</span>
<span class="ltx_bibblock">Multi-Modal Answer Validation for Knowledge-Based VQA.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">https://arxiv.org/abs/2103.12248</em>.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamada etÂ al. (2016)</span>
<span class="ltx_bibblock">
Yamada, I.; Shindo, H.; Takeda, H.; and Takefuji, Y. 2016.

</span>
<span class="ltx_bibblock">Joint Learning of the Embedding of Words and Entities for Named
Entity Disambiguation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Conference on Computational Natural
Language Learning (CoNLL)</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Yang, Z.; Gan, Z.; Wang, J.; Hu, X.; Lu, Y.; Liu, Z.; and Wang, L. 2021.

</span>
<span class="ltx_bibblock">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.05014</em>.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Yu, F.; Tang, J.; Yin, W.; Sun, Y.; Tian, H.; Wu, H.; and Wang, H. 2020.

</span>
<span class="ltx_bibblock">ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through
Scene Graph.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2006.16934.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.; Choi, Y.; and Gao, J.
2021.

</span>
<span class="ltx_bibblock">VinVL: Making Visual Representations Matter in Vision-Language
Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">CVPR 2021</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziaeefard and Lecue (2020)</span>
<span class="ltx_bibblock">
Ziaeefard, M.; and Lecue, F. 2020.

</span>
<span class="ltx_bibblock">Towards Knowledge-Augmented Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the International Conference on Computational
Linguistics (COLING)</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<figure id="A1.F2" class="ltx_figure"><img src="/html/2112.06888/assets/x2.png" id="A1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="118" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of KVQA questions by question type where all models perform poorly</figcaption>
</figure>
<figure id="A1.T5" class="ltx_table">
<table id="A1.T5.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A1.T5.1.1" class="ltx_tr">
<th id="A1.T5.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Model</th>
<th id="A1.T5.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Type</th>
<td id="A1.T5.1.1.4" class="ltx_td ltx_align_right ltx_border_t">S 1</td>
<td id="A1.T5.1.1.5" class="ltx_td ltx_align_right ltx_border_t">S 2</td>
<td id="A1.T5.1.1.6" class="ltx_td ltx_align_right ltx_border_t">S 3</td>
<td id="A1.T5.1.1.7" class="ltx_td ltx_align_right ltx_border_t">S 4</td>
<td id="A1.T5.1.1.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">S 5</td>
<td id="A1.T5.1.1.9" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">Avg</td>
<td id="A1.T5.1.1.10" class="ltx_td ltx_align_right ltx_border_t">Std</td>
<td id="A1.T5.1.1.11" class="ltx_td ltx_align_right ltx_border_t">Max</td>
<td id="A1.T5.1.1.12" class="ltx_td ltx_align_right ltx_border_t">Median</td>
<td id="A1.T5.1.1.1" class="ltx_td ltx_align_right ltx_border_t">% <math id="A1.T5.1.1.1.m1.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="A1.T5.1.1.1.m1.1a"><mo id="A1.T5.1.1.1.m1.1.1" xref="A1.T5.1.1.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A1.T5.1.1.1.m1.1b"><gt id="A1.T5.1.1.1.m1.1.1.cmml" xref="A1.T5.1.1.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.1.1.1.m1.1c">&gt;</annotation></semantics></math> Capt</td>
</tr>
<tr id="A1.T5.1.2.1" class="ltx_tr">
<th id="A1.T5.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"># questions</th>
<th id="A1.T5.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">test</th>
<td id="A1.T5.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t">18697</td>
<td id="A1.T5.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t">18505</td>
<td id="A1.T5.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t">18696</td>
<td id="A1.T5.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t">19248</td>
<td id="A1.T5.1.2.1.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">18120</td>
<td id="A1.T5.1.2.1.8" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="A1.T5.1.2.1.9" class="ltx_td ltx_border_t"></td>
<td id="A1.T5.1.2.1.10" class="ltx_td ltx_border_t"></td>
<td id="A1.T5.1.2.1.11" class="ltx_td ltx_border_t"></td>
<td id="A1.T5.1.2.1.12" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A1.T5.1.3.2" class="ltx_tr">
<th id="A1.T5.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Plain</th>
<th id="A1.T5.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">-</th>
<td id="A1.T5.1.3.2.3" class="ltx_td ltx_align_right ltx_border_t">47.27</td>
<td id="A1.T5.1.3.2.4" class="ltx_td ltx_align_right ltx_border_t">47.54</td>
<td id="A1.T5.1.3.2.5" class="ltx_td ltx_align_right ltx_border_t">47.34</td>
<td id="A1.T5.1.3.2.6" class="ltx_td ltx_align_right ltx_border_t">48.35</td>
<td id="A1.T5.1.3.2.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">47.18</td>
<td id="A1.T5.1.3.2.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">47.54</td>
<td id="A1.T5.1.3.2.9" class="ltx_td ltx_align_right ltx_border_t">0.47</td>
<td id="A1.T5.1.3.2.10" class="ltx_td ltx_align_right ltx_border_t">48.35</td>
<td id="A1.T5.1.3.2.11" class="ltx_td ltx_align_right ltx_border_t">47.34</td>
<td id="A1.T5.1.3.2.12" class="ltx_td ltx_align_right ltx_border_t">0</td>
</tr>
<tr id="A1.T5.1.4.3" class="ltx_tr">
<th id="A1.T5.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Capt</th>
<th id="A1.T5.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">-</th>
<td id="A1.T5.1.4.3.3" class="ltx_td ltx_align_right">49.84</td>
<td id="A1.T5.1.4.3.4" class="ltx_td ltx_align_right">50.43</td>
<td id="A1.T5.1.4.3.5" class="ltx_td ltx_align_right">50.66</td>
<td id="A1.T5.1.4.3.6" class="ltx_td ltx_align_right">51.05</td>
<td id="A1.T5.1.4.3.7" class="ltx_td ltx_align_right ltx_border_r">49.25</td>
<td id="A1.T5.1.4.3.8" class="ltx_td ltx_align_right ltx_border_r">50.25</td>
<td id="A1.T5.1.4.3.9" class="ltx_td ltx_align_right">0.71</td>
<td id="A1.T5.1.4.3.10" class="ltx_td ltx_align_right">51.05</td>
<td id="A1.T5.1.4.3.11" class="ltx_td ltx_align_right">50.43</td>
<td id="A1.T5.1.4.3.12" class="ltx_td ltx_align_right">0</td>
</tr>
<tr id="A1.T5.1.5.4" class="ltx_tr">
<th id="A1.T5.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERper</th>
<th id="A1.T5.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">as is</th>
<td id="A1.T5.1.5.4.3" class="ltx_td ltx_align_right ltx_border_t">50.05</td>
<td id="A1.T5.1.5.4.4" class="ltx_td ltx_align_right ltx_border_t">50.67</td>
<td id="A1.T5.1.5.4.5" class="ltx_td ltx_align_right ltx_border_t">50.25</td>
<td id="A1.T5.1.5.4.6" class="ltx_td ltx_align_right ltx_border_t">51.58</td>
<td id="A1.T5.1.5.4.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">49.28</td>
<td id="A1.T5.1.5.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">50.37</td>
<td id="A1.T5.1.5.4.9" class="ltx_td ltx_align_right ltx_border_t">0.85</td>
<td id="A1.T5.1.5.4.10" class="ltx_td ltx_align_right ltx_border_t">51.58</td>
<td id="A1.T5.1.5.4.11" class="ltx_td ltx_align_right ltx_border_t">50.25</td>
<td id="A1.T5.1.5.4.12" class="ltx_td ltx_align_right ltx_border_t">0.8</td>
</tr>
<tr id="A1.T5.1.6.5" class="ltx_tr">
<th id="A1.T5.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<th id="A1.T5.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">links</th>
<td id="A1.T5.1.6.5.3" class="ltx_td ltx_align_right">50.11</td>
<td id="A1.T5.1.6.5.4" class="ltx_td ltx_align_right">50.28</td>
<td id="A1.T5.1.6.5.5" class="ltx_td ltx_align_right">50.73</td>
<td id="A1.T5.1.6.5.6" class="ltx_td ltx_align_right">50.27</td>
<td id="A1.T5.1.6.5.7" class="ltx_td ltx_align_right ltx_border_r">50.73</td>
<td id="A1.T5.1.6.5.8" class="ltx_td ltx_align_right ltx_border_r">50.42</td>
<td id="A1.T5.1.6.5.9" class="ltx_td ltx_align_right">0.29</td>
<td id="A1.T5.1.6.5.10" class="ltx_td ltx_align_right">50.73</td>
<td id="A1.T5.1.6.5.11" class="ltx_td ltx_align_right">50.28</td>
<td id="A1.T5.1.6.5.12" class="ltx_td ltx_align_right">0.6</td>
</tr>
<tr id="A1.T5.1.7.6" class="ltx_tr">
<th id="A1.T5.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERper</th>
<th id="A1.T5.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">noisy</th>
<td id="A1.T5.1.7.6.3" class="ltx_td ltx_align_right">51.10</td>
<td id="A1.T5.1.7.6.4" class="ltx_td ltx_align_right">50.24</td>
<td id="A1.T5.1.7.6.5" class="ltx_td ltx_align_right">50.25</td>
<td id="A1.T5.1.7.6.6" class="ltx_td ltx_align_right">51.34</td>
<td id="A1.T5.1.7.6.7" class="ltx_td ltx_align_right ltx_border_r">50.53</td>
<td id="A1.T5.1.7.6.8" class="ltx_td ltx_align_right ltx_border_r">50.69</td>
<td id="A1.T5.1.7.6.9" class="ltx_td ltx_align_right">0.50</td>
<td id="A1.T5.1.7.6.10" class="ltx_td ltx_align_right">51.34</td>
<td id="A1.T5.1.7.6.11" class="ltx_td ltx_align_right">50.53</td>
<td id="A1.T5.1.7.6.12" class="ltx_td ltx_align_right">0.6</td>
</tr>
<tr id="A1.T5.1.8.7" class="ltx_tr">
<th id="A1.T5.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">NERagro</th>
<th id="A1.T5.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">as is</th>
<td id="A1.T5.1.8.7.3" class="ltx_td ltx_align_right ltx_border_t">49.72</td>
<td id="A1.T5.1.8.7.4" class="ltx_td ltx_align_right ltx_border_t">50.05</td>
<td id="A1.T5.1.8.7.5" class="ltx_td ltx_align_right ltx_border_t">50.33</td>
<td id="A1.T5.1.8.7.6" class="ltx_td ltx_align_right ltx_border_t">51.09</td>
<td id="A1.T5.1.8.7.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">50.11</td>
<td id="A1.T5.1.8.7.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">50.26</td>
<td id="A1.T5.1.8.7.9" class="ltx_td ltx_align_right ltx_border_t">0.51</td>
<td id="A1.T5.1.8.7.10" class="ltx_td ltx_align_right ltx_border_t">51.09</td>
<td id="A1.T5.1.8.7.11" class="ltx_td ltx_align_right ltx_border_t">50.11</td>
<td id="A1.T5.1.8.7.12" class="ltx_td ltx_align_right ltx_border_t">0.4</td>
</tr>
<tr id="A1.T5.1.9.8" class="ltx_tr">
<th id="A1.T5.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<th id="A1.T5.1.9.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">links</th>
<td id="A1.T5.1.9.8.3" class="ltx_td ltx_align_right">50.29</td>
<td id="A1.T5.1.9.8.4" class="ltx_td ltx_align_right">50.24</td>
<td id="A1.T5.1.9.8.5" class="ltx_td ltx_align_right">49.64</td>
<td id="A1.T5.1.9.8.6" class="ltx_td ltx_align_right">50.86</td>
<td id="A1.T5.1.9.8.7" class="ltx_td ltx_align_right ltx_border_r">50.63</td>
<td id="A1.T5.1.9.8.8" class="ltx_td ltx_align_right ltx_border_r">50.33</td>
<td id="A1.T5.1.9.8.9" class="ltx_td ltx_align_right">0.46</td>
<td id="A1.T5.1.9.8.10" class="ltx_td ltx_align_right">50.86</td>
<td id="A1.T5.1.9.8.11" class="ltx_td ltx_align_right">50.29</td>
<td id="A1.T5.1.9.8.12" class="ltx_td ltx_align_right">0.4</td>
</tr>
<tr id="A1.T5.1.10.9" class="ltx_tr">
<th id="A1.T5.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">NERagro</th>
<th id="A1.T5.1.10.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">noisy</th>
<td id="A1.T5.1.10.9.3" class="ltx_td ltx_align_right">50.08</td>
<td id="A1.T5.1.10.9.4" class="ltx_td ltx_align_right">50.78</td>
<td id="A1.T5.1.10.9.5" class="ltx_td ltx_align_right">50.72</td>
<td id="A1.T5.1.10.9.6" class="ltx_td ltx_align_right">51.38</td>
<td id="A1.T5.1.10.9.7" class="ltx_td ltx_align_right ltx_border_r">50.91</td>
<td id="A1.T5.1.10.9.8" class="ltx_td ltx_align_right ltx_border_r">50.77</td>
<td id="A1.T5.1.10.9.9" class="ltx_td ltx_align_right">0.47</td>
<td id="A1.T5.1.10.9.10" class="ltx_td ltx_align_right">51.38</td>
<td id="A1.T5.1.10.9.11" class="ltx_td ltx_align_right">50.78</td>
<td id="A1.T5.1.10.9.12" class="ltx_td ltx_align_right">1.0</td>
</tr>
<tr id="A1.T5.1.11.10" class="ltx_tr">
<th id="A1.T5.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">KVQAmeta</th>
<th id="A1.T5.1.11.10.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">as is</th>
<td id="A1.T5.1.11.10.3" class="ltx_td ltx_align_right ltx_border_t">52.20</td>
<td id="A1.T5.1.11.10.4" class="ltx_td ltx_align_right ltx_border_t">52.64</td>
<td id="A1.T5.1.11.10.5" class="ltx_td ltx_align_right ltx_border_t"><span id="A1.T5.1.11.10.5.1" class="ltx_text ltx_font_bold">52.70</span></td>
<td id="A1.T5.1.11.10.6" class="ltx_td ltx_align_right ltx_border_t">53.22</td>
<td id="A1.T5.1.11.10.7" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">52.50</td>
<td id="A1.T5.1.11.10.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">52.65</td>
<td id="A1.T5.1.11.10.9" class="ltx_td ltx_align_right ltx_border_t">0.37</td>
<td id="A1.T5.1.11.10.10" class="ltx_td ltx_align_right ltx_border_t">53.22</td>
<td id="A1.T5.1.11.10.11" class="ltx_td ltx_align_right ltx_border_t">52.64</td>
<td id="A1.T5.1.11.10.12" class="ltx_td ltx_align_right ltx_border_t">1.0</td>
</tr>
<tr id="A1.T5.1.12.11" class="ltx_tr">
<th id="A1.T5.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">KVQAmeta</th>
<th id="A1.T5.1.12.11.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">links</th>
<td id="A1.T5.1.12.11.3" class="ltx_td ltx_align_right">52.83</td>
<td id="A1.T5.1.12.11.4" class="ltx_td ltx_align_right">52.60</td>
<td id="A1.T5.1.12.11.5" class="ltx_td ltx_align_right">51.92</td>
<td id="A1.T5.1.12.11.6" class="ltx_td ltx_align_right">53.44</td>
<td id="A1.T5.1.12.11.7" class="ltx_td ltx_align_right ltx_border_r">52.60</td>
<td id="A1.T5.1.12.11.8" class="ltx_td ltx_align_right ltx_border_r">52.68</td>
<td id="A1.T5.1.12.11.9" class="ltx_td ltx_align_right">0.55</td>
<td id="A1.T5.1.12.11.10" class="ltx_td ltx_align_right">53.44</td>
<td id="A1.T5.1.12.11.11" class="ltx_td ltx_align_right">52.60</td>
<td id="A1.T5.1.12.11.12" class="ltx_td ltx_align_right">1.0</td>
</tr>
<tr id="A1.T5.1.13.12" class="ltx_tr">
<th id="A1.T5.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">KVQAmeta</th>
<th id="A1.T5.1.13.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">noisy</th>
<td id="A1.T5.1.13.12.3" class="ltx_td ltx_align_right ltx_border_b"><span id="A1.T5.1.13.12.3.1" class="ltx_text ltx_font_bold">53.01</span></td>
<td id="A1.T5.1.13.12.4" class="ltx_td ltx_align_right ltx_border_b"><span id="A1.T5.1.13.12.4.1" class="ltx_text ltx_font_bold">52.86</span></td>
<td id="A1.T5.1.13.12.5" class="ltx_td ltx_align_right ltx_border_b">52.37</td>
<td id="A1.T5.1.13.12.6" class="ltx_td ltx_align_right ltx_border_b"><span id="A1.T5.1.13.12.6.1" class="ltx_text ltx_font_bold">53.58</span></td>
<td id="A1.T5.1.13.12.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A1.T5.1.13.12.7.1" class="ltx_text ltx_font_bold">52.34</span></td>
<td id="A1.T5.1.13.12.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r"><span id="A1.T5.1.13.12.8.1" class="ltx_text ltx_font_bold">52.83</span></td>
<td id="A1.T5.1.13.12.9" class="ltx_td ltx_align_right ltx_border_b">0.51</td>
<td id="A1.T5.1.13.12.10" class="ltx_td ltx_align_right ltx_border_b">53.58</td>
<td id="A1.T5.1.13.12.11" class="ltx_td ltx_align_right ltx_border_b">52.86</td>
<td id="A1.T5.1.13.12.12" class="ltx_td ltx_align_right ltx_border_b">1.0</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>KVQA results over all splits, models and link types. Last column is Percent of Times model outperforms CAPT results</figcaption>
</figure>
<figure id="A1.T6" class="ltx_table">
<table id="A1.T6.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T6.1.1.1" class="ltx_tr">
<th id="A1.T6.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_border_t"></th>
<th id="A1.T6.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t"></th>
<th id="A1.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">bimodal generic</th>
<th id="A1.T6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3">transformer attention</th>
<th id="A1.T6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Qs w/</th>
</tr>
<tr id="A1.T6.1.2.2" class="ltx_tr">
<th id="A1.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Model</th>
<th id="A1.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">Type</th>
<th id="A1.T6.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">top1</th>
<th id="A1.T6.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column">top5</th>
<th id="A1.T6.1.2.2.5" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">top10</th>
<th id="A1.T6.1.2.2.6" class="ltx_td ltx_align_left ltx_th ltx_th_column">top1</th>
<th id="A1.T6.1.2.2.7" class="ltx_td ltx_align_left ltx_th ltx_th_column">top5</th>
<th id="A1.T6.1.2.2.8" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r">top10</th>
<th id="A1.T6.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">EBERT</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T6.1.3.1" class="ltx_tr">
<td id="A1.T6.1.3.1.1" class="ltx_td ltx_align_left ltx_border_t">NERper</td>
<td id="A1.T6.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">as is</td>
<td id="A1.T6.1.3.1.3" class="ltx_td ltx_align_left ltx_border_t">0.66</td>
<td id="A1.T6.1.3.1.4" class="ltx_td ltx_align_left ltx_border_t">11.48</td>
<td id="A1.T6.1.3.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">31.23</td>
<td id="A1.T6.1.3.1.6" class="ltx_td ltx_align_left ltx_border_t">0.29</td>
<td id="A1.T6.1.3.1.7" class="ltx_td ltx_align_left ltx_border_t">6.13</td>
<td id="A1.T6.1.3.1.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">22.64</td>
<td id="A1.T6.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t">.78</td>
</tr>
<tr id="A1.T6.1.4.2" class="ltx_tr">
<td id="A1.T6.1.4.2.1" class="ltx_td ltx_align_left">NERper</td>
<td id="A1.T6.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r">links</td>
<td id="A1.T6.1.4.2.3" class="ltx_td ltx_align_left">0.32</td>
<td id="A1.T6.1.4.2.4" class="ltx_td ltx_align_left">8.67</td>
<td id="A1.T6.1.4.2.5" class="ltx_td ltx_align_left ltx_border_r">33.32</td>
<td id="A1.T6.1.4.2.6" class="ltx_td ltx_align_left">0.39</td>
<td id="A1.T6.1.4.2.7" class="ltx_td ltx_align_left">6.90</td>
<td id="A1.T6.1.4.2.8" class="ltx_td ltx_align_left ltx_border_r">25.24</td>
<td id="A1.T6.1.4.2.9" class="ltx_td ltx_align_center">.79</td>
</tr>
<tr id="A1.T6.1.5.3" class="ltx_tr">
<td id="A1.T6.1.5.3.1" class="ltx_td ltx_align_left">NERper</td>
<td id="A1.T6.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r">noisy</td>
<td id="A1.T6.1.5.3.3" class="ltx_td ltx_align_left">0.13</td>
<td id="A1.T6.1.5.3.4" class="ltx_td ltx_align_left">4.75</td>
<td id="A1.T6.1.5.3.5" class="ltx_td ltx_align_left ltx_border_r">21.62</td>
<td id="A1.T6.1.5.3.6" class="ltx_td ltx_align_left">0.73</td>
<td id="A1.T6.1.5.3.7" class="ltx_td ltx_align_left">7.11</td>
<td id="A1.T6.1.5.3.8" class="ltx_td ltx_align_left ltx_border_r">23.38</td>
<td id="A1.T6.1.5.3.9" class="ltx_td ltx_align_center">.94</td>
</tr>
<tr id="A1.T6.1.6.4" class="ltx_tr">
<td id="A1.T6.1.6.4.1" class="ltx_td ltx_align_left ltx_border_t">NERagro</td>
<td id="A1.T6.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">as is</td>
<td id="A1.T6.1.6.4.3" class="ltx_td ltx_align_left ltx_border_t">0.31</td>
<td id="A1.T6.1.6.4.4" class="ltx_td ltx_align_left ltx_border_t">4.93</td>
<td id="A1.T6.1.6.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">19.60</td>
<td id="A1.T6.1.6.4.6" class="ltx_td ltx_align_left ltx_border_t">0.38</td>
<td id="A1.T6.1.6.4.7" class="ltx_td ltx_align_left ltx_border_t">7.41</td>
<td id="A1.T6.1.6.4.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">28.32</td>
<td id="A1.T6.1.6.4.9" class="ltx_td ltx_align_center ltx_border_t">.91</td>
</tr>
<tr id="A1.T6.1.7.5" class="ltx_tr">
<td id="A1.T6.1.7.5.1" class="ltx_td ltx_align_left">NERagro</td>
<td id="A1.T6.1.7.5.2" class="ltx_td ltx_align_left ltx_border_r">links</td>
<td id="A1.T6.1.7.5.3" class="ltx_td ltx_align_left">0.56</td>
<td id="A1.T6.1.7.5.4" class="ltx_td ltx_align_left">14.75</td>
<td id="A1.T6.1.7.5.5" class="ltx_td ltx_align_left ltx_border_r">44.46</td>
<td id="A1.T6.1.7.5.6" class="ltx_td ltx_align_left">1.10</td>
<td id="A1.T6.1.7.5.7" class="ltx_td ltx_align_left">18.52</td>
<td id="A1.T6.1.7.5.8" class="ltx_td ltx_align_left ltx_border_r">50.02</td>
<td id="A1.T6.1.7.5.9" class="ltx_td ltx_align_center">.97</td>
</tr>
<tr id="A1.T6.1.8.6" class="ltx_tr">
<td id="A1.T6.1.8.6.1" class="ltx_td ltx_align_left">NERagro</td>
<td id="A1.T6.1.8.6.2" class="ltx_td ltx_align_left ltx_border_r">noisy</td>
<td id="A1.T6.1.8.6.3" class="ltx_td ltx_align_left">1.30</td>
<td id="A1.T6.1.8.6.4" class="ltx_td ltx_align_left">20.53</td>
<td id="A1.T6.1.8.6.5" class="ltx_td ltx_align_left ltx_border_r">44.94</td>
<td id="A1.T6.1.8.6.6" class="ltx_td ltx_align_left">1.43</td>
<td id="A1.T6.1.8.6.7" class="ltx_td ltx_align_left">18.23</td>
<td id="A1.T6.1.8.6.8" class="ltx_td ltx_align_left ltx_border_r">40.95</td>
<td id="A1.T6.1.8.6.9" class="ltx_td ltx_align_center">.97</td>
</tr>
<tr id="A1.T6.1.9.7" class="ltx_tr">
<td id="A1.T6.1.9.7.1" class="ltx_td ltx_align_left ltx_border_t">KVQAmeta</td>
<td id="A1.T6.1.9.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">as is</td>
<td id="A1.T6.1.9.7.3" class="ltx_td ltx_align_left ltx_border_t">0.12</td>
<td id="A1.T6.1.9.7.4" class="ltx_td ltx_align_left ltx_border_t">2.77</td>
<td id="A1.T6.1.9.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">8.52</td>
<td id="A1.T6.1.9.7.6" class="ltx_td ltx_align_left ltx_border_t">0.18</td>
<td id="A1.T6.1.9.7.7" class="ltx_td ltx_align_left ltx_border_t">6.30</td>
<td id="A1.T6.1.9.7.8" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">15.56</td>
<td id="A1.T6.1.9.7.9" class="ltx_td ltx_align_center ltx_border_t">.87</td>
</tr>
<tr id="A1.T6.1.10.8" class="ltx_tr">
<td id="A1.T6.1.10.8.1" class="ltx_td ltx_align_left">KVQAmeta</td>
<td id="A1.T6.1.10.8.2" class="ltx_td ltx_align_left ltx_border_r">links</td>
<td id="A1.T6.1.10.8.3" class="ltx_td ltx_align_left">0.39</td>
<td id="A1.T6.1.10.8.4" class="ltx_td ltx_align_left">4.26</td>
<td id="A1.T6.1.10.8.5" class="ltx_td ltx_align_left ltx_border_r">12.96</td>
<td id="A1.T6.1.10.8.6" class="ltx_td ltx_align_left">4.06</td>
<td id="A1.T6.1.10.8.7" class="ltx_td ltx_align_left">12.57</td>
<td id="A1.T6.1.10.8.8" class="ltx_td ltx_align_left ltx_border_r">23.80</td>
<td id="A1.T6.1.10.8.9" class="ltx_td ltx_align_center">.95</td>
</tr>
<tr id="A1.T6.1.11.9" class="ltx_tr">
<td id="A1.T6.1.11.9.1" class="ltx_td ltx_align_left ltx_border_b">KVQAmeta</td>
<td id="A1.T6.1.11.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">noisy</td>
<td id="A1.T6.1.11.9.3" class="ltx_td ltx_align_left ltx_border_b">0.15</td>
<td id="A1.T6.1.11.9.4" class="ltx_td ltx_align_left ltx_border_b">5.15</td>
<td id="A1.T6.1.11.9.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">23.75</td>
<td id="A1.T6.1.11.9.6" class="ltx_td ltx_align_left ltx_border_b">0.42</td>
<td id="A1.T6.1.11.9.7" class="ltx_td ltx_align_left ltx_border_b">10.02</td>
<td id="A1.T6.1.11.9.8" class="ltx_td ltx_align_left ltx_border_b ltx_border_r">36.19</td>
<td id="A1.T6.1.11.9.9" class="ltx_td ltx_align_center ltx_border_b">.99</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>KVQA entity knowledge injection explainability on split 1 for various entity span sets. For instance, 11.48 % of inference questions have E-BERT entities in their top 5 tokens for the NERper plain entity set model and overall 78% of questions in that entity set have E-BERT injected entities.</figcaption>
</figure>
<figure id="A1.F3" class="ltx_figure"><img src="/html/2112.06888/assets/x3.png" id="A1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example 1 of a KVQA question where E-BERT is beneficial for KVQAmeta noisy entity set model. The rows show visual and token explanations for BM-GAE and TRF over the question/text (left column) and the 5 variants â€œQuestion", â€œ+Caption", NERagro, NERper and KVQAmeta we explore . Next to each models name is their prediction and whether this top1 prediction is correct (1) or not, and then whether the correct answer exists in the top 5 predictions of the model which are additionally shown along with their logit values. Below that we see the top 5 most important tokens found by the explanation method followed by the set of Entities used for possible knowledge injection.</figcaption>
</figure>
<figure id="A1.F4" class="ltx_figure"><img src="/html/2112.06888/assets/x4.png" id="A1.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="196" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>More examples of KVQA questions by question type</figcaption>
</figure>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2112.06888/assets/x5.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Example 2 of BM-GAE and TRF explanations for a KVQA question where E-BERT is beneficial for KVQAmeta</figcaption>
</figure>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2112.06888/assets/x6.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Example 1 of BM-GAE and TRF explanations for a KVQA question where E-BERT is harmful for KVQAmeta</figcaption>
</figure>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2112.06888/assets/x7.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="437" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Example 2 of BM-GAE and TRF explanations for a KVQA question where E-BERT is harmful for KVQAmeta</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2112.06887" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2112.06888" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2112.06888">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2112.06888" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2112.06889" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 15:53:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
