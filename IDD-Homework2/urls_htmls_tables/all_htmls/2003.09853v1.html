<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2003.09853] Visual Question Answering for Cultural Heritage</title><meta property="og:description" content="Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Visual Question Answering for Cultural Heritage">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Visual Question Answering for Cultural Heritage">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2003.09853">

<!--Generated on Sat Mar  9 08:19:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Visual Question Answering for Cultural Heritage</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pietro Bongini
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Federico Becattini
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Andrew D. Bagdanov
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Alberto Del Bimbo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Media Integration and Communication Center (MICC) - University of Florence.
<br class="ltx_break">Viale Morgagni 65, Florence, Italy.
</span>p.bongini@unifi.it, federico.becattini@unifi, andrew.bagdanov@unifi.it, alberto.delbimbo@unifi.it</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Technology and the fruition of cultural heritage are becoming increasingly more entwined, especially with the advent of smart audio guides, virtual and augmented reality, and interactive installations. Machine learning and computer vision are important components of this ongoing integration, enabling new interaction modalities between user and museum.
Nonetheless, the most frequent way of interacting with paintings and statues still remains taking pictures. Yet images alone can only convey the aesthetics of the artwork, lacking is information which is often required to fully understand and appreciate it. Usually this additional knowledge comes both from the artwork itself (and therefore the image depicting it) and from an external source of knowledge, such as an information sheet. While the former can be inferred by computer vision algorithms, the latter needs more structured data to pair visual content with relevant information.
Regardless of its source, this information still must be be effectively transmitted to the user. A popular emerging trend in computer vision is Visual Question Answering (VQA), in which users can interact with a neural network by posing questions in natural language and receiving answers about the visual content. We believe that this will be the evolution of smart audio guides for museum visits and simple image browsing on personal smartphones. This will turn the classic audio guide into a smart personal instructor with which the visitor can interact by asking for explanations focused on specific interests. The advantages are twofold: on the one hand the cognitive burden of the visitor will decrease, limiting the flow of information to what the user actually wants to hear; and on the other hand it proposes the most natural way of interacting with a guide, favoring engagement.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Museum visits have adapted throughout the years to exploit technological advances. Nowadays cultural heritage heavily relies on some form of multimedia content to deliver information to the user in ways that limit cognitive burden and engage the visitor as much as possible. This is especially true for young visitors, where gamification techniques have often proven effective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Technology can help bridge the gap between user interests and the message the museum wants to convey.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Videos, 3D reconstructions and augmented realities, among others, have become an integral part of the visit, which has now shifted its focus not solely on artworks but also on how they are organized and presented. To offer a richer experience, smart audio guides have also been developed, gradually replacing information sheets or offering some sort of augmented visit relying on sensors available on personal smartphones.
Despite the increasing diffusion of devices to help guide the visitor, the most effective way to convey most information still remains a human guide with whom the visitor may interact to ask for clarifications or deeper discussions on topics of interest. In fact, the user requires a natural way to interact with whomever is providing the information, be it an actual museum guide or a piece of software.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">At the same time, the diffusion of personal assistants on smartphones is aiding an increasing number of people with everyday tasks. These assistants, though, still offer little or no help in the area of cultural heritage. This is due to the need to process complex pieces of structured information, which are often transversal to several domains.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Machine Learning is starting to reach out to the complexity of these tasks. In particular the emerging topic of <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">Visual Question Answering</em> is able to engage a user by answering questions about visual media  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. VQA algorithms merge the capabilities of Computer Vision to understand image content and those of Natural Language Processing to reason about questions and provide relevant answers. VQA builds upon the Question Answering literature, where questions are answered related to text instead of visual content.
Interest in VQA has grown quickly, but it has still not been applied to cultural heritage since the knowledge required to answer the variety of questions a user might ask about an artwork are not contained within the opera itself. A full understanding requires external knowledge usually obtainable only from experts (e.g. museum guides) or information sheets.
This knowledge can be processed separately since it is often available in a textual form, whether it is provided directly from the museum or retrieved from online resources.
Therefore, to be able to address the dual nature of the task, i.e. answering to both visual and contextual questions, VQA and QA must be combined.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In this work we make a first step towards the development of a Visual Question Answering model for cultural heritage by combining the capabilities of a VQA model and a QA model.
Our first contribution is to introduce a module that accurately discriminates between visual and contextual questions. Our second contribution is to design a model made of two branches able to answer to both kinds of questions. Our experiments demonstrate the effectiveness of our technique for question classification the performance of our general question answering model.
To evaluate our model, we annotated a subset of ArtPedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> with visual and contextual question-answer pairs.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our dataset will be publicly released upon publication of this work.</span></span></span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">In the next section we briefly review works from the literature relevant to our contribution. In section <a href="#S3" title="3 Method ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> we describe our approach to integrating Visual and Contextual Question Answering and Contextual for the cultural heritage domain, and in section <a href="#S4" title="4 Experimental results ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> we report on a number of experiments we performed to quantify the performance of our approach. We conclude in section <a href="#S5" title="5 Conclusions ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> with a discussion of our contribution.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Visual Question Answering (VQA) is an emerging topic which aims at automatically answering open-ended questions about a specific image. Together with image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, VQA is the main point of contact between the communities of Natural Language Processing (NLP) and Computer Vision.
This intersection consists in a great variety of sub-tasks like question reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, object recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, etc.
For this reason, VQA requires a high-level understanding of images and questions. A limitation of Visual Question Answering techniques is that they are not able to answer questions whose answers require knowledge external to the image (e.g. Who is the man in the image? How is this person called?). These kind of questions are arguably the most interesting since humans are likely to ask questions about what they are not able to deduce from the image.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Most approaches in VQA are based on Deep Learning and use Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to interpret images and Recurrent Neural Networks (RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to interpret sentences or phrases. The extracted visual and textual feature vectors are then typically jointly embedded by concatenation, element-wise sum, or product to then infer an answer. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> Anderson <span id="S2.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> designed a bottom-up attention mechanism based on salient objects in the images. In particular, instead of considering the entire image divided in cells (as done in previous methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>) they use object features as attention candidates. These features are extracted using a detector such as Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> trained on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. This technique was an important step forward for the VQA community and increased VQA performance considerably. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> the authors use the bottom-up objects candidates together with the modules that they developed to achieve state-of-the-art performance.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section we describe our approach to open-ended visual question answering. We first show the general model that characterizes our technique then we describe the sub-modules.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Visual Question Answering with visual and contextual questions</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The main idea of this work is to classify the type (visual or contextual) of the input question so that the question can be answered by the most suitable sub-model. We rely on a question classifier to understand if the question concerns exclusively visual traits of an image or if an external source of information is needed to provide a correct answer. The question is then fed to a VQA or a QA model, depending on the output of the classifier. In both cases the question must be analyzed and understood, yet the usage for two separate architectures is driven by the need to process different additional sources of information. If the question is visual, then the answer is generated from the image, whereas if the question is contextual then the answer is generated using external textual descriptions.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2003.09853/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="186" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model overview. A question classifier categorizes the question as visual or contextual. The correspondent module is used to answer the question relying either on the image or external descriptions.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The overall pipeline (see figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Visual Question Answering with visual and contextual questions ‣ 3 Method ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) used by our approach to answer a question is the following:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Question Classification</span>. The question is given in input to the question classifier module that determines if the question is contextual or visual.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">[Visual] Question Answering</span>. Depending on the predicted question type, the corresponding module is activated to generate the answer.</p>
<ol id="S3.I1.i2.I1" class="ltx_enumerate">
<li id="S3.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S3.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i1.p1.1" class="ltx_p">If the question is contextual, the question is given in input to a Question Answering module that takes in input also an external information useful to answer the question. This system produces an output answer only based on this external information.</p>
</div>
</li>
<li id="S3.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S3.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.I1.i2.p1.1" class="ltx_p">If the question is visual, the question and the image are given as input to a Visual Question Answering module. This system produces an output answer based on the content of the image.</p>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Question Classifier Module</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The question classifier module consists of a Bert <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> module for text classification. BERT makes use of a Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, an attention mechanism that learns contextual relations between words (or sub-words) in a text. The Transformer is trained in a bidirectional way in order to have a deeper knowledge of language context and flow. This language model is extremely versatile since it can be used for different tasks like text classification, next word in sentence prediction, question answering and entity recognition. This model is turned into a question classification architecture by adding a classification layer on top of the Transformer output. The input question is represented as the sum of three different embeddings: the token embeddings, the segmentation embeddings and the position embeddings. Moreover, two special tokens are added at the start and in the end of the question.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Contextual Question Answering Module</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The Model used for the Question Answering task is another Bert module that focuses on this task. In this case the module takes in input both a question and a textual description. Since this system uses the textual information to answer the question, the text must contain relevant information to generate an appropriate answer.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Visual Question Answering Module</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The architecture of the Visual Question Answering module is similar to the one used by Anderson et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in their Bottom-up Top-Down approach. Here the salient regions of the image are extracted by a Faster R-CNN  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> pre-trained on the Visual Genome dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The words of the question are represented with a Glove embedding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> and then the question is encoded by a Gated Recurrent Unit (GRU) to condense each question into a fixed size descriptor. An attention mechanism between the encoded question and the salient image regions is developed to weigh the candidate regions that are useful to answer the question. Then the weighted region representations and the question representation are projected into a common space and are joined with an element-wise product. Finally the joint representation passes two fully connected layers and a softmax activation that produces the output answer.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2003.09853/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Sample outputs of the three components of our architecture. Correct answers are shown in black, wrong answers in red.</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we describe experiments conducted to evaluate the performance of our approach. We first introduce the datasets used for training and testing our network, then we describe the protocols adopted for the experiments and the obtained results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">For our experiments we used the standard VQA v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and Artpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, a dataset containing images of famous paintings.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">VQA v2</h5>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">This dataset contains 443,757 training questions/answers referred to 82,783 training images. The number of test examples is about the same of the training examples, instead the validation examples are about the half. Each image has more questions referred to it and these are of multiple types like relation between objects, activity recognition, counting, object detection and so on. Each question is answered by ten annotators and the given answers compose the ground truth. VQA v2 is currently the most used benchmark for Visual Question Answering tasks.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">OK-VQA</h5>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">OK-VQA is a subset of the VQA v2 dataset and it contains 14,055 open-ended questions where each of these has five ground truth answers. In particular OK-VQA contains all the questions of VQA v2 that cannot be answered with processing only the corresponding image but require external knowledge. We use OK-VQA jointly with the original VQA dataset to obtain sets of questions related to the image (visual questions) or to external knowledge (contextual questions).</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Artpedia</h5>

<div id="S4.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px3.p1.1" class="ltx_p">The Artpedia dataset contains a collection of 2,930 paintings, each associated to a variable number of textual descriptions collected from WikiPedia. Each sentence is labelled either as a visual sentence or as a contextual sentence, if it does not describe the visual content of the artwork. Contextual sentences can describe the historical context of the artwork, its author, the artistic influence or the place where the painting is exhibited. The dataset contains a total of 28,212 sentences, 9,173 labelled as visual sentences and the remaining 19,039 as contextual sentences. This is not a Visual Question Answering dataset, so we manually annotated a subset of images with both visual and contextual question-answer pairs, based on the available images and descriptions.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Experimental protocols</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.3" class="ltx_p">Our model is composed by three sub-modules: the question classifier that classifies if a question requires visual or contextual information, the question answering module which answers to contextual questions and the visual question answering module which answers to visual questions. The three modules generate different outputs and we evaluate each one of them independently. The Visual Question Answering module answers with short sentences of at most three words chosen from the set of answers. For this reason, as common practice in the VQA literature, we can consider the problem as a classification task and estimate the accuracy to asses its performance:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="Accuracy=\frac{N_{c}}{N_{a}}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><mrow id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1a" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.4" xref="S4.E1.m1.1.1.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1b" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.5" xref="S4.E1.m1.1.1.2.5.cmml">u</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1c" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.6" xref="S4.E1.m1.1.1.2.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1d" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.7" xref="S4.E1.m1.1.1.2.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1e" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.8" xref="S4.E1.m1.1.1.2.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.1f" xref="S4.E1.m1.1.1.2.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.9" xref="S4.E1.m1.1.1.2.9.cmml">y</mi></mrow><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mfrac id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml">N</mi><mi id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml">c</mi></msub><msub id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml">N</mi><mi id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml">a</mi></msub></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><times id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2.1"></times><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝐴</ci><ci id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3">𝑐</ci><ci id="S4.E1.m1.1.1.2.4.cmml" xref="S4.E1.m1.1.1.2.4">𝑐</ci><ci id="S4.E1.m1.1.1.2.5.cmml" xref="S4.E1.m1.1.1.2.5">𝑢</ci><ci id="S4.E1.m1.1.1.2.6.cmml" xref="S4.E1.m1.1.1.2.6">𝑟</ci><ci id="S4.E1.m1.1.1.2.7.cmml" xref="S4.E1.m1.1.1.2.7">𝑎</ci><ci id="S4.E1.m1.1.1.2.8.cmml" xref="S4.E1.m1.1.1.2.8">𝑐</ci><ci id="S4.E1.m1.1.1.2.9.cmml" xref="S4.E1.m1.1.1.2.9">𝑦</ci></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><divide id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3"></divide><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2">𝑁</ci><ci id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3">𝑐</ci></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2">𝑁</ci><ci id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3">𝑎</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">Accuracy=\frac{N_{c}}{N_{a}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p1.2" class="ltx_p">where <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="N_{c}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msub id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">N</mi><mi id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">𝑁</ci><ci id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">N_{c}</annotation></semantics></math> is the number of correct answers and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="N_{a}" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><msub id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml">N</mi><mi id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2">𝑁</ci><ci id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">N_{a}</annotation></semantics></math> the number of total answers.
The same metric can be used for the question classifier module, since it solves a binary classification task.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.3" class="ltx_p">The question answering model instead, since it can potentially rely on structured and more complex information from the meta-data, is able to answer to questions with more words, articulating short sentences. For this reason we evaluate its performance not only with Accuracy but also with F1-measure, a metric that takes into account the global correctness of the answer:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="F1=2\times\frac{Precision\times Recall}{Precision+Recall}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">​</mo><mn id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mn id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">×</mo><mfrac id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml"><mrow id="S4.E2.m1.1.1.3.3.2" xref="S4.E2.m1.1.1.3.3.2.cmml"><mrow id="S4.E2.m1.1.1.3.3.2.2" xref="S4.E2.m1.1.1.3.3.2.2.cmml"><mrow id="S4.E2.m1.1.1.3.3.2.2.2" xref="S4.E2.m1.1.1.3.3.2.2.2.cmml"><mi id="S4.E2.m1.1.1.3.3.2.2.2.2" xref="S4.E2.m1.1.1.3.3.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.3" xref="S4.E2.m1.1.1.3.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1a" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.4" xref="S4.E2.m1.1.1.3.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1b" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.5" xref="S4.E2.m1.1.1.3.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1c" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.6" xref="S4.E2.m1.1.1.3.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1d" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.7" xref="S4.E2.m1.1.1.3.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1e" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.8" xref="S4.E2.m1.1.1.3.3.2.2.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1f" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.9" xref="S4.E2.m1.1.1.3.3.2.2.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.2.2.1g" xref="S4.E2.m1.1.1.3.3.2.2.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.2.2.10" xref="S4.E2.m1.1.1.3.3.2.2.2.10.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.E2.m1.1.1.3.3.2.2.1" xref="S4.E2.m1.1.1.3.3.2.2.1.cmml">×</mo><mi id="S4.E2.m1.1.1.3.3.2.2.3" xref="S4.E2.m1.1.1.3.3.2.2.3.cmml">R</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1" xref="S4.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.3" xref="S4.E2.m1.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1a" xref="S4.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.4" xref="S4.E2.m1.1.1.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1b" xref="S4.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.5" xref="S4.E2.m1.1.1.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1c" xref="S4.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.6" xref="S4.E2.m1.1.1.3.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1d" xref="S4.E2.m1.1.1.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.2.7" xref="S4.E2.m1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S4.E2.m1.1.1.3.3.3" xref="S4.E2.m1.1.1.3.3.3.cmml"><mrow id="S4.E2.m1.1.1.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.2.cmml"><mi id="S4.E2.m1.1.1.3.3.3.2.2" xref="S4.E2.m1.1.1.3.3.3.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.3" xref="S4.E2.m1.1.1.3.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1a" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.4" xref="S4.E2.m1.1.1.3.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1b" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.5" xref="S4.E2.m1.1.1.3.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1c" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.6" xref="S4.E2.m1.1.1.3.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1d" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.7" xref="S4.E2.m1.1.1.3.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1e" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.8" xref="S4.E2.m1.1.1.3.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1f" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.9" xref="S4.E2.m1.1.1.3.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.2.1g" xref="S4.E2.m1.1.1.3.3.3.2.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.2.10" xref="S4.E2.m1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S4.E2.m1.1.1.3.3.3.1" xref="S4.E2.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S4.E2.m1.1.1.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.cmml"><mi id="S4.E2.m1.1.1.3.3.3.3.2" xref="S4.E2.m1.1.1.3.3.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.3.3" xref="S4.E2.m1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1a" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.3.4" xref="S4.E2.m1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1b" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.3.5" xref="S4.E2.m1.1.1.3.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1c" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.3.6" xref="S4.E2.m1.1.1.3.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.3.3.1d" xref="S4.E2.m1.1.1.3.3.3.3.1.cmml">​</mo><mi id="S4.E2.m1.1.1.3.3.3.3.7" xref="S4.E2.m1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">𝐹</ci><cn type="integer" id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">1</cn></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><times id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></times><cn type="integer" id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2">2</cn><apply id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3"><divide id="S4.E2.m1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3"></divide><apply id="S4.E2.m1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2"><times id="S4.E2.m1.1.1.3.3.2.1.cmml" xref="S4.E2.m1.1.1.3.3.2.1"></times><apply id="S4.E2.m1.1.1.3.3.2.2.cmml" xref="S4.E2.m1.1.1.3.3.2.2"><times id="S4.E2.m1.1.1.3.3.2.2.1.cmml" xref="S4.E2.m1.1.1.3.3.2.2.1"></times><apply id="S4.E2.m1.1.1.3.3.2.2.2.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2"><times id="S4.E2.m1.1.1.3.3.2.2.2.1.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.1"></times><ci id="S4.E2.m1.1.1.3.3.2.2.2.2.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.2">𝑃</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.3.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.3">𝑟</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.4.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.4">𝑒</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.5.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.5">𝑐</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.6.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.6">𝑖</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.7.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.7">𝑠</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.8.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.8">𝑖</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.9.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.9">𝑜</ci><ci id="S4.E2.m1.1.1.3.3.2.2.2.10.cmml" xref="S4.E2.m1.1.1.3.3.2.2.2.10">𝑛</ci></apply><ci id="S4.E2.m1.1.1.3.3.2.2.3.cmml" xref="S4.E2.m1.1.1.3.3.2.2.3">𝑅</ci></apply><ci id="S4.E2.m1.1.1.3.3.2.3.cmml" xref="S4.E2.m1.1.1.3.3.2.3">𝑒</ci><ci id="S4.E2.m1.1.1.3.3.2.4.cmml" xref="S4.E2.m1.1.1.3.3.2.4">𝑐</ci><ci id="S4.E2.m1.1.1.3.3.2.5.cmml" xref="S4.E2.m1.1.1.3.3.2.5">𝑎</ci><ci id="S4.E2.m1.1.1.3.3.2.6.cmml" xref="S4.E2.m1.1.1.3.3.2.6">𝑙</ci><ci id="S4.E2.m1.1.1.3.3.2.7.cmml" xref="S4.E2.m1.1.1.3.3.2.7">𝑙</ci></apply><apply id="S4.E2.m1.1.1.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3"><plus id="S4.E2.m1.1.1.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3.1"></plus><apply id="S4.E2.m1.1.1.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.2"><times id="S4.E2.m1.1.1.3.3.3.2.1.cmml" xref="S4.E2.m1.1.1.3.3.3.2.1"></times><ci id="S4.E2.m1.1.1.3.3.3.2.2.cmml" xref="S4.E2.m1.1.1.3.3.3.2.2">𝑃</ci><ci id="S4.E2.m1.1.1.3.3.3.2.3.cmml" xref="S4.E2.m1.1.1.3.3.3.2.3">𝑟</ci><ci id="S4.E2.m1.1.1.3.3.3.2.4.cmml" xref="S4.E2.m1.1.1.3.3.3.2.4">𝑒</ci><ci id="S4.E2.m1.1.1.3.3.3.2.5.cmml" xref="S4.E2.m1.1.1.3.3.3.2.5">𝑐</ci><ci id="S4.E2.m1.1.1.3.3.3.2.6.cmml" xref="S4.E2.m1.1.1.3.3.3.2.6">𝑖</ci><ci id="S4.E2.m1.1.1.3.3.3.2.7.cmml" xref="S4.E2.m1.1.1.3.3.3.2.7">𝑠</ci><ci id="S4.E2.m1.1.1.3.3.3.2.8.cmml" xref="S4.E2.m1.1.1.3.3.3.2.8">𝑖</ci><ci id="S4.E2.m1.1.1.3.3.3.2.9.cmml" xref="S4.E2.m1.1.1.3.3.3.2.9">𝑜</ci><ci id="S4.E2.m1.1.1.3.3.3.2.10.cmml" xref="S4.E2.m1.1.1.3.3.3.2.10">𝑛</ci></apply><apply id="S4.E2.m1.1.1.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3"><times id="S4.E2.m1.1.1.3.3.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.3.3.1"></times><ci id="S4.E2.m1.1.1.3.3.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.3.3.2">𝑅</ci><ci id="S4.E2.m1.1.1.3.3.3.3.3.cmml" xref="S4.E2.m1.1.1.3.3.3.3.3">𝑒</ci><ci id="S4.E2.m1.1.1.3.3.3.3.4.cmml" xref="S4.E2.m1.1.1.3.3.3.3.4">𝑐</ci><ci id="S4.E2.m1.1.1.3.3.3.3.5.cmml" xref="S4.E2.m1.1.1.3.3.3.3.5">𝑎</ci><ci id="S4.E2.m1.1.1.3.3.3.3.6.cmml" xref="S4.E2.m1.1.1.3.3.3.3.6">𝑙</ci><ci id="S4.E2.m1.1.1.3.3.3.3.7.cmml" xref="S4.E2.m1.1.1.3.3.3.3.7">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">F1=2\times\frac{Precision\times Recall}{Precision+Recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.2" class="ltx_p">where <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="Precision" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1a" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.4" xref="S4.SS2.p2.1.m1.1.1.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1b" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.5" xref="S4.SS2.p2.1.m1.1.1.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1c" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.6" xref="S4.SS2.p2.1.m1.1.1.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1d" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.7" xref="S4.SS2.p2.1.m1.1.1.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1e" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.8" xref="S4.SS2.p2.1.m1.1.1.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1f" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.9" xref="S4.SS2.p2.1.m1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.1.m1.1.1.1g" xref="S4.SS2.p2.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.1.m1.1.1.10" xref="S4.SS2.p2.1.m1.1.1.10.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><times id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></times><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">𝑃</ci><ci id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">𝑟</ci><ci id="S4.SS2.p2.1.m1.1.1.4.cmml" xref="S4.SS2.p2.1.m1.1.1.4">𝑒</ci><ci id="S4.SS2.p2.1.m1.1.1.5.cmml" xref="S4.SS2.p2.1.m1.1.1.5">𝑐</ci><ci id="S4.SS2.p2.1.m1.1.1.6.cmml" xref="S4.SS2.p2.1.m1.1.1.6">𝑖</ci><ci id="S4.SS2.p2.1.m1.1.1.7.cmml" xref="S4.SS2.p2.1.m1.1.1.7">𝑠</ci><ci id="S4.SS2.p2.1.m1.1.1.8.cmml" xref="S4.SS2.p2.1.m1.1.1.8">𝑖</ci><ci id="S4.SS2.p2.1.m1.1.1.9.cmml" xref="S4.SS2.p2.1.m1.1.1.9">𝑜</ci><ci id="S4.SS2.p2.1.m1.1.1.10.cmml" xref="S4.SS2.p2.1.m1.1.1.10">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">Precision</annotation></semantics></math> is defined as the number of correct words divided by the length of the answer and <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="Recall" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mrow id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml"><mi id="S4.SS2.p2.2.m2.1.1.2" xref="S4.SS2.p2.2.m2.1.1.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.3" xref="S4.SS2.p2.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1a" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.4" xref="S4.SS2.p2.2.m2.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1b" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.5" xref="S4.SS2.p2.2.m2.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1c" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.6" xref="S4.SS2.p2.2.m2.1.1.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m2.1.1.1d" xref="S4.SS2.p2.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS2.p2.2.m2.1.1.7" xref="S4.SS2.p2.2.m2.1.1.7.cmml">l</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><apply id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1"><times id="S4.SS2.p2.2.m2.1.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1.1"></times><ci id="S4.SS2.p2.2.m2.1.1.2.cmml" xref="S4.SS2.p2.2.m2.1.1.2">𝑅</ci><ci id="S4.SS2.p2.2.m2.1.1.3.cmml" xref="S4.SS2.p2.2.m2.1.1.3">𝑒</ci><ci id="S4.SS2.p2.2.m2.1.1.4.cmml" xref="S4.SS2.p2.2.m2.1.1.4">𝑐</ci><ci id="S4.SS2.p2.2.m2.1.1.5.cmml" xref="S4.SS2.p2.2.m2.1.1.5">𝑎</ci><ci id="S4.SS2.p2.2.m2.1.1.6.cmml" xref="S4.SS2.p2.2.m2.1.1.6">𝑙</ci><ci id="S4.SS2.p2.2.m2.1.1.7.cmml" xref="S4.SS2.p2.2.m2.1.1.7">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">Recall</annotation></semantics></math> as the number of correct words divided by the length of the ground truth.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1.1" class="ltx_td ltx_border_r"></td>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r">OK-VQA/VQA v2</td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center">Artpedia</td>
</tr>
<tr id="S4.T1.1.2.2" class="ltx_tr">
<td id="S4.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Question Classifier</td>
<td id="S4.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.868</td>
<td id="S4.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">0.938</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.3.1" class="ltx_text ltx_font_bold">Question classifier:</span> accuracy of our question classifier on questions from both the OK-VQA and VQA v2 datasets and from Artpedia.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">QA model</span></th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">VQA model</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center">
<table id="S4.T2.1.2.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.1.1.1.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r">Contextual</td>
<td id="S4.T2.1.2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr">Visual</td>
<td id="S4.T2.1.2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r">Accuracy</td>
<td id="S4.T2.1.2.1.1.1.1.4" class="ltx_td ltx_align_center">F1-score</td>
</tr>
<tr id="S4.T2.1.2.1.1.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1.1.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.2.1.1.1.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">✗</td>
<td id="S4.T2.1.2.1.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.684</td>
<td id="S4.T2.1.2.1.1.1.2.4" class="ltx_td ltx_align_center ltx_border_t">0.832</td>
</tr>
<tr id="S4.T2.1.2.1.1.1.3" class="ltx_tr">
<td id="S4.T2.1.2.1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S4.T2.1.2.1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_rr">✓</td>
<td id="S4.T2.1.2.1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r">0.176</td>
<td id="S4.T2.1.2.1.1.1.3.4" class="ltx_td ltx_align_center">0.150</td>
</tr>
<tr id="S4.T2.1.2.1.1.1.4" class="ltx_tr">
<td id="S4.T2.1.2.1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.2.1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_rr">✓</td>
<td id="S4.T2.1.2.1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r">0.504</td>
<td id="S4.T2.1.2.1.1.1.4.4" class="ltx_td ltx_align_center">0.417</td>
</tr>
</table>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center">
<table id="S4.T2.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.1.2.1.1" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.1.1" class="ltx_td ltx_align_center ltx_border_r">Contextual</td>
<td id="S4.T2.1.2.1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_rr">Visual</td>
<td id="S4.T2.1.2.1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_r">Accuracy</td>
</tr>
<tr id="S4.T2.1.2.1.2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T2.1.2.1.2.1.2.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">✗</td>
<td id="S4.T2.1.2.1.2.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.000</td>
</tr>
<tr id="S4.T2.1.2.1.2.1.3" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.3.1" class="ltx_td ltx_align_center ltx_border_r">✗</td>
<td id="S4.T2.1.2.1.2.1.3.2" class="ltx_td ltx_align_center ltx_border_rr">✓</td>
<td id="S4.T2.1.2.1.2.1.3.3" class="ltx_td ltx_align_center ltx_border_r">0.524</td>
</tr>
<tr id="S4.T2.1.2.1.2.1.4" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.4.1" class="ltx_td ltx_align_center ltx_border_r">✓</td>
<td id="S4.T2.1.2.1.2.1.4.2" class="ltx_td ltx_align_center ltx_border_rr">✓</td>
<td id="S4.T2.1.2.1.2.1.4.3" class="ltx_td ltx_align_center ltx_border_r">0.251</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results of the two answering models on contextual questions, visual questions and both visual and contextual questions from Artpedia. Note that the VQA model does not have access to the external information required to answer the contextual questions, making it unable to answer correctly. See section <a href="#S4.SS3.SSS4" title="4.3.4 Full pipeline ‣ 4.3 Experimental results ‣ 4 Experimental results ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.4</span></a> for analysis of the performance of our full model on combined Visual/Contextual Question Answering</figcaption>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Experimental results</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In order to evaluate the performance of our model we make different experiments. We measure the performance of the model analyzing each component independently.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Question Classifier</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">We train the question classifier module with questions of both the OK-VQA and VQA v2 datasets. We take from VQA v2 a number of visual questions equal to the number of questions that require external knowledge from OK-VQA. The obtained dataset is then split into train and test sets.
The question classifier is supposed to understand from the structure of the question whether the answer concerns the visual content or not. This is a generic classifier, agnostic from the domain of the task. In fact, VQA v2 and OK-VQA contain generic images, while we are interested in applications in the cultural heritage domain. We demonstrate the effectiveness of our approach and its ability to transfer to the cultural heritage domain by evaluating it both on the VQA/OK-VQA dataset and on a new dataset comprised of a subset of Artpedia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Since this dataset does not contain questions but only images and descriptions, we took 30 images from this dataset and annotated them with a variable number of both visual and contextual questions (from 3 to 5 for both categories).
The accuracy of our question classifier module is shown in Tab. <a href="#S4.T1" title="Table 1 ‣ 4.2 Experimental protocols ‣ 4 Experimental results ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We can observe that it is able to predict the type of the question correctly in most cases.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Contextual Question Answering</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">We test our question answering module on the subset of Artpedia containing 30 images that we annotated. In particular, we test the accuracy of our module in three different experiments: test on contextual questions, test on visual questions and test with both visual and contextual questions. Note that the outputs of the visual and contextual modules are different, since VQA is treated as a classification problem, while for QA
From the results shown in Tab. <a href="#S4.T2" title="Table 2 ‣ 4.2 Experimental protocols ‣ 4 Experimental results ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> we can deduce that our question answering module works very well with contextual questions and obtains worse results with visual questions. This can be justified from the fact that visual questions refer to visible details of paintings that cannot be described in visual sentences of ArtPedia.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Visual Question Answering</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Similarly to the tests conducted for the question answering module, we evaluate the visual question answering module on both visual and contextual questions. In Table <a href="#S4.T2" title="Table 2 ‣ 4.2 Experimental protocols ‣ 4 Experimental results ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> results of our visual question answering model are shown. We can observe that conversely from the question answering module this model performs well on visual questions and is not able to answer correctly to contextual questions. This is motivated by the fact that contextual questions require external knowledge (e.g. author, year) that a purely visual question answering engine does not have access to.</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4 </span>Full pipeline</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Finally, we combine the capabilities of all the modules together and we test on both visual and contextual questions, obtaining an accuracy of 0.570.
The full pipeline, thanks to the question classifier, is able to correctly distinguish between visual and contextual questions. The visual question answering module and the question answering module receive as input almost all questions that they are able to answer (contextual question for the question answering module and visual questions for visual question answering module). For this reason the complete model exceeds the performances of both single answering modules.
Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.4 Visual Question Answering Module ‣ 3 Method ‣ Visual Question Answering for Cultural Heritage" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows some qualitative result of the three components of the pipeline. The components correctly handle most of the questions but some common failure cases can be observed. For instance the Question Answering model might add details to the answer that are not present in the ground truth and the Visual Question Answering model might confuse some elements of the painting with similar objects.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper we presented an approach for Visual Question Answering in the Cultural Heritage domain. We have addressed two important issues: the need to process both both image and contextual knowledge contained and the lack of data availability. The model we presented combines the capabilities of a VQA and a QA model, relying on a question classifier to predict whether it refers to visual or contextual content. To assess the effectiveness of our model we annotated a subset of the ArtPedia dataset with visual and contextual question-answer pairs.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. This work was partially supported by the project ARS01_00421: “PON IDEHA - Innovazioni per l’elaborazione dei dati nel settore del Patrimonio Culturale.”</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">References</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and L. Zhang.

</span>
<span class="ltx_bibblock">Bottom-up and top-down attention for image captioning and visual
question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 6077–6086, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and
D. Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Becattini, A. Ferracani, L. Landucci, D. Pezzatini, T. Uricchio, and
A. Del Bimbo.

</span>
<span class="ltx_bibblock">Imaging novecento. a mobile app for automatic recognition of artworks
and transfer of artistic styles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Euro-Mediterranean Conference</span>, pages 781–791. Springer,
2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. Cadene, H. Ben-Younes, M. Cord, and N. Thome.

</span>
<span class="ltx_bibblock">Murel: Multimodal relational reasoning for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 1989–1998, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell.

</span>
<span class="ltx_bibblock">Decaf: A deep convolutional activation feature for generic visual
recognition, 2013.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Making the v in vqa matter: Elevating the role of image understanding
in visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 6904–6913, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</span>, 2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu.

</span>
<span class="ltx_bibblock">Advanced deep-learning techniques for salient and category-specific
object detection: a survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">IEEE Signal Processing Magazine</span>, 35(1):84–100, 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Ioannides, N. Magnenat-Thalmann, and G. Papagiannakis.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Mixed Reality and Gamification for Cultural Heritage</span>, volume 2.

</span>
<span class="ltx_bibblock">Springer, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
S. R. Kheradpisheh, M. Ganjtabesh, S. J. Thorpe, and T. Masquelier.

</span>
<span class="ltx_bibblock">Stdp-based spiking deep convolutional neural networks for object
recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Neural Networks</span>, 99:56–67, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,
Y. Kalantidis, L.-J. Li, D. A. Shamma, M. Bernstein, and L. Fei-Fei.

</span>
<span class="ltx_bibblock">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.

</span>
<span class="ltx_bibblock">2016.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Liu, Z. Zhu, N. Ye, S. Guadarrama, and K. Murphy.

</span>
<span class="ltx_bibblock">Improved image captioning via policy gradient optimization of spider.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</span>, pages 873–881, 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh.

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>, pages
289–297, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille.

</span>
<span class="ltx_bibblock">Deep captioning with multimodal recurrent neural networks (m-rnn),
2014.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3195–3204, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Pennington, R. Socher, and C. Manning.

</span>
<span class="ltx_bibblock">Glove: Global vectors for word representation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2014 conference on empirical methods in
natural language processing (EMNLP)</span>, pages 1532–1543, 2014.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun.

</span>
<span class="ltx_bibblock">Faster r-cnn: Towards real-time object detection with region proposal
networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
91–99, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, pages 4613–4621, 2016.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M. Stefanini, M. Cornia, L. Baraldi, M. Corsini, and R. Cucchiara.

</span>
<span class="ltx_bibblock">Artpedia: A new visual-semantic dataset with visual and contextual
sentences in the artistic domain.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">International Conference on Image Analysis and Processing</span>,
pages 729–740. Springer, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
5998–6008, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian.

</span>
<span class="ltx_bibblock">Deep modular co-attention networks for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 6281–6290, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2003.09852" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2003.09853" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2003.09853">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2003.09853" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2003.09854" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 08:19:08 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
