<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.12191] Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization</title><meta property="og:description" content="Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such mod…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.12191">

<!--Generated on Mon Mar 11 13:41:55 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Reassessing Evaluation Practices in Visual Question Answering:
<br class="ltx_break">A Case Study on Out-of-Distribution Generalization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aishwarya Agrawal<math id="id3.3.m1.1" class="ltx_math_unparsed" alttext="{}^{~{},\ddagger,\diamondsuit}" display="inline"><semantics id="id3.3.m1.1a"><msup id="id3.3.m1.1.1"><mi id="id3.3.m1.1.1a"></mi><mrow id="id3.3.m1.1.1.1"><mo rspace="0em" id="id3.3.m1.1.1.1.1">,</mo><mo lspace="0em" rspace="0em" id="id3.3.m1.1.1.1.2">‡</mo><mo id="id3.3.m1.1.1.1.3">,</mo><mi mathvariant="normal" id="id3.3.m1.1.1.1.4">♢</mi></mrow></msup><annotation encoding="application/x-tex" id="id3.3.m1.1b">{}^{~{},\ddagger,\diamondsuit}</annotation></semantics></math>  Ivana Kajić<sup id="id13.13.id1" class="ltx_sup"><span id="id13.13.id1.1" class="ltx_text ltx_font_italic">∗,♢</span></sup>  Emanuele Bugliarello<sup id="id14.14.id2" class="ltx_sup"><span id="id14.14.id2.1" class="ltx_text ltx_font_italic">∗,△</span></sup>
<br class="ltx_break"><span id="id9.9.4" class="ltx_text ltx_font_bold">Elnaz Davoodi<sup id="id9.9.4.1" class="ltx_sup"><span id="id9.9.4.1.1" class="ltx_text ltx_font_medium ltx_font_italic">†,♢</span></sup>  Anita Gergely<sup id="id9.9.4.2" class="ltx_sup"><span id="id9.9.4.2.1" class="ltx_text ltx_font_medium ltx_font_italic">†,♢</span></sup>  Phil Blunsom<sup id="id9.9.4.3" class="ltx_sup"><span id="id9.9.4.3.1" class="ltx_text ltx_font_medium ltx_font_italic">♮</span></sup>  Aida Nematzadeh<sup id="id9.9.4.4" class="ltx_sup"><span id="id9.9.4.4.1" class="ltx_text ltx_font_medium ltx_font_italic">∗,‡,♢</span></sup>
<br class="ltx_break"></span>
<sup id="id15.15.id3" class="ltx_sup"><span id="id15.15.id3.1" class="ltx_text ltx_font_italic">♢</span></sup>DeepMind  <sup id="id16.16.id4" class="ltx_sup"><span id="id16.16.id4.1" class="ltx_text ltx_font_italic">△</span></sup>University of Copenhagen   <sup id="id17.17.id5" class="ltx_sup"><span id="id17.17.id5.1" class="ltx_text ltx_font_italic">♮</span></sup>University of Oxford
<br class="ltx_break">
<span id="id18.18.id6" class="ltx_text ltx_font_typewriter">{aiagrawal,kivana,nematzadeh}@deepmind.com</span>   
<span id="id19.19.id7" class="ltx_text ltx_font_typewriter">emanuele@di.ku.dk</span>
</span><span class="ltx_author_notes">  denotes equal first author contribution. <sup id="id20.20.id1" class="ltx_sup">†</sup> denotes equal contribution. <sup id="id21.21.id2" class="ltx_sup">‡</sup> denotes equal senior contribution. Detailed contributions follow at the end of the paper.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">Vision-and-language (V&amp;L) models pretrained on large-scale multimodal data have demonstrated strong performance on various tasks such as image captioning and visual question answering (VQA). The quality of such models is commonly assessed by measuring their performance on unseen data that typically comes from the same distribution as the training data. However, when evaluated under out-of-distribution (out-of-dataset) settings for VQA, we observe that these models exhibit poor generalization. We comprehensively evaluate two pretrained V&amp;L models under different settings (i.e. classification and open-ended text generation) by conducting cross-dataset evaluations. We find that these models tend to learn to solve the benchmark, rather than learning the high-level skills required by the VQA task. We also find that in most cases generative models are less susceptible to shifts in data distribution compared to discriminative ones, and that multimodal pretraining is generally helpful for OOD generalization. Finally, we revisit assumptions underlying the use of automatic VQA evaluation metrics, and empirically show that their stringent nature repeatedly penalizes models for correct responses.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Visual Question Answering (VQA) is the task of automatically answering natural language open-ended questions about images. Tackling VQA involves multiple skills, such as language and visual understanding, integrating information between the two (vision and language) modalities, and commonsense and knowledge based reasoning. One of the goals of the VQA research has been fostering the development of systems that are able to answer <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">any open-ended question about any image</em>. This motivation has inspired a fruitful line of research in designing VQA benchmarks <cite class="ltx_cite ltx_citemacro_cite">(<em id="S1.p1.1.2.1" class="ltx_emph ltx_font_italic">e.g.</em>, Malinowski and Fritz, <a href="#bib.bib33" title="" class="ltx_ref">2014</a>; Antol et al., <a href="#bib.bib8" title="" class="ltx_ref">2015</a>; Krishna et al., <a href="#bib.bib27" title="" class="ltx_ref">2017</a>; Goyal et al., <a href="#bib.bib20" title="" class="ltx_ref">2017</a>; Hudson and Manning, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> and models <cite class="ltx_cite ltx_citemacro_cite">(<em id="S1.p1.1.3.1" class="ltx_emph ltx_font_italic">e.g.</em>, Yang et al., <a href="#bib.bib52" title="" class="ltx_ref">2015</a>; Anderson et al., <a href="#bib.bib7" title="" class="ltx_ref">2018</a>; Lu et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>; Cho et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we investigate if recent pretrained VQA models can indeed answer any open-ended question about images or if they are mostly suitable for answering questions from the VQA benchmarks they are optimized for.
In other words, <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">are models learning to solve the task or learning to solve the datasets?</em> We believe the former is more aligned with the goal of building real-world VQA systems.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To measure whether models learn to solve the task of VQA, we believe we need to examine their <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">out-of-distribution (OOD)</em> generalization capabilities: how they perform on examples drawn from a distribution other than that of the training set.
In this work, we extensively evaluate OOD generalization of current pretrained V&amp;L models by conducting cross-dataset evaluations (without any adaptation to the test domain).</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Through our extensive experiments, we provide in-depth discussion on the following questions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><em id="S1.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">How well do recent models generalize under OOD settings?</em> We observe a notable drop in performance from IID to OOD settings across models and benchmarks, demonstrating that models mostly learn to solve specific benchmarks as opposed to learning general skills for answering questions about images. This result is not simply due to a mismatch between the set of answers between the training and test VQA datasets, nor due to poor representation of test answers in VQA training data.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><em id="S1.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Is multimodal pretraining beneficial for OOD generalization?</em> We find that while image–text pretraining is helpful in most OOD settings, it is not always more useful than in IID ones. Moreover, it is least useful for OOD evaluation on the <span id="S1.I1.i2.p1.1.2" class="ltx_text ltx_font_smallcaps">VizWiz</span> benchmark, highlighting the challenges of a real-world benchmark.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><em id="S1.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Is generative modeling more robust to distribution shifts?</em> In most cases, we observe that generative models—which are not bound to predictions over a fixed set of answers curated from the training data—are more robust to OOD evaluation than discriminative (<em id="S1.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">i.e.</em>, classification-based) ones. Moreover, we quantify what the limitations of discriminative models are for real-world VQA applications (<em id="S1.I1.i3.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, answering questions of visually-impaired users), where the answers a deployed model needs to produce cannot be pre-determined.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><em id="S1.I1.i4.p1.1.1" class="ltx_emph ltx_font_italic">Are current automatic VQA metrics too stringent for OOD evaluation?</em> We examine if the performance of our pretrained models is negatively impacted by the current standard VQA accuracy metrics, which match predicted answer strings to a limited number of ground-truth answers. Human evaluation reveals the stringent nature of such accuracy metrics, which is especially pronounced in the OOD settings. Nevertheless, while the IID-to-OOD performance gap is reduced after human evaluation, models still exhibit poor generalization to OOD VQA benchmarks.</p>
</div>
</li>
</ul>
<p id="S1.p4.2" class="ltx_p">We believe our OOD evaluations and supporting analyses expose the shortcomings of current models, and recommend future work to adopt these evaluation practices to provide real-world, robust assessment of VQA systems.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Beyond IID evaluation in VQA.</span>
Previous work has evaluated VQA models beyond the IID setting for robustness to <em id="S2.p1.1.2" class="ltx_emph ltx_font_italic">specific and controlled</em> aspects – novel compositions of seen concepts <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib3" title="" class="ltx_ref">2017</a>); Johnson et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, change in prior distributions of answers per question type <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>); Gokhale et al. (<a href="#bib.bib19" title="" class="ltx_ref">2020</a>); Niu et al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>, adversarial examples provided by humans <cite class="ltx_cite ltx_citemacro_cite">Sheng et al. (<a href="#bib.bib42" title="" class="ltx_ref">2021</a>); Li et al. (<a href="#bib.bib29" title="" class="ltx_ref">2021b</a>)</cite>, consistency, negation, and simple perturbation in questions <cite class="ltx_cite ltx_citemacro_cite">Jimenez et al. (<a href="#bib.bib25" title="" class="ltx_ref">2022</a>)</cite>, counter-examples <cite class="ltx_cite ltx_citemacro_cite">Dancette et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>, and controlled shifts in language and vision modalities <cite class="ltx_cite ltx_citemacro_cite">Akula et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>.
Our focus, however, is to evaluate for <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">overall</em> robustness to OOD data without controlling for specific aspects, by testing our models on different OOD benchmarks. We believe our experimental setting more closely emulates the expected experience of deployed VQA systems. Moreover, when the exact nature of distribution shift between train and test splits is known (such as in <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>), approaches developed to tackle such shifts tend to rely on the explicit knowledge of construction of such OOD splits resulting in inflated sense of progress <cite class="ltx_cite ltx_citemacro_cite">Teney et al. (<a href="#bib.bib44" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Similar to us, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2021</a>); Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> also present some experimental results on VQA OOD evaluation, however they do it in limited manner (<em id="S2.p2.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, do not consider all pairs of datasets, do not evaluate the effect of multimodal pretraining, etc.). To our best knowledge, ours is the first work to extensively quantifying the extent of IID to OOD performance drops in current VQA models and study the effect of several factors: answer overlap, multimodal pretraining, generative vs. discriminative modeling, and stringent evaluation metric.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Domain adaptation in VQA.</span>
Some studies <cite class="ltx_cite ltx_citemacro_cite">Jabri et al. (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>); Chao et al. (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>); Zhang et al. (<a href="#bib.bib53" title="" class="ltx_ref">2021</a>)</cite> have explored domain adaptation of VQA models from one VQA benchmark to another.
Our focus, instead, is on evaluating <em id="S2.p3.1.2" class="ltx_emph ltx_font_italic">zero-shot</em> cross-benchmark generalization <em id="S2.p3.1.3" class="ltx_emph ltx_font_italic">without</em> any adaptation.
This allows us to assess the robustness of current models towards unforeseen distribution shifts.
Our work is similar to that of <cite class="ltx_cite ltx_citemacro_citet">Torralba and Efros (<a href="#bib.bib45" title="" class="ltx_ref">2011</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Hendrycks et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, who study OOD generalization in vision and text.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Zero-shot VQA with pretrained models.</span>
In an emerging line of research <cite class="ltx_cite ltx_citemacro_cite">Tsimpoukelli et al. (<a href="#bib.bib47" title="" class="ltx_ref">2021</a>); Alayrac et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>); Song et al. (<a href="#bib.bib43" title="" class="ltx_ref">2022</a>); Piergiovanni et al. (<a href="#bib.bib37" title="" class="ltx_ref">2022</a>)</cite>, large-scale pretrained unimodal models <cite class="ltx_cite ltx_citemacro_cite">Brown et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>); Radford et al. (<a href="#bib.bib38" title="" class="ltx_ref">2021</a>)</cite> are repurposed to tackle VQA in zero-shot or few-shot fashion.
While such zero-shot VQA evaluations are a better test of generalization than IID evaluations, our focus, differently, is on investigating whether models can generalize to unseen datasets <em id="S2.p4.1.2" class="ltx_emph ltx_font_italic">upon being taught the task by showing examples from one dataset</em>. Moreover, this line of work does not focus on a thorough analysis of models in OOD settings (which is hard to define for these models due to the massive amount of data they are pretrained on).</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we present our framework to examine OOD generalization in VQA.
We examine two pretrained Transformers across five benchmarks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Models</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We evaluate the performance of two representative, widely-used pretrained models that have achieved strong performance in various V&amp;L tasks in the last few years: <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> and <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite>.
We evaluate these models in a broad range of settings (generative/discriminative, w/wo pretraining, and multiple benchmarks), resulting in 128 experiments.
We chose these models as they include components shown to be important in the literature: cross-attention (<span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">ViLBERT</span> and <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">ALBEF</span>), and contrastive learning (<span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_smallcaps">ALBEF</span>).
We note that our goal is to study trends that hold across different models, and we leave for future work controlled comparisons across architectures.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.4" class="ltx_p"><span id="S3.SS1.p2.4.5" class="ltx_text ltx_font_bold ltx_font_smallcaps">ViLBERT</span> is one of the first, yet strong models in the recent pretrain–fine-tune paradigm in V&amp;L.
Its inputs are a sequence of sub-word tokens <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib49" title="" class="ltx_ref">2016</a>)</cite>, and a set of regions of interest given by a Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib39" title="" class="ltx_ref">2015</a>); Anderson et al. (<a href="#bib.bib7" title="" class="ltx_ref">2018</a>)</cite>.
The authors fine-tune it on <span id="S3.SS1.p2.4.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> by learning a classifier over the most frequent answers.
We first re-implement this model successfully, and then extend it to a generative setting by pretraining and fine-tuning a Transformer decoder (more details in <a href="#A1" title="Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">A</span></a>).
We denote the discriminative/generative version as <span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S3.SS1.p2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S3.SS1.p2.1.1.m1.1a"><msub id="S3.SS1.p2.1.1.m1.1.1" xref="S3.SS1.p2.1.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.1.m1.1.1a" xref="S3.SS1.p2.1.1.m1.1.1.cmml"></mi><mtext id="S3.SS1.p2.1.1.m1.1.1.1" xref="S3.SS1.p2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.1.m1.1b"><apply id="S3.SS1.p2.1.1.m1.1.1.cmml" xref="S3.SS1.p2.1.1.m1.1.1"><ci id="S3.SS1.p2.1.1.m1.1.1.1a.cmml" xref="S3.SS1.p2.1.1.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p2.1.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>/<span id="S3.SS1.p2.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S3.SS1.p2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S3.SS1.p2.2.2.m1.1a"><msub id="S3.SS1.p2.2.2.m1.1.1" xref="S3.SS1.p2.2.2.m1.1.1.cmml"><mi id="S3.SS1.p2.2.2.m1.1.1a" xref="S3.SS1.p2.2.2.m1.1.1.cmml"></mi><mtext id="S3.SS1.p2.2.2.m1.1.1.1" xref="S3.SS1.p2.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.2.m1.1b"><apply id="S3.SS1.p2.2.2.m1.1.1.cmml" xref="S3.SS1.p2.2.2.m1.1.1"><ci id="S3.SS1.p2.2.2.m1.1.1.1a.cmml" xref="S3.SS1.p2.2.2.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p2.2.2.m1.1.1.1.cmml" xref="S3.SS1.p2.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>.
Unless otherwise specified, results for <span id="S3.SS1.p2.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S3.SS1.p2.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S3.SS1.p2.3.3.m1.1a"><msub id="S3.SS1.p2.3.3.m1.1.1" xref="S3.SS1.p2.3.3.m1.1.1.cmml"><mi id="S3.SS1.p2.3.3.m1.1.1a" xref="S3.SS1.p2.3.3.m1.1.1.cmml"></mi><mtext id="S3.SS1.p2.3.3.m1.1.1.1" xref="S3.SS1.p2.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.3.m1.1b"><apply id="S3.SS1.p2.3.3.m1.1.1.cmml" xref="S3.SS1.p2.3.3.m1.1.1"><ci id="S3.SS1.p2.3.3.m1.1.1.1a.cmml" xref="S3.SS1.p2.3.3.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p2.3.3.m1.1.1.1.cmml" xref="S3.SS1.p2.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> are from our code base for direct comparison with <span id="S3.SS1.p2.4.4" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S3.SS1.p2.4.4.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S3.SS1.p2.4.4.m1.1a"><msub id="S3.SS1.p2.4.4.m1.1.1" xref="S3.SS1.p2.4.4.m1.1.1.cmml"><mi id="S3.SS1.p2.4.4.m1.1.1a" xref="S3.SS1.p2.4.4.m1.1.1.cmml"></mi><mtext id="S3.SS1.p2.4.4.m1.1.1.1" xref="S3.SS1.p2.4.4.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.4.m1.1b"><apply id="S3.SS1.p2.4.4.m1.1.1.cmml" xref="S3.SS1.p2.4.4.m1.1.1"><ci id="S3.SS1.p2.4.4.m1.1.1.1a.cmml" xref="S3.SS1.p2.4.4.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p2.4.4.m1.1.1.1.cmml" xref="S3.SS1.p2.4.4.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.4.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p"><span id="S3.SS1.p3.3.4" class="ltx_text ltx_font_bold ltx_font_smallcaps">ALBEF</span> is a state-of-the-art V&amp;L encoder whose visual inputs are image patches encoded by a vision Transformer <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>); Touvron et al. (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite> that is jointly trained with the rest of the model.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite> fine-tune <span id="S3.SS1.p3.3.5" class="ltx_text ltx_font_smallcaps">ALBEF</span> on <span id="S3.SS1.p3.3.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> by adding a 6-layer Transformer decoder to generate answers (<span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S3.SS1.p3.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S3.SS1.p3.1.1.m1.1a"><msub id="S3.SS1.p3.1.1.m1.1.1" xref="S3.SS1.p3.1.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.1.m1.1.1a" xref="S3.SS1.p3.1.1.m1.1.1.cmml"></mi><mtext id="S3.SS1.p3.1.1.m1.1.1.1" xref="S3.SS1.p3.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.1.m1.1b"><apply id="S3.SS1.p3.1.1.m1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1"><ci id="S3.SS1.p3.1.1.m1.1.1.1a.cmml" xref="S3.SS1.p3.1.1.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p3.1.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>).
We use the official implementation,<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/salesforce/ALBEF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/ALBEF</a>.</span></span></span> and furthermore train a discriminative variant (<span id="S3.SS1.p3.2.2" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S3.SS1.p3.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S3.SS1.p3.2.2.m1.1a"><msub id="S3.SS1.p3.2.2.m1.1.1" xref="S3.SS1.p3.2.2.m1.1.1.cmml"><mi id="S3.SS1.p3.2.2.m1.1.1a" xref="S3.SS1.p3.2.2.m1.1.1.cmml"></mi><mtext id="S3.SS1.p3.2.2.m1.1.1.1" xref="S3.SS1.p3.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.2.m1.1b"><apply id="S3.SS1.p3.2.2.m1.1.1.cmml" xref="S3.SS1.p3.2.2.m1.1.1"><ci id="S3.SS1.p3.2.2.m1.1.1.1a.cmml" xref="S3.SS1.p3.2.2.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p3.2.2.m1.1.1.1.cmml" xref="S3.SS1.p3.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>) by learning a multi-answer classifier, as in <span id="S3.SS1.p3.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S3.SS1.p3.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S3.SS1.p3.3.3.m1.1a"><msub id="S3.SS1.p3.3.3.m1.1.1" xref="S3.SS1.p3.3.3.m1.1.1.cmml"><mi id="S3.SS1.p3.3.3.m1.1.1a" xref="S3.SS1.p3.3.3.m1.1.1.cmml"></mi><mtext id="S3.SS1.p3.3.3.m1.1.1.1" xref="S3.SS1.p3.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.3.m1.1b"><apply id="S3.SS1.p3.3.3.m1.1.1.cmml" xref="S3.SS1.p3.3.3.m1.1.1"><ci id="S3.SS1.p3.3.3.m1.1.1.1a.cmml" xref="S3.SS1.p3.3.3.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p3.3.3.m1.1.1.1.cmml" xref="S3.SS1.p3.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p">In our analysis, we investigate the role of multimodal pretraining.
<span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> was pretrained on 3M image–text pairs from Conceptual Captions (CC; <cite class="ltx_cite ltx_citemacro_citet">Sharma et al. <a href="#bib.bib41" title="" class="ltx_ref">2018</a></cite>).
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite> released two checkpoints for <span id="S3.SS1.p4.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span>: one pretrained on 4M images from CC, MS-COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib30" title="" class="ltx_ref">2014</a>)</cite>, SBU <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. (<a href="#bib.bib36" title="" class="ltx_ref">2011</a>)</cite> and Visual Genome <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>; the other one is further pretrained on Conceptual 12M <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite> for a total of 14M images.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We also conducted experiments with <span id="footnote2.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> pretrained on same datasets as the 4M <span id="footnote2.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> checkpoint. We found no significant difference compared to the results presented throughout this paper.</span></span></span></p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Datasets and Evaluation Metrics</h3>

<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:208.1pt;height:53.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-69.6pt,18.0pt) scale(0.599334107583244,0.599334107583244) ;">
<table id="S3.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Dataset</span></th>
<th id="S3.T1.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold"># Train (imgs / qns)</span></th>
<th id="S3.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold"># Val (imgs / qns)</span></th>
<th id="S3.T1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold"># Classes</span></th>
<th id="S3.T1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Coverage [%]</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="S3.T1.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">82,783 / 443,757</td>
<td id="S3.T1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">40,504 / 214,354</td>
<td id="S3.T1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">3,129</td>
<td id="S3.T1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">98.07 / 98.07</td>
</tr>
<tr id="S3.T1.1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="S3.T1.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">72,140 / 943,000</td>
<td id="S3.T1.1.1.3.2.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">10,234 / 132,062</td>
<td id="S3.T1.1.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">1,533</td>
<td id="S3.T1.1.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">99.78 / 99.79</td>
</tr>
<tr id="S3.T1.1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="S3.T1.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">59,635 / 868,259</td>
<td id="S3.T1.1.1.4.3.3" class="ltx_td ltx_align_right ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">39,645 / 577,063</td>
<td id="S3.T1.1.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">3,449</td>
<td id="S3.T1.1.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:2.5pt;padding-right:2.5pt;">76.55 / 76.55</td>
</tr>
<tr id="S3.T1.1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="S3.T1.1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="S3.T1.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">20,523 /   20,523</td>
<td id="S3.T1.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:2.5pt;padding-right:2.5pt;">4,319 /     4,319</td>
<td id="S3.T1.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">3,112</td>
<td id="S3.T1.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">96.76 / 97.01</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Datasets statistics. #classes is the number of classes we use for the discriminative models; coverage is the percentage of questions that can be answered with our selected classes in train/validation splits.</figcaption>
</figure>
<figure id="S3.F1" class="ltx_figure">
<div id="S3.F1.1" class="ltx_block">
<img src="/html/2205.12191/assets/figs/%22iid_vs_ood_vilbert_BERT_+_3M.pdf%22" id="S3.F1.g1" class="ltx_graphics ltx_centering" alt="Refer to caption"><img src="/html/2205.12191/assets/figs/%22iid_vs_ood_albef_big_BERT_+_14M.pdf%22" id="S3.F1.g2" class="ltx_graphics ltx_centering" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
IID (highlighted in bold) vs. OOD performance. Top: <span id="S3.F1.4.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> pretrained on CC. Bottom: <span id="S3.F1.5.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> pretrained on CC, VG, SBU, MS-COCO and C12M datasets. All models are initialized with BERT weights.</figcaption>
</figure>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">We ground our analysis on five diverse VQA datasets: <span id="S3.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite>, <span id="S3.SS2.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">GQA</span> <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite>, <span id="S3.SS2.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">Visual Genome</span> (<span id="S3.SS2.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">VG</span>; <cite class="ltx_cite ltx_citemacro_citet">Krishna et al. <a href="#bib.bib27" title="" class="ltx_ref">2017</a></cite>), <span id="S3.SS2.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_smallcaps">VizWiz</span> <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> and <span id="S3.SS2.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_smallcaps">VQA-CP</span> <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>.
<span id="S3.SS2.SSS0.Px1.p1.1.7" class="ltx_text ltx_font_smallcaps">VQAv2</span> is the most commonly used VQA dataset to date.
<span id="S3.SS2.SSS0.Px1.p1.1.8" class="ltx_text ltx_font_smallcaps">VQA-CP</span> re-splits it such that, for every question type, train and test sets have different prior distributions of answers.
<span id="S3.SS2.SSS0.Px1.p1.1.9" class="ltx_text ltx_font_smallcaps">VG</span> includes questions centered around either the full image or a specific region.
<span id="S3.SS2.SSS0.Px1.p1.1.10" class="ltx_text ltx_font_smallcaps">GQA</span> is a large-scale dataset that focuses on compositionality of template-generated questions.
Finally, <span id="S3.SS2.SSS0.Px1.p1.1.11" class="ltx_text ltx_font_smallcaps">VizWiz</span> is the only real-world VQA dataset, collected from visually-impaired people.
<span id="S3.SS2.SSS0.Px1.p1.1.12" class="ltx_text ltx_font_smallcaps">VG</span> and <span id="S3.SS2.SSS0.Px1.p1.1.13" class="ltx_text ltx_font_smallcaps">GQA</span> have one answer per question, while the other datasets include 10 answers per question.
See <a href="#S3.T1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and <a href="#A1" title="Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">A</span></a> for more details.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p">There are several differences among these datasets.
Both <span id="S3.SS2.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="S3.SS2.SSS0.Px1.p2.1.2" class="ltx_text ltx_font_smallcaps">GQA</span> mostly have one-word answers (89% and 81%, respectively) whilst there are fewer in <span id="S3.SS2.SSS0.Px1.p2.1.3" class="ltx_text ltx_font_smallcaps">VG</span> (57%) and <span id="S3.SS2.SSS0.Px1.p2.1.4" class="ltx_text ltx_font_smallcaps">VizWiz</span> (67%).
The type of questions also varies: <span id="S3.SS2.SSS0.Px1.p2.1.5" class="ltx_text ltx_font_smallcaps">VG</span> does not contain binary ‘yes/no’ questions, but rather spans 6 WH-questions.
By design, <span id="S3.SS2.SSS0.Px1.p2.1.6" class="ltx_text ltx_font_smallcaps">GQA</span> questions require more compositional skills but do not test for counting; while <span id="S3.SS2.SSS0.Px1.p2.1.7" class="ltx_text ltx_font_smallcaps">VizWiz</span> questions are more conversational as they were collected through a speech interface and has a significant proportion of OCR questions (21%).
Moreover, a significant number of <span id="S3.SS2.SSS0.Px1.p2.1.8" class="ltx_text ltx_font_smallcaps">VizWiz</span> questions (28%) are <em id="S3.SS2.SSS0.Px1.p2.1.9" class="ltx_emph ltx_font_italic">unanswerable</em> due to the challenges faced by the visually-impaired users in taking pictures, resulting in poor focus, poor lighting or entirely missing the entity of interest.
As such, the distribution of images in <span id="S3.SS2.SSS0.Px1.p2.1.10" class="ltx_text ltx_font_smallcaps">VizWiz</span> is different from other datasets.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation metrics.</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">These VQA benchmarks compute model accuracy between its prediction and the ground-truth answer(s) by string matching (after simple pre-processing).
<span id="S3.SS2.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="S3.SS2.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>, each with 10 answers per question, account for diversity in ground-truth answers by scoring a given model answer as <math id="S3.SS2.SSS0.Px2.p1.1.m1.2" class="ltx_Math" alttext="\texttt{min}\{1.0,0.3\times\texttt{count}\}" display="inline"><semantics id="S3.SS2.SSS0.Px2.p1.1.m1.2a"><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3a.cmml">min</mtext><mo lspace="0em" rspace="0em" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml">​</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">{</mo><mn id="S3.SS2.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">1.0</mn><mo id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">,</mo><mrow id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.cmml"><mn id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2.cmml">0.3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.cmml">×</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3a.cmml">count</mtext></mrow><mo stretchy="false" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.4" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p1.1.m1.2b"><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2"><times id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.2"></times><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3a.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.3">min</mtext></ci><set id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1"><cn type="float" id="S3.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.1.1">1.0</cn><apply id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1"><times id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1"></times><cn type="float" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.2">0.3</cn><ci id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3a.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.3">count</mtext></ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p1.1.m1.2c">\texttt{min}\{1.0,0.3\times\texttt{count}\}</annotation></semantics></math>, where <span id="S3.SS2.SSS0.Px2.p1.1.3" class="ltx_text ltx_font_typewriter">count</span> is the number of annotators that used that answer.
For <span id="S3.SS2.SSS0.Px2.p1.1.4" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="S3.SS2.SSS0.Px2.p1.1.5" class="ltx_text ltx_font_smallcaps">VG</span>, both with one answer per question, we use top-1 accuracy.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We note that <span id="footnote3.1" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="footnote3.2" class="ltx_text ltx_font_smallcaps">VG</span> propose top-5 accuracy. We, instead, opt for top-1 accuracy to keep a consistent setup with <span id="footnote3.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="footnote3.4" class="ltx_text ltx_font_smallcaps">VizWiz</span>. And we believe top-5 accuracy is impractical for many applications, such as answering questions for visually-impaired users.</span></span></span></p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.2" class="ltx_p">Following common practice, for discriminative models, we select the top-<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">k</annotation></semantics></math> most frequent answers as the set of answer classes to perform classification over.
Here <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">k</annotation></semantics></math> is a dataset-dependent variable, chosen to cover most of the questions (see <a href="#S3.T1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a>).
All models are trained on the respective training sets and evaluated on the validation sets.
For <span id="S3.SS3.p1.2.1" class="ltx_text ltx_font_smallcaps">VG</span>, we randomly split the data into training and validation (60%/40%) with no image is in both splits.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Out-of-Distribution Generalization</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We examine to what extent our models learn to solve a specific VQA benchmark by latching on dataset-specific correlations, as opposed to learning more general skills required in VQA.
We fine-tune a pretrained model on the train split of one benchmark (<em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">GQA</span>) and evaluate it on the validation split of a different one (<em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, <span id="S4.p1.1.4" class="ltx_text ltx_font_smallcaps">VG</span>).
Overall, we evaluate models by fine-tuning them on each benchmark and testing them against all benchmarks.
If pretrained models are indeed learning the VQA skill, we expect to see a small drop in performance between the IID and OOD settings.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.3" class="ltx_p">The results are presented in <a href="#S3.F1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>, with different evaluation benchmarks grouped on the <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.p2.1.m1.1a"><mi id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">x</annotation></semantics></math>-axis. First, across all models and for each benchmark, we see a notable drop in the VQA accuracy from the IID to the OOD setting. While such a drop might be anticipated, we found the extent of the drop surprising given the impressive performance of current pretrained VL models.
For all models shown, the largest drops are observed when evaluating models on the <span id="S4.p2.3.2" class="ltx_text ltx_font_smallcaps">VizWiz</span> benchmark.
Moreover, even the smallest performance drop, which happens when fine-tuning models on <span id="S4.p2.3.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> and evaluating them on <span id="S4.p2.3.4" class="ltx_text ltx_font_smallcaps">VG</span>, remains relatively large (<em id="S4.p2.3.5" class="ltx_emph ltx_font_italic">i.e.</em>, <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="5.3" display="inline"><semantics id="S4.p2.2.m2.1a"><mn id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml">5.3</mn><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><cn type="float" id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1">5.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">5.3</annotation></semantics></math> points for <span id="S4.p2.3.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S4.p2.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S4.p2.3.1.m1.1a"><msub id="S4.p2.3.1.m1.1.1" xref="S4.p2.3.1.m1.1.1.cmml"><mi id="S4.p2.3.1.m1.1.1a" xref="S4.p2.3.1.m1.1.1.cmml"></mi><mtext id="S4.p2.3.1.m1.1.1.1" xref="S4.p2.3.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p2.3.1.m1.1b"><apply id="S4.p2.3.1.m1.1.1.cmml" xref="S4.p2.3.1.m1.1.1"><ci id="S4.p2.3.1.m1.1.1.1a.cmml" xref="S4.p2.3.1.m1.1.1.1"><mtext mathsize="70%" id="S4.p2.3.1.m1.1.1.1.cmml" xref="S4.p2.3.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>).
These results show that <em id="S4.p2.3.6" class="ltx_emph ltx_font_italic">pretrained models are largely learning the fine-tuning benchmark without learning to solve the VQA task</em>.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Second, we observe that fine-tuning on <span id="S4.p3.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> results in the lowest drop in IID to OOD performance across all conditions—the <span id="S4.p3.1.2" class="ltx_text ltx_font_smallcaps">VQAv2</span> bar (shown in blue in <a href="#S3.F1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) is the closet to the IID one for <span id="S4.p3.1.3" class="ltx_text ltx_font_smallcaps">GQA</span>, <span id="S4.p3.1.4" class="ltx_text ltx_font_smallcaps">VG</span>, and <span id="S4.p3.1.5" class="ltx_text ltx_font_smallcaps">VizWiz</span>.
We conclude that fine-tuning on <span id="S4.p3.1.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> yields a model that best generalizes to OOD settings in our benchmarks. This result is not simply due to the size of the fine-tuning benchmark as <span id="S4.p3.1.7" class="ltx_text ltx_font_smallcaps">VG</span> is larger than <span id="S4.p3.1.8" class="ltx_text ltx_font_smallcaps">VQAv2</span>.
Similarly, all the models achieve highest OOD performance on <span id="S4.p3.1.9" class="ltx_text ltx_font_smallcaps">VQAv2</span>.
We conjecture that <span id="S4.p3.1.10" class="ltx_text ltx_font_smallcaps">VQAv2</span> is the most diverse benchmark of our selection.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluating on Shared Answer Sets</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2205.12191/assets/figs/%22in_vocab_all.pdf%22" id="S4.F2.g1" class="ltx_graphics ltx_centering" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
IID (#) vs OOD performance when controlling for the shared shared answer set. Solid bars are as in <a href="#S3.F1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>; stacked dotted bars are improvements when evaluating on questions with shared answer sets between IID and OOD settings. For IID, the shared answer set is computed with respect to a dataset denoted with *.</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">Discriminative models treat VQA as a multi-answer classification task over the set of top-<math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">k</annotation></semantics></math> most frequent answers in the fine-tuning data.
This limits their performance: if a certain answer is not frequent in the fine-tuning data, a discriminative model will perform poorly for such an answer during test time.
While this limitation also affects IID evaluation, we expect it to have a stronger effect in OOD generalization (due to potentially different answer distributions between the fine-tuning and test sets).
We next examine to what extent this limitation affects OOD performance by controlling for the mismatch in answer sets between the fine-tuning and test sets.
We do so by considering only the test questions whose answers are included in the top-<math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">k</annotation></semantics></math> answers of a given fine-tuning dataset (for more details, see <a href="#A2.SS0.SSS0.Px1" title="Evaluation with Shared Answer Sets ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a>).</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><a href="#S4.F2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows the improvement in the VQA accuracy over the IID and OOD evaluation accuracy (in <a href="#S3.F1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a>) when controlling for the shared answer set.
For IID evaluation, only one intersection of answer sets is reported, corresponding to the smallest gap between IID and OOD evaluation, with remaining numbers reported in <a href="#A2.T10" title="In Effect of pretraining data size on ALBEF ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a> (<a href="#A2" title="Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a>).
Thus, the difference between the height of the IID bar (#) and the OOD bar (*) with respect to which answer intersection between IID and OOD is computed, represents the best case scenario for OOD generalization, <em id="S4.SS1.p2.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, the least drop from IID to OOD.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">We observe a similar pattern across the models: in most cases, using a shared answer set improves the performance.
<em id="S4.SS1.p3.1.1" class="ltx_emph ltx_font_italic">Overall, we still observe a notable gap between the OOD and IID settings for the best case OOD generalization scenario, showing that a shared answer set does not circumvent the difficulty of OOD generalization for these models.</em>
A few cases where IID evaluations with a shared answer set hurt performance are discussed in <a href="#A2" title="Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a>.
When evaluating on the shared answer set, we further examine if the drop in accuracy from IID to OOD is due to the low frequency of the test answer classes in the OOD fine-tuning set. The details of the correlation computation and the results are explained in <a href="#A2.SS0.SSS0.Px2" title="Answer Frequency Correlation ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a> and <a href="#A2.T9" title="In Answer Frequency Correlation ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">9</span></a>, respectively.
This result indicates that frequency of the answer class is a contributing factor to the weak OOD generalization, but we also explore other causes in <a href="#S7" title="7 Qualitative Analysis ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.3.1.1" class="ltx_tr">
<th id="S4.T2.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="S4.T2.3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="S4.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="S4.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="S4.T2.3.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.3.2.1" class="ltx_tr">
<th id="S4.T2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="S4.T2.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">92.9</td>
<td id="S4.T2.3.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">96.7</td>
<td id="S4.T2.3.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">65.1</td>
<td id="S4.T2.3.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">43.6</td>
</tr>
<tr id="S4.T2.3.3.2" class="ltx_tr">
<th id="S4.T2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="S4.T2.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">73.5</td>
<td id="S4.T2.3.3.2.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">99.9</td>
<td id="S4.T2.3.3.2.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">44.8</td>
<td id="S4.T2.3.3.2.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">36.6</td>
</tr>
<tr id="S4.T2.3.4.3" class="ltx_tr">
<th id="S4.T2.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="S4.T2.3.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">52.7</td>
<td id="S4.T2.3.4.3.3" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">62.4</td>
<td id="S4.T2.3.4.3.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">74.2</td>
<td id="S4.T2.3.4.3.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">32.3</td>
</tr>
<tr id="S4.T2.3.5.4" class="ltx_tr">
<th id="S4.T2.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S4.T2.3.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="S4.T2.3.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">79.4</td>
<td id="S4.T2.3.5.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">82.5</td>
<td id="S4.T2.3.5.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">40.9</td>
<td id="S4.T2.3.5.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">86.2</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Maximum achievable accuracy for all test answers based on the top-<math id="S4.T2.2.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.T2.2.m1.1b"><mi id="S4.T2.2.m1.1.1" xref="S4.T2.2.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T2.2.m1.1c"><ci id="S4.T2.2.m1.1.1.cmml" xref="S4.T2.2.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.m1.1d">k</annotation></semantics></math> answers present in the respective fine-tuning sets. Rows correspond to fine-tuning datasets, columns correspond to the test benchmarks</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>The Case for the Generative Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">A discriminative model cannot correctly answer questions for which the answers lie outside the predefined top-<math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation></semantics></math> classes; therefore, by treating VQA as a classification task, we can define the upper-bound performance of discriminative models on VQA by computing the accuracy given all answers in the test set being answered correctly.
The upper-bound VQA accuracy is shown in <a href="#S4.T2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>; we observe a large drop from IID to OOD evaluations for most conditions.
<span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> has the lowest achievable accuracies in OOD evaluation.</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div id="S4.F3.1" class="ltx_block">
<img src="/html/2205.12191/assets/figs/%22pretraining_score_drop_vilbert_BERT_+_3M.pdf%22" id="S4.F3.g1" class="ltx_graphics ltx_centering" alt="Refer to caption"><img src="/html/2205.12191/assets/figs/%22pretraining_score_drop_albef_big_BERT_+_14M.pdf%22" id="S4.F3.g2" class="ltx_graphics ltx_centering" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Percentage point difference in VQA accuracy between models with and without multimodal pretraining, for OOD and IID (highlighted in bold) evaluations. All models are initialized with BERT weights.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2205.12191/assets/figs/%22iid_vs_ood_gaps.pdf%22" id="S4.F4.g1" class="ltx_graphics ltx_centering" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Difference in <math id="S4.F4.2.m1.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.F4.2.m1.1b"><mrow id="S4.F4.2.m1.1.1" xref="S4.F4.2.m1.1.1.cmml"><mi mathvariant="normal" id="S4.F4.2.m1.1.1.2" xref="S4.F4.2.m1.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.F4.2.m1.1.1.1" xref="S4.F4.2.m1.1.1.1.cmml">​</mo><mtext id="S4.F4.2.m1.1.1.3" xref="S4.F4.2.m1.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.F4.2.m1.1c"><apply id="S4.F4.2.m1.1.1.cmml" xref="S4.F4.2.m1.1.1"><times id="S4.F4.2.m1.1.1.1.cmml" xref="S4.F4.2.m1.1.1.1"></times><ci id="S4.F4.2.m1.1.1.2.cmml" xref="S4.F4.2.m1.1.1.2">Δ</ci><ci id="S4.F4.2.m1.1.1.3a.cmml" xref="S4.F4.2.m1.1.1.3"><mtext id="S4.F4.2.m1.1.1.3.cmml" xref="S4.F4.2.m1.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F4.2.m1.1d">\Delta\,\text{OOD}</annotation></semantics></math> values between discriminative and generative models.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">However, our <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S4.SS2.p2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S4.SS2.p2.1.1.m1.1a"><msub id="S4.SS2.p2.1.1.m1.1.1" xref="S4.SS2.p2.1.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.1.m1.1.1a" xref="S4.SS2.p2.1.1.m1.1.1.cmml"></mi><mtext id="S4.SS2.p2.1.1.m1.1.1.1" xref="S4.SS2.p2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.1.m1.1b"><apply id="S4.SS2.p2.1.1.m1.1.1.cmml" xref="S4.SS2.p2.1.1.m1.1.1"><ci id="S4.SS2.p2.1.1.m1.1.1.1a.cmml" xref="S4.SS2.p2.1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p2.1.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> and <span id="S4.SS2.p2.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S4.SS2.p2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S4.SS2.p2.2.2.m1.1a"><msub id="S4.SS2.p2.2.2.m1.1.1" xref="S4.SS2.p2.2.2.m1.1.1.cmml"><mi id="S4.SS2.p2.2.2.m1.1.1a" xref="S4.SS2.p2.2.2.m1.1.1.cmml"></mi><mtext id="S4.SS2.p2.2.2.m1.1.1.1" xref="S4.SS2.p2.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.2.m1.1b"><apply id="S4.SS2.p2.2.2.m1.1.1.cmml" xref="S4.SS2.p2.2.2.m1.1.1"><ci id="S4.SS2.p2.2.2.m1.1.1.1a.cmml" xref="S4.SS2.p2.2.2.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p2.2.2.m1.1.1.1.cmml" xref="S4.SS2.p2.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> models still perform notably worse than maximum achievable accuracy in all settings (smallest gap of 21.5% across all conditions, see <a href="#A2.F7" title="In Maximum Achievable Scores ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">7</span></a> in <a href="#A2" title="Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a>); <em id="S4.SS2.p2.2.3" class="ltx_emph ltx_font_italic">as a result, the poor OOD performance in the discriminative setting is not simply due to the low maximum achievable accuracy</em>.
We conclude that the common practice of modeling VQA as a classification task severely limits the generalization capability of models to new datasets. On the other hand, generative models do not suffer from a fixed class set. They can generate a larger set of answers—all words for which the tokens occur in the pretraining data, including those that are out-of-vocabulary for the given VQA fine-tune datasets.
We argue that generative modeling is a more promising solution for real-world application of VQA; similarly, recent work has identified text generation as a way to unify various V&amp;L tasks <cite class="ltx_cite ltx_citemacro_citep">(<em id="S4.SS2.p2.2.4.1" class="ltx_emph ltx_font_italic">e.g.</em>, Cho et al., <a href="#bib.bib14" title="" class="ltx_ref">2021</a>; Wang et al., <a href="#bib.bib48" title="" class="ltx_ref">2022</a>; Alayrac et al., <a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.11" class="ltx_p">We next ask <em id="S4.SS2.p3.2.2" class="ltx_emph ltx_font_italic">whether our <span id="S4.SS2.p3.1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S4.SS2.p3.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S4.SS2.p3.1.1.1.m1.1a"><msub id="S4.SS2.p3.1.1.1.m1.1.1" xref="S4.SS2.p3.1.1.1.m1.1.1.cmml"><mi id="S4.SS2.p3.1.1.1.m1.1.1a" xref="S4.SS2.p3.1.1.1.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S4.SS2.p3.1.1.1.m1.1.1.1" xref="S4.SS2.p3.1.1.1.m1.1.1.1b.cmml"><em id="S4.SS2.p3.1.1.1.m1.1.1.1.1nest" class="ltx_emph ltx_font_italic" style="font-size:70%;">GEN</em></mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.1.1.m1.1b"><apply id="S4.SS2.p3.1.1.1.m1.1.1.cmml" xref="S4.SS2.p3.1.1.1.m1.1.1"><ci id="S4.SS2.p3.1.1.1.m1.1.1.1b.cmml" xref="S4.SS2.p3.1.1.1.m1.1.1.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS2.p3.1.1.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.1.1.m1.1.1.1"><em id="S4.SS2.p3.1.1.1.m1.1.1.1.1anest" class="ltx_emph ltx_font_italic" style="font-size:70%;">GEN</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> and <span id="S4.SS2.p3.2.2.2" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S4.SS2.p3.2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S4.SS2.p3.2.2.2.m1.1a"><msub id="S4.SS2.p3.2.2.2.m1.1.1" xref="S4.SS2.p3.2.2.2.m1.1.1.cmml"><mi id="S4.SS2.p3.2.2.2.m1.1.1a" xref="S4.SS2.p3.2.2.2.m1.1.1.cmml"></mi><mtext class="ltx_mathvariant_italic" id="S4.SS2.p3.2.2.2.m1.1.1.1" xref="S4.SS2.p3.2.2.2.m1.1.1.1b.cmml"><em id="S4.SS2.p3.2.2.2.m1.1.1.1.1nest" class="ltx_emph ltx_font_italic" style="font-size:70%;">GEN</em></mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.2.2.m1.1b"><apply id="S4.SS2.p3.2.2.2.m1.1.1.cmml" xref="S4.SS2.p3.2.2.2.m1.1.1"><ci id="S4.SS2.p3.2.2.2.m1.1.1.1b.cmml" xref="S4.SS2.p3.2.2.2.m1.1.1.1"><mtext class="ltx_mathvariant_italic" mathsize="70%" id="S4.SS2.p3.2.2.2.m1.1.1.1.cmml" xref="S4.SS2.p3.2.2.2.m1.1.1.1"><em id="S4.SS2.p3.2.2.2.m1.1.1.1.1anest" class="ltx_emph ltx_font_italic" style="font-size:70%;">GEN</em></mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> models are more successful in OOD generalization compared to their discriminative counterparts</em>. For each model (<em id="S4.SS2.p3.11.4" class="ltx_emph ltx_font_italic">i.e.</em>, generative/discriminative <span id="S4.SS2.p3.11.5" class="ltx_text ltx_font_smallcaps">ALBEF</span>/<span id="S4.SS2.p3.11.6" class="ltx_text ltx_font_smallcaps">ViLBERT</span>), we first calculate the gap between the IID setting and each OOD setting (<em id="S4.SS2.p3.11.7" class="ltx_emph ltx_font_italic">i.e.</em>, <math id="S4.SS2.p3.3.m1.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.3.m1.1a"><mrow id="S4.SS2.p3.3.m1.1.1" xref="S4.SS2.p3.3.m1.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.3.m1.1.1.2" xref="S4.SS2.p3.3.m1.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.3.m1.1.1.1" xref="S4.SS2.p3.3.m1.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.3.m1.1.1.3" xref="S4.SS2.p3.3.m1.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.3.m1.1b"><apply id="S4.SS2.p3.3.m1.1.1.cmml" xref="S4.SS2.p3.3.m1.1.1"><times id="S4.SS2.p3.3.m1.1.1.1.cmml" xref="S4.SS2.p3.3.m1.1.1.1"></times><ci id="S4.SS2.p3.3.m1.1.1.2.cmml" xref="S4.SS2.p3.3.m1.1.1.2">Δ</ci><ci id="S4.SS2.p3.3.m1.1.1.3a.cmml" xref="S4.SS2.p3.3.m1.1.1.3"><mtext id="S4.SS2.p3.3.m1.1.1.3.cmml" xref="S4.SS2.p3.3.m1.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.3.m1.1c">\Delta\,\text{OOD}</annotation></semantics></math>), resulting in three values per benchmark. For instance, for the <span id="S4.SS2.p3.11.8" class="ltx_text ltx_font_smallcaps">VQAv2</span> benchmark, <math id="S4.SS2.p3.4.m2.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.4.m2.1a"><mrow id="S4.SS2.p3.4.m2.1.1" xref="S4.SS2.p3.4.m2.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.4.m2.1.1.2" xref="S4.SS2.p3.4.m2.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.4.m2.1.1.1" xref="S4.SS2.p3.4.m2.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.4.m2.1.1.3" xref="S4.SS2.p3.4.m2.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.4.m2.1b"><apply id="S4.SS2.p3.4.m2.1.1.cmml" xref="S4.SS2.p3.4.m2.1.1"><times id="S4.SS2.p3.4.m2.1.1.1.cmml" xref="S4.SS2.p3.4.m2.1.1.1"></times><ci id="S4.SS2.p3.4.m2.1.1.2.cmml" xref="S4.SS2.p3.4.m2.1.1.2">Δ</ci><ci id="S4.SS2.p3.4.m2.1.1.3a.cmml" xref="S4.SS2.p3.4.m2.1.1.3"><mtext id="S4.SS2.p3.4.m2.1.1.3.cmml" xref="S4.SS2.p3.4.m2.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.4.m2.1c">\Delta\,\text{OOD}</annotation></semantics></math> numbers are calculated between the model fine-tuned on <span id="S4.SS2.p3.11.9" class="ltx_text ltx_font_smallcaps">VQAv2</span> and those fined-tuned on <span id="S4.SS2.p3.11.10" class="ltx_text ltx_font_smallcaps">VG</span>, <span id="S4.SS2.p3.11.11" class="ltx_text ltx_font_smallcaps">GQA</span>, and <span id="S4.SS2.p3.11.12" class="ltx_text ltx_font_smallcaps">VizWiz</span>. Note that the higher the <math id="S4.SS2.p3.5.m3.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.5.m3.1a"><mrow id="S4.SS2.p3.5.m3.1.1" xref="S4.SS2.p3.5.m3.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.5.m3.1.1.2" xref="S4.SS2.p3.5.m3.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.5.m3.1.1.1" xref="S4.SS2.p3.5.m3.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.5.m3.1.1.3" xref="S4.SS2.p3.5.m3.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.5.m3.1b"><apply id="S4.SS2.p3.5.m3.1.1.cmml" xref="S4.SS2.p3.5.m3.1.1"><times id="S4.SS2.p3.5.m3.1.1.1.cmml" xref="S4.SS2.p3.5.m3.1.1.1"></times><ci id="S4.SS2.p3.5.m3.1.1.2.cmml" xref="S4.SS2.p3.5.m3.1.1.2">Δ</ci><ci id="S4.SS2.p3.5.m3.1.1.3a.cmml" xref="S4.SS2.p3.5.m3.1.1.3"><mtext id="S4.SS2.p3.5.m3.1.1.3.cmml" xref="S4.SS2.p3.5.m3.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.5.m3.1c">\Delta\,\text{OOD}</annotation></semantics></math> value, the poorer a model is in OOD generalization. We then compute the difference between the <math id="S4.SS2.p3.6.m4.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.6.m4.1a"><mrow id="S4.SS2.p3.6.m4.1.1" xref="S4.SS2.p3.6.m4.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.6.m4.1.1.2" xref="S4.SS2.p3.6.m4.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.6.m4.1.1.1" xref="S4.SS2.p3.6.m4.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.6.m4.1.1.3" xref="S4.SS2.p3.6.m4.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.6.m4.1b"><apply id="S4.SS2.p3.6.m4.1.1.cmml" xref="S4.SS2.p3.6.m4.1.1"><times id="S4.SS2.p3.6.m4.1.1.1.cmml" xref="S4.SS2.p3.6.m4.1.1.1"></times><ci id="S4.SS2.p3.6.m4.1.1.2.cmml" xref="S4.SS2.p3.6.m4.1.1.2">Δ</ci><ci id="S4.SS2.p3.6.m4.1.1.3a.cmml" xref="S4.SS2.p3.6.m4.1.1.3"><mtext id="S4.SS2.p3.6.m4.1.1.3.cmml" xref="S4.SS2.p3.6.m4.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.6.m4.1c">\Delta\,\text{OOD}</annotation></semantics></math> values of the generative and discriminate models. <a href="#S4.F4" title="In 4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a> visualizes this result; the benchmarks are shown on the <math id="S4.SS2.p3.7.m5.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p3.7.m5.1a"><mi id="S4.SS2.p3.7.m5.1.1" xref="S4.SS2.p3.7.m5.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.7.m5.1b"><ci id="S4.SS2.p3.7.m5.1.1.cmml" xref="S4.SS2.p3.7.m5.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.7.m5.1c">x</annotation></semantics></math>-axis and each circle represents the difference in <math id="S4.SS2.p3.8.m6.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.8.m6.1a"><mrow id="S4.SS2.p3.8.m6.1.1" xref="S4.SS2.p3.8.m6.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.8.m6.1.1.2" xref="S4.SS2.p3.8.m6.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.8.m6.1.1.1" xref="S4.SS2.p3.8.m6.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.8.m6.1.1.3" xref="S4.SS2.p3.8.m6.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.8.m6.1b"><apply id="S4.SS2.p3.8.m6.1.1.cmml" xref="S4.SS2.p3.8.m6.1.1"><times id="S4.SS2.p3.8.m6.1.1.1.cmml" xref="S4.SS2.p3.8.m6.1.1.1"></times><ci id="S4.SS2.p3.8.m6.1.1.2.cmml" xref="S4.SS2.p3.8.m6.1.1.2">Δ</ci><ci id="S4.SS2.p3.8.m6.1.1.3a.cmml" xref="S4.SS2.p3.8.m6.1.1.3"><mtext id="S4.SS2.p3.8.m6.1.1.3.cmml" xref="S4.SS2.p3.8.m6.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.8.m6.1c">\Delta\,\text{OOD}</annotation></semantics></math> values between the generative and discriminative model for a given fine-tuning dataset. If a generative model is more robust to OOD evaluation, we expect to see smaller <math id="S4.SS2.p3.9.m7.1" class="ltx_Math" alttext="\Delta\,\text{OOD}" display="inline"><semantics id="S4.SS2.p3.9.m7.1a"><mrow id="S4.SS2.p3.9.m7.1.1" xref="S4.SS2.p3.9.m7.1.1.cmml"><mi mathvariant="normal" id="S4.SS2.p3.9.m7.1.1.2" xref="S4.SS2.p3.9.m7.1.1.2.cmml">Δ</mi><mo lspace="0.170em" rspace="0em" id="S4.SS2.p3.9.m7.1.1.1" xref="S4.SS2.p3.9.m7.1.1.1.cmml">​</mo><mtext id="S4.SS2.p3.9.m7.1.1.3" xref="S4.SS2.p3.9.m7.1.1.3a.cmml">OOD</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.9.m7.1b"><apply id="S4.SS2.p3.9.m7.1.1.cmml" xref="S4.SS2.p3.9.m7.1.1"><times id="S4.SS2.p3.9.m7.1.1.1.cmml" xref="S4.SS2.p3.9.m7.1.1.1"></times><ci id="S4.SS2.p3.9.m7.1.1.2.cmml" xref="S4.SS2.p3.9.m7.1.1.2">Δ</ci><ci id="S4.SS2.p3.9.m7.1.1.3a.cmml" xref="S4.SS2.p3.9.m7.1.1.3"><mtext id="S4.SS2.p3.9.m7.1.1.3.cmml" xref="S4.SS2.p3.9.m7.1.1.3">OOD</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.9.m7.1c">\Delta\,\text{OOD}</annotation></semantics></math> value for that model compared to its discriminative counter part: when the circles are below the <math id="S4.SS2.p3.10.m8.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S4.SS2.p3.10.m8.1a"><mi id="S4.SS2.p3.10.m8.1.1" xref="S4.SS2.p3.10.m8.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.10.m8.1b"><ci id="S4.SS2.p3.10.m8.1.1.cmml" xref="S4.SS2.p3.10.m8.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.10.m8.1c">x</annotation></semantics></math>-axis (depicting negative values), the generative model is more robust than the discriminative one. We observe <span id="S4.SS2.p3.11.3" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S4.SS2.p3.11.3.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S4.SS2.p3.11.3.m1.1a"><msub id="S4.SS2.p3.11.3.m1.1.1" xref="S4.SS2.p3.11.3.m1.1.1.cmml"><mi id="S4.SS2.p3.11.3.m1.1.1a" xref="S4.SS2.p3.11.3.m1.1.1.cmml"></mi><mtext id="S4.SS2.p3.11.3.m1.1.1.1" xref="S4.SS2.p3.11.3.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.11.3.m1.1b"><apply id="S4.SS2.p3.11.3.m1.1.1.cmml" xref="S4.SS2.p3.11.3.m1.1.1"><ci id="S4.SS2.p3.11.3.m1.1.1.1a.cmml" xref="S4.SS2.p3.11.3.m1.1.1.1"><mtext mathsize="70%" id="S4.SS2.p3.11.3.m1.1.1.1.cmml" xref="S4.SS2.p3.11.3.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.11.3.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> models often outperform their discriminative counterparts with respect to OOD generalization.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>The Effect of Multimodal Pretraining </h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Previous work has shown that pretraining on multimodal (<em id="S5.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, image–text) data improves IID performance <cite class="ltx_cite ltx_citemacro_citep">(<em id="S5.p1.1.2.1" class="ltx_emph ltx_font_italic">e.g.</em>, Lu et al., <a href="#bib.bib32" title="" class="ltx_ref">2019</a>; Li et al., <a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite>; here, we ask if multimodal pretraining can help in OOD settings as well.
We repeat the experiments in <a href="#S4" title="4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4</span></a> without pretraining our models on multimodal data; instead we train the models on the train split of one benchmark and test it on the validation split of another.
<a href="#S4.F3" title="In 4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the difference between the VQA accuracy of models with and without multimodal pretraining: each bar shows the gap between a bar in <a href="#S3.F1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">1</span></a> and the equivalent experiment without multimodal pretraining.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We observe that multimodal pretraining is helpful in almost all conditions, since the majority of values displayed in <a href="#S4.F3" title="In 4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> are positive.
Pretraining is improving OOD performance likely because it can reduce the gap between the train and OOD test data by potentially exposing the model to a more diverse set of data points during pretraining. In our experiments, the maximum gain from multimodal pretraining is indeed observed in OOD settings for both <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> (fine-tune on <span id="S5.p2.1.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>; test on <span id="S5.p2.1.3" class="ltx_text ltx_font_smallcaps">GQA</span>) and <span id="S5.p2.1.4" class="ltx_text ltx_font_smallcaps">ALBEF</span> (fine-tune on <span id="S5.p2.1.5" class="ltx_text ltx_font_smallcaps">GQA</span>; test on <span id="S5.p2.1.6" class="ltx_text ltx_font_smallcaps">VQAv2</span>); however, <em id="S5.p2.1.7" class="ltx_emph ltx_font_italic">multimodal pretraining is <span id="S5.p2.1.7.1" class="ltx_text ltx_font_bold">not</span> always more useful in OOD settings compared to IID ones</em>. For example, when evaluating <span id="S5.p2.1.8" class="ltx_text ltx_font_smallcaps">ViLBERT</span> on <span id="S5.p2.1.9" class="ltx_text ltx_font_smallcaps">VQAv2</span>, pretraining helps the IID setting more than some of the OOD ones.
Lastly, multimodal pretraining is detrimental for some cases where models are fine-tuned on <span id="S5.p2.1.10" class="ltx_text ltx_font_smallcaps">VizWiz</span>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">We observe that multimodal pretraining is more effective for the generative <span id="S5.p3.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> compared to the discriminative <span id="S5.p3.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> (cf. the shaded and solid bar with the same color in <a href="#S4.F3" title="In 4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a> bottom).
For the <span id="S5.p3.1.3" class="ltx_text ltx_font_smallcaps">ViLBERT</span> model, we generally do not observe such a pattern—discriminative and generative models mostly show comparable improvements due to multimodal pretraining.
We observe only small improvements when increasing the size of the multimodal pretraining dataset for the <span id="S5.p3.1.4" class="ltx_text ltx_font_smallcaps">ALBEF</span> model (see <a href="#A2.F8" title="In Effect of pretraining data size on ALBEF ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a> in <a href="#A2" title="Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">B</span></a> for more details).</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.8.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.8.8.9.1" class="ltx_tr">
<th id="S5.T3.8.8.9.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.9.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<td id="S5.T3.8.8.9.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.9.1.2.1" class="ltx_text ltx_font_bold">MM PT</span></td>
<td id="S5.T3.8.8.9.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.9.1.3.1" class="ltx_text ltx_font_bold">VQAv2</span></td>
<td id="S5.T3.8.8.9.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.9.1.4.1" class="ltx_text ltx_font_bold">VQA-CP</span></td>
<td id="S5.T3.8.8.9.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.9.1.5.1" class="ltx_text ltx_font_bold">Drop</span></td>
</tr>
<tr id="S5.T3.8.8.10.2" class="ltx_tr">
<th id="S5.T3.8.8.10.2.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.10.2.1.1" class="ltx_text" style="color:#808080;">CF-VQA</span></th>
<td id="S5.T3.8.8.10.2.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.10.2.2.1" class="ltx_text" style="color:#808080;">–</span></td>
<td id="S5.T3.8.8.10.2.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.10.2.3.1" class="ltx_text" style="color:#808080;">53.6</span></td>
<td id="S5.T3.8.8.10.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.10.2.4.1" class="ltx_text" style="color:#808080;">63.5</span></td>
<td id="S5.T3.8.8.10.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.10.2.5.1" class="ltx_text" style="color:#808080;">9.9</span></td>
</tr>
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S5.T3.1.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S5.T3.1.1.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.1.m1.1.1a" xref="S5.T3.1.1.1.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.1.1.1.1.1.m1.1.1.1" xref="S5.T3.1.1.1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1"><ci id="S5.T3.1.1.1.1.1.m1.1.1.1a.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></th>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">no</td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">66.7</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">42.5</td>
<td id="S5.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">24.2</td>
</tr>
<tr id="S5.T3.2.2.2" class="ltx_tr">
<th id="S5.T3.2.2.2.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.2.2.2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S5.T3.2.2.2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S5.T3.2.2.2.1.1.m1.1a"><msub id="S5.T3.2.2.2.1.1.m1.1.1" xref="S5.T3.2.2.2.1.1.m1.1.1.cmml"><mi id="S5.T3.2.2.2.1.1.m1.1.1a" xref="S5.T3.2.2.2.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.2.2.2.1.1.m1.1.1.1" xref="S5.T3.2.2.2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.1.1.m1.1b"><apply id="S5.T3.2.2.2.1.1.m1.1.1.cmml" xref="S5.T3.2.2.2.1.1.m1.1.1"><ci id="S5.T3.2.2.2.1.1.m1.1.1.1a.cmml" xref="S5.T3.2.2.2.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.2.2.2.1.1.m1.1.1.1.cmml" xref="S5.T3.2.2.2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></th>
<td id="S5.T3.2.2.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">yes</td>
<td id="S5.T3.2.2.2.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">67.0</td>
<td id="S5.T3.2.2.2.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">42.9</td>
<td id="S5.T3.2.2.2.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">24.1</td>
</tr>
<tr id="S5.T3.3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.3.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.3.3.3.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.3.3.3.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S5.T3.3.3.3.1.1.m1.1a"><msub id="S5.T3.3.3.3.1.1.m1.1.1" xref="S5.T3.3.3.3.1.1.m1.1.1.cmml"><mi id="S5.T3.3.3.3.1.1.m1.1.1a" xref="S5.T3.3.3.3.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.3.3.3.1.1.m1.1.1.1" xref="S5.T3.3.3.3.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.1.1.m1.1b"><apply id="S5.T3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T3.3.3.3.1.1.m1.1.1"><ci id="S5.T3.3.3.3.1.1.m1.1.1.1a.cmml" xref="S5.T3.3.3.3.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.3.3.3.1.1.m1.1.1.1.cmml" xref="S5.T3.3.3.3.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></th>
<td id="S5.T3.3.3.3.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">no</td>
<td id="S5.T3.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">64.0</td>
<td id="S5.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">40.1</td>
<td id="S5.T3.3.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">23.9</td>
</tr>
<tr id="S5.T3.4.4.4" class="ltx_tr">
<th id="S5.T3.4.4.4.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.4.4.4.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.4.4.4.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S5.T3.4.4.4.1.1.m1.1a"><msub id="S5.T3.4.4.4.1.1.m1.1.1" xref="S5.T3.4.4.4.1.1.m1.1.1.cmml"><mi id="S5.T3.4.4.4.1.1.m1.1.1a" xref="S5.T3.4.4.4.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.4.4.4.1.1.m1.1.1.1" xref="S5.T3.4.4.4.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.1.1.m1.1b"><apply id="S5.T3.4.4.4.1.1.m1.1.1.cmml" xref="S5.T3.4.4.4.1.1.m1.1.1"><ci id="S5.T3.4.4.4.1.1.m1.1.1.1a.cmml" xref="S5.T3.4.4.4.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.4.4.4.1.1.m1.1.1.1.cmml" xref="S5.T3.4.4.4.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></th>
<td id="S5.T3.4.4.4.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">yes (4M)</td>
<td id="S5.T3.4.4.4.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">70.0</td>
<td id="S5.T3.4.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">44.4</td>
<td id="S5.T3.4.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">25.6</td>
</tr>
<tr id="S5.T3.5.5.5" class="ltx_tr">
<th id="S5.T3.5.5.5.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.5.5.5.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.5.5.5.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S5.T3.5.5.5.1.1.m1.1a"><msub id="S5.T3.5.5.5.1.1.m1.1.1" xref="S5.T3.5.5.5.1.1.m1.1.1.cmml"><mi id="S5.T3.5.5.5.1.1.m1.1.1a" xref="S5.T3.5.5.5.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.5.5.5.1.1.m1.1.1.1" xref="S5.T3.5.5.5.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.1.1.m1.1b"><apply id="S5.T3.5.5.5.1.1.m1.1.1.cmml" xref="S5.T3.5.5.5.1.1.m1.1.1"><ci id="S5.T3.5.5.5.1.1.m1.1.1.1a.cmml" xref="S5.T3.5.5.5.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.5.5.5.1.1.m1.1.1.1.cmml" xref="S5.T3.5.5.5.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></th>
<td id="S5.T3.5.5.5.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">yes (14M)</td>
<td id="S5.T3.5.5.5.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">70.3</td>
<td id="S5.T3.5.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">45.2</td>
<td id="S5.T3.5.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">25.1</td>
</tr>
<tr id="S5.T3.6.6.6" class="ltx_tr">
<th id="S5.T3.6.6.6.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.6.6.6.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.6.6.6.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S5.T3.6.6.6.1.1.m1.1a"><msub id="S5.T3.6.6.6.1.1.m1.1.1" xref="S5.T3.6.6.6.1.1.m1.1.1.cmml"><mi id="S5.T3.6.6.6.1.1.m1.1.1a" xref="S5.T3.6.6.6.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.6.6.6.1.1.m1.1.1.1" xref="S5.T3.6.6.6.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.1.1.m1.1b"><apply id="S5.T3.6.6.6.1.1.m1.1.1.cmml" xref="S5.T3.6.6.6.1.1.m1.1.1"><ci id="S5.T3.6.6.6.1.1.m1.1.1.1a.cmml" xref="S5.T3.6.6.6.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.6.6.6.1.1.m1.1.1.1.cmml" xref="S5.T3.6.6.6.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></th>
<td id="S5.T3.6.6.6.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">no</td>
<td id="S5.T3.6.6.6.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">61.4</td>
<td id="S5.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">36.6</td>
<td id="S5.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">24.8</td>
</tr>
<tr id="S5.T3.7.7.7" class="ltx_tr">
<th id="S5.T3.7.7.7.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.7.7.7.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.7.7.7.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S5.T3.7.7.7.1.1.m1.1a"><msub id="S5.T3.7.7.7.1.1.m1.1.1" xref="S5.T3.7.7.7.1.1.m1.1.1.cmml"><mi id="S5.T3.7.7.7.1.1.m1.1.1a" xref="S5.T3.7.7.7.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.7.7.7.1.1.m1.1.1.1" xref="S5.T3.7.7.7.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.7.7.7.1.1.m1.1b"><apply id="S5.T3.7.7.7.1.1.m1.1.1.cmml" xref="S5.T3.7.7.7.1.1.m1.1.1"><ci id="S5.T3.7.7.7.1.1.m1.1.1.1a.cmml" xref="S5.T3.7.7.7.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.7.7.7.1.1.m1.1.1.1.cmml" xref="S5.T3.7.7.7.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.7.7.7.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></th>
<td id="S5.T3.7.7.7.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">yes (4M)</td>
<td id="S5.T3.7.7.7.3" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">71.0</td>
<td id="S5.T3.7.7.7.4" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">49.2</td>
<td id="S5.T3.7.7.7.5" class="ltx_td ltx_align_center" style="padding-left:3.5pt;padding-right:3.5pt;">21.8</td>
</tr>
<tr id="S5.T3.8.8.8" class="ltx_tr">
<th id="S5.T3.8.8.8.1" class="ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="S5.T3.8.8.8.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="S5.T3.8.8.8.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S5.T3.8.8.8.1.1.m1.1a"><msub id="S5.T3.8.8.8.1.1.m1.1.1" xref="S5.T3.8.8.8.1.1.m1.1.1.cmml"><mi id="S5.T3.8.8.8.1.1.m1.1.1a" xref="S5.T3.8.8.8.1.1.m1.1.1.cmml"></mi><mtext id="S5.T3.8.8.8.1.1.m1.1.1.1" xref="S5.T3.8.8.8.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.1.1.m1.1b"><apply id="S5.T3.8.8.8.1.1.m1.1.1.cmml" xref="S5.T3.8.8.8.1.1.m1.1.1"><ci id="S5.T3.8.8.8.1.1.m1.1.1.1a.cmml" xref="S5.T3.8.8.8.1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.T3.8.8.8.1.1.m1.1.1.1.cmml" xref="S5.T3.8.8.8.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></th>
<td id="S5.T3.8.8.8.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">yes (14M)</td>
<td id="S5.T3.8.8.8.3" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">72.1</td>
<td id="S5.T3.8.8.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">49.6</td>
<td id="S5.T3.8.8.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">22.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance of models on <span id="S5.T3.13.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> (IID) and <span id="S5.T3.14.2" class="ltx_text ltx_font_smallcaps">VQA-CP</span> (OOD). The last column shows drop in performance from <span id="S5.T3.15.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> to <span id="S5.T3.16.4" class="ltx_text ltx_font_smallcaps">VQA-CP</span>. MM PT: Multimodal Pretraining.</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Evaluation on VQA-CP</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we evaluate the models<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><span id="footnote4.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> and <span id="footnote4.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="footnote4.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="footnote4.1.m1.1b"><msub id="footnote4.1.m1.1.1" xref="footnote4.1.m1.1.1.cmml"><mi id="footnote4.1.m1.1.1b" xref="footnote4.1.m1.1.1.cmml"></mi><mtext id="footnote4.1.m1.1.1.1" xref="footnote4.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="footnote4.1.m1.1c"><apply id="footnote4.1.m1.1.1.cmml" xref="footnote4.1.m1.1.1"><ci id="footnote4.1.m1.1.1.1a.cmml" xref="footnote4.1.m1.1.1.1"><mtext mathsize="70%" id="footnote4.1.m1.1.1.1.cmml" xref="footnote4.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote4.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span> (using the official codebase).</span></span></span>
on the VQA under Changing Priors dataset (<span id="S6.p1.1.1" class="ltx_text ltx_font_smallcaps">VQA-CP</span>; <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. <a href="#bib.bib2" title="" class="ltx_ref">2018</a></cite>). This dataset is designed such that, for every question type, train and test splits have different prior distributions of answers. Thus, models that overfit to answer priors in training data and lack sufficient visual grounding show poor generalization on the <span id="S6.p1.1.2" class="ltx_text ltx_font_smallcaps">VQA-CP</span> test set.
For comparison, we also evaluate the performance of Counterfactual VQA (CF-VQA; <cite class="ltx_cite ltx_citemacro_citet">Niu et al. <a href="#bib.bib35" title="" class="ltx_ref">2021</a></cite>), a state-of-art method on <span id="S6.p1.1.3" class="ltx_text ltx_font_smallcaps">VQA-CP</span>, which does not use either the Transformer architecture nor multimodal pretraining.
However, it explicitly tackles the language (<em id="S6.p1.1.4" class="ltx_emph ltx_font_italic">i.e.</em>, question and answer) biases in VQA-CP.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><a href="#S5.T3" title="In 5 The Effect of Multimodal Pretraining ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows that for all the Transformer-based models, there is a large drop in the performance (at least 22%) from <span id="S6.p2.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> to <span id="S6.p2.1.2" class="ltx_text ltx_font_smallcaps">VQA-CP</span>. Thus, in spite of advances in the Transformer architecture and pretraining on diverse datasets, models are still overfitting to answer priors in the training data and lack sufficient visual grounding <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. However, the drop is much less for CF-VQA (10%), suggesting that <em id="S6.p2.1.3" class="ltx_emph ltx_font_italic">incorporating inductive biases specific to the generalization problem</em> (<em id="S6.p2.1.4" class="ltx_emph ltx_font_italic">i.e.</em>, modeling language bias) <em id="S6.p2.1.5" class="ltx_emph ltx_font_italic">helps more than the Transformer architecture or scaling up the amount of pretraining data</em>.
We also observe that the drop from <span id="S6.p2.1.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> to <span id="S6.p2.1.7" class="ltx_text ltx_font_smallcaps">VQA-CP</span> is often lower for the generative <span id="S6.p2.1.8" class="ltx_text ltx_font_smallcaps">ALBEF</span> than the discriminative <span id="S6.p2.1.9" class="ltx_text ltx_font_smallcaps">ALBEF</span> (except for <span id="S6.p2.1.10" class="ltx_text ltx_font_smallcaps">ALBEF</span> without any multimodal pretraining). Thus, <em id="S6.p2.1.11" class="ltx_emph ltx_font_italic">generative models are more robust than the discriminative ones</em>, especially when they are pretrained (similarly to the observations made in <a href="#S4.SS2" title="4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>).
As for the effect of pretraining, for generative <span id="S6.p2.1.12" class="ltx_text ltx_font_smallcaps">ALBEF</span>, pretraining helps reduce the drop from <span id="S6.p2.1.13" class="ltx_text ltx_font_smallcaps">VQAv2</span> to <span id="S6.p2.1.14" class="ltx_text ltx_font_smallcaps">VQA-CP</span>. However, for discriminative models, pretraining does not seem to help generalization (in fact, it worsen <span id="S6.p2.1.15" class="ltx_text ltx_font_smallcaps">ALBEF</span>).</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Qualitative Analysis</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">To dig deeper into the potential causes of the poor OOD generalization of our pretrained models, we perform a qualitative study.
To this end, we randomly sample and manually examine failure cases in top-30 answer classes with the highest performance drop when moving from IID to OOD evaluation. We only focus on answer classes that are present in both the train and test splits, ensuring that performance drop is not due to the absence of answer classes in the training set.
We report the top-5 classes that contribute the most to the drop in performance for each OOD setting in <a href="#A3.T11" title="In Poor performance of GQA model on color questions (both IID and OOD): ‣ Appendix C Potential Causes of Poor OOD Generalization: A Qualitative Study ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a> (<a href="#A3" title="Appendix C Potential Causes of Poor OOD Generalization: A Qualitative Study ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">C</span></a>). We notice that the following answer classes appear frequently across different OOD settings: yes/no answers, directions (left/right), colors, and numbers. In the following, we discuss a few major potential causes for the poor OOD generalization, and mention <span id="S7.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p1.1.1.m1.1a"><msub id="S7.p1.1.1.m1.1.1" xref="S7.p1.1.1.m1.1.1.cmml"><mi id="S7.p1.1.1.m1.1.1a" xref="S7.p1.1.1.m1.1.1.cmml"></mi><mtext id="S7.p1.1.1.m1.1.1.1" xref="S7.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p1.1.1.m1.1b"><apply id="S7.p1.1.1.m1.1.1.cmml" xref="S7.p1.1.1.m1.1.1"><ci id="S7.p1.1.1.m1.1.1.1a.cmml" xref="S7.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S7.p1.1.1.m1.1.1.1.cmml" xref="S7.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> responses as examples in the discussion, although similar observations hold for other models.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.2" class="ltx_p"><span id="S7.p2.2.3" class="ltx_text ltx_font_bold">Poor reasoning skills.</span>
Models evaluated on <span id="S7.p2.2.4" class="ltx_text ltx_font_smallcaps">GQA</span>, but fine-tuned on another dataset, show the highest performance drop on classes such as “yes”, “no”, “right”, “left”, “top”, and “bottom”. For instance, <span id="S7.p2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p2.1.1.m1.1a"><msub id="S7.p2.1.1.m1.1.1" xref="S7.p2.1.1.m1.1.1.cmml"><mi id="S7.p2.1.1.m1.1.1a" xref="S7.p2.1.1.m1.1.1.cmml"></mi><mtext id="S7.p2.1.1.m1.1.1.1" xref="S7.p2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p2.1.1.m1.1b"><apply id="S7.p2.1.1.m1.1.1.cmml" xref="S7.p2.1.1.m1.1.1"><ci id="S7.p2.1.1.m1.1.1.1a.cmml" xref="S7.p2.1.1.m1.1.1.1"><mtext mathsize="70%" id="S7.p2.1.1.m1.1.1.1.cmml" xref="S7.p2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.p2.2.5" class="ltx_text ltx_font_smallcaps">VQAv2</span>, and evaluated on <span id="S7.p2.2.6" class="ltx_text ltx_font_smallcaps">GQA</span> underperforms <span id="S7.p2.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p2.2.2.m1.1a"><msub id="S7.p2.2.2.m1.1.1" xref="S7.p2.2.2.m1.1.1.cmml"><mi id="S7.p2.2.2.m1.1.1a" xref="S7.p2.2.2.m1.1.1.cmml"></mi><mtext id="S7.p2.2.2.m1.1.1.1" xref="S7.p2.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p2.2.2.m1.1b"><apply id="S7.p2.2.2.m1.1.1.cmml" xref="S7.p2.2.2.m1.1.1"><ci id="S7.p2.2.2.m1.1.1.1a.cmml" xref="S7.p2.2.2.m1.1.1.1"><mtext mathsize="70%" id="S7.p2.2.2.m1.1.1.1.cmml" xref="S7.p2.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p2.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> that has been both fine-tuned and evaluated on <span id="S7.p2.2.7" class="ltx_text ltx_font_smallcaps">GQA</span> by 24% for the answer class “no.”
Upon qualitative examination, we find that for many of such failure cases, the <span id="S7.p2.2.8" class="ltx_text ltx_font_smallcaps">GQA</span> questions are more compositional and hence require more complex reasoning (<em id="S7.p2.2.9" class="ltx_emph ltx_font_italic">e.g.</em>, “Are there both bison and zebras in the image?”, “Is the cheese to the right or to the left of the empty plate?”) than the questions for the same answer classes in other datasets (<em id="S7.p2.2.10" class="ltx_emph ltx_font_italic">e.g.</em>, from <span id="S7.p2.2.11" class="ltx_text ltx_font_smallcaps">VQAv2</span> train set: “Is the TV turned on?”, “Which hand is the man holding up?”).
This study re-affirms previous findings that VQA models lack sufficient logical, spatial, and compositional reasoning skills <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> but for the more recent, pretrained Transformer models.</p>
</div>
<figure id="S7.F5" class="ltx_figure"><img src="/html/2205.12191/assets/x1.png" id="S7.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="159" height="73" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples where models’ prediction are correct but not accounted for in the ground-truth set. <math id="S7.F5.3.m1.1" class="ltx_Math" alttext="\langle~{}\rangle" display="inline"><semantics id="S7.F5.3.m1.1b"><mrow id="S7.F5.3.m1.1.1.2"><mo rspace="0.330em" stretchy="false" id="S7.F5.3.m1.1.1.2.1" xref="S7.F5.3.m1.1.1.1.cmml">⟨</mo><mo stretchy="false" id="S7.F5.3.m1.1.1.2.2" xref="S7.F5.3.m1.1.1.1.cmml">⟩</mo></mrow><annotation-xml encoding="MathML-Content" id="S7.F5.3.m1.1c"><list id="S7.F5.3.m1.1.1.1.cmml" xref="S7.F5.3.m1.1.1.2.1"></list></annotation-xml><annotation encoding="application/x-tex" id="S7.F5.3.m1.1d">\langle~{}\rangle</annotation></semantics></math> denotes a list of unique (out of 10) ground-truth answers. <span id="S7.F5.9.2" class="ltx_text ltx_font_smallcaps">VG</span> (<span id="S7.F5.10.3" class="ltx_text ltx_font_smallcaps">VQAv2</span>) model refers to a <span id="S7.F5.4.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.F5.4.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.F5.4.1.m1.1b"><msub id="S7.F5.4.1.m1.1.1" xref="S7.F5.4.1.m1.1.1.cmml"><mi id="S7.F5.4.1.m1.1.1b" xref="S7.F5.4.1.m1.1.1.cmml"></mi><mtext id="S7.F5.4.1.m1.1.1.1" xref="S7.F5.4.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.F5.4.1.m1.1c"><apply id="S7.F5.4.1.m1.1.1.cmml" xref="S7.F5.4.1.m1.1.1"><ci id="S7.F5.4.1.m1.1.1.1a.cmml" xref="S7.F5.4.1.m1.1.1.1"><mtext mathsize="70%" id="S7.F5.4.1.m1.1.1.1.cmml" xref="S7.F5.4.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.F5.4.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.F5.11.4" class="ltx_text ltx_font_smallcaps">VG</span> (<span id="S7.F5.12.5" class="ltx_text ltx_font_smallcaps">VQAv2</span>).</figcaption>
</figure>
<div id="S7.p3" class="ltx_para">
<p id="S7.p3.2" class="ltx_p"><span id="S7.p3.2.3" class="ltx_text ltx_font_bold">Overfitting to the answer priors.</span> Previous studies have shown that VQA models tend to be biased towards the prior distribution of answers in the training set (per question type) <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. We find that this limitation exists in the more recent pretrained models as well, and it is especially hurtful in the OOD settings because the priors need not be the same across train and test sets, unlike in the IID settings. For instance, <span id="S7.p3.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p3.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p3.1.1.m1.1a"><msub id="S7.p3.1.1.m1.1.1" xref="S7.p3.1.1.m1.1.1.cmml"><mi id="S7.p3.1.1.m1.1.1a" xref="S7.p3.1.1.m1.1.1.cmml"></mi><mtext id="S7.p3.1.1.m1.1.1.1" xref="S7.p3.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p3.1.1.m1.1b"><apply id="S7.p3.1.1.m1.1.1.cmml" xref="S7.p3.1.1.m1.1.1"><ci id="S7.p3.1.1.m1.1.1.1a.cmml" xref="S7.p3.1.1.m1.1.1.1"><mtext mathsize="70%" id="S7.p3.1.1.m1.1.1.1.cmml" xref="S7.p3.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.p3.2.4" class="ltx_text ltx_font_smallcaps">VQAv2</span> predicts “2” for a lot questions with target answer “1” in the <span id="S7.p3.2.5" class="ltx_text ltx_font_smallcaps">VG</span> test set. Similarly, sometimes <span id="S7.p3.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p3.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p3.2.2.m1.1a"><msub id="S7.p3.2.2.m1.1.1" xref="S7.p3.2.2.m1.1.1.cmml"><mi id="S7.p3.2.2.m1.1.1a" xref="S7.p3.2.2.m1.1.1.cmml"></mi><mtext id="S7.p3.2.2.m1.1.1.1" xref="S7.p3.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p3.2.2.m1.1b"><apply id="S7.p3.2.2.m1.1.1.cmml" xref="S7.p3.2.2.m1.1.1"><ci id="S7.p3.2.2.m1.1.1.1a.cmml" xref="S7.p3.2.2.m1.1.1.1"><mtext mathsize="70%" id="S7.p3.2.2.m1.1.1.1.cmml" xref="S7.p3.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.p3.2.6" class="ltx_text ltx_font_smallcaps">VG</span> incorrectly predicts “helmet” for <span id="S7.p3.2.7" class="ltx_text ltx_font_smallcaps">VQAv2</span> test questions such as “What is the skateboarder wearing to protect his head?”, “What protective gear is he wearing?” when the skateboarder is not wearing anything. This indicates that the model is relying on answer priors rather than visual grounding. Our experimental results on VQA-CP (<a href="#S6" title="6 Evaluation on VQA-CP ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">6</span></a>) directly quantify the extent of such limitations in current models.</p>
</div>
<div id="S7.p4" class="ltx_para">
<p id="S7.p4.2" class="ltx_p"><span id="S7.p4.2.3" class="ltx_text ltx_font_bold">Overfitting to the question format.</span> We observe instances of models failing to correctly answer questions when the format of the questions changes between the fine-tuning and test sets.
For instance, questions about “chair” in the <span id="S7.p4.2.4" class="ltx_text ltx_font_smallcaps">VQAv2</span> fine-tuning set are mostly of the form “What is … sitting on?” whereas in the <span id="S7.p4.2.5" class="ltx_text ltx_font_smallcaps">GQA</span> test set, they are mostly of the form “What kind of furniture is …?”. Thus, the “chair” class accuracy of <span id="S7.p4.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p4.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p4.1.1.m1.1a"><msub id="S7.p4.1.1.m1.1.1" xref="S7.p4.1.1.m1.1.1.cmml"><mi id="S7.p4.1.1.m1.1.1a" xref="S7.p4.1.1.m1.1.1.cmml"></mi><mtext id="S7.p4.1.1.m1.1.1.1" xref="S7.p4.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p4.1.1.m1.1b"><apply id="S7.p4.1.1.m1.1.1.cmml" xref="S7.p4.1.1.m1.1.1"><ci id="S7.p4.1.1.m1.1.1.1a.cmml" xref="S7.p4.1.1.m1.1.1.1"><mtext mathsize="70%" id="S7.p4.1.1.m1.1.1.1.cmml" xref="S7.p4.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.p4.2.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> drops from 48% when tested on <span id="S7.p4.2.7" class="ltx_text ltx_font_smallcaps">VQAv2</span> to 38% on the <span id="S7.p4.2.8" class="ltx_text ltx_font_smallcaps">GQA</span> test set. Similarly, <span id="S7.p4.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S7.p4.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S7.p4.2.2.m1.1a"><msub id="S7.p4.2.2.m1.1.1" xref="S7.p4.2.2.m1.1.1.cmml"><mi id="S7.p4.2.2.m1.1.1a" xref="S7.p4.2.2.m1.1.1.cmml"></mi><mtext id="S7.p4.2.2.m1.1.1.1" xref="S7.p4.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S7.p4.2.2.m1.1b"><apply id="S7.p4.2.2.m1.1.1.cmml" xref="S7.p4.2.2.m1.1.1"><ci id="S7.p4.2.2.m1.1.1.1a.cmml" xref="S7.p4.2.2.m1.1.1.1"><mtext mathsize="70%" id="S7.p4.2.2.m1.1.1.1.cmml" xref="S7.p4.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="S7.p4.2.9" class="ltx_text ltx_font_smallcaps">GQA</span> fails terribly for “dog” and “cat” classes on the <span id="S7.p4.2.10" class="ltx_text ltx_font_smallcaps">VG</span> test set (accuracy drops of 47% and 43% respectively between <span id="S7.p4.2.11" class="ltx_text ltx_font_smallcaps">GQA</span>–<span id="S7.p4.2.12" class="ltx_text ltx_font_smallcaps">GQA</span> (fine-tuned on <span id="S7.p4.2.13" class="ltx_text ltx_font_smallcaps">GQA</span>, tested on <span id="S7.p4.2.14" class="ltx_text ltx_font_smallcaps">GQA</span>) and <span id="S7.p4.2.15" class="ltx_text ltx_font_smallcaps">GQA</span>–<span id="S7.p4.2.16" class="ltx_text ltx_font_smallcaps">VG</span>). <span id="S7.p4.2.17" class="ltx_text ltx_font_smallcaps">GQA</span> questions are mostly of the form “What animal …?” or “What kind of animal …?” whereas <span id="S7.p4.2.18" class="ltx_text ltx_font_smallcaps">VG</span> questions often do not mention the word “animal” and are of the form “Who is …?” or “What is …?” (<em id="S7.p4.2.19" class="ltx_emph ltx_font_italic">e.g.</em>, “Who is holding the Frisbee?”, “What is on the leash?”). To the best of our knowledge, no previous work has reported such behavior of VQA models (i.e., they tend to overfit to the question format).</p>
</div>
<div id="S7.p5" class="ltx_para">
<p id="S7.p5.1" class="ltx_p">Finally, we observe cases where correct model responses are evaluated as incorrect by the VQA evaluation metric, as such responses differ from the ground-truth answers. In the next section, we provide examples of such cases and examine the impact of <span id="S7.p5.1.1" class="ltx_text ltx_font_bold">stringent evaluation metric</span> on poor OOD generalization by engaging human raters to evaluate responses.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Human Evaluation</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In our qualitative study, we observed that the stringent nature of the standard VQA evaluation metrics (<em id="S8.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em>, performing string matching of model responses with a small set of ground-truth answers) repeatedly penalizes models for correct responses because those responses do not exist in the set of ground-truth answers (<a href="#S7.F5" title="In 7 Qualitative Analysis ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>).
For example, the evaluation metric fails to take into account differences (between model response and ground-truth) due to specificity of the answers (<em id="S8.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, “on table” vs. “table”, “pizza slices” vs. “pizza”), synonyms, and different interpretations of the question (<em id="S8.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, <a href="#S7.F5" title="In 7 Qualitative Analysis ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a> right).</p>
</div>
<div id="S8.p2" class="ltx_para">
<p id="S8.p2.1" class="ltx_p">In this section, we aim to quantify how robust standard VQA metrics are by performing human evaluation of our models for both IID and OOD settings. The details of the setup and in-depth results are provided in <a href="#A4" title="Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">D</span></a>. Below we present our main findings.</p>
</div>
<figure id="S8.F6" class="ltx_figure"><img src="/html/2205.12191/assets/figs/%22auto_score_on_human_sample_5_score_4_diff_v2.pdf%22" id="S8.F6.g1" class="ltx_graphics ltx_centering" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Difference in human and automatic accuracy of <span id="S8.F6.3.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.F6.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S8.F6.3.1.m1.1b"><msub id="S8.F6.3.1.m1.1.1" xref="S8.F6.3.1.m1.1.1.cmml"><mi id="S8.F6.3.1.m1.1.1b" xref="S8.F6.3.1.m1.1.1.cmml"></mi><mtext id="S8.F6.3.1.m1.1.1.1" xref="S8.F6.3.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.F6.3.1.m1.1c"><apply id="S8.F6.3.1.m1.1.1.cmml" xref="S8.F6.3.1.m1.1.1"><ci id="S8.F6.3.1.m1.1.1.1a.cmml" xref="S8.F6.3.1.m1.1.1.1"><mtext mathsize="70%" id="S8.F6.3.1.m1.1.1.1.cmml" xref="S8.F6.3.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.F6.3.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span> (shaded bars) and <span id="S8.F6.4.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.F6.4.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S8.F6.4.2.m1.1b"><msub id="S8.F6.4.2.m1.1.1" xref="S8.F6.4.2.m1.1.1.cmml"><mi id="S8.F6.4.2.m1.1.1b" xref="S8.F6.4.2.m1.1.1.cmml"></mi><mtext id="S8.F6.4.2.m1.1.1.1" xref="S8.F6.4.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.F6.4.2.m1.1c"><apply id="S8.F6.4.2.m1.1.1.cmml" xref="S8.F6.4.2.m1.1.1"><ci id="S8.F6.4.2.m1.1.1.1a.cmml" xref="S8.F6.4.2.m1.1.1.1"><mtext mathsize="70%" id="S8.F6.4.2.m1.1.1.1.cmml" xref="S8.F6.4.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.F6.4.2.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span> (plain bars) for <span id="S8.F6.9.3" class="ltx_text ltx_font_smallcaps">GQA</span>, <span id="S8.F6.10.4" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="S8.F6.11.5" class="ltx_text ltx_font_smallcaps">VG</span> and <span id="S8.F6.12.6" class="ltx_text ltx_font_smallcaps">VizWiz</span>. Accuracies in bold denote the IID settings.</figcaption>
</figure>
<section id="S8.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Human evaluation yields notably higher accuracies than the automatic evaluation.</h4>

<div id="S8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px1.p1.2" class="ltx_p">This is shown in <a href="#S8.F6" title="In 8 Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, where the increase can be up to 33.5% when moving from automatic to human evaluation.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In some cases, human evaluation yields lower accuracy than the automatic evaluation. We discuss this in <a href="#A4" title="Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">D</span></a>.</span></span></span>
This implies the current automatic metrics miss out on a lot of correct responses due to their stringent nature.
Interestingly, this increase in model accuracy from automatic to human evaluation is higher for <span id="S8.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px1.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S8.SS0.SSS0.Px1.p1.1.1.m1.1a"><msub id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1a" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.1.1.m1.1b"><apply id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1"><ci id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> than <span id="S8.SS0.SSS0.Px1.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px1.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S8.SS0.SSS0.Px1.p1.2.2.m1.1a"><msub id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1a" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px1.p1.2.2.m1.1b"><apply id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1"><ci id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px1.p1.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px1.p1.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> for all the benchmarks. This is expected because the generative model is more likely to produce longer, more varied answers, which might not be awarded using automatic metric but are still correct responses. Moreover, human evaluation helps OOD settings more than the IID settings for most of the benchmarks (<em id="S8.SS0.SSS0.Px1.p1.2.3" class="ltx_emph ltx_font_italic">e.g.</em>, <span id="S8.SS0.SSS0.Px1.p1.2.4" class="ltx_text ltx_font_smallcaps">GQA</span>, <span id="S8.SS0.SSS0.Px1.p1.2.5" class="ltx_text ltx_font_smallcaps">VQAv2</span>). This is also expected, because in the OOD settings, a model might not learn the format of the test answer (“on table” vs. “table”, “clear vs. sunny”) from the train set (unlike in the IID settings) and hence it is more likely to be penalized by the automatic accuracy metric.
Thus, we conclude that the currently used accuracy metrics for VQA are not robust, especially for generative models and OOD evaluation settings. Hence, to more accurately evaluate the goodness of our models, <em id="S8.SS0.SSS0.Px1.p1.2.6" class="ltx_emph ltx_font_italic">we need to develop better evaluation metrics for VQA</em>.</p>
</div>
</section>
<section id="S8.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Even after human evaluation, models still exhibit poor OOD generalization.</h4>

<div id="S8.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S8.SS0.SSS0.Px2.p1.4" class="ltx_p">Although human evaluation improves the models’ accuracies and more so for the OOD than the IID settings, we observe that the models’ performance in OOD settings is still worse compared to that of IID settings, albeit with reduced margin (see <a href="#A4" title="Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">D</span></a> for quantitative results).
We also note that while <span id="S8.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px2.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S8.SS0.SSS0.Px2.p1.1.1.m1.1a"><msub id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1a" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px2.p1.1.1.m1.1b"><apply id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1"><ci id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px2.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> usually outperformed <span id="S8.SS0.SSS0.Px2.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px2.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S8.SS0.SSS0.Px2.p1.2.2.m1.1a"><msub id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1a" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px2.p1.2.2.m1.1b"><apply id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1"><ci id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px2.p1.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> with the automatic evaluation, <span id="S8.SS0.SSS0.Px2.p1.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px2.p1.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="S8.SS0.SSS0.Px2.p1.3.3.m1.1a"><msub id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1a" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px2.p1.3.3.m1.1b"><apply id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1"><ci id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.3.3.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px2.p1.3.3.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> outperforms <span id="S8.SS0.SSS0.Px2.p1.4.4" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="S8.SS0.SSS0.Px2.p1.4.4.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="S8.SS0.SSS0.Px2.p1.4.4.m1.1a"><msub id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.cmml"><mi id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1a" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.cmml"></mi><mtext id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="S8.SS0.SSS0.Px2.p1.4.4.m1.1b"><apply id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1"><ci id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1a.cmml" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1"><mtext mathsize="70%" id="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1.cmml" xref="S8.SS0.SSS0.Px2.p1.4.4.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS0.SSS0.Px2.p1.4.4.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> for all the test sets under human evaluation. This reinforces the observations in <a href="#S4.SS2" title="4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.2</span></a> regarding stronger OOD generalization of generative models over discriminative ones.</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Conclusion</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">In this study, we show that, despite their impressive performance when evaluated on test data drawn from the same distribution as the training data, recent V&amp;L models perform poorly in out-of-distribution (OOD) settings.
We conclude that these models learn to solve specific benchmarks as opposed to the skill of visual question answering (VQA).
Interestingly, in most cases, we observe that the generative models are more robust to OOD generalization compared to the discriminative ones. Moreover, pretraining the models on large image–text data often helps in OOD generalization.
Our results also highlight the importance of human evaluation for a more accurate assessment of model performance: we find that the current VQA automatic metrics miss out on a notable number of correct model responses.
Human evaluation is especially important as the community is shifting towards generative VQA models which, unlike discriminative ones, can produce answers that go beyond those seen in a training/fine-tuning dataset.
Finally, to make progress towards more capable models, we need more rigorous evaluation protocols that shed light on models’ strengths and short-comings. We believe testing models in OOD settings is a step towards this direction as it helps evaluate models for general skills required to solve the task as opposed to benchmark-specific correlations.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We list some limitations of our work which could benefit from future investigations.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">First, when exploring potential factors for poor OOD generalization, our quantitative analysis focused only on differences in answer distributions between fine-tuning and test datasets. However, future work should investigate differences in question distribution, image distribution and combinations of these three variables.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Second, it would be interesting to conduct further investigation to understand why multimodal pretraining does not help in certain cases. A correlation analysis between improvement in accuracy (due to multimodal pretraining), and between pretraining and fine-tuning/test data could be useful.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">Third, the models investigated in our study (<span id="Sx1.p4.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> and <span id="Sx1.p4.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span>) are pretrained on a relatively small number (millions) of data points compared to language-only pretrained transformers, such as BERT, trained on billions of tokens. Such large-scale pretraining has been shown to improve OOD robustness for language-only models <cite class="ltx_cite ltx_citemacro_cite">Hendrycks et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. Hence, we leave for future work to investigate multimodal models trained on billions of image–text pairs (for instance, LAION-5B; <cite class="ltx_cite ltx_citemacro_citet">Schuhmann et al. <a href="#bib.bib40" title="" class="ltx_ref">2021</a></cite>).</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">Lastly, in this study, we only focus on standard VQA evaluation metrics for each benchmark. However, it would be interesting to also evaluate the robustness of metrics such as WUPS <cite class="ltx_cite ltx_citemacro_cite">Malinowski and Fritz (<a href="#bib.bib33" title="" class="ltx_ref">2014</a>)</cite> that compute answer similarities based on the distance between them in the WordNet <cite class="ltx_cite ltx_citemacro_cite">Miller (<a href="#bib.bib34" title="" class="ltx_ref">1995</a>)</cite> tree and are expected to be more robust than the standard metrics.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Below we present some considerations related to the ethical and broader impact of our work.</p>
</div>
<div id="Sx2.p2" class="ltx_para">
<p id="Sx2.p2.1" class="ltx_p">First, all datasets used in our study are from published work and are publicly available, including the <span id="Sx2.p2.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> data <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> which has been curated from visually impaired users and released publicly after proper filtering to preserve the privacy of the users.</p>
</div>
<div id="Sx2.p3" class="ltx_para">
<p id="Sx2.p3.1" class="ltx_p">Second, for human evaluation of our models, we collected human data via the Amazon Mechanical Turk platform. We detail the data collection process and measures taken to control the quality of collected data in <a href="#A4" title="Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">App.</span> <span class="ltx_text ltx_ref_tag">D</span></a>. As for the ethical considerations related to collecting data from human subjects, our data collection campaign was approved by an ethics review board in our institution.
Human subjects were paid at the rate of 0.15 USD per HIT (Human Intelligence Task) resulting in an hourly payment well above minimum wage.</p>
</div>
<div id="Sx2.p4" class="ltx_para">
<p id="Sx2.p4.1" class="ltx_p">Third, by testing models on a data distribution different from the training one, the OOD evaluation setting studied in our work has the following broader impacts: it highlights (1) the challenges of generalizing to real-world VQA datasets such as <span id="Sx2.p4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span>, and (2) the kind of biases learned (and also potentially amplified) by the models.</p>
</div>
<div id="Sx2.p5" class="ltx_para">
<p id="Sx2.p5.1" class="ltx_p">Lastly, we discuss both potentially beneficial and harmful applications of the task of Visual Question Answering studied in our work. VQA has many potential applications beneficial for society:</p>
<ul id="Sx2.I1" class="ltx_itemize">
<li id="Sx2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I1.i1.p1" class="ltx_para">
<p id="Sx2.I1.i1.p1.1" class="ltx_p">Aiding visually impaired users in understanding their surroundings (Human: <span id="Sx2.I1.i1.p1.1.1" class="ltx_text ltx_font_typewriter">What is on the shelf above the microwave?</span> AI: <span id="Sx2.I1.i1.p1.1.2" class="ltx_text ltx_font_typewriter">Canned containers.</span>)</p>
</div>
</li>
<li id="Sx2.I1.i2" class="ltx_item" style="list-style-type:none;padding-top:0.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I1.i2.p1" class="ltx_para">
<p id="Sx2.I1.i2.p1.1" class="ltx_p">Teaching children through interactive demos (Kid: <span id="Sx2.I1.i2.p1.1.1" class="ltx_text ltx_font_typewriter">What animal is that?</span> AI: <span id="Sx2.I1.i2.p1.1.2" class="ltx_text ltx_font_typewriter">That is Dall Sheep. You can find those in Alaska.</span>)</p>
</div>
</li>
<li id="Sx2.I1.i3" class="ltx_item" style="list-style-type:none;padding-top:0.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I1.i3.p1" class="ltx_para">
<p id="Sx2.I1.i3.p1.1" class="ltx_p">Aiding analysts in processing large quantities of visual surveillance data (Analyst: <span id="Sx2.I1.i3.p1.1.1" class="ltx_text ltx_font_typewriter">What kind of car did the man in red shirt leave in?</span> AI: <span id="Sx2.I1.i3.p1.1.2" class="ltx_text ltx_font_typewriter">Blue Toyota Prius.</span>)</p>
</div>
</li>
<li id="Sx2.I1.i4" class="ltx_item" style="list-style-type:none;padding-top:0.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I1.i4.p1" class="ltx_para">
<p id="Sx2.I1.i4.p1.1" class="ltx_p">Interacting with in-home physical robots (Human: <span id="Sx2.I1.i4.p1.1.1" class="ltx_text ltx_font_typewriter">Is my laptop in my bedroom upstairs?</span> AI: <span id="Sx2.I1.i4.p1.1.2" class="ltx_text ltx_font_typewriter">Yes.</span> Human: <span id="Sx2.I1.i4.p1.1.3" class="ltx_text ltx_font_typewriter">Is the charger plugged in?</span>)</p>
</div>
</li>
<li id="Sx2.I1.i5" class="ltx_item" style="list-style-type:none;padding-top:0.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I1.i5.p1" class="ltx_para">
<p id="Sx2.I1.i5.p1.1" class="ltx_p">Making visual social media content more accessible (AI: <span id="Sx2.I1.i5.p1.1.1" class="ltx_text ltx_font_typewriter">Your friend Bob just uploaded a picture from his Hawaii trip.</span> Human: <span id="Sx2.I1.i5.p1.1.2" class="ltx_text ltx_font_typewriter">Great, is he at the beach?</span> AI: <span id="Sx2.I1.i5.p1.1.3" class="ltx_text ltx_font_typewriter">No, on a mountain.</span>)</p>
</div>
</li>
</ul>
<p id="Sx2.p5.2" class="ltx_p">But like most other technology, VQA could also be used for potentially harmful applications such as:</p>
<ul id="Sx2.I2" class="ltx_itemize">
<li id="Sx2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I2.i1.p1" class="ltx_para">
<p id="Sx2.I2.i1.p1.1" class="ltx_p">Invasion of individual’s privacy by using VQA to query streams of video data being recorded by CCTV cameras at public places.</p>
</div>
</li>
<li id="Sx2.I2.i2" class="ltx_item" style="list-style-type:none;padding-top:0.0pt;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx2.I2.i2.p1" class="ltx_para">
<p id="Sx2.I2.i2.p1.1" class="ltx_p">Visually impaired users often need assistance with parsing data containing personal information <cite class="ltx_cite ltx_citemacro_cite">Ahmed et al. (<a href="#bib.bib4" title="" class="ltx_ref">2015</a>)</cite>, such as credit cards, personal mails, etc. Such VQA systems could be configured to leak/retain personally identifiable information.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Contributions</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p"><em id="Sx3.p1.1.1" class="ltx_emph ltx_font_italic">Aishwarya Agrawal</em> initiated and designed the project, ran experiments and analyses on the official codebase of ViLBERT, contributed significantly to paper writing, and provided project support and advice.
<em id="Sx3.p1.1.2" class="ltx_emph ltx_font_italic">Ivana Kajić</em> was responsible for the project’s technical infrastructure for the re-implementation of ViLBERT, ran experiments and analyses, and contributed significantly to paper writing.
<em id="Sx3.p1.1.3" class="ltx_emph ltx_font_italic">Emanuele Bugliarello</em> co-led the data preparation, ran experiments and analyses on ALBEF, and contributed significantly to paper writing.
<em id="Sx3.p1.1.4" class="ltx_emph ltx_font_italic">Elnaz Davoodi</em> co-led the data preparation, and helped setting up and running re-implementation experiments.
<em id="Sx3.p1.1.5" class="ltx_emph ltx_font_italic">Anita Gergely</em> led the human evaluation experiments, and contributed to paper writing.
<em id="Sx3.p1.1.6" class="ltx_emph ltx_font_italic">Phil Blunsom</em> provided project advice.
<em id="Sx3.p1.1.7" class="ltx_emph ltx_font_italic">Aida Nematzadeh</em> provided significant project support and advice, helped running experiments, and contributed significantly to paper writing.</p>
</div>
</section>
<section id="Sx4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">We are grateful to Antoine Miech, Lisa Anne Hendricks and Chris Dyer for their constructive feedback. <span id="Sx4.p1.1.1" class="ltx_ERROR undefined">\euflag</span>
During this project, <em id="Sx4.p1.1.2" class="ltx_emph ltx_font_italic">Emanuele Bugliarello</em> was supported by the funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement No 801199.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agarap (2018)</span>
<span class="ltx_bibblock">
Abien Fred Agarap. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1803.08375" title="" class="ltx_ref ltx_href">Deep learning using
rectified linear units (relu)</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1803.08375</em>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2018)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Agrawal_Dont_Just_Assume_CVPR_2018_paper" title="" class="ltx_ref ltx_href">Don’t just assume; look and answer: Overcoming priors for visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2017)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Aniruddha Kembhavi, Dhruv Batra, and Devi Parikh. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1704.08243" title="" class="ltx_ref ltx_href">C-VQA: A compositional
split of the visual question answering (vqa) v1.0 dataset</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1704.08243.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahmed et al. (2015)</span>
<span class="ltx_bibblock">
Tousif Ahmed, Roberto Hoyle, Kay Connelly, David Crandall, and Apu Kapadia.
2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://dl.acm.org/doi/10.1145/2702123.2702334" title="" class="ltx_ref ltx_href">Privacy
concerns and behaviors of people with visual impairments</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 33rd Annual ACM Conference on Human
Factors in Computing Systems</em>, pages 3523–3532.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akula et al. (2021)</span>
<span class="ltx_bibblock">
Arjun Akula, Soravit Changpinyo, Boqing Gong, Piyush Sharma, Song-Chun Zhu, and
Radu Soricut. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2021.emnlp-main.164" title="" class="ltx_ref ltx_href">CrossVQA: Scalably generating benchmarks for systematically testing
VQA generalization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 2148–2166, Online and Punta Cana,
Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. (2022)</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina
Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew
Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo
Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=EbMuimAbPbs" title="" class="ltx_ref ltx_href">Flamingo: a
visual language model for few-shot learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anderson et al. (2018)</span>
<span class="ltx_bibblock">
Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen
Gould, and Lei Zhang. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html" title="" class="ltx_ref ltx_href">Bottom-up and top-down attention for image captioning and visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 6077–6086.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. (2015)</span>
<span class="ltx_bibblock">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C. Lawrence Zitnick, and Devi Parikh. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_iccv_2015/html/Antol_VQA_Visual_Question_ICCV_2015_paper" title="" class="ltx_ref ltx_href">VQA: Visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf" title="" class="ltx_ref ltx_href">Language models are few-shot learners</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 33, pages 1877–1901. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bugliarello et al. (2021)</span>
<span class="ltx_bibblock">
Emanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki, and Desmond Elliott.
2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00408" title="" class="ltx_ref ltx_href">Multimodal pretraining
unmasked: A meta-analysis and a unified framework of vision-and-language
BERTs</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:978–994.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Changpinyo et al. (2021)</span>
<span class="ltx_bibblock">
Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Changpinyo_Conceptual_12M_Pushing_Web-Scale_Image-Text_Pre-Training_To_Recognize_Long-Tail_Visual_CVPR_2021_paper" title="" class="ltx_ref ltx_href">Conceptual 12m: Pushing web-scale image-text pre-training to recognize
long-tail visual concepts</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 3558–3568.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chao et al. (2018)</span>
<span class="ltx_bibblock">
Wei-Lun Chao, Hexiang Hu, and Fei Sha. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Chao_Cross-Dataset_Adaptation_for_CVPR_2018_paper" title="" class="ltx_ref ltx_href">Cross-dataset adaptation for visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-030-58577-8_7" title="" class="ltx_ref ltx_href">UNITER: Universal image-text representation learning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">European Conference on Computer Vision (ECCV)</em>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al. (2021)</span>
<span class="ltx_bibblock">
Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/cho21a.html" title="" class="ltx_ref ltx_href">Unifying
vision-and-language tasks via text generation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning</em>, volume 139 of <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 1931–1942. PMLR.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dancette et al. (2021)</span>
<span class="ltx_bibblock">
Corentin Dancette, Rémi Cadène, Damien Teney, and Matthieu Cord. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/html/Dancette_Beyond_Question-Based_Biases_Assessing_Multimodal_Shortcut_Learning_in_Visual_Question_ICCV_2021_paper.html" title="" class="ltx_ref ltx_href">Beyond question-based biases: Assessing multimodal shortcut learning in
visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em>, pages 1574–1583.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2009.5206848" title="" class="ltx_ref ltx_href">Imagenet: A
large-scale hierarchical image database</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2009 IEEE Conference on Computer Vision and Pattern
Recognition</em>, pages 248–255.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/N19-1423" title="" class="ltx_ref ltx_href">BERT: Pre-training of
deep bidirectional transformers for language understanding</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 4171–4186,
Minneapolis, Minnesota. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2021)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=YicbFdNTTy" title="" class="ltx_ref ltx_href">An image is worth
16x16 words: Transformers for image recognition at scale</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gokhale et al. (2020)</span>
<span class="ltx_bibblock">
Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.63" title="" class="ltx_ref ltx_href">MUTANT: A
training paradigm for out-of-distribution generalization in visual question
answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 878–892, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper" title="" class="ltx_ref ltx_href">Making the v in VQA matter: Elevating the role of image understanding in
visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurari et al. (2018)</span>
<span class="ltx_bibblock">
Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey P. Bigham. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2018/html/Gurari_VizWiz_Grand_Challenge_CVPR_2018_paper.html" title="" class="ltx_ref ltx_href">VizWiz grand challenge: Answering visual questions from blind people</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2020)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and
Dawn Song. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2020.acl-main.244" title="" class="ltx_ref ltx_href">Pretrained
transformers improve out-of-distribution robustness</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 2744–2751, Online. Association for
Computational Linguistics.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hudson and Manning (2019)</span>
<span class="ltx_bibblock">
Drew A. Hudson and Christopher D. Manning. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_CVPR_2019/html/Hudson_GQA_A_New_Dataset_for_Real-World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html" title="" class="ltx_ref ltx_href">GQA: A new dataset for real-world visual reasoning and compositional
question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jabri et al. (2016)</span>
<span class="ltx_bibblock">
Allan Jabri, Armand Joulin, and Laurens van der Maaten. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-46484-8_44" title="" class="ltx_ref ltx_href">Revisiting
visual question answering baselines</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2016</em>, pages 727–739, Cham.
Springer International Publishing.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jimenez et al. (2022)</span>
<span class="ltx_bibblock">
Carlos E. Jimenez, Olga Russakovsky, and Karthik Narasimhan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.443" title="" class="ltx_ref ltx_href">CARETS: A
consistency and robustness evaluative test suite for VQA</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 6392–6405,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2017)</span>
<span class="ltx_bibblock">
Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei,
C. Lawrence Zitnick, and Ross Girshick. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Johnson_CLEVR_A_Diagnostic_CVPR_2017_paper" title="" class="ltx_ref ltx_href">CLEVR: A diagnostic dataset for compositional language and elementary
visual reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2017)</span>
<span class="ltx_bibblock">
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al.
2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://dl.acm.org/doi/10.1007/s11263-016-0981-7" title="" class="ltx_ref ltx_href">Visual
Genome: Connecting language and vision using crowdsourced dense image
annotations</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">International journal of computer vision</em>, 123(1):32–73.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021a)</span>
<span class="ltx_bibblock">
Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Chu Hong Hoi. 2021a.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2021/file/505259756244493872b7709a8a01b536-Paper.pdf" title="" class="ltx_ref ltx_href">Align before fuse: Vision and language representation learning with momentum
distillation</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 34, pages 9694–9705. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2021b)</span>
<span class="ltx_bibblock">
Linjie Li, Jie Lei, Zhe Gan, and Jingjing Liu. 2021b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Adversarial_VQA_A_New_Benchmark_for_Evaluating_the_Robustness_of_ICCV_2021_paper" title="" class="ltx_ref ltx_href">Adversarial VQA: A new benchmark for evaluating the robustness of vqa
models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">International Conference on Computer Vision (ICCV)</em>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://link.springer.com/chapter/10.1007/978-3-319-10602-1_48" title="" class="ltx_ref ltx_href">Microsoft
COCO: Common objects in context</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2014</em>, pages 740–755, Cham.
Springer International Publishing.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=Bkg6RiCqY7" title="" class="ltx_ref ltx_href">Decoupled weight
decay regularization</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2019)</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf" title="" class="ltx_ref ltx_href">ViLBERT: Pretraining task-agnostic visiolinguistic representations for
vision-and-language tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 32. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski and Fritz (2014)</span>
<span class="ltx_bibblock">
Mateusz Malinowski and Mario Fritz. 2014.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2014/file/d516b13671a4179d9b7b458a6ebdeb92-Paper.pdf" title="" class="ltx_ref ltx_href">A multi-world approach to question answering about real-world scenes based
on uncertain input</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 27. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miller (1995)</span>
<span class="ltx_bibblock">
George A. Miller. 1995.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/219717.219748" title="" class="ltx_ref ltx_href">Wordnet: A lexical
database for english</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Commun. ACM</em>, 38(11):39–41.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu et al. (2021)</span>
<span class="ltx_bibblock">
Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong
Wen. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Niu_Counterfactual_VQA_A_Cause-Effect_Look_at_Language_Bias_CVPR_2021_paper" title="" class="ltx_ref ltx_href">Counterfactual VQA: A cause-effect look at language bias</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, pages 12700–12710.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ordonez et al. (2011)</span>
<span class="ltx_bibblock">
Vicente Ordonez, Girish Kulkarni, and Tamara Berg. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf" title="" class="ltx_ref ltx_href">Im2text: Describing images using 1 million captioned photographs</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 24. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Piergiovanni et al. (2022)</span>
<span class="ltx_bibblock">
AJ Piergiovanni, Wei Li, Weicheng Kuo, Mohammad Saffar, Fred Bertsch, and
Anelia Angelova. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2205.00949" title="" class="ltx_ref ltx_href">Answer-Me: Multi-task
open-vocabulary visual question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.00949</em>.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/radford21a.html" title="" class="ltx_ref ltx_href">Learning
transferable visual models from natural language supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning</em>, volume 139 of <em id="bib.bib38.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 8748–8763. PMLR.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2015)</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf" title="" class="ltx_ref ltx_href">Faster R-CNN: Towards real-time object detection with region proposal
networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 28. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuhmann et al. (2021)</span>
<span class="ltx_bibblock">
Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk,
Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran
Komatsuzaki. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/2111.02114" title="" class="ltx_ref ltx_href">LAION-400M: Open dataset
of clip-filtered 400 million image-text pairs</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.02114</em>.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al. (2018)</span>
<span class="ltx_bibblock">
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. 2018.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/P18-1238" title="" class="ltx_ref ltx_href">Conceptual Captions:
A cleaned, hypernymed, image alt-text dataset for automatic image
captioning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2556–2565,
Melbourne, Australia. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al. (2021)</span>
<span class="ltx_bibblock">
Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Magana, Tristan Thrush,
Wojciech Galuba, Devi Parikh, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2021/file/aa97d584861474f4097cf13ccb5325da-Paper.pdf" title="" class="ltx_ref ltx_href">Human-adversarial visual question answering</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 34, pages 20346–20359. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2022)</span>
<span class="ltx_bibblock">
Haoyu Song, Li Dong, Weinan Zhang, Ting Liu, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.18653/v1/2022.acl-long.421" title="" class="ltx_ref ltx_href">CLIP models
are few-shot learners: Empirical studies on VQA and visual entailment</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 6088–6100,
Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teney et al. (2020)</span>
<span class="ltx_bibblock">
Damien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher
Kanan, and Anton van den Hengel. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2020/file/045117b0e0a11a242b9765e79cbf113f-Paper.pdf" title="" class="ltx_ref ltx_href">On the value of out-of-distribution testing: An example of
goodhart's law</a>.

</span>
<span class="ltx_bibblock">33:407–417.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torralba and Efros (2011)</span>
<span class="ltx_bibblock">
Antonio Torralba and Alexei A. Efros. 2011.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/CVPR.2011.5995347" title="" class="ltx_ref ltx_href">Unbiased look at
dataset bias</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">CVPR 2011</em>, pages 1521–1528.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2021)</span>
<span class="ltx_bibblock">
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
Sablayrolles, and Herve Jegou. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.mlr.press/v139/touvron21a.html" title="" class="ltx_ref ltx_href">Training
data-efficient image transformers &amp; distillation through attention</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 38th International Conference on Machine
Learning</em>, volume 139 of <em id="bib.bib46.2.2" class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>,
pages 10347–10357. PMLR.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tsimpoukelli et al. (2021)</span>
<span class="ltx_bibblock">
Maria Tsimpoukelli, Jacob L. Menick, Serkan Cabi, S. M. Ali Eslami, Oriol
Vinyals, and Felix Hill. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper/2021/file/01b7575c38dac42f3cfb7d500438b875-Paper.pdf" title="" class="ltx_ref ltx_href">Multimodal few-shot learning with frozen language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volume 34, pages 200–212. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=GUrhfTuf_3" title="" class="ltx_ref ltx_href">SimVLM: Simple
visual language model pretraining with weak supervision</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2016)</span>
<span class="ltx_bibblock">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1609.08144" title="" class="ltx_ref ltx_href">Google’s neural machine
translation system: Bridging the gap between human and machine translation</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1609.08144</em>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2019)</span>
<span class="ltx_bibblock">
Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://arxiv.org/abs/1901.06706" title="" class="ltx_ref ltx_href">Visual entailment: A novel
task for fine-grained image understanding</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1901.06706</em>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2017)</span>
<span class="ltx_bibblock">
Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content_cvpr_2017/html/Xie_Aggregated_Residual_Transformations_CVPR_2017_paper" title="" class="ltx_ref ltx_href">Aggregated residual transformations for deep neural networks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em>.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2015)</span>
<span class="ltx_bibblock">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alexander J. Smola. 2015.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1511.02274" title="" class="ltx_ref ltx_href">Stacked attention networks
for image question answering</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1511.02274.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2021)</span>
<span class="ltx_bibblock">
Mingda Zhang, Tristan D. Maidment, Ahmad Diab, Adriana Kovashka, and Rebecca
Hwa. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Domain-Robust_VQA_With_Diverse_Datasets_and_Methods_but_No_Target_CVPR_2021_paper" title="" class="ltx_ref ltx_href">Domain-robust VQA with diverse datasets and methods but no target labels</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">2021 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 7042–7052.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental Setup Details</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">In this section, we report additional details regarding our experimental setup.</p>
</div>
<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Models</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">We evaluate the performance of two strong models, <span id="A1.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a href="#bib.bib32" title="" class="ltx_ref">2019</a>)</cite> and <span id="A1.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite>.
These models belong to the family of pretrained Transformer that has recently achieved state-of-the-art performance on several V&amp;L tasks, and are specifically instances of dual-stream architectures <cite class="ltx_cite ltx_citemacro_cite">Bugliarello et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>.
In this paradigm, models are first pretrained on a large collection of image–caption pairs, and then fine-tuned to solve specific downstream tasks.
<span id="A1.SS1.p1.1.3" class="ltx_text ltx_font_smallcaps">ViLBERT</span> is pretrained using three objectives, masked language modeling (MLM; <cite class="ltx_cite ltx_citemacro_citet">Devlin et al. <a href="#bib.bib17" title="" class="ltx_ref">2019</a></cite>), masked region modeling and image–text matching (ITM; <cite class="ltx_cite ltx_citemacro_citet">Chen et al. <a href="#bib.bib13" title="" class="ltx_ref">2020</a></cite>).
<span id="A1.SS1.p1.1.4" class="ltx_text ltx_font_smallcaps">ALBEF</span> is pretrained using MLM, ITM and an image–text contrastive loss <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite>.
We refer the reader to <a href="#S3" title="3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3</span></a> for an overall description of these models.
<a href="#A1.T4" title="In ALBEF. ‣ A.1 Models ‣ Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">4</span></a> lists pretraining and architecture details for both models.
All the models were fine-tuned using the AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib31" title="" class="ltx_ref">2019</a>)</cite> optimizer, with model-specific hyperparameters in <a href="#A1.T6" title="In ALBEF. ‣ A.1 Models ‣ Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">
<span id="A1.SS1.SSS0.Px1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span>.</h4>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS1.SSS0.Px1.p1.1" class="ltx_p">In this model, the textual inputs are first processed through 6 Transformer layers, before being combined with visual inputs through inter- and intra-modal attention layers.
We re-implement this architecture, and confirm comparable performance by reproducing its results with the ones obtained through the official codebase<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a target="_blank" href="https://github.com/facebookresearch/vilbert-multi-task/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/facebookresearch/vilbert-multi-task/</a>.</span></span></span> (see <a href="#A1.T5" title="In ALBEF. ‣ A.1 Models ‣ Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">5</span></a> for IID performance of both implementations).
A key difference between the two implementations is in the image features: while the official model uses 10–100 regions of interest (RoIs) from a ResNeXt-152 <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib51" title="" class="ltx_ref">2017</a>)</cite>, our re-implementation relies on 100 RoIs extracted by Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a href="#bib.bib39" title="" class="ltx_ref">2015</a>)</cite> trained on <span id="A1.SS1.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">VG</span>.</p>
</div>
<div id="A1.SS1.SSS0.Px1.p2" class="ltx_para">
<p id="A1.SS1.SSS0.Px1.p2.1" class="ltx_p">We then extend our codebase to implement a generative version of <span id="A1.SS1.SSS0.Px1.p2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> by replacing the discriminative decoder with an autoregressive decoding head.<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Our implementation of the generative decoder follows that of <span id="footnote7.1" class="ltx_text ltx_font_typewriter">TransformerDecoder</span> available at <a target="_blank" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/transformer.py</a>.</span></span></span>
The decoder is trained with teacher-forcing, and in datasets with several responses, the most frequent response is used as the ground truth response. Pretraining on CC, when used, is done by generating text. We also examine the effect of pretraining on both the encoder and decoder, and find the learning to be more stable when using only pretrained encoder, although further hyperparameter exploration could mitigate this difference. We study the effects of multimodal pretraining on the Conceptual Captions dataset <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite> with 3M images.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">
<span id="A1.SS1.SSS0.Px2.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span>.</h4>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS1.SSS0.Px2.p1.2" class="ltx_p">Like <span id="A1.SS1.SSS0.Px2.p1.2.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span>, <span id="A1.SS1.SSS0.Px2.p1.2.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> is a dual-stream encoder but with two main differences: first, the visual inputs are image patches that are processed through a vision Transformer <cite class="ltx_cite ltx_citemacro_cite">Dosovitskiy et al. (<a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite>; and second, the cross-modal interactions happen through standard Transformer cross-attention at each layer (whereas <span id="A1.SS1.SSS0.Px2.p1.2.3" class="ltx_text ltx_font_smallcaps">ViLBERT</span> uses co-attention layers specifically designed for intra- and inter-modal interactions).
In addition, the model is trained with pseudo-targets that are generated from a moving-average version of its weights.
We run experiments using the official codebase.<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/salesforce/ALBEF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/salesforce/ALBEF</a>.</span></span></span>
The visual backbone is a DeIT-B/16 <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib46" title="" class="ltx_ref">2021</a>)</cite> pretrained on ImageNet-1k <cite class="ltx_cite ltx_citemacro_cite">Deng et al. (<a href="#bib.bib16" title="" class="ltx_ref">2009</a>)</cite> at resolution 224<math id="A1.SS1.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.SS1.SSS0.Px2.p1.1.m1.1a"><mo id="A1.SS1.SSS0.Px2.p1.1.m1.1.1" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.1.m1.1b"><times id="A1.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A1.SS1.SSS0.Px2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.1.m1.1c">\times</annotation></semantics></math>224, and further trained during the multimodal pretraining phase.
For the downstream VQA benchmarks, we follow the authors and resize input images to 384<math id="A1.SS1.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="A1.SS1.SSS0.Px2.p1.2.m2.1a"><mo id="A1.SS1.SSS0.Px2.p1.2.m2.1.1" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A1.SS1.SSS0.Px2.p1.2.m2.1b"><times id="A1.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A1.SS1.SSS0.Px2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSS0.Px2.p1.2.m2.1c">\times</annotation></semantics></math>384 and apply random augmentation during fine-tuning.
<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a href="#bib.bib28" title="" class="ltx_ref">2021a</a>)</cite> formulated the VQA task as generative by adding a 6-layer Transformer decoder initialized from the pretrained encoder.
We follow this approach and also evaluate a discriminative version by learning a two-layer MLP with ReLU <cite class="ltx_cite ltx_citemacro_cite">Agarap (<a href="#bib.bib1" title="" class="ltx_ref">2018</a>)</cite> non-linearity in between, following the authors’ setup for the Visual Entailment benchmark <cite class="ltx_cite ltx_citemacro_cite">Xie et al. (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>.
We found the hyperparameters proposed by <cite class="ltx_cite ltx_citemacro_citet">Bugliarello et al. (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> to work better.
During inference, we evaluate <span id="A1.SS1.SSS0.Px2.p1.2.4" class="ltx_text ltx_font_smallcaps">ALBEF</span> in two ways: first, following the authors, we rank the in-domain candidate answers based on their likelihood; second, we let the model generate any possible answer in an open-ended fashion through greedy decoding.
We found these two approaches to minimally affect final performance (see <a href="#A1.T7" title="In ALBEF. ‣ A.1 Models ‣ Appendix A Experimental Setup Details ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">7</span></a>).
Unless otherwise specified, we report results given by generation as it reflects open-ended question answering.</p>
</div>
<figure id="A1.T4" class="ltx_table">
<div id="A1.T4.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:208.1pt;height:42.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-71.9pt,14.7pt) scale(0.591339889044523,0.591339889044523) ;">
<table id="A1.T4.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T4.1.1.1.1" class="ltx_tr">
<th id="A1.T4.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A1.T4.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.1.1.2.1" class="ltx_text ltx_font_bold"># Params</span></th>
<th id="A1.T4.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Pretrain data</span></th>
<th id="A1.T4.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.1.1.4.1" class="ltx_text ltx_font_bold"># Images</span></th>
<th id="A1.T4.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.1.1.5.1" class="ltx_text ltx_font_bold"># Captions</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T4.1.1.2.1" class="ltx_tr">
<td id="A1.T4.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T4.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span></td>
<td id="A1.T4.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">240M</td>
<td id="A1.T4.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">CC</td>
<td id="A1.T4.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">3.3M</td>
<td id="A1.T4.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">3.3M</td>
</tr>
<tr id="A1.T4.1.1.3.2" class="ltx_tr">
<td id="A1.T4.1.1.3.2.1" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;">
<span id="A1.T4.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> (4M)</td>
<td id="A1.T4.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;">450M</td>
<td id="A1.T4.1.1.3.2.3" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;">+COCO+SBU+VG</td>
<td id="A1.T4.1.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">4M</td>
<td id="A1.T4.1.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">5M</td>
</tr>
<tr id="A1.T4.1.1.4.3" class="ltx_tr">
<td id="A1.T4.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">
<span id="A1.T4.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> (14M)</td>
<td id="A1.T4.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb ltx_border_r" style="padding-left:3.5pt;padding-right:3.5pt;">450M</td>
<td id="A1.T4.1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">     +C12M</td>
<td id="A1.T4.1.1.4.3.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">14M</td>
<td id="A1.T4.1.1.4.3.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">15M</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Pretrained models details. ALBEF size includes both the main model and its moving average. Pretraining data: CC <cite class="ltx_cite ltx_citemacro_cite">Sharma et al. (<a href="#bib.bib41" title="" class="ltx_ref">2018</a>)</cite>, COCO <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib30" title="" class="ltx_ref">2014</a>)</cite>, SBU <cite class="ltx_cite ltx_citemacro_cite">Ordonez et al. (<a href="#bib.bib36" title="" class="ltx_ref">2011</a>)</cite>, VG <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite>, C12M <cite class="ltx_cite ltx_citemacro_cite">Changpinyo et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite>.</figcaption>
</figure>
<figure id="A1.T5" class="ltx_table">
<div id="A1.T5.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:208.1pt;height:46.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.3pt,3.9pt) scale(0.857357298183679,0.857357298183679) ;">
<table id="A1.T5.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T5.3.1.1.1" class="ltx_tr">
<th id="A1.T5.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T5.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A1.T5.3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T5.3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A1.T5.3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T5.3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A1.T5.3.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T5.3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A1.T5.3.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T5.3.1.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T5.3.1.2.1" class="ltx_tr">
<th id="A1.T5.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">Official codebase</th>
<td id="A1.T5.3.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">67.04</td>
<td id="A1.T5.3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">66.78</td>
<td id="A1.T5.3.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">40.69</td>
<td id="A1.T5.3.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">44.46</td>
</tr>
<tr id="A1.T5.3.1.3.2" class="ltx_tr">
<th id="A1.T5.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">Re-implementation</th>
<td id="A1.T5.3.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">65.75</td>
<td id="A1.T5.3.1.3.2.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">61.51</td>
<td id="A1.T5.3.1.3.2.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">40.31</td>
<td id="A1.T5.3.1.3.2.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">47.46</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison between the official and our codebases for <span id="A1.T5.2.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A1.T5.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A1.T5.2.1.m1.1b"><msub id="A1.T5.2.1.m1.1.1" xref="A1.T5.2.1.m1.1.1.cmml"><mi id="A1.T5.2.1.m1.1.1b" xref="A1.T5.2.1.m1.1.1.cmml"></mi><mtext id="A1.T5.2.1.m1.1.1.1" xref="A1.T5.2.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T5.2.1.m1.1c"><apply id="A1.T5.2.1.m1.1.1.cmml" xref="A1.T5.2.1.m1.1.1"><ci id="A1.T5.2.1.m1.1.1.1a.cmml" xref="A1.T5.2.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T5.2.1.m1.1.1.1.cmml" xref="A1.T5.2.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T5.2.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span> in the IID setting.</figcaption>
</figure>
<figure id="A1.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.T6.st1" class="ltx_table ltx_figure_panel">
<div id="A1.T6.st1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:154pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(90.1pt,-32.0pt) scale(1.71109250822754,1.71109250822754) ;">
<table id="A1.T6.st1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T6.st1.1.1.1.1" class="ltx_tr">
<th id="A1.T6.st1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Benchmark</span></th>
<th id="A1.T6.st1.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Qn len</span></th>
<th id="A1.T6.st1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.3.1" class="ltx_text ltx_font_bold"># Classes</span></th>
<th id="A1.T6.st1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">BS</span></th>
<th id="A1.T6.st1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">LR</span></th>
<th id="A1.T6.st1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.1.1.6.1" class="ltx_text ltx_font_bold"># Epochs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T6.st1.1.1.2.1" class="ltx_tr">
<th id="A1.T6.st1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A1.T6.st1.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st1.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">3,129</td>
<td id="A1.T6.st1.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st1.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">4e-5</td>
<td id="A1.T6.st1.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st1.1.1.3.2" class="ltx_tr">
<th id="A1.T6.st1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A1.T6.st1.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">26</td>
<td id="A1.T6.st1.1.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">1,533</td>
<td id="A1.T6.st1.1.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st1.1.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">4e-5</td>
<td id="A1.T6.st1.1.1.3.2.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st1.1.1.4.3" class="ltx_tr">
<th id="A1.T6.st1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A1.T6.st1.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st1.1.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">3,449</td>
<td id="A1.T6.st1.1.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st1.1.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">4e-5</td>
<td id="A1.T6.st1.1.1.4.3.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st1.1.1.5.4" class="ltx_tr">
<th id="A1.T6.st1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st1.1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A1.T6.st1.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">40</td>
<td id="A1.T6.st1.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">3,112</td>
<td id="A1.T6.st1.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st1.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">4e-5</td>
<td id="A1.T6.st1.1.1.5.4.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Parameters used for <span id="A1.T6.st1.3.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> models. The internal codebase uses LAMB optimizer with the initial LR of 1e-3, with the best checkpoint selected on eval dataset.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.T6.st2" class="ltx_table ltx_figure_panel">
<div id="A1.T6.st2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:154pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(90.1pt,-32.0pt) scale(1.71109250822754,1.71109250822754) ;">
<table id="A1.T6.st2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T6.st2.1.1.1.1" class="ltx_tr">
<th id="A1.T6.st2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Benchmark</span></th>
<th id="A1.T6.st2.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Qn len</span></th>
<th id="A1.T6.st2.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.3.1" class="ltx_text ltx_font_bold"># Classes</span></th>
<th id="A1.T6.st2.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.4.1" class="ltx_text ltx_font_bold">BS</span></th>
<th id="A1.T6.st2.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.5.1" class="ltx_text ltx_font_bold">LR</span></th>
<th id="A1.T6.st2.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.1.1.6.1" class="ltx_text ltx_font_bold"># Epochs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T6.st2.1.1.2.1" class="ltx_tr">
<th id="A1.T6.st2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A1.T6.st2.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st2.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">3,129</td>
<td id="A1.T6.st2.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st2.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">1e-4</td>
<td id="A1.T6.st2.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st2.1.1.3.2" class="ltx_tr">
<th id="A1.T6.st2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A1.T6.st2.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">26</td>
<td id="A1.T6.st2.1.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">1,533</td>
<td id="A1.T6.st2.1.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st2.1.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">1e-4</td>
<td id="A1.T6.st2.1.1.3.2.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st2.1.1.4.3" class="ltx_tr">
<th id="A1.T6.st2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A1.T6.st2.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st2.1.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">3,449</td>
<td id="A1.T6.st2.1.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st2.1.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">1e-4</td>
<td id="A1.T6.st2.1.1.4.3.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st2.1.1.5.4" class="ltx_tr">
<th id="A1.T6.st2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st2.1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A1.T6.st2.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">40</td>
<td id="A1.T6.st2.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">3,129</td>
<td id="A1.T6.st2.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st2.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">1e-4</td>
<td id="A1.T6.st2.1.1.5.4.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>Discriminative <span id="A1.T6.st2.3.1" class="ltx_text ltx_font_smallcaps">ALBEF</span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.T6.st3" class="ltx_table ltx_figure_panel">
<div id="A1.T6.st3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:150.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.4pt,-30.4pt) scale(1.67492457372196,1.67492457372196) ;">
<table id="A1.T6.st3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T6.st3.1.1.1.1" class="ltx_tr">
<th id="A1.T6.st3.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Benchmark</span></th>
<th id="A1.T6.st3.1.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Qn len</span></th>
<th id="A1.T6.st3.1.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Answer len</span></th>
<th id="A1.T6.st3.1.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.4.1" class="ltx_text ltx_font_bold">BS</span></th>
<th id="A1.T6.st3.1.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.5.1" class="ltx_text ltx_font_bold">LR</span></th>
<th id="A1.T6.st3.1.1.1.1.6" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.1.1.6.1" class="ltx_text ltx_font_bold"># Epochs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T6.st3.1.1.2.1" class="ltx_tr">
<td id="A1.T6.st3.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A1.T6.st3.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st3.1.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">6</td>
<td id="A1.T6.st3.1.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st3.1.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">2e-5</td>
<td id="A1.T6.st3.1.1.2.1.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st3.1.1.3.2" class="ltx_tr">
<td id="A1.T6.st3.1.1.3.2.1" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A1.T6.st3.1.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">26</td>
<td id="A1.T6.st3.1.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">5</td>
<td id="A1.T6.st3.1.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st3.1.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">2e-5</td>
<td id="A1.T6.st3.1.1.3.2.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st3.1.1.4.3" class="ltx_tr">
<td id="A1.T6.st3.1.1.4.3.1" class="ltx_td ltx_align_left" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A1.T6.st3.1.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">16</td>
<td id="A1.T6.st3.1.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">8</td>
<td id="A1.T6.st3.1.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st3.1.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">2e-5</td>
<td id="A1.T6.st3.1.1.4.3.6" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
<tr id="A1.T6.st3.1.1.5.4" class="ltx_tr">
<td id="A1.T6.st3.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T6.st3.1.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A1.T6.st3.1.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">40</td>
<td id="A1.T6.st3.1.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">11</td>
<td id="A1.T6.st3.1.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">256</td>
<td id="A1.T6.st3.1.1.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">2e-5</td>
<td id="A1.T6.st3.1.1.5.4.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">20</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(c) </span>Generative <span id="A1.T6.st3.3.1" class="ltx_text ltx_font_smallcaps">ALBEF</span>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Hyperparameters used in our experiments. Question and answer lengths are in tokens, BS is the batch size, LR is the learning rate.</figcaption>
</figure>
<figure id="A1.T7" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.T7.st1" class="ltx_table ltx_figure_panel">
<table id="A1.T7.st1.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T7.st1.3.1.1" class="ltx_tr">
<th id="A1.T7.st1.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="A1.T7.st1.3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A1.T7.st1.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A1.T7.st1.3.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A1.T7.st1.3.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T7.st1.3.2.1" class="ltx_tr">
<th id="A1.T7.st1.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A1.T7.st1.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">72.37</td>
<td id="A1.T7.st1.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">50.56</td>
<td id="A1.T7.st1.3.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">38.94</td>
<td id="A1.T7.st1.3.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">19.81</td>
</tr>
<tr id="A1.T7.st1.3.3.2" class="ltx_tr">
<th id="A1.T7.st1.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A1.T7.st1.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">50.32</td>
<td id="A1.T7.st1.3.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">64.26</td>
<td id="A1.T7.st1.3.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">22.80</td>
<td id="A1.T7.st1.3.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">12.51</td>
</tr>
<tr id="A1.T7.st1.3.4.3" class="ltx_tr">
<th id="A1.T7.st1.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A1.T7.st1.3.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">33.40</td>
<td id="A1.T7.st1.3.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">24.99</td>
<td id="A1.T7.st1.3.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">43.35</td>
<td id="A1.T7.st1.3.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">12.60</td>
</tr>
<tr id="A1.T7.st1.3.5.4" class="ltx_tr">
<th id="A1.T7.st1.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st1.3.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A1.T7.st1.3.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">34.17</td>
<td id="A1.T7.st1.3.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">22.73</td>
<td id="A1.T7.st1.3.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">8.89</td>
<td id="A1.T7.st1.3.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">48.44</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Ranking-based evaluation of <span id="A1.T7.st1.2.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A1.T7.st1.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A1.T7.st1.2.1.m1.1b"><msub id="A1.T7.st1.2.1.m1.1.1" xref="A1.T7.st1.2.1.m1.1.1.cmml"><mi id="A1.T7.st1.2.1.m1.1.1b" xref="A1.T7.st1.2.1.m1.1.1.cmml"></mi><mtext id="A1.T7.st1.2.1.m1.1.1.1" xref="A1.T7.st1.2.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.st1.2.1.m1.1c"><apply id="A1.T7.st1.2.1.m1.1.1.cmml" xref="A1.T7.st1.2.1.m1.1.1"><ci id="A1.T7.st1.2.1.m1.1.1.1a.cmml" xref="A1.T7.st1.2.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T7.st1.2.1.m1.1.1.1.cmml" xref="A1.T7.st1.2.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.st1.2.1.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.T7.st2" class="ltx_table ltx_figure_panel">
<table id="A1.T7.st2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A1.T7.st2.3.1.1" class="ltx_tr">
<th id="A1.T7.st2.3.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="A1.T7.st2.3.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A1.T7.st2.3.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A1.T7.st2.3.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A1.T7.st2.3.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A1.T7.st2.3.2.1" class="ltx_tr">
<th id="A1.T7.st2.3.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A1.T7.st2.3.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">72.09</td>
<td id="A1.T7.st2.3.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">50.10</td>
<td id="A1.T7.st2.3.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">39.20</td>
<td id="A1.T7.st2.3.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">19.81</td>
</tr>
<tr id="A1.T7.st2.3.3.2" class="ltx_tr">
<th id="A1.T7.st2.3.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A1.T7.st2.3.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">50.33</td>
<td id="A1.T7.st2.3.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">64.24</td>
<td id="A1.T7.st2.3.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">22.79</td>
<td id="A1.T7.st2.3.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">12.50</td>
</tr>
<tr id="A1.T7.st2.3.4.3" class="ltx_tr">
<th id="A1.T7.st2.3.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A1.T7.st2.3.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">33.39</td>
<td id="A1.T7.st2.3.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">23.64</td>
<td id="A1.T7.st2.3.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">44.55</td>
<td id="A1.T7.st2.3.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">12.25</td>
</tr>
<tr id="A1.T7.st2.3.5.4" class="ltx_tr">
<th id="A1.T7.st2.3.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A1.T7.st2.3.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A1.T7.st2.3.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">34.44</td>
<td id="A1.T7.st2.3.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">22.80</td>
<td id="A1.T7.st2.3.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">9.13</td>
<td id="A1.T7.st2.3.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">47.14</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>Generation-based evaluation of <span id="A1.T7.st2.2.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A1.T7.st2.2.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A1.T7.st2.2.1.m1.1b"><msub id="A1.T7.st2.2.1.m1.1.1" xref="A1.T7.st2.2.1.m1.1.1.cmml"><mi id="A1.T7.st2.2.1.m1.1.1b" xref="A1.T7.st2.2.1.m1.1.1.cmml"></mi><mtext id="A1.T7.st2.2.1.m1.1.1.1" xref="A1.T7.st2.2.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.st2.2.1.m1.1c"><apply id="A1.T7.st2.2.1.m1.1.1.cmml" xref="A1.T7.st2.2.1.m1.1.1"><ci id="A1.T7.st2.2.1.m1.1.1.1a.cmml" xref="A1.T7.st2.2.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T7.st2.2.1.m1.1.1.1.cmml" xref="A1.T7.st2.2.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.st2.2.1.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance of <span id="A1.T7.3.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A1.T7.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A1.T7.3.1.m1.1b"><msub id="A1.T7.3.1.m1.1.1" xref="A1.T7.3.1.m1.1.1.cmml"><mi id="A1.T7.3.1.m1.1.1b" xref="A1.T7.3.1.m1.1.1.cmml"></mi><mtext id="A1.T7.3.1.m1.1.1.1" xref="A1.T7.3.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.3.1.m1.1c"><apply id="A1.T7.3.1.m1.1.1.cmml" xref="A1.T7.3.1.m1.1.1"><ci id="A1.T7.3.1.m1.1.1.1a.cmml" xref="A1.T7.3.1.m1.1.1.1"><mtext mathsize="70%" id="A1.T7.3.1.m1.1.1.1.cmml" xref="A1.T7.3.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.1.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span> (14M) when tested via ranking (top) and generation (bottom). Rows correspond to the fine-tuning datasets, columns correspond to the test benchmarks. The model performs similarly in both setups. We found similar results for <span id="A1.T7.4.2" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A1.T7.4.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A1.T7.4.2.m1.1b"><msub id="A1.T7.4.2.m1.1.1" xref="A1.T7.4.2.m1.1.1.cmml"><mi id="A1.T7.4.2.m1.1.1b" xref="A1.T7.4.2.m1.1.1.cmml"></mi><mtext id="A1.T7.4.2.m1.1.1.1" xref="A1.T7.4.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A1.T7.4.2.m1.1c"><apply id="A1.T7.4.2.m1.1.1.cmml" xref="A1.T7.4.2.m1.1.1"><ci id="A1.T7.4.2.m1.1.1.1a.cmml" xref="A1.T7.4.2.m1.1.1.1"><mtext mathsize="70%" id="A1.T7.4.2.m1.1.1.1.cmml" xref="A1.T7.4.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.4.2.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span> (4M).</figcaption>
</figure>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Datasets</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p"><a href="#S3.T1" title="In 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">1</span></a> lists statistics for each dataset in our study.
<span id="A1.SS2.p1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <cite class="ltx_cite ltx_citemacro_cite">Goyal et al. (<a href="#bib.bib20" title="" class="ltx_ref">2017</a>)</cite> is the most commonly used VQA dataset to date, it consists of 265K images and 1.1M question-image pairs, each with 10 ground-truth answers.
<span id="A1.SS2.p1.1.2" class="ltx_text ltx_font_smallcaps">VQA-CP</span> <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite> re-splits the <span id="A1.SS2.p1.1.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> dataset such that, for every question type, train and test sets have different prior distributions of answers.
<span id="A1.SS2.p1.1.4" class="ltx_text ltx_font_smallcaps">VG</span> <cite class="ltx_cite ltx_citemacro_cite">Krishna et al. (<a href="#bib.bib27" title="" class="ltx_ref">2017</a>)</cite> includes 108K images and 1.7M questions, each paired with a single answer, centered around either the full image or a specific region within it.
<span id="A1.SS2.p1.1.5" class="ltx_text ltx_font_smallcaps">GQA</span> <cite class="ltx_cite ltx_citemacro_cite">Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> is another large-scale effort (22M questions, each with one answer) that focuses on compositionality of template-generated questions for real-world images (from <span id="A1.SS2.p1.1.6" class="ltx_text ltx_font_smallcaps">VG</span>). Following prior work, we use the <span id="A1.SS2.p1.1.7" class="ltx_text ltx_font_smallcaps">GQA</span> <em id="A1.SS2.p1.1.8" class="ltx_emph ltx_font_italic">balanced</em> subset (1.5M questions).
Finally, <span id="A1.SS2.p1.1.9" class="ltx_text ltx_font_smallcaps">VizWiz</span> <cite class="ltx_cite ltx_citemacro_cite">Gurari et al. (<a href="#bib.bib21" title="" class="ltx_ref">2018</a>)</cite> is the only real-world VQA dataset as it was collected from visually impaired people. It consists of 31K image-question pairs, each paired with 10 answers.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Training Details</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.2" class="ltx_p">Following common practice, for discriminative models, we select the top-<math id="A1.SS3.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS3.p1.1.m1.1a"><mi id="A1.SS3.p1.1.m1.1.1" xref="A1.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.1.m1.1b"><ci id="A1.SS3.p1.1.m1.1.1.cmml" xref="A1.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.1.m1.1c">k</annotation></semantics></math> most frequent answers from the fine-tuning dataset, as the set of answer classes to perform classification over. Here <math id="A1.SS3.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS3.p1.2.m2.1a"><mi id="A1.SS3.p1.2.m2.1.1" xref="A1.SS3.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p1.2.m2.1b"><ci id="A1.SS3.p1.2.m2.1.1.cmml" xref="A1.SS3.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p1.2.m2.1c">k</annotation></semantics></math> is a dataset-dependent variable.
For <span id="A1.SS3.p1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="A1.SS3.p1.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>, we use the same answer sets as <span id="A1.SS3.p1.2.3" class="ltx_text ltx_font_smallcaps">ViLBERT</span> (3,129 and 1,533, respectively).
For <span id="A1.SS3.p1.2.4" class="ltx_text ltx_font_smallcaps">VizWiz</span>, we select the answers that appear at least 8 times in training and validation sets, for a total of 3,112 answers that cover 97% of the data.
For <span id="A1.SS3.p1.2.5" class="ltx_text ltx_font_smallcaps">VG</span>, we select the answers that appear at least 29 times in the dataset, for a total of 3,449 answers that cover 76.5% of the data.
Importantly, combined with the VQA accuracy metric defined above, this results in an upperbound to the accuracy that discriminative models can achieve in each dataset (see <a href="#S4.T2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p">All models are trained on the respective training sets and evaluated on the validation sets, which lets us conduct in-depth analyses that would otherwise be impossible to carry out on private test sets.
As there is no official split of <span id="A1.SS3.p2.1.1" class="ltx_text ltx_font_smallcaps">VG</span>, we randomly sample the data into training (60%) and validation (40%) such that no image appears in both splits.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Results</h2>

<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Evaluation with Shared Answer Sets</h4>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p1.3" class="ltx_p">While different answer sets are an apparent issue for discriminative models, they also impact the performance of generative models, as the number of data points for each answer class seen by the generative model during fine-tuning varies: data-points in top-<math id="A2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="A2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.1.m1.1c">k</annotation></semantics></math> answer set are more frequent than others (by definition of top-<math id="A2.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p1.2.m2.1a"><mi id="A2.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.2.m2.1b"><ci id="A2.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.2.m2.1c">k</annotation></semantics></math>).
In other words, even though a tokenizer used to produce an answer could generate it, it is unlikely (or less likely) to do so if it has not seen (or seen rarely) that combination of tokens during fine-tuning. Thus, even for generative models, we consider performance on top-<math id="A2.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="A2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="A2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px1.p1.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p1.3.m3.1c">k</annotation></semantics></math> most frequent classes for each benchmark.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p2.3" class="ltx_p">Thus, we report the accuracy on the subset of test questions whose answers are shared between <em id="A2.SS0.SSS0.Px1.p2.3.1" class="ltx_emph ltx_font_italic">both</em> the IID and the OOD models.
For instance, when comparing the performance of the <span id="A2.SS0.SSS0.Px1.p2.3.2" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="A2.SS0.SSS0.Px1.p2.3.3" class="ltx_text ltx_font_smallcaps">VG</span> fine-tuned models on the <span id="A2.SS0.SSS0.Px1.p2.3.4" class="ltx_text ltx_font_smallcaps">VQAv2</span> test set, we compute the average accuracy on those <span id="A2.SS0.SSS0.Px1.p2.3.5" class="ltx_text ltx_font_smallcaps">VQAv2</span> questions whose ground truth answers are present in the top-<math id="A2.SS0.SSS0.Px1.p2.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p2.1.m1.1a"><mi id="A2.SS0.SSS0.Px1.p2.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p2.1.m1.1b"><ci id="A2.SS0.SSS0.Px1.p2.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p2.1.m1.1c">k</annotation></semantics></math> answers from <span id="A2.SS0.SSS0.Px1.p2.3.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> as well as the top-<math id="A2.SS0.SSS0.Px1.p2.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p2.2.m2.1a"><mi id="A2.SS0.SSS0.Px1.p2.2.m2.1.1" xref="A2.SS0.SSS0.Px1.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p2.2.m2.1b"><ci id="A2.SS0.SSS0.Px1.p2.2.m2.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p2.2.m2.1c">k</annotation></semantics></math> answers from <span id="A2.SS0.SSS0.Px1.p2.3.7" class="ltx_text ltx_font_smallcaps">VG</span>: we extract the common answer labels (between <span id="A2.SS0.SSS0.Px1.p2.3.8" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="A2.SS0.SSS0.Px1.p2.3.9" class="ltx_text ltx_font_smallcaps">VG</span> top-<math id="A2.SS0.SSS0.Px1.p2.3.m3.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p2.3.m3.1a"><mi id="A2.SS0.SSS0.Px1.p2.3.m3.1.1" xref="A2.SS0.SSS0.Px1.p2.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p2.3.m3.1b"><ci id="A2.SS0.SSS0.Px1.p2.3.m3.1.1.cmml" xref="A2.SS0.SSS0.Px1.p2.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p2.3.m3.1c">k</annotation></semantics></math> answers) and compute performance on test questions belonging to these shared answer labels only.</p>
</div>
<div id="A2.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p3.1" class="ltx_p">For IID evaluations, there are several possible ways to define shared answer sets based on OOD vocabs. While a subset is shown in <a href="#S4.F2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#A2.T10" title="In Effect of pretraining data size on ALBEF ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">10</span></a> lists the VQA accuracy of each model in the IID settings when evaluated on the questions in the test sets whose answers are shared between the top-<math id="A2.SS0.SSS0.Px1.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS0.SSS0.Px1.p3.1.m1.1a"><mi id="A2.SS0.SSS0.Px1.p3.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="A2.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p3.1.m1.1c">k</annotation></semantics></math> answers in both the IID and the OOD settings (see <a href="#S4.SS1" title="4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> for more details).</p>
</div>
<div id="A2.SS0.SSS0.Px1.p4" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p4.4" class="ltx_p">In some IID cases, restricting the answer set to common answers hurts the performance (indicated as a lack of dotted bar in <a href="#S4.F2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>). Interestingly, this pattern is observed across all models for some IID evaluations where the shared answer set is computed with respect to the <span id="A2.SS0.SSS0.Px1.p4.4.5" class="ltx_text ltx_font_smallcaps">VG</span> benchmark only: <span id="A2.SS0.SSS0.Px1.p4.4.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> for <span id="A2.SS0.SSS0.Px1.p4.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.SS0.SSS0.Px1.p4.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p4.1.1.m1.1a"><msub id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1a" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p4.1.1.m1.1b"><apply id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1"><ci id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p4.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> (-7.65 pp drop), <span id="A2.SS0.SSS0.Px1.p4.4.7" class="ltx_text ltx_font_smallcaps">VQAv2</span> (-8.65 pp drop) and <span id="A2.SS0.SSS0.Px1.p4.4.8" class="ltx_text ltx_font_smallcaps">GQA</span> (-8.00 pp drop) for <span id="A2.SS0.SSS0.Px1.p4.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.SS0.SSS0.Px1.p4.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p4.2.2.m1.1a"><msub id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1a" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p4.2.2.m1.1b"><apply id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1"><ci id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p4.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>, <span id="A2.SS0.SSS0.Px1.p4.4.9" class="ltx_text ltx_font_smallcaps">VQAv2</span> (-6.86 pp drop) for <span id="A2.SS0.SSS0.Px1.p4.3.3" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.SS0.SSS0.Px1.p4.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p4.3.3.m1.1a"><msub id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1a" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p4.3.3.m1.1b"><apply id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1"><ci id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p4.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>, and <span id="A2.SS0.SSS0.Px1.p4.4.10" class="ltx_text ltx_font_smallcaps">VQAv2</span> (-6.89 pp drop) for <span id="A2.SS0.SSS0.Px1.p4.4.4" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.SS0.SSS0.Px1.p4.4.4.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.SS0.SSS0.Px1.p4.4.4.m1.1a"><msub id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1a" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px1.p4.4.4.m1.1b"><apply id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1"><ci id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px1.p4.4.4.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px1.p4.4.4.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>.
This seems to indicate that the <span id="A2.SS0.SSS0.Px1.p4.4.11" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="A2.SS0.SSS0.Px1.p4.4.12" class="ltx_text ltx_font_smallcaps">VQAv2</span> questions corresponding to shared ans set with <span id="A2.SS0.SSS0.Px1.p4.4.13" class="ltx_text ltx_font_smallcaps">VG</span> are more difficult than the average difficulty of these test sets.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.T8.st1" class="ltx_table ltx_figure_panel">
<table id="A2.T8.st1.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T8.st1.3.3.4.1" class="ltx_tr">
<th id="A2.T8.st1.3.3.4.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="A2.T8.st1.3.3.4.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.4.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A2.T8.st1.3.3.4.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.4.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A2.T8.st1.3.3.4.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.4.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A2.T8.st1.3.3.4.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.4.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T8.st1.1.1.1" class="ltx_tr">
<th id="A2.T8.st1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A2.T8.st1.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.41</td>
<td id="A2.T8.st1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.51</td>
<td id="A2.T8.st1.1.1.1.1" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.11<sup id="A2.T8.st1.1.1.1.1.1" class="ltx_sup">∧</sup>
</td>
</tr>
<tr id="A2.T8.st1.2.2.2" class="ltx_tr">
<th id="A2.T8.st1.2.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.2.2.2.2.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A2.T8.st1.2.2.2.3" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.25</td>
<td id="A2.T8.st1.2.2.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st1.2.2.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.44</td>
<td id="A2.T8.st1.2.2.2.1" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.14<sup id="A2.T8.st1.2.2.2.1.1" class="ltx_sup">∧</sup>
</td>
</tr>
<tr id="A2.T8.st1.3.3.3" class="ltx_tr">
<th id="A2.T8.st1.3.3.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.3.2.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A2.T8.st1.3.3.3.3" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.28</td>
<td id="A2.T8.st1.3.3.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.38</td>
<td id="A2.T8.st1.3.3.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st1.3.3.3.1" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.03<sup id="A2.T8.st1.3.3.3.1.1" class="ltx_sup">∧</sup>
</td>
</tr>
<tr id="A2.T8.st1.3.3.5.1" class="ltx_tr">
<th id="A2.T8.st1.3.3.5.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st1.3.3.5.1.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A2.T8.st1.3.3.5.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.46</td>
<td id="A2.T8.st1.3.3.5.1.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.54</td>
<td id="A2.T8.st1.3.3.5.1.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.48</td>
<td id="A2.T8.st1.3.3.5.1.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(a) </span>Discriminative <span id="A2.T8.st1.5.1" class="ltx_text ltx_font_smallcaps">ALBEF</span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A2.T8.st2" class="ltx_table ltx_figure_panel">
<table id="A2.T8.st2.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T8.st2.1.1.2.1" class="ltx_tr">
<th id="A2.T8.st2.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="A2.T8.st2.1.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.2.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A2.T8.st2.1.1.2.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.2.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A2.T8.st2.1.1.2.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.2.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A2.T8.st2.1.1.2.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.2.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T8.st2.1.1.3.1" class="ltx_tr">
<th id="A2.T8.st2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A2.T8.st2.1.1.3.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st2.1.1.3.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.45</td>
<td id="A2.T8.st2.1.1.3.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.48</td>
<td id="A2.T8.st2.1.1.3.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.26</td>
</tr>
<tr id="A2.T8.st2.1.1.4.2" class="ltx_tr">
<th id="A2.T8.st2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.4.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A2.T8.st2.1.1.4.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.26</td>
<td id="A2.T8.st2.1.1.4.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st2.1.1.4.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.45</td>
<td id="A2.T8.st2.1.1.4.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.22</td>
</tr>
<tr id="A2.T8.st2.1.1.1" class="ltx_tr">
<th id="A2.T8.st2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A2.T8.st2.1.1.1.3" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.27</td>
<td id="A2.T8.st2.1.1.1.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.35</td>
<td id="A2.T8.st2.1.1.1.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st2.1.1.1.1" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.09<sup id="A2.T8.st2.1.1.1.1.1" class="ltx_sup">∧</sup>
</td>
</tr>
<tr id="A2.T8.st2.1.1.5.3" class="ltx_tr">
<th id="A2.T8.st2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st2.1.1.5.3.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A2.T8.st2.1.1.5.3.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.48</td>
<td id="A2.T8.st2.1.1.5.3.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.56</td>
<td id="A2.T8.st2.1.1.5.3.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.52</td>
<td id="A2.T8.st2.1.1.5.3.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(b) </span>Generative <span id="A2.T8.st2.3.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span>.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A2.T8.st3" class="ltx_table ltx_figure_panel">
<table id="A2.T8.st3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T8.st3.1.1.1" class="ltx_tr">
<th id="A2.T8.st3.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"></th>
<th id="A2.T8.st3.1.1.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A2.T8.st3.1.1.1.3" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A2.T8.st3.1.1.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A2.T8.st3.1.1.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T8.st3.1.2.1" class="ltx_tr">
<th id="A2.T8.st3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A2.T8.st3.1.2.1.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st3.1.2.1.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.49</td>
<td id="A2.T8.st3.1.2.1.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.50</td>
<td id="A2.T8.st3.1.2.1.5" class="ltx_td ltx_align_right ltx_border_t" style="padding-left:3.5pt;padding-right:3.5pt;">0.27</td>
</tr>
<tr id="A2.T8.st3.1.3.2" class="ltx_tr">
<th id="A2.T8.st3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A2.T8.st3.1.3.2.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.30</td>
<td id="A2.T8.st3.1.3.2.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st3.1.3.2.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.45</td>
<td id="A2.T8.st3.1.3.2.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.34</td>
</tr>
<tr id="A2.T8.st3.1.4.3" class="ltx_tr">
<th id="A2.T8.st3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A2.T8.st3.1.4.3.2" class="ltx_td ltx_nopad_l ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.25</td>
<td id="A2.T8.st3.1.4.3.3" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.38</td>
<td id="A2.T8.st3.1.4.3.4" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
<td id="A2.T8.st3.1.4.3.5" class="ltx_td ltx_align_right" style="padding-left:3.5pt;padding-right:3.5pt;">0.16</td>
</tr>
<tr id="A2.T8.st3.1.5.4" class="ltx_tr">
<th id="A2.T8.st3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;"><span id="A2.T8.st3.1.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A2.T8.st3.1.5.4.2" class="ltx_td ltx_nopad_l ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.50</td>
<td id="A2.T8.st3.1.5.4.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.57</td>
<td id="A2.T8.st3.1.5.4.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">0.51</td>
<td id="A2.T8.st3.1.5.4.5" class="ltx_td ltx_align_right ltx_border_bb" style="padding-left:3.5pt;padding-right:3.5pt;">–</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">(c) </span>Discriminative <span id="A2.T8.st3.3.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span>.</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 8: </span>Spearman’s rank correlation between drops in test accuracy (from IID to OOD) and the differences in proportion of answer classes between IID and OOD fine-tune sets. Unless otherwise specified with a <sup id="A2.T8.8.1" class="ltx_sup">∧</sup> character, <math id="A2.T8.5.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A2.T8.5.m2.1b"><mi id="A2.T8.5.m2.1.1" xref="A2.T8.5.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A2.T8.5.m2.1c"><ci id="A2.T8.5.m2.1.1.cmml" xref="A2.T8.5.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.5.m2.1d">\rho</annotation></semantics></math> values are significant with <math id="A2.T8.6.m3.1" class="ltx_Math" alttext="p&lt;.05" display="inline"><semantics id="A2.T8.6.m3.1b"><mrow id="A2.T8.6.m3.1.1" xref="A2.T8.6.m3.1.1.cmml"><mi id="A2.T8.6.m3.1.1.2" xref="A2.T8.6.m3.1.1.2.cmml">p</mi><mo id="A2.T8.6.m3.1.1.1" xref="A2.T8.6.m3.1.1.1.cmml">&lt;</mo><mn id="A2.T8.6.m3.1.1.3" xref="A2.T8.6.m3.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T8.6.m3.1c"><apply id="A2.T8.6.m3.1.1.cmml" xref="A2.T8.6.m3.1.1"><lt id="A2.T8.6.m3.1.1.1.cmml" xref="A2.T8.6.m3.1.1.1"></lt><ci id="A2.T8.6.m3.1.1.2.cmml" xref="A2.T8.6.m3.1.1.2">𝑝</ci><cn type="float" id="A2.T8.6.m3.1.1.3.cmml" xref="A2.T8.6.m3.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.6.m3.1d">p&lt;.05</annotation></semantics></math>. Rows correspond to the fine-tuning datasets, columns correspond to the test benchmarks.</figcaption>
</figure>
</section>
<section id="A2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Answer Frequency Correlation</h4>

<figure id="A2.T9" class="ltx_table">
<table id="A2.T9.7" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T9.7.1.1" class="ltx_tr">
<th id="A2.T9.7.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"></th>
<th id="A2.T9.7.1.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T9.7.1.1.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<th id="A2.T9.7.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T9.7.1.1.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<th id="A2.T9.7.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T9.7.1.1.4.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<th id="A2.T9.7.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:4.5pt;padding-right:4.5pt;"><span id="A2.T9.7.1.1.5.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T9.7.2.1" class="ltx_tr">
<th id="A2.T9.7.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T9.7.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></th>
<td id="A2.T9.7.2.1.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td id="A2.T9.7.2.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0.43</td>
<td id="A2.T9.7.2.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0.51</td>
<td id="A2.T9.7.2.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.5pt;padding-right:4.5pt;">0.25</td>
</tr>
<tr id="A2.T9.7.3.2" class="ltx_tr">
<th id="A2.T9.7.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T9.7.3.2.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></th>
<td id="A2.T9.7.3.2.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.27</td>
<td id="A2.T9.7.3.2.3" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td id="A2.T9.7.3.2.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.43</td>
<td id="A2.T9.7.3.2.5" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.19</td>
</tr>
<tr id="A2.T9.7.4.3" class="ltx_tr">
<th id="A2.T9.7.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T9.7.4.3.1.1" class="ltx_text ltx_font_smallcaps">VG</span></th>
<td id="A2.T9.7.4.3.2" class="ltx_td ltx_nopad_l ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.26</td>
<td id="A2.T9.7.4.3.3" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.36</td>
<td id="A2.T9.7.4.3.4" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
<td id="A2.T9.7.4.3.5" class="ltx_td ltx_align_center" style="padding-left:4.5pt;padding-right:4.5pt;">0.13</td>
</tr>
<tr id="A2.T9.7.5.4" class="ltx_tr">
<th id="A2.T9.7.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">
<span id="A2.T9.7.5.4.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></th>
<td id="A2.T9.7.5.4.2" class="ltx_td ltx_nopad_l ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">0.47</td>
<td id="A2.T9.7.5.4.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">0.55</td>
<td id="A2.T9.7.5.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">0.48</td>
<td id="A2.T9.7.5.4.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:4.5pt;padding-right:4.5pt;">–</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 9: </span>Spearman’s rank correlation between drops in test accuracy (from IID to OOD) and the differences in proportion of answer classes between IID and OOD fine-tuning sets for <span id="A2.T9.4.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T9.4.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T9.4.1.m1.1b"><msub id="A2.T9.4.1.m1.1.1" xref="A2.T9.4.1.m1.1.1.cmml"><mi id="A2.T9.4.1.m1.1.1b" xref="A2.T9.4.1.m1.1.1.cmml"></mi><mtext id="A2.T9.4.1.m1.1.1.1" xref="A2.T9.4.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T9.4.1.m1.1c"><apply id="A2.T9.4.1.m1.1.1.cmml" xref="A2.T9.4.1.m1.1.1"><ci id="A2.T9.4.1.m1.1.1.1a.cmml" xref="A2.T9.4.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T9.4.1.m1.1.1.1.cmml" xref="A2.T9.4.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.4.1.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span>. <math id="A2.T9.5.m1.1" class="ltx_Math" alttext="p&lt;.05" display="inline"><semantics id="A2.T9.5.m1.1b"><mrow id="A2.T9.5.m1.1.1" xref="A2.T9.5.m1.1.1.cmml"><mi id="A2.T9.5.m1.1.1.2" xref="A2.T9.5.m1.1.1.2.cmml">p</mi><mo id="A2.T9.5.m1.1.1.1" xref="A2.T9.5.m1.1.1.1.cmml">&lt;</mo><mn id="A2.T9.5.m1.1.1.3" xref="A2.T9.5.m1.1.1.3.cmml">.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T9.5.m1.1c"><apply id="A2.T9.5.m1.1.1.cmml" xref="A2.T9.5.m1.1.1"><lt id="A2.T9.5.m1.1.1.1.cmml" xref="A2.T9.5.m1.1.1.1"></lt><ci id="A2.T9.5.m1.1.1.2.cmml" xref="A2.T9.5.m1.1.1.2">𝑝</ci><cn type="float" id="A2.T9.5.m1.1.1.3.cmml" xref="A2.T9.5.m1.1.1.3">.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.5.m1.1d">p&lt;.05</annotation></semantics></math> for all <math id="A2.T9.6.m2.1" class="ltx_Math" alttext="\rho" display="inline"><semantics id="A2.T9.6.m2.1b"><mi id="A2.T9.6.m2.1.1" xref="A2.T9.6.m2.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="A2.T9.6.m2.1c"><ci id="A2.T9.6.m2.1.1.cmml" xref="A2.T9.6.m2.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T9.6.m2.1d">\rho</annotation></semantics></math>. Rows correspond to the fine-tuning datasets, columns correspond to the test benchmarks.</figcaption>
</figure>
<div id="A2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px2.p1.1" class="ltx_p">In order to examine the relationship between accuracy drop for less frequent classes, we first compute per answer-class accuracy (average accuracy of all test questions belonging to the same answer class) for answers in shared answer set.
We then sort the shared answer classes based on their weighted drop in per-class accuracy from IID to OOD (IID accuracy - OOD accuracy), <em id="A2.SS0.SSS0.Px2.p1.1.1" class="ltx_emph ltx_font_italic">i.e.</em> absolute drop in per-class accuracy weighted by number of data points belonging to that class in the test set.
We then compute the Spearman’s rank correlation of these weighted drop in per-class accuracies with difference in percentage frequencies of the answer classes between IID and OOD fine-tuning sets (percentage frequency of an answer class in IID minus its percentage frequency in OOD).</p>
</div>
<div id="A2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="A2.SS0.SSS0.Px2.p2.3" class="ltx_p"><a href="#A2.T8" title="In Evaluation with Shared Answer Sets ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">8</span></a> list Spearman’s rank correlations of IID-to-OOD drops in test accuracy vs. proportion of answer classes in respective (IID and OOD) fine-tuning sets for <span id="A2.SS0.SSS0.Px2.p2.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.SS0.SSS0.Px2.p2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.SS0.SSS0.Px2.p2.1.1.m1.1a"><msub id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1a" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p2.1.1.m1.1b"><apply id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1"><ci id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>, <span id="A2.SS0.SSS0.Px2.p2.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.SS0.SSS0.Px2.p2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.SS0.SSS0.Px2.p2.2.2.m1.1a"><msub id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1a" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p2.2.2.m1.1b"><apply id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1"><ci id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p2.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> and <span id="A2.SS0.SSS0.Px2.p2.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.SS0.SSS0.Px2.p2.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.SS0.SSS0.Px2.p2.3.3.m1.1a"><msub id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.cmml"><mi id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1a" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.cmml"></mi><mtext id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.SS0.SSS0.Px2.p2.3.3.m1.1b"><apply id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1"><ci id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1a.cmml" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1"><mtext mathsize="70%" id="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1.cmml" xref="A2.SS0.SSS0.Px2.p2.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS0.SSS0.Px2.p2.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> (see <a href="#S4.SS1" title="4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> for more details).
As a simple baseline test, we also compute correlations and p-values for a permuted dataset to confirm their lack of significance, or correlation values close to zero.</p>
</div>
</section>
<section id="A2.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Maximum Achievable Scores</h4>

<div id="A2.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px3.p1.1" class="ltx_p">Table <a href="#S4.T2" title="Tab. 2 ‣ 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> lists maximum achievable accuracies, and Figure <a href="#A2.F7" title="Fig. 7 ‣ Maximum Achievable Scores ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the difference between those scores and bar values shown in Figure <a href="#S3.F1" title="Fig. 1 ‣ 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In our analyses, we also noted that differences in answer pre-processing strategies can result in slightly different numbers than those reported in <a href="#S4.T2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">2</span></a>. However, those differences did not change the conclusion of our findings.</p>
</div>
<figure id="A2.F7" class="ltx_figure">
<div id="A2.F7.1" class="ltx_block">
<img src="/html/2205.12191/assets/figs/%22diff_iid_vs_ood_vilbert_BERT_+_3M.pdf%22" id="A2.F7.g1" class="ltx_graphics ltx_centering" alt="Refer to caption"><img src="/html/2205.12191/assets/figs/%22diff_iid_vs_ood_albef_big_BERT_+_14M.pdf%22" id="A2.F7.g2" class="ltx_graphics ltx_centering" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
Percentage point difference between maximum achievable accuracies in Table <a href="#S4.T2" title="Tab. 2 ‣ 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and accuracies in Figure <a href="#S3.F1" title="Fig. 1 ‣ 3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Results for <span id="A2.F7.4.1" class="ltx_text ltx_font_smallcaps">ViLBERT</span> pretrained on the same data as <span id="A2.F7.5.2" class="ltx_text ltx_font_smallcaps">ALBEF</span> 4M are also shown.</figcaption>
</figure>
</section>
<section id="A2.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Effect of pretraining data size on <span id="A2.SS0.SSS0.Px4.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> </h4>

<div id="A2.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px4.p1.1" class="ltx_p">For the <span id="A2.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> model, while we often observe improvements by increasing the size of the multimodal pretraining dataset (4M vs. 14M), the improvements are small.
When pretraining on the smaller dataset (4M, see <a href="#A2.F8" title="In Effect of pretraining data size on ALBEF ‣ Appendix B Additional Results ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">8</span></a>), we observe a median improvement (over no pretraining) of  1.9% for the discriminative and  4.9% for the generative <span id="A2.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_smallcaps">ALBEF</span>, while the median additional improvements due to larger pretraining dataset (14M) are  0.1% and  0.6% respectively (refer to <a href="#S4.F3" title="In 4.2 The Case for the Generative Evaluation ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">3</span></a>).
Surprisingly, there are also dataset pairs for which larger pretraining has a negative effect when compared to the performance with a smaller pretraining set (<em id="A2.SS0.SSS0.Px4.p1.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, <span id="A2.SS0.SSS0.Px4.p1.1.4" class="ltx_text ltx_font_smallcaps">ALBEF</span> model fine-tuned on <span id="A2.SS0.SSS0.Px4.p1.1.5" class="ltx_text ltx_font_smallcaps">VizWiz</span> and tested on <span id="A2.SS0.SSS0.Px4.p1.1.6" class="ltx_text ltx_font_smallcaps">VQAv2</span>).</p>
</div>
<figure id="A2.F8" class="ltx_figure"><img src="/html/2205.12191/assets/figs/%22pretraining_score_drop_albef_BERT_+_4M.pdf%22" id="A2.F8.g1" class="ltx_graphics ltx_centering" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
Difference in VQA accuracy (p.p.) for <span id="A2.F8.2.1" class="ltx_text ltx_font_smallcaps">ALBEF</span> that has and has not been pretrained on the 4M dataset.</figcaption>
</figure>
<figure id="A2.T10" class="ltx_table">
<table id="A2.T10.96" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T10.96.97.1" class="ltx_tr">
<th id="A2.T10.96.97.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">Model</th>
<th id="A2.T10.96.97.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">Test</th>
<th id="A2.T10.96.97.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">Fine-tune</th>
<th id="A2.T10.96.97.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">Answer Set</th>
<th id="A2.T10.96.97.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">VQA Acc. (IID)</th>
<th id="A2.T10.96.97.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">VQA Acc. (OOD)</th>
<th id="A2.T10.96.97.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-left:2.5pt;padding-right:2.5pt;">Difference</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T10.2.2" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.1.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.1.1.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.1.1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.1.1.1.1.m1.1a"><msub id="A2.T10.1.1.1.1.m1.1.1" xref="A2.T10.1.1.1.1.m1.1.1.cmml"><mi id="A2.T10.1.1.1.1.m1.1.1a" xref="A2.T10.1.1.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.1.1.1.1.m1.1.1.1" xref="A2.T10.1.1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.1.1.1.1.m1.1b"><apply id="A2.T10.1.1.1.1.m1.1.1.cmml" xref="A2.T10.1.1.1.1.m1.1.1"><ci id="A2.T10.1.1.1.1.m1.1.1.1a.cmml" xref="A2.T10.1.1.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.1.1.1.1.m1.1.1.1.cmml" xref="A2.T10.1.1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.1.1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.2.2.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.2.2.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.2.2.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA<span id="A2.T10.2.2.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.2.2.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.2.2.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.2.2.2.1.1.m1.1.1" xref="A2.T10.2.2.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.2.2.2.1.1.m1.1b"><intersect id="A2.T10.2.2.2.1.1.m1.1.1.cmml" xref="A2.T10.2.2.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.2.2.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.5.1" class="ltx_text" style="background-color:#F2F2F2;">63.05</span></td>
<td id="A2.T10.2.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.6.1" class="ltx_text" style="background-color:#F2F2F2;">48.53</span></td>
<td id="A2.T10.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.2.2.7.1" class="ltx_text" style="background-color:#F2F2F2;">14.52</span></td>
</tr>
<tr id="A2.T10.4.4" class="ltx_tr">
<td id="A2.T10.3.3.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.3.3.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.3.3.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.3.3.1.1.m1.1a"><msub id="A2.T10.3.3.1.1.m1.1.1" xref="A2.T10.3.3.1.1.m1.1.1.cmml"><mi id="A2.T10.3.3.1.1.m1.1.1a" xref="A2.T10.3.3.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.3.3.1.1.m1.1.1.1" xref="A2.T10.3.3.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.3.3.1.1.m1.1b"><apply id="A2.T10.3.3.1.1.m1.1.1.cmml" xref="A2.T10.3.3.1.1.m1.1.1"><ci id="A2.T10.3.3.1.1.m1.1.1.1a.cmml" xref="A2.T10.3.3.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.3.3.1.1.m1.1.1.1.cmml" xref="A2.T10.3.3.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.3.3.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.4.4.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.4.4.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.4.4.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.4.4.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.4.4.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.4.4.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.4.4.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.4.4.2.m1.1a"><mo id="A2.T10.4.4.2.m1.1.1" xref="A2.T10.4.4.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.4.4.2.m1.1b"><intersect id="A2.T10.4.4.2.m1.1.1.cmml" xref="A2.T10.4.4.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.4.4.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.4.4.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.4.4.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">52.32</td>
<td id="A2.T10.4.4.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">35.08</td>
<td id="A2.T10.4.4.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">17.24</td>
</tr>
<tr id="A2.T10.6.6" class="ltx_tr">
<td id="A2.T10.5.5.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.5.5.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.5.5.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.5.5.1.1.m1.1a"><msub id="A2.T10.5.5.1.1.m1.1.1" xref="A2.T10.5.5.1.1.m1.1.1.cmml"><mi id="A2.T10.5.5.1.1.m1.1.1a" xref="A2.T10.5.5.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.5.5.1.1.m1.1.1.1" xref="A2.T10.5.5.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.5.5.1.1.m1.1b"><apply id="A2.T10.5.5.1.1.m1.1.1.cmml" xref="A2.T10.5.5.1.1.m1.1.1"><ci id="A2.T10.5.5.1.1.m1.1.1.1a.cmml" xref="A2.T10.5.5.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.5.5.1.1.m1.1.1.1.cmml" xref="A2.T10.5.5.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.5.5.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.6.6.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.6.6.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.6.6.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.6.6.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.6.6.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.6.6.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.6.6.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.6.6.2.m1.1a"><mo id="A2.T10.6.6.2.m1.1.1" xref="A2.T10.6.6.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.6.6.2.m1.1b"><intersect id="A2.T10.6.6.2.m1.1.1.cmml" xref="A2.T10.6.6.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.6.6.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.6.6.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.6.6.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.91</td>
<td id="A2.T10.6.6.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">28.39</td>
<td id="A2.T10.6.6.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">36.52</td>
</tr>
<tr id="A2.T10.8.8" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.7.7.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.7.7.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.7.7.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.7.7.1.1.m1.1a"><msub id="A2.T10.7.7.1.1.m1.1.1" xref="A2.T10.7.7.1.1.m1.1.1.cmml"><mi id="A2.T10.7.7.1.1.m1.1.1a" xref="A2.T10.7.7.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.7.7.1.1.m1.1.1.1" xref="A2.T10.7.7.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.7.7.1.1.m1.1b"><apply id="A2.T10.7.7.1.1.m1.1.1.cmml" xref="A2.T10.7.7.1.1.m1.1.1"><ci id="A2.T10.7.7.1.1.m1.1.1.1a.cmml" xref="A2.T10.7.7.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.7.7.1.1.m1.1.1.1.cmml" xref="A2.T10.7.7.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.7.7.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.8.8.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.8.8.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.8.8.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG<span id="A2.T10.8.8.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.8.8.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.8.8.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.8.8.2.1.1.m1.1.1" xref="A2.T10.8.8.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.8.8.2.1.1.m1.1b"><intersect id="A2.T10.8.8.2.1.1.m1.1.1.cmml" xref="A2.T10.8.8.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.8.8.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.8.8.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.5.1" class="ltx_text" style="background-color:#F2F2F2;">58.18</span></td>
<td id="A2.T10.8.8.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.6.1" class="ltx_text" style="background-color:#F2F2F2;">51.97</span></td>
<td id="A2.T10.8.8.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.8.8.7.1" class="ltx_text" style="background-color:#F2F2F2;">6.21</span></td>
</tr>
<tr id="A2.T10.10.10" class="ltx_tr">
<td id="A2.T10.9.9.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.9.9.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.9.9.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.9.9.1.1.m1.1a"><msub id="A2.T10.9.9.1.1.m1.1.1" xref="A2.T10.9.9.1.1.m1.1.1.cmml"><mi id="A2.T10.9.9.1.1.m1.1.1a" xref="A2.T10.9.9.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.9.9.1.1.m1.1.1.1" xref="A2.T10.9.9.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.9.9.1.1.m1.1b"><apply id="A2.T10.9.9.1.1.m1.1.1.cmml" xref="A2.T10.9.9.1.1.m1.1.1"><ci id="A2.T10.9.9.1.1.m1.1.1.1a.cmml" xref="A2.T10.9.9.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.9.9.1.1.m1.1.1.1.cmml" xref="A2.T10.9.9.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.9.9.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.10.10.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.10.10.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.10.10.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.10.10.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.10.10.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.10.10.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.10.10.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.10.10.2.m1.1a"><mo id="A2.T10.10.10.2.m1.1.1" xref="A2.T10.10.10.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.10.10.2.m1.1b"><intersect id="A2.T10.10.10.2.m1.1.1.cmml" xref="A2.T10.10.10.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.10.10.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.10.10.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.10.10.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">59.57</td>
<td id="A2.T10.10.10.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">39.15</td>
<td id="A2.T10.10.10.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">20.42</td>
</tr>
<tr id="A2.T10.12.12" class="ltx_tr">
<td id="A2.T10.11.11.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.11.11.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.11.11.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.11.11.1.1.m1.1a"><msub id="A2.T10.11.11.1.1.m1.1.1" xref="A2.T10.11.11.1.1.m1.1.1.cmml"><mi id="A2.T10.11.11.1.1.m1.1.1a" xref="A2.T10.11.11.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.11.11.1.1.m1.1.1.1" xref="A2.T10.11.11.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.11.11.1.1.m1.1b"><apply id="A2.T10.11.11.1.1.m1.1.1.cmml" xref="A2.T10.11.11.1.1.m1.1.1"><ci id="A2.T10.11.11.1.1.m1.1.1.1a.cmml" xref="A2.T10.11.11.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.11.11.1.1.m1.1.1.1.cmml" xref="A2.T10.11.11.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.11.11.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.12.12.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.12.12.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.12.12.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.12.12.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.12.12.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.12.12.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.12.12.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.12.12.2.m1.1a"><mo id="A2.T10.12.12.2.m1.1.1" xref="A2.T10.12.12.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.12.12.2.m1.1b"><intersect id="A2.T10.12.12.2.m1.1.1.cmml" xref="A2.T10.12.12.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.12.12.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.12.12.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.12.12.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">60.65</td>
<td id="A2.T10.12.12.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">13.23</td>
<td id="A2.T10.12.12.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">47.42</td>
</tr>
<tr id="A2.T10.14.14" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.13.13.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.13.13.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.13.13.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.13.13.1.1.m1.1a"><msub id="A2.T10.13.13.1.1.m1.1.1" xref="A2.T10.13.13.1.1.m1.1.1.cmml"><mi id="A2.T10.13.13.1.1.m1.1.1a" xref="A2.T10.13.13.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.13.13.1.1.m1.1.1.1" xref="A2.T10.13.13.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.13.13.1.1.m1.1b"><apply id="A2.T10.13.13.1.1.m1.1.1.cmml" xref="A2.T10.13.13.1.1.m1.1.1"><ci id="A2.T10.13.13.1.1.m1.1.1.1a.cmml" xref="A2.T10.13.13.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.13.13.1.1.m1.1.1.1.cmml" xref="A2.T10.13.13.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.13.13.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.14.14.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.14.14.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.14.14.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz<span id="A2.T10.14.14.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.14.14.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.14.14.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.14.14.2.1.1.m1.1.1" xref="A2.T10.14.14.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.14.14.2.1.1.m1.1b"><intersect id="A2.T10.14.14.2.1.1.m1.1.1.cmml" xref="A2.T10.14.14.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.14.14.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.14.14.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.5.1" class="ltx_text" style="background-color:#F2F2F2;">51.78</span></td>
<td id="A2.T10.14.14.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.6.1" class="ltx_text" style="background-color:#F2F2F2;">22.37</span></td>
<td id="A2.T10.14.14.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.14.14.7.1" class="ltx_text" style="background-color:#F2F2F2;">29.41</span></td>
</tr>
<tr id="A2.T10.16.16" class="ltx_tr">
<td id="A2.T10.15.15.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.15.15.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.15.15.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.15.15.1.1.m1.1a"><msub id="A2.T10.15.15.1.1.m1.1.1" xref="A2.T10.15.15.1.1.m1.1.1.cmml"><mi id="A2.T10.15.15.1.1.m1.1.1a" xref="A2.T10.15.15.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.15.15.1.1.m1.1.1.1" xref="A2.T10.15.15.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.15.15.1.1.m1.1b"><apply id="A2.T10.15.15.1.1.m1.1.1.cmml" xref="A2.T10.15.15.1.1.m1.1.1"><ci id="A2.T10.15.15.1.1.m1.1.1.1a.cmml" xref="A2.T10.15.15.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.15.15.1.1.m1.1.1.1.cmml" xref="A2.T10.15.15.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.15.15.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.16.16.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.16.16.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.16.16.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.16.16.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.16.16.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.16.16.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.16.16.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.16.16.2.m1.1a"><mo id="A2.T10.16.16.2.m1.1.1" xref="A2.T10.16.16.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.16.16.2.m1.1b"><intersect id="A2.T10.16.16.2.m1.1.1.cmml" xref="A2.T10.16.16.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.16.16.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.16.16.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.16.16.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">51.05</td>
<td id="A2.T10.16.16.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">15.40</td>
<td id="A2.T10.16.16.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">35.65</td>
</tr>
<tr id="A2.T10.18.18" class="ltx_tr">
<td id="A2.T10.17.17.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.17.17.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.17.17.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.17.17.1.1.m1.1a"><msub id="A2.T10.17.17.1.1.m1.1.1" xref="A2.T10.17.17.1.1.m1.1.1.cmml"><mi id="A2.T10.17.17.1.1.m1.1.1a" xref="A2.T10.17.17.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.17.17.1.1.m1.1.1.1" xref="A2.T10.17.17.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.17.17.1.1.m1.1b"><apply id="A2.T10.17.17.1.1.m1.1.1.cmml" xref="A2.T10.17.17.1.1.m1.1.1"><ci id="A2.T10.17.17.1.1.m1.1.1.1a.cmml" xref="A2.T10.17.17.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.17.17.1.1.m1.1.1.1.cmml" xref="A2.T10.17.17.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.17.17.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.18.18.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.18.18.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.18.18.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.18.18.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.18.18.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.18.18.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.18.18.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.18.18.2.m1.1a"><mo id="A2.T10.18.18.2.m1.1.1" xref="A2.T10.18.18.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.18.18.2.m1.1b"><intersect id="A2.T10.18.18.2.m1.1.1.cmml" xref="A2.T10.18.18.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.18.18.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.18.18.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.18.18.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">50.07</td>
<td id="A2.T10.18.18.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">13.59</td>
<td id="A2.T10.18.18.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">36.48</td>
</tr>
<tr id="A2.T10.20.20" class="ltx_tr">
<td id="A2.T10.19.19.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.19.19.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.19.19.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.19.19.1.1.m1.1a"><msub id="A2.T10.19.19.1.1.m1.1.1" xref="A2.T10.19.19.1.1.m1.1.1.cmml"><mi id="A2.T10.19.19.1.1.m1.1.1a" xref="A2.T10.19.19.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.19.19.1.1.m1.1.1.1" xref="A2.T10.19.19.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.19.19.1.1.m1.1b"><apply id="A2.T10.19.19.1.1.m1.1.1.cmml" xref="A2.T10.19.19.1.1.m1.1.1"><ci id="A2.T10.19.19.1.1.m1.1.1.1a.cmml" xref="A2.T10.19.19.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.19.19.1.1.m1.1.1.1.cmml" xref="A2.T10.19.19.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.19.19.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.20.20.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.20.20.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.20.20.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.20.20.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.20.20.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.20.20.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.20.20.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.20.20.2.m1.1a"><mo id="A2.T10.20.20.2.m1.1.1" xref="A2.T10.20.20.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.20.20.2.m1.1b"><intersect id="A2.T10.20.20.2.m1.1.1.cmml" xref="A2.T10.20.20.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.20.20.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.20.20.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.20.20.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">71.74</td>
<td id="A2.T10.20.20.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">52.42</td>
<td id="A2.T10.20.20.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">19.32</td>
</tr>
<tr id="A2.T10.22.22" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.21.21.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.21.21.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.21.21.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.21.21.1.1.m1.1a"><msub id="A2.T10.21.21.1.1.m1.1.1" xref="A2.T10.21.21.1.1.m1.1.1.cmml"><mi id="A2.T10.21.21.1.1.m1.1.1a" xref="A2.T10.21.21.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.21.21.1.1.m1.1.1.1" xref="A2.T10.21.21.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.21.21.1.1.m1.1b"><apply id="A2.T10.21.21.1.1.m1.1.1.cmml" xref="A2.T10.21.21.1.1.m1.1.1"><ci id="A2.T10.21.21.1.1.m1.1.1.1a.cmml" xref="A2.T10.21.21.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.21.21.1.1.m1.1.1.1.cmml" xref="A2.T10.21.21.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.21.21.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.22.22.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.22.22.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.22.22.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2<span id="A2.T10.22.22.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.22.22.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.22.22.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.22.22.2.1.1.m1.1.1" xref="A2.T10.22.22.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.22.22.2.1.1.m1.1b"><intersect id="A2.T10.22.22.2.1.1.m1.1.1.cmml" xref="A2.T10.22.22.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.22.22.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VG</span></td>
<td id="A2.T10.22.22.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.5.1" class="ltx_text" style="background-color:#F2F2F2;">58.10</span></td>
<td id="A2.T10.22.22.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.6.1" class="ltx_text" style="background-color:#F2F2F2;">48.84</span></td>
<td id="A2.T10.22.22.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.22.22.7.1" class="ltx_text" style="background-color:#F2F2F2;">9.26</span></td>
</tr>
<tr id="A2.T10.24.24" class="ltx_tr">
<td id="A2.T10.23.23.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.23.23.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.23.23.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.23.23.1.1.m1.1a"><msub id="A2.T10.23.23.1.1.m1.1.1" xref="A2.T10.23.23.1.1.m1.1.1.cmml"><mi id="A2.T10.23.23.1.1.m1.1.1a" xref="A2.T10.23.23.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.23.23.1.1.m1.1.1.1" xref="A2.T10.23.23.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.23.23.1.1.m1.1b"><apply id="A2.T10.23.23.1.1.m1.1.1.cmml" xref="A2.T10.23.23.1.1.m1.1.1"><ci id="A2.T10.23.23.1.1.m1.1.1.1a.cmml" xref="A2.T10.23.23.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.23.23.1.1.m1.1.1.1.cmml" xref="A2.T10.23.23.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.23.23.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.24.24.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.24.24.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.24.24.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.24.24.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.24.24.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.24.24.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.24.24.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.24.24.2.m1.1a"><mo id="A2.T10.24.24.2.m1.1.1" xref="A2.T10.24.24.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.24.24.2.m1.1b"><intersect id="A2.T10.24.24.2.m1.1.1.cmml" xref="A2.T10.24.24.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.24.24.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.24.24.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.24.24.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">68.20</td>
<td id="A2.T10.24.24.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">33.87</td>
<td id="A2.T10.24.24.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">34.33</td>
</tr>
<tr id="A2.T10.26.26" class="ltx_tr">
<td id="A2.T10.25.25.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.25.25.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.25.25.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.25.25.1.1.m1.1a"><msub id="A2.T10.25.25.1.1.m1.1.1" xref="A2.T10.25.25.1.1.m1.1.1.cmml"><mi id="A2.T10.25.25.1.1.m1.1.1a" xref="A2.T10.25.25.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.25.25.1.1.m1.1.1.1" xref="A2.T10.25.25.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.25.25.1.1.m1.1b"><apply id="A2.T10.25.25.1.1.m1.1.1.cmml" xref="A2.T10.25.25.1.1.m1.1.1"><ci id="A2.T10.25.25.1.1.m1.1.1.1a.cmml" xref="A2.T10.25.25.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.25.25.1.1.m1.1.1.1.cmml" xref="A2.T10.25.25.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.25.25.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.26.26.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.26.26.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.26.26.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.26.26.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.26.26.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.26.26.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.26.26.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.26.26.2.m1.1a"><mo id="A2.T10.26.26.2.m1.1.1" xref="A2.T10.26.26.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.26.26.2.m1.1b"><intersect id="A2.T10.26.26.2.m1.1.1.cmml" xref="A2.T10.26.26.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.26.26.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.26.26.2.2" class="ltx_text ltx_font_smallcaps">VQAv2</span>
</td>
<td id="A2.T10.26.26.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">67.01</td>
<td id="A2.T10.26.26.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">44.03</td>
<td id="A2.T10.26.26.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;">22.98</td>
</tr>
<tr id="A2.T10.28.28" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.27.27.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.27.27.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.27.27.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.27.27.1.1.m1.1a"><msub id="A2.T10.27.27.1.1.m1.1.1" xref="A2.T10.27.27.1.1.m1.1.1.cmml"><mi id="A2.T10.27.27.1.1.m1.1.1a" xref="A2.T10.27.27.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.27.27.1.1.m1.1.1.1" xref="A2.T10.27.27.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.27.27.1.1.m1.1b"><apply id="A2.T10.27.27.1.1.m1.1.1.cmml" xref="A2.T10.27.27.1.1.m1.1.1"><ci id="A2.T10.27.27.1.1.m1.1.1.1a.cmml" xref="A2.T10.27.27.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.27.27.1.1.m1.1.1.1.cmml" xref="A2.T10.27.27.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.27.27.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.28.28.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.28.28.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.28.28.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA<span id="A2.T10.28.28.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.28.28.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.28.28.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.28.28.2.1.1.m1.1.1" xref="A2.T10.28.28.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.28.28.2.1.1.m1.1b"><intersect id="A2.T10.28.28.2.1.1.m1.1.1.cmml" xref="A2.T10.28.28.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.28.28.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VG</span></td>
<td id="A2.T10.28.28.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.5.1" class="ltx_text" style="background-color:#F2F2F2;">57.35</span></td>
<td id="A2.T10.28.28.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.6.1" class="ltx_text" style="background-color:#F2F2F2;">34.40</span></td>
<td id="A2.T10.28.28.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.28.28.7.1" class="ltx_text" style="background-color:#F2F2F2;">22.95</span></td>
</tr>
<tr id="A2.T10.30.30" class="ltx_tr">
<td id="A2.T10.29.29.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.29.29.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.29.29.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.29.29.1.1.m1.1a"><msub id="A2.T10.29.29.1.1.m1.1.1" xref="A2.T10.29.29.1.1.m1.1.1.cmml"><mi id="A2.T10.29.29.1.1.m1.1.1a" xref="A2.T10.29.29.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.29.29.1.1.m1.1.1.1" xref="A2.T10.29.29.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.29.29.1.1.m1.1b"><apply id="A2.T10.29.29.1.1.m1.1.1.cmml" xref="A2.T10.29.29.1.1.m1.1.1"><ci id="A2.T10.29.29.1.1.m1.1.1.1a.cmml" xref="A2.T10.29.29.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.29.29.1.1.m1.1.1.1.cmml" xref="A2.T10.29.29.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.29.29.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.30.30.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.30.30.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.30.30.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.30.30.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.30.30.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.30.30.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.30.30.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.30.30.2.m1.1a"><mo id="A2.T10.30.30.2.m1.1.1" xref="A2.T10.30.30.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.30.30.2.m1.1b"><intersect id="A2.T10.30.30.2.m1.1.1.cmml" xref="A2.T10.30.30.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.30.30.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.30.30.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.30.30.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">69.63</td>
<td id="A2.T10.30.30.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">20.73</td>
<td id="A2.T10.30.30.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">48.90</td>
</tr>
<tr id="A2.T10.32.32" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.31.31.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.31.31.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.31.31.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.31.31.1.1.m1.1a"><msub id="A2.T10.31.31.1.1.m1.1.1" xref="A2.T10.31.31.1.1.m1.1.1.cmml"><mi id="A2.T10.31.31.1.1.m1.1.1a" xref="A2.T10.31.31.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.31.31.1.1.m1.1.1.1" xref="A2.T10.31.31.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.31.31.1.1.m1.1b"><apply id="A2.T10.31.31.1.1.m1.1.1.cmml" xref="A2.T10.31.31.1.1.m1.1.1"><ci id="A2.T10.31.31.1.1.m1.1.1.1a.cmml" xref="A2.T10.31.31.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.31.31.1.1.m1.1.1.1.cmml" xref="A2.T10.31.31.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.31.31.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.32.32.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.32.32.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.32.32.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG<span id="A2.T10.32.32.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.32.32.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.32.32.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.32.32.2.1.1.m1.1.1" xref="A2.T10.32.32.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.32.32.2.1.1.m1.1b"><intersect id="A2.T10.32.32.2.1.1.m1.1.1.cmml" xref="A2.T10.32.32.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.32.32.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.32.32.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.5.1" class="ltx_text" style="background-color:#F2F2F2;">55.52</span></td>
<td id="A2.T10.32.32.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.6.1" class="ltx_text" style="background-color:#F2F2F2;">47.93</span></td>
<td id="A2.T10.32.32.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.32.32.7.1" class="ltx_text" style="background-color:#F2F2F2;">7.59</span></td>
</tr>
<tr id="A2.T10.34.34" class="ltx_tr">
<td id="A2.T10.33.33.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.33.33.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.33.33.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.33.33.1.1.m1.1a"><msub id="A2.T10.33.33.1.1.m1.1.1" xref="A2.T10.33.33.1.1.m1.1.1.cmml"><mi id="A2.T10.33.33.1.1.m1.1.1a" xref="A2.T10.33.33.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.33.33.1.1.m1.1.1.1" xref="A2.T10.33.33.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.33.33.1.1.m1.1b"><apply id="A2.T10.33.33.1.1.m1.1.1.cmml" xref="A2.T10.33.33.1.1.m1.1.1"><ci id="A2.T10.33.33.1.1.m1.1.1.1a.cmml" xref="A2.T10.33.33.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.33.33.1.1.m1.1.1.1.cmml" xref="A2.T10.33.33.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.33.33.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.34.34.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.34.34.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.34.34.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.34.34.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.34.34.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.34.34.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.34.34.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.34.34.2.m1.1a"><mo id="A2.T10.34.34.2.m1.1.1" xref="A2.T10.34.34.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.34.34.2.m1.1b"><intersect id="A2.T10.34.34.2.m1.1.1.cmml" xref="A2.T10.34.34.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.34.34.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.34.34.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.34.34.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">57.65</td>
<td id="A2.T10.34.34.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">39.26</td>
<td id="A2.T10.34.34.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">18.39</td>
</tr>
<tr id="A2.T10.36.36" class="ltx_tr">
<td id="A2.T10.35.35.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.35.35.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.35.35.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.35.35.1.1.m1.1a"><msub id="A2.T10.35.35.1.1.m1.1.1" xref="A2.T10.35.35.1.1.m1.1.1.cmml"><mi id="A2.T10.35.35.1.1.m1.1.1a" xref="A2.T10.35.35.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.35.35.1.1.m1.1.1.1" xref="A2.T10.35.35.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.35.35.1.1.m1.1b"><apply id="A2.T10.35.35.1.1.m1.1.1.cmml" xref="A2.T10.35.35.1.1.m1.1.1"><ci id="A2.T10.35.35.1.1.m1.1.1.1a.cmml" xref="A2.T10.35.35.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.35.35.1.1.m1.1.1.1.cmml" xref="A2.T10.35.35.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.35.35.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.36.36.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.36.36.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.36.36.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.36.36.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.36.36.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.36.36.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.36.36.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.36.36.2.m1.1a"><mo id="A2.T10.36.36.2.m1.1.1" xref="A2.T10.36.36.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.36.36.2.m1.1b"><intersect id="A2.T10.36.36.2.m1.1.1.cmml" xref="A2.T10.36.36.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.36.36.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.36.36.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.36.36.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">58.84</td>
<td id="A2.T10.36.36.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">7.67</td>
<td id="A2.T10.36.36.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">51.17</td>
</tr>
<tr id="A2.T10.38.38" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.37.37.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.37.37.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.37.37.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.37.37.1.1.m1.1a"><msub id="A2.T10.37.37.1.1.m1.1.1" xref="A2.T10.37.37.1.1.m1.1.1.cmml"><mi id="A2.T10.37.37.1.1.m1.1.1a" xref="A2.T10.37.37.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.37.37.1.1.m1.1.1.1" xref="A2.T10.37.37.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.37.37.1.1.m1.1b"><apply id="A2.T10.37.37.1.1.m1.1.1.cmml" xref="A2.T10.37.37.1.1.m1.1.1"><ci id="A2.T10.37.37.1.1.m1.1.1.1a.cmml" xref="A2.T10.37.37.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.37.37.1.1.m1.1.1.1.cmml" xref="A2.T10.37.37.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.37.37.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.38.38.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.38.38.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.38.38.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz<span id="A2.T10.38.38.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.38.38.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.38.38.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.38.38.2.1.1.m1.1.1" xref="A2.T10.38.38.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.38.38.2.1.1.m1.1b"><intersect id="A2.T10.38.38.2.1.1.m1.1.1.cmml" xref="A2.T10.38.38.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.38.38.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.38.38.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.5.1" class="ltx_text" style="background-color:#F2F2F2;">43.06</span></td>
<td id="A2.T10.38.38.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.6.1" class="ltx_text" style="background-color:#F2F2F2;">19.58</span></td>
<td id="A2.T10.38.38.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.38.38.7.1" class="ltx_text" style="background-color:#F2F2F2;">23.48</span></td>
</tr>
<tr id="A2.T10.40.40" class="ltx_tr">
<td id="A2.T10.39.39.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.39.39.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.39.39.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.39.39.1.1.m1.1a"><msub id="A2.T10.39.39.1.1.m1.1.1" xref="A2.T10.39.39.1.1.m1.1.1.cmml"><mi id="A2.T10.39.39.1.1.m1.1.1a" xref="A2.T10.39.39.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.39.39.1.1.m1.1.1.1" xref="A2.T10.39.39.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.39.39.1.1.m1.1b"><apply id="A2.T10.39.39.1.1.m1.1.1.cmml" xref="A2.T10.39.39.1.1.m1.1.1"><ci id="A2.T10.39.39.1.1.m1.1.1.1a.cmml" xref="A2.T10.39.39.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.39.39.1.1.m1.1.1.1.cmml" xref="A2.T10.39.39.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.39.39.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.40.40.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.40.40.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.40.40.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.40.40.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.40.40.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.40.40.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.40.40.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.40.40.2.m1.1a"><mo id="A2.T10.40.40.2.m1.1.1" xref="A2.T10.40.40.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.40.40.2.m1.1b"><intersect id="A2.T10.40.40.2.m1.1.1.cmml" xref="A2.T10.40.40.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.40.40.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.40.40.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.40.40.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">42.57</td>
<td id="A2.T10.40.40.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">13.71</td>
<td id="A2.T10.40.40.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">28.86</td>
</tr>
<tr id="A2.T10.42.42" class="ltx_tr">
<td id="A2.T10.41.41.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.41.41.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.41.41.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.41.41.1.1.m1.1a"><msub id="A2.T10.41.41.1.1.m1.1.1" xref="A2.T10.41.41.1.1.m1.1.1.cmml"><mi id="A2.T10.41.41.1.1.m1.1.1a" xref="A2.T10.41.41.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.41.41.1.1.m1.1.1.1" xref="A2.T10.41.41.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.41.41.1.1.m1.1b"><apply id="A2.T10.41.41.1.1.m1.1.1.cmml" xref="A2.T10.41.41.1.1.m1.1.1"><ci id="A2.T10.41.41.1.1.m1.1.1.1a.cmml" xref="A2.T10.41.41.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.41.41.1.1.m1.1.1.1.cmml" xref="A2.T10.41.41.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.41.41.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.42.42.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.42.42.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.42.42.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.42.42.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.42.42.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.42.42.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.42.42.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.42.42.2.m1.1a"><mo id="A2.T10.42.42.2.m1.1.1" xref="A2.T10.42.42.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.42.42.2.m1.1b"><intersect id="A2.T10.42.42.2.m1.1.1.cmml" xref="A2.T10.42.42.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.42.42.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.42.42.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.42.42.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">41.11</td>
<td id="A2.T10.42.42.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">13.20</td>
<td id="A2.T10.42.42.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">27.91</td>
</tr>
<tr id="A2.T10.44.44" class="ltx_tr">
<td id="A2.T10.43.43.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.43.43.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.43.43.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.43.43.1.1.m1.1a"><msub id="A2.T10.43.43.1.1.m1.1.1" xref="A2.T10.43.43.1.1.m1.1.1.cmml"><mi id="A2.T10.43.43.1.1.m1.1.1a" xref="A2.T10.43.43.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.43.43.1.1.m1.1.1.1" xref="A2.T10.43.43.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.43.43.1.1.m1.1b"><apply id="A2.T10.43.43.1.1.m1.1.1.cmml" xref="A2.T10.43.43.1.1.m1.1.1"><ci id="A2.T10.43.43.1.1.m1.1.1.1a.cmml" xref="A2.T10.43.43.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.43.43.1.1.m1.1.1.1.cmml" xref="A2.T10.43.43.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.43.43.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.44.44.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.44.44.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.44.44.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.44.44.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.44.44.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.44.44.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.44.44.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.44.44.2.m1.1a"><mo id="A2.T10.44.44.2.m1.1.1" xref="A2.T10.44.44.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.44.44.2.m1.1b"><intersect id="A2.T10.44.44.2.m1.1.1.cmml" xref="A2.T10.44.44.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.44.44.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.44.44.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.44.44.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">68.01</td>
<td id="A2.T10.44.44.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">52.35</td>
<td id="A2.T10.44.44.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">15.66</td>
</tr>
<tr id="A2.T10.46.46" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.45.45.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.45.45.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ViLBERT<math id="A2.T10.45.45.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.45.45.1.1.m1.1a"><msub id="A2.T10.45.45.1.1.m1.1.1" xref="A2.T10.45.45.1.1.m1.1.1.cmml"><mi id="A2.T10.45.45.1.1.m1.1.1a" xref="A2.T10.45.45.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.45.45.1.1.m1.1.1.1" xref="A2.T10.45.45.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.45.45.1.1.m1.1b"><apply id="A2.T10.45.45.1.1.m1.1.1.cmml" xref="A2.T10.45.45.1.1.m1.1.1"><ci id="A2.T10.45.45.1.1.m1.1.1.1a.cmml" xref="A2.T10.45.45.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.45.45.1.1.m1.1.1.1.cmml" xref="A2.T10.45.45.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.45.45.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.46.46.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.46.46.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.46.46.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2<span id="A2.T10.46.46.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.46.46.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.46.46.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.46.46.2.1.1.m1.1.1" xref="A2.T10.46.46.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.46.46.2.1.1.m1.1b"><intersect id="A2.T10.46.46.2.1.1.m1.1.1.cmml" xref="A2.T10.46.46.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.46.46.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VG</span></td>
<td id="A2.T10.46.46.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.5.1" class="ltx_text" style="background-color:#F2F2F2;">53.59</span></td>
<td id="A2.T10.46.46.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.6.1" class="ltx_text" style="background-color:#F2F2F2;">46.78</span></td>
<td id="A2.T10.46.46.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.46.46.7.1" class="ltx_text" style="background-color:#F2F2F2;">6.81</span></td>
</tr>
<tr id="A2.T10.48.48" class="ltx_tr">
<td id="A2.T10.47.47.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.47.47.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A2.T10.47.47.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.47.47.1.1.m1.1a"><msub id="A2.T10.47.47.1.1.m1.1.1" xref="A2.T10.47.47.1.1.m1.1.1.cmml"><mi id="A2.T10.47.47.1.1.m1.1.1a" xref="A2.T10.47.47.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.47.47.1.1.m1.1.1.1" xref="A2.T10.47.47.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.47.47.1.1.m1.1b"><apply id="A2.T10.47.47.1.1.m1.1.1.cmml" xref="A2.T10.47.47.1.1.m1.1.1"><ci id="A2.T10.47.47.1.1.m1.1.1.1a.cmml" xref="A2.T10.47.47.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.47.47.1.1.m1.1.1.1.cmml" xref="A2.T10.47.47.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.47.47.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.48.48.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.48.48.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.48.48.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.48.48.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.48.48.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.48.48.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.48.48.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.48.48.2.m1.1a"><mo id="A2.T10.48.48.2.m1.1.1" xref="A2.T10.48.48.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.48.48.2.m1.1b"><intersect id="A2.T10.48.48.2.m1.1.1.cmml" xref="A2.T10.48.48.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.48.48.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.48.48.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.48.48.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.69</td>
<td id="A2.T10.48.48.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">26.82</td>
<td id="A2.T10.48.48.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">37.87</td>
</tr>
<tr id="A2.T10.50.50" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.49.49.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.49.49.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.49.49.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.49.49.1.1.m1.1a"><msub id="A2.T10.49.49.1.1.m1.1.1" xref="A2.T10.49.49.1.1.m1.1.1.cmml"><mi id="A2.T10.49.49.1.1.m1.1.1a" xref="A2.T10.49.49.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.49.49.1.1.m1.1.1.1" xref="A2.T10.49.49.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.49.49.1.1.m1.1b"><apply id="A2.T10.49.49.1.1.m1.1.1.cmml" xref="A2.T10.49.49.1.1.m1.1.1"><ci id="A2.T10.49.49.1.1.m1.1.1.1a.cmml" xref="A2.T10.49.49.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.49.49.1.1.m1.1.1.1.cmml" xref="A2.T10.49.49.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.49.49.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.50.50.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.50.50.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.50.50.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA<span id="A2.T10.50.50.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.50.50.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.50.50.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.50.50.2.1.1.m1.1.1" xref="A2.T10.50.50.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.50.50.2.1.1.m1.1b"><intersect id="A2.T10.50.50.2.1.1.m1.1.1.cmml" xref="A2.T10.50.50.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.50.50.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.50.50.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.5.1" class="ltx_text" style="background-color:#F2F2F2;">63.24</span></td>
<td id="A2.T10.50.50.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.6.1" class="ltx_text" style="background-color:#F2F2F2;">51.06</span></td>
<td id="A2.T10.50.50.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.50.50.7.1" class="ltx_text" style="background-color:#F2F2F2;">12.18</span></td>
</tr>
<tr id="A2.T10.52.52" class="ltx_tr">
<td id="A2.T10.51.51.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.51.51.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.51.51.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.51.51.1.1.m1.1a"><msub id="A2.T10.51.51.1.1.m1.1.1" xref="A2.T10.51.51.1.1.m1.1.1.cmml"><mi id="A2.T10.51.51.1.1.m1.1.1a" xref="A2.T10.51.51.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.51.51.1.1.m1.1.1.1" xref="A2.T10.51.51.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.51.51.1.1.m1.1b"><apply id="A2.T10.51.51.1.1.m1.1.1.cmml" xref="A2.T10.51.51.1.1.m1.1.1"><ci id="A2.T10.51.51.1.1.m1.1.1.1a.cmml" xref="A2.T10.51.51.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.51.51.1.1.m1.1.1.1.cmml" xref="A2.T10.51.51.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.51.51.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.52.52.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.52.52.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.52.52.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.52.52.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.52.52.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.52.52.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.52.52.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.52.52.2.m1.1a"><mo id="A2.T10.52.52.2.m1.1.1" xref="A2.T10.52.52.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.52.52.2.m1.1b"><intersect id="A2.T10.52.52.2.m1.1.1.cmml" xref="A2.T10.52.52.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.52.52.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.52.52.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.52.52.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">53.09</td>
<td id="A2.T10.52.52.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">37.97</td>
<td id="A2.T10.52.52.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">15.12</td>
</tr>
<tr id="A2.T10.54.54" class="ltx_tr">
<td id="A2.T10.53.53.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.53.53.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.53.53.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.53.53.1.1.m1.1a"><msub id="A2.T10.53.53.1.1.m1.1.1" xref="A2.T10.53.53.1.1.m1.1.1.cmml"><mi id="A2.T10.53.53.1.1.m1.1.1a" xref="A2.T10.53.53.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.53.53.1.1.m1.1.1.1" xref="A2.T10.53.53.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.53.53.1.1.m1.1b"><apply id="A2.T10.53.53.1.1.m1.1.1.cmml" xref="A2.T10.53.53.1.1.m1.1.1"><ci id="A2.T10.53.53.1.1.m1.1.1.1a.cmml" xref="A2.T10.53.53.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.53.53.1.1.m1.1.1.1.cmml" xref="A2.T10.53.53.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.53.53.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.54.54.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.54.54.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.54.54.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.54.54.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.54.54.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.54.54.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.54.54.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.54.54.2.m1.1a"><mo id="A2.T10.54.54.2.m1.1.1" xref="A2.T10.54.54.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.54.54.2.m1.1b"><intersect id="A2.T10.54.54.2.m1.1.1.cmml" xref="A2.T10.54.54.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.54.54.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.54.54.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.54.54.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.85</td>
<td id="A2.T10.54.54.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">22.14</td>
<td id="A2.T10.54.54.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">42.71</td>
</tr>
<tr id="A2.T10.56.56" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.55.55.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.55.55.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.55.55.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.55.55.1.1.m1.1a"><msub id="A2.T10.55.55.1.1.m1.1.1" xref="A2.T10.55.55.1.1.m1.1.1.cmml"><mi id="A2.T10.55.55.1.1.m1.1.1a" xref="A2.T10.55.55.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.55.55.1.1.m1.1.1.1" xref="A2.T10.55.55.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.55.55.1.1.m1.1b"><apply id="A2.T10.55.55.1.1.m1.1.1.cmml" xref="A2.T10.55.55.1.1.m1.1.1"><ci id="A2.T10.55.55.1.1.m1.1.1.1a.cmml" xref="A2.T10.55.55.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.55.55.1.1.m1.1.1.1.cmml" xref="A2.T10.55.55.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.55.55.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.56.56.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.56.56.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.56.56.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG<span id="A2.T10.56.56.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.56.56.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.56.56.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.56.56.2.1.1.m1.1.1" xref="A2.T10.56.56.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.56.56.2.1.1.m1.1b"><intersect id="A2.T10.56.56.2.1.1.m1.1.1.cmml" xref="A2.T10.56.56.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.56.56.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.56.56.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.5.1" class="ltx_text" style="background-color:#F2F2F2;">61.38</span></td>
<td id="A2.T10.56.56.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.6.1" class="ltx_text" style="background-color:#F2F2F2;">55.83</span></td>
<td id="A2.T10.56.56.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.56.56.7.1" class="ltx_text" style="background-color:#F2F2F2;">5.55</span></td>
</tr>
<tr id="A2.T10.58.58" class="ltx_tr">
<td id="A2.T10.57.57.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.57.57.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.57.57.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.57.57.1.1.m1.1a"><msub id="A2.T10.57.57.1.1.m1.1.1" xref="A2.T10.57.57.1.1.m1.1.1.cmml"><mi id="A2.T10.57.57.1.1.m1.1.1a" xref="A2.T10.57.57.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.57.57.1.1.m1.1.1.1" xref="A2.T10.57.57.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.57.57.1.1.m1.1b"><apply id="A2.T10.57.57.1.1.m1.1.1.cmml" xref="A2.T10.57.57.1.1.m1.1.1"><ci id="A2.T10.57.57.1.1.m1.1.1.1a.cmml" xref="A2.T10.57.57.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.57.57.1.1.m1.1.1.1.cmml" xref="A2.T10.57.57.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.57.57.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.58.58.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.58.58.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.58.58.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.58.58.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.58.58.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.58.58.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.58.58.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.58.58.2.m1.1a"><mo id="A2.T10.58.58.2.m1.1.1" xref="A2.T10.58.58.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.58.58.2.m1.1b"><intersect id="A2.T10.58.58.2.m1.1.1.cmml" xref="A2.T10.58.58.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.58.58.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.58.58.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.58.58.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">63.76</td>
<td id="A2.T10.58.58.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">43.82</td>
<td id="A2.T10.58.58.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">19.94</td>
</tr>
<tr id="A2.T10.60.60" class="ltx_tr">
<td id="A2.T10.59.59.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.59.59.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.59.59.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.59.59.1.1.m1.1a"><msub id="A2.T10.59.59.1.1.m1.1.1" xref="A2.T10.59.59.1.1.m1.1.1.cmml"><mi id="A2.T10.59.59.1.1.m1.1.1a" xref="A2.T10.59.59.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.59.59.1.1.m1.1.1.1" xref="A2.T10.59.59.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.59.59.1.1.m1.1b"><apply id="A2.T10.59.59.1.1.m1.1.1.cmml" xref="A2.T10.59.59.1.1.m1.1.1"><ci id="A2.T10.59.59.1.1.m1.1.1.1a.cmml" xref="A2.T10.59.59.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.59.59.1.1.m1.1.1.1.cmml" xref="A2.T10.59.59.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.59.59.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.60.60.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.60.60.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.60.60.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.60.60.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.60.60.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.60.60.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.60.60.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.60.60.2.m1.1a"><mo id="A2.T10.60.60.2.m1.1.1" xref="A2.T10.60.60.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.60.60.2.m1.1b"><intersect id="A2.T10.60.60.2.m1.1.1.cmml" xref="A2.T10.60.60.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.60.60.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.60.60.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.60.60.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">64.12</td>
<td id="A2.T10.60.60.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">4.52</td>
<td id="A2.T10.60.60.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">59.60</td>
</tr>
<tr id="A2.T10.62.62" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.61.61.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.61.61.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.61.61.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.61.61.1.1.m1.1a"><msub id="A2.T10.61.61.1.1.m1.1.1" xref="A2.T10.61.61.1.1.m1.1.1.cmml"><mi id="A2.T10.61.61.1.1.m1.1.1a" xref="A2.T10.61.61.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.61.61.1.1.m1.1.1.1" xref="A2.T10.61.61.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.61.61.1.1.m1.1b"><apply id="A2.T10.61.61.1.1.m1.1.1.cmml" xref="A2.T10.61.61.1.1.m1.1.1"><ci id="A2.T10.61.61.1.1.m1.1.1.1a.cmml" xref="A2.T10.61.61.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.61.61.1.1.m1.1.1.1.cmml" xref="A2.T10.61.61.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.61.61.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.62.62.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.62.62.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.62.62.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz<span id="A2.T10.62.62.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.62.62.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.62.62.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.62.62.2.1.1.m1.1.1" xref="A2.T10.62.62.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.62.62.2.1.1.m1.1b"><intersect id="A2.T10.62.62.2.1.1.m1.1.1.cmml" xref="A2.T10.62.62.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.62.62.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.62.62.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.5.1" class="ltx_text" style="background-color:#F2F2F2;">42.42</span></td>
<td id="A2.T10.62.62.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.6.1" class="ltx_text" style="background-color:#F2F2F2;">26.23</span></td>
<td id="A2.T10.62.62.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.62.62.7.1" class="ltx_text" style="background-color:#F2F2F2;">16.19</span></td>
</tr>
<tr id="A2.T10.64.64" class="ltx_tr">
<td id="A2.T10.63.63.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.63.63.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.63.63.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.63.63.1.1.m1.1a"><msub id="A2.T10.63.63.1.1.m1.1.1" xref="A2.T10.63.63.1.1.m1.1.1.cmml"><mi id="A2.T10.63.63.1.1.m1.1.1a" xref="A2.T10.63.63.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.63.63.1.1.m1.1.1.1" xref="A2.T10.63.63.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.63.63.1.1.m1.1b"><apply id="A2.T10.63.63.1.1.m1.1.1.cmml" xref="A2.T10.63.63.1.1.m1.1.1"><ci id="A2.T10.63.63.1.1.m1.1.1.1a.cmml" xref="A2.T10.63.63.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.63.63.1.1.m1.1.1.1.cmml" xref="A2.T10.63.63.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.63.63.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.64.64.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.64.64.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.64.64.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.64.64.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.64.64.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.64.64.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.64.64.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.64.64.2.m1.1a"><mo id="A2.T10.64.64.2.m1.1.1" xref="A2.T10.64.64.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.64.64.2.m1.1b"><intersect id="A2.T10.64.64.2.m1.1.1.cmml" xref="A2.T10.64.64.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.64.64.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.64.64.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.64.64.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">40.49</td>
<td id="A2.T10.64.64.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">20.44</td>
<td id="A2.T10.64.64.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">20.05</td>
</tr>
<tr id="A2.T10.66.66" class="ltx_tr">
<td id="A2.T10.65.65.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.65.65.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.65.65.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.65.65.1.1.m1.1a"><msub id="A2.T10.65.65.1.1.m1.1.1" xref="A2.T10.65.65.1.1.m1.1.1.cmml"><mi id="A2.T10.65.65.1.1.m1.1.1a" xref="A2.T10.65.65.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.65.65.1.1.m1.1.1.1" xref="A2.T10.65.65.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.65.65.1.1.m1.1b"><apply id="A2.T10.65.65.1.1.m1.1.1.cmml" xref="A2.T10.65.65.1.1.m1.1.1"><ci id="A2.T10.65.65.1.1.m1.1.1.1a.cmml" xref="A2.T10.65.65.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.65.65.1.1.m1.1.1.1.cmml" xref="A2.T10.65.65.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.65.65.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.66.66.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.66.66.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.66.66.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.66.66.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.66.66.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.66.66.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.66.66.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.66.66.2.m1.1a"><mo id="A2.T10.66.66.2.m1.1.1" xref="A2.T10.66.66.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.66.66.2.m1.1b"><intersect id="A2.T10.66.66.2.m1.1.1.cmml" xref="A2.T10.66.66.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.66.66.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.66.66.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.66.66.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">38.15</td>
<td id="A2.T10.66.66.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">19.68</td>
<td id="A2.T10.66.66.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">18.47</td>
</tr>
<tr id="A2.T10.68.68" class="ltx_tr">
<td id="A2.T10.67.67.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.67.67.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.67.67.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.67.67.1.1.m1.1a"><msub id="A2.T10.67.67.1.1.m1.1.1" xref="A2.T10.67.67.1.1.m1.1.1.cmml"><mi id="A2.T10.67.67.1.1.m1.1.1a" xref="A2.T10.67.67.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.67.67.1.1.m1.1.1.1" xref="A2.T10.67.67.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.67.67.1.1.m1.1b"><apply id="A2.T10.67.67.1.1.m1.1.1.cmml" xref="A2.T10.67.67.1.1.m1.1.1"><ci id="A2.T10.67.67.1.1.m1.1.1.1a.cmml" xref="A2.T10.67.67.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.67.67.1.1.m1.1.1.1.cmml" xref="A2.T10.67.67.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.67.67.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.68.68.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.68.68.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.68.68.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.68.68.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.68.68.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.68.68.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.68.68.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.68.68.2.m1.1a"><mo id="A2.T10.68.68.2.m1.1.1" xref="A2.T10.68.68.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.68.68.2.m1.1b"><intersect id="A2.T10.68.68.2.m1.1.1.cmml" xref="A2.T10.68.68.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.68.68.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.68.68.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.68.68.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">76.64</td>
<td id="A2.T10.68.68.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">57.18</td>
<td id="A2.T10.68.68.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">19.46</td>
</tr>
<tr id="A2.T10.70.70" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.69.69.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.69.69.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.69.69.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.69.69.1.1.m1.1a"><msub id="A2.T10.69.69.1.1.m1.1.1" xref="A2.T10.69.69.1.1.m1.1.1.cmml"><mi id="A2.T10.69.69.1.1.m1.1.1a" xref="A2.T10.69.69.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.69.69.1.1.m1.1.1.1" xref="A2.T10.69.69.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.69.69.1.1.m1.1b"><apply id="A2.T10.69.69.1.1.m1.1.1.cmml" xref="A2.T10.69.69.1.1.m1.1.1"><ci id="A2.T10.69.69.1.1.m1.1.1.1a.cmml" xref="A2.T10.69.69.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.69.69.1.1.m1.1.1.1.cmml" xref="A2.T10.69.69.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.69.69.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.70.70.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.70.70.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.70.70.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2<span id="A2.T10.70.70.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.70.70.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.70.70.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.70.70.2.1.1.m1.1.1" xref="A2.T10.70.70.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.70.70.2.1.1.m1.1b"><intersect id="A2.T10.70.70.2.1.1.m1.1.1.cmml" xref="A2.T10.70.70.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.70.70.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VG</span></td>
<td id="A2.T10.70.70.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.5.1" class="ltx_text" style="background-color:#F2F2F2;">63.47</span></td>
<td id="A2.T10.70.70.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.6.1" class="ltx_text" style="background-color:#F2F2F2;">52.78</span></td>
<td id="A2.T10.70.70.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.70.70.7.1" class="ltx_text" style="background-color:#F2F2F2;">10.69</span></td>
</tr>
<tr id="A2.T10.72.72" class="ltx_tr">
<td id="A2.T10.71.71.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.71.71.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.71.71.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A2.T10.71.71.1.1.m1.1a"><msub id="A2.T10.71.71.1.1.m1.1.1" xref="A2.T10.71.71.1.1.m1.1.1.cmml"><mi id="A2.T10.71.71.1.1.m1.1.1a" xref="A2.T10.71.71.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.71.71.1.1.m1.1.1.1" xref="A2.T10.71.71.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.71.71.1.1.m1.1b"><apply id="A2.T10.71.71.1.1.m1.1.1.cmml" xref="A2.T10.71.71.1.1.m1.1.1"><ci id="A2.T10.71.71.1.1.m1.1.1.1a.cmml" xref="A2.T10.71.71.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.71.71.1.1.m1.1.1.1.cmml" xref="A2.T10.71.71.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.71.71.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span></td>
<td id="A2.T10.72.72.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.72.72.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.72.72.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.72.72.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.72.72.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.72.72.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.72.72.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.72.72.2.m1.1a"><mo id="A2.T10.72.72.2.m1.1.1" xref="A2.T10.72.72.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.72.72.2.m1.1b"><intersect id="A2.T10.72.72.2.m1.1.1.cmml" xref="A2.T10.72.72.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.72.72.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.72.72.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.72.72.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">72.84</td>
<td id="A2.T10.72.72.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">28.08</td>
<td id="A2.T10.72.72.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">44.76</td>
</tr>
<tr id="A2.T10.74.74" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.73.73.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.73.73.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.73.73.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.73.73.1.1.m1.1a"><msub id="A2.T10.73.73.1.1.m1.1.1" xref="A2.T10.73.73.1.1.m1.1.1.cmml"><mi id="A2.T10.73.73.1.1.m1.1.1a" xref="A2.T10.73.73.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.73.73.1.1.m1.1.1.1" xref="A2.T10.73.73.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.73.73.1.1.m1.1b"><apply id="A2.T10.73.73.1.1.m1.1.1.cmml" xref="A2.T10.73.73.1.1.m1.1.1"><ci id="A2.T10.73.73.1.1.m1.1.1.1a.cmml" xref="A2.T10.73.73.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.73.73.1.1.m1.1.1.1.cmml" xref="A2.T10.73.73.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.73.73.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.74.74.3" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.74.74.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA</span></td>
<td id="A2.T10.74.74.2" class="ltx_td ltx_align_left ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">GQA<span id="A2.T10.74.74.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.74.74.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.74.74.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.74.74.2.1.1.m1.1.1" xref="A2.T10.74.74.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.74.74.2.1.1.m1.1b"><intersect id="A2.T10.74.74.2.1.1.m1.1.1.cmml" xref="A2.T10.74.74.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.74.74.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.74.74.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.5.1" class="ltx_text" style="background-color:#F2F2F2;">65.81</span></td>
<td id="A2.T10.74.74.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.6.1" class="ltx_text" style="background-color:#F2F2F2;">51.72</span></td>
<td id="A2.T10.74.74.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.74.74.7.1" class="ltx_text" style="background-color:#F2F2F2;">14.09</span></td>
</tr>
<tr id="A2.T10.76.76" class="ltx_tr">
<td id="A2.T10.75.75.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.75.75.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.75.75.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.75.75.1.1.m1.1a"><msub id="A2.T10.75.75.1.1.m1.1.1" xref="A2.T10.75.75.1.1.m1.1.1.cmml"><mi id="A2.T10.75.75.1.1.m1.1.1a" xref="A2.T10.75.75.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.75.75.1.1.m1.1.1.1" xref="A2.T10.75.75.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.75.75.1.1.m1.1b"><apply id="A2.T10.75.75.1.1.m1.1.1.cmml" xref="A2.T10.75.75.1.1.m1.1.1"><ci id="A2.T10.75.75.1.1.m1.1.1.1a.cmml" xref="A2.T10.75.75.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.75.75.1.1.m1.1.1.1.cmml" xref="A2.T10.75.75.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.75.75.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.76.76.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.76.76.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.76.76.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.76.76.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.76.76.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.76.76.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.76.76.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.76.76.2.m1.1a"><mo id="A2.T10.76.76.2.m1.1.1" xref="A2.T10.76.76.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.76.76.2.m1.1b"><intersect id="A2.T10.76.76.2.m1.1.1.cmml" xref="A2.T10.76.76.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.76.76.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.76.76.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.76.76.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">56.08</td>
<td id="A2.T10.76.76.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">37.71</td>
<td id="A2.T10.76.76.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">18.37</td>
</tr>
<tr id="A2.T10.78.78" class="ltx_tr">
<td id="A2.T10.77.77.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.77.77.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.77.77.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.77.77.1.1.m1.1a"><msub id="A2.T10.77.77.1.1.m1.1.1" xref="A2.T10.77.77.1.1.m1.1.1.cmml"><mi id="A2.T10.77.77.1.1.m1.1.1a" xref="A2.T10.77.77.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.77.77.1.1.m1.1.1.1" xref="A2.T10.77.77.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.77.77.1.1.m1.1b"><apply id="A2.T10.77.77.1.1.m1.1.1.cmml" xref="A2.T10.77.77.1.1.m1.1.1"><ci id="A2.T10.77.77.1.1.m1.1.1.1a.cmml" xref="A2.T10.77.77.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.77.77.1.1.m1.1.1.1.cmml" xref="A2.T10.77.77.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.77.77.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.78.78.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.78.78.3.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.78.78.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.78.78.4.1" class="ltx_text ltx_font_smallcaps">GQA</span></td>
<td id="A2.T10.78.78.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.78.78.2.1" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A2.T10.78.78.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.78.78.2.m1.1a"><mo id="A2.T10.78.78.2.m1.1.1" xref="A2.T10.78.78.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.78.78.2.m1.1b"><intersect id="A2.T10.78.78.2.m1.1.1.cmml" xref="A2.T10.78.78.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.78.78.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.78.78.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.78.78.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">67.05</td>
<td id="A2.T10.78.78.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">27.61</td>
<td id="A2.T10.78.78.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">39.44</td>
</tr>
<tr id="A2.T10.80.80" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.79.79.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.79.79.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.79.79.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.79.79.1.1.m1.1a"><msub id="A2.T10.79.79.1.1.m1.1.1" xref="A2.T10.79.79.1.1.m1.1.1.cmml"><mi id="A2.T10.79.79.1.1.m1.1.1a" xref="A2.T10.79.79.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.79.79.1.1.m1.1.1.1" xref="A2.T10.79.79.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.79.79.1.1.m1.1b"><apply id="A2.T10.79.79.1.1.m1.1.1.cmml" xref="A2.T10.79.79.1.1.m1.1.1"><ci id="A2.T10.79.79.1.1.m1.1.1.1a.cmml" xref="A2.T10.79.79.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.79.79.1.1.m1.1.1.1.cmml" xref="A2.T10.79.79.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.79.79.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.80.80.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.80.80.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG</span></td>
<td id="A2.T10.80.80.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VG<span id="A2.T10.80.80.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.80.80.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.80.80.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.80.80.2.1.1.m1.1.1" xref="A2.T10.80.80.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.80.80.2.1.1.m1.1b"><intersect id="A2.T10.80.80.2.1.1.m1.1.1.cmml" xref="A2.T10.80.80.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.80.80.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.80.80.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.5.1" class="ltx_text" style="background-color:#F2F2F2;">62.71</span></td>
<td id="A2.T10.80.80.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.6.1" class="ltx_text" style="background-color:#F2F2F2;">57.33</span></td>
<td id="A2.T10.80.80.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.80.80.7.1" class="ltx_text" style="background-color:#F2F2F2;">5.38</span></td>
</tr>
<tr id="A2.T10.82.82" class="ltx_tr">
<td id="A2.T10.81.81.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.81.81.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.81.81.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.81.81.1.1.m1.1a"><msub id="A2.T10.81.81.1.1.m1.1.1" xref="A2.T10.81.81.1.1.m1.1.1.cmml"><mi id="A2.T10.81.81.1.1.m1.1.1a" xref="A2.T10.81.81.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.81.81.1.1.m1.1.1.1" xref="A2.T10.81.81.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.81.81.1.1.m1.1b"><apply id="A2.T10.81.81.1.1.m1.1.1.cmml" xref="A2.T10.81.81.1.1.m1.1.1"><ci id="A2.T10.81.81.1.1.m1.1.1.1a.cmml" xref="A2.T10.81.81.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.81.81.1.1.m1.1.1.1.cmml" xref="A2.T10.81.81.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.81.81.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.82.82.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.82.82.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.82.82.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.82.82.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.82.82.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.82.82.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.82.82.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.82.82.2.m1.1a"><mo id="A2.T10.82.82.2.m1.1.1" xref="A2.T10.82.82.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.82.82.2.m1.1b"><intersect id="A2.T10.82.82.2.m1.1.1.cmml" xref="A2.T10.82.82.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.82.82.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.82.82.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.82.82.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">65.48</td>
<td id="A2.T10.82.82.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">51.17</td>
<td id="A2.T10.82.82.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">14.31</td>
</tr>
<tr id="A2.T10.84.84" class="ltx_tr">
<td id="A2.T10.83.83.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.83.83.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.83.83.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.83.83.1.1.m1.1a"><msub id="A2.T10.83.83.1.1.m1.1.1" xref="A2.T10.83.83.1.1.m1.1.1.cmml"><mi id="A2.T10.83.83.1.1.m1.1.1a" xref="A2.T10.83.83.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.83.83.1.1.m1.1.1.1" xref="A2.T10.83.83.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.83.83.1.1.m1.1b"><apply id="A2.T10.83.83.1.1.m1.1.1.cmml" xref="A2.T10.83.83.1.1.m1.1.1"><ci id="A2.T10.83.83.1.1.m1.1.1.1a.cmml" xref="A2.T10.83.83.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.83.83.1.1.m1.1.1.1.cmml" xref="A2.T10.83.83.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.83.83.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.84.84.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.84.84.3.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.84.84.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.84.84.4.1" class="ltx_text ltx_font_smallcaps">VG</span></td>
<td id="A2.T10.84.84.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.84.84.2.1" class="ltx_text ltx_font_smallcaps">VG</span> <math id="A2.T10.84.84.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.84.84.2.m1.1a"><mo id="A2.T10.84.84.2.m1.1.1" xref="A2.T10.84.84.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.84.84.2.m1.1b"><intersect id="A2.T10.84.84.2.m1.1.1.cmml" xref="A2.T10.84.84.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.84.84.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.84.84.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.84.84.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">66.20</td>
<td id="A2.T10.84.84.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">21.13</td>
<td id="A2.T10.84.84.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">45.07</td>
</tr>
<tr id="A2.T10.86.86" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.85.85.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.85.85.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.85.85.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.85.85.1.1.m1.1a"><msub id="A2.T10.85.85.1.1.m1.1.1" xref="A2.T10.85.85.1.1.m1.1.1.cmml"><mi id="A2.T10.85.85.1.1.m1.1.1a" xref="A2.T10.85.85.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.85.85.1.1.m1.1.1.1" xref="A2.T10.85.85.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.85.85.1.1.m1.1b"><apply id="A2.T10.85.85.1.1.m1.1.1.cmml" xref="A2.T10.85.85.1.1.m1.1.1"><ci id="A2.T10.85.85.1.1.m1.1.1.1a.cmml" xref="A2.T10.85.85.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.85.85.1.1.m1.1.1.1.cmml" xref="A2.T10.85.85.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.85.85.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.86.86.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.86.86.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz</span></td>
<td id="A2.T10.86.86.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VizWiz<span id="A2.T10.86.86.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.86.86.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.86.86.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.86.86.2.1.1.m1.1.1" xref="A2.T10.86.86.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.86.86.2.1.1.m1.1b"><intersect id="A2.T10.86.86.2.1.1.m1.1.1.cmml" xref="A2.T10.86.86.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.86.86.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VQAv2</span></td>
<td id="A2.T10.86.86.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.5.1" class="ltx_text" style="background-color:#F2F2F2;">52.85</span></td>
<td id="A2.T10.86.86.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.6.1" class="ltx_text" style="background-color:#F2F2F2;">28.96</span></td>
<td id="A2.T10.86.86.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.86.86.7.1" class="ltx_text" style="background-color:#F2F2F2;">23.89</span></td>
</tr>
<tr id="A2.T10.88.88" class="ltx_tr">
<td id="A2.T10.87.87.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.87.87.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.87.87.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.87.87.1.1.m1.1a"><msub id="A2.T10.87.87.1.1.m1.1.1" xref="A2.T10.87.87.1.1.m1.1.1.cmml"><mi id="A2.T10.87.87.1.1.m1.1.1a" xref="A2.T10.87.87.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.87.87.1.1.m1.1.1.1" xref="A2.T10.87.87.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.87.87.1.1.m1.1b"><apply id="A2.T10.87.87.1.1.m1.1.1.cmml" xref="A2.T10.87.87.1.1.m1.1.1"><ci id="A2.T10.87.87.1.1.m1.1.1.1a.cmml" xref="A2.T10.87.87.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.87.87.1.1.m1.1.1.1.cmml" xref="A2.T10.87.87.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.87.87.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.88.88.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.88.88.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.88.88.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.88.88.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.88.88.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.88.88.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.88.88.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.88.88.2.m1.1a"><mo id="A2.T10.88.88.2.m1.1.1" xref="A2.T10.88.88.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.88.88.2.m1.1b"><intersect id="A2.T10.88.88.2.m1.1.1.cmml" xref="A2.T10.88.88.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.88.88.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.88.88.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.88.88.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">52.58</td>
<td id="A2.T10.88.88.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">22.21</td>
<td id="A2.T10.88.88.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">30.37</td>
</tr>
<tr id="A2.T10.90.90" class="ltx_tr">
<td id="A2.T10.89.89.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.89.89.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.89.89.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.89.89.1.1.m1.1a"><msub id="A2.T10.89.89.1.1.m1.1.1" xref="A2.T10.89.89.1.1.m1.1.1.cmml"><mi id="A2.T10.89.89.1.1.m1.1.1a" xref="A2.T10.89.89.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.89.89.1.1.m1.1.1.1" xref="A2.T10.89.89.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.89.89.1.1.m1.1b"><apply id="A2.T10.89.89.1.1.m1.1.1.cmml" xref="A2.T10.89.89.1.1.m1.1.1"><ci id="A2.T10.89.89.1.1.m1.1.1.1a.cmml" xref="A2.T10.89.89.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.89.89.1.1.m1.1.1.1.cmml" xref="A2.T10.89.89.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.89.89.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.90.90.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.90.90.3.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.90.90.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.90.90.4.1" class="ltx_text ltx_font_smallcaps">VizWiz</span></td>
<td id="A2.T10.90.90.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.90.90.2.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> <math id="A2.T10.90.90.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.90.90.2.m1.1a"><mo id="A2.T10.90.90.2.m1.1.1" xref="A2.T10.90.90.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.90.90.2.m1.1b"><intersect id="A2.T10.90.90.2.m1.1.1.cmml" xref="A2.T10.90.90.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.90.90.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.90.90.2.2" class="ltx_text ltx_font_smallcaps">VG</span>
</td>
<td id="A2.T10.90.90.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">51.94</td>
<td id="A2.T10.90.90.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">23.56</td>
<td id="A2.T10.90.90.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">28.38</td>
</tr>
<tr id="A2.T10.92.92" class="ltx_tr">
<td id="A2.T10.91.91.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.91.91.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.91.91.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.91.91.1.1.m1.1a"><msub id="A2.T10.91.91.1.1.m1.1.1" xref="A2.T10.91.91.1.1.m1.1.1.cmml"><mi id="A2.T10.91.91.1.1.m1.1.1a" xref="A2.T10.91.91.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.91.91.1.1.m1.1.1.1" xref="A2.T10.91.91.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.91.91.1.1.m1.1b"><apply id="A2.T10.91.91.1.1.m1.1.1.cmml" xref="A2.T10.91.91.1.1.m1.1.1"><ci id="A2.T10.91.91.1.1.m1.1.1.1a.cmml" xref="A2.T10.91.91.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.91.91.1.1.m1.1.1.1.cmml" xref="A2.T10.91.91.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.91.91.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.92.92.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.92.92.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.92.92.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.92.92.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.92.92.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.92.92.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.92.92.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.92.92.2.m1.1a"><mo id="A2.T10.92.92.2.m1.1.1" xref="A2.T10.92.92.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.92.92.2.m1.1b"><intersect id="A2.T10.92.92.2.m1.1.1.cmml" xref="A2.T10.92.92.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.92.92.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.92.92.2.2" class="ltx_text ltx_font_smallcaps">GQA</span>
</td>
<td id="A2.T10.92.92.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">78.03</td>
<td id="A2.T10.92.92.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">62.93</td>
<td id="A2.T10.92.92.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;">15.10</td>
</tr>
<tr id="A2.T10.94.94" class="ltx_tr" style="background-color:#F2F2F2;">
<td id="A2.T10.93.93.1" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.93.93.1.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">ALBEF<math id="A2.T10.93.93.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.93.93.1.1.m1.1a"><msub id="A2.T10.93.93.1.1.m1.1.1" xref="A2.T10.93.93.1.1.m1.1.1.cmml"><mi id="A2.T10.93.93.1.1.m1.1.1a" xref="A2.T10.93.93.1.1.m1.1.1.cmml"></mi><mtext mathbackground="#F2F2F2" id="A2.T10.93.93.1.1.m1.1.1.1" xref="A2.T10.93.93.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.93.93.1.1.m1.1b"><apply id="A2.T10.93.93.1.1.m1.1.1.cmml" xref="A2.T10.93.93.1.1.m1.1.1"><ci id="A2.T10.93.93.1.1.m1.1.1.1a.cmml" xref="A2.T10.93.93.1.1.m1.1.1.1"><mtext mathbackground="#F2F2F2" mathsize="70%" id="A2.T10.93.93.1.1.m1.1.1.1.cmml" xref="A2.T10.93.93.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.93.93.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.94.94.3" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.3.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.94.94.4" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.4.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2</span></td>
<td id="A2.T10.94.94.2" class="ltx_td ltx_align_left" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.2.1" class="ltx_text ltx_font_smallcaps" style="background-color:#F2F2F2;">VQAv2<span id="A2.T10.94.94.2.1.1" class="ltx_text ltx_font_upright"> <math id="A2.T10.94.94.2.1.1.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.94.94.2.1.1.m1.1a"><mo mathbackground="#F2F2F2" id="A2.T10.94.94.2.1.1.m1.1.1" xref="A2.T10.94.94.2.1.1.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.94.94.2.1.1.m1.1b"><intersect id="A2.T10.94.94.2.1.1.m1.1.1.cmml" xref="A2.T10.94.94.2.1.1.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.94.94.2.1.1.m1.1c">\cap</annotation></semantics></math> </span>VG</span></td>
<td id="A2.T10.94.94.5" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.5.1" class="ltx_text" style="background-color:#F2F2F2;">65.20</span></td>
<td id="A2.T10.94.94.6" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.6.1" class="ltx_text" style="background-color:#F2F2F2;">55.64</span></td>
<td id="A2.T10.94.94.7" class="ltx_td ltx_align_center" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.94.94.7.1" class="ltx_text" style="background-color:#F2F2F2;">9.56</span></td>
</tr>
<tr id="A2.T10.96.96" class="ltx_tr">
<td id="A2.T10.95.95.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.95.95.1.1" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A2.T10.95.95.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A2.T10.95.95.1.1.m1.1a"><msub id="A2.T10.95.95.1.1.m1.1.1" xref="A2.T10.95.95.1.1.m1.1.1.cmml"><mi id="A2.T10.95.95.1.1.m1.1.1a" xref="A2.T10.95.95.1.1.m1.1.1.cmml"></mi><mtext id="A2.T10.95.95.1.1.m1.1.1.1" xref="A2.T10.95.95.1.1.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A2.T10.95.95.1.1.m1.1b"><apply id="A2.T10.95.95.1.1.m1.1.1.cmml" xref="A2.T10.95.95.1.1.m1.1.1"><ci id="A2.T10.95.95.1.1.m1.1.1.1a.cmml" xref="A2.T10.95.95.1.1.m1.1.1.1"><mtext mathsize="70%" id="A2.T10.95.95.1.1.m1.1.1.1.cmml" xref="A2.T10.95.95.1.1.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.95.95.1.1.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span></td>
<td id="A2.T10.96.96.3" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.96.96.3.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.96.96.4" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;"><span id="A2.T10.96.96.4.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></td>
<td id="A2.T10.96.96.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">
<span id="A2.T10.96.96.2.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A2.T10.96.96.2.m1.1" class="ltx_Math" alttext="\cap" display="inline"><semantics id="A2.T10.96.96.2.m1.1a"><mo id="A2.T10.96.96.2.m1.1.1" xref="A2.T10.96.96.2.m1.1.1.cmml">∩</mo><annotation-xml encoding="MathML-Content" id="A2.T10.96.96.2.m1.1b"><intersect id="A2.T10.96.96.2.m1.1.1.cmml" xref="A2.T10.96.96.2.m1.1.1"></intersect></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.96.96.2.m1.1c">\cap</annotation></semantics></math> <span id="A2.T10.96.96.2.2" class="ltx_text ltx_font_smallcaps">VizWiz</span>
</td>
<td id="A2.T10.96.96.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">74.38</td>
<td id="A2.T10.96.96.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">39.37</td>
<td id="A2.T10.96.96.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.5pt;padding-right:2.5pt;">35.01</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>VQA accuracy of each model in the IID settings (see column VQA Acc. (IID)) when evaluated on the questions in the test sets whose answers are shared between the top-<math id="A2.T10.98.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.T10.98.m1.1b"><mi id="A2.T10.98.m1.1.1" xref="A2.T10.98.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.T10.98.m1.1c"><ci id="A2.T10.98.m1.1.1.cmml" xref="A2.T10.98.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T10.98.m1.1d">k</annotation></semantics></math> answers in both the IID and the OOD settings. Please refer to <a href="#S4.SS1" title="4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.1</span></a> for more details.
Answer Set: OOD benchmarks with respect to which IID shared answer set accuracy is computed.
VQA Acc. (OOD): OOD accuracy on questions corresponding to the shared answer set, <em id="A2.T10.100.1" class="ltx_emph ltx_font_italic">i.e.</em> when fine-tuned on the OOD dataset mentioned in Answer Set column and tested on the benchmark mentioned in the Test column.
Difference: VQA Acc. (IID) - VQA Acc. (OOD). Gray bands highlight the OOD benchmarks with respect to which IID shared answer set accuracy is computed in <a href="#S4.F2" title="In 4.1 Evaluating on Shared Answer Sets ‣ 4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Potential Causes of Poor OOD Generalization: A Qualitative Study</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.3" class="ltx_p">In section <a href="#S4" title="4 Out-of-Distribution Generalization ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we observe that our pretrained models exhibit poor OOD generalization for the task of VQA. We also noted that this poor generalization is not entirely explained by the absence or poor representation of test answer classes in the training data. Here, we perform a qualitative study to dig deeper into the potential causes of the poor OOD generalization. We manually examine 20 randomly-sampled qualitative examples of failure cases on top-30 answer classes contributing the most to the drop in performance from IID to OOD. We only focus on answer classes that are shared between the train and test splits to make sure the performance drop is not due to the absence of answer classes in the training dataset. We report the top-5 classes that contribute the most to the drop in performance for each OOD setting in <a href="#A3.T11" title="In Poor performance of GQA model on color questions (both IID and OOD): ‣ Appendix C Potential Causes of Poor OOD Generalization: A Qualitative Study ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a>. Below, we describe four major potential causes<span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>For poor OOD generalization on the <span id="footnote9.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> benchmark, one of the reasons could be difference in image distributions between <span id="footnote9.2" class="ltx_text ltx_font_smallcaps">VizWiz</span> (that contains many blurry pictures, or pictures with poor lighting conditions) and other three datasets (that contain clear pictures).</span></span></span> for the poor OOD generalization that we can infer from our qualitative study on <span id="A3.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.p1.1.1.m1.1a"><msub id="A3.p1.1.1.m1.1.1" xref="A3.p1.1.1.m1.1.1.cmml"><mi id="A3.p1.1.1.m1.1.1a" xref="A3.p1.1.1.m1.1.1.cmml"></mi><mtext id="A3.p1.1.1.m1.1.1.1" xref="A3.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.p1.1.1.m1.1b"><apply id="A3.p1.1.1.m1.1.1.cmml" xref="A3.p1.1.1.m1.1.1"><ci id="A3.p1.1.1.m1.1.1.1a.cmml" xref="A3.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A3.p1.1.1.m1.1.1.1.cmml" xref="A3.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> <span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>We use the model trained with the official codebase.</span></span></span>
and <span id="A3.p1.2.2" class="ltx_text ltx_font_smallcaps">ALBEF<math id="A3.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A3.p1.2.2.m1.1a"><msub id="A3.p1.2.2.m1.1.1" xref="A3.p1.2.2.m1.1.1.cmml"><mi id="A3.p1.2.2.m1.1.1a" xref="A3.p1.2.2.m1.1.1.cmml"></mi><mtext id="A3.p1.2.2.m1.1.1.1" xref="A3.p1.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.p1.2.2.m1.1b"><apply id="A3.p1.2.2.m1.1.1.cmml" xref="A3.p1.2.2.m1.1.1"><ci id="A3.p1.2.2.m1.1.1.1a.cmml" xref="A3.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="A3.p1.2.2.m1.1.1.1.cmml" xref="A3.p1.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span>. The specific examples reported below are for <span id="A3.p1.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.p1.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.p1.3.3.m1.1a"><msub id="A3.p1.3.3.m1.1.1" xref="A3.p1.3.3.m1.1.1.cmml"><mi id="A3.p1.3.3.m1.1.1a" xref="A3.p1.3.3.m1.1.1.cmml"></mi><mtext id="A3.p1.3.3.m1.1.1.1" xref="A3.p1.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.p1.3.3.m1.1b"><apply id="A3.p1.3.3.m1.1.1.cmml" xref="A3.p1.3.3.m1.1.1"><ci id="A3.p1.3.3.m1.1.1.1a.cmml" xref="A3.p1.3.3.m1.1.1.1"><mtext mathsize="70%" id="A3.p1.3.3.m1.1.1.1.cmml" xref="A3.p1.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.p1.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span>.</p>
</div>
<section id="A3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Poor reasoning skills.</h4>

<div id="A3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px1.p1.1" class="ltx_p">In <a href="#A3.T11" title="In Poor performance of GQA model on color questions (both IID and OOD): ‣ Appendix C Potential Causes of Poor OOD Generalization: A Qualitative Study ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a>, we can see that a model fine-tuned on <span id="A3.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="A3.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">VG</span>, or <span id="A3.SS0.SSS0.Px1.p1.1.3" class="ltx_text ltx_font_smallcaps">VizWiz</span> and evaluated on <span id="A3.SS0.SSS0.Px1.p1.1.4" class="ltx_text ltx_font_smallcaps">GQA</span> shows the highest performance drop on classes such as “yes”, “no”, “right”, “left”, “top”, and “bottom”. For instance, <span id="A3.SS0.SSS0.Px1.p1.1.5" class="ltx_text ltx_font_smallcaps">VQAv2</span>–<span id="A3.SS0.SSS0.Px1.p1.1.6" class="ltx_text ltx_font_smallcaps">GQA</span> (fine-tuned on <span id="A3.SS0.SSS0.Px1.p1.1.7" class="ltx_text ltx_font_smallcaps">VQAv2</span>, evaluated on <span id="A3.SS0.SSS0.Px1.p1.1.8" class="ltx_text ltx_font_smallcaps">GQA</span>) model underperforms <span id="A3.SS0.SSS0.Px1.p1.1.9" class="ltx_text ltx_font_smallcaps">GQA</span>-<span id="A3.SS0.SSS0.Px1.p1.1.10" class="ltx_text ltx_font_smallcaps">GQA</span> model by 24% for “no.” Upon qualitative examination, we find that for many of such failure cases, the <span id="A3.SS0.SSS0.Px1.p1.1.11" class="ltx_text ltx_font_smallcaps">GQA</span> questions are more compositional and hence require more complex reasoning (<em id="A3.SS0.SSS0.Px1.p1.1.12" class="ltx_emph ltx_font_italic">e.g.</em>, “Are there both bison and zebras in the image?”, “Is the cheese to the right or to the left of the empty plate?”) than the questions for the same answer classes in other datasets (<em id="A3.SS0.SSS0.Px1.p1.1.13" class="ltx_emph ltx_font_italic">e.g.</em>, from <span id="A3.SS0.SSS0.Px1.p1.1.14" class="ltx_text ltx_font_smallcaps">VQAv2</span> train set: “Is the TV turned on?”, “Which hand is the man holding up?”).
This study re-affirms previous findings <cite class="ltx_cite ltx_citemacro_cite">Johnson et al. (<a href="#bib.bib26" title="" class="ltx_ref">2017</a>); Hudson and Manning (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>)</cite> – VQA models lack sufficient logical, spatial, and compositional reasoning skills – for the more recent, pretrained Transformer models.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overfitting to the answer priors.</h4>

<div id="A3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px2.p1.2" class="ltx_p">Previous studies have shown that VQA models tend to be biased towards the prior distribution of answers in the training set (per question type) <cite class="ltx_cite ltx_citemacro_cite">Agrawal et al. (<a href="#bib.bib2" title="" class="ltx_ref">2018</a>)</cite>. We find that this limitation exists in the more recent pretrained models as well, and it is especially hurtful in the OOD settings because the priors need not be the same across train and test sets, unlike in the IID settings. For instance, <span id="A3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px2.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.1.1.m1.1a"><msub id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1a" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.1.1.m1.1b"><apply id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1"><ci id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px2.p1.2.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> predicts “2” for a lot questions with target answer “1” in the <span id="A3.SS0.SSS0.Px2.p1.2.4" class="ltx_text ltx_font_smallcaps">VG</span> test set. Similarly, sometimes <span id="A3.SS0.SSS0.Px2.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px2.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.2.2.m1.1a"><msub id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1a" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.2.2.m1.1b"><apply id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1"><ci id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px2.p1.2.5" class="ltx_text ltx_font_smallcaps">VG</span> incorrectly predicts “helmet” for <span id="A3.SS0.SSS0.Px2.p1.2.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> test questions such as “What is the skateboarder wearing to protect his head?”, “What protective gear is he wearing?” when the skateboarder is not wearing anything. This indicates that the model is relying on answer priors rather than visual grounding. Our experimental results on VQA-CP (<a href="#S6" title="6 Evaluation on VQA-CP ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">6</span></a>) directly quantify the extent of such limitations in current models.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Overfitting to the question format.</h4>

<div id="A3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px3.p1.3" class="ltx_p">For each answer class, there is usually a limited variation in the format of questions in the fine-tuning set. For some of the answer classes showing poor OOD generalization, we found that certain question formats are quite dominant in the fine-tuning set, and that these dominant formats are different between the OOD fine-tuning and test sets.
Thus, we conjecture that models are likely overfitting to such dominant formats in fine-tuning data and hence fail to generalize at test time when the format changes.
For instance, questions about “chair” in the <span id="A3.SS0.SSS0.Px3.p1.3.4" class="ltx_text ltx_font_smallcaps">VQAv2</span> fine-tuning set are mostly of the form “What is … sitting on”? whereas in the <span id="A3.SS0.SSS0.Px3.p1.3.5" class="ltx_text ltx_font_smallcaps">GQA</span> test set, they are mostly of the form “What kind of furniture is …?”. Thus, the “chair” class accuracy of <span id="A3.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px3.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px3.p1.1.1.m1.1a"><msub id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1a" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px3.p1.1.1.m1.1b"><apply id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1"><ci id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px3.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px3.p1.3.6" class="ltx_text ltx_font_smallcaps">VQAv2</span> drops from 48% when tested on <span id="A3.SS0.SSS0.Px3.p1.3.7" class="ltx_text ltx_font_smallcaps">VQAv2</span> to 38% on the <span id="A3.SS0.SSS0.Px3.p1.3.8" class="ltx_text ltx_font_smallcaps">GQA</span> test set. As another example, <span id="A3.SS0.SSS0.Px3.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px3.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px3.p1.2.2.m1.1a"><msub id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1a" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px3.p1.2.2.m1.1b"><apply id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1"><ci id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px3.p1.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> trained on <span id="A3.SS0.SSS0.Px3.p1.3.9" class="ltx_text ltx_font_smallcaps">GQA</span> fails terribly for “dog” and “cat” classes on <span id="A3.SS0.SSS0.Px3.p1.3.10" class="ltx_text ltx_font_smallcaps">VG</span> test set (accuracy drops of 47% and 43% respectively, where drop is between <span id="A3.SS0.SSS0.Px3.p1.3.11" class="ltx_text ltx_font_smallcaps">GQA</span>–<span id="A3.SS0.SSS0.Px3.p1.3.12" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="A3.SS0.SSS0.Px3.p1.3.13" class="ltx_text ltx_font_smallcaps">GQA</span>–<span id="A3.SS0.SSS0.Px3.p1.3.14" class="ltx_text ltx_font_smallcaps">VG</span>). <span id="A3.SS0.SSS0.Px3.p1.3.15" class="ltx_text ltx_font_smallcaps">GQA</span> questions are mostly of the form “What animal …?” or “What kind of animal …?” whereas <span id="A3.SS0.SSS0.Px3.p1.3.16" class="ltx_text ltx_font_smallcaps">VG</span> questions often do not mention the word “animal” and are of the form “Who is …?” or “What is …?” (<em id="A3.SS0.SSS0.Px3.p1.3.17" class="ltx_emph ltx_font_italic">e.g.</em>, “Who is holding the Frisbee?”, “What is on the leash?”). Similarly, for the answer class “pizza”, <span id="A3.SS0.SSS0.Px3.p1.3.3" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px3.p1.3.3.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px3.p1.3.3.m1.1a"><msub id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1a" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px3.p1.3.3.m1.1b"><apply id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1"><ci id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p1.3.3.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px3.p1.3.3.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px3.p1.3.18" class="ltx_text ltx_font_smallcaps">VG</span> has mostly seen questions of the format “What food is this?”, “What is the man eating?”, “What is on the plate?”, “What’s in the box?” in <span id="A3.SS0.SSS0.Px3.p1.3.19" class="ltx_text ltx_font_smallcaps">VG</span> fine-tuning set. However, when evaluated on the <span id="A3.SS0.SSS0.Px3.p1.3.20" class="ltx_text ltx_font_smallcaps">VQAv2</span> test set, the model fails to respond correctly for questions about “pizza” such as, “What snack is this?” (model response: “pineapple”), “What recipe this will become?” (model response: “cheese”), “What’s in the bowl” (model response: “tomato sauce”). For the last example, perhaps the model is not expecting pizza to be in a bowl.</p>
</div>
<div id="A3.SS0.SSS0.Px3.p2" class="ltx_para">
<p id="A3.SS0.SSS0.Px3.p2.2" class="ltx_p">Related to above, we observed that sometimes <span id="A3.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px3.p2.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px3.p2.1.1.m1.1a"><msub id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1a" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px3.p2.1.1.m1.1b"><apply id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1"><ci id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p2.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px3.p2.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fails to produce the correct answer type for a given question. For instance, <span id="A3.SS0.SSS0.Px3.p2.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px3.p2.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px3.p2.2.2.m1.1a"><msub id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1a" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px3.p2.2.2.m1.1b"><apply id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1"><ci id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px3.p2.2.2.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px3.p2.2.2.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px3.p2.2.3" class="ltx_text ltx_font_smallcaps">VG</span> responds with “woman” to the question “Is the person who is cutting these carrots right handed or left handed?”. So it appears as if the <span id="A3.SS0.SSS0.Px3.p2.2.4" class="ltx_text ltx_font_smallcaps">VG</span> model does not understand the question structure in this example, <em id="A3.SS0.SSS0.Px3.p2.2.5" class="ltx_emph ltx_font_italic">i.e.</em> the response is expected to be either “right” or “left”. Similarly, for the question “Are there more blue or black shirts?”, <span id="A3.SS0.SSS0.Px3.p2.2.6" class="ltx_text ltx_font_smallcaps">VG</span> model responds with “rolled up”. Similarly, it answers “1 on right” to the question “What type of apple is shown?”, instead of describing some attribute of apple such as “green”.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stringent evaluation metric.</h4>

<div id="A3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px4.p1.1" class="ltx_p">We notice that sometimes the models’ responses are correct but they are evaluated as incorrect because those responses do not exist in the ground-truth answers. For instance, <span id="A3.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span>–<span id="A3.SS0.SSS0.Px4.p1.1.2" class="ltx_text ltx_font_smallcaps">VG</span> model gets penalized for answering “table” instead of “on table”<span id="footnote11" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Note that before computing the accuracy, both the predicted and the ground truth answers are pre-processed for answer normalization but such pre-processing is very basic. More details of the pre-processing can be found at <a target="_blank" href="https://visualqa.org/evaluation.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://visualqa.org/evaluation.html</a></span></span></span> (Q: “Where is …?”) or “sunny” instead of “clear” (Q: “How is the weather?”). More examples in <a href="#S7.F5" title="In 7 Qualitative Analysis ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">5</span></a>. This effect is expected to be more pronounced for the OOD evaluation than IID, because in IID a model can learn the format of the test answer (“on table” vs. “table”, “clear vs. sunny”) from the train set, whereas in OOD the format in the train set can be different from the test set. Also, such stringent evaluation (<em id="A3.SS0.SSS0.Px4.p1.1.3" class="ltx_emph ltx_font_italic">i.e.</em>, performing string matching with a small set of ground-truth answers) is expected to hurt generative models more than discriminative ones because they show more variations in the form of the answers as they are not limited by a fixed answer vocabulary (<em id="A3.SS0.SSS0.Px4.p1.1.4" class="ltx_emph ltx_font_italic">e.g.</em>., “pizza slices” instead of “pizza” (Q: “What are these?”), “pizzeria” instead of “pizza” (Q: “What kind of restaurant is this?”). We observed that, <span id="A3.SS0.SSS0.Px4.p1.1.5" class="ltx_text ltx_font_smallcaps">VG</span> model (model fine-tuned on <span id="A3.SS0.SSS0.Px4.p1.1.6" class="ltx_text ltx_font_smallcaps">VG</span>) evaluated on <span id="A3.SS0.SSS0.Px4.p1.1.7" class="ltx_text ltx_font_smallcaps">GQA</span> answers questions about “man” with “snowboarder”, “man on left” (i.e. more descriptive referring expressions) than just saying “man” but it does not get any credit because <span id="A3.SS0.SSS0.Px4.p1.1.8" class="ltx_text ltx_font_smallcaps">GQA</span> ground truth is “man”. To quantify the extent of this issue and measure its effect on discriminative vs. generative models, IID vs. OOD settings, we perform human evaluation of machine generated answers and provide additional insights in <a href="#S8" title="8 Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">8</span></a>.</p>
</div>
</section>
<section id="A3.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Poor performance of GQA model on color questions (both IID and OOD):</h4>

<div id="A3.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px5.p1.1" class="ltx_p"><span id="A3.SS0.SSS0.Px5.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A3.SS0.SSS0.Px5.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A3.SS0.SSS0.Px5.p1.1.1.m1.1a"><msub id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.cmml"><mi id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1a" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.cmml"></mi><mtext id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px5.p1.1.1.m1.1b"><apply id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1"><ci id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1a.cmml" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1.cmml" xref="A3.SS0.SSS0.Px5.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px5.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> fine-tuned on <span id="A3.SS0.SSS0.Px5.p1.1.2" class="ltx_text ltx_font_smallcaps">GQA</span> does not seem to be transferring well to color questions in the <span id="A3.SS0.SSS0.Px5.p1.1.3" class="ltx_text ltx_font_smallcaps">VQAv2</span> and <span id="A3.SS0.SSS0.Px5.p1.1.4" class="ltx_text ltx_font_smallcaps">VG</span> test sets (and even in IID <span id="A3.SS0.SSS0.Px5.p1.1.5" class="ltx_text ltx_font_smallcaps">GQA</span> test set). In <a href="#A3.T11" title="In Poor performance of GQA model on color questions (both IID and OOD): ‣ Appendix C Potential Causes of Poor OOD Generalization: A Qualitative Study ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">11</span></a>, we can see that the top-5 answer classes with highest drop in IID-to-OOD performance for <span id="A3.SS0.SSS0.Px5.p1.1.6" class="ltx_text ltx_font_smallcaps">GQA</span> model have quite a few colors. For instance, for the answer class “red” in the <span id="A3.SS0.SSS0.Px5.p1.1.7" class="ltx_text ltx_font_smallcaps">VG</span> test set, <span id="A3.SS0.SSS0.Px5.p1.1.8" class="ltx_text ltx_font_smallcaps">GQA</span> model fails to correctly answer simple questions (given the kind of questions <span id="A3.SS0.SSS0.Px5.p1.1.9" class="ltx_text ltx_font_smallcaps">GQA</span> model is fine-tuned on) such as “What is the primary color of the sign on the right?”, “What is the main color of the strawberry?”, “What color is the pull luggage of the woman?”, “What color are the pepperonis?”. It is not clear why <span id="A3.SS0.SSS0.Px5.p1.1.10" class="ltx_text ltx_font_smallcaps">GQA</span> model does not perform well on color questions.</p>
</div>
<figure id="A3.T11" class="ltx_table">
<div id="A3.T11.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:791.3pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-67.5pt,153.7pt) scale(0.719993316370266,0.719993316370266) ;">
<table id="A3.T11.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A3.T11.1.1.1.1" class="ltx_tr">
<th id="A3.T11.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A3.T11.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Train data</span></th>
<th id="A3.T11.1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A3.T11.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Test data</span></th>
<th id="A3.T11.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A3.T11.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="A3.T11.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="A3.T11.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Answer classes</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A3.T11.1.1.2.1" class="ltx_tr">
<td id="A3.T11.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="15"><span id="A3.T11.1.1.2.1.1.1" class="ltx_text">VQAv2</span></td>
<td id="A3.T11.1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.2.1.2.1" class="ltx_text">GQA</span></td>
<td id="A3.T11.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">no, yes, left, right, top</td>
</tr>
<tr id="A3.T11.1.1.3.2" class="ltx_tr">
<td id="A3.T11.1.1.3.2.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.3.2.2" class="ltx_td ltx_align_left">no, yes, right, bottom, color</td>
</tr>
<tr id="A3.T11.1.1.4.3" class="ltx_tr">
<td id="A3.T11.1.1.4.3.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.4.3.2" class="ltx_td ltx_align_left">no, yes, right, left, bottom</td>
</tr>
<tr id="A3.T11.1.1.5.4" class="ltx_tr">
<td id="A3.T11.1.1.5.4.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.5.4.2" class="ltx_td ltx_align_left">no, left, yes, bottom, chair</td>
</tr>
<tr id="A3.T11.1.1.6.5" class="ltx_tr">
<td id="A3.T11.1.1.6.5.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.6.5.2" class="ltx_td ltx_align_left">left, no, yes, bottom, top</td>
</tr>
<tr id="A3.T11.1.1.7.6" class="ltx_tr">
<td id="A3.T11.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.7.6.1.1" class="ltx_text">VG</span></td>
<td id="A3.T11.1.1.7.6.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.7.6.3" class="ltx_td ltx_align_left ltx_border_t">1, no 1, daytime, on table, in sky</td>
</tr>
<tr id="A3.T11.1.1.8.7" class="ltx_tr">
<td id="A3.T11.1.1.8.7.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.8.7.2" class="ltx_td ltx_align_left">daytime, 1, white, 2, black</td>
</tr>
<tr id="A3.T11.1.1.9.8" class="ltx_tr">
<td id="A3.T11.1.1.9.8.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.9.8.2" class="ltx_td ltx_align_left">daytime, white, 2, black, 1</td>
</tr>
<tr id="A3.T11.1.1.10.9" class="ltx_tr">
<td id="A3.T11.1.1.10.9.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.10.9.2" class="ltx_td ltx_align_left">1, daytime, in sky, on table, white</td>
</tr>
<tr id="A3.T11.1.1.11.10" class="ltx_tr">
<td id="A3.T11.1.1.11.10.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.11.10.2" class="ltx_td ltx_align_left">1, daytime, black, in sky, clear</td>
</tr>
<tr id="A3.T11.1.1.12.11" class="ltx_tr">
<td id="A3.T11.1.1.12.11.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.12.11.1.1" class="ltx_text">VizWiz</span></td>
<td id="A3.T11.1.1.12.11.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.12.11.3" class="ltx_td ltx_align_left ltx_border_t">no, blue, yes, white, black</td>
</tr>
<tr id="A3.T11.1.1.13.12" class="ltx_tr">
<td id="A3.T11.1.1.13.12.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.13.12.2" class="ltx_td ltx_align_left">yes, black, water bottle, corn, soup</td>
</tr>
<tr id="A3.T11.1.1.14.13" class="ltx_tr">
<td id="A3.T11.1.1.14.13.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.14.13.2" class="ltx_td ltx_align_left">pink, brown, corn, wine, keys</td>
</tr>
<tr id="A3.T11.1.1.15.14" class="ltx_tr">
<td id="A3.T11.1.1.15.14.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.15.14.2" class="ltx_td ltx_align_left">keyboard, no, soup, cake, samsung</td>
</tr>
<tr id="A3.T11.1.1.16.15" class="ltx_tr">
<td id="A3.T11.1.1.16.15.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.16.15.2" class="ltx_td ltx_align_left">soup, lotion, black, brown, corn</td>
</tr>
<tr id="A3.T11.1.1.17.16" class="ltx_tr">
<td id="A3.T11.1.1.17.16.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="15"><span id="A3.T11.1.1.17.16.1.1" class="ltx_text">GQA</span></td>
<td id="A3.T11.1.1.17.16.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.17.16.2.1" class="ltx_text">VQAv2</span></td>
<td id="A3.T11.1.1.17.16.3" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.17.16.4" class="ltx_td ltx_align_left ltx_border_t">yes, no, white, red, black</td>
</tr>
<tr id="A3.T11.1.1.18.17" class="ltx_tr">
<td id="A3.T11.1.1.18.17.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.18.17.2" class="ltx_td ltx_align_left">no, yes, white, red, tennis</td>
</tr>
<tr id="A3.T11.1.1.19.18" class="ltx_tr">
<td id="A3.T11.1.1.19.18.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.19.18.2" class="ltx_td ltx_align_left">no, yes, white, red, tennis</td>
</tr>
<tr id="A3.T11.1.1.20.19" class="ltx_tr">
<td id="A3.T11.1.1.20.19.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.20.19.2" class="ltx_td ltx_align_left">no, yes, white, red, right</td>
</tr>
<tr id="A3.T11.1.1.21.20" class="ltx_tr">
<td id="A3.T11.1.1.21.20.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.21.20.2" class="ltx_td ltx_align_left">no, yes, right, red, black and white</td>
</tr>
<tr id="A3.T11.1.1.22.21" class="ltx_tr">
<td id="A3.T11.1.1.22.21.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.22.21.1.1" class="ltx_text">VG</span></td>
<td id="A3.T11.1.1.22.21.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.22.21.3" class="ltx_td ltx_align_left ltx_border_t">white, trees, green, black, black and white</td>
</tr>
<tr id="A3.T11.1.1.23.22" class="ltx_tr">
<td id="A3.T11.1.1.23.22.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.23.22.2" class="ltx_td ltx_align_left">white, black, trees, green, blue</td>
</tr>
<tr id="A3.T11.1.1.24.23" class="ltx_tr">
<td id="A3.T11.1.1.24.23.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.24.23.2" class="ltx_td ltx_align_left">white, trees, black, green, brown</td>
</tr>
<tr id="A3.T11.1.1.25.24" class="ltx_tr">
<td id="A3.T11.1.1.25.24.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.25.24.2" class="ltx_td ltx_align_left">white, trees, black and white, grass, green</td>
</tr>
<tr id="A3.T11.1.1.26.25" class="ltx_tr">
<td id="A3.T11.1.1.26.25.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.26.25.2" class="ltx_td ltx_align_left">trees, green, black and white, black, grass</td>
</tr>
<tr id="A3.T11.1.1.27.26" class="ltx_tr">
<td id="A3.T11.1.1.27.26.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.27.26.1.1" class="ltx_text">VizWiz</span></td>
<td id="A3.T11.1.1.27.26.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.27.26.3" class="ltx_td ltx_align_left ltx_border_t">no, blue, yes, white, laptop</td>
</tr>
<tr id="A3.T11.1.1.28.27" class="ltx_tr">
<td id="A3.T11.1.1.28.27.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.28.27.2" class="ltx_td ltx_align_left">blue, white, black, dog, laptop</td>
</tr>
<tr id="A3.T11.1.1.29.28" class="ltx_tr">
<td id="A3.T11.1.1.29.28.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.29.28.2" class="ltx_td ltx_align_left">white, blue, laptop, black, dog</td>
</tr>
<tr id="A3.T11.1.1.30.29" class="ltx_tr">
<td id="A3.T11.1.1.30.29.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.30.29.2" class="ltx_td ltx_align_left">no, keyboard, soup, red, cake</td>
</tr>
<tr id="A3.T11.1.1.31.30" class="ltx_tr">
<td id="A3.T11.1.1.31.30.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.31.30.2" class="ltx_td ltx_align_left">no, dog, keyboard, laptop, blue</td>
</tr>
<tr id="A3.T11.1.1.32.31" class="ltx_tr">
<td id="A3.T11.1.1.32.31.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="15"><span id="A3.T11.1.1.32.31.1.1" class="ltx_text">VG</span></td>
<td id="A3.T11.1.1.32.31.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.32.31.2.1" class="ltx_text">VQAv2</span></td>
<td id="A3.T11.1.1.32.31.3" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.32.31.4" class="ltx_td ltx_align_left ltx_border_t">0, white, nothing, gray, red</td>
</tr>
<tr id="A3.T11.1.1.33.32" class="ltx_tr">
<td id="A3.T11.1.1.33.32.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.33.32.2" class="ltx_td ltx_align_left">0, 3, left, nothing, brown</td>
</tr>
<tr id="A3.T11.1.1.34.33" class="ltx_tr">
<td id="A3.T11.1.1.34.33.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.34.33.2" class="ltx_td ltx_align_left">0, 1, gray, left, wii</td>
</tr>
<tr id="A3.T11.1.1.35.34" class="ltx_tr">
<td id="A3.T11.1.1.35.34.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.35.34.2" class="ltx_td ltx_align_left">0, nothing, left, brown, 2</td>
</tr>
<tr id="A3.T11.1.1.36.35" class="ltx_tr">
<td id="A3.T11.1.1.36.35.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.36.35.2" class="ltx_td ltx_align_left">0, 3, nothing, right, gray</td>
</tr>
<tr id="A3.T11.1.1.37.36" class="ltx_tr">
<td id="A3.T11.1.1.37.36.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.37.36.1.1" class="ltx_text">GQA</span></td>
<td id="A3.T11.1.1.37.36.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.37.36.3" class="ltx_td ltx_align_left ltx_border_t">right, left, bottom, top, gray</td>
</tr>
<tr id="A3.T11.1.1.38.37" class="ltx_tr">
<td id="A3.T11.1.1.38.37.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.38.37.2" class="ltx_td ltx_align_left">bottom, left, top, color, large</td>
</tr>
<tr id="A3.T11.1.1.39.38" class="ltx_tr">
<td id="A3.T11.1.1.39.38.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.39.38.2" class="ltx_td ltx_align_left">left, bottom, color, top, gray</td>
</tr>
<tr id="A3.T11.1.1.40.39" class="ltx_tr">
<td id="A3.T11.1.1.40.39.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.40.39.2" class="ltx_td ltx_align_left">left, bottom, top, black, chair</td>
</tr>
<tr id="A3.T11.1.1.41.40" class="ltx_tr">
<td id="A3.T11.1.1.41.40.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.41.40.2" class="ltx_td ltx_align_left">left, bottom, color, top, gray</td>
</tr>
<tr id="A3.T11.1.1.42.41" class="ltx_tr">
<td id="A3.T11.1.1.42.41.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.42.41.1.1" class="ltx_text">VizWiz</span></td>
<td id="A3.T11.1.1.42.41.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.42.41.3" class="ltx_td ltx_align_left ltx_border_t">blue, black, grey, red, soup</td>
</tr>
<tr id="A3.T11.1.1.43.42" class="ltx_tr">
<td id="A3.T11.1.1.43.42.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.43.42.2" class="ltx_td ltx_align_left">grey, black, blue, white, computer screen</td>
</tr>
<tr id="A3.T11.1.1.44.43" class="ltx_tr">
<td id="A3.T11.1.1.44.43.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.44.43.2" class="ltx_td ltx_align_left">grey, blue, black, pink, computer screen</td>
</tr>
<tr id="A3.T11.1.1.45.44" class="ltx_tr">
<td id="A3.T11.1.1.45.44.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.45.44.2" class="ltx_td ltx_align_left">grey, soup, remote, cake, samsung</td>
</tr>
<tr id="A3.T11.1.1.46.45" class="ltx_tr">
<td id="A3.T11.1.1.46.45.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.46.45.2" class="ltx_td ltx_align_left">grey, blue, soup, wine, pink</td>
</tr>
<tr id="A3.T11.1.1.47.46" class="ltx_tr">
<td id="A3.T11.1.1.47.46.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="15"><span id="A3.T11.1.1.47.46.1.1" class="ltx_text">VizWiz</span></td>
<td id="A3.T11.1.1.47.46.2" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.47.46.2.1" class="ltx_text">VQAv2</span></td>
<td id="A3.T11.1.1.47.46.3" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.47.46.4" class="ltx_td ltx_align_left ltx_border_t">no, yes, 1, 2, white</td>
</tr>
<tr id="A3.T11.1.1.48.47" class="ltx_tr">
<td id="A3.T11.1.1.48.47.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.48.47.2" class="ltx_td ltx_align_left">no, 1, 2, 0, white</td>
</tr>
<tr id="A3.T11.1.1.49.48" class="ltx_tr">
<td id="A3.T11.1.1.49.48.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.49.48.2" class="ltx_td ltx_align_left">no, yes, 1, 2, white</td>
</tr>
<tr id="A3.T11.1.1.50.49" class="ltx_tr">
<td id="A3.T11.1.1.50.49.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.50.49.2" class="ltx_td ltx_align_left">no, 1, 2, yes, blue</td>
</tr>
<tr id="A3.T11.1.1.51.50" class="ltx_tr">
<td id="A3.T11.1.1.51.50.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.51.50.2" class="ltx_td ltx_align_left">yes, no, 1, 2, 0</td>
</tr>
<tr id="A3.T11.1.1.52.51" class="ltx_tr">
<td id="A3.T11.1.1.52.51.1" class="ltx_td ltx_align_left ltx_border_t" rowspan="5"><span id="A3.T11.1.1.52.51.1.1" class="ltx_text">GQA</span></td>
<td id="A3.T11.1.1.52.51.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.52.51.3" class="ltx_td ltx_align_left ltx_border_t">no, right, left, man, bottom</td>
</tr>
<tr id="A3.T11.1.1.53.52" class="ltx_tr">
<td id="A3.T11.1.1.53.52.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.53.52.2" class="ltx_td ltx_align_left">no, right, bottom, man, top</td>
</tr>
<tr id="A3.T11.1.1.54.53" class="ltx_tr">
<td id="A3.T11.1.1.54.53.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.54.53.2" class="ltx_td ltx_align_left">no, yes, left, right, man</td>
</tr>
<tr id="A3.T11.1.1.55.54" class="ltx_tr">
<td id="A3.T11.1.1.55.54.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.55.54.2" class="ltx_td ltx_align_left">no, left, bottom, top, man</td>
</tr>
<tr id="A3.T11.1.1.56.55" class="ltx_tr">
<td id="A3.T11.1.1.56.55.1" class="ltx_td ltx_align_left">Generative ALBEF</td>
<td id="A3.T11.1.1.56.55.2" class="ltx_td ltx_align_left">yes, left, no, bottom, top</td>
</tr>
<tr id="A3.T11.1.1.57.56" class="ltx_tr">
<td id="A3.T11.1.1.57.56.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" rowspan="5"><span id="A3.T11.1.1.57.56.1.1" class="ltx_text">VG</span></td>
<td id="A3.T11.1.1.57.56.2" class="ltx_td ltx_align_left ltx_border_t">Discriminative ViLBERT</td>
<td id="A3.T11.1.1.57.56.3" class="ltx_td ltx_align_left ltx_border_t">1, white, green, 2, black</td>
</tr>
<tr id="A3.T11.1.1.58.57" class="ltx_tr">
<td id="A3.T11.1.1.58.57.1" class="ltx_td ltx_align_left">Discriminative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.58.57.2" class="ltx_td ltx_align_left">1, 2, white, green, man</td>
</tr>
<tr id="A3.T11.1.1.59.58" class="ltx_tr">
<td id="A3.T11.1.1.59.58.1" class="ltx_td ltx_align_left">Generative ViLBERT (in-house)</td>
<td id="A3.T11.1.1.59.58.2" class="ltx_td ltx_align_left">1, 2, white, green, black</td>
</tr>
<tr id="A3.T11.1.1.60.59" class="ltx_tr">
<td id="A3.T11.1.1.60.59.1" class="ltx_td ltx_align_left">Discriminative ALBEF</td>
<td id="A3.T11.1.1.60.59.2" class="ltx_td ltx_align_left">1, green, white, 1, 2, blue</td>
</tr>
<tr id="A3.T11.1.1.61.60" class="ltx_tr">
<td id="A3.T11.1.1.61.60.1" class="ltx_td ltx_align_left ltx_border_bb">Generative ALBEF</td>
<td id="A3.T11.1.1.61.60.2" class="ltx_td ltx_align_left ltx_border_bb">1, 2, black, man, green</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Top-5 answer classes with highest performance drop from IID to OOD (for the same test set) for all OOD configurations. The answer classes are sorted by drop in weighted (wtd) accuracy, i.e. drop in absolute (abs) accuracy weighted by the # test questions for that answer class.</figcaption>
</figure>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Human Evaluation</h2>

<section id="A4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Method.</h4>

<div id="A4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p1.2" class="ltx_p">We used Amazon Mechanical Turk to collect human judgment about model responses on a random subset of 10K questions for each of the test sets—<span id="A4.SS0.SSS0.Px1.p1.2.3" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="A4.SS0.SSS0.Px1.p1.2.4" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="A4.SS0.SSS0.Px1.p1.2.5" class="ltx_text ltx_font_smallcaps">VG</span>. Since the size of <span id="A4.SS0.SSS0.Px1.p1.2.6" class="ltx_text ltx_font_smallcaps">VizWiz</span> test set is less than 10K, we collected human judgment on all the <span id="A4.SS0.SSS0.Px1.p1.2.7" class="ltx_text ltx_font_smallcaps">VizWiz</span> test questions. However, we dropped the questions that were tagged as “unanswerable” or “unsuitable” (more details are provided below under “Filtering VizWiz data”). The total number of <span id="A4.SS0.SSS0.Px1.p1.2.8" class="ltx_text ltx_font_smallcaps">VizWiz</span> test questions for which we collected human judgment is 1440 (per model). We performed human evaluation of the responses from the following models – <span id="A4.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.SS0.SSS0.Px1.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A4.SS0.SSS0.Px1.p1.1.1.m1.1a"><msub id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1a" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml"></mi><mtext id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px1.p1.1.1.m1.1b"><apply id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1"><ci id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1a.cmml" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px1.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> <span id="footnote12" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>For <span id="footnote12.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="footnote12.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="footnote12.1.m1.1b"><msub id="footnote12.1.m1.1.1" xref="footnote12.1.m1.1.1.cmml"><mi id="footnote12.1.m1.1.1b" xref="footnote12.1.m1.1.1.cmml"></mi><mtext id="footnote12.1.m1.1.1.1" xref="footnote12.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="footnote12.1.m1.1c"><apply id="footnote12.1.m1.1.1.cmml" xref="footnote12.1.m1.1.1"><ci id="footnote12.1.m1.1.1.1a.cmml" xref="footnote12.1.m1.1.1.1"><mtext mathsize="70%" id="footnote12.1.m1.1.1.1.cmml" xref="footnote12.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote12.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span>, we had initially collected human judgements for the version trained using the official codebase, and we did not collect annotations again for our re-implementation due to time constraints. Given our results above, we do not expect significant differences between the two versions.</span></span></span> and <span id="A4.SS0.SSS0.Px1.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.SS0.SSS0.Px1.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A4.SS0.SSS0.Px1.p1.2.2.m1.1a"><msub id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1a" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml"></mi><mtext id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px1.p1.2.2.m1.1b"><apply id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1"><ci id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1a.cmml" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px1.p1.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px1.p1.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> trained on the <span id="A4.SS0.SSS0.Px1.p1.2.9" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="A4.SS0.SSS0.Px1.p1.2.10" class="ltx_text ltx_font_smallcaps">GQA</span>, <span id="A4.SS0.SSS0.Px1.p1.2.11" class="ltx_text ltx_font_smallcaps">VG</span> datasets.
We did not collect human judgments for models <em id="A4.SS0.SSS0.Px1.p1.2.12" class="ltx_emph ltx_font_italic">fine-tuned</em> on <span id="A4.SS0.SSS0.Px1.p1.2.13" class="ltx_text ltx_font_smallcaps">VizWiz</span>, because a significant proportion of the responses from these models tend to be “unanswerable” or “unsuitable” (35% on <span id="A4.SS0.SSS0.Px1.p1.2.14" class="ltx_text ltx_font_smallcaps">VQAv2</span>, 39% on <span id="A4.SS0.SSS0.Px1.p1.2.15" class="ltx_text ltx_font_smallcaps">GQA</span>, 65% on <span id="A4.SS0.SSS0.Px1.p1.2.16" class="ltx_text ltx_font_smallcaps">VG</span>, and 64% on <span id="A4.SS0.SSS0.Px1.p1.2.17" class="ltx_text ltx_font_smallcaps">VizWiz</span>).
Collecting human feedback about such responses would not provide useful insights, because all questions in <span id="A4.SS0.SSS0.Px1.p1.2.18" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="A4.SS0.SSS0.Px1.p1.2.19" class="ltx_text ltx_font_smallcaps">GQA</span> and <span id="A4.SS0.SSS0.Px1.p1.2.20" class="ltx_text ltx_font_smallcaps">VG</span> should be answerable, therefore all cases of “unanswerable” should be incorrect. Such responses are just a side effect of a model’s priors caused by all the unanswerable training points in the <span id="A4.SS0.SSS0.Px1.p1.2.21" class="ltx_text ltx_font_smallcaps">VizWiz</span> fine-tuning set.</p>
</div>
<div id="A4.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p2.1" class="ltx_p">For each response, we asked 5 raters to evaluate the question, image, and a given model response, and indicate through a binary choice whether they considered the model response a correct answer to the question or not. To control the quality of the data, we filtered out low quality data using different heuristics such as distribution of yes/no answers for each worker, their mean submission times, average agreement with their fellow workers, or average alignment with the automatic accuracy.<span id="footnote13" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>How frequently a worker’s response (yes/no) aligns with the automatic accuracy computed (100.0/0.0) More specifically, we equate the worker’s <em id="footnote13.1" class="ltx_emph ltx_font_italic">yes</em> response with 100.0 and <em id="footnote13.2" class="ltx_emph ltx_font_italic">no</em> with 0.0 and look at the average difference between the worker’s response and the automatic accuracy</span></span></span>. In each of these cases, we looked at random samples from the outliers to qualitatively confirm our hypothesis. More details about the human evaluation interface are presented in the next paragraph.</p>
</div>
<div id="A4.SS0.SSS0.Px1.p3" class="ltx_para">
<p id="A4.SS0.SSS0.Px1.p3.1" class="ltx_p">To compute human accuracy of a model response (for a given question and image), we considered a response correct if at least 4 raters voted it is correct, and incorrect otherwise. We decided so in order to decrease noise introduced by cases where there was low agreement between raters.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data collection interface.</h4>

<div id="A4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px2.p1.1" class="ltx_p"><a href="#A4.F11" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a> shows a sample of the interface the MTurk raters used to submit their responses.
The workers were shown some examples, but in order not to bias them, we did not give them detailed guidance as to what should be considered correct for not - rather we asked them to rely on common sense, and consider an answer correct if it seems both factually accurate and natural in the context. See <a href="#A4.F11" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">11</span></a> for details.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Filtering VizWiz data.</h4>

<div id="A4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px3.p1.1" class="ltx_p">For human evaluation we filtered out all image-question pairs for which the ground truth answer indicates it is <em id="A4.SS0.SSS0.Px3.p1.1.1" class="ltx_emph ltx_font_italic">unanswerable</em>. That is, we have not collected human feedback for questions for which the ground truth answer appears in the following list:</p>
</div>
<div id="A4.SS0.SSS0.Px3.p2" class="ltx_para">
<ul id="A4.I1" class="ltx_itemize">
<li id="A4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i1.p1" class="ltx_para">
<p id="A4.I1.i1.p1.1" class="ltx_p">"unanswerable", "unsuitable"</p>
</div>
</li>
<li id="A4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i2.p1" class="ltx_para">
<p id="A4.I1.i2.p1.1" class="ltx_p">"insufficient image"</p>
</div>
</li>
<li id="A4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i3.p1" class="ltx_para">
<p id="A4.I1.i3.p1.1" class="ltx_p">"unknown", "unsure", "not clear"</p>
</div>
</li>
<li id="A4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i4.p1" class="ltx_para">
<p id="A4.I1.i4.p1.1" class="ltx_p">"blurry", "too blurry"</p>
</div>
</li>
<li id="A4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i5.p1" class="ltx_para">
<p id="A4.I1.i5.p1.1" class="ltx_p">"i don’t know", "don’t know", "i don no", "no idea"</p>
</div>
</li>
<li id="A4.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i6.p1" class="ltx_para">
<p id="A4.I1.i6.p1.1" class="ltx_p">"unusable image", "unsuitable image", "unstable image", "insufficient image quality", "unreadable"</p>
</div>
</li>
<li id="A4.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i7.p1" class="ltx_para">
<p id="A4.I1.i7.p1.1" class="ltx_p">"i can’t tell", "can’t tell", "can’t see"</p>
</div>
</li>
<li id="A4.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I1.i8.p1" class="ltx_para">
<p id="A4.I1.i8.p1.1" class="ltx_p">"0" <span id="footnote14" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>Qualitative examples have suggested that often this was used to indicate <em id="footnote14.1" class="ltx_emph ltx_font_italic">unanswerable</em>.</span></span></span></p>
</div>
</li>
</ul>
</div>
<div id="A4.SS0.SSS0.Px3.p3" class="ltx_para">
<p id="A4.SS0.SSS0.Px3.p3.1" class="ltx_p">In particular, this left us with 1440 questions for the <span id="A4.SS0.SSS0.Px3.p3.1.1" class="ltx_text ltx_font_smallcaps">VizWiz</span> dataset.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Results.</h4>

<div id="A4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px4.p1.2" class="ltx_p">We report the human accuracies for <span id="A4.SS0.SSS0.Px4.p1.1.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.SS0.SSS0.Px4.p1.1.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A4.SS0.SSS0.Px4.p1.1.1.m1.1a"><msub id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1a" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.cmml"></mi><mtext id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p1.1.1.m1.1b"><apply id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1"><ci id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1a.cmml" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1"><mtext mathsize="70%" id="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p1.1.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p1.1.1.m1.1c">{}_{\text{DISC}}</annotation></semantics></math></span> and <span id="A4.SS0.SSS0.Px4.p1.2.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.SS0.SSS0.Px4.p1.2.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A4.SS0.SSS0.Px4.p1.2.2.m1.1a"><msub id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.cmml"><mi id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1a" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.cmml"></mi><mtext id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px4.p1.2.2.m1.1b"><apply id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1"><ci id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1a.cmml" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1"><mtext mathsize="70%" id="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1.cmml" xref="A4.SS0.SSS0.Px4.p1.2.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px4.p1.2.2.m1.1c">{}_{\text{GEN}}</annotation></semantics></math></span> in <a href="#A4.F9" title="In Results. ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a> (bottom). We also report the accuracies obtained using automatic metrics (please see <a href="#S3.SS2" title="3.2 Datasets and Evaluation Metrics ‣ 3 Experimental Setup ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">3.2</span></a> for description of automatic metrics for each dataset) computed over the same random subset of test questions as that used for human evaluation in <a href="#A4.F9" title="In Results. ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">9</span></a> (top).
Please refer to <a href="#S8" title="8 Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">8</span></a> for discussion of results.</p>
</div>
<figure id="A4.F9" class="ltx_figure">
<div id="A4.F9.5" class="ltx_block">
<img src="/html/2205.12191/assets/figs/%22human_auto_score.pdf%22" id="A4.F9.g1" class="ltx_graphics ltx_centering" alt="Refer to caption"><img src="/html/2205.12191/assets/figs/%22human_score_4.pdf%22" id="A4.F9.g2" class="ltx_graphics ltx_centering" alt="Refer to caption">
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Human evaluation of <span id="A4.F9.3.1" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.F9.3.1.m1.1" class="ltx_Math" alttext="{}_{\text{DISC}}" display="inline"><semantics id="A4.F9.3.1.m1.1b"><msub id="A4.F9.3.1.m1.1.1" xref="A4.F9.3.1.m1.1.1.cmml"><mi id="A4.F9.3.1.m1.1.1b" xref="A4.F9.3.1.m1.1.1.cmml"></mi><mtext id="A4.F9.3.1.m1.1.1.1" xref="A4.F9.3.1.m1.1.1.1a.cmml">DISC</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.F9.3.1.m1.1c"><apply id="A4.F9.3.1.m1.1.1.cmml" xref="A4.F9.3.1.m1.1.1"><ci id="A4.F9.3.1.m1.1.1.1a.cmml" xref="A4.F9.3.1.m1.1.1.1"><mtext mathsize="70%" id="A4.F9.3.1.m1.1.1.1.cmml" xref="A4.F9.3.1.m1.1.1.1">DISC</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.F9.3.1.m1.1d">{}_{\text{DISC}}</annotation></semantics></math></span> (shaded bars) and <span id="A4.F9.4.2" class="ltx_text ltx_font_smallcaps">ViLBERT<math id="A4.F9.4.2.m1.1" class="ltx_Math" alttext="{}_{\text{GEN}}" display="inline"><semantics id="A4.F9.4.2.m1.1b"><msub id="A4.F9.4.2.m1.1.1" xref="A4.F9.4.2.m1.1.1.cmml"><mi id="A4.F9.4.2.m1.1.1b" xref="A4.F9.4.2.m1.1.1.cmml"></mi><mtext id="A4.F9.4.2.m1.1.1.1" xref="A4.F9.4.2.m1.1.1.1a.cmml">GEN</mtext></msub><annotation-xml encoding="MathML-Content" id="A4.F9.4.2.m1.1c"><apply id="A4.F9.4.2.m1.1.1.cmml" xref="A4.F9.4.2.m1.1.1"><ci id="A4.F9.4.2.m1.1.1.1a.cmml" xref="A4.F9.4.2.m1.1.1.1"><mtext mathsize="70%" id="A4.F9.4.2.m1.1.1.1.cmml" xref="A4.F9.4.2.m1.1.1.1">GEN</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.F9.4.2.m1.1d">{}_{\text{GEN}}</annotation></semantics></math></span> (plain bars) and comparison with automatic evaluation on a random subset (10K) of test questions for each test dataset – <span id="A4.F9.10.3" class="ltx_text ltx_font_smallcaps">GQA</span>, <span id="A4.F9.11.4" class="ltx_text ltx_font_smallcaps">VQAv2</span>, <span id="A4.F9.12.5" class="ltx_text ltx_font_smallcaps">VG</span> and <span id="A4.F9.13.6" class="ltx_text ltx_font_smallcaps">VizWiz</span>. Accuracies in bold denote the IID settings. Top: accuracies obtained using automatic evaluation. Bottom: accuracies obtained using human evaluation.</figcaption>
</figure>
</section>
<section id="A4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Qualitative examples of questions being incorrectly penalized by automatic evaluation</h4>

<div id="A4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px5.p1.1" class="ltx_p"><a href="#A4.T12" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">12</span></a> shows some examples for responses which were awarded <math id="A4.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="0.0" display="inline"><semantics id="A4.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="A4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">0.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px5.p1.1.m1.1b"><cn type="float" id="A4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px5.p1.1.m1.1.1">0.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px5.p1.1.m1.1c">0.0</annotation></semantics></math> accuracy using automatic metrics but were marked as correct by all 5 raters during human evaluation.</p>
</div>
</section>
<section id="A4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Discussion on VQA data quality</h4>

<div id="A4.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="A4.SS0.SSS0.Px6.p1.1" class="ltx_p">For the collected human judgement data, we find that for a significant number of questions (32%) there was low agreement between the 5 raters, i.e. either 3/5 answered <em id="A4.SS0.SSS0.Px6.p1.1.1" class="ltx_emph ltx_font_italic">correct</em> while the remaining 2/5 answered <em id="A4.SS0.SSS0.Px6.p1.1.2" class="ltx_emph ltx_font_italic">incorrect</em>, or vice-versa. Note that this is after we already filtered out low quality judgements. We have to recognize that, despite our best efforts to control data quality using our heuristics, there might still be low quality data in our dataset. Low quality annotations can be misleading and might distort the results of our analysis. Yet, we believe that we have collected a large enough sample to dampen the effect of these on the reported results.
Upon examining some examples from such low agreement questions, we find that many such cases highlight the quality of the VQA data. For instance, questions not being sufficiently objective but up for interpretation, questions phrased poorly that make it difficult to understand what the question is asking about, <em id="A4.SS0.SSS0.Px6.p1.1.3" class="ltx_emph ltx_font_italic">etc.</em>. We discuss these further below.</p>
</div>
<div id="A4.SS0.SSS0.Px6.p2" class="ltx_para">
<ul id="A4.I2" class="ltx_itemize">
<li id="A4.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I2.i1.p1" class="ltx_para">
<p id="A4.I2.i1.p1.1" class="ltx_p"><span id="A4.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Low agreement due to ambiguity.</span>
One reason why human raters might give different feedback stems from ambiguity and subjectivity around the question and the contents of the image. In these cases it is up to the raters subjective opinion to judge whether the answer is acceptable or not. Find some qualitative examples in <a href="#A4.T13" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">13</span></a>.</p>
</div>
</li>
<li id="A4.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A4.I2.i2.p1" class="ltx_para">
<p id="A4.I2.i2.p1.1" class="ltx_p"><span id="A4.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Low agreement due to poor quality question.</span>
Some questions in the original dataset are of rather poor quality which makes it near impossible for the rater to provide a valuable response. Find some qualitative examples of such questions in <a href="#A4.T14" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
</li>
</ul>
</div>
<div id="A4.SS0.SSS0.Px6.p3" class="ltx_para">
<p id="A4.SS0.SSS0.Px6.p3.7" class="ltx_p">Also surprisingly, from <a href="#S8.F6" title="In 8 Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we see that for models fine-tuned on <span id="A4.SS0.SSS0.Px6.p3.7.1" class="ltx_text ltx_font_smallcaps">VQAv2</span> or <span id="A4.SS0.SSS0.Px6.p3.7.2" class="ltx_text ltx_font_smallcaps">GQA</span> and tested on <span id="A4.SS0.SSS0.Px6.p3.7.3" class="ltx_text ltx_font_smallcaps">VQAv2</span>, and models fine-tuned on <span id="A4.SS0.SSS0.Px6.p3.7.4" class="ltx_text ltx_font_smallcaps">GQA</span> and tested on <span id="A4.SS0.SSS0.Px6.p3.7.5" class="ltx_text ltx_font_smallcaps">GQA</span>, human evaluation yields lower accuracy than automatic evaluation! This is not as expected. Upon examining some examples for responses with <math id="A4.SS0.SSS0.Px6.p3.1.m1.1" class="ltx_Math" alttext="100.0" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.1.m1.1a"><mn id="A4.SS0.SSS0.Px6.p3.1.m1.1.1" xref="A4.SS0.SSS0.Px6.p3.1.m1.1.1.cmml">100.0</mn><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.1.m1.1b"><cn type="float" id="A4.SS0.SSS0.Px6.p3.1.m1.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.1.m1.1.1">100.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.1.m1.1c">100.0</annotation></semantics></math> automatic accuracy but marked as <em id="A4.SS0.SSS0.Px6.p3.7.6" class="ltx_emph ltx_font_italic">incorrect</em> by at least 4 human raters, we again find some noise in the ground-truth answers. <a href="#A4.T15" title="In Discussion on VQA data quality ‣ Appendix D Human Evaluation ‣ Reassessing Evaluation Practices in Visual Question Answering: A Case Study on Out-of-Distribution Generalization" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Tab.</span> <span class="ltx_text ltx_ref_tag">15</span></a> shows some examples. Below we report the number of questions where at least 4 human raters voted incorrect even though the automatic metric indicated &gt;=90.0 accuracy. Generative case: {<span id="A4.SS0.SSS0.Px6.p3.7.7" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A4.SS0.SSS0.Px6.p3.2.m2.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.2.m2.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.2.m2.1.1" xref="A4.SS0.SSS0.Px6.p3.2.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.2.m2.1b"><ci id="A4.SS0.SSS0.Px6.p3.2.m2.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.2.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.2.m2.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.8" class="ltx_text ltx_font_smallcaps">GQA</span> (fine-tuned on <span id="A4.SS0.SSS0.Px6.p3.7.9" class="ltx_text ltx_font_smallcaps">GQA</span>, tested on <span id="A4.SS0.SSS0.Px6.p3.7.10" class="ltx_text ltx_font_smallcaps">GQA</span>): 86, <span id="A4.SS0.SSS0.Px6.p3.7.11" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A4.SS0.SSS0.Px6.p3.3.m3.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.3.m3.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.3.m3.1.1" xref="A4.SS0.SSS0.Px6.p3.3.m3.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.3.m3.1b"><ci id="A4.SS0.SSS0.Px6.p3.3.m3.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.3.m3.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.3.m3.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.12" class="ltx_text ltx_font_smallcaps">VQAv2</span>: 49, <span id="A4.SS0.SSS0.Px6.p3.7.13" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A4.SS0.SSS0.Px6.p3.4.m4.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.4.m4.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.4.m4.1.1" xref="A4.SS0.SSS0.Px6.p3.4.m4.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.4.m4.1b"><ci id="A4.SS0.SSS0.Px6.p3.4.m4.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.4.m4.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.4.m4.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.14" class="ltx_text ltx_font_smallcaps">VQAv2</span>: 48}, Discriminative case: {<span id="A4.SS0.SSS0.Px6.p3.7.15" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A4.SS0.SSS0.Px6.p3.5.m5.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.5.m5.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.5.m5.1.1" xref="A4.SS0.SSS0.Px6.p3.5.m5.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.5.m5.1b"><ci id="A4.SS0.SSS0.Px6.p3.5.m5.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.5.m5.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.5.m5.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.16" class="ltx_text ltx_font_smallcaps">GQA</span>: 128, <span id="A4.SS0.SSS0.Px6.p3.7.17" class="ltx_text ltx_font_smallcaps">GQA</span> <math id="A4.SS0.SSS0.Px6.p3.6.m6.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.6.m6.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.6.m6.1.1" xref="A4.SS0.SSS0.Px6.p3.6.m6.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.6.m6.1b"><ci id="A4.SS0.SSS0.Px6.p3.6.m6.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.6.m6.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.6.m6.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.18" class="ltx_text ltx_font_smallcaps">VQAv2</span>: 52, <span id="A4.SS0.SSS0.Px6.p3.7.19" class="ltx_text ltx_font_smallcaps">VQAv2</span> <math id="A4.SS0.SSS0.Px6.p3.7.m7.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="A4.SS0.SSS0.Px6.p3.7.m7.1a"><mo stretchy="false" id="A4.SS0.SSS0.Px6.p3.7.m7.1.1" xref="A4.SS0.SSS0.Px6.p3.7.m7.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px6.p3.7.m7.1b"><ci id="A4.SS0.SSS0.Px6.p3.7.m7.1.1.cmml" xref="A4.SS0.SSS0.Px6.p3.7.m7.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px6.p3.7.m7.1c">\rightarrow</annotation></semantics></math> <span id="A4.SS0.SSS0.Px6.p3.7.20" class="ltx_text ltx_font_smallcaps">VQAv2</span>: 76}.</p>
</div>
<figure id="A4.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.12191/assets/figs/%22mturk_interface_2.pdf%22" id="A4.F11.g1" class="ltx_graphics ltx_centering ltx_figure_panel" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Sample of the MTurk interface the raters used to annotate data.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2205.12191/assets/figs/%22mturk_interface.pdf%22" id="A4.F11.g2" class="ltx_graphics ltx_centering ltx_figure_panel" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Instructions given to MTurk raters.</figcaption>
</figure>
<figure id="A4.T12" class="ltx_table ltx_align_center">
<table id="A4.T12.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T12.3.3" class="ltx_tr">
<td id="A4.T12.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.1.1.1.1.1" class="ltx_p" style="width:78.0pt;"><img src="/html/2205.12191/assets/figs/2413078.jpeg" id="A4.T12.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="108" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T12.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.2.2.2.1.1" class="ltx_p" style="width:173.4pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000546983.jpeg" id="A4.T12.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="242" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T12.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.3.3.1.1" class="ltx_p" style="width:153.9pt;"><img src="/html/2205.12191/assets/figs/2413903.jpeg" id="A4.T12.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="215" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T12.3.4.1" class="ltx_tr">
<td id="A4.T12.3.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.4.1.1.1.1" class="ltx_p" style="width:78.0pt;">dataset: <span id="A4.T12.3.4.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">VG</span></span>
</span>
</td>
<td id="A4.T12.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.4.1.2.1.1" class="ltx_p" style="width:173.4pt;">dataset: <span id="A4.T12.3.4.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
<td id="A4.T12.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.4.1.3.1.1" class="ltx_p" style="width:153.9pt;">dataset: <span id="A4.T12.3.4.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></span>
</span>
</td>
</tr>
<tr id="A4.T12.3.5.2" class="ltx_tr">
<td id="A4.T12.3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.5.2.1.1.1" class="ltx_p" style="width:78.0pt;">img_id: 2413078</span>
</span>
</td>
<td id="A4.T12.3.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.5.2.2.1.1" class="ltx_p" style="width:173.4pt;">img_id: 546983</span>
</span>
</td>
<td id="A4.T12.3.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.5.2.3.1.1" class="ltx_p" style="width:153.9pt;">img_id: 2413903</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.6.3" class="ltx_tr">
<td id="A4.T12.3.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.6.3.1.1.1" class="ltx_p" style="width:78.0pt;">q_id: 151766</span>
</span>
</td>
<td id="A4.T12.3.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.6.3.2.1.1" class="ltx_p" style="width:173.4pt;">q_id: 546983002</span>
</span>
</td>
<td id="A4.T12.3.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.6.3.3.1.1" class="ltx_p" style="width:153.9pt;">q_id: 5199731</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.7.4" class="ltx_tr">
<td id="A4.T12.3.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.7.4.1.1.1" class="ltx_p" style="width:78.0pt;">Q: What are they wearing on their heads?</span>
</span>
</td>
<td id="A4.T12.3.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.7.4.2.1.1" class="ltx_p" style="width:173.4pt;">Q: What is flying in the
sky?</span>
</span>
</td>
<td id="A4.T12.3.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.7.4.3.1.1" class="ltx_p" style="width:153.9pt;">Q: Which kind of device is
on the table?</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.8.5" class="ltx_tr">
<td id="A4.T12.3.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.8.5.1.1.1" class="ltx_p" style="width:78.0pt;">A: helmet</span>
</span>
</td>
<td id="A4.T12.3.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.8.5.2.1.1" class="ltx_p" style="width:173.4pt;">A: kite</span>
</span>
</td>
<td id="A4.T12.3.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.8.5.3.1.1" class="ltx_p" style="width:153.9pt;">A: laptop</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.9.6" class="ltx_tr">
<td id="A4.T12.3.9.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.9.6.1.1.1" class="ltx_p" style="width:78.0pt;">GT: helmets</span>
</span>
</td>
<td id="A4.T12.3.9.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.9.6.2.1.1" class="ltx_p" style="width:173.4pt;">GT: kites</span>
</span>
</td>
<td id="A4.T12.3.9.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.9.6.3.1.1" class="ltx_p" style="width:153.9pt;">GT: cell phone</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.10.7" class="ltx_tr">
<td id="A4.T12.3.10.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.10.7.1.1.1" class="ltx_p" style="width:78.0pt;">accurarcy: 0.0</span>
</span>
</td>
<td id="A4.T12.3.10.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.10.7.2.1.1" class="ltx_p" style="width:173.4pt;">accurarcy: 0.0</span>
</span>
</td>
<td id="A4.T12.3.10.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.10.7.3.1.1" class="ltx_p" style="width:153.9pt;">accurarcy: 0.0</span>
</span>
</td>
</tr>
<tr id="A4.T12.3.11.8" class="ltx_tr">
<td id="A4.T12.3.11.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T12.3.11.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.11.8.1.1.1" class="ltx_p" style="width:78.0pt;">votes: 5 yes, 0 no</span>
</span>
</td>
<td id="A4.T12.3.11.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.11.8.2.1.1" class="ltx_p" style="width:173.4pt;">votes: 5 yes, 0 no</span>
</span>
</td>
<td id="A4.T12.3.11.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T12.3.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T12.3.11.8.3.1.1" class="ltx_p" style="width:153.9pt;">votes: 5 yes, 0 no</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 12: </span>A few examples of questions to which the model gave a response that was objectively correct, yet the automatic evaluation metric has marked these data points as 0% accurate. (<em id="A4.T12.8.1" class="ltx_emph ltx_font_italic">votes</em> here refers to how many raters selected <em id="A4.T12.9.2" class="ltx_emph ltx_font_italic">yes</em> (i.e. correct) or <em id="A4.T12.10.3" class="ltx_emph ltx_font_italic">no</em> (i.e. incorrect) when asked about this data point, while GT stands for <em id="A4.T12.11.4" class="ltx_emph ltx_font_italic">ground truth</em>.)</figcaption>
</figure>
<figure id="A4.T13" class="ltx_table ltx_align_center">
<table id="A4.T13.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T13.3.3" class="ltx_tr">
<td id="A4.T13.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.1.1.1.1.1" class="ltx_p" style="width:121.4pt;"><img src="/html/2205.12191/assets/figs/2358330.jpeg" id="A4.T13.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="165" height="180" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T13.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.2.2.2.1.1" class="ltx_p" style="width:86.7pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000254750.jpeg" id="A4.T13.2.2.2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="120" height="180" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T13.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.3.3.1.1" class="ltx_p" style="width:195.1pt;"><img src="/html/2205.12191/assets/figs/2338989.jpeg" id="A4.T13.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="268" height="180" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T13.3.4.1" class="ltx_tr">
<td id="A4.T13.3.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.4.1.1.1.1" class="ltx_p" style="width:121.4pt;">dataset: <span id="A4.T13.3.4.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">VG</span></span>
</span>
</td>
<td id="A4.T13.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.4.1.2.1.1" class="ltx_p" style="width:86.7pt;">dataset: <span id="A4.T13.3.4.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
<td id="A4.T13.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.4.1.3.1.1" class="ltx_p" style="width:195.1pt;">dataset: <span id="A4.T13.3.4.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></span>
</span>
</td>
</tr>
<tr id="A4.T13.3.5.2" class="ltx_tr">
<td id="A4.T13.3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.5.2.1.1.1" class="ltx_p" style="width:121.4pt;">img_id: 2358330</span>
</span>
</td>
<td id="A4.T13.3.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.5.2.2.1.1" class="ltx_p" style="width:86.7pt;">img_id: 254750</span>
</span>
</td>
<td id="A4.T13.3.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.5.2.3.1.1" class="ltx_p" style="width:195.1pt;">img_id: 2338989</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.6.3" class="ltx_tr">
<td id="A4.T13.3.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.6.3.1.1.1" class="ltx_p" style="width:121.4pt;">q_id: 700783</span>
</span>
</td>
<td id="A4.T13.3.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.6.3.2.1.1" class="ltx_p" style="width:86.7pt;">q_id: 254750003</span>
</span>
</td>
<td id="A4.T13.3.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.6.3.3.1.1" class="ltx_p" style="width:195.1pt;">q_id: 17319928</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.7.4" class="ltx_tr">
<td id="A4.T13.3.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.7.4.1.1.1" class="ltx_p" style="width:121.4pt;">Q: Where is he riding?</span>
</span>
</td>
<td id="A4.T13.3.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.7.4.2.1.1" class="ltx_p" style="width:86.7pt;">Q: Where is the toilet paper?</span>
</span>
</td>
<td id="A4.T13.3.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.7.4.3.1.1" class="ltx_p" style="width:195.1pt;">Q: What is on the green sign?</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.8.5" class="ltx_tr">
<td id="A4.T13.3.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.8.5.1.1.1" class="ltx_p" style="width:121.4pt;">A: park</span>
</span>
</td>
<td id="A4.T13.3.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.8.5.2.1.1" class="ltx_p" style="width:86.7pt;">A: bathroom</span>
</span>
</td>
<td id="A4.T13.3.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.8.5.3.1.1" class="ltx_p" style="width:195.1pt;">A: word</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.9.6" class="ltx_tr">
<td id="A4.T13.3.9.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.9.6.1.1.1" class="ltx_p" style="width:121.4pt;">GT: in street</span>
</span>
</td>
<td id="A4.T13.3.9.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.9.6.2.1.1" class="ltx_p" style="width:86.7pt;">GT: on sink</span>
</span>
</td>
<td id="A4.T13.3.9.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.9.6.3.1.1" class="ltx_p" style="width:195.1pt;">GT: flag</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.10.7" class="ltx_tr">
<td id="A4.T13.3.10.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.10.7.1.1.1" class="ltx_p" style="width:121.4pt;">accuracy: 0.0</span>
</span>
</td>
<td id="A4.T13.3.10.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.10.7.2.1.1" class="ltx_p" style="width:86.7pt;">accuracy: 0.0</span>
</span>
</td>
<td id="A4.T13.3.10.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.10.7.3.1.1" class="ltx_p" style="width:195.1pt;">accuracy: 0.0</span>
</span>
</td>
</tr>
<tr id="A4.T13.3.11.8" class="ltx_tr">
<td id="A4.T13.3.11.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T13.3.11.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.11.8.1.1.1" class="ltx_p" style="width:121.4pt;">votes: 3 yes, 2 no</span>
</span>
</td>
<td id="A4.T13.3.11.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.11.8.2.1.1" class="ltx_p" style="width:86.7pt;">votes: 3 yes, 2 no</span>
</span>
</td>
<td id="A4.T13.3.11.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T13.3.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T13.3.11.8.3.1.1" class="ltx_p" style="width:195.1pt;">votes: 3 yes, 2 no</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 13: </span>Low agreement due to ambiguity. In many cases, whether an answer is correct could be up to interpretation. (<em id="A4.T13.8.1" class="ltx_emph ltx_font_italic">votes</em> here refers to how many raters selected <em id="A4.T13.9.2" class="ltx_emph ltx_font_italic">yes</em> (i.e. correct) or <em id="A4.T13.10.3" class="ltx_emph ltx_font_italic">no</em> (i.e. incorrect) when asked about this data point, while GT stands for <em id="A4.T13.11.4" class="ltx_emph ltx_font_italic">ground truth</em>)</figcaption>
</figure>
<figure id="A4.T14" class="ltx_table ltx_align_center">
<table id="A4.T14.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T14.3.3" class="ltx_tr">
<td id="A4.T14.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.1.1.1.1.1" class="ltx_p" style="width:86.7pt;"><img src="/html/2205.12191/assets/figs/2396675.jpeg" id="A4.T14.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="122" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T14.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.2.2.2.1.1" class="ltx_p" style="width:164.8pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000503518.jpeg" id="A4.T14.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="227" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T14.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.3.3.1.1" class="ltx_p" style="width:156.1pt;"><img src="/html/2205.12191/assets/figs/2346071.jpeg" id="A4.T14.3.3.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="216" height="162" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T14.3.4.1" class="ltx_tr">
<td id="A4.T14.3.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.4.1.1.1.1" class="ltx_p" style="width:86.7pt;">dataset: <span id="A4.T14.3.4.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">VG</span></span>
</span>
</td>
<td id="A4.T14.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.4.1.2.1.1" class="ltx_p" style="width:164.8pt;">dataset: <span id="A4.T14.3.4.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
<td id="A4.T14.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.4.1.3.1.1" class="ltx_p" style="width:156.1pt;">dataset: <span id="A4.T14.3.4.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">GQA</span></span>
</span>
</td>
</tr>
<tr id="A4.T14.3.5.2" class="ltx_tr">
<td id="A4.T14.3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.5.2.1.1.1" class="ltx_p" style="width:86.7pt;">img_id: 2396675</span>
</span>
</td>
<td id="A4.T14.3.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.5.2.2.1.1" class="ltx_p" style="width:164.8pt;">img_id: 503518</span>
</span>
</td>
<td id="A4.T14.3.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.5.2.3.1.1" class="ltx_p" style="width:156.1pt;">img_id: 2346071</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.6.3" class="ltx_tr">
<td id="A4.T14.3.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.6.3.1.1.1" class="ltx_p" style="width:86.7pt;">q_id: 1453804</span>
</span>
</td>
<td id="A4.T14.3.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.6.3.2.1.1" class="ltx_p" style="width:164.8pt;">q_id: 503518006</span>
</span>
</td>
<td id="A4.T14.3.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.6.3.3.1.1" class="ltx_p" style="width:156.1pt;">q_id: 5863992</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.7.4" class="ltx_tr">
<td id="A4.T14.3.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.7.4.1.1.1" class="ltx_p" style="width:86.7pt;">Q: What is the kitchen dresser?</span>
</span>
</td>
<td id="A4.T14.3.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.7.4.2.1.1" class="ltx_p" style="width:164.8pt;">Q: What is happening?</span>
</span>
</td>
<td id="A4.T14.3.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.7.4.3.1.1" class="ltx_p" style="width:156.1pt;">Q: What kind of furniture is playing a game?</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.8.5" class="ltx_tr">
<td id="A4.T14.3.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.8.5.1.1.1" class="ltx_p" style="width:86.7pt;">A: cabinet</span>
</span>
</td>
<td id="A4.T14.3.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.8.5.2.1.1" class="ltx_p" style="width:164.8pt;">A: phone</span>
</span>
</td>
<td id="A4.T14.3.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.8.5.3.1.1" class="ltx_p" style="width:156.1pt;">A: table</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.9.6" class="ltx_tr">
<td id="A4.T14.3.9.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.9.6.1.1.1" class="ltx_p" style="width:86.7pt;">GT: brown</span>
</span>
</td>
<td id="A4.T14.3.9.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.9.6.2.1.1" class="ltx_p" style="width:164.8pt;">GT: watching videos, showing phone</span>
</span>
</td>
<td id="A4.T14.3.9.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.9.6.3.1.1" class="ltx_p" style="width:156.1pt;">GT: couch</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.10.7" class="ltx_tr">
<td id="A4.T14.3.10.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.10.7.1.1.1" class="ltx_p" style="width:86.7pt;">accuracy: 0.0</span>
</span>
</td>
<td id="A4.T14.3.10.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.10.7.2.1.1" class="ltx_p" style="width:164.8pt;">accuracy: 0.0</span>
</span>
</td>
<td id="A4.T14.3.10.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.10.7.3.1.1" class="ltx_p" style="width:156.1pt;">accuracy: 0.0</span>
</span>
</td>
</tr>
<tr id="A4.T14.3.11.8" class="ltx_tr">
<td id="A4.T14.3.11.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T14.3.11.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.11.8.1.1.1" class="ltx_p" style="width:86.7pt;">votes: 2 yes, 3 no</span>
</span>
</td>
<td id="A4.T14.3.11.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.11.8.2.1.1" class="ltx_p" style="width:164.8pt;">votes: 2 yes, 3 no</span>
</span>
</td>
<td id="A4.T14.3.11.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T14.3.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T14.3.11.8.3.1.1" class="ltx_p" style="width:156.1pt;">votes: 2 yes, 3 no</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 14: </span>Low agreement due poor question quality. Some questions have poor phrasing that make it difficult to understand what exactly is being asked. In these cases even the humans are not sure what to answer. (<em id="A4.T14.8.1" class="ltx_emph ltx_font_italic">votes</em> here refers to how many raters selected <em id="A4.T14.9.2" class="ltx_emph ltx_font_italic">yes</em> (i.e. correct) or <em id="A4.T14.10.3" class="ltx_emph ltx_font_italic">no</em> (i.e. incorrect) when asked about this data point, while GT stands for <em id="A4.T14.11.4" class="ltx_emph ltx_font_italic">ground truth</em>)</figcaption>
</figure>
<figure id="A4.T15" class="ltx_table ltx_align_center">
<table id="A4.T15.3" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A4.T15.3.3" class="ltx_tr">
<td id="A4.T15.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.1.1.1.1.1" class="ltx_p" style="width:93.2pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000367228.jpeg" id="A4.T15.1.1.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="128" height="192" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T15.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.2.2.2.1.1" class="ltx_p" style="width:210.3pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000197745.jpeg" id="A4.T15.2.2.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="289" height="192" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="A4.T15.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.3.3.1.1" class="ltx_p" style="width:104.1pt;"><img src="/html/2205.12191/assets/figs/COCO_val2014_000000264737.jpeg" id="A4.T15.3.3.3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="144" height="192" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="A4.T15.3.4.1" class="ltx_tr">
<td id="A4.T15.3.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.4.1.1.1.1" class="ltx_p" style="width:93.2pt;">dataset: <span id="A4.T15.3.4.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
<td id="A4.T15.3.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.4.1.2.1.1" class="ltx_p" style="width:210.3pt;">dataset: <span id="A4.T15.3.4.1.2.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
<td id="A4.T15.3.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.4.1.3.1.1" class="ltx_p" style="width:104.1pt;">dataset: <span id="A4.T15.3.4.1.3.1.1.1" class="ltx_text ltx_font_smallcaps">VQAv2</span></span>
</span>
</td>
</tr>
<tr id="A4.T15.3.5.2" class="ltx_tr">
<td id="A4.T15.3.5.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.5.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.5.2.1.1.1" class="ltx_p" style="width:93.2pt;">img_id: 367228</span>
</span>
</td>
<td id="A4.T15.3.5.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.5.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.5.2.2.1.1" class="ltx_p" style="width:210.3pt;">img_id: 197745</span>
</span>
</td>
<td id="A4.T15.3.5.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.5.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.5.2.3.1.1" class="ltx_p" style="width:104.1pt;">img_id: 264737</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.6.3" class="ltx_tr">
<td id="A4.T15.3.6.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.6.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.6.3.1.1.1" class="ltx_p" style="width:93.2pt;">q_id: 367228001</span>
</span>
</td>
<td id="A4.T15.3.6.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.6.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.6.3.2.1.1" class="ltx_p" style="width:210.3pt;">q_id: 197745007</span>
</span>
</td>
<td id="A4.T15.3.6.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.6.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.6.3.3.1.1" class="ltx_p" style="width:104.1pt;">q_id: 264737002</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.7.4" class="ltx_tr">
<td id="A4.T15.3.7.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.7.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.7.4.1.1.1" class="ltx_p" style="width:93.2pt;">Q: Is the kite flying high enough?</span>
</span>
</td>
<td id="A4.T15.3.7.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.7.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.7.4.2.1.1" class="ltx_p" style="width:210.3pt;">Q: How many spots are on this animal?</span>
</span>
</td>
<td id="A4.T15.3.7.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.7.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.7.4.3.1.1" class="ltx_p" style="width:104.1pt;">Q: How many animals are in the picture?</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.8.5" class="ltx_tr">
<td id="A4.T15.3.8.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.8.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.8.5.1.1.1" class="ltx_p" style="width:93.2pt;">A: yes</span>
</span>
</td>
<td id="A4.T15.3.8.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.8.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.8.5.2.1.1" class="ltx_p" style="width:210.3pt;">A: 100</span>
</span>
</td>
<td id="A4.T15.3.8.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.8.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.8.5.3.1.1" class="ltx_p" style="width:104.1pt;">A: 6</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.9.6" class="ltx_tr">
<td id="A4.T15.3.9.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.9.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.9.6.1.1.1" class="ltx_p" style="width:93.2pt;">GT: [no, yes, yes, no, no, no, yes, no, no, yes]</span>
</span>
</td>
<td id="A4.T15.3.9.6.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.9.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.9.6.2.1.1" class="ltx_p" style="width:210.3pt;">GT: [70, 100, 100, numerous, 200, 100, 100, 100, 20, lots]</span>
</span>
</td>
<td id="A4.T15.3.9.6.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.9.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.9.6.3.1.1" class="ltx_p" style="width:104.1pt;">GT: [7, 6, 6, 9, 6, 6, 6, 7, 7, 6]</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.10.7" class="ltx_tr">
<td id="A4.T15.3.10.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.10.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.10.7.1.1.1" class="ltx_p" style="width:93.2pt;">accuracy: 100.0</span>
</span>
</td>
<td id="A4.T15.3.10.7.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.10.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.10.7.2.1.1" class="ltx_p" style="width:210.3pt;">accuracy: 100.0</span>
</span>
</td>
<td id="A4.T15.3.10.7.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.10.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.10.7.3.1.1" class="ltx_p" style="width:104.1pt;">accuracy: 100.0</span>
</span>
</td>
</tr>
<tr id="A4.T15.3.11.8" class="ltx_tr">
<td id="A4.T15.3.11.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="A4.T15.3.11.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.11.8.1.1.1" class="ltx_p" style="width:93.2pt;">votes: 1 yes, 4 no</span>
</span>
</td>
<td id="A4.T15.3.11.8.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.11.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.11.8.2.1.1" class="ltx_p" style="width:210.3pt;">votes: 1 yes, 4 no</span>
</span>
</td>
<td id="A4.T15.3.11.8.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="A4.T15.3.11.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A4.T15.3.11.8.3.1.1" class="ltx_p" style="width:104.1pt;">votes: 1 yes, 4 no</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 15: </span>Examples of the few cases where humans considered the response incorrect despite 100.0 automatic accuracy. (<em id="A4.T15.8.1" class="ltx_emph ltx_font_italic">votes</em> here refers to how many raters selected <em id="A4.T15.9.2" class="ltx_emph ltx_font_italic">yes</em> (i.e. correct) or <em id="A4.T15.10.3" class="ltx_emph ltx_font_italic">no</em> (i.e. incorrect) when asked about this data point, while GT stands for <em id="A4.T15.11.4" class="ltx_emph ltx_font_italic">ground truth</em>)</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.12190" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.12191" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.12191">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.12191" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.12192" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 13:41:55 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
