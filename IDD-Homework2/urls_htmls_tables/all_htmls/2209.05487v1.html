<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2209.05487] Understanding Time Variations of DNN Inference in Autonomous Driving</title><meta property="og:description" content="Deep neural networks (DNNs) are widely used in autonomous driving due to their high accuracy for perception, decision, and control. In safety-critical systems like autonomous driving, executing tasks like sensing and p‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding Time Variations of DNN Inference in Autonomous Driving">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Understanding Time Variations of DNN Inference in Autonomous Driving">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2209.05487">

<!--Generated on Wed Mar 13 21:38:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
time variations,  deep neural networks,  autonomous driving
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Understanding Time Variations of DNN Inference in Autonomous Driving</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">

Liangkai¬†Liu1,
Yanzhi¬†Wang2, and
Weisong¬†Shi3
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
1Department of Computer Science, Wayne State University

<br class="ltx_break">2Department of Electrical &amp; Computer Engineering, Northeastern University

<br class="ltx_break">3Department of Computer and Information Sciences, University of Delaware
<br class="ltx_break">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Deep neural networks (DNNs) are widely used in autonomous driving due to their high accuracy for perception, decision, and control. In safety-critical systems like autonomous driving, executing tasks like sensing and perception in real-time is vital to the vehicle‚Äôs safety, which requires the application‚Äôs execution time to be predictable. However, non-negligible time variations are observed in DNN inference. Current DNN inference studies either ignore the time variation issue or rely on the scheduler to handle it. None of the current work explains the root causes of DNN inference time variations. Understanding the time variations of the DNN inference becomes a fundamental challenge in real-time scheduling for autonomous driving. <span id="id1.id1.1" class="ltx_text" style="color:#000000;">In this work, we analyze the time variation in DNN inference in fine granularity from six perspectives: data, I/O, model, runtime, hardware, and end-to-end perception system. Six insights are derived in understanding the time variations for DNN inference.</span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
time variations, deep neural networks, autonomous driving

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Owing to its high safety and efficiency, autonomous driving has become the fundamental technology for the next generation of transportation. Deep Neural Networks (DNNs) are widely deployed in the autonomous driving system for sensing, perception, decision, and control. Typical examples include: YOLOv3 and Faster R-CNN for object detection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>; Deeplabv3 for image semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>; LaneNet and PINet for lane detection¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. There are two main reasons for the success of DNNs in autonomous driving systems. The first is the higher accuracy compared with traditional computer vision-based approaches¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. The other is that DNNs can process raw data, making it suitable for autonomous driving vehicles since it generates terabytes of raw sensor data daily¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">As a safety-critical system, autonomous driving sets high requirements in accuracy, real-time, robustness, etc. The high accuracy of DNN-based algorithms promotes the development of autonomous vehicles. However, satisfying the real-time requirements of the sensing, perception, and planning tasks are still significant challenges. According to¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, when the vehicle drives at 40 km per hour in urban areas, these autonomous functions should be effective every 1 m with task execution time less than 100ms. As DNN models are widely used in object detection/classification, lane tracking, and decision-making applications, guaranteeing the real-time execution of the DNN inference becomes the key to satisfying the real-time requirements of autonomous driving.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Generally, for safety-critical applications like sensing, perception, control, etc., deadline-based scheduling is used by the real-time scheduler to guarantee safety. Setting up deadlines is usually based on the worst observed execution time. However, although the model structure and weights are fixed, non-negligible time variations still exist in DNN inference¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>. Prior works¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> observed the time variations for DNN inference in mobile devices and found that inference time follows an approximately Gaussian distribution. However, the statistic-based approach performs poorly when time variations are enormous. Another work¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> on the anytime DNN system also observed the time variations issue and provided a Kalman Filter-based estimation for latency distribution. <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="D^{3}" display="inline"><semantics id="S1.p3.1.m1.1a"><msup id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">D</mi><mn id="S1.p3.1.m1.1.1.3" xref="S1.p3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1">superscript</csymbol><ci id="S1.p3.1.m1.1.1.2.cmml" xref="S1.p3.1.m1.1.1.2">ùê∑</ci><cn type="integer" id="S1.p3.1.m1.1.1.3.cmml" xref="S1.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">D^{3}</annotation></semantics></math> is a work that addresses the time variations in AV systems with dynamic deadlines rather than static deadlines¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. However, none of the existing approaches could handle huge time variations since they do not consider the roots causing DNN inference time variations. Time variations bring a big challenge for real-time schedulers because the deadline with the worst observed execution time could waste many processor resources. Many resources would be saved if the DNN inference time could be fixed or narrowed down to a specific range. A detailed and in-depth analysis of DNN inference time variation is missing. Therefore, <span id="S1.p3.1.1" class="ltx_text ltx_font_italic">understanding the DNN inference time variation is key to optimizing the DNN inference runtime for autonomous driving</span>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this paper, we undertake a comprehensive analysis of DNN inference time variations in a general autonomous driving system. We analyze the time variation issues in fine granularity. For typical DNN models, we consider the variability in DNN inference from six perspectives: <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">data, I/O, model, runtime,</span> <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">hardware</span>, and end-to-end perception system. Six insights are derived for reducing DNN inference time variations.
In summary, this paper makes the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">The time variation issues of DNN inference in autonomous driving are thoroughly studied. We found that the majority of DNN models show variations larger than 100ms, which significantly affects autonomous driving safety.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Through a comprehensive analysis of the time variation of DNN model inference from six perspectives, we derive six insights into the relationship between DNN inference time variations and variability of data, I/O, model, runtime, hardware, and end-to-end perception system.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">The rest of the paper is organized as follows. Section¬†<a href="#S2" title="II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> presents the background and motivation of this work. Section¬†<a href="#S3" title="III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> discusses the time variation analysis for typical DNN models. Section¬†<a href="#S4" title="IV End-to-End System for Autonomous Driving ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> present the implementation of the end-to-end perception system. Section¬†<a href="#S5" title="V System-level Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> dicussed the time variation analysis for the end-to-end perception system.
Section¬†<a href="#S6" title="VI Related Work ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a> describes the related work. Section¬†<a href="#S7" title="VII Conclusion ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a> concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background and Motivation</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The recent proliferation of computing technologies, e.g., sensors, computer vision, machine learning, hardware acceleration, and the broad deployment of communication mechanisms, e.g., DSRC, C-V2X, 5G, have pushed the horizon of autonomous driving, which automates the decision and control of vehicles by leveraging the perception results based on multiple sensors. The key to the success of these autonomous systems is making reliable decisions in a real-time fashion. However, since the sensing data and runtime of the computing devices vary, the end-to-end latency for the perception, planning, and vehicle controls show non-negligible variation. This time variation affects the system predictability, making the operating system‚Äôs scheduling less efficient.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In this section, we present the motivation for this work by answering three questions:</p>
</div>
<div id="S2.p3" class="ltx_para">
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">How prevalent are DNNs for autonomous driving?</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">How vital is timing variation for autonomous driving, and what is the state-of-the-art?</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">What are the potential issues that affected the time variations in DNN inference?</p>
</div>
</li>
</ul>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">DNN Inference for Autonomous Driving</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">DNN models have been widely used in the autonomous driving system for sensing, perception, and localization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ II-A DNN Inference for Autonomous Driving ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an overview of the state-of-the-art autonomous driving systems. The design purely relies on cameras for sensing the environment. As shown in the figure, eight primary components are divided into three parts: sensing, perception, and decision¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. Sensing is the process that sensors capture information from the environment. Perception represents understanding the environment with algorithms applied to the sensing data, including localization, detection, semantic segmentation, and sensor fusion. The role of sensor fusion is to collaborate among all the perception components and generate locations for objects, lanes, and open spaces for the planning module. The decision is composed of global planning, local planning, and vehicle control. Global planning generates the routes between origin and destination, while local planning generates the trajectory and control commands on brake, throttle, and steering. Finally, the control commands are sent to the drive-by-wire system and applied to the vehicle.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We can find that the core modules on perception and decision are all based on neural networks. The main reason is that DNN has an excellent performance in many applications. Take image classification as an example. The ImageNet challenge winner in 2017 decreased the classification error to 0.023, while human beings‚Äô error is 0.05¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. DNNs can also learn features from raw, heterogeneous, and noisy data, suitable for autonomous driving scenarios since terabytes of raw sensor data are generated on the vehicle¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2209.05487/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="368" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A typical autonomous driving system.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Time Variations in DNN Inference</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The proliferation of deep learning achieves enormous performance improvement and brings lots of issues in computation complexity, energy consumption, and time variation for safety-critical systems¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. For autonomous driving vehicles, time variation affects predictability, which guarantees the vehicle‚Äôs safety and improves resource utilization¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">However, current DNN-based computing systems show poor performance in terms of time variation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. To illustrate the time variation issue in a state-of-the-art DNN-based autonomous driving system, we choose eleven models/algorithms covering the whole pipeline of the autonomous driving system and measure the end-to-end latency with the same input. There are seven DNN models for perception: YOLOv3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, Faster R-CNN ResNet101¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>, Mask R-CNN Inceptionv2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and SSD MobileNetv2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> are for object detection; PINet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and LaneNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> for lane detection; Deeplabv3 with MobileNetv2 for semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Localization and planning algorithms are tested by running ROS Navigation offline with recorded sensor data¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Adaptive Monte Carlo localization (AMCL)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and ORB-SLAM2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> are deployed for localization. A<sup id="S2.SS2.p2.1.1" class="ltx_sup">‚àó</sup>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> and Dynamic Window Approach (DWA)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> are deployed for global and local path planning, respectively. Table¬†<a href="#S2.T1" title="TABLE I ‚Ä£ II-B Time Variations in DNN Inference ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the mean, the range, and the division of the range and the mean of the end-to-end latency for eleven models/algorithms. The range is defined as the difference between the maximum value and the minimum value. From Table¬†<a href="#S2.T1" title="TABLE I ‚Ä£ II-B Time Variations in DNN Inference ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, we can observe that majority of the time is consumed by the DNN-based perception tasks: object detection, lane detection, and segmentation. Among all the seven DNN models, four models have a range larger than 100ms. LaneNet shows the biggest range with 282ms. If we consider the percentage of the range over the mean, the variations of the last three models for lane detection, localization, and planning are larger than 100 percent. AMCL shows the poorest performance with 675.5 percent. However, localization and planning tasks only occupy a limited portion of the end-to-end autonomous driving pipeline.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The mean, range, and variations of the eleven models used in the autonomous driving system.</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:163.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-70.7pt,26.5pt) scale(0.754197724417237,0.754197724417237) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Task</span></td>
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Model</span></td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Mean (ms)</span></td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Range (ms)</span></td>
<td id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="S2.T1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Range / Mean (%)</span></td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S2.T1.1.1.2.2.1.1" class="ltx_text">Object Detection</span></td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">YOLOv3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>
</td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">173</span></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">57</td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">32.8</td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_center">Faster R-CNN Resnet101¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>
</td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.3.3.2.1" class="ltx_text ltx_font_bold">413</span></td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.3.3.3.1" class="ltx_text ltx_font_bold">128</span></td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_center">31</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_center">Mask R-CNN Inceptionv2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.4.4.2.1" class="ltx_text ltx_font_bold">266</span></td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.4.4.3.1" class="ltx_text ltx_font_bold">104</span></td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_center">39.1</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_center">SSD MobileNetv2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.5.5.2.1" class="ltx_text ltx_font_bold">144</span></td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_center">70</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_center">48.6</td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S2.T1.1.1.6.6.1.1" class="ltx_text">Lane Detection</span></td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">LaneNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>
</td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">82</td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.6.6.4.1" class="ltx_text ltx_font_bold">282</span></td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.6.6.5.1" class="ltx_text ltx_font_bold">344</span></td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_center">PINet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.7.7.2.1" class="ltx_text ltx_font_bold">127</span></td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.7.7.3.1" class="ltx_text ltx_font_bold">263</span></td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.7.7.4.1" class="ltx_text ltx_font_bold">207.1</span></td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_align_center ltx_border_t">Segmentation</td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_t">Deeplabv3¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.8.8.3.1" class="ltx_text ltx_font_bold">149</span></td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_t">19</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_t">12.8</td>
</tr>
<tr id="S2.T1.1.1.9.9" class="ltx_tr">
<td id="S2.T1.1.1.9.9.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S2.T1.1.1.9.9.1.1" class="ltx_text">Localization</span></td>
<td id="S2.T1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">AMCL¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>
</td>
<td id="S2.T1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">1.3</td>
<td id="S2.T1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">8.7</td>
<td id="S2.T1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.9.9.5.1" class="ltx_text ltx_font_bold">675.5</span></td>
</tr>
<tr id="S2.T1.1.1.10.10" class="ltx_tr">
<td id="S2.T1.1.1.10.10.1" class="ltx_td ltx_align_center">ORB-SLAM2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>
</td>
<td id="S2.T1.1.1.10.10.2" class="ltx_td ltx_align_center">53</td>
<td id="S2.T1.1.1.10.10.3" class="ltx_td ltx_align_center">56</td>
<td id="S2.T1.1.1.10.10.4" class="ltx_td ltx_align_center"><span id="S2.T1.1.1.10.10.4.1" class="ltx_text ltx_font_bold">105.6</span></td>
</tr>
<tr id="S2.T1.1.1.11.11" class="ltx_tr">
<td id="S2.T1.1.1.11.11.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="2"><span id="S2.T1.1.1.11.11.1.1" class="ltx_text">Planning</span></td>
<td id="S2.T1.1.1.11.11.2" class="ltx_td ltx_align_center ltx_border_t">A*¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>
</td>
<td id="S2.T1.1.1.11.11.3" class="ltx_td ltx_align_center ltx_border_t">79</td>
<td id="S2.T1.1.1.11.11.4" class="ltx_td ltx_align_center ltx_border_t">97</td>
<td id="S2.T1.1.1.11.11.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S2.T1.1.1.11.11.5.1" class="ltx_text ltx_font_bold">122.3</span></td>
</tr>
<tr id="S2.T1.1.1.12.12" class="ltx_tr">
<td id="S2.T1.1.1.12.12.1" class="ltx_td ltx_align_center ltx_border_bb">DWA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S2.T1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_bb">23</td>
<td id="S2.T1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_bb">73</td>
<td id="S2.T1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.1.1.12.12.4.1" class="ltx_text ltx_font_bold">323.8</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2209.05487/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="170" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The box plot of end-to-end latency for typical models/algorithms in autonomous driving pipeline.</figcaption>
</figure>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">How about the distributions of the end-to-end latency? Figure¬†<a href="#S2.F2" title="Figure 2 ‚Ä£ II-B Time Variations in DNN Inference ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the box plot of the end-to-end inference time of the above eleven models/algorithms. We can observe that perception tasks have a much wider latency distribution than localization and planning tasks. Non-negligible amount of latencies that lie above the 75th percentile. Besides, there are many outliers (abnormal data) among all the perception tasks. Although these outliers have a low probability, they affect the scheduler‚Äôs performance, especially for the safety-critical system like autonomous driving. If we consider the autonomous driving system a hard real-time system, the scheduler assigns deadlines for each task based on the worst-observed execution time. Take LaneNet as an example. The scheduler would set the deadline larger than the worst observed execution time, which is 340ms¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>. Although it guarantees the safety of this task, it also brings enormous inefficiency because the actual execution time is less than 160ms over 95 percent of the time, which means around 180ms is wasted for most jobs. If we narrow the execution time range to less than 50ms, we can save almost 110ms in LaneNet for every job.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Time variation is expected to make a significant impact on safety-critical systems. Since perception tasks consume most of the time, this work‚Äôs focus would be on the DNN models in perception of the autonomous driving system. We propose using profiling tools to understand DNN inference time variations in current autonomous driving systems and model their time variation.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.4.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.5.2" class="ltx_text ltx_font_italic">Uncertainties in DNN Inference</span>
</h3>

<figure id="S2.F3" class="ltx_figure"><img src="/html/2209.05487/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="194" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The timeline of DNN inference.</figcaption>
</figure>
<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">What are the potential issues that affect the time variations in DNN inference? To answer this question, we first analyze the timeline of DNN inference. Figure¬†<a href="#S2.F3" title="Figure 3 ‚Ä£ II-C Uncertainties in DNN Inference ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows a general DNN inference timeline in TensorFlow. The timeline starts by calling <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">inference()</span>, which first loads the graphs and weights into the memory. The process starts reading the input, where <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">sub()</span> is used to subscribe to an image stream. The Robot Operating System¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (ROS) provides data communications between different components. Next, it pre-processes the image, including resizing, converting color space from one to another, etc. Next, the processed image is passed to the <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_italic">Session</span> and loaded into the processor to run network inference. Finally, post-processing transforms the bounding boxes <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">transform()</span> or fit pixel proposals into lanes (<span id="S2.SS3.p1.1.5" class="ltx_text ltx_font_italic">lane_fit()</span>) on the image.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">From the timeline of DNN inference, we derive several uncertainties that contribute to the time variation issue. The first is data, which means the value and distribution of image pixels. The sparsity matrix is expected to have less time on inference than the dense matrix¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The second is the data I/O to the session graph. How the running graph reads data from ROS messages could affect the inference time variations. The third is the model, which means the model‚Äôs structure and complexity in multiply and accumulate (MAC) operations. The fourth is runtime, owing to the contention of concurrent jobs for resources like memory, CPU, and GPU. How many processes allow preemption, their scheduling policies, priorities, etc., are the factors that affect the runtime variation. Finally, the hardware also affects the time variation of DNN inference. GPU is expected to run faster than CPU, but a multi-core system is supposed to show more time variations than a single-core system. How will different architectures affect the time variations of DNN inference? In summary, five aspects of uncertainties in DNN inference need to be studied: data, I/O, model, runtime, and hardware.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS4.4.1.1" class="ltx_text">II-D</span> </span><span id="S2.SS4.5.2" class="ltx_text ltx_font_italic">Profiling Tools</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">For current DNN-based autonomous driving systems, a big challenge is how to explore the time variation in DNN model inference. Our approach is to profile the system with a variety of granularity. In general, we use three profiling tools: code level (<span id="S2.SS4.p1.1.1" class="ltx_text ltx_font_italic">cProfiler</span>) and system level (<span id="S2.SS4.p1.1.2" class="ltx_text ltx_font_italic">nvprof</span> and <span id="S2.SS4.p1.1.3" class="ltx_text ltx_font_italic">Linux Perf</span>).</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p"><span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">cProfiler</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is a python library for code-level profiling. It collects statistics that describe how often and for how long parts of the program are executed. The number of function calls can identify code bugs and possible inline-expansion points (high call counts). Internal time statistics can identify ‚Äúhot loops‚Äù that should be carefully optimized. Cumulative time statistics are used to identify high-level errors in the selection of algorithms. We use <span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_italic">cProfiler</span> to get the call graph with time breakdowns of the code.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Although application-level profiling gives us some explanations of DNN inference time variations, it is not enough to explain the variability of the DNN inference for different architectures. Therefore, we conduct system call level profiling with <span id="S2.SS4.p3.1.1" class="ltx_text ltx_font_italic">Linux perf</span> to show model inference performance at the system call level. <span id="S2.SS4.p3.1.2" class="ltx_text ltx_font_italic">Linux perf</span> uses the system performance counter to monitor the whole system <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. We use <span id="S2.SS4.p3.1.3" class="ltx_text ltx_font_italic">Linux perf</span> to collect the system-level metrics, including CPU cycles, context switches, CPU migrations, page faults, instructions, branches, branch misses, cache misses, etc.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">GPU has been widely used in DNN execution acceleration. <span id="S2.SS4.p4.1.1" class="ltx_text ltx_font_italic">nvprof</span> is a profiling tool provided by NVIDIA that enables the collection of a timeline of CUDA-related activities on CPU and GPU, including kernel execution, memory transfers, memory set, CUDA API calls, and events or metrics for CUDA kernels¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. We use <span id="S2.SS4.p4.1.2" class="ltx_text ltx_font_italic">nvprof</span> to collect timing analysis for GPU activities to find the roots for DNN inference time variations.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Model Inference Profiling</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To understand the uncertainties and quantify their impacts on DNN inference time variations, we need to profile the execution of model inference in fine granularity. This section discusses the profiling of typical DNN models based on the uncertainty analysis in Section¬†<a href="#S2" title="II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Experiment Setup</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To begin with, we present the experimental setup for DNN inference profiling. We choose three types of computing devices that cover CPU and GPU-based processors. Besides, we create an image dataset based on the KITTI dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Hardware and software setup.</span> The devices we use for profiling include NVIDIA Jetson AGX Xavier, Xavier NX, and Intel Fog Reference. Table¬†<a href="#S3.T2" title="TABLE II ‚Ä£ III-A Experiment Setup ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows these devices‚Äô memory, CPU, and GPU configurations¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. Both the Jetson boards are installed with <span id="S3.SS1.p2.1.2" class="ltx_text ltx_font_typewriter">JetPack 4.4-DP</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (L4T R32.4.2) and use an <span id="S3.SS1.p2.1.3" class="ltx_text ltx_font_typewriter">l4t-ml</span> docker image¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> as the base image for system setup. The <span id="S3.SS1.p2.1.4" class="ltx_text ltx_font_typewriter">l4t-ml</span> image includes several libraries for machine learning-related applications: <span id="S3.SS1.p2.1.5" class="ltx_text ltx_font_typewriter">TensorFlow 1.15</span>, <span id="S3.SS1.p2.1.6" class="ltx_text ltx_font_typewriter">PyTorch v1.5.0</span>, <span id="S3.SS1.p2.1.7" class="ltx_text ltx_font_typewriter">torchvision v0.6.0</span>, <span id="S3.SS1.p2.1.8" class="ltx_text ltx_font_typewriter">CUDA 10.2</span>, <span id="S3.SS1.p2.1.9" class="ltx_text ltx_font_typewriter">cuDNN 8.0.0</span>, <span id="S3.SS1.p2.1.10" class="ltx_text ltx_font_typewriter">OpenCV 4.1</span>, etc. On top of the <span id="S3.SS1.p2.1.11" class="ltx_text ltx_font_typewriter">l4t-ml</span> image, we implemented a ROS-based perception pipeline for autonomous driving. <span id="S3.SS1.p2.1.12" class="ltx_text ltx_font_typewriter">ROS Melodic</span> and <span id="S3.SS1.p2.1.13" class="ltx_text ltx_font_typewriter">ROS Galactic</span> are deployed as the communication middleware. To reduce the impact of different hardware on the time variation, the model inference profiling of data, I/O, model, and runtime variances are conducted on NVIDIA Jetson AGX boards, which have the same chip as the auto-graded NVIDIA DRIVE AGX Xavier board¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Besides, we disable the Dynamic Voltage and Frequency Scaling (DVFS) on Jetson AGX Xavier with <span id="S3.SS1.p2.1.14" class="ltx_text ltx_font_typewriter">jetson_benchmark</span>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and turn off all other user applications before the experiment. The Jetson broad is set at <span id="S3.SS1.p2.1.15" class="ltx_text ltx_font_typewriter">MAXN</span> power mode, where both CPU and GPU run with the highest frequency. Since accuracy is essential for the autonomous driving scenario, all the DNN models are trained and tested with full precision (FP32).</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span><span id="S3.T2.2.1" class="ltx_text" style="color:#000000;">Hardware configurations of devices in profiling.</span></figcaption>
<div id="S3.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:141.5pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-66.2pt,21.5pt) scale(0.766072338774123,0.766072338774123) ;">
<table id="S3.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.3.1.1.1" class="ltx_tr">
<th id="S3.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Devices</span></th>
<th id="S3.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">CPU</span></th>
<th id="S3.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GPU</span></th>
<th id="S3.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Memory</span></th>
<th id="S3.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T2.3.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AI Performance</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.3.1.2.1" class="ltx_tr">
<td id="S3.T2.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">AGX Xavier</span></td>
<td id="S3.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.2.1.2.1.1" class="ltx_tr">
<td id="S3.T2.3.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.2.1.1.1.1" class="ltx_text" style="font-size:80%;">8-core NVIDIA Carmel Arm¬Æv8.2</span></td>
</tr>
<tr id="S3.T2.3.1.2.1.2.1.2" class="ltx_tr">
<td id="S3.T2.3.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.2.1.2.1.1" class="ltx_text" style="font-size:80%;">64-bit CPU 8MB L2 + 4MB L3</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.2.1.3.1.1" class="ltx_tr">
<td id="S3.T2.3.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.3.1.1.1.1" class="ltx_text" style="font-size:80%;">512-core NVIDIA Volta‚Ñ¢</span></td>
</tr>
<tr id="S3.T2.3.1.2.1.3.1.2" class="ltx_tr">
<td id="S3.T2.3.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.3.1.2.1.1" class="ltx_text" style="font-size:80%;">¬†with 64 Tensor Cores</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.2.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.2.1.4.1.1" class="ltx_tr">
<td id="S3.T2.3.1.2.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.4.1.1.1.1" class="ltx_text" style="font-size:80%;">32 GB 256-bit LPDDR4x</span></td>
</tr>
<tr id="S3.T2.3.1.2.1.4.1.2" class="ltx_tr">
<td id="S3.T2.3.1.2.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.2.1.4.1.2.1.1" class="ltx_text" style="font-size:80%;">136.5 GB/s</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.2.1.5.1" class="ltx_text" style="font-size:80%;">32 TOPs</span></td>
</tr>
<tr id="S3.T2.3.1.3.2" class="ltx_tr">
<td id="S3.T2.3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">Xavier NX</span></td>
<td id="S3.T2.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.3.2.2.1.1" class="ltx_tr">
<td id="S3.T2.3.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.2.1.1.1.1" class="ltx_text" style="font-size:80%;">6-core NVIDIA Carmel ARM¬Æv8.2</span></td>
</tr>
<tr id="S3.T2.3.1.3.2.2.1.2" class="ltx_tr">
<td id="S3.T2.3.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.2.1.2.1.1" class="ltx_text" style="font-size:80%;">64-bit CPU 6MB L2 + 4MB L3</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.3.2.3.1.1" class="ltx_tr">
<td id="S3.T2.3.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.3.1.1.1.1" class="ltx_text" style="font-size:80%;">384-core NVIDIA Volta‚Ñ¢</span></td>
</tr>
<tr id="S3.T2.3.1.3.2.3.1.2" class="ltx_tr">
<td id="S3.T2.3.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.3.1.2.1.1" class="ltx_text" style="font-size:80%;">¬†with 48 Tensor Cores</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.3.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.3.2.4.1.1" class="ltx_tr">
<td id="S3.T2.3.1.3.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.4.1.1.1.1" class="ltx_text" style="font-size:80%;">8 GB 128-bit LPDDR4x</span></td>
</tr>
<tr id="S3.T2.3.1.3.2.4.1.2" class="ltx_tr">
<td id="S3.T2.3.1.3.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.3.2.4.1.2.1.1" class="ltx_text" style="font-size:80%;">59.7 GB/s</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.3.2.5.1" class="ltx_text" style="font-size:80%;">21 TOPs</span></td>
</tr>
<tr id="S3.T2.3.1.4.3" class="ltx_tr">
<td id="S3.T2.3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">Fog Node</span></td>
<td id="S3.T2.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.4.3.2.1.1" class="ltx_tr">
<td id="S3.T2.3.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.4.3.2.1.1.1.1" class="ltx_text" style="font-size:80%;">8 Intel(R) Xeon(R)</span></td>
</tr>
<tr id="S3.T2.3.1.4.3.2.1.2" class="ltx_tr">
<td id="S3.T2.3.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.4.3.2.1.2.1.1" class="ltx_text" style="font-size:80%;">CPU E3-1275 v5 @ 3.60GHz</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S3.T2.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">
<table id="S3.T2.3.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.4.3.4.1.1" class="ltx_tr">
<td id="S3.T2.3.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.4.3.4.1.1.1.1" class="ltx_text" style="font-size:80%;">32 GB DDR4</span></td>
</tr>
<tr id="S3.T2.3.1.4.3.4.1.2" class="ltx_tr">
<td id="S3.T2.3.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.4.3.4.1.2.1.1" class="ltx_text" style="font-size:80%;">34.1 GB/s</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.3.1.4.3.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S3.T2.3.1.5.4" class="ltx_tr">
<td id="S3.T2.3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T2.3.1.5.4.1.1" class="ltx_text" style="font-size:80%;">GPU Workstation</span></td>
<td id="S3.T2.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S3.T2.3.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.5.4.2.1.1" class="ltx_tr">
<td id="S3.T2.3.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.5.4.2.1.1.1.1" class="ltx_text" style="font-size:80%;">28 Intel¬Æ Core‚Ñ¢ i9-9940X</span></td>
</tr>
<tr id="S3.T2.3.1.5.4.2.1.2" class="ltx_tr">
<td id="S3.T2.3.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.5.4.2.1.2.1.1" class="ltx_text" style="font-size:80%;">CPU @ 3.30GHz</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T2.3.1.5.4.3.1" class="ltx_text" style="font-size:80%;">4 NVIDIA GeForce RTX 2080 Ti/PCIe/SSE2</span></td>
<td id="S3.T2.3.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S3.T2.3.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.3.1.5.4.4.1.1" class="ltx_tr">
<td id="S3.T2.3.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.5.4.4.1.1.1.1" class="ltx_text" style="font-size:80%;">64 GB DDR4</span></td>
</tr>
<tr id="S3.T2.3.1.5.4.4.1.2" class="ltx_tr">
<td id="S3.T2.3.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T2.3.1.5.4.4.1.2.1.1" class="ltx_text" style="font-size:80%;">85 GB/s</span></td>
</tr>
</table>
</td>
<td id="S3.T2.3.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S3.T2.3.1.5.4.5.1" class="ltx_text" style="font-size:80%;">312 TOPS</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Dataset Descriptions.</span> We create an image dataset as a uniform input to the profiling process based on the KITTI dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The image dataset covers three scenarios (city, residential, and road) with 1,800 images. By sampling the image dataset with different frequencies (1, 2, 5, and 10 FPS), we get four groups of images for each scenario, with 60, 120, 300, and 600 images for each scenario. In addition to the image dataset, an auto-grade camera with 1920x1080 resolution and 30 FPS is also used as input to the DNNs.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.4" class="ltx_p"><span id="S3.SS1.p4.4.1" class="ltx_text ltx_font_bold">Metrics.</span> To give detailed profiling results of the DNN-based applications, we measure several metrics, including latency, processor utilization, memory utilization, etc. We calculate latency‚Äôs statistic metrics, including the range and the coefficient of variation. Equation¬†(<a href="#S3.E1" title="In III-A Experiment Setup ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) defines the range as the difference between the maximum and minimum values. The coefficient of variation (<math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><msub id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">c</mi><mi id="S3.SS1.p4.1.m1.1.1.3" xref="S3.SS1.p4.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">ùëê</ci><ci id="S3.SS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">c_{v}</annotation></semantics></math>) is used to evaluate the relative variability, and it is calculated using the standard variation <math id="S3.SS1.p4.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS1.p4.2.m2.1a"><mi id="S3.SS1.p4.2.m2.1.1" xref="S3.SS1.p4.2.m2.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.2.m2.1b"><ci id="S3.SS1.p4.2.m2.1.1.cmml" xref="S3.SS1.p4.2.m2.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.2.m2.1c">\sigma</annotation></semantics></math> divided by the mean value <math id="S3.SS1.p4.3.m3.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.SS1.p4.3.m3.1a"><mi id="S3.SS1.p4.3.m3.1.1" xref="S3.SS1.p4.3.m3.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.3.m3.1b"><ci id="S3.SS1.p4.3.m3.1.1.cmml" xref="S3.SS1.p4.3.m3.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.3.m3.1c">\mu</annotation></semantics></math>. <math id="S3.SS1.p4.4.m4.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.SS1.p4.4.m4.1a"><msub id="S3.SS1.p4.4.m4.1.1" xref="S3.SS1.p4.4.m4.1.1.cmml"><mi id="S3.SS1.p4.4.m4.1.1.2" xref="S3.SS1.p4.4.m4.1.1.2.cmml">c</mi><mi id="S3.SS1.p4.4.m4.1.1.3" xref="S3.SS1.p4.4.m4.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.4.m4.1b"><apply id="S3.SS1.p4.4.m4.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.4.m4.1.1.1.cmml" xref="S3.SS1.p4.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p4.4.m4.1.1.2.cmml" xref="S3.SS1.p4.4.m4.1.1.2">ùëê</ci><ci id="S3.SS1.p4.4.m4.1.1.3.cmml" xref="S3.SS1.p4.4.m4.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.4.m4.1c">c_{v}</annotation></semantics></math> is a positive value. The higher the value is, the higher variations the data has.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para">
<p id="S3.SS1.p5.1" class="ltx_p"><span id="S3.SS1.p5.1.1" class="ltx_text ltx_font_bold">Range:</span></p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.2" class="ltx_Math" alttext="R=max(t_{i})-min(t_{i})" display="block"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml">R</mi><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">=</mo><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2a" xref="S3.E1.m1.1.1.1.1.2.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">x</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2b" xref="S3.E1.m1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">‚àí</mo><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.2.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.4" xref="S3.E1.m1.2.2.2.2.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2a" xref="S3.E1.m1.2.2.2.2.2.cmml">‚Äã</mo><mi id="S3.E1.m1.2.2.2.2.5" xref="S3.E1.m1.2.2.2.2.5.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.2b" xref="S3.E1.m1.2.2.2.2.2.cmml">‚Äã</mo><mrow id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.2.2.1.1.1" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.1.2.cmml">t</mi><mi id="S3.E1.m1.2.2.2.2.1.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></eq><ci id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4">ùëÖ</ci><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><minus id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></minus><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1"><times id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"></times><ci id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3">ùëö</ci><ci id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4">ùëé</ci><ci id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5">ùë•</ci><apply id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.2">ùë°</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2"><times id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2"></times><ci id="S3.E1.m1.2.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2.3">ùëö</ci><ci id="S3.E1.m1.2.2.2.2.4.cmml" xref="S3.E1.m1.2.2.2.2.4">ùëñ</ci><ci id="S3.E1.m1.2.2.2.2.5.cmml" xref="S3.E1.m1.2.2.2.2.5">ùëõ</ci><apply id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.2">ùë°</ci><ci id="S3.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.1.3">ùëñ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">R=max(t_{i})-min(t_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS1.p6" class="ltx_para">
<p id="S3.SS1.p6.1" class="ltx_p"><span id="S3.SS1.p6.1.1" class="ltx_text ltx_font_bold">Coefficient of Variation:</span></p>
<table id="S3.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E2.m1.1" class="ltx_Math" alttext="c_{v}=\frac{\sigma}{\mu}" display="block"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.2.2" xref="S3.E2.m1.1.1.2.2.cmml">c</mi><mi id="S3.E2.m1.1.1.2.3" xref="S3.E2.m1.1.1.2.3.cmml">v</mi></msub><mo id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml">=</mo><mfrac id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">œÉ</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">Œº</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ùëê</ci><ci id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3">ùë£</ci></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><divide id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3"></divide><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">ùúé</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">ùúá</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">c_{v}=\frac{\sigma}{\mu}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Data Variability</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">As the input to the DNN inference, data variability is expected to affect the whole pipeline significantly. The application scenarios and FPS are expected to affect the execution time of the DNN models since more lanes and objects are supposed to be detected downtown than in the countryside. Meanwhile, since the critical operations of DNN inference are multiply and accumulate (MAC), the distribution of the matrix/pixel might also affect the execution time. One example is that the sparse matrix needs fewer operations than the dense matrix. Moreover, the weather brings another uncertainty for sensor data when the model is deployed in real-world environment. Therefore, we discuss the impact of the data variability on the time variation of DNN inference in three aspects: scenarios, pixel sizes &amp; distribution, and weather‚Äôs impact.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2209.05487/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>CDF for latency results of YOLOv3, Faster R-CNN, and PINet under different scenarios.</figcaption>
</figure>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Scenarios.</span><span id="S3.SS2.p2.1.2" class="ltx_text" style="color:#000000;"> We use the datasets from three scenarios (i.e., city, residential, and road) to evaluate the impact of data variability. Three DNN inferences are covered: YOLOv3 and Faster R-CNN for object detection, PINet for lane detection. The results under different scenarios are shown in Figure¬†</span><a href="#S3.F4" title="Figure 4 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S3.SS2.p2.1.3" class="ltx_text" style="color:#000000;">.</span><span id="S3.SS2.p2.1.4" class="ltx_text"> The performance for different scenarios shows differently: PINet and Faster R-CNN show massive time variations between those three scenarios, while YOLOv3 does not. The reason is that different scenarios bring variable possibilities to detect lanes and objects. As a representative of one-stage based object detection, YOLOv3‚Äôs latency is not affected by the scenarios. However, Faster R-CNN is a two-stage based object detection. Different scenarios have different numbers of potential objects, contributing to the inference time variations. This observation also implies that the scheduling of object and lane detection tasks should take the running scenario into account. We will discuss it in detail in model variability.
</span></p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2209.05487/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="98" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The post-processing time and the number of detected objects in Faster R-CNN ResNet101 for a sequence of images. Right table contains the correlation coefficient of post-processing time and number of detected objects for five object detection models.</figcaption>
</figure>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Since the different scenarios mainly affect the possibility of detecting objects, the number of objects becomes a potential connection between the scenario and the time variation of DNN inference. To prove it, we choose several DNN models for object detection and get the time sequence of the latency breakdown and the number of objects. Figure¬†<a href="#S3.F5" title="Figure 5 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows an example of the results for Faster R-CNN ResNet101. The variation in the number of objects is almost consistent with the variations in the post-processing time. To get more accurate correlation results, we calculate the correlation coefficient (<math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ùëù</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">p</annotation></semantics></math>) of five object detection models‚Äô post-processing and the number of detected objects. The results for Faster R-CNN, Mask R-CNN, YOLOv3, SSD MobileNetv2, and SSD Inceptionv2 are 0.98, 0.98, 0.43, 0.91, and 0.96, respectively. These results explain the relationship between usage scenarios and inference time variations. Since lane detection can be seen as pixel-level regression, the scenarios‚Äô impact on its DNN inference is similar.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2209.05487/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="430" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The CDF of end-to-end latency for YOLOv3, Faster R-CNN, and PINet with different pixel distributions; The CDF of end-to-end latency for Faster R-CNN with difference image sizes.</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Pixel sizes &amp; distributions.</span><span id="S3.SS2.p4.1.2" class="ltx_text"> In addition to the scenarios of the input data, the pixel distribution and sizes are also expected to affect the variation of DNN inference time. To show the effect of pixel distributions, we choose three types of data inputs to the DNNs and measure time variations. These image inputs include all zero (black), all 255 (white), and random matrix. We get the results for running these three cases on YOLOv3, Faster R-CNN, and PINet, as shown in Figure¬†<a href="#S3.F6" title="Figure 6 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We can observe significant time variation in PINet when the random matrix is applied because PINet is a pixel-level regression to lanes. Random values of the pixels add the computations for detecting pixel proposals for lanes. On the contrary, there is no significant difference between YOLOv3 and Faster R-CNN because object detection is a box-level detection. Changing limited pixels does not make a big difference for a box containing thousands of pixels.</span></p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.2" class="ltx_p"><span id="S3.SS2.p5.2.2" class="ltx_text" style="color:#000000;">For profiling pixel size‚Äôs impact on inference time variations, we choose the Faster R-CNN model and scale the same input image ([3, 375, 1242]) by multiplying the width and heights with different ratios <math id="S3.SS2.p5.1.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p5.1.1.m1.1a"><mi mathcolor="#000000" id="S3.SS2.p5.1.1.m1.1.1" xref="S3.SS2.p5.1.1.m1.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.1.m1.1b"><ci id="S3.SS2.p5.1.1.m1.1.1.cmml" xref="S3.SS2.p5.1.1.m1.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.1.m1.1c">\lambda</annotation></semantics></math>: 0.01, 0.05, 0.1, 1, 2, 5, and 10. Each scaled image is sent to execute model inference 100 times. The results for CDF of the end-to-end latency are shown in Figure¬†<a href="#S3.F6" title="Figure 6 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We can find that the CDFs are close to each other except for the case when <math id="S3.SS2.p5.2.2.m2.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p5.2.2.m2.1a"><mi mathcolor="#000000" id="S3.SS2.p5.2.2.m2.1.1" xref="S3.SS2.p5.2.2.m2.1.1.cmml">Œª</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.2.m2.1b"><ci id="S3.SS2.p5.2.2.m2.1.1.cmml" xref="S3.SS2.p5.2.2.m2.1.1">ùúÜ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.2.m2.1c">\lambda</annotation></semantics></math> equals 10, which shows higher average latency and inference time variations than others. Through the study on the implementation, we found that the difference is mainly caused by the pre-processing, where the input image will be transposed and cropped if the size is larger than the maximum value.</span></p>
</div>
<figure id="S3.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>The mean (<math id="S3.T4.4.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.T4.4.m1.1b"><mi id="S3.T4.4.m1.1.1" xref="S3.T4.4.m1.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.T4.4.m1.1c"><ci id="S3.T4.4.m1.1.1.cmml" xref="S3.T4.4.m1.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.m1.1d">\mu</annotation></semantics></math>), variation (<math id="S3.T4.5.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.T4.5.m2.1b"><mi id="S3.T4.5.m2.1.1" xref="S3.T4.5.m2.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.5.m2.1c"><ci id="S3.T4.5.m2.1.1.cmml" xref="S3.T4.5.m2.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.m2.1d">\sigma</annotation></semantics></math>), and variation coefficient (<math id="S3.T4.6.m3.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.T4.6.m3.1b"><msub id="S3.T4.6.m3.1.1" xref="S3.T4.6.m3.1.1.cmml"><mi id="S3.T4.6.m3.1.1.2" xref="S3.T4.6.m3.1.1.2.cmml">c</mi><mi id="S3.T4.6.m3.1.1.3" xref="S3.T4.6.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T4.6.m3.1c"><apply id="S3.T4.6.m3.1.1.cmml" xref="S3.T4.6.m3.1.1"><csymbol cd="ambiguous" id="S3.T4.6.m3.1.1.1.cmml" xref="S3.T4.6.m3.1.1">subscript</csymbol><ci id="S3.T4.6.m3.1.1.2.cmml" xref="S3.T4.6.m3.1.1.2">ùëê</ci><ci id="S3.T4.6.m3.1.1.3.cmml" xref="S3.T4.6.m3.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.m3.1d">c_{v}</annotation></semantics></math>) of the end-to-end inference time for Faster R-CNN and PINet under different raining cases.</figcaption>
<div id="S3.T4.12" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:149.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.9pt,-2.7pt) scale(1.03805640378768,1.03805640378768) ;">
<table id="S3.T4.12.6" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.12.6.7.1" class="ltx_tr">
<th id="S3.T4.12.6.7.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">End-to-End Inference Time (ms)</th>
<th id="S3.T4.12.6.7.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="3"><span id="S3.T4.12.6.7.1.2.1" class="ltx_text ltx_font_bold">Faster R-CNN</span></th>
<th id="S3.T4.12.6.7.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3"><span id="S3.T4.12.6.7.1.3.1" class="ltx_text ltx_font_bold">PINet</span></th>
</tr>
<tr id="S3.T4.12.6.6" class="ltx_tr">
<th id="S3.T4.12.6.6.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Raining Case</th>
<th id="S3.T4.7.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S3.T4.7.1.1.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.T4.7.1.1.1.m1.1a"><mi id="S3.T4.7.1.1.1.m1.1.1" xref="S3.T4.7.1.1.1.m1.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.T4.7.1.1.1.m1.1b"><ci id="S3.T4.7.1.1.1.m1.1.1.cmml" xref="S3.T4.7.1.1.1.m1.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.1.1.1.m1.1c">\mu</annotation></semantics></math></th>
<th id="S3.T4.8.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S3.T4.8.2.2.2.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.T4.8.2.2.2.m1.1a"><mi id="S3.T4.8.2.2.2.m1.1.1" xref="S3.T4.8.2.2.2.m1.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.8.2.2.2.m1.1b"><ci id="S3.T4.8.2.2.2.m1.1.1.cmml" xref="S3.T4.8.2.2.2.m1.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.2.2.2.m1.1c">\sigma</annotation></semantics></math></th>
<th id="S3.T4.9.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><math id="S3.T4.9.3.3.3.m1.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.T4.9.3.3.3.m1.1a"><msub id="S3.T4.9.3.3.3.m1.1.1" xref="S3.T4.9.3.3.3.m1.1.1.cmml"><mi id="S3.T4.9.3.3.3.m1.1.1.2" xref="S3.T4.9.3.3.3.m1.1.1.2.cmml">c</mi><mi id="S3.T4.9.3.3.3.m1.1.1.3" xref="S3.T4.9.3.3.3.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T4.9.3.3.3.m1.1b"><apply id="S3.T4.9.3.3.3.m1.1.1.cmml" xref="S3.T4.9.3.3.3.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.9.3.3.3.m1.1.1.1.cmml" xref="S3.T4.9.3.3.3.m1.1.1">subscript</csymbol><ci id="S3.T4.9.3.3.3.m1.1.1.2.cmml" xref="S3.T4.9.3.3.3.m1.1.1.2">ùëê</ci><ci id="S3.T4.9.3.3.3.m1.1.1.3.cmml" xref="S3.T4.9.3.3.3.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.9.3.3.3.m1.1c">c_{v}</annotation></semantics></math></th>
<th id="S3.T4.10.4.4.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S3.T4.10.4.4.4.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.T4.10.4.4.4.m1.1a"><mi id="S3.T4.10.4.4.4.m1.1.1" xref="S3.T4.10.4.4.4.m1.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.T4.10.4.4.4.m1.1b"><ci id="S3.T4.10.4.4.4.m1.1.1.cmml" xref="S3.T4.10.4.4.4.m1.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.10.4.4.4.m1.1c">\mu</annotation></semantics></math></th>
<th id="S3.T4.11.5.5.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S3.T4.11.5.5.5.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.T4.11.5.5.5.m1.1a"><mi id="S3.T4.11.5.5.5.m1.1.1" xref="S3.T4.11.5.5.5.m1.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.11.5.5.5.m1.1b"><ci id="S3.T4.11.5.5.5.m1.1.1.cmml" xref="S3.T4.11.5.5.5.m1.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.11.5.5.5.m1.1c">\sigma</annotation></semantics></math></th>
<th id="S3.T4.12.6.6.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><math id="S3.T4.12.6.6.6.m1.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.T4.12.6.6.6.m1.1a"><msub id="S3.T4.12.6.6.6.m1.1.1" xref="S3.T4.12.6.6.6.m1.1.1.cmml"><mi id="S3.T4.12.6.6.6.m1.1.1.2" xref="S3.T4.12.6.6.6.m1.1.1.2.cmml">c</mi><mi id="S3.T4.12.6.6.6.m1.1.1.3" xref="S3.T4.12.6.6.6.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T4.12.6.6.6.m1.1b"><apply id="S3.T4.12.6.6.6.m1.1.1.cmml" xref="S3.T4.12.6.6.6.m1.1.1"><csymbol cd="ambiguous" id="S3.T4.12.6.6.6.m1.1.1.1.cmml" xref="S3.T4.12.6.6.6.m1.1.1">subscript</csymbol><ci id="S3.T4.12.6.6.6.m1.1.1.2.cmml" xref="S3.T4.12.6.6.6.m1.1.1.2">ùëê</ci><ci id="S3.T4.12.6.6.6.m1.1.1.3.cmml" xref="S3.T4.12.6.6.6.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.12.6.6.6.m1.1c">c_{v}</annotation></semantics></math></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.12.6.8.1" class="ltx_tr">
<td id="S3.T4.12.6.8.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T4.12.6.8.1.1.1" class="ltx_text ltx_font_italic">0 mm/hour</span></td>
<td id="S3.T4.12.6.8.1.2" class="ltx_td ltx_align_center ltx_border_t">320.7</td>
<td id="S3.T4.12.6.8.1.3" class="ltx_td ltx_align_center ltx_border_t">1217.9</td>
<td id="S3.T4.12.6.8.1.4" class="ltx_td ltx_align_center ltx_border_t">3.8</td>
<td id="S3.T4.12.6.8.1.5" class="ltx_td ltx_align_center ltx_border_t">228.9</td>
<td id="S3.T4.12.6.8.1.6" class="ltx_td ltx_align_center ltx_border_t">7862.6</td>
<td id="S3.T4.12.6.8.1.7" class="ltx_td ltx_align_center ltx_border_t">34.4</td>
</tr>
<tr id="S3.T4.12.6.9.2" class="ltx_tr">
<td id="S3.T4.12.6.9.2.1" class="ltx_td ltx_align_center"><span id="S3.T4.12.6.9.2.1.1" class="ltx_text ltx_font_italic">25 mm/hour</span></td>
<td id="S3.T4.12.6.9.2.2" class="ltx_td ltx_align_center">326.5</td>
<td id="S3.T4.12.6.9.2.3" class="ltx_td ltx_align_center">1129.6</td>
<td id="S3.T4.12.6.9.2.4" class="ltx_td ltx_align_center">3.5</td>
<td id="S3.T4.12.6.9.2.5" class="ltx_td ltx_align_center">216.9</td>
<td id="S3.T4.12.6.9.2.6" class="ltx_td ltx_align_center">7142.3</td>
<td id="S3.T4.12.6.9.2.7" class="ltx_td ltx_align_center">32.9</td>
</tr>
<tr id="S3.T4.12.6.10.3" class="ltx_tr">
<td id="S3.T4.12.6.10.3.1" class="ltx_td ltx_align_center"><span id="S3.T4.12.6.10.3.1.1" class="ltx_text ltx_font_italic">50 mm/hour</span></td>
<td id="S3.T4.12.6.10.3.2" class="ltx_td ltx_align_center">324.3</td>
<td id="S3.T4.12.6.10.3.3" class="ltx_td ltx_align_center">1082.5</td>
<td id="S3.T4.12.6.10.3.4" class="ltx_td ltx_align_center">3.3</td>
<td id="S3.T4.12.6.10.3.5" class="ltx_td ltx_align_center">215</td>
<td id="S3.T4.12.6.10.3.6" class="ltx_td ltx_align_center">7033.4</td>
<td id="S3.T4.12.6.10.3.7" class="ltx_td ltx_align_center">32.7</td>
</tr>
<tr id="S3.T4.12.6.11.4" class="ltx_tr">
<td id="S3.T4.12.6.11.4.1" class="ltx_td ltx_align_center"><span id="S3.T4.12.6.11.4.1.1" class="ltx_text ltx_font_italic">100 mm/hour</span></td>
<td id="S3.T4.12.6.11.4.2" class="ltx_td ltx_align_center">319.3</td>
<td id="S3.T4.12.6.11.4.3" class="ltx_td ltx_align_center">950.4</td>
<td id="S3.T4.12.6.11.4.4" class="ltx_td ltx_align_center">3</td>
<td id="S3.T4.12.6.11.4.5" class="ltx_td ltx_align_center">205.9</td>
<td id="S3.T4.12.6.11.4.6" class="ltx_td ltx_align_center">6633.3</td>
<td id="S3.T4.12.6.11.4.7" class="ltx_td ltx_align_center">32.2</td>
</tr>
<tr id="S3.T4.12.6.12.5" class="ltx_tr">
<td id="S3.T4.12.6.12.5.1" class="ltx_td ltx_align_center"><span id="S3.T4.12.6.12.5.1.1" class="ltx_text ltx_font_italic">150 mm/hour</span></td>
<td id="S3.T4.12.6.12.5.2" class="ltx_td ltx_align_center">314.9</td>
<td id="S3.T4.12.6.12.5.3" class="ltx_td ltx_align_center">898.9</td>
<td id="S3.T4.12.6.12.5.4" class="ltx_td ltx_align_center">2.9</td>
<td id="S3.T4.12.6.12.5.5" class="ltx_td ltx_align_center">199.2</td>
<td id="S3.T4.12.6.12.5.6" class="ltx_td ltx_align_center">5410.6</td>
<td id="S3.T4.12.6.12.5.7" class="ltx_td ltx_align_center">27.2</td>
</tr>
<tr id="S3.T4.12.6.13.6" class="ltx_tr">
<td id="S3.T4.12.6.13.6.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.1.1" class="ltx_text ltx_font_italic">200 mm/hour</span></td>
<td id="S3.T4.12.6.13.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.2.1" class="ltx_text ltx_font_bold">309.1</span></td>
<td id="S3.T4.12.6.13.6.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.3.1" class="ltx_text ltx_font_bold">805.5</span></td>
<td id="S3.T4.12.6.13.6.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.4.1" class="ltx_text ltx_font_bold">2.6</span></td>
<td id="S3.T4.12.6.13.6.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.5.1" class="ltx_text ltx_font_bold">190.4</span></td>
<td id="S3.T4.12.6.13.6.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.6.1" class="ltx_text ltx_font_bold">5068.1</span></td>
<td id="S3.T4.12.6.13.6.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T4.12.6.13.6.7.1" class="ltx_text ltx_font_bold">26.6</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS2.p6" class="ltx_para ltx_noindent">
<p id="S3.SS2.p6.3" class="ltx_p"><span id="S3.SS2.p6.3.1" class="ltx_text ltx_font_bold">Weather‚Äôs impact.</span> When the model is deployed in a real-world environment, the weather is expected to affect the accuracy and the inference time of DNN models. To show the weather‚Äôs impact on the DNN inference time variation, we render different levels of rain into the KITTI dataset and measure the end-to-end model inference time¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>. Six raining cases (0/25/50/100/150/200 mm/hour) are covered, and we choose Faster R-CNN and PINet to show the impact of rains on the inference time. Table¬†<a href="#S3.T4" title="TABLE IV ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows the results of end-to-end inference time for Faster R-CNN and PINet under six rainy cases. By comparing the mean (<math id="S3.SS2.p6.1.m1.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="S3.SS2.p6.1.m1.1a"><mi id="S3.SS2.p6.1.m1.1.1" xref="S3.SS2.p6.1.m1.1.1.cmml">Œº</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.1.m1.1b"><ci id="S3.SS2.p6.1.m1.1.1.cmml" xref="S3.SS2.p6.1.m1.1.1">ùúá</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.1.m1.1c">\mu</annotation></semantics></math>), the variation (<math id="S3.SS2.p6.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS2.p6.2.m2.1a"><mi id="S3.SS2.p6.2.m2.1.1" xref="S3.SS2.p6.2.m2.1.1.cmml">œÉ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.2.m2.1b"><ci id="S3.SS2.p6.2.m2.1.1.cmml" xref="S3.SS2.p6.2.m2.1.1">ùúé</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.2.m2.1c">\sigma</annotation></semantics></math>), and the coefficient of variation (<math id="S3.SS2.p6.3.m3.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.SS2.p6.3.m3.1a"><msub id="S3.SS2.p6.3.m3.1.1" xref="S3.SS2.p6.3.m3.1.1.cmml"><mi id="S3.SS2.p6.3.m3.1.1.2" xref="S3.SS2.p6.3.m3.1.1.2.cmml">c</mi><mi id="S3.SS2.p6.3.m3.1.1.3" xref="S3.SS2.p6.3.m3.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p6.3.m3.1b"><apply id="S3.SS2.p6.3.m3.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p6.3.m3.1.1.1.cmml" xref="S3.SS2.p6.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p6.3.m3.1.1.2.cmml" xref="S3.SS2.p6.3.m3.1.1.2">ùëê</ci><ci id="S3.SS2.p6.3.m3.1.1.3.cmml" xref="S3.SS2.p6.3.m3.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p6.3.m3.1c">c_{v}</annotation></semantics></math>) under different raining cases, we find that the end-to-end inference time‚Äôs value and variation decrease as the rain level increases. The reason is that heavy rain makes thousands of pixel value changes, and the probability for a group of pixels to be lanes and objects is decreased. To prove it, we record the number of object box proposals, pixel lane proposals, detected objects, and lanes from the inference pipeline of Faster R-CNN and PINet, shown in Figure¬†<a href="#S3.F7" title="Figure 7 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. We can find the number of proposals for objects and lanes is decreased when the rain level increases. Besides, considering the 25th and 75th percentile of the distribution, the range and the variation of objects and lanes are also decreased, which is highly correlated with the end-to-end inference time variations.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2209.05487/assets/x7.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="364" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The box plot of proposals and objects for Faster R-CNN and PINet under different raining levels.</figcaption>
</figure>
<div id="S3.SS2.p7" class="ltx_para ltx_noindent">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold">Insight 1:</span>¬†<span id="S3.SS2.p7.1.2" class="ltx_text ltx_font_italic">The scenario affects the DNN inference time variations by the potential number of detected lanes/objects. The pixel distributions have a higher impact on lane detection than object detection. The time variations of object and lane detection decrease as the rain level increases.</span></p>
</div>
<figure id="S3.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2209.05487/assets/x8.png" id="S3.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="99" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2209.05487/assets/x9.png" id="S3.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="184" height="95" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F8.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The publish-subscribe model in (a) ROS1 IPC (b) ROS2 DDS. Each publisher/subscriber is corresponding to a Node in ROS1 or a participant in ROS2.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">I/O Variability</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">As an essential step for the DNN inference, access to the data could affect the whole execution pipeline. ROS is an anonymous publish-subscribe middleware system widely used for data communication. The publisher-subscriber mechanism is the main communication pattern in the autonomous driving system since multiple models need sensor data for localization, detection, segmentation, etc. The socket-based Inter-Process Communication (IPC) mechanism in ROS1 brings high compatibility and extensibility¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. ROS2 is an evolved version of ROS1. ROS2 uses Data Distribution Service (DDS)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> as its communication foundation, which works well in real-time distributed systems. In this part, we compare the time variations under two types of communication mechanisms: ROS IPC and ROS2 DDS.</p>
</div>
<figure id="S3.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Three messages used in comparison of ROS1 IPC and ROS2 DDS.</figcaption>
<div id="S3.T5.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:81pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(24.2pt,-4.5pt) scale(1.12555294947446,1.12555294947446) ;">
<table id="S3.T5.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.1.1.1.1" class="ltx_tr">
<td id="S3.T5.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Name</span></td>
<td id="S3.T5.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Message type</span></td>
<td id="S3.T5.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Source</span></td>
<td id="S3.T5.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Dimention( [width, height, channel] )</span></td>
<td id="S3.T5.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Size</span></td>
</tr>
<tr id="S3.T5.1.1.2.2" class="ltx_tr">
<td id="S3.T5.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">msg1</td>
<td id="S3.T5.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.2.2.2.1" class="ltx_text ltx_font_typewriter">Image</span></td>
<td id="S3.T5.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image File</td>
<td id="S3.T5.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[192, 108, 3]</td>
<td id="S3.T5.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">62KB</td>
</tr>
<tr id="S3.T5.1.1.3.3" class="ltx_tr">
<td id="S3.T5.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">msg2</td>
<td id="S3.T5.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T5.1.1.3.3.2.1" class="ltx_text ltx_font_typewriter">Image</span></td>
<td id="S3.T5.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image File</td>
<td id="S3.T5.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">[1920, 1080, 3]</td>
<td id="S3.T5.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.2MB</td>
</tr>
<tr id="S3.T5.1.1.4.4" class="ltx_tr">
<td id="S3.T5.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">msg3</td>
<td id="S3.T5.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T5.1.1.4.4.2.1" class="ltx_text ltx_font_typewriter">Image</span></td>
<td id="S3.T5.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Camera</td>
<td id="S3.T5.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">[1920, 1080, 3]</td>
<td id="S3.T5.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">6.2MB</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2209.05487/assets/x10.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The box plot for message transmission latency of ROS1 IPC and ROS2 DDS when the number of subscribers increased from one to eight.</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Figure¬†<a href="#S3.F8" title="Figure 8 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows the structure of the publish-subscribe model used in ROS1 Inter-Process Communications (IPC) and ROS2 (Data Distribution Service) DDS. ROS1 has a centralized design for data communication. It has a master node that provides naming and registration services for all the other ROS nodes, topics, services, and actions. ROS node is a process to perform a particular computation, while ROS topics are named buses for ROS nodes to exchange messages¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Figure¬†<a href="#S3.F8.sf1" title="In Figure 8 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(a)</span></a> is a general 1 to N communication pattern (one data publisher, N data subscribers). The underlying transmission is based on <span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_typewriter">TCPROS</span> by default¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. When <span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_typewriter">N</span> nodes subscribe to a topic, the message would be copied <span id="S3.SS3.p2.1.3" class="ltx_text ltx_font_typewriter">N-1</span> times and sent to the subscriber in sequence order. Unlike ROS1, ROS2 follows a distributed design for fault tolerance purposes. As is shown in Figure¬†<a href="#S3.F8.sf2" title="In Figure 8 ‚Ä£ III-B Data Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8(b)</span></a>, a global data space is implemented in DDS, which all independent applications can access. The underlying communication in ROS2 DDS is based on UDP and shared memory to avoid data copies¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">To compare the performance of ROS1 IPC and ROS2 DDS, we deployed three <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_typewriter">Image</span> messages in ROS/ROS2. Table¬†<a href="#S3.T5" title="TABLE V ‚Ä£ III-C I/O Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the descriptions of these messages: one is read directly from a USB camera with a resolution of 1920x1080, another is randomly generated with the same size as the camera‚Äôs frame, and the remaining one has a smaller size than the former two messages. The <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_typewriter">Image</span> publisher‚Äôs queue size is 1 for both ROS1 IPC and ROS2 DDS. The deployed ROS DDS is <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_typewriter">eProsima Fast DDS</span>, explicitly optimized for ROS2¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>. We measure the latency of message transmission from the time a message is published until the time another node subscribes to it. In addition, we set up a different number of subscribers between one to eight and recorded the communication latency for each subscriber.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">The results of the box plot for communication latency with ROS1 IPC and ROS2 DDS are shown in Figure¬†<a href="#S3.F9" title="Figure 9 ‚Ä£ III-C I/O Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. By comparing the latency distribution, we find that for both ROS1 IPC and ROS2 DDS, the range of communication latency increases when the number of subscribers increases, which indicates the I/O time variations increase when more subscribers are accessing the same <span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_typewriter">Topic</span>. Moreover, when the message size is small (<span id="S3.SS3.p4.1.2" class="ltx_text ltx_font_typewriter">msg1</span>), ROS2 DDS shows lower communication latency and variations than ROS1 IPC. This is owing to the overhead of message copy in ROS1 IPC. However, when the message size becomes larger (<span id="S3.SS3.p4.1.3" class="ltx_text ltx_font_typewriter">msg2</span>), ROS1 IPC begins to show better performance than ROS2 DDS. The reason is that ROS2 DDS invokes UDP calls for communication, while the maximum UDP datagram size in ROS2 is 64KB. Plenty of time is consumed by the message splitting and merging in ROS2 DDS, which is much higher than message copy overhead in ROS1 IPC. Besides, the communication latency variation among subscribers in ROS1 IPC is also lower than in ROS2 DDS. We can find that when transmitting <span id="S3.SS3.p4.1.4" class="ltx_text ltx_font_typewriter">msg2</span> and <span id="S3.SS3.p4.1.5" class="ltx_text ltx_font_typewriter">msg3</span> with one to eight patterns, four have lower latency and a smaller range, while the other four subscribers have much higher results. The main reason is that the message splitting and merging in UDP consumes so many CPU calls that the communication cannot support eight links simultaneously.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para ltx_noindent">
<p id="S3.SS3.p5.1" class="ltx_p"><span id="S3.SS3.p5.1.1" class="ltx_text ltx_font_bold">Insight 2:</span>¬†<span id="S3.SS3.p5.1.2" class="ltx_text ltx_font_italic">The variations of I/O latency increase significantly when the number of subscribers to the same topic increases. ROS2 DDS shows lower latency and time variations for small messages, while ROS1 IPC performs better for large messages.</span></p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Model Variability</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the end-to-end timeline of DNN inference, the model plays an essential part in its time variations. Models trained under different scenarios or network structures are expected to perform differently. Within the DNN inference, most of the time is consumed by the model inference, which raises the question of whether the inference‚Äôs variation also dominates the model‚Äôs time variations. Since the model‚Äôs variability is a complex topic, we focus on six detection models in this part.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2209.05487/assets/x11.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="415" height="345" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The latency breakdown and time variations for four object detection models and two lane detection models with the images dataset. We can observe the relationship of the end-to-end inference time variations with the breakdown‚Äôs latency. Among all these six models, the time variations of YOLOv3, SSD-MobileNetv2 are determined by model inference, while the post-processing determines the time variations of the remaining four models.</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.1" class="ltx_p"><span id="S3.SS4.p2.1.1" class="ltx_text ltx_font_bold">Inference and post-processing dominated.</span> In Figure¬†<a href="#S3.F10" title="Figure 10 ‚Ä£ III-D Model Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we apply four DNN models for object detection and two for lane detection on the image dataset to show the changing latency breakdowns with different images. Based on each part‚Äôs trend in each model, we can divide the four models into two groups: inference-dominated and post-processing dominated. The inference-dominated model means the variation of the end-to-end latency is correlated with the inference time, which contains YOLOv3 and SSD-MobileNetv2. The post-processing dominated model means the variation of the end-to-end latency is greatly affected by the post-processing part. The detailed correlation analysis results for end-to-end latency with breakdowns (reading, pre-processing, inference, and post-processing) are shown in Table¬†<a href="#S3.T6" title="TABLE VI ‚Ä£ III-D Model Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, which gives quantitative proof of dominating factors of DNN inference time.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Why does it happen for these DNN models? We found the original answer when looking inside the design for the DNNs. As we have learned from the design of these six models, YOLOv3 and SSD use a one-stage approach that uniformly samples on the image to get a certain number of bounding boxes and relies on the convolution layers for feature extraction to calculate the probability of object class. This one-stage design leads to a static number of objects from the inference part, enabling minor time variations of the post-processing step. In contrast, Faster R-CNN ResNet101 and Mask R-CNN Inceptionv2 are based on a two-stage approach. The first stage generates a sparse set of candidate object proposals, and the second one determines the accurate object regions and the corresponding class/lane labels using convolutional neural networks¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite>. This two-stage design causes slight time variations in the convolution neural networks. However, it makes the number of objects in post-processing dynamic, which explains why the time variations are correlated to post-processing time.</p>
</div>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.1" class="ltx_p">Similarly, LaneNet and PINet also follow a two-stage design, where the first stage generates pixel proposals and the second stage clusters pixel proposals into lane groups. To prove our assumption, we collect the number of proposals and the post-processing time from DNN inference. Then we normalize them and calculate their correlation coefficients. Figure¬†<a href="#S3.F11" title="Figure 11 ‚Ä£ III-D Model Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> shows the time sequence and correlation coefficient results for Faster R-CNN, LaneNet, and PINet. The correlation coefficient between the number of proposals and post-processing time is constantly higher than 0.89.</p>
</div>
<figure id="S3.T6" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Correlation coefficients of End-to-End latency with the breakdowns. The more closer to 1, the more correlated.</figcaption>
<div id="S3.T6.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:91pt;vertical-align:-2.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.7pt,1.0pt) scale(0.978886021691609,0.978886021691609) ;">
<table id="S3.T6.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T6.1.1.1.1" class="ltx_tr">
<th id="S3.T6.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Correlation Coefficients</span></th>
<th id="S3.T6.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T6.1.1.1.1.2.1" class="ltx_text ltx_font_bold">read</span></th>
<th id="S3.T6.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T6.1.1.1.1.3.1" class="ltx_text ltx_font_bold">pre-processing</span></th>
<th id="S3.T6.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T6.1.1.1.1.4.1" class="ltx_text ltx_font_bold">inference</span></th>
<th id="S3.T6.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T6.1.1.1.1.5.1" class="ltx_text ltx_font_bold">post-processing</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T6.1.1.2.1" class="ltx_tr">
<td id="S3.T6.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T6.1.1.2.1.1.1" class="ltx_text ltx_font_italic">YOLOv3</span></td>
<td id="S3.T6.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.220</td>
<td id="S3.T6.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.429</td>
<td id="S3.T6.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T6.1.1.2.1.4.1" class="ltx_text ltx_font_bold">0.906</span></td>
<td id="S3.T6.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.378</td>
</tr>
<tr id="S3.T6.1.1.3.2" class="ltx_tr">
<td id="S3.T6.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S3.T6.1.1.3.2.1.1" class="ltx_text ltx_font_italic">Faster R-CNN ResNet101</span></td>
<td id="S3.T6.1.1.3.2.2" class="ltx_td ltx_align_center">-0.108</td>
<td id="S3.T6.1.1.3.2.3" class="ltx_td ltx_align_center">0.060</td>
<td id="S3.T6.1.1.3.2.4" class="ltx_td ltx_align_center">0.681</td>
<td id="S3.T6.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S3.T6.1.1.3.2.5.1" class="ltx_text ltx_font_bold">0.896</span></td>
</tr>
<tr id="S3.T6.1.1.4.3" class="ltx_tr">
<td id="S3.T6.1.1.4.3.1" class="ltx_td ltx_align_center"><span id="S3.T6.1.1.4.3.1.1" class="ltx_text ltx_font_italic">Mask R-CNN Inceptionv2</span></td>
<td id="S3.T6.1.1.4.3.2" class="ltx_td ltx_align_center">-0.131</td>
<td id="S3.T6.1.1.4.3.3" class="ltx_td ltx_align_center">0.035</td>
<td id="S3.T6.1.1.4.3.4" class="ltx_td ltx_align_center">0.619</td>
<td id="S3.T6.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S3.T6.1.1.4.3.5.1" class="ltx_text ltx_font_bold">0.946</span></td>
</tr>
<tr id="S3.T6.1.1.5.4" class="ltx_tr">
<td id="S3.T6.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.1.1.5.4.1.1" class="ltx_text ltx_font_italic">SSD MobileNetv2</span></td>
<td id="S3.T6.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_bb">0.169</td>
<td id="S3.T6.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb">0.056</td>
<td id="S3.T6.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T6.1.1.5.4.4.1" class="ltx_text ltx_font_bold">0.963</span></td>
<td id="S3.T6.1.1.5.4.5" class="ltx_td ltx_align_center ltx_border_bb">0.525</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2209.05487/assets/x12.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="368" height="383" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The time sequence and correlation coefficients of proposals and post-processing time</figcaption>
</figure>
<div id="S3.SS4.p5" class="ltx_para ltx_noindent">
<p id="S3.SS4.p5.1" class="ltx_p"><span id="S3.SS4.p5.1.1" class="ltx_text ltx_font_bold">Insight 3:</span>¬†<span id="S3.SS4.p5.1.2" class="ltx_text ltx_font_italic">The design of the model‚Äôs structure significantly impacts the time variations of the DNN inference. For object detection, one-stage models show less variation than two-stage based ones. The time variations for two-stage based object detection and lane detection models are mainly caused by the number of objects/lanes proposals from the first stage.</span></p>
</div>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS5.4.1.1" class="ltx_text">III-E</span> </span><span id="S3.SS5.5.2" class="ltx_text ltx_font_italic">Runtime Variability</span>
</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">With fixed input, model, and hardware, runtime becomes an essential factor contributing to the time variation of DNN inference. However, since a simple application can invoke thousands of system calls with millions of instructions to computing architecture, it is tough to accurately predict the execution time for a given application. Since GPU tasks are non-preemptive, CPU scheduling becomes the main uncertainty in runtime¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. In this paper, the DNN inference runtime profiling focuses on finding the connections between the CPU scheduling policy and the time variation.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p"><span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_bold">Scheduling policy setup.</span>
Due to the hard real-time requirements from safety-critical applications, real-time operating systems or systems with real-time kernel patches are widely used in computing systems for autonomous vehicles. In this paper, we use the NVIDIA Jetson AGX board and configure it with an RT-kernel patch. In <span id="S3.SS5.p2.1.2" class="ltx_text ltx_font_italic">L4T R32.4.2</span>, the RT kernel allows preemption for most system calls¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, giving more space to guarantee the deadlines for safety-critical applications.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para">
<p id="S3.SS5.p3.1" class="ltx_p">We choose four scheduling policies: <span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_italic">SCHED_OTHER</span>, <span id="S3.SS5.p3.1.2" class="ltx_text ltx_font_italic">SCHED_FIFO</span>, <span id="S3.SS5.p3.1.3" class="ltx_text ltx_font_italic">SCHED_RR</span>, and <span id="S3.SS5.p3.1.4" class="ltx_text ltx_font_italic">SCHED_DEADLINE</span>. <span id="S3.SS5.p3.1.5" class="ltx_text ltx_font_italic">SCHED_OTHER</span> is AGX‚Äôs default scheduling policy for user applications for maximum processor utilization. <span id="S3.SS5.p3.1.6" class="ltx_text ltx_font_italic">SCHED_FIFO</span> schedules in a first-come-first-serve method, while <span id="S3.SS5.p3.1.7" class="ltx_text ltx_font_italic">SCHED_RR</span> schedules in a round-robin way. <span id="S3.SS5.p3.1.8" class="ltx_text ltx_font_italic">SCHED_DEADLINE</span> is a CPU scheduler based on the Earliest Deadline First (EDF)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>. We choose two DNN models, PINet and YOLOv3, for the runtime profiling. The priorities for PINet and YOLOv3 are shown in Table¬†<a href="#S3.T7" title="TABLE VII ‚Ä£ III-E Runtime Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VII</span></a>. It shows that the priorities of <span id="S3.SS5.p3.1.9" class="ltx_text ltx_font_italic">SCHED_FIFO</span> and <span id="S3.SS5.p3.1.10" class="ltx_text ltx_font_italic">SCHED_RR</span> are all 99, while others are 0. For the setting up of deadlines, we choose two deadlines for each of them. Deadline-1 is set up based on the worst observed execution time (225ms for YOLOv3 and 300ms for PINet). Deadline-2 is based on the average end-to-end DNN inference time (200ms for YOLOv3 and 150ms for PINet). Under each scheduling policy, the mean value, 50, 80, and 99 percentiles, is used to show end-to-end latency distribution. Besides, the coefficient of variation is also calculated to show the quantified time variations.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.1" class="ltx_p">Since the competition of applications significantly impacts the execution time, we conduct experiments for each model in two steps. First, we run the model inference without competition, called the single test. Then we conduct the experiments with the resource competition from another DNN model, called the compete test. By default, the competition model will use <span id="S3.SS5.p4.1.1" class="ltx_text ltx_font_italic">SCHED_OTHER</span> as the scheduling policy.</p>
</div>
<figure id="S3.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>The scheduling setup for PINet and YOLOv3.</figcaption>
<div id="S3.T7.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:185.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(36.2pt,-15.5pt) scale(1.20033008769068,1.20033008769068) ;">
<table id="S3.T7.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T7.1.1.1.1" class="ltx_tr">
<th id="S3.T7.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S3.T7.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Scheduling Policies</span></th>
<th id="S3.T7.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt">
<table id="S3.T7.1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T7.1.1.1.1.2.1.1" class="ltx_tr">
<td id="S3.T7.1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T7.1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Priority</span></td>
</tr>
<tr id="S3.T7.1.1.1.1.2.1.2" class="ltx_tr">
<td id="S3.T7.1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S3.T7.1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">min/max</span></td>
</tr>
</table>
</th>
<th id="S3.T7.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T7.1.1.1.1.3.1" class="ltx_text ltx_font_bold">PINet</span></th>
<th id="S3.T7.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T7.1.1.1.1.4.1" class="ltx_text ltx_font_bold">YOLOv3</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T7.1.1.2.1" class="ltx_tr">
<th id="S3.T7.1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S3.T7.1.1.2.1.1.1" class="ltx_text ltx_font_italic" style="color:#23292E;">SCHED_OTHER</span></th>
<th id="S3.T7.1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">0/0</th>
<td id="S3.T7.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S3.T7.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0</td>
</tr>
<tr id="S3.T7.1.1.3.2" class="ltx_tr">
<th id="S3.T7.1.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T7.1.1.3.2.1.1" class="ltx_text ltx_font_italic" style="color:#23292E;">SCHED_FIFO</span></th>
<th id="S3.T7.1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1/99</th>
<td id="S3.T7.1.1.3.2.3" class="ltx_td ltx_align_center">99</td>
<td id="S3.T7.1.1.3.2.4" class="ltx_td ltx_align_center">99</td>
</tr>
<tr id="S3.T7.1.1.4.3" class="ltx_tr">
<th id="S3.T7.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S3.T7.1.1.4.3.1.1" class="ltx_text ltx_font_italic" style="color:#23292E;">SCHED_RR</span></th>
<th id="S3.T7.1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1/99</th>
<td id="S3.T7.1.1.4.3.3" class="ltx_td ltx_align_center">99</td>
<td id="S3.T7.1.1.4.3.4" class="ltx_td ltx_align_center">99</td>
</tr>
<tr id="S3.T7.1.1.5.4" class="ltx_tr">
<th id="S3.T7.1.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S3.T7.1.1.5.4.1.1" class="ltx_text ltx_font_italic" style="color:#23292E;">SCHED_DEADLINE</span></th>
<th id="S3.T7.1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t">0/0</th>
<td id="S3.T7.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S3.T7.1.1.5.4.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T7.1.1.5.4.3.1.1" class="ltx_tr">
<td id="S3.T7.1.1.5.4.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">0</td>
</tr>
<tr id="S3.T7.1.1.5.4.3.1.2" class="ltx_tr">
<td id="S3.T7.1.1.5.4.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">deadline-1: 300ms</td>
</tr>
<tr id="S3.T7.1.1.5.4.3.1.3" class="ltx_tr">
<td id="S3.T7.1.1.5.4.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">deadline-2: 150ms</td>
</tr>
</table>
</td>
<td id="S3.T7.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<table id="S3.T7.1.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T7.1.1.5.4.4.1.1" class="ltx_tr">
<td id="S3.T7.1.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">0</td>
</tr>
<tr id="S3.T7.1.1.5.4.4.1.2" class="ltx_tr">
<td id="S3.T7.1.1.5.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">deadline-1: 225ms</td>
</tr>
<tr id="S3.T7.1.1.5.4.4.1.3" class="ltx_tr">
<td id="S3.T7.1.1.5.4.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center">deadline-2: 200ms</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS5.p5" class="ltx_para ltx_noindent">
<p id="S3.SS5.p5.1" class="ltx_p"><span id="S3.SS5.p5.1.1" class="ltx_text ltx_font_bold">Time variations under RT kernel.</span>
The RT kernel results in latency‚Äôs mean, 50, 80, and 99 percentiles are shown in Figure¬†<a href="#S3.F12" title="Figure 12 ‚Ä£ III-E Runtime Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>.
Deadline-based scheduling shows the worst time variations among all the RT scheduling policies (FIFO, RR, and Deadline). One explanation is that the scheduler does not terminate tasks even when it has already passed the deadline. The variation of the deadline-based approach can be decreased with the termination, but it will reduce the FPS of the detection. Besides, deadline-based scheduling with the average time is much better than the deadline with the worst observed execution time, which raises another question in selecting an appropriate deadline. The quantified results for the coefficient of variation are shown in Table¬†<a href="#S3.T8" title="TABLE VIII ‚Ä£ III-E Runtime Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VIII</span></a>. The results prove our analysis above that the deadline-based scheduling shows the worst performance in variation. Besides, the DNN inference time variations are much more severe in the compete test than in the single test, mainly caused by contention for non-preemptive system resources. Analysis of the time variations for multi-tenant DNN inference exceeds the topic of this paper and will be our future work.</p>
</div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2209.05487/assets/x13.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="276" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Latency‚Äôs mean, 50, 80, and 99 percentiles of PINet and YOLOv3.</figcaption>
</figure>
<div id="S3.SS5.p6" class="ltx_para ltx_noindent">
<p id="S3.SS5.p6.1" class="ltx_p"><span id="S3.SS5.p6.1.1" class="ltx_text ltx_font_bold">Insight 4:</span>¬†<span id="S3.SS5.p6.1.2" class="ltx_text ltx_font_italic">Deadline-based scheduling shows more time variations than other real-time scheduling policies. Setting average time as the deadline has much fewer time variations than the worst observed execution time in DNN inference.</span></p>
</div>
<figure id="S3.T8" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE VIII: </span>Coefficient of variation under RT kernel.</figcaption>
<div id="S3.T8.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:112pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(7.8pt,-2.0pt) scale(1.03722880741784,1.03722880741784) ;">
<table id="S3.T8.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T8.1.1.1" class="ltx_tr">
<th id="S3.T8.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<math id="S3.T8.1.1.1.1.m1.1" class="ltx_Math" alttext="c_{v}" display="inline"><semantics id="S3.T8.1.1.1.1.m1.1a"><msub id="S3.T8.1.1.1.1.m1.1.1" xref="S3.T8.1.1.1.1.m1.1.1.cmml"><mi id="S3.T8.1.1.1.1.m1.1.1.2" xref="S3.T8.1.1.1.1.m1.1.1.2.cmml">c</mi><mi id="S3.T8.1.1.1.1.m1.1.1.3" xref="S3.T8.1.1.1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T8.1.1.1.1.m1.1b"><apply id="S3.T8.1.1.1.1.m1.1.1.cmml" xref="S3.T8.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T8.1.1.1.1.m1.1.1.1.cmml" xref="S3.T8.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T8.1.1.1.1.m1.1.1.2.cmml" xref="S3.T8.1.1.1.1.m1.1.1.2">ùëê</ci><ci id="S3.T8.1.1.1.1.m1.1.1.3.cmml" xref="S3.T8.1.1.1.1.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T8.1.1.1.1.m1.1c">c_{v}</annotation></semantics></math><span id="S3.T8.1.1.1.1.1" class="ltx_text ltx_font_bold"> - RT</span>
</th>
<th id="S3.T8.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T8.1.1.1.2.1" class="ltx_text ltx_font_bold">PINet-single</span></th>
<th id="S3.T8.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T8.1.1.1.3.1" class="ltx_text ltx_font_bold">PINet-compete</span></th>
<th id="S3.T8.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T8.1.1.1.4.1" class="ltx_text ltx_font_bold">YOLOv3-single</span></th>
<th id="S3.T8.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S3.T8.1.1.1.5.1" class="ltx_text ltx_font_bold">YOLOv3-compete</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T8.1.1.2.1" class="ltx_tr">
<td id="S3.T8.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T8.1.1.2.1.1.1" class="ltx_text ltx_font_italic">Deadline-1</span></td>
<td id="S3.T8.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.30</td>
<td id="S3.T8.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">0.33</td>
<td id="S3.T8.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">0.13</td>
<td id="S3.T8.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">0.21</td>
</tr>
<tr id="S3.T8.1.1.3.2" class="ltx_tr">
<td id="S3.T8.1.1.3.2.1" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.3.2.1.1" class="ltx_text ltx_font_italic">Deadline-2</span></td>
<td id="S3.T8.1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.3.2.2.1" class="ltx_text ltx_font_bold">0.44</span></td>
<td id="S3.T8.1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.3.2.3.1" class="ltx_text ltx_font_bold">0.44</span></td>
<td id="S3.T8.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.3.2.4.1" class="ltx_text ltx_font_bold">0.27</span></td>
<td id="S3.T8.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.3.2.5.1" class="ltx_text ltx_font_bold">0.23</span></td>
</tr>
<tr id="S3.T8.1.1.4.3" class="ltx_tr">
<td id="S3.T8.1.1.4.3.1" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.4.3.1.1" class="ltx_text ltx_font_italic">FIFO</span></td>
<td id="S3.T8.1.1.4.3.2" class="ltx_td ltx_align_center">0.33</td>
<td id="S3.T8.1.1.4.3.3" class="ltx_td ltx_align_center">0.31</td>
<td id="S3.T8.1.1.4.3.4" class="ltx_td ltx_align_center">0.02</td>
<td id="S3.T8.1.1.4.3.5" class="ltx_td ltx_align_center">0.14</td>
</tr>
<tr id="S3.T8.1.1.5.4" class="ltx_tr">
<td id="S3.T8.1.1.5.4.1" class="ltx_td ltx_align_center"><span id="S3.T8.1.1.5.4.1.1" class="ltx_text ltx_font_italic">OTHER</span></td>
<td id="S3.T8.1.1.5.4.2" class="ltx_td ltx_align_center">0.32</td>
<td id="S3.T8.1.1.5.4.3" class="ltx_td ltx_align_center">0.30</td>
<td id="S3.T8.1.1.5.4.4" class="ltx_td ltx_align_center">0.02</td>
<td id="S3.T8.1.1.5.4.5" class="ltx_td ltx_align_center">0.15</td>
</tr>
<tr id="S3.T8.1.1.6.5" class="ltx_tr">
<td id="S3.T8.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T8.1.1.6.5.1.1" class="ltx_text ltx_font_italic">RR</span></td>
<td id="S3.T8.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb">0.33</td>
<td id="S3.T8.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb">0.31</td>
<td id="S3.T8.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb">0.02</td>
<td id="S3.T8.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">0.15</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS6.4.1.1" class="ltx_text">III-F</span> </span><span id="S3.SS6.5.2" class="ltx_text ltx_font_italic">Hardware Variability</span>
</h3>

<div id="S3.SS6.p1" class="ltx_para">
<p id="S3.SS6.p1.1" class="ltx_p">Since most autonomous driving applications are DNN-based and require massive computations, the variability of the hardware in CPU, GPU, and memory configurations dramatically affects the DNN inference performance. This part discusses the hardware‚Äôs variability in two aspects: end-to-end latency for different devices and time variations under CPU and GPU-based architectures.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.1" class="ltx_p"><span id="S3.SS6.p2.1.1" class="ltx_text ltx_font_bold">End-to-end latency for different devices.</span> We use all four devices listed in Table¬†<a href="#S3.T2" title="TABLE II ‚Ä£ III-A Experiment Setup ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> for running PINet and YOLOv3 model inference experiments to collect the end-to-end latency with breakdowns. The results are shown in Figure¬†<a href="#S3.F13" title="Figure 13 ‚Ä£ III-F Hardware Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a>. <span id="S3.SS6.p2.1.2" class="ltx_text" style="color:#000000;">The GPU workstation performs the best in average end-to-end inference latency among all the devices. However, all devices show long tail latency with non-negligible time variations. For two-stage based PINet, the Fog Node tends to have a smaller time variation range than Jetson AGX and Xavier NX. The reason is that the post-processing mainly causes the time variations of PINet on the CPU side, while the Fog node has more powerful CPUs. For YOLOv3, which is one-stage based, the GPU desktop shows the lowest inference time variations because the inference part mainly dominates the time variations of the one-stage based model on the GPU side. The GPU workstation has much higher AI performance in TOPs than the other two Jetson boards.</span></p>
</div>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2209.05487/assets/x14.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span><span id="S3.F13.2.1" class="ltx_text" style="color:#000000;">The CDF of end-to-end latency for PINet and YOLOv3 with different hardware.</span></figcaption>
</figure>
<div id="S3.SS6.p3" class="ltx_para ltx_noindent">
<p id="S3.SS6.p3.1" class="ltx_p"><span id="S3.SS6.p3.1.1" class="ltx_text ltx_font_bold">Time variations under CPU &amp; GPU architecture.</span> Since the average latency of PINet and YOLOv3 on Jetson AGX, Fog Node, and GPU workstation are close. At the same time, the inference time variations have a big difference. We use these three devices to profile the impact of CPU and GPU architectures on DNN time variations. To begin with, we use <span id="S3.SS6.p3.1.2" class="ltx_text ltx_font_italic">nvprof</span> to profile the GPU activities on AGX Xavier. We found that for PINet and YOLOv3, only the inference part is executed on GPU while read, pre-processing, and post-processing are usually executed on the CPU. To find the lower level roots causing the time variations in post-processing, we use <span id="S3.SS6.p3.1.3" class="ltx_text ltx_font_italic">Linux perf</span> to monitor the system events when PINet and YOLOv3 models are executed with/without post-processing. The collected system events include the instructions, cache references, cache misses, CPU cycles, context switches, CPU migrations, etc.
Table¬†<a href="#S3.T9" title="TABLE IX ‚Ä£ III-F Hardware Variability ‚Ä£ III Model Inference Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IX</span></a> shows the ratio of collected events on DNN inference between with and without post-processing. The higher the value is, the more events happen during the post-processing. Based on the comparison results, we can find that more events happen in AGX Xavier than Fog node, including cache misses, instructions, cycles, context switches, CPU clocks, and CPU migrations. Events like CPU migrations, context switches, cycles, and CPU clocks directly contributed to the time variations in post-processing. The Fog node has a server-level CPU, while the AGX Xavier‚Äôs CPU is mainly used on embedding and mobile systems. We conclude that embedding systems on autonomous vehicles with more powerful CPUs could help decrease the time variations in DNN inference.</p>
</div>
<figure id="S3.T9" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IX: </span>The events comparison of running PINet and YOLOv3 on Jetson AGX and Fog Node with/without post-processing.</figcaption>
<div id="S3.T9.1" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:252.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.5pt,-36.4pt) scale(1.40494111257225,1.40494111257225) ;">
<table id="S3.T9.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T9.1.1.1.1" class="ltx_tr">
<th id="S3.T9.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.1.1.1.1" class="ltx_text ltx_font_italic">w. / w. o. post-process</span></th>
<td id="S3.T9.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="3"><span id="S3.T9.1.1.1.1.2.1" class="ltx_text ltx_font_bold">PINet</span></td>
<td id="S3.T9.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="S3.T9.1.1.1.1.3.1" class="ltx_text ltx_font_bold">YOLOv3</span></td>
</tr>
<tr id="S3.T9.1.1.2.2" class="ltx_tr">
<th id="S3.T9.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.1.1" class="ltx_text ltx_font_italic">metrics</span></th>
<td id="S3.T9.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.2.1" class="ltx_text ltx_font_bold">AGX</span></td>
<td id="S3.T9.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Fog</span></td>
<td id="S3.T9.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T9.1.1.2.2.4.1" class="ltx_text ltx_font_bold">GPU</span></td>
<td id="S3.T9.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.5.1" class="ltx_text ltx_font_bold">AGX</span></td>
<td id="S3.T9.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Fog</span></td>
<td id="S3.T9.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.2.2.7.1" class="ltx_text ltx_font_bold">GPU</span></td>
</tr>
<tr id="S3.T9.1.1.3.3" class="ltx_tr">
<th id="S3.T9.1.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T9.1.1.3.3.1.1" class="ltx_text ltx_font_italic">branch-misses</span></th>
<td id="S3.T9.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T9.1.1.3.3.2.1" class="ltx_text ltx_font_bold">1.18</span></td>
<td id="S3.T9.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.02</td>
<td id="S3.T9.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">1.07</td>
<td id="S3.T9.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1</td>
<td id="S3.T9.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.02</td>
<td id="S3.T9.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.02</td>
</tr>
<tr id="S3.T9.1.1.4.4" class="ltx_tr">
<th id="S3.T9.1.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.4.4.1.1" class="ltx_text ltx_font_italic">cache-misses</span></th>
<td id="S3.T9.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.04</td>
<td id="S3.T9.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
<td id="S3.T9.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T9.1.1.4.4.4.1" class="ltx_text ltx_font_bold">1.25</span></td>
<td id="S3.T9.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
<td id="S3.T9.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
<td id="S3.T9.1.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
</tr>
<tr id="S3.T9.1.1.5.5" class="ltx_tr">
<th id="S3.T9.1.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.5.5.1.1" class="ltx_text ltx_font_italic">cache-references</span></th>
<td id="S3.T9.1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.5.5.2.1" class="ltx_text ltx_font_bold">1.09</span></td>
<td id="S3.T9.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
<td id="S3.T9.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">1.07</td>
<td id="S3.T9.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.5.5.5.1" class="ltx_text ltx_font_bold">1.03</span></td>
<td id="S3.T9.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S3.T9.1.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.01</td>
</tr>
<tr id="S3.T9.1.1.6.6" class="ltx_tr">
<th id="S3.T9.1.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.6.6.1.1" class="ltx_text ltx_font_italic">instructions</span></th>
<td id="S3.T9.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.06</td>
<td id="S3.T9.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.04</td>
<td id="S3.T9.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T9.1.1.6.6.4.1" class="ltx_text ltx_font_bold">1.11</span></td>
<td id="S3.T9.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.6.6.5.1" class="ltx_text ltx_font_bold">1.04</span></td>
<td id="S3.T9.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S3.T9.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
</tr>
<tr id="S3.T9.1.1.7.7" class="ltx_tr">
<th id="S3.T9.1.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.7.7.1.1" class="ltx_text ltx_font_italic">cycles</span></th>
<td id="S3.T9.1.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.7.7.2.1" class="ltx_text ltx_font_bold">1.12</span></td>
<td id="S3.T9.1.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.02</td>
<td id="S3.T9.1.1.7.7.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">1.04</td>
<td id="S3.T9.1.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.7.7.5.1" class="ltx_text ltx_font_bold">1.04</span></td>
<td id="S3.T9.1.1.7.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S3.T9.1.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.02</td>
</tr>
<tr id="S3.T9.1.1.8.8" class="ltx_tr">
<th id="S3.T9.1.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.8.8.1.1" class="ltx_text ltx_font_italic">context-switches</span></th>
<td id="S3.T9.1.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.06</td>
<td id="S3.T9.1.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.31</td>
<td id="S3.T9.1.1.8.8.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T9.1.1.8.8.4.1" class="ltx_text ltx_font_bold">1.15</span></td>
<td id="S3.T9.1.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.8.8.5.1" class="ltx_text ltx_font_bold">1.36</span></td>
<td id="S3.T9.1.1.8.8.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.99</td>
<td id="S3.T9.1.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.15</td>
</tr>
<tr id="S3.T9.1.1.9.9" class="ltx_tr">
<th id="S3.T9.1.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.9.9.1.1" class="ltx_text ltx_font_italic">cpu-migrations</span></th>
<td id="S3.T9.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.95</td>
<td id="S3.T9.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.66</td>
<td id="S3.T9.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S3.T9.1.1.9.9.4.1" class="ltx_text ltx_font_bold">4.54</span></td>
<td id="S3.T9.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T9.1.1.9.9.5.1" class="ltx_text ltx_font_bold">1.10</span></td>
<td id="S3.T9.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.01</td>
<td id="S3.T9.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.03</td>
</tr>
<tr id="S3.T9.1.1.10.10" class="ltx_tr">
<th id="S3.T9.1.1.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T9.1.1.10.10.1.1" class="ltx_text ltx_font_italic">page-faults</span></th>
<td id="S3.T9.1.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1</td>
<td id="S3.T9.1.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.65</td>
<td id="S3.T9.1.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">1</td>
<td id="S3.T9.1.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1</td>
<td id="S3.T9.1.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1.04</td>
<td id="S3.T9.1.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.98</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S3.SS6.p4" class="ltx_para ltx_noindent">
<p id="S3.SS6.p4.1" class="ltx_p"><span id="S3.SS6.p4.1.1" class="ltx_text ltx_font_bold">Insight 5:</span>¬†<span id="S3.SS6.p4.1.2" class="ltx_text ltx_font_italic">The time variations in PINet are mainly caused by the post-processing on the CPU. Although GPU performs better in conducting matrix operations, having a powerful CPU in the autonomous vehicle embedding system would help to decrease the time variations in DNN inference.</span></p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">End-to-End System for Autonomous Driving</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Autonomous driving vehicles are composed of a variety of applications for sensing, perception, and control. The performance of the system is expected to rely on the coordination of several modules. To evaluate autonomous driving system‚Äôs variation, we propose to build an end-to-end prototype based on ROS in the NVIDIA Jetson AGX board with RT kernel.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Overview of the End-to-End System</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Based on the overview autonomous driving system in Figure¬†<a href="#S2.F1" title="Figure 1 ‚Ä£ II-A DNN Inference for Autonomous Driving ‚Ä£ II Background and Motivation ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we develop a ROS framework for the end-to-end system, as shown in Figure¬†<a href="#S4.F14" title="Figure 14 ‚Ä£ IV-A Overview of the End-to-End System ‚Ä£ IV End-to-End System for Autonomous Driving ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>. Since this paper focuses on the variations of DNN inference and the planning modules are all rule-based algorithms with stable execution time, the ROS framework does not include that part. The whole pipeline starts with the <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">/image</span> node, capturing images from the cameras or image files, and publishing it. Three perception nodes subscribe <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">/image_raw</span> messages and execute the DNN inference on the images. Three perception nodes are responsible for Simultaneous Localization and Mapping (SLAM), object detection, and semantic segmentation. After DNN inference, three nodes publish their results, covering the position information, objects, and semantics. Another node called <span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">/fusion</span> subscribes to these three nodes. It synchronizes them to get the sensor fusion results, which gives the control module the location of the vehicle and obstacles and open space for driving.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">In our design, the algorithm used for SLAM is ORB-SLAM2, which is a pure camera-based approach to capture key points in pixels and localize the vehicles and generate maps simultaneously¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. YOLOv3 is used for object detection, while Deeplabv3 is used for semantic segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<figure id="S4.F14" class="ltx_figure"><img src="/html/2209.05487/assets/x15.png" id="S4.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The ROS framework of the end-to-end system.</figcaption>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">ROS Nodes and Topics</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">A ROS node (i.e., the nodes in the figure) is a process to perform a particular computation while ROS topics (i.e., the arrows in the figure) are named buses for ROS nodes to exchange messages¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. In the end-to-end system, we implement five ROS nodes to access the sensor data and process the data: <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">/image</span>, <span id="S4.SS2.p1.1.2" class="ltx_text ltx_font_italic">/darknet_ros</span>, <span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_italic">/slam</span>, <span id="S4.SS2.p1.1.4" class="ltx_text ltx_font_italic">/segmentation</span>, and <span id="S4.SS2.p1.1.5" class="ltx_text ltx_font_italic">/fusion</span>. Publish-subscriber-based message sharing is used to transmit messages between these nodes.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">The ROS topics are defined to exchange messages between ROS nodes. Ten ROS topics are implemented to exchange messages, including images, positions, objects, and other customized messages. The summarized descriptions of some ROS topics are reported in Table¬†<a href="#S4.T10" title="TABLE X ‚Ä£ IV-B ROS Nodes and Topics ‚Ä£ IV End-to-End System for Autonomous Driving ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">X</span></a>. There are two messages based on <span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_typewriter">Image</span> type: header, height, width, encoding, data, etc. The ROS topic‚Äôs header contains the sequence ID, timestamp, and frame ID to represent a specific message. Since timestamp and sequence ID are needed for the synchronization, we implement <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_italic">/pose_timestamp</span> based on <span id="S4.SS2.p2.1.3" class="ltx_text ltx_font_italic">/pose</span>, which contains the position and orientation data. For object detection, bounding boxes are used to present the detected objects, which is determined by min and max values of the x and y-axis and the probability and the object class. <span id="S4.SS2.p2.1.4" class="ltx_text ltx_font_italic">/bounding_boxes</span> includes all the bounding boxes for one image and contains a header inside. For semantic segmentation, the results are shown as different colors inside the image to represent different segments. An <span id="S4.SS2.p2.1.5" class="ltx_text ltx_font_typewriter">Image</span>-based topic called <span id="S4.SS2.p2.1.6" class="ltx_text ltx_font_italic">/semantic</span> is used to represent the results with the message header.</p>
</div>
<figure id="S4.T10" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE X: </span>ROS topics used in the end-to-end system</figcaption>
<div id="S4.T10.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:161.4pt;vertical-align:-8.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-26.1pt,9.2pt) scale(0.892373366942689,0.892373366942689) ;">
<table id="S4.T10.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T10.1.1.1.1" class="ltx_tr">
<th id="S4.T10.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.1.1.1.1" class="ltx_text ltx_font_bold">ROS Topics</span></th>
<th id="S4.T10.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T10.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Library</span></th>
<th id="S4.T10.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T10.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Type</span></th>
<th id="S4.T10.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S4.T10.1.1.1.1.4.1" class="ltx_text ltx_font_bold">Fields</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T10.1.1.2.1" class="ltx_tr">
<td id="S4.T10.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.2.1.1.1" class="ltx_text ltx_font_italic">/image_raw</span></td>
<td id="S4.T10.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">sensor_msgs</td>
<td id="S4.T10.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Image</td>
<td id="S4.T10.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T10.1.1.2.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.1.1.2.1.4.1.1" class="ltx_tr">
<td id="S4.T10.1.1.2.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">header, height, width, encoding, data, etc</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T10.1.1.3.2" class="ltx_tr">
<td id="S4.T10.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.3.2.1.1" class="ltx_text ltx_font_italic">/pose_timestamp</span></td>
<td id="S4.T10.1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">geometry_msgs</td>
<td id="S4.T10.1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PoseStamped</td>
<td id="S4.T10.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">header, pose</td>
</tr>
<tr id="S4.T10.1.1.4.3" class="ltx_tr">
<td id="S4.T10.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.4.3.1.1" class="ltx_text ltx_font_italic">/pose</span></td>
<td id="S4.T10.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">geometry_msgs</td>
<td id="S4.T10.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Pose</td>
<td id="S4.T10.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T10.1.1.4.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.1.1.4.3.4.1.1" class="ltx_tr">
<td id="S4.T10.1.1.4.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">position (x, y, z float64),</td>
</tr>
<tr id="S4.T10.1.1.4.3.4.1.2" class="ltx_tr">
<td id="S4.T10.1.1.4.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">orientation (x, y, z, w, float64)</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T10.1.1.5.4" class="ltx_tr">
<td id="S4.T10.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.5.4.1.1" class="ltx_text ltx_font_italic">/bounding_boxes</span></td>
<td id="S4.T10.1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">darknet_ros_msgs</td>
<td id="S4.T10.1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BoundingBoxes</td>
<td id="S4.T10.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T10.1.1.5.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.1.1.5.4.4.1.1" class="ltx_tr">
<td id="S4.T10.1.1.5.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">header, image_header, bounding_boxes</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T10.1.1.6.5" class="ltx_tr">
<td id="S4.T10.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.6.5.1.1" class="ltx_text ltx_font_italic">/bounding_box</span></td>
<td id="S4.T10.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">darknet_ros_msgs</td>
<td id="S4.T10.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">BoundingBox</td>
<td id="S4.T10.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<table id="S4.T10.1.1.6.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.1.1.6.5.4.1.1" class="ltx_tr">
<td id="S4.T10.1.1.6.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">probability, xmin, ymin, xmax, ymax, id, class</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T10.1.1.7.6" class="ltx_tr">
<td id="S4.T10.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T10.1.1.7.6.1.1" class="ltx_text ltx_font_italic">/semantics</span></td>
<td id="S4.T10.1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">sensor_msgs</td>
<td id="S4.T10.1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Image</td>
<td id="S4.T10.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T10.1.1.7.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T10.1.1.7.6.4.1.1" class="ltx_tr">
<td id="S4.T10.1.1.7.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">header, height, width, encoding, data, etc</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Message Synchronization</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For ROS nodes that need to subscribe to multiple ROS topics and process them together, message synchronization becomes one of the implementation issue. Typically, message synchronization is based on the timestamp and sequence ID, so we convert the <span id="S4.SS3.p1.1.1" class="ltx_text ltx_font_italic">/pose</span> message to <span id="S4.SS3.p1.1.2" class="ltx_text ltx_font_italic">/pose_timestamp</span> to add on the header.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">The <span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_italic">/fusion</span> node‚Äôs objective is to combine all the perception results of the same image frame. The first thing is to make a unique ID for each image frame. In the beginning, the <span id="S4.SS3.p2.1.2" class="ltx_text ltx_font_italic">/image</span> node attaches timestamp information and frame ID to each message it publishes out. For three perception nodes, after DNN inference on the coming image frame, the timestamp and sequence ID of the coming images will be used as the header‚Äôs timestamp and sequence ID of the new message like <span id="S4.SS3.p2.1.3" class="ltx_text ltx_font_italic">/pose_timestamp</span>, <span id="S4.SS3.p2.1.4" class="ltx_text ltx_font_italic">/semantic</span>, etc. With unique IDs on each image frame and detection results, the remaining question is how to make them synchronized. In our design, we use <span id="S4.SS3.p2.1.5" class="ltx_text ltx_font_italic">message_filter<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text ltx_font_upright">1</span></span><span id="footnote1.4" class="ltx_text ltx_font_upright">http://wiki.ros.org/message_filters</span></span></span></span></span> with Approximate Time Synchronizer to manage the fusion process. The approximate synchronizer sets queue size as 100 and 100ms as the slop, which means the message with a time difference less than 100ms is considered synchronized.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">System-level Profiling</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Compared with the typical model inference, the end-to-end system is expected to have more uncertainties, contributing to higher variations. In this section, with the ROS-based end-to-end system, we investigate the time variation issues for the real autonomous driving system in two aspects: typical module time analysis and end-to-end system time analysis.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic">Latency Analysis for each Module</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">There are three typical modules in the ROS-based prototype: ORB-SLAM2 based localization and mapping, YOLOv3 based object detection, and Deeplabv3 for semantic segmentation. These modules subscribe to the image topic from the <span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_italic">/image</span> node and execute the DNN inference, which means the I/O changes into ROS communications. The total delay is defined as the difference between the model inference finish time and the image‚Äôs sending timestamp from <span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_italic">/image</span> node. In contrast, the total inference time is defined as the total time spent within the perception module. The results for running each module separately is shown in Figure¬†<a href="#S5.F15" title="Figure 15 ‚Ä£ V-A Latency Analysis for each Module ‚Ä£ V System-level Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a>. We can observe that for YOLOv3 and Deeplabv3, the difference between total delay and total inference is huge, while ORB-SLAM2 is small. The difference is because ROS communication has overhead in transmitting images using pub/sub mechanisms. It is supposed to convert RGB images into ROS images before publishing out and needs to convert it back to RGB images for DNN inference after receiving it. The results of <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="C_{v}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">C</mi><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">ùê∂</ci><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">C_{v}</annotation></semantics></math> present that the total delay variation is higher than the total inference for YOLOv3 and Deeplabv3.</p>
</div>
<figure id="S5.F15" class="ltx_figure"><img src="/html/2209.05487/assets/x16.png" id="S5.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="208" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>The latency analysis for each ROS perception module when running separately.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic">System Latency Analysis</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">With multiple perception modules subscribing to the <span id="S5.SS2.p1.1.1" class="ltx_text ltx_font_italic">/image_raw</span> topics simultaneously and the ROS node <span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_italic">/fusion</span> combines their results, the system latency is expected to show more variations than the separate module case. The results for the system latency of each module is shown in Figure¬†<a href="#S5.F16" title="Figure 16 ‚Ä£ V-B System Latency Analysis ‚Ä£ V System-level Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. We can find that the time variations of YOLOv3 and Deeplabv3‚Äôs total delay are much higher than the typical module case - with the highest delay almost attains 4000ms, which is dangerous for the safety-critical autonomous driving systems. Besides, the 99 percentile for YOLOv3 and Deeplabv3 is more than the combination of that in a typical module case, which implies huge tail latency caused by the accumulation of variations with the competing of concurrent tasks. <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="C_{v}" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><msub id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">C</mi><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">ùê∂</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">ùë£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">C_{v}</annotation></semantics></math> shows models‚Äô variations in which ORB-SLAM2 shows low variation while two DNN-based module show huge variations.</p>
</div>
<figure id="S5.F16" class="ltx_figure"><img src="/html/2209.05487/assets/x17.png" id="S5.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>The latency analysis for each ROS module when the end-to-end system is running.</figcaption>
</figure>
<figure id="S5.F17" class="ltx_figure"><img src="/html/2209.05487/assets/x18.png" id="S5.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>The delay between fusion messages.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">To understand the real impact of this enormous variation to the perception system, we record the timestamps for the <span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_italic">/fusion</span> node to get synchronized messages from three perception modules and generate the fusion data. Figure¬†<a href="#S5.F17" title="Figure 17 ‚Ä£ V-B System Latency Analysis ‚Ä£ V System-level Profiling ‚Ä£ Understanding Time Variations of DNN Inference in Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> shows the delays between the fusion messages in milliseconds. Queue size with 100 and 1000 of the buffer within the synchronizer are tested, respectively. The delay variations under queue size with 1000 are less than that with 100. For a queue size of 100, we can found that there are tremendous variations for the delay. The worst case of the fusion message‚Äôs delay goes to over 10000ms, which means that the control module gets the lane or objects‚Äô perception results after 10 seconds since the camera captures objects or lane. The average and minimum value of the delay also goes to 773ms and 242ms, respectively. This huge variation makes the control of the vehicle very challenging because it is hard to predict.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Insight 6:</span>¬†<span id="S5.SS2.p3.1.2" class="ltx_text ltx_font_italic">Communication middleware like ROS brings huge extra variations to the end-to-end autonomous driving systems. Long-tail latency exists, and it accumulates with competing with concurrent tasks. Adding queue size inside the synchronizer helps to reduce the variations.</span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Autonomous vehicles are proposed to understand the environment and drive without human intervention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>. DNNs play an essential role in the sensing, perception, decision, and control tasks in autonomous driving. Generally, the research on DNNs for autonomous driving can be divided into two categories: training DNN models with higher accuracy and improving the runtime performance of trained DNN models.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Many DNN-based algorithms are deployed in autonomous vehicles for object detection, lane detection, semantic segmentation, localization, etc.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. The object detection algorithms can be divided into two types: one-stage based algorithms like YOLO and SSD¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>; two-stage based algorithms like Fast R-CNN, Mask R-CNN, etc.¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. The key difference is whether there is a proposal bounding box stage. Semantic segmentation is used to detect driving segments. The fully convolutional neural network has been applied and achieves good performance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. LaneNet is a lane detection algorithm that uses an instance segmentation problem and applies image semantic segmentation algorithms¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. Another approach called PINet adds key points estimation with the instance segmentation¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">After the DNN models get trained, optimizing the model inference in latency, energy consumption, and memory utilization becomes a big challenge. In 2015, Han <span id="S6.p3.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> proposed pruning redundant connections and retraining the deep learning models to fine-tune the weights effectively, reducing computing complexity. Reducing the precision of operations and operands is another direction for the runtime optimization of DNN inference. Reducing precision is usually achieved by reducing the number of bits/levels representing the data, decreasing the computation requirements and storage costs¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>. Besides, the profiling of the DNN inference also gets more attention¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>. MLPerf targets a uniformed profiling benchmark for the machine learning algorithms at the edge¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">DNNs are widely used in autonomous driving due to their high accuracy for perception, decision, etc. Understanding the variation of the DNN inference of autonomous driving becomes a fundamental challenge in real-time and efficient scheduling. Non-negligible time variations are observed in DNN inference, which significantly challenges scheduling safety-critical tasks. Therefore, in this work, we analyze the time variation in DNN inference in fine granularity and derive six insights into understanding DNN inference time variations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
AMCL.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://wiki.ros.org/amcl</span>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
dwa_local_planner.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://wiki.ros.org/dwa\_local\_planner</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
global_planner.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://wiki.ros.org/global_planner</span>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
JetPack SDK 4.4 DP Archive.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://developer.nvidia.com/jetpack-sdk-44-dp-archive</span>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jetson Modules.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://developer.nvidia.com/embedded/jetson-modules</span>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
jetson_benchmarks.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://github.com/NVIDIA-AI-IOT/jetson\_benchmarks</span>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Navigation.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://wiki.ros.org/navigation</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
NVIDIA DRIVE AGX Developer Kit.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://developer.nvidia.com/drive/drive-agx</span>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
NVIDIA L4T ML.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ngc.nvidia.com/catalog/containers/nvidia:l4t-ml</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
TCPROS.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://wiki.ros.org/ROS/TCPROS</span>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
The KITTI Vision Benchmark Suite.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://www.cvlibs.net/datasets/kitti/</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
The Python Profilers.

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://docs.python.org/3.6/library/profile.html#module-cProfile</span>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Robot Operating System(ROS), Powering the World‚Äôs Robots, 2019.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Can Basaran and Kyoung-Don Kang.

</span>
<span class="ltx_bibblock">Supporting preemptive task executions and memory copies in GPGPUs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2012 24th Euromicro Conference on Real-Time Systems</span>, pages
287‚Äì296. IEEE, 2012.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Thomas Bradley.

</span>
<span class="ltx_bibblock">GPU performance analysis and optimisation.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">NVIDIA Corporation</span>, 2012.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Rethinking atrous convolution for semantic image segmentation.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1706.05587</span>, 2017.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yu-Hsin Chen, Joel Emer, and Vivienne Sze.

</span>
<span class="ltx_bibblock">Eyeriss: A spatial architecture for energy-efficient dataflow for
convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">ACM SIGARCH Computer Architecture News</span>, volume¬†44, pages
367‚Äì379. IEEE Press, 2016.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jeffrey Dean.

</span>
<span class="ltx_bibblock">The deep learning revolution and its implications for computer
architecture and chip design.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2020 IEEE International Solid-State Circuits
Conference-(ISSCC)</span>, pages 8‚Äì14. IEEE, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
ePROSIMA.

</span>
<span class="ltx_bibblock">eProsima Fast RTPS.

</span>
<span class="ltx_bibblock">[Online].

</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.eprosima.com/index.php/products-all/eprosima-fast-rtps</span>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Are we ready for autonomous driving? the KITTI vision benchmark
suite.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE
Conference on</span>, pages 3354‚Äì3361. IEEE, 2012.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Ionel Gog, Sukrit Kalra, Peter Schafhalter, Joseph¬†E Gonzalez, and Ion Stoica.

</span>
<span class="ltx_bibblock">D3: a dynamic deadline-driven approach for building autonomous
vehicles.

</span>
<span class="ltx_bibblock">In <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of the Seventeenth European Conference on
Computer Systems</span>, pages 453‚Äì471, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Brendan Gregg.

</span>
<span class="ltx_bibblock">Linux performance analysis and tools.

</span>
<span class="ltx_bibblock">Technical report, Technical report, Joyent, 2013.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Sorin Grigorescu, Bogdan Trasnea, Tiberiu Cocias, and Gigel Macesanu.

</span>
<span class="ltx_bibblock">A survey of deep learning techniques for autonomous driving.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Journal of Field Robotics</span>, 37(3):362‚Äì386, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Song Han, Jeff Pool, John Tran, and William¬†J Dally.

</span>
<span class="ltx_bibblock">Learning both weights and connections for efficient neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1506.02626</span>, 2015.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask R-CNN.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE International Conference on Computer
Vision (ICCV)</span>, pages 2961‚Äì2969, 2017.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Andrew¬†G Howard, Menglong Zhu, Bo¬†Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">MobileNets: efficient convolutional neural networks for mobile
vision applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Shinpei Kato, Eijiro Takeuchi, Yoshio Ishiguro, Yoshiki Ninomiya, Kazuya
Takeda, and Tsuyoshi Hamada.

</span>
<span class="ltx_bibblock">An open approach to autonomous vehicles.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Micro</span>, 35(6):60‚Äì68, 2015.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Yeongmin Ko, Jiwon Jun, Donghwuy Ko, and Moongu Jeon.

</span>
<span class="ltx_bibblock">Key points estimation and point instance segmentation approach for
lane detection.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.06604</span>, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey¬†E Hinton.

</span>
<span class="ltx_bibblock">ImageNet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
1097‚Äì1105, 2012.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Liangkai Liu, Jiamin Chen, Marco Brocanelli, and Weisong Shi.

</span>
<span class="ltx_bibblock">E2M: an energy-efficient middleware for computer vision
applications on autonomous mobile robots.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the 4th ACM/IEEE Symposium on Edge Computing
(SEC)</span>, pages 59‚Äì73, 2019.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Liangkai Liu, Sidi Lu, Ren Zhong, Baofu Wu, Yongtao Yao, Qingyang Zhang, and
Weisong Shi.

</span>
<span class="ltx_bibblock">Computing systems for autonomous driving: State-of-the-art and
challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.14349</span>, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Shaoshan Liu, Liyun Li, Jie Tang, Shuang Wu, and Jean-Luc Gaudiot.

</span>
<span class="ltx_bibblock">Creating autonomous vehicle systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Synthesis Lectures on Computer Science</span>, 6(1):i‚Äì186, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Shaoshan Liu, Liangkai Liu, Jie Tang, Bo¬†Yu, Yifan Wang, and Weisong Shi.

</span>
<span class="ltx_bibblock">Edge computing for autonomous driving: Opportunities and challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 107(8):1697‚Äì1716, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
Cheng-Yang Fu, and Alexander¬†C Berg.

</span>
<span class="ltx_bibblock">SSD: Single shot multibox detector.

</span>
<span class="ltx_bibblock">In <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">European conference on computer vision</span>, pages 21‚Äì37.
Springer, 2016.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Wei Liu, Hao Wu, Ziyue Jiang, Yifan Gong, and Jiangming Jin.

</span>
<span class="ltx_bibblock">A robotic communication middleware combining high performance and
high reliability.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">2020 IEEE 32nd International Symposium on Computer
Architecture and High Performance Computing (SBAC-PAD)</span>, pages 217‚Äì224.
IEEE, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.

</span>
<span class="ltx_bibblock">Fully convolutional networks for semantic segmentation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 3431‚Äì3440, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Raul Mur-Artal et¬†al.

</span>
<span class="ltx_bibblock">ORB-SLAM2: an open-source SLAM system for monocular, stereo and
rgb-d cameras.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.06475</span>, 2016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Davy Neven, Bert De¬†Brabandere, Stamatios Georgoulis, Marc Proesmans, and Luc
Van¬†Gool.

</span>
<span class="ltx_bibblock">Towards end-to-end lane detection: an instance segmentation approach.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">2018 IEEE Intelligent Vehicles Symposium (IV)</span>, pages
286‚Äì291. IEEE, 2018.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Gerardo Pardo-Castellote.

</span>
<span class="ltx_bibblock">Omg data-distribution service: Architectural overview.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">23rd International Conference on Distributed Computing
Systems Workshops, 2003. Proceedings.</span>, pages 200‚Äì206. IEEE, 2003.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Chang¬†Yun Park.

</span>
<span class="ltx_bibblock">Predicting deterministic execution times of real-time programs.

</span>
<span class="ltx_bibblock">1992.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy
Leibs, Rob Wheeler, and Andrew¬†Y Ng.

</span>
<span class="ltx_bibblock">ROS: an open-source robot operating system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">ICRA workshop on open source software</span>, volume¬†3, page¬†5.
Kobe, Japan, 2009.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Vijay¬†Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson, Guenther
Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark
Charlebois, William Chou, et¬†al.

</span>
<span class="ltx_bibblock">MLPerf inference benchmark.

</span>
<span class="ltx_bibblock">In <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">2020 ACM/IEEE 47th Annual International Symposium on Computer
Architecture (ISCA)</span>, pages 446‚Äì459. IEEE, 2020.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Joseph Redmon and Ali Farhadi.

</span>
<span class="ltx_bibblock">YOLOv3: an incremental improvement.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1804.02767</span>, 2018.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Federico Reghenzani, Giuseppe Massari, and William Fornaciari.

</span>
<span class="ltx_bibblock">The real-time linux kernel: A survey on Preempt_RT.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 52(1):1‚Äì36, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.

</span>
<span class="ltx_bibblock">Faster R-CNN: Towards real-time object detection with region
proposal networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, pages
91‚Äì99, 2015.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Marco Spuri and Giorgio¬†C Buttazzo.

</span>
<span class="ltx_bibblock">Efficient aperiodic service under earliest deadline scheduling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">RTSS</span>, pages 2‚Äì11, 1994.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel¬†S Emer.

</span>
<span class="ltx_bibblock">Efficient processing of deep neural networks: A tutorial and survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 105(12):2295‚Äì2329, 2017.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Maxime Tremblay, Shirsendu¬†Sukanta Halder, Raoul de¬†Charette, and
Jean-Fran√ßois Lalonde.

</span>
<span class="ltx_bibblock">Rain rendering for evaluating and improving robustness to bad
weather.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">International Journal of Computer Vision</span>, 129(2):341‚Äì360,
2021.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Chengcheng Wan, Muhammad Santriaji, Eri Rogers, Henry Hoffmann, Michael Maire,
and Shan Lu.

</span>
<span class="ltx_bibblock">ALERT: Accurate learning for energy and timeliness.

</span>
<span class="ltx_bibblock">In <span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">2020 USENIX Annual Technical Conference (USENIX ATC 20)</span>,
pages 353‚Äì369, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Reinhard Wilhelm, Jakob Engblom, Andreas Ermedahl, Niklas Holsti, Stephan
Thesing, David Whalley, Guillem Bernat, Christian Ferdinand, Reinhold
Heckmann, Tulika Mitra, et¬†al.

</span>
<span class="ltx_bibblock">The worst-case execution-time problem‚Äîoverview of methods and
survey of tools.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Embedded Computing Systems (TECS)</span>,
7(3):1‚Äì53, 2008.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Carole-Jean Wu, David Brooks, Kevin Chen, Douglas Chen, Sy¬†Choudhury, Marat
Dukhan, Kim Hazelwood, Eldad Isaac, Yangqing Jia, Bill Jia, et¬†al.

</span>
<span class="ltx_bibblock">Machine learning at Facebook: Understanding inference at the edge.

</span>
<span class="ltx_bibblock">In <span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">2019 IEEE International Symposium on High Performance
Computer Architecture (HPCA)</span>, pages 331‚Äì344. IEEE, 2019.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Tianze Wu, Baofu Wu, Sa¬†Wang, Liangkai Liu, Shaoshan Liu, Yungang Bao, and
Weisong Shi.

</span>
<span class="ltx_bibblock">Oops! it‚Äôs too late. your autonomous driving system needs a faster
middleware.

</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">IEEE Robotics and Automation Letters</span>, 6(4):7301‚Äì7308, 2021.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan¬†Z Li.

</span>
<span class="ltx_bibblock">Single-shot refinement neural network for object detection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span>, pages 4203‚Äì4212, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2209.05486" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2209.05487" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2209.05487">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2209.05487" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2209.05488" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 21:38:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
