<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery</title>
<!--Generated on Tue Oct  8 06:23:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Road lane extraction,  aerial imagery,  deep learning,  semantic segmentation,  transfer learning.
" lang="en" name="keywords"/>
<base href="/html/2410.05717v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S1" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S2" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S2.SS1" title="In II Related Works ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Datasets for Lane Segmentation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S2.SS2" title="In II Related Works ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Existing Semantic Segmentation Models</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S2.SS3" title="In II Related Works ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Evolution of Lane Marking Detection Methods </span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S3" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Description of Training Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S3.SS1" title="In III Description of Training Datasets ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">SkyScapes Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S3.SS2" title="In III Description of Training Datasets ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Waterloo Urban Scene Dataset</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Methodology for Automated Lane Marking Extraction</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.SS1" title="In IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Description of the General Workflow</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.SS2" title="In IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Benchmarked Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.SS2.SSS1" title="In IV-B Benchmarked Models ‣ IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>1 </span>CNN-based models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.SS2.SSS2" title="In IV-B Benchmarked Models ‣ IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span>2 </span>Transformer-based Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.SS3" title="In IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Experiment Details</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results and Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS1" title="In V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Model Performance and Adaptation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS1.SSS1" title="In V-A Model Performance and Adaptation ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span>1 </span>SkyScapes Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS1.SSS2" title="In V-A Model Performance and Adaptation ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span>2 </span>Waterloo Urban Scene Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS2" title="In V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Visualization of Results </span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS2.SSS1" title="In V-B Visualization of Results ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>1 </span>SkyScapes Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS2.SSS2" title="In V-B Visualization of Results ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span>2 </span>Waterloo Urban Scene Dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS3" title="In V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Discussion</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS3.SSS1" title="In V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>1 </span>Dataset and Annotation Quality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS3.SSS2" title="In V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>2 </span>Model Architecture and Benchmark Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.SS3.SSS3" title="In V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span>3 </span>Transfer Learning and Partially Labelled Dataset</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S6" title="In Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuanchen (Willow) Liu, Shuxin Qiao, Kyle Gao, Hongjie He, Michael A. Chapman, Linlin Xu, and Jonathan Li
</span><span class="ltx_author_notes">Manuscript received April 19, 2021; revised August 16, 2021.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This research addresses the need for high-definition (HD) maps for autonomous vehicles (AVs), focusing on road lane information derived from aerial imagery. While Earth observation data offers valuable resources for map creation, specialized models for road lane extraction are still underdeveloped in remote sensing. In this study, we perform an extensive comparison of twelve foundational deep learning-based semantic segmentation models for road lane marking extraction from high-definition remote sensing images, assessing their performance under transfer learning with partially labeled datasets. These models were fine-tuned on the partially labeled Waterloo Urban Scene dataset, and pre-trained on the SkyScapes dataset, simulating a likely scenario of real-life model deployment under partial labeling. We observed and assessed the fine-tuning performance and overall performance. Models showed significant performance improvements after fine-tuning, with mean IoU scores ranging from 33.56% to 76.11%, and recall ranging from 66.0% to 98.96%. Transformer-based models outperformed convolutional neural networks, emphasizing the importance of model pre-training and fine-tuning in enhancing HD map development for AV navigation.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Road lane extraction, aerial imagery, deep learning, semantic segmentation, transfer learning.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid development of autonomous driving technologies highlights the critical role of high-definition (HD) maps, essential for the safe and efficient operation of autonomous vehicles (AVs). These maps provide precise environmental details, such as traffic signals, road features, and lane markings, enabling AVs to navigate accurately. While Earth observation data, like LiDAR and satellite imagery, are valuable for HD map creation, they have limitations such as uneven point distribution and lower spatial resolution. Aerial imagery from drones or UAVs offers a superior alternative with higher resolution and broader coverage, capturing lane markings more effectively.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Integrating AI and deep learning is crucial for processing this data and automating the extraction of key features, although challenges remain in dense urban environments. Traditional remote sensing methods for lane marking extraction are limited, whereas computer vision advances in semantic segmentation offer promising but underexplored solutions for this task.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">This study addresses the gap by conducting a comparative analysis of twelve deep learning-based semantic segmentation models tailored to road lane extraction from aerial imagery. Using transfer learning on a partially labeled dataset, the research evaluates the models’ performance across two datasets, providing a benchmark for future work in HD map development for AVs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Road lanes are essential components of road infrastructure, delineating paths for vehicle movement and facilitating smooth traffic flow while conveying important traffic regulations. These lanes are marked with various symbols on the road surface, which can differ significantly in shape, size, length, and color, reflecting the diversity of traffic rules and cultural norms globally. From dashed lines to solid lines, and from arrows to pedestrian crossings, each marking serves a specific purpose and is geometrically designed with clear boundaries to ensure visibility.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Datasets for Lane Segmentation</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">In the context of enhancing lane segmentation model performance, selecting appropriate training datasets is crucial. These datasets must provide high-resolution images that reveal detailed lane features, offer annotations for various lane types, be readily accessible for research purposes, and be sufficiently large to support effective model training and validation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">HD map data is mainly collected using mobile mapping systems equipped with sensors like LiDAR. Ground-level datasets such as the TuSimple Lane Detection Challenge Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib1" title="">1</a>]</cite>, the Road and Lane Dataset from the Karlsruhe Institute of Technology and Toyota Technological Institute (KITTI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib2" title="">2</a>]</cite>, the California Institute of Technology (Caltech) Lanes Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib3" title="">3</a>]</cite>, and the Berkeley Deep Drive 100K (BDD100K) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib4" title="">4</a>]</cite> are common but lack comprehensive coverage for extensive traffic management, often hindered by limited field of view and physical obstructions, making urban mapping laborious and resource-intensive. In contrast, aerial image datasets are crucial for large-scale traffic management systems. Despite various available sources, detailed annotations for lane markings are scarce. Notable datasets like the International Society for Photogrammetry and Remote Sensing (ISPRS) Potsdam and Vaihingen dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib5" title="">5</a>]</cite> provide high-resolution images but lack lane-specific information, while Massachusetts Roads <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib6" title="">6</a>]</cite> and SpaceNet datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib7" title="">7</a>]</cite> offer urban annotations without sufficient detail for precise segmentation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The SkyScapes dataset stands out with its 13 cm resolution images and detailed annotations across 12 classes of lane markings in both urban and suburban environments. It is freely accessible, making it an invaluable resource for accurate detection and classification of road lanes from an aerial perspective.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Existing Semantic Segmentation Models</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In computer vision, semantic segmentation is crucial for complex real-world applications, offering pixel-wise labeling that divides images into segments corresponding to different objects or regions. Deep learning has transformed this field by moving from traditional clustering and contour-based methods to more precise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib8" title="">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib9" title="">9</a>]</cite>, pixel-level segmentation, thereby enhancing accuracy and scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Semantic segmentation models are typically divided into Convolutional Neural Network (CNN)-based and Transformer-based. Significant advancements in CNNs, particularly through the development of Encoder-Decoder architecture, have greatly influenced semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib11" title="">11</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib12" title="">12</a>]</cite>. Models like the Fully Convolutional Network (FCN) modify CNNs to handle images of any size by substituting fully connected layers with convolutional layers for size-invariant segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib13" title="">13</a>]</cite>. Other notable models include FastFCN, which introduces Joint Pyramid Upsampling to merge multi-scale features efficiently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib14" title="">14</a>]</cite>; U-Net, known for its skip connections and symmetric encoder-decoder structure which preserve spatial context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib12" title="">12</a>]</cite>; MobileNetV3, which optimizes for mobile devices by integrating Hardware-aware Network Architecture Search (NAS) and the NetAdapt algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib15" title="">15</a>]</cite>; ANN (Asymmetric Non-local Neural Networks), which reduces computational demands through pyramid sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib16" title="">16</a>]</cite>; and DeepLabV3 with its atrous convolutions for enhanced receptive fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib17" title="">17</a>]</cite>. Additional advancements include DeepLabV3+, which adds an encoder-decoder structure for refined edge detailing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib18" title="">18</a>]</cite>, PSPNet that incorporates a Pyramid Pooling Module for aggregating multi-scale contextual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib19" title="">19</a>]</cite>, and SegNeXt, which introduces a convolutional attention mechanism to improve computational efficiency while maintaining segmentation accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The landscape of semantic segmentation underwent further evolution with the introduction of Transformer-based models, which apply the self-attention mechanism to image data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib21" title="">21</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib22" title="">22</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib23" title="">23</a>]</cite>, excelling at modeling long-range dependencies and capturing global context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib24" title="">24</a>]</cite>. Vision Transformer (ViT) treats images as sequences of patches, applying self-attention to capture complex spatial relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib21" title="">21</a>]</cite>. Innovations like the Twins-SVT streamline spatial attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib18" title="">18</a>]</cite>, while models like SegFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib25" title="">25</a>]</cite> and Swin Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib26" title="">26</a>]</cite> integrate advanced features that optimize segmentation tasks by efficiently processing images at varying resolutions and combining multi-scale features.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.4.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.5.2">Evolution of Lane Marking Detection Methods </span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Deep learning has significantly advanced the analysis of aerial imagery for road extraction, a fundamental step towards accurate lane detection. Techniques such as the Multi-Feature Pyramid Network (MFPN) proposed by have effectively managed varying road widths <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib27" title="">27</a>]</cite> and incorporated semantic segmentation and tensor voting to connect fragmented road segments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib28" title="">28</a>]</cite>. Furthermore, Generative Adversarial Networks (GANs) have facilitated the extraction of detailed road features, including pavement and centerlines, demonstrating deep learning’s expanding role in comprehensive road network mapping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib29" title="">29</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Research on road lane detection using aerial imagery remains relatively sparse, yet Azimi et al. (2019) have made notable contributions with their innovations such as Aerial LaneNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib30" title="">30</a>]</cite> and SkyScapesNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib6" title="">6</a>]</cite>. Aerial LaneNet employs Symmetric Fully Convolutional Neural Networks (FCNN) enhanced with Wavelet Transforms, focusing primarily on the binary classification of lane markings against the background . Building upon this foundation, SkyScapesNet, which is based on the Fully Convolutional DenseNet (FC-DenseNet) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib31" title="">31</a>]</cite>, leverages the SkyScapes dataset to combine dense semantic segmentation with semantic edge detection, enabling the segmentation of multiple road lane classes and the identification of small-scale urban features.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Description of Training Datasets</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">SkyScapes Dataset</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The SkyScapes Dataset consists of a comprehensive collection of aerial images captured over Munich, Germany <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib6" title="">6</a>]</cite>. This dataset provides a detailed aerial view of the city’s extensive transportation infrastructure within its urban and rural areas. Captured with a helicopter-mounted Digital Single-Lens Reflex (DSLR) camera system, the SkyScapes Dataset includes 16 RGB images, each with a resolution of 5,616×3,744 pixels, offering a ground sampling distance of 13 cm per pixel. It spans an area of 5.7 km².</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">This dataset is manually annotated with 31 semantic categories, focusing on elements typical in urban settings, such as various types of roads, parking spaces, bikeways, sidewalks, buildings, and different vehicle types. It includes a detailed categorization of 12 different lane marking types, such as dash-lines, crosswalks, stop-lines, and parking zones.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Waterloo Urban Scene Dataset</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Waterloo, located in Ontario, Canada, is part of the Waterloo-Kitchener area. It features an extensive network of roads and significant highways that connect it with the Greater Toronto Area and nearby regions.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Derived from the readily available Waterloo Building Dataset, the Waterloo Urban Scene Dataset offers high-resolution aerial ortho imagery of the Waterloo region <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib32" title="">32</a>]</cite>. Covering an extensive area of 205.8 km² and providing a fine spatial resolution of 12 cm per pixel, this dataset is perfectly suited for urban and traffic semantic segmentation projects.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">To tailor the Waterloo Urban Scene Dataset for this research, we enhanced it with manually added annotations to establish a comprehensive ground truth, essential for assessing the developed model’s effectiveness across diverse datasets. Originally lacking specific traffic classifications, 14 new classes were introduced, including various road and lane markings, vehicles, sidewalks, crosswalks, and traffic islands.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">The annotation process was meticulously organized into three main categories to improve clarity and specificity: facility types (e.g., roads, sidewalks, traffic islands), road lane markings, and vehicles. This categorization reflects actual conditions and aids in precise model evaluation. An essential aspect of our annotation approach was the establishment of a priority system for overlapping classes in the imagery, crucial for resolving ambiguities where a pixel belongs to multiple classes. It is noteworthy that even if background elements are obscured by foreground components—for instance, when a vehicle overlays a single solid line—the background element, such as the line, is still labeled entirely with overlapping class polygons.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Methodology for Automated Lane Marking Extraction</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Description of the General Workflow</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Both datasets underwent data augmentation and parameter calculation before entering the training phase. Original images were flipped and cropped for data augmentation. For parameter calculation, the mean and standard deviation were computed on the training images to normalize the inputs for the both training and validation phase, which helps convergence.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Following this, twelve comparative models were trained using the Skyscapes dataset and evaluated to produce performance metrics on the Skyscapes test dataset. Then, these twelve models, initialized with weights from their training on the Skyscapes dataset, were fine-tuned on the Waterloo Urban Scenes dataset, after which their performance metrics were generated for this dataset.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Benchmarked Models</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Due to having twelve models in our benchmark, for conciseness, we will offer a brief summary of the models’ architecture and properties without diagrams or equations. More details on the respective architectures can be found in the original publications. Details on the computational requirements of these models can be found in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.T1" title="TABLE I ‣ IV-B2 Transformer-based Models ‣ IV-B Benchmarked Models ‣ IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS1.4.1.1">IV-B</span>1 </span>CNN-based models</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Convolutional Neural Network (CNN) in semantic segmentation are designed to classify each pixel in an image into a specific category, thus creating a detailed pixel-wise segmentation of the image. By utilizing convolutional layers to extract features at different spatial scales and pooling layers to downsample the features, CNNs can reasonably capture both local and global context information within an image. Additionally, the use of skip connections, such as in U-Net architecture, helps preserve spatial information and improve segmentation accuracy. Overall, CNNs have proven to be highly effective for semantic segmentation tasks in various domains such as medical imaging, autonomous driving, and satellite imagery analysis</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">FCN (Fully Convolutional Network):</span> This model modifies CNNs to process full images directly. By substituting fully connected layers with convolutional layers, the model outputs spatial maps suitable for inputs of any size, a significant advancement over traditional CNNs that demand fixed-size inputs (Long et al., 2015). This ”fully convolutional” design enables adaptable and size-invariant segmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">FastFCN (Fast Fully Convolutional Network):</span> This model introduces critical innovations over conventional CNN approaches (Wu et al., 2019). Its main advancement is the Joint Pyramid Upsampling (JPU) module, which efficiently merges multi-scale features, bypassing the extensive pooling and upsampling layers typical of CNNs and FCNs. This allows for high-resolution semantic segmentation with reduced computational demand, facilitating quicker processing speeds than traditional models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p4.1.1">U-Net:</span> This model leverages a convolutional neural network (CNN) in a symmetric encoder-decoder framework (Ronneberger et al., 2015). It adds skip connections which concatenate feature maps from the encoding path with the decoder’s upsampled output, enhancing detail localization. This structure effectively maintains spatial context, addressing the common challenge of detail loss in deeper layers found in traditional CNN segmentation approaches.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p5.1.1">MobileNetV3:</span> This model innovates within CNN architectures, optimizing for mobile device constraints without compromising performance (Howard et al., 2019). By integrating Hardware-Aware Network Architecture Search (NAS) and the NetAdapt algorithm, the algorithm fine-tunes its structure for optimal functionality on mobile CPUs. MobileNetV3 introduces architectural enhancements, such as the Lite Reduced Atrous Spatial Pyramid Pooling (LR-ASPP), to improve semantic segmentation efficiency. Additionally, a streamlined segmentation decoder is incorporated to boost performance in dense pixel prediction tasks, ensuring computational efficiency is maintained.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p6">
<p class="ltx_p" id="S4.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p6.1.1">ANN (Asymmetric Non-local Neural Networks):</span> This model is a CNN-based framework that introduces two innovations: the Asymmetric Pyramid Non-local Block (APNB) and the Asymmetric Fusion Non-local Block (AFNB) (Zhu et al., 2019). APNB reduces computation and memory usage by applying pyramid sampling to non-local blocks, maintaining performance while addressing the high resource demands of traditional non-local operations. AFNB improves segmentation by fusing multi-level features and addressing long-range dependencies, overcoming typical CNN limitations in capturing these dependencies efficiently.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p7">
<p class="ltx_p" id="S4.SS2.SSS1.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p7.1.1">DeepLabV3:</span> This model marks a notable development in semantic segmentation, integrating atrous convolutions and Atrous Spatial Pyramid Pooling (ASPP) within a CNN architecture (Chen et al., 2017a). Atrous convolutions are employed to broaden the receptive field, preserving the resolution of feature maps, and enhancing the model’s ability to assimilate expansive contextual details without downsampling. The ASPP module leverages atrous convolutions at varied dilation rates to efficiently capture information across multiple scales, ensuring precise segmentation of objects of different sizes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p8">
<p class="ltx_p" id="S4.SS2.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p8.1.1">DeepLabV3+:</span> This model enhances DeepLabV3 by adding an encoder-decoder structure for better detail and edge precision in semantic segmentation (Chu et al., 2021). It improves on outlining object boundaries and pixel labelling by refining the ASPP module with a decoder to efficiently capture object edges. Depth-wise separable convolution in the ASPP and decoder minimizes computational complexity, ensuring efficient, high-performance segmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS1.p9">
<p class="ltx_p" id="S4.SS2.SSS1.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p9.1.1">Pyramid Scene Parsing Network (PSPNet):</span> This model employs a CNN-based structure, elevating scene parsing through its Pyramid Pooling Module, which aggregates multi-scale contextual information for superior global comprehension (Zhao et al., 2017). Utilizing features from four distinct pyramid scales, it captures a wide array of global details, essential for the precise parsing and interpretation of complex scenes. This strategic approach overcomes the challenge of fusing global contextual insights, significantly boosting scene parsing accuracy by thoroughly analyzing the interconnected relationships present within images.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p10">
<p class="ltx_p" id="S4.SS2.SSS1.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p10.1.1">SegNeXt:</span> This model is CNN-based and introduces a novel convolutional attention mechanism to enhance computational efficiency (Guo et al., 2022). It leverages convolutional operations for spatial hierarchy management and local feature extraction, key for segmentation, avoiding the computational load of transformers’ self-attention. This mechanism efficiently encodes spatial context with specialized convolutions, aiming to balance computational and parameter efficiency with high segmentation accuracy across various datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S4.SS2.SSS2.4.1.1">IV-B</span>2 </span>Transformer-based Models</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">The image Transformer is a relatively novel type of deep learning architecture that leverages the transformer-based architecture, originally designed for natural language processing tasks, for semantic segmentation in images. First introduced by <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">vit</span>]</cite>, by treating the image as a sequence of patches and applying self-attention mechanisms, these image Transformers can capture long-range dependencies and context information effectively. This family of model has shown promising results in semantic segmentation tasks by enabling efficient processing of spatial relationships and capturing global context within the image. The image Transformers have the potential to outperform traditional convolutional neural networks in certain segmentation tasks by offering a different approach to feature extraction and context aggregation in images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">Twins-SVT:</span> This model, particularly through its variants Twins Pooled Convolutional Pyramid Vision Transformer (Twins-PCPVT) and Twins Scaled Vision Transformer (Twins-SVT), revolutionizes spatial attention in vision transformers with a novel and streamlined design (Chu et al., 2021). This innovation is marked by a simplified yet potent spatial attention mechanism that stands in contrast to the complex and resource-intensive approaches of traditional models. By employing a direct and efficient spatial attention strategy, both Twins-PCPVT and Twins-SVT architectures achieve high computational efficiency through optimized matrix multiplications, ensuring robust model performance without the burden of excessive computational demands.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.1">Segmenting Transformers (SegFormer):</span> This model is transformer-based and capitalizes on self-attention mechanisms to grasp global dependencies for improved scene understanding (Xie et al., 2021). It innovatively omits positional encoding, avoiding issues related to varying input image resolutions during testing. Additionally, SegFormer integrates a lightweight Multi-Layer Perceptron (MLP) decoder to blend multiscale features from the encoder, efficiently marrying local and global context for accurate segmentation outcomes.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p4.1.1">Shifted Window (Swin) Transformer:</span> This model is transformer-based and features a novel shifted window design that diverges from the fixed-size patches (Liu et al., 2021). This design enables adaptive feature extraction across scales - which is essential for semantic segmentation. It achieves linear computational complexity with image size, improving upon the quadratic complexity of standard transformers. Its hierarchical structure facilitates efficient processing of images at different resolutions, effectively extracting local and global features with enhanced accuracy and scalability.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Summary of benchmarked models: computational requirements and weight of training objectives</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.1.1.1"><span class="ltx_text" id="S4.T1.3.1.1.1.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.1.1.2"><span class="ltx_text" id="S4.T1.3.1.1.2.1" style="font-size:90%;">Base</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.1.1.3"><span class="ltx_text" id="S4.T1.3.1.1.3.1" style="font-size:90%;">FLOPS</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.3.1.1.4"><span class="ltx_text" id="S4.T1.3.1.1.4.1" style="font-size:90%;">Parameters</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.1.1.5"><span class="ltx_text" id="S4.T1.3.1.1.5.1" style="font-size:90%;">Cross Entropy</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.1.1.6"><span class="ltx_text" id="S4.T1.3.1.1.6.1" style="font-size:90%;">Dice</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.1.1.7"><span class="ltx_text" id="S4.T1.3.1.1.7.1" style="font-size:90%;">Focal</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.1.1.8"><span class="ltx_text" id="S4.T1.3.1.1.8.1" style="font-size:90%;">Tversky</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.3.2.1.1">
<span class="ltx_text" id="S4.T1.3.2.1.1.1" style="font-size:90%;">FCN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.2.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib13" title="">13</a><span class="ltx_text" id="S4.T1.3.2.1.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.2.1.2"><span class="ltx_text" id="S4.T1.3.2.1.2.1" style="font-size:90%;">ResNet101</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.2.1.3"><span class="ltx_text" id="S4.T1.3.2.1.3.1" style="font-size:90%;">276 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.2.1.4"><span class="ltx_text" id="S4.T1.3.2.1.4.1" style="font-size:90%;">66.1 M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.2.1.5"><span class="ltx_text" id="S4.T1.3.2.1.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.2.1.6"><span class="ltx_text" id="S4.T1.3.2.1.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_border_t" id="S4.T1.3.2.1.7"></td>
<td class="ltx_td ltx_border_t" id="S4.T1.3.2.1.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.3.2.1">
<span class="ltx_text" id="S4.T1.3.3.2.1.1" style="font-size:90%;">FastFCN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.3.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib14" title="">14</a><span class="ltx_text" id="S4.T1.3.3.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.2.2"><span class="ltx_text" id="S4.T1.3.3.2.2.1" style="font-size:90%;">ResNet50</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2.3"><span class="ltx_text" id="S4.T1.3.3.2.3.1" style="font-size:90%;">94.6 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.3.2.4"><span class="ltx_text" id="S4.T1.3.3.2.4.1" style="font-size:90%;">53.3 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2.5"><span class="ltx_text" id="S4.T1.3.3.2.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.2.6"><span class="ltx_text" id="S4.T1.3.3.2.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.3.2.7"></td>
<td class="ltx_td" id="S4.T1.3.3.2.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.4.3.1">
<span class="ltx_text" id="S4.T1.3.4.3.1.1" style="font-size:90%;">U-Net </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.4.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib12" title="">12</a><span class="ltx_text" id="S4.T1.3.4.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.4.3.2"><span class="ltx_text" id="S4.T1.3.4.3.2.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.4.3.3"><span class="ltx_text" id="S4.T1.3.4.3.3.1" style="font-size:90%;">203 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.4.3.4"><span class="ltx_text" id="S4.T1.3.4.3.4.1" style="font-size:90%;">29.0 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.4.3.5"><span class="ltx_text" id="S4.T1.3.4.3.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.4.3.6"><span class="ltx_text" id="S4.T1.3.4.3.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.4.3.7"></td>
<td class="ltx_td" id="S4.T1.3.4.3.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.5.4.1">
<span class="ltx_text" id="S4.T1.3.5.4.1.1" style="font-size:90%;">DeepLabV3 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.5.4.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib17" title="">17</a><span class="ltx_text" id="S4.T1.3.5.4.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.5.4.2"><span class="ltx_text" id="S4.T1.3.5.4.2.1" style="font-size:90%;">ResNet101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.4.3"><span class="ltx_text" id="S4.T1.3.5.4.3.1" style="font-size:90%;">346 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.5.4.4"><span class="ltx_text" id="S4.T1.3.5.4.4.1" style="font-size:90%;">84.7 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.4.5"><span class="ltx_text" id="S4.T1.3.5.4.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.5.4.6"><span class="ltx_text" id="S4.T1.3.5.4.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.5.4.7"></td>
<td class="ltx_td" id="S4.T1.3.5.4.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.6.5.1">
<span class="ltx_text" id="S4.T1.3.6.5.1.1" style="font-size:90%;">DeepLabV3+ </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.6.5.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib18" title="">18</a><span class="ltx_text" id="S4.T1.3.6.5.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.6.5.2"><span class="ltx_text" id="S4.T1.3.6.5.2.1" style="font-size:90%;">ResNet101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.5.3"><span class="ltx_text" id="S4.T1.3.6.5.3.1" style="font-size:90%;">254 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.6.5.4"><span class="ltx_text" id="S4.T1.3.6.5.4.1" style="font-size:90%;">60.2 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.5.5"><span class="ltx_text" id="S4.T1.3.6.5.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.6.5.6"><span class="ltx_text" id="S4.T1.3.6.5.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.6.5.7"></td>
<td class="ltx_td" id="S4.T1.3.6.5.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.7.6.1">
<span class="ltx_text" id="S4.T1.3.7.6.1.1" style="font-size:90%;">ANN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.7.6.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib16" title="">16</a><span class="ltx_text" id="S4.T1.3.7.6.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.7.6.2"><span class="ltx_text" id="S4.T1.3.7.6.2.1" style="font-size:90%;">ResNet101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.6.3"><span class="ltx_text" id="S4.T1.3.7.6.3.1" style="font-size:90%;">263 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.7.6.4"><span class="ltx_text" id="S4.T1.3.7.6.4.1" style="font-size:90%;">62.9 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.6.5"><span class="ltx_text" id="S4.T1.3.7.6.5.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.6.6"><span class="ltx_text" id="S4.T1.3.7.6.6.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.7.6.7"><span class="ltx_text" id="S4.T1.3.7.6.7.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td" id="S4.T1.3.7.6.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.8.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.8.7.1">
<span class="ltx_text" id="S4.T1.3.8.7.1.1" style="font-size:90%;">MobileNetV3 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.8.7.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib15" title="">15</a><span class="ltx_text" id="S4.T1.3.8.7.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.8.7.2"><span class="ltx_text" id="S4.T1.3.8.7.2.1" style="font-size:90%;">Large</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.7.3"><span class="ltx_text" id="S4.T1.3.8.7.3.1" style="font-size:90%;">8.80 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.8.7.4"><span class="ltx_text" id="S4.T1.3.8.7.4.1" style="font-size:90%;">3.30 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.7.5"><span class="ltx_text" id="S4.T1.3.8.7.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.8.7.6"><span class="ltx_text" id="S4.T1.3.8.7.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.8.7.7"></td>
<td class="ltx_td" id="S4.T1.3.8.7.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.9.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.9.8.1">
<span class="ltx_text" id="S4.T1.3.9.8.1.1" style="font-size:90%;">PSPNet </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.9.8.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib19" title="">19</a><span class="ltx_text" id="S4.T1.3.9.8.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.9.8.2"><span class="ltx_text" id="S4.T1.3.9.8.2.1" style="font-size:90%;">ResNet101</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.8.3"><span class="ltx_text" id="S4.T1.3.9.8.3.1" style="font-size:90%;">256 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.9.8.4"><span class="ltx_text" id="S4.T1.3.9.8.4.1" style="font-size:90%;">65.6 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.8.5"><span class="ltx_text" id="S4.T1.3.9.8.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.9.8.6"><span class="ltx_text" id="S4.T1.3.9.8.6.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td" id="S4.T1.3.9.8.7"></td>
<td class="ltx_td" id="S4.T1.3.9.8.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.10.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.10.9.1">
<span class="ltx_text" id="S4.T1.3.10.9.1.1" style="font-size:90%;">SegNeXt </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.10.9.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib20" title="">20</a><span class="ltx_text" id="S4.T1.3.10.9.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.10.9.2"><span class="ltx_text" id="S4.T1.3.10.9.2.1" style="font-size:90%;">Base</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.9.3"><span class="ltx_text" id="S4.T1.3.10.9.3.1" style="font-size:90%;">34.5 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.10.9.4"><span class="ltx_text" id="S4.T1.3.10.9.4.1" style="font-size:90%;">27.6 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.9.5"><span class="ltx_text" id="S4.T1.3.10.9.5.1" style="font-size:90%;">10/11</span></td>
<td class="ltx_td" id="S4.T1.3.10.9.6"></td>
<td class="ltx_td" id="S4.T1.3.10.9.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.10.9.8"><span class="ltx_text" id="S4.T1.3.10.9.8.1" style="font-size:90%;">1/11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.11.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.11.10.1">
<span class="ltx_text" id="S4.T1.3.11.10.1.1" style="font-size:90%;">Twins </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.11.10.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib18" title="">18</a><span class="ltx_text" id="S4.T1.3.11.10.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.11.10.2"><span class="ltx_text" id="S4.T1.3.11.10.2.1" style="font-size:90%;">Base</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.10.3"><span class="ltx_text" id="S4.T1.3.11.10.3.1" style="font-size:90%;">70.2 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.11.10.4"><span class="ltx_text" id="S4.T1.3.11.10.4.1" style="font-size:90%;">59.7 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.10.5"><span class="ltx_text" id="S4.T1.3.11.10.5.1" style="font-size:90%;">10/11</span></td>
<td class="ltx_td" id="S4.T1.3.11.10.6"></td>
<td class="ltx_td" id="S4.T1.3.11.10.7"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.11.10.8"><span class="ltx_text" id="S4.T1.3.11.10.8.1" style="font-size:90%;">1/11</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.12.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.3.12.11.1">
<span class="ltx_text" id="S4.T1.3.12.11.1.1" style="font-size:90%;">Swin </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.12.11.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib26" title="">26</a><span class="ltx_text" id="S4.T1.3.12.11.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.12.11.2"><span class="ltx_text" id="S4.T1.3.12.11.2.1" style="font-size:90%;">Base</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.11.3"><span class="ltx_text" id="S4.T1.3.12.11.3.1" style="font-size:90%;">305 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.3.12.11.4"><span class="ltx_text" id="S4.T1.3.12.11.4.1" style="font-size:90%;">120 M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.11.5"><span class="ltx_text" id="S4.T1.3.12.11.5.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.11.6"><span class="ltx_text" id="S4.T1.3.12.11.6.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.12.11.7"><span class="ltx_text" id="S4.T1.3.12.11.7.1" style="font-size:90%;">1/3</span></td>
<td class="ltx_td" id="S4.T1.3.12.11.8"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.13.12">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.3.13.12.1">
<span class="ltx_text" id="S4.T1.3.13.12.1.1" style="font-size:90%;">SegFormer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.3.13.12.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib25" title="">25</a><span class="ltx_text" id="S4.T1.3.13.12.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.3.13.12.2"><span class="ltx_text" id="S4.T1.3.13.12.2.1" style="font-size:90%;">Base</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.13.12.3"><span class="ltx_text" id="S4.T1.3.13.12.3.1" style="font-size:90%;">74.6 G</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S4.T1.3.13.12.4"><span class="ltx_text" id="S4.T1.3.13.12.4.1" style="font-size:90%;">82.0 M</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.13.12.5"><span class="ltx_text" id="S4.T1.3.13.12.5.1" style="font-size:90%;">1/2</span></td>
<td class="ltx_td ltx_border_b" id="S4.T1.3.13.12.6"></td>
<td class="ltx_td ltx_border_b" id="S4.T1.3.13.12.7"></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.3.13.12.8"><span class="ltx_text" id="S4.T1.3.13.12.8.1" style="font-size:90%;">1/2</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Experiment Details</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">As the core of our transfer learning strategy for twelve models, each utilizing a backbone pre-trained on the ImageNet 1K dataset. These backbones encompass a range of architectures, ResNet-50, ResNet-101, and Vision ViT. Initially, the models were trained on the SkyScapes dataset as a domain transfer. Upon achieving satisfactory performance on the SkyScapes dataset, the models undergo a subsequent phase of fine-tuning for the Waterloo Urban Scene dataset.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">The training was conducted on one NVIDIA RTX 3080 GPU with 10 GB memory. Given the intensive computational requirements, a batch size of 2 was opted for most model training setup. The models were trained around 20 epochs, a duration determined to be sufficient for converging to a stable solution without overfitting, based on the validation performance. To enhance the utility of data, neighboring patches were overlapped by 50% in both horizontal and vertical dimensions.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">The selection of learning rates was informed by established practices in pretrained model configurations, ensuring convergence. Tailored to the sensitivity of individual models to large adjustments, specific learning rates were assigned: ANN and Twins used 0.0001; DeepLabV3, DeepLabV3+, FCN, FastFCN, MobileNetV3, PSPNet, U-Net used 0.01; SegFormer, SegNeXt, and Swin used 0.00006. A uniform strategy encompassed a warm-up phase, spanning approximately half an epoch, during which learning rates were minimized to stabilize initial model states. Subsequently, a Poly Learning Rate schedule, with a power of 0.9, was employed to systematically reduce learning rates, thereby optimizing training progression.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Both AdamW and Stochastic Gradient Descent (SGD) optimizers were utilized, capitalizing on their respective strengths in managing sparse gradients and momentum. AdamW used favored for Transformer models, namely SegFormer, SegNeXt, Swin, and Twins, while SGD was applied to the remaining models. A diverse suite of loss functions, including cross-entropy loss, Tversky loss, dice loss, and focal loss, were combined as weighted sum for model training. The details are summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S4.T1" title="TABLE I ‣ IV-B2 Transformer-based Models ‣ IV-B Benchmarked Models ‣ IV Methodology for Automated Lane Marking Extraction ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">I</span></a>.

</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results and Discussion</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.4.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.5.2">Model Performance and Adaptation</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS1.SSS1.4.1.1">V-A</span>1 </span>SkyScapes Dataset</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">This section presents a comprehensive analysis of 12 models’ performance on the SkyScapes dataset. The evaluation employs a suite of metrics, including mean Intersection over Union (mIoU), mean Accuracy (mAcc), overall Accuracy (aAcc), mean Recall, mean Precision, and mean F1 score, to facilitate a detailed examination of each model’s performance.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">Based on Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.T2" title="TABLE II ‣ V-A2 Waterloo Urban Scene Dataset ‣ V-A Model Performance and Adaptation ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">II</span></a>, transformer-based models outperform CNN-based models. Within the transformer category, models such as SegFormer and Swin achieve superior results compared to traditional CNN models, highlighting the effectiveness of attention mechanisms over non-attention-based approaches. Notably, SegNeXt, even without employing a transformer structure, achieves the second-best result in terms of mIoU among the 12 models through its unique convolutional attention mechanism.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">Generally, Recall exceeds Precision across the models except for Twins and SegFormer. This suggests their predictions tended to minimize false negatives over false positive to achieve high recall, perhaps over predicting pixels as positives.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.1">All models demonstrate exceptionally high Accuracy, with each model achieving at least 97.55% mAcc and some nearing 99.92%. This phenomenon is attributed to the dominant presence of background pixels, where accurate background prediction significantly influences the Accuracy metric, leading to scale imbalance and diminished result sensitivity.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p5">
<p class="ltx_p" id="S5.SS1.SSS1.p5.1">In conclusion, examining 12 models on the SkyScapes dataset provides valuable insights into how transformer-based and CNN-based models perform across various metrics. The analysis showed that predicting the background class was generally the easiest task for all models, mainly because it makes up most pixels. However, PSPNet and MobileNetV3 stood out for their lower mIoU scores for the background, suggesting they might incorrectly classify more pixels as belonging to other classes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS1.SSS2.4.1.1">V-A</span>2 </span>Waterloo Urban Scene Dataset</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">This section extends the comparative analysis of model performance from the SkyScapes dataset to the Waterloo Urban Scene dataset by employing identical evaluation metrics, aiming to understand how different models perform across diverse urban imaging domains.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">The results section from Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.T3" title="TABLE III ‣ V-A2 Waterloo Urban Scene Dataset ‣ V-A Model Performance and Adaptation ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">III</span></a> showcases a notable improvement across all metrics on the Waterloo Urban Scene dataset, with mIoU now ranging between 33.56% to 76.11% and F1 scores spanning from 44.34% to 85.35%. This marks a significant enhancement in model performance compared to Skyscapes benchmarks, potentially attributed to the advantages of pretraining on the SkyScapes dataset and the unique characteristics of the Waterloo Urban Scene dataset itself.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Benchmark of the state-of-the-art on the SkyScapes-Lane task over all 12 classes (in %)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.3.1.1.1"><span class="ltx_text" id="S5.T2.3.1.1.1.1" style="font-size:80%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.2.1" style="font-size:80%;">IoU Mean</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.3.1" style="font-size:80%;">mAcc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.4.1" style="font-size:80%;">aAcc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.5.1" style="font-size:80%;">mRecall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.6.1" style="font-size:80%;">mPrecision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.3.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.7.1" style="font-size:80%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T2.3.2.1.1"><span class="ltx_text" id="S5.T2.3.2.1.1.1" style="font-size:80%;">FCN</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.2"><span class="ltx_text" id="S5.T2.3.2.1.2.1" style="font-size:80%;">10.68</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.3"><span class="ltx_text" id="S5.T2.3.2.1.3.1" style="font-size:80%;">98.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.4"><span class="ltx_text" id="S5.T2.3.2.1.4.1" style="font-size:80%;">93.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.5"><span class="ltx_text" id="S5.T2.3.2.1.5.1" style="font-size:80%;">13.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.6"><span class="ltx_text" id="S5.T2.3.2.1.6.1" style="font-size:80%;">11.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1.7"><span class="ltx_text" id="S5.T2.3.2.1.7.1" style="font-size:80%;">13.53</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.3.2.1"><span class="ltx_text" id="S5.T2.3.3.2.1.1" style="font-size:80%;">FastFCN</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.2"><span class="ltx_text" id="S5.T2.3.3.2.2.1" style="font-size:80%;">16.33</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.3"><span class="ltx_text" id="S5.T2.3.3.2.3.1" style="font-size:80%;">99.77</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.4"><span class="ltx_text" id="S5.T2.3.3.2.4.1" style="font-size:80%;">98.61</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.5"><span class="ltx_text" id="S5.T2.3.3.2.5.1" style="font-size:80%;">31.10</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.6"><span class="ltx_text" id="S5.T2.3.3.2.6.1" style="font-size:80%;">21.02</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.3.2.7"><span class="ltx_text" id="S5.T2.3.3.2.7.1" style="font-size:80%;">22.36</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.4.3.1"><span class="ltx_text" id="S5.T2.3.4.3.1.1" style="font-size:80%;">U-Net</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.2"><span class="ltx_text" id="S5.T2.3.4.3.2.1" style="font-size:80%;">14.98</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.3"><span class="ltx_text" id="S5.T2.3.4.3.3.1" style="font-size:80%;">99.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.4"><span class="ltx_text" id="S5.T2.3.4.3.4.1" style="font-size:80%;">95.43</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.5"><span class="ltx_text" id="S5.T2.3.4.3.5.1" style="font-size:80%;">50.45</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.6"><span class="ltx_text" id="S5.T2.3.4.3.6.1" style="font-size:80%;">18.97</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.4.3.7"><span class="ltx_text" id="S5.T2.3.4.3.7.1" style="font-size:80%;">21.80</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.5.4.1"><span class="ltx_text" id="S5.T2.3.5.4.1.1" style="font-size:80%;">DeepLabV3</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.2"><span class="ltx_text" id="S5.T2.3.5.4.2.1" style="font-size:80%;">10.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.3"><span class="ltx_text" id="S5.T2.3.5.4.3.1" style="font-size:80%;">98.79</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.4"><span class="ltx_text" id="S5.T2.3.5.4.4.1" style="font-size:80%;">92.72</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.5"><span class="ltx_text" id="S5.T2.3.5.4.5.1" style="font-size:80%;">35.32</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.6"><span class="ltx_text" id="S5.T2.3.5.4.6.1" style="font-size:80%;">11.12</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.5.4.7"><span class="ltx_text" id="S5.T2.3.5.4.7.1" style="font-size:80%;">12.69</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.6.5.1"><span class="ltx_text" id="S5.T2.3.6.5.1.1" style="font-size:80%;">DeepLabV3+</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.2"><span class="ltx_text" id="S5.T2.3.6.5.2.1" style="font-size:80%;">18.08</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.3"><span class="ltx_text" id="S5.T2.3.6.5.3.1" style="font-size:80%;">99.84</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.4"><span class="ltx_text" id="S5.T2.3.6.5.4.1" style="font-size:80%;">99.02</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.5"><span class="ltx_text" id="S5.T2.3.6.5.5.1" style="font-size:80%;">34.62</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.6"><span class="ltx_text" id="S5.T2.3.6.5.6.1" style="font-size:80%;">23.58</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.6.5.7"><span class="ltx_text" id="S5.T2.3.6.5.7.1" style="font-size:80%;">24.91</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.7.6.1"><span class="ltx_text" id="S5.T2.3.7.6.1.1" style="font-size:80%;">ANN</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.2"><span class="ltx_text" id="S5.T2.3.7.6.2.1" style="font-size:80%;">20.94</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.3"><span class="ltx_text" id="S5.T2.3.7.6.3.1" style="font-size:80%;">99.41</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.4"><span class="ltx_text" id="S5.T2.3.7.6.4.1" style="font-size:80%;">96.47</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.5"><span class="ltx_text ltx_font_bold" id="S5.T2.3.7.6.5.1" style="font-size:80%;">66.00</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.6"><span class="ltx_text" id="S5.T2.3.7.6.6.1" style="font-size:80%;">22.92</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.7.6.7"><span class="ltx_text" id="S5.T2.3.7.6.7.1" style="font-size:80%;">29.88</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.8.7.1"><span class="ltx_text" id="S5.T2.3.8.7.1.1" style="font-size:80%;">MobileNetV3</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.2"><span class="ltx_text" id="S5.T2.3.8.7.2.1" style="font-size:80%;">11.74</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.3"><span class="ltx_text" id="S5.T2.3.8.7.3.1" style="font-size:80%;">98.91</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.4"><span class="ltx_text" id="S5.T2.3.8.7.4.1" style="font-size:80%;">93.47</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.5"><span class="ltx_text" id="S5.T2.3.8.7.5.1" style="font-size:80%;">54.55</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.6"><span class="ltx_text" id="S5.T2.3.8.7.6.1" style="font-size:80%;">12.45</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.8.7.7"><span class="ltx_text" id="S5.T2.3.8.7.7.1" style="font-size:80%;">15.25</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.9.8.1"><span class="ltx_text" id="S5.T2.3.9.8.1.1" style="font-size:80%;">PSPNet</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.2"><span class="ltx_text" id="S5.T2.3.9.8.2.1" style="font-size:80%;">8.68</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.3"><span class="ltx_text" id="S5.T2.3.9.8.3.1" style="font-size:80%;">97.55</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.4"><span class="ltx_text" id="S5.T2.3.9.8.4.1" style="font-size:80%;">85.29</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.5"><span class="ltx_text" id="S5.T2.3.9.8.5.1" style="font-size:80%;">32.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.6"><span class="ltx_text" id="S5.T2.3.9.8.6.1" style="font-size:80%;">10.09</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.9.8.7"><span class="ltx_text" id="S5.T2.3.9.8.7.1" style="font-size:80%;">10.65</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.10.9.1"><span class="ltx_text" id="S5.T2.3.10.9.1.1" style="font-size:80%;">SegNeXt</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.2"><span class="ltx_text" id="S5.T2.3.10.9.2.1" style="font-size:80%;">32.26</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.3"><span class="ltx_text" id="S5.T2.3.10.9.3.1" style="font-size:80%;">99.86</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.4"><span class="ltx_text" id="S5.T2.3.10.9.4.1" style="font-size:80%;">99.14</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.5"><span class="ltx_text" id="S5.T2.3.10.9.5.1" style="font-size:80%;">49.96</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.6"><span class="ltx_text" id="S5.T2.3.10.9.6.1" style="font-size:80%;">48.98</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.10.9.7"><span class="ltx_text" id="S5.T2.3.10.9.7.1" style="font-size:80%;">44.20</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.11.10.1"><span class="ltx_text" id="S5.T2.3.11.10.1.1" style="font-size:80%;">Twins</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.2"><span class="ltx_text" id="S5.T2.3.11.10.2.1" style="font-size:80%;">30.11</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.3"><span class="ltx_text" id="S5.T2.3.11.10.3.1" style="font-size:80%;">99.85</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.4"><span class="ltx_text" id="S5.T2.3.11.10.4.1" style="font-size:80%;">99.08</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.5"><span class="ltx_text" id="S5.T2.3.11.10.5.1" style="font-size:80%;">45.79</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.6"><span class="ltx_text" id="S5.T2.3.11.10.6.1" style="font-size:80%;">51.81</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.11.10.7"><span class="ltx_text" id="S5.T2.3.11.10.7.1" style="font-size:80%;">41.87</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T2.3.12.11.1"><span class="ltx_text" id="S5.T2.3.12.11.1.1" style="font-size:80%;">Swin</span></th>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.2"><span class="ltx_text" id="S5.T2.3.12.11.2.1" style="font-size:80%;">30.51</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.3"><span class="ltx_text" id="S5.T2.3.12.11.3.1" style="font-size:80%;">99.84</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.4"><span class="ltx_text" id="S5.T2.3.12.11.4.1" style="font-size:80%;">99.02</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.5"><span class="ltx_text" id="S5.T2.3.12.11.5.1" style="font-size:80%;">60.03</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.6"><span class="ltx_text" id="S5.T2.3.12.11.6.1" style="font-size:80%;">40.27</span></td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.12.11.7"><span class="ltx_text" id="S5.T2.3.12.11.7.1" style="font-size:80%;">42.97</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T2.3.13.12.1"><span class="ltx_text" id="S5.T2.3.13.12.1.1" style="font-size:80%;">SegFormer</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.13.12.2.1" style="font-size:80%;">33.56</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.13.12.3.1" style="font-size:80%;">99.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.13.12.4.1" style="font-size:80%;">99.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.5"><span class="ltx_text" id="S5.T2.3.13.12.5.1" style="font-size:80%;">43.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.6"><span class="ltx_text ltx_font_bold" id="S5.T2.3.13.12.6.1" style="font-size:80%;">64.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.3.13.12.7"><span class="ltx_text ltx_font_bold" id="S5.T2.3.13.12.7.1" style="font-size:80%;">44.34</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Benchmark of the state-of-the-art on the Waterloo Urban Scene Dataset over all 15 classes (in %)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S5.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.1.1.1"><span class="ltx_text" id="S5.T3.3.1.1.1.1" style="font-size:80%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.2.1" style="font-size:80%;">IoU Mean</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.3.1" style="font-size:80%;">mAcc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.4.1" style="font-size:80%;">aAcc</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.5.1" style="font-size:80%;">mRecall</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.6.1" style="font-size:80%;">mPrecision</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.3.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.7.1" style="font-size:80%;">F1</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.3.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T3.3.2.1.1"><span class="ltx_text" id="S5.T3.3.2.1.1.1" style="font-size:80%;">FCN</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.2"><span class="ltx_text" id="S5.T3.3.2.1.2.1" style="font-size:80%;">44.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.3"><span class="ltx_text" id="S5.T3.3.2.1.3.1" style="font-size:80%;">99.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.4"><span class="ltx_text" id="S5.T3.3.2.1.4.1" style="font-size:80%;">96.84</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.5"><span class="ltx_text" id="S5.T3.3.2.1.5.1" style="font-size:80%;">93.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.6"><span class="ltx_text" id="S5.T3.3.2.1.6.1" style="font-size:80%;">47.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.3.2.1.7"><span class="ltx_text" id="S5.T3.3.2.1.7.1" style="font-size:80%;">56.38</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.3.2.1"><span class="ltx_text" id="S5.T3.3.3.2.1.1" style="font-size:80%;">FastFCN</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.2"><span class="ltx_text" id="S5.T3.3.3.2.2.1" style="font-size:80%;">50.48</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.3"><span class="ltx_text" id="S5.T3.3.3.2.3.1" style="font-size:80%;">99.76</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.4"><span class="ltx_text" id="S5.T3.3.3.2.4.1" style="font-size:80%;">98.16</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.5"><span class="ltx_text" id="S5.T3.3.3.2.5.1" style="font-size:80%;">98.54</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.6"><span class="ltx_text" id="S5.T3.3.3.2.6.1" style="font-size:80%;">51.46</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.2.7"><span class="ltx_text" id="S5.T3.3.3.2.7.1" style="font-size:80%;">62.07</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.4.3.1"><span class="ltx_text" id="S5.T3.3.4.3.1.1" style="font-size:80%;">U-Net</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.2"><span class="ltx_text" id="S5.T3.3.4.3.2.1" style="font-size:80%;">44.64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.3"><span class="ltx_text" id="S5.T3.3.4.3.3.1" style="font-size:80%;">99.52</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.4"><span class="ltx_text" id="S5.T3.3.4.3.4.1" style="font-size:80%;">96.37</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.5"><span class="ltx_text" id="S5.T3.3.4.3.5.1" style="font-size:80%;">91.14</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.6"><span class="ltx_text" id="S5.T3.3.4.3.6.1" style="font-size:80%;">46.95</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.4.3.7"><span class="ltx_text" id="S5.T3.3.4.3.7.1" style="font-size:80%;">56.62</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.5.4.1"><span class="ltx_text" id="S5.T3.3.5.4.1.1" style="font-size:80%;">DeepLabV3</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.2"><span class="ltx_text" id="S5.T3.3.5.4.2.1" style="font-size:80%;">47.55</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.3"><span class="ltx_text" id="S5.T3.3.5.4.3.1" style="font-size:80%;">99.63</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.4"><span class="ltx_text" id="S5.T3.3.5.4.4.1" style="font-size:80%;">97.22</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.5"><span class="ltx_text" id="S5.T3.3.5.4.5.1" style="font-size:80%;">93.97</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.6"><span class="ltx_text" id="S5.T3.3.5.4.6.1" style="font-size:80%;">49.44</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.5.4.7"><span class="ltx_text" id="S5.T3.3.5.4.7.1" style="font-size:80%;">60.31</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.6.5.1"><span class="ltx_text" id="S5.T3.3.6.5.1.1" style="font-size:80%;">DeepLabV3+</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.2"><span class="ltx_text" id="S5.T3.3.6.5.2.1" style="font-size:80%;">51.03</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.3"><span class="ltx_text" id="S5.T3.3.6.5.3.1" style="font-size:80%;">99.65</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.4"><span class="ltx_text" id="S5.T3.3.6.5.4.1" style="font-size:80%;">97.34</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.5"><span class="ltx_text" id="S5.T3.3.6.5.5.1" style="font-size:80%;">95.34</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.6"><span class="ltx_text" id="S5.T3.3.6.5.6.1" style="font-size:80%;">53.20</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.6.5.7"><span class="ltx_text" id="S5.T3.3.6.5.7.1" style="font-size:80%;">63.74</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.7.6.1"><span class="ltx_text" id="S5.T3.3.7.6.1.1" style="font-size:80%;">ANN</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.2"><span class="ltx_text" id="S5.T3.3.7.6.2.1" style="font-size:80%;">46.50</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.3"><span class="ltx_text" id="S5.T3.3.7.6.3.1" style="font-size:80%;">99.67</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.4"><span class="ltx_text" id="S5.T3.3.7.6.4.1" style="font-size:80%;">97.54</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.5"><span class="ltx_text" id="S5.T3.3.7.6.5.1" style="font-size:80%;">96.30</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.6"><span class="ltx_text" id="S5.T3.3.7.6.6.1" style="font-size:80%;">47.91</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.7.6.7"><span class="ltx_text" id="S5.T3.3.7.6.7.1" style="font-size:80%;">58.23</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.8.7.1"><span class="ltx_text" id="S5.T3.3.8.7.1.1" style="font-size:80%;">MobileNetV3</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.2"><span class="ltx_text" id="S5.T3.3.8.7.2.1" style="font-size:80%;">34.94</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.3"><span class="ltx_text" id="S5.T3.3.8.7.3.1" style="font-size:80%;">99.37</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.4"><span class="ltx_text" id="S5.T3.3.8.7.4.1" style="font-size:80%;">95.26</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.5"><span class="ltx_text" id="S5.T3.3.8.7.5.1" style="font-size:80%;">89.89</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.6"><span class="ltx_text" id="S5.T3.3.8.7.6.1" style="font-size:80%;">37.44</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.8.7.7"><span class="ltx_text" id="S5.T3.3.8.7.7.1" style="font-size:80%;">46.03</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.9.8.1"><span class="ltx_text" id="S5.T3.3.9.8.1.1" style="font-size:80%;">PSPNet</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.2"><span class="ltx_text" id="S5.T3.3.9.8.2.1" style="font-size:80%;">50.48</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.3"><span class="ltx_text" id="S5.T3.3.9.8.3.1" style="font-size:80%;">99.72</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.4"><span class="ltx_text" id="S5.T3.3.9.8.4.1" style="font-size:80%;">97.87</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.5"><span class="ltx_text" id="S5.T3.3.9.8.5.1" style="font-size:80%;">96.73</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.6"><span class="ltx_text" id="S5.T3.3.9.8.6.1" style="font-size:80%;">51.90</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.9.8.7"><span class="ltx_text" id="S5.T3.3.9.8.7.1" style="font-size:80%;">62.29</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.10.9.1"><span class="ltx_text" id="S5.T3.3.10.9.1.1" style="font-size:80%;">SegNeXt</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.2"><span class="ltx_text" id="S5.T3.3.10.9.2.1" style="font-size:80%;">65.77</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.3"><span class="ltx_text" id="S5.T3.3.10.9.3.1" style="font-size:80%;">99.66</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.4"><span class="ltx_text" id="S5.T3.3.10.9.4.1" style="font-size:80%;">97.45</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.5"><span class="ltx_text" id="S5.T3.3.10.9.5.1" style="font-size:80%;">96.81</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.6"><span class="ltx_text" id="S5.T3.3.10.9.6.1" style="font-size:80%;">67.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.10.9.7"><span class="ltx_text" id="S5.T3.3.10.9.7.1" style="font-size:80%;">77.60</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.11.10.1"><span class="ltx_text" id="S5.T3.3.11.10.1.1" style="font-size:80%;">Twins</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.2"><span class="ltx_text" id="S5.T3.3.11.10.2.1" style="font-size:80%;">62.86</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.3"><span class="ltx_text" id="S5.T3.3.11.10.3.1" style="font-size:80%;">99.23</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.4"><span class="ltx_text" id="S5.T3.3.11.10.4.1" style="font-size:80%;">94.24</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.5"><span class="ltx_text" id="S5.T3.3.11.10.5.1" style="font-size:80%;">88.15</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.6"><span class="ltx_text" id="S5.T3.3.11.10.6.1" style="font-size:80%;">68.64</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.11.10.7"><span class="ltx_text" id="S5.T3.3.11.10.7.1" style="font-size:80%;">76.01</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S5.T3.3.12.11.1"><span class="ltx_text" id="S5.T3.3.12.11.1.1" style="font-size:80%;">Swin</span></th>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.2"><span class="ltx_text" id="S5.T3.3.12.11.2.1" style="font-size:80%;">60.22</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.12.11.3.1" style="font-size:80%;">99.84</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.12.11.4.1" style="font-size:80%;">98.78</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.5"><span class="ltx_text ltx_font_bold" id="S5.T3.3.12.11.5.1" style="font-size:80%;">98.96</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.6"><span class="ltx_text" id="S5.T3.3.12.11.6.1" style="font-size:80%;">60.84</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.12.11.7"><span class="ltx_text" id="S5.T3.3.12.11.7.1" style="font-size:80%;">72.48</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r" id="S5.T3.3.13.12.1"><span class="ltx_text" id="S5.T3.3.13.12.1.1" style="font-size:80%;">SegFormer</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.13.12.2.1" style="font-size:80%;">76.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.3"><span class="ltx_text" id="S5.T3.3.13.12.3.1" style="font-size:80%;">99.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.4"><span class="ltx_text" id="S5.T3.3.13.12.4.1" style="font-size:80%;">98.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.5"><span class="ltx_text" id="S5.T3.3.13.12.5.1" style="font-size:80%;">97.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.6"><span class="ltx_text ltx_font_bold" id="S5.T3.3.13.12.6.1" style="font-size:80%;">77.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.3.13.12.7"><span class="ltx_text ltx_font_bold" id="S5.T3.3.13.12.7.1" style="font-size:80%;">85.35</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1">In line with the trends observed on the SkyScapes dataset, transformer-based models continue to outperform traditional CNN-based models on the Waterloo Urban Scene dataset. Specifically, SegNeXt, with its convolutional attention mechanism, achieves the second highest mIoU, trailing only behind SegFormer. SegFormer leads in mIoU, mean precision, and F1 score, with impressive scores of 76.11%, 77.44%, and 85.34%, respectively. Swin, on the other hand, excels in mAcc, aAcc, and mean recall, recording the highest values at 99.84%, 98.74%, and 98.96%, respectively. Most models demonstrate exceptionally high recall values, exceeding 90%, with the exception of MobileNetV3 and Twins, which record slightly lower recalls at 89.89% and 88.15%, respectively. This trend suggests that models, in general, tend to predict more pixels outside the actual ground truth area than fewer pixels within it, as evidenced by the lower precision scores compared to recall scores.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1">Drawing on the insights from the SkyScapes dataset, the analysis of model performance on the Waterloo Urban Scene dataset reveals similar trends. Align with the findings from the SkyScapes dataset, the analysis of models on the Waterloo Urban Scene dataset reveals enhanced performance in the sequence of Road, Traffic Island, Sidewalk, and Vehicle classes which showed larger presence in the imagery.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p5">
<p class="ltx_p" id="S5.SS1.SSS2.p5.1">This shift in performance suggests a different weighting of class importance within the Waterloo Urban Scene dataset. With playing a more significant role in overall model evaluation metrics, these four classes highlight the importance of dataset diversity in understanding model behavior. These four larger object classes directly attributed to the effectiveness with their alignment on the core objectives of semantic segmentation models tailored for aerial view scene analysis. Conversely, classes characterized by smaller or linear features, like Dash Lines and Long Lines, showcase a opposite trend, with less significant performance enhancements, suggesting the key issue that needs to be overcome in road lane markings segmentation task.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.4.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.5.2">Visualization of Results </span>
</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS1.4.1.1">V-B</span>1 </span>SkyScapes Dataset</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">This section provides visual representations of the predictions generated by 12 models on the SkyScapes dataset. Through side-by-side comparisons with input images and ground truth labels.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1">Among the visualized predictions, the initial focus is drawn towards PSPNet and MobileNetV3, where misclassification errors are significant, particularly in the background class. This observation aligns with their previously noted lower accuracy. Conversely, despite SegNeXt achieving the second highest mIoU, it is noteworthy that transformer-based models such as SegFormer and Swin exhibit superior visualization results. These transformer-based models demonstrate remarkable fidelity to the ground truth, exhibiting minimal distortion in lane thickness and negligible irrelevant errors. Furthermore, U-Net’s performance is notable for its adeptness in delineating boundaries; however, it appears to weak in classifying multiple classes.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">These findings underscore the strengths and weaknesses inherent in each model’s architecture and highlight the importance of considering the original design objectives when assessing their performance across diverse tasks. Such insights gained from the visual analysis offer valuable perspectives for further refinement and optimization of semantic segmentation models.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">Conversely, all transformer-based models, along with SegNeXt, tend to create lane markings that are finer and closer to the ground truth. Most models are accurate in capturing both long and dashed lines. The detection of zebra zones presents a challenge for several models; however, DeepLabV3+, SegFormer, SegNeXt, Swin, and Twins demonstrate the capability to accurately recognize the zebra zone.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p5">
<p class="ltx_p" id="S5.SS2.SSS1.p5.1">However, some models, particularly MobileNetV3 and U-Net, struggle with a high rate of background misclassification, while DeepLabV3 and FCN also exhibit minor issues with background misclassification. This issue with background misclassification is further validated by Table <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.T2" title="TABLE II ‣ V-A2 Waterloo Urban Scene Dataset ‣ V-A Model Performance and Adaptation ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">II</span></a>, where these models’ background mIoU is low.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS2.SSS2.4.1.1">V-B</span>2 </span>Waterloo Urban Scene Dataset</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">In the complex intersection scenario shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.F1" title="Figure 1 ‣ V-B2 Waterloo Urban Scene Dataset ‣ V-B Visualization of Results ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">1</span></a>, the road lane classifications include features such as stop lines, crosswalks, turn signs, single solid lines, dashed lines, and small dashed lines. The figure also presents classes for vehicles, sidewalks, and roads. Given the scene’s complexity, while many models capture the overall appearance, certain elements like turn signs are depicted with excessive thickness, making them difficult to recognize. Swin stands out by accurately classifying most of the scene without significant background misclassifications, although small dashed lines and turn signs are somewhat indistinct. SegFormer also performs well, effectively identifying road lanes despite some confusion in classifying certain background areas as roads. This is deemed acceptable given that these areas share similar color and texture with the road, as verified by the ground truth. MobileNetV3, U-Net, and Twins appear to be the least effective models, showing misclassifications and inconsistencies in scene interpretation, especially when roads are covered by shadows or vegetation.</p>
</div>
<figure class="ltx_figure" id="S5.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="747" id="S5.F1.g1" src="extracted/5909363/Picture2.png" width="678"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>A comparative visualization of road lane detection at an intersection by 12 different models on a Waterloo Urban Scene dataset sample. Each model’s output is showcased alongside the original image and the ground truth for reference.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.4.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.5.2">Discussion</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS1.4.1.1">V-C</span>1 </span>Dataset and Annotation Quality</h4>
<div class="ltx_para" id="S5.SS3.SSS1.p1">
<p class="ltx_p" id="S5.SS3.SSS1.p1.1">Differences in datasets, as observed between the Waterloo Urban Scene and SkyScapes datasets, can significantly influence model performance. A more balanced distribution, as seen in the Waterloo Urban Scene dataset, facilitates better learning by the model, leading to improved performance. Additionally, higher-resolution datasets like Waterloo Urban Scene may present larger class objects, simplifying the learning task for models designed for aerial-view applications. Understanding dataset characteristics is crucial for optimizing model training and performance evaluation in remote sensing tasks. When examining pixel counts across datasets, it’s evident that the Waterloo Urban Scene Dataset encompasses a wider range of classes, and less skewed class distribution contributing to the observed differences.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS1.p2">
<p class="ltx_p" id="S5.SS3.SSS1.p2.1">Aerial images frequently include various obstructions such as trees, vehicles, and shadows, which can obscure critical details. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.F4" title="Figure 4 ‣ V-C1 Dataset and Annotation Quality ‣ V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates how background features in aerial views, such as road lane markings, often become obscured by trees and utility poles. Consequently, the SkyScapes dataset adopts a questionable annotation practice that only marks road lane markings that are visible in the aerial images, ignoring their actual existence. This method leads to ground truth annotations and training data where road lane markings appear fragmented, as depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.F4" title="Figure 4 ‣ V-C1 Dataset and Annotation Quality ‣ V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F4.1" style="width:138.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="S5.F4.1.g1" src="extracted/5909363/Picture3.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>A raw aerial image from the SkyScapes dataset where trees and utility poles obstruct the visibility of road lane markings.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F4.2" style="width:138.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="S5.F4.2.g1" src="extracted/5909363/Picture4.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The ground truth image, illustrating that road lane markings are annotated as discontinuous rather than continuous due to obstructions.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F4.3" style="width:138.8pt;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="598" id="S5.F4.3.g1" src="extracted/5909363/Picture5.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Prediction result from the Swin model on the SkyScapes test dataset, showing relatively continuous road lane markings.</figcaption>
</figure>
</div>
</div>
</figure>
<div class="ltx_para" id="S5.SS3.SSS1.p3">
<p class="ltx_p" id="S5.SS3.SSS1.p3.1">Despite these challenges, some models have demonstrated an ability to overcome such data limitations. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#S5.F4" title="Figure 4 ‣ V-C1 Dataset and Annotation Quality ‣ V-C Discussion ‣ V Results and Discussion ‣ Advancements in Road Lane Mapping: Comparative Fine-Tuning Analysis of Deep Learning-based Semantic Segmentation Methods Using Aerial Imagery"><span class="ltx_text ltx_ref_tag">4</span></a> showcases how the Swin model, for example, effectively discerns the spatial relationships between adjacent road lane marking pixels. This capability allows the model to reconstruct the continuity of road lane markings, thus compensating for the gaps and discontinuities present in the source aerial images. This adaptability highlights the potential of advanced deep learning models to mitigate the effects of noise in aerial imagery, thereby enhancing the reliability of the data derived from these images for various practical applications.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS2.4.1.1">V-C</span>2 </span>Model Architecture and Benchmark Results</h4>
<div class="ltx_para" id="S5.SS3.SSS2.p1">
<p class="ltx_p" id="S5.SS3.SSS2.p1.1">Transformer-based models exhibit an advantage in capturing long-range dependencies, which is crucial for understanding complex scenes in remote sensing imagery. Unlike CNNs, which primarily focus on local dependencies, transformers can efficiently learn relationships between distant pixels. However, transformers require intensive training to extract features effectively. The absence of inherent knowledge about pixel distributions and local relations necessitates pre-trained backbones for transformers to achieve robust performance, particularly in smaller datasets with simpler scenes.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p2">
<p class="ltx_p" id="S5.SS3.SSS2.p2.1">Recent research has shown that CNNs can emulate the long-range dependency capturing capability of transformers through the integration of attention mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib33" title="">33</a>]</cite>. Models like SegNeXt demonstrate the efficacy of this approach, suggesting that CNNs can rival transformers in certain tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2410.05717v1#bib.bib20" title="">20</a>]</cite>. Additionally, incorporating special pooling layers after CNNs can further enhance performance while reducing computational complexity and runtime.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS2.p3">
<p class="ltx_p" id="S5.SS3.SSS2.p3.1">Both are fundamental metrics in evaluating semantic segmentation performance. As recall increases, precision tends to decrease, and vice versa. For example, in SkyScapes dataset, ANN demonstrates a recall of 66.0% and a precision of 22.92%, while SegFormer, a transformer-based model, exhibits a recall of 43.85% and a precision of 64.33%. Achieving a balance between these metrics is essential for accurate detection. Strategies to improve recall and precision include refining prediction boundaries to better match ground truth objects and minimizing over-predictions or under-predictions. Utilizing the F1 score, which combines both recall and precision, provides a comprehensive assessment of model performance, particularly in tasks where balancing these metrics is challenging. IoU and F1 score evaluate the overlap between predicted and ground truth regions, considering both shape and location. They provide valuable insights into the accuracy of object detection algorithms. Similar trends can be observed whereby models with high precision and recall also have high mIoU and F1 scores. Accuracy may be skewed by class imbalances, particularly in datasets where certain classes dominate, such as backgrounds in remote sensing imagery. This dominance inflates accuracy scores, potentially masking performance issues in other classes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S5.SS3.SSS3.4.1.1">V-C</span>3 </span>Transfer Learning and Partially Labelled Dataset</h4>
<div class="ltx_para" id="S5.SS3.SSS3.p1">
<p class="ltx_p" id="S5.SS3.SSS3.p1.1">In our experiments, we observed many models were capable of achieving high performance after a simple transfer learning based on full supervision on the initial pretraining dataset. The performance of models on the Waterloo Urban Scene dataset in general surpassed their performance on Skyscape dataset and previous state-of-the-art road lane extraction results. That is to say, given we were able to achieve excellent results on a partially labelled dataset by pretraining on an existing dataset, as such, believe the transfer learning to be a worthwhile step to undertake. We also note that both pretraining and fine-tuning shared a high degree of similarity when comparing to other possible transfer learning schemes such as pretraining on an unlabelled dataset, or a non-remote sensing dataset, then fine-tuning on the Waterloo Urban Scene dataset, and that these other transfer learning schemes can be investigated if no similar fully labeled supervised pretraining datasets exist.</p>
</div>
<div class="ltx_para" id="S5.SS3.SSS3.p2">
<p class="ltx_p" id="S5.SS3.SSS3.p2.1">In the absence of dataset labels, it is also possible to consider generative AI, which could possibly be highly beneficial for enhancing the detection and extraction of lane markings. Specifically, diffusion models have the potential to transform the training process for 2D models in lane detection, by synthesizing/adapting training images with different conditions, such as fluctuations in lighting, weather changes, and the presence of dynamic obstacles, as well as be able to transfer and generate annotations for these generated images.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This research presents a comprehensive comparative analysis of 12 state-of-the-art CNN- and transformer-based models for road lane extraction using semantic segmentation. The models were evaluated on the SkyScapes Dataset and further fine-tuned on the newly annotated Waterloo Urban Scene Dataset. Transformer-based models, such as Swin and SegFormer, demonstrated superior performance, particularly with common lane markings like solid and dashed lines. Despite challenges posed by noise, such as trees and shadows in aerial imagery, certain models effectively captured the continuity of lane markings by discerning pixel relationships.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The findings offer valuable insights into the strengths and limitations of both CNN and transformer models in the specific context of road lane extraction from aerial imagery. This research not only benchmarks these models but also provides a foundation for future work aimed at refining road lane extraction algorithms and improving datasets for HD map development.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In conclusion, this analysis highlights the capabilities of different semantic segmentation models and underscores the importance of model selection for improving road lane extraction in autonomous vehicle navigation systems. It sets a solid foundation for advancing HD mapping, contributing to more reliable and efficient navigation for autonomous vehicles.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the members of the Geospatial Intelligence and Mapping (GIM) Lab at the University of Waterloo for their contribution of annotation to the Waterloo Urban Scene Dataset, and Deutsches Zentrum für Luft- und Raumfahrt (DLR) for granting us access to and use of the SkyScapes Dataset.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Yoo, H. S. Lee, H. Myeong, S. Yun, H. Park, J. Cho, and D. H. Kim, “End-to-end lane marker detection via row-wise classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</em>, 2020, pp. 1006–1007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics: The kitti dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">The International Journal of Robotics Research</em>, vol. 32, no. 11, pp. 1231–1237, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
F. Chao, S. Yu-Pei, and J. Ya-Jie, “Multi-lane detection based on deep convolutional neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">IEEE access</em>, vol. 7, pp. 150 833–150 841, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, “Bdd100k: A diverse driving dataset for heterogeneous multitask learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 2636–2645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Rottensteiner, G. Sohn, M. Gerke, and J. D. Wegner, “Isprs semantic labeling contest,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">ISPRS: Leopoldshöhe, Germany</em>, vol. 1, no. 4, p. 4, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. M. Azimi, C. Henry, L. Sommer, A. Schumann, and E. Vig, “Skyscapes fine-grained semantic understanding of aerial scenes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2019, pp. 7393–7403.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Van Etten, D. Hogan, J. M. Manso, J. Shermeyer, N. Weir, and R. Lewis, “The multi-temporal urban development spacenet dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 6398–6407.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. Weinland, R. Ronfard, and E. Boyer, “A survey of vision-based methods for action representation, segmentation and recognition,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Computer vision and image understanding</em>, vol. 115, no. 2, pp. 224–241, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Sonka, V. Hlavac, and R. Boyle, <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Image processing, analysis and machine vision</em>.   Springer, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Guo, Y. Liu, T. Georgiou, and M. S. Lew, “A review of semantic segmentation using deep neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">International journal of multimedia information retrieval</em>, vol. 7, pp. 87–93, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional encoder-decoder architecture for image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 39, no. 12, pp. 2481–2495, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 3431–3440.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Wu, J. Zhang, K. Huang, K. Liang, and Y. Yu, “Fastfcn: Rethinking dilated convolution in the backbone for semantic segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:1903.11816</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Searching for mobilenetv3,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 1314–1324.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, “Asymmetric non-local neural networks for semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2019, pp. 593–602.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
L.-C. Chen, G. Papandreou, F. Schroff, and H. Adam, “Rethinking atrous convolution for semantic image segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:1706.05587</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen, “Twins: Revisiting the design of spatial attention in vision transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Advances in neural information processing systems</em>, vol. 34, pp. 9355–9366, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2017, pp. 2881–2890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
M.-H. Guo, C.-Z. Lu, Q. Hou, Z. Liu, M.-M. Cheng, and S.-M. Hu, “Segnext: Rethinking convolutional attention design for semantic segmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Advances in Neural Information Processing Systems</em>, vol. 35, pp. 1140–1156, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “An image is worth 16x16 words: Transformers for image recognition at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers for dense prediction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 12 179–12 188.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng, T. Xiang, P. H. Torr <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 6881–6890.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer for semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 7262–7272.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer: Simple and efficient design for semantic segmentation with transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Advances in neural information processing systems</em>, vol. 34, pp. 12 077–12 090, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin transformer: Hierarchical vision transformer using shifted windows,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 10 012–10 022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
L. Gao, W. Shi, Z. Miao, and Z. Lv, “Method based on edge constraint and fast marching for road centerline extraction from very high-resolution remote sensing images,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Remote Sensing</em>, vol. 10, no. 6, p. 900, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
L. Gao, W. Song, J. Dai, and Y. Chen, “Road extraction from high-resolution remote sensing imagery using refined deep residual convolutional neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Remote Sensing</em>, vol. 11, no. 5, p. 552, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
X. Zhang, X. Han, C. Li, X. Tang, H. Zhou, and L. Jiao, “Aerial image road extraction based on an improved generative adversarial network,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Remote Sensing</em>, vol. 11, no. 8, p. 930, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
S. M. Azimi, P. Fischer, M. Körner, and P. Reinartz, “Aerial lanenet: Lane-marking semantic segmentation in aerial imagery using wavelet-enhanced cost-sensitive symmetric fully convolutional neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">IEEE Transactions on Geoscience and Remote Sensing</em>, vol. 57, no. 5, pp. 2920–2938, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio, “The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</em>, 2017, pp. 11–19.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. He, Z. Jiang, K. Gao, S. Narges Fatholahi, W. Tan, B. Hu, H. Xu, M. A. Chapman, and J. Li, “Waterloo building dataset: A city-scale vector building dataset for mapping building footprints using aerial orthoimagery,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Geomatica</em>, vol. 75, no. 3, pp. 99–115, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet for the 2020s,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2022, pp. 11 976–11 986.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct  8 06:23:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
