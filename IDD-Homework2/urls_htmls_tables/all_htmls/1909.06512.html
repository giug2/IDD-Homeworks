<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1909.06512] On Safeguarding Privacy and Security in the Framework of Federated Learning</title><meta property="og:description" content="Motivated by the advancing computational capacity of wireless end-user equipment (UE), as well as the increasing concerns about sharing private data, a new machine learning (ML) paradigm has emerged, namely federated l…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="On Safeguarding Privacy and Security in the Framework of Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="On Safeguarding Privacy and Security in the Framework of Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1909.06512">

<!--Generated on Thu Mar 14 14:53:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated Learning,  Privacy,  Security
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On Safeguarding Privacy and Security in the Framework of Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuan Ma
,
Jun Li, <em id="id2.2.id1" class="ltx_emph ltx_font_italic">Senior Member, IEEE</em>,

Ming Ding, <em id="id3.3.id2" class="ltx_emph ltx_font_italic">Senior Member, IEEE</em>,

<br class="ltx_break">Howard H. Yang, <em id="id4.4.id3" class="ltx_emph ltx_font_italic">Member, IEEE</em>,

Feng Shu, <em id="id5.5.id4" class="ltx_emph ltx_font_italic">Member, IEEE</em>
<br class="ltx_break">Tony Q. S. Quek, <em id="id6.6.id5" class="ltx_emph ltx_font_italic">Fellow, IEEE</em>,
and H. Vincent Poor, <em id="id7.7.id6" class="ltx_emph ltx_font_italic">Fellow, IEEE</em>
</span><span class="ltx_author_notes">This work is supported in part by the National Key R<math id="id1.1.m1.1" class="ltx_Math" alttext="\&amp;" display="inline"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><and id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\&amp;</annotation></semantics></math>D Program under Grants 2018YFB1004800, and by National Natural Science Foundation of China under 61872184 and 61727802, and by the U.S. National Science Foundation under Grant CCF-1908308. The corresponding authors are Jun Li and Feng Shu.C. Ma, J. Li and F. Shu are with the School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, China. (e-mail: {chuan.ma, jun.li, feng.shu}@njust.edu.cn).J. Li is also with the Department of Software Engineering, Institute of Cybernetics, National Research Tomsk Polytechnic University, Tomsk, 634050, Russia.M. Ding is with Data61, CSIRO, Australia (e-mail: Ming.Ding@data61.csiro.au).H. H. Yang and T. Q. S. Quek are with the Information System Technology and Design Pillar, Singapore University of Technology and Design (e-mail: {howard yang, tonyquek}@sutd.edu.sg).H. V. Poor is with the Department of Electrical Engineering, Princeton University, Princeton, NJ 08544 USA (e-mail: poor@princeton.edu).</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id8.id1" class="ltx_p">Motivated by the advancing computational capacity of wireless end-user equipment (UE), as well as the increasing concerns about sharing private data, a new machine learning (ML) paradigm has emerged, namely federated learning (FL). Specifically, FL allows a decoupling of data provision at UEs and ML model aggregation at a central unit. By training model locally, FL is capable of avoiding direct data leakage from the UEs, thereby preserving privacy and security to some extend. However, even if raw data are not disclosed from UEs, individual’s private information can still be extracted by some recently discovered attacks against the FL architecture. In this work, we analyze the privacy and security issues in FL, and discuss several challenges on preserving privacy and security when designing FL systems. In addition, we provide extensive simulation results to showcase the discussed issues and possible solutions.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated Learning, Privacy, Security

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent technological advancements are currently transforming the ways in which data is created and processed. With the advent of the internet-of-things (IoT), the number of intelligent devices in the world is rapidly growing in the last couple of years. Many of these devices are equipped with various sensors and increasingly powerful hardware, which allow them to not just collect, but more importantly, process data at unprecedented scales.
In a concurrent development, artificial intelligence (AI) has revolutionized the ways that information is extracted with ground breaking successes in areas such as computer vision, natural language processing, voice recognition, etc<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Therefore, there is high demand for harnessing the rich data provided by distributed devices to improve machine learning models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">At the same time, data privacy has become a growing concern for clients. In particular, the emergence of centralized searchable data repositories has made the leakage of private information, e.g. health conditions, travel information, and financial data, an urgent social problem <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Furthermore, the diverse set of open data applications, such as census data dissemination and social networks, place more emphasis on privacy concerns. In such practices, the access to real-life datasets may cause information leakage even in pure research activities. Consequently, privacy preservation has become a critical issue.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To tackle the challenge of protecting individuals’ privacy, a new paradigm has emerged, i.e., federated learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which allows a decoupling of data provision at end-user equipment (UE) and machine learning model aggregation, such as network parameters of deep learning, at a centralized server. The purpose of FL is to cooperatively learn a global model without sacrificing the data privacy directly.
In particular, FL has distinct privacy advantages compared to data center training on a <span id="S1.p3.1.1" class="ltx_text">dataset<span id="S1.p3.1.1.1" class="ltx_text">. At a server, holding even an “anonymized” dataset can still put client privacy at risk via linkage to other datasets. In contrast, the information transmitted for FL consists of the minimal updates to improve a particular machine learning model. The updates themselves can be ephemeral, and will never contain more information than the raw training data (by the data processing inequality). Further, the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying metadata over a mixed network such as Tor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> or via a trusted third party. These generic approaches include de-identification methods like anonymization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, obfuscation methods like differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, cryptographic techniques like homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and secure multi-party computation (SMC) protocols like oblivious transfer and garbled circuits <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</span></span></p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, although the data is not explicitly shared in the original format, it is still possible for adversaries to reconstruct the raw data approximately,
especially when the architecture and parameters are not completely protected. In addition, FL can expose intermediate results such as parameter updates from an optimization algorithm like stochastic gradient descent (SGD), and the transmission of these gradients may actually leak private information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> when exposed together with a data structure such as image pixels. In addition, the existence of malevolent users may induce further security issues. Therefore, the design of FL still needs further protection of parameters as well as investigations on the tradeoffs between the privacy-security-level and the system performance.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Inspired by this research gap, we briefly investigate the potential privacy and security issues in FL. Specifically, we clarify that the current protection methods are mainly focused on the server and client side, and then investigate four important aspects of current designs, including convergence, data poisoning, scaling up and model aggregation.
The remainder of this article is organized as follows. Section II introduces the basic model and key directions on the protection of FL. Section III illustrates challenges and opportunities in developing private and secure FL, and Section IV provides probable solutions and future work for discussion. Finally, conclusions are drawn in Section V.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">We first introduce the basic model of FL, which is illustrated in Fig. <a href="#S2.F1" title="Figure 1 ‣ II Background ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As can be seen from Fig. <a href="#S2.F1" title="Figure 1 ‣ II Background ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, each client downloads a globally shared model from the broadcasting server for local training, whereas the server periodically collects all trained parameters to perform a global average and then redistributes the improved model back to the clients. After adequate training and updating iterations, usually termed as communication rounds, between the server and its associated clients, the objective function is able to converge to the global optimal, and the convergence property of FL can be quantitatively demonstrated.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1909.06512/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="203" height="111" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The structure of private and secure federated learning framework</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Difference between Security and Privacy</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Although security and privacy are used interchangeably in the literature, it is important to highlight the difference between them. On one hand, security issues refer to unauthorized/malicious access, change or denial to data. Such attacks are usually launched by hackers with expert knowledge of the target system or network. Hence, the fundamental three goals of security are confidentiality, integrity, and availability.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">On the other hand, privacy issues generally refer to unintentional disclosure of personal information, usually from open-access data. For example, from a side-by-side comparison of a vote registration dataset and an anonymous set of healthcare sensor records (e.g., no individual’s name and ID), an attacker may be able to identify certain individuals and learn about their health conditions. This is because some quasi-identifiers such as gender, birth date, and zip code are the same in both datasets. As can be seen from the above example, privacy attacks only require common sense and involve no hacking activities. The fundamental reason of privacy issues is that a seemingly harmless open dataset may contain clues to individual’s private information in real life. Hence, alternative goals such as anonymity, unlinkability, and unobservability have been proposed for privacy protection.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Security and Privacy Protection for FL</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">During the learning process there exists several privacy and security issues, and we can generally clarify the corresponding protection methods into three categories: privacy protection at the client side, privacy protection at the server side, and security protection for the FL.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Privacy protection at the client side</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In FL, clients will upload their learning results including parameter values and weights to the server, but they may not trust the server since a curious server might have a look at the uploaded data to infer private information. To alleviate this concern, clients can employ some privacy-preservation technologies as follows:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">Perturbation: The idea of perturbation is adding noise to the uploaded parameters by clients. This line of work often uses differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> to obscure certain sensitive attributes until the third party is not able to distinguish the individual, thereby making the data impossible to be restored so as to protect user privacy. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, authors introduced a differential privacy approach to FL in order to add protection to client-side data. However, the root of these methods still require that data are transmitted elsewhere and they usually involve a trade-off between accuracy and privacy, which needs adjustments.</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">Dummy: The concept of dummy method stems from the location privacy protection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Dummy model parameters along with the true one will be sent to the server from clients, which may hide client’s contribution during training. Because of the aggregation processed at the server, the system performance can still be guaranteed.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Privacy protection at the server side</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">After collecting updated parameters from clients, the server will perform a weighted average to these parameters according to data size. However, when the server broadcasts the aggregated parameters to clients for model synchronizing, this information may leak as there may exist eavesdroppers. Thus, protections at the server side are also of significance.</p>
<ul id="S2.I2" class="ltx_itemize">
<li id="S2.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i1.p1" class="ltx_para">
<p id="S2.I2.i1.p1.1" class="ltx_p">Aggregation: The key idea of aggregation is collecting data or model parameters from different clients on the server side. After aggregation, the adversaries or the un-trustful server cannot inspect client information according to this aggregated parameters. <span id="S2.I2.i1.p1.1.1" class="ltx_text">In addition, in some scenarios, sever has the liberty to select clients with high quality parameters or non-sensitive requirements.<span id="S2.I2.i1.p1.1.1.1" class="ltx_text"> However, the question on how to design an appropriate aggregation mechanism is still a challenging task for current FL.</span></span></p>
</div>
</li>
<li id="S2.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I2.i2.p1" class="ltx_para">
<p id="S2.I2.i2.p1.1" class="ltx_p">Secure Multi-Party Computation (SMC): The root of SMC is using encryption to make individual devices’ updates uninspected by a server, instead of only revealing the sum after a sufficient number of updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. In details, SMC is a four-round interactive protocol optionally enabled during the reporting phase of a given communication round. In each protocol round, the server gathers messages from all devices, then uses the set of device messages to compute an independent response and return to each device. The third round constitutes a commit phase, during which devices upload cryptographically masked model updates to the server. Finally, there is a finalization phase that devices reveal sufficient cryptographic secrets to allow the server to unmask the aggregated model update.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS3.4.1.1" class="ltx_text">II-B</span>3 </span>Security protection for FL framework</h4>

<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">As for the security of the whole FL framework, it mainly considers the model-stealing attacks. Specially, any participant in FL may introduce hidden backdoor functionality into the joint global model, e.g., to ensure that an image classifier assigns an attacker-chosen label to images with certain features, or that a word predictor completes certain sentences with an attacker. Consequently, there are also some protecting measures on the security design for FL.</p>
<ul id="S2.I3" class="ltx_itemize">
<li id="S2.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i1.p1" class="ltx_para">
<p id="S2.I3.i1.p1.1" class="ltx_p">Homomorphic Encryption: Homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is adopted to protect user data through parameters exchange under encryption mechanism. That is the parameters are coded before uploading, and the public-private decoding keys are also need to transmit, which may cause extra communication cost.</p>
</div>
</li>
<li id="S2.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I3.i2.p1" class="ltx_para">
<p id="S2.I3.i2.p1.1" class="ltx_p">Back-door Defender: Existing defenses against backdoor attacks are not effective as most of them require access to the training data. In addition, the FL system cannot ensure all clients are not malicious and has no visibility into what participants are doing locally, and prevents anyone from auditing participants’ updates to the joint model.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Challenges on Private and Secure FL</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we clarify four main issues in the private and secure FL system, and propose specific discussions on each issue.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Convergence: An Issue Caused by Privacy Protection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">As pointed out in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the theoretical convergence guarantees have not been fully explored in the federated average learning, even although recent works can provide approximate convergence guarantee to some extent. However, these works usually assumed unrealistic scenarios, e.g., (i) the data is either shared across devices or distributed in an independent and identically distributed (<em id="S3.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.i.d.</em>) manner, and (ii) all devices are involved in communication at each round.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">If privacy protection is considered, the convergence of FL cannot be guaranteed for the current system setting. The main reason is that learning parameters will be in a non <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">i.i.d.</em> manner if perturbation method is applied at the client side. Moreover, even if the convergence can be satisfied when appropriate measures are proposed, the learning performance should be properly characterized. Previous work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> has shown that convergence can be guaranteed when artificial noises are added into a deep learning network, but the learning accuracy decreases around 40% when solving a MNIST classification problem. As such, the following aspects need to be addressed:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Theoretical results should be provided about the convergence of privacy-preserving FL.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Learning performances, i.e., learning accuracy, communication rounds and variations of loss functions, need to be investigated when privacy protection is considered.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Privacy protection algorithm, both theoretically and empirically, should be devised. In addition, the tradeoff between the privacy level and the convergency speed also needs further investigation.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">To explain this using a concrete example, let us consider the perturbation method described in Section II-A. If artificial noises are added at the client side, the aggregated noise power will influence the updated system parameters, and these parameters are not i.i.d.. Thus, the global weighted parameters at the server side may appear differences from the original one without noises. When the SGD is applied, the descent trend may change to a different or even an opposite direction if inappropriate noise is added. In this way, we cannot guarantee the convergence of the algorithm. In addition, even if the convergence is satisfied, the reduction in convergency speed, i.e., the communication rounds between clients and the server, and the learning performance, i.e., the classification accuracy, should be carefully quantified and analyzed.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Data Poisoning: A Security Issue</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In FL, clients, who previously acted only as passive data providers, can now observe intermediate model states and might contribute arbitrary updates as part of the decentralized training process. This creates an opportunity for malicious clients to manipulate the training process with little restriction. In particular, adversaries posing as honest clients can send erroneous updates that maliciously influence the performances of the training model, a process that is known as model poisoning.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Traditional poisoning attacks compromise the training data to change the model’s behavior at inference time. Researchers have considered the situation when one of members of a FL system maliciously attacks others by allowing a backdoor to be inserted to filch others’ data <cite class="ltx_cite ltx_citemacro_cite">[<span class="ltx_ref ltx_missing_citation ltx_ref_self">8765347</span>]</cite>. They showed that an adversarial participant can infer membership as well as properties associated with a subset of the training data. In addition, some malicious clients may update unreasonable parameters, which in turn harm the system performance. On the other hand, there exists possible eavesdroppers during server broadcasting the intermediate machine learning model states. Thus,
the data poisoning on the security issues can be summarized as follows:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">How to measure the loss performance if any malicious clients produce data or model poisoning?</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">How to recognize and prevent these poisoning behaviors from clients?</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">How to improve the security level by preventing eavesdroppers during the communication?</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Scaling Up Issue: A Privacy and Security Issue</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">It is straightforward to extend the current FL system into a large one, e.g., hundreds or thousands of clients, due to the availability of high-performance and low-price devices. However, this vast scale will bring out several practical issues: device availability that correlates with the local data distribution in complex ways (e.g., time zone dependency); unreliable device connectivity and interrupted execution; orchestration of lock-step execution across devices with varying availability; and limited device storage and compute resources. All these issues can be concluded as scaling up issues, and the most important and urgent issue is what will happen if more UEs are able to participate in FL. Specifically, the following aspects need to be addressed:</p>
<ul id="S3.I3" class="ltx_itemize">
<li id="S3.I3.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i1.p1" class="ltx_para">
<p id="S3.I3.i1.p1.1" class="ltx_p">If more UEs participate in FL, it will lead to less communication rounds thanks to more computations in each round, which should be an obvious advantage.</p>
</div>
</li>
<li id="S3.I3.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i2.p1" class="ltx_para">
<p id="S3.I3.i2.p1.1" class="ltx_p">If more UEs participate in FL, there will be less impact of data poison attack because it becomes difficult for an adversary to control a large number of UEs.</p>
</div>
</li>
<li id="S3.I3.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I3.i3.p1" class="ltx_para">
<p id="S3.I3.i3.p1.1" class="ltx_p">If more UEs participate in FL, will it provide better privacy protection? The intuition is that hiding a UE in a larger dataset is easier than doing the same in a smaller dataset.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">In summary, it is unknown whether having more UEs is helpful to reduce the learning time or accuracy, and we will provide related experimental results in Section IV-C. In addition, a typical wireless scenario for scheme designing and performance investigation that multiple communication modes, i.e., LTE, WiFi, 5G, etc., exists in the uploading process. Resources allocation for these multiple modes needs to be optimized as most of works are not considering wireless transmission. In a wireless setting, the communication links between the server and the clients are uncertain and imperfect and this effect needs to be carefully studied in the design of the FL system, especially in the large scale one <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Model Aggregation: A Security Issue</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The aggregation is mainly processed at the server after collecting individuals’ parameters, and updates the global model. This process is particularly important as it should absorb the advantages of the clients and determine the end of learning. If protection method is applied at the client side, such as the perturbation applied before collecting model parameters, the aggregation cannot be simply a conventional averaging process. The main reasons can be concluded as: (i) the noise power of perturbation is increasing along with the number of clients; (ii) the server should know the stochastic information from clients and the design of the aggregation method needs to distinguish the privacy-sensitive clients from privacy-insensitive ones. Therefore, a more intelligent aggregation process should be provided as follows:</p>
<ul id="S3.I4" class="ltx_itemize">
<li id="S3.I4.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i1.p1" class="ltx_para">
<p id="S3.I4.i1.p1.1" class="ltx_p">An intelligent aggregator should recognize the differences of clients and employ different aggregating strategies for them.</p>
</div>
</li>
<li id="S3.I4.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i2.p1" class="ltx_para">
<p id="S3.I4.i2.p1.1" class="ltx_p">An intelligent aggregator should resolve the noise-added problem provided by the privacy protection. For example, the use of minimum mean square estimation (MMSE) aggregator can serve as an effective candidate.</p>
</div>
</li>
<li id="S3.I4.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I4.i3.p1" class="ltx_para">
<p id="S3.I4.i3.p1.1" class="ltx_p">An intelligent aggregator should update parameter weights for the participating clients during different communication rounds.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">In particular, some form of recognition mechanism can be integrated into the aggregation process. It is able to adjust the parameter weights according to the quality of parameters or system feedback. Furthermore, some anomaly detection schemes can be considered to identify outliers during communications. The aggregator should be sufficiently intelligent as it can select appropriate clients for learning to achieve fast convergence and high performance.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiment Results and Possible Solutions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we provide simulations to demonstrate the aforementioned issues and discuss some possible solutions.
For each experiment, we first partition the original training data into disjoint non i.i.d. training sets, and locally compute SGD updates on each dataset, and then aggregate updates using an averaging method to train a globally shared classifier. We evaluate the prototype on the well-known classification dataset: MNIST, a digit classification problem which distinguishes 10 digital number from 0 to 9, and the system fails to complete the classification if the accuracy cannot exceed 10%. The provided dataset in MNIST is divided into 60,000 training examples and 10,000 test examples. The global epoch is set to 300 iterations at the server side, while 120 iterations are implemented at each client side, and the local batch size is set to 1200. In the following figures, we collect 20 runs for each experiment and record the average results.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Convergence</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">In this subsection, we first show the classification accuracy with different noise powers, where local learning applies convolutional neutral network (CNN) system.
To achieve the privacy protection, we employ the perturbation method on the client side. In details, different artificial noises with same power i.e., gaussian noise <math id="S4.SS1.p1.1.m1.2" class="ltx_Math" alttext="N_{1}\sim N(0,\delta)" display="inline"><semantics id="S4.SS1.p1.1.m1.2a"><mrow id="S4.SS1.p1.1.m1.2.3" xref="S4.SS1.p1.1.m1.2.3.cmml"><msub id="S4.SS1.p1.1.m1.2.3.2" xref="S4.SS1.p1.1.m1.2.3.2.cmml"><mi id="S4.SS1.p1.1.m1.2.3.2.2" xref="S4.SS1.p1.1.m1.2.3.2.2.cmml">N</mi><mn id="S4.SS1.p1.1.m1.2.3.2.3" xref="S4.SS1.p1.1.m1.2.3.2.3.cmml">1</mn></msub><mo id="S4.SS1.p1.1.m1.2.3.1" xref="S4.SS1.p1.1.m1.2.3.1.cmml">∼</mo><mrow id="S4.SS1.p1.1.m1.2.3.3" xref="S4.SS1.p1.1.m1.2.3.3.cmml"><mi id="S4.SS1.p1.1.m1.2.3.3.2" xref="S4.SS1.p1.1.m1.2.3.3.2.cmml">N</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.1.m1.2.3.3.1" xref="S4.SS1.p1.1.m1.2.3.3.1.cmml">​</mo><mrow id="S4.SS1.p1.1.m1.2.3.3.3.2" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="S4.SS1.p1.1.m1.2.3.3.3.2.1" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">(</mo><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">0</mn><mo id="S4.SS1.p1.1.m1.2.3.3.3.2.2" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">,</mo><mi id="S4.SS1.p1.1.m1.2.2" xref="S4.SS1.p1.1.m1.2.2.cmml">δ</mi><mo stretchy="false" id="S4.SS1.p1.1.m1.2.3.3.3.2.3" xref="S4.SS1.p1.1.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.2b"><apply id="S4.SS1.p1.1.m1.2.3.cmml" xref="S4.SS1.p1.1.m1.2.3"><csymbol cd="latexml" id="S4.SS1.p1.1.m1.2.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.1">similar-to</csymbol><apply id="S4.SS1.p1.1.m1.2.3.2.cmml" xref="S4.SS1.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.2.3.2.1.cmml" xref="S4.SS1.p1.1.m1.2.3.2">subscript</csymbol><ci id="S4.SS1.p1.1.m1.2.3.2.2.cmml" xref="S4.SS1.p1.1.m1.2.3.2.2">𝑁</ci><cn type="integer" id="S4.SS1.p1.1.m1.2.3.2.3.cmml" xref="S4.SS1.p1.1.m1.2.3.2.3">1</cn></apply><apply id="S4.SS1.p1.1.m1.2.3.3.cmml" xref="S4.SS1.p1.1.m1.2.3.3"><times id="S4.SS1.p1.1.m1.2.3.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.3.1"></times><ci id="S4.SS1.p1.1.m1.2.3.3.2.cmml" xref="S4.SS1.p1.1.m1.2.3.3.2">𝑁</ci><interval closure="open" id="S4.SS1.p1.1.m1.2.3.3.3.1.cmml" xref="S4.SS1.p1.1.m1.2.3.3.3.2"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">0</cn><ci id="S4.SS1.p1.1.m1.2.2.cmml" xref="S4.SS1.p1.1.m1.2.2">𝛿</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.2c">N_{1}\sim N(0,\delta)</annotation></semantics></math> and Laplace noise <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="N_{2}\sim Lap(\lambda)" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.2" xref="S4.SS1.p1.2.m2.1.2.cmml"><msub id="S4.SS1.p1.2.m2.1.2.2" xref="S4.SS1.p1.2.m2.1.2.2.cmml"><mi id="S4.SS1.p1.2.m2.1.2.2.2" xref="S4.SS1.p1.2.m2.1.2.2.2.cmml">N</mi><mn id="S4.SS1.p1.2.m2.1.2.2.3" xref="S4.SS1.p1.2.m2.1.2.2.3.cmml">2</mn></msub><mo id="S4.SS1.p1.2.m2.1.2.1" xref="S4.SS1.p1.2.m2.1.2.1.cmml">∼</mo><mrow id="S4.SS1.p1.2.m2.1.2.3" xref="S4.SS1.p1.2.m2.1.2.3.cmml"><mi id="S4.SS1.p1.2.m2.1.2.3.2" xref="S4.SS1.p1.2.m2.1.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.2.3.1" xref="S4.SS1.p1.2.m2.1.2.3.1.cmml">​</mo><mi id="S4.SS1.p1.2.m2.1.2.3.3" xref="S4.SS1.p1.2.m2.1.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.2.3.1a" xref="S4.SS1.p1.2.m2.1.2.3.1.cmml">​</mo><mi id="S4.SS1.p1.2.m2.1.2.3.4" xref="S4.SS1.p1.2.m2.1.2.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p1.2.m2.1.2.3.1b" xref="S4.SS1.p1.2.m2.1.2.3.1.cmml">​</mo><mrow id="S4.SS1.p1.2.m2.1.2.3.5.2" xref="S4.SS1.p1.2.m2.1.2.3.cmml"><mo stretchy="false" id="S4.SS1.p1.2.m2.1.2.3.5.2.1" xref="S4.SS1.p1.2.m2.1.2.3.cmml">(</mo><mi id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">λ</mi><mo stretchy="false" id="S4.SS1.p1.2.m2.1.2.3.5.2.2" xref="S4.SS1.p1.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.2.cmml" xref="S4.SS1.p1.2.m2.1.2"><csymbol cd="latexml" id="S4.SS1.p1.2.m2.1.2.1.cmml" xref="S4.SS1.p1.2.m2.1.2.1">similar-to</csymbol><apply id="S4.SS1.p1.2.m2.1.2.2.cmml" xref="S4.SS1.p1.2.m2.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.2.2.1.cmml" xref="S4.SS1.p1.2.m2.1.2.2">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.2.2.2.cmml" xref="S4.SS1.p1.2.m2.1.2.2.2">𝑁</ci><cn type="integer" id="S4.SS1.p1.2.m2.1.2.2.3.cmml" xref="S4.SS1.p1.2.m2.1.2.2.3">2</cn></apply><apply id="S4.SS1.p1.2.m2.1.2.3.cmml" xref="S4.SS1.p1.2.m2.1.2.3"><times id="S4.SS1.p1.2.m2.1.2.3.1.cmml" xref="S4.SS1.p1.2.m2.1.2.3.1"></times><ci id="S4.SS1.p1.2.m2.1.2.3.2.cmml" xref="S4.SS1.p1.2.m2.1.2.3.2">𝐿</ci><ci id="S4.SS1.p1.2.m2.1.2.3.3.cmml" xref="S4.SS1.p1.2.m2.1.2.3.3">𝑎</ci><ci id="S4.SS1.p1.2.m2.1.2.3.4.cmml" xref="S4.SS1.p1.2.m2.1.2.3.4">𝑝</ci><ci id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">𝜆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">N_{2}\sim Lap(\lambda)</annotation></semantics></math> are added to the local parameters, respectively.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1909.06512/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="167" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Communication rounds versus accuracy with different noise powers in CNN</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">From the figure, we can observe that the accuracy performance is largely affected by the added noise while less influenced by the particular distribution of the noise.
In addition,
it will lead to poor performance or even system failure when large noises, i.e., <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="\lambda=10" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi id="S4.SS1.p2.1.m1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.cmml">λ</mi><mo id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.p2.1.m1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><eq id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">𝜆</ci><cn type="integer" id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\lambda=10</annotation></semantics></math>, are added. This is due to the fact that the SGD algorithm has converged to a poor local minimum solution.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/1909.06512/assets/x3.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="167" alt="Refer to caption">
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Communication rounds versus accuracy with different noise powers in MLP</figcaption>
</figure>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In addition, we verify this observation by applying multi-layer perception (MLP) system at clients. As can be seen in Fig. <a href="#S4.F3" title="Figure 3 ‣ IV-A Convergence ‣ IV Experiment Results and Possible Solutions ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the added noise seems to have slight influence on the accuracy. <span id="S4.SS1.p3.1.1" class="ltx_text"> It it mainly because in the MLP system there is an auto-filtering process which can delete perceptions or parameters with bad performance.<span id="S4.SS1.p3.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Technical Problems:</span> The fundamental relationship between the convergence bound and the noise power needs to be characterized.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p"><span id="S4.SS1.p5.1.1" class="ltx_text ltx_font_bold">Solution:</span> In the noise-added FL system, there is a fundamental tradeoff between the privacy level and learning performance. Intuitively speaking, a higher privacy level will incur more noise on the system, and lead to worse learning performance. Therefore, it is necessary to investigate the theoretical relationship between the noise scale, the local training iterations, the number of communication rounds as well as number of clients.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Data Poisoning</span>
</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/1909.06512/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance comparison with different number of malicious clients in CNN</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Data Poisoning ‣ IV Experiment Results and Possible Solutions ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show the performance comparison with different number of malicious clients. We set a CNN system for 30 clients, and the malicious clients will upload fake value of parameters in each communication round. The fake value can be the opposite of the true value, or random numbers within [-1, 1]. From Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-B Data Poisoning ‣ IV Experiment Results and Possible Solutions ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we can see that the system performance will be influenced if malicious clients exist. In addition, the system will fail when more malicious clients participating in.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Technical Problems:</span> Mechanisms to prevent data poisoning need to be investigated.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Solution:</span> There are three main ways to prevent the data poisoning in privacy-aware FL system. The first one is to recognize malicious clients when the system sets up. In this scenario, machine learning techniques can be utilized. For example, a supervised learning algorithm can be used to find malicious clients during each communication round. Another technique focuses on the aggregation process. After each aggregation, according to the quality of the uploaded learning parameters from the clients, the server can adjust the aggregation weights for each client. In this way, the server is able to put more confidence in the clients that are more helpful to achieve fast convergence and good learning performance. Third, concepts from social networks can be applied to update the weights in each communication round by exploiting the social relationship of each client to the overall system performance.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Scaling Up Issue</span>
</h3>

<figure id="S4.F5" class="ltx_figure"><img src="/html/1909.06512/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="163" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance comparison with different number of clients in CNN</figcaption>
</figure>
<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">In this subsection, we first show the classification accuracy with different clients numbers. From Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-C Scaling Up Issue ‣ IV Experiment Results and Possible Solutions ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we can find that with the increasing number of clients, the performance does not show much gain. However, the total delay can be largely reduced when more clients exists. In particular, the clients are randomly distributed in a <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="1\times 1" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">×</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">1</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">1\times 1</annotation></semantics></math> km<sup id="S4.SS3.p1.3.1" class="ltx_sup">2</sup> square area and we record the summation of the maximum calculation and transmission time as the delay in each communication round for different number of clients. Then we set the learning stops when the accuracy exceeds <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="90\%" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">90</mn><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">90</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">90\%</annotation></semantics></math> and record the total communication round, and calculate the total delay. Note that this result might be different for other delay models.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p"><span id="S4.SS3.p2.1.1" class="ltx_text ltx_font_bold">Technical Problems:</span> In a large scale network, the server may suffer from a long waiting time and complicated resource allocation during parameter uploading.</p>
</div>
<div id="S4.SS3.p3" class="ltx_para">
<p id="S4.SS3.p3.1" class="ltx_p"><span id="S4.SS3.p3.1.1" class="ltx_text ltx_font_bold">Solution:</span> For the scaling up issue, one promising method to address the long waiting time is setting up an upload delay deadline for each client. At each learning epoch, server will collect at least required clients’ parameters before executing next round of FL. If the waiting time exceeds this deadline, the current learning epoch is abandoned. In addition, to deal with the large number of clients, we can use the concept of user clustering in game theory. By partitioning clients into different clusters factitiously, each cluster of clients will compete with each other to complete the learning goal. The server will also provide benefits in return. In this new structure design, the large number of clients will be separated by their common interests, similar physical location or same uploading ways.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.4.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.5.2" class="ltx_text ltx_font_italic">Model Aggregation</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The model aggregation should be intelligent. It not only deals with the large amount of noise while guaranteeing the system performance, but also applies various aggregation methods for different clients. In the traditional federated learning setting, the current strategy of the aggregation weight depends to the training size, but a more intelligent aggregator should be designed for multiple objectives. In addition, the selection for the updated parameters can also be adjusted. For example, the server can choose the uploading ones with better channel or parameter qualities.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/1909.06512/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="184" height="167" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance comparison with different number of malicious clients under the proposed aggregation method in CNN</figcaption>
</figure>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p"><span id="S4.SS4.p2.1.1" class="ltx_text ltx_font_bold">Technical Problems:</span> We need to design an intelligent aggregator in the current FL setting.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p"><span id="S4.SS4.p3.1.1" class="ltx_text ltx_font_bold">Solution:</span> In Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-D Model Aggregation ‣ IV Experiment Results and Possible Solutions ‣ On Safeguarding Privacy and Security in the Framework of Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we propose an intelligent aggregation method to address the malicious clients’ problem. The proposed algorithm includes two parts: 1) Add a test process at the server side, and update the aggregation weight according to the testing performance for the uploaded parameters from each client. 2) Increase the local epochs for each client. As can be seen in the figure, the proposed algorithm can alleviate the performance degradation caused by the malicious clients. In addition, more local epochs are needed when more malicious clients exist in the FL system.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this article, we have investigated potential privacy and security issues in federated learning (FL). We have pointed out that the privacy protection can be carried out at the client or the server side and security protection is mainly meant for the system level. In addition, we have argued that the considered issues can be classified into convergence, data poisoning, scaling up and model aggregation problems. Lastly, we have also provided some possible solutions for protecting privacy and security in designing FL systems.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
X. Wang, Y. Han, C. Wang, Q. Zhao, X. Chen, and M. Chen, “In-edge ai:
Intelligentizing mobile edge computing, caching and communication by
federated learning,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1809.07857</em>, 2018.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
B. Liu, M. Ding, T. Zhu, Y. Xiang, and W. Zhou, “Adversaries or allies?
privacy and deep learning in big data era,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Concurrency and
Computation: Practice and Experience</em>, p. e5102, 2019.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Konečnỳ, H. B. McMahan, D. Ramage, and P. Richtárik,
“Federated optimization: Distributed machine learning for on-device
intelligence,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02527</em>, 2016.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
D. L. Chaum, “Untraceable electronic mail, return addresses, and digital
pseudonyms,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Communications of the ACM</em>, vol. 24, no. 2, pp. 84–90,
1981.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Machanavajjhala, J. Gehrke, D. Kifer, and M. Venkitasubramaniam,
“l-diversity: Privacy beyond k-anonymity,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">22nd International
Conference on Data Engineering (ICDE’06)</em>.   IEEE, 2006, pp. 24–24.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating noise to
sensitivity in private data analysis,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Theory of cryptography
conference</em>.   Springer, 2006, pp.
265–284.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
N. Papernot, M. Abadi, U. Erlingsson, I. Goodfellow, and K. Talwar,
“Semi-supervised knowledge transfer for deep learning from private training
data,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.05755</em>, 2016.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. Rosulek, “Improvements for gate-hiding garbled circuits,” in
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">International Conference on Cryptology in India</em>.   Springer, 2017, pp. 325–345.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai,
“Privacy-preserving deep learning via additively homomorphic encryption,”
<em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Information Forensics and Security</em>, vol. 13,
no. 5, pp. 1333–1345, May 2018.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. C. Geyer, T. Klein, and M. Nabi, “Differentially private federated
learning: A client level perspective,” <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1712.07557</em>, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
H. Kido, Y. Yanagisawa, and T. Satoh, “Protection of location privacy
using dummies for location-based services,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">21st International
Conference on Data Engineering Workshops (ICDEW’05)</em>, April 2005, pp.
1248–1248.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor
federated learning,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.00459</em>, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. K. Sahu, T. Li, M. Sanjabi, M. Zaheer, A. Talwalkar, and V. Smith, “On the
convergence of federated optimization in heterogeneous networks,” 2018.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and
L. Zhang, “Deep learning with differential privacy,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings
of the 2016 ACM SIGSAC Conference on Computer and Communications
Security</em>.   ACM, 2016, pp. 308–318.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. H. Yang, Z. Liu, T. Q. Quek, and H. V. Poor, “Scheduling policies for
federated learning in wireless networks,” <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Commun.,
revised. Available as ArXiv: 1908.06287</em>, 2019.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1909.06511" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1909.06512" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1909.06512">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1909.06512" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1909.06513" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 14:53:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
