<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.05580] Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels</title><meta property="og:description" content="Accurate product information is critical for e-commerce stores to allow customers to browse, filter, and search for products. Product data quality is affected by missing or incorrect information resulting in poor custoâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.05580">

<!--Generated on Thu Feb 29 09:04:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Domain adaptation Synthetic data Pattern classification.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Zalando SE, Valeska-Gert-StraÃŸe 5, 10243 Berlin, Germany </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Insight Centre for Data Analytics, Dublin City University, Dublin, Ireland</span></span></span>
<h1 class="ltx_title ltx_title_document">Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Enric Moreu 
</span><span class="ltx_author_notes">1122
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-0555-3013" title="ORCID identifier" class="ltx_ref">0000-0002-0555-3013</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alex Martinelli
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Martina Naughton
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Philip Kelly
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Noel E. Oâ€™Connor
</span><span class="ltx_author_notes">22
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4033-9135" title="ORCID identifier" class="ltx_ref">0000-0002-4033-9135</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Accurate product information is critical for e-commerce stores to allow customers to browse, filter, and search for products. Product data quality is affected by missing or incorrect information resulting in poor customer experience. While machine learning can be used to correct inaccurate or missing information, achieving high performance on fashion image classification tasks requires large amounts of annotated data, but it is expensive to generate due to labeling costs. One solution can be to generate synthetic data which requires no manual labeling. However, training a model with a dataset of solely synthetic images can lead to poor generalization when performing inference on real-world data because of the domain shift. We introduce a new unsupervised domain adaptation technique that converts images from the synthetic domain into the real-world domain. Our approach combines a generative neural network and a classifier that are jointly trained to produce realistic images while preserving the synthetic label information. We found that using real-world pseudo-labels during training helps the classifier to generalize in the real-world domain, reducing the synthetic bias. We successfully train a visual pattern classification model in the fashion domain without real-world annotations. Experiments show that our method outperforms other unsupervised domain adaptation algorithms.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Domain adaptation Synthetic data Pattern classification.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In 2021, 75% of EU internet users bought goods or services onlineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. One of the main drivers of increased e-commerce engagement has been convenience, allowing customers to browse and purchase a wide variety of categories and brands in a single site.
If important product metadata is either missing or incorrect, it becomes difficult for customers to find products as the number of available products on e-commerce sites grows.
Online stores typically offer a set of filters (e.g. pattern, color, size, or sleeve length) that make use of such metadata and help customers to find specific products. If such critical information is missing or incorrect then the product cannot be effectively merchandised. Machine learning has been used for fashion e-commerce in recent works to analyze product images, e.g. clothes retrievalÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, detecting the outlineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or to find clothes that match an outfitÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. In this paper, our prime interest is a visual classification task which consists of classifying patterns in catalog images of clothing. Patterns describe the decorative design of clothes, and they are important because they are widely used by customers to find products online. Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows fashion visual pattern examples in the syntetic and real-world domains.</p>
</div>
<figure id="S1.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth2.png" id="S1.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth3.png" id="S1.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/real1.png" id="S1.F1.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/real2.png" id="S1.F1.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/real3.png" id="S1.F1.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic samples from our Zalando SDG dataset (first row) and real samples from the DeepFashion datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> (second row) representing the striped, floral, and plain categories.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Fashion pattern classification is challenging. Fashion images often include models in different poses with complex backgrounds. Achieving high performance requires large annotated datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. However, public datasets are only available for non-commercial use or do not cover the specific attributes or diversity we require, while generating private datasets with fine-grained and balanced annotations is expensive. In addition, publicly available fashion datasets typically have underrepresented classes with only a few samples. For example, in the Deep Fashion datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> there are 6633 images with the â€œsolidâ€ pattern while only 242 images contain the â€œlatticeâ€ pattern. Categories that are underrepresented during training achieve a lower performance, thus reducing the overall performance.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">We address these problems by generating artificial samples using Synthetic Data Generation (SDG) techniques. Synthetic data has shown promising results in domains where few images are available for trainingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
The main advantage of synthetic data is that it can generate unlimited artificial images because labels are automatically produced by the 3D engine when rendering the images.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">However, synthetic images are not a precise reflection of the real-world domain in which the model will operate. Computer vision models are easily biased by the underlying distribution in which they are trainedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Even if the synthetic images use realistic lighting and textures that look realistic to humans, the model will tend to over-optimize against the traits of the synthetic domain and wonâ€™t generalize well to real data.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We consider this problem in the context of unsupervised domain adaptationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> by using the knowledge from another related domain where annotations are available. We assume that we have abundant annotated data on the source domain (synthetic images) and a target domain (real images) where no labels are available.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Unsupervised domain adaptation has shown excellent results when translating images to other domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> ; nevertheless, translated images canâ€™t be readily used to train classification models because image features, such as patterns, are distorted during the translation step since the translation model doesnâ€™t have information about the features.
Specifically, when complex patterns are shifted to a different domain, they can be distorted to a level that they no longer adhere to the original pattern label for the synthetic image. For example, when an image with the â€œcamouflageâ€ pattern is translated from the synthetic to the real domain, the pattern could be accidentally distorted to â€œfloralâ€.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">In this paper, we introduce a new unsupervised domain adaptation approach that doesnâ€™t require groundtruth labels. First, we produce a synthetic dataset for fashion pattern classification using SDG that equally represents all the classes. Second, we jointly train a generative model and a classifier that will make synthetic images look realistic while preserving the class patterns. In the final stage of the training, real-world pseudo-labeled images are used to improve the model generalization toward real images. The contributions of this paper are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a novel architecture that performs the image translation task while jointly training a classification model.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We outperform other state-of-the-art unsupervised domain adaptation algorithms in the visual fashion pattern classification task.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">The remainder of the paper is organized as follows: Section 2 reviews relevant work; Section 3 explains our method; Section 4 presents our synthetic dataset and experiments, and Section 5 concludes the paper.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Synthetic data has been used extensively in the computer vision field. Techniques to generate synthetic datasets range from simple methods generating primitive shapesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to photorealistic rendering using game enginesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Although high quality synthetic images can appear realistic to humans, they donâ€™t necessarily help the computer vision models to generalize to real-world images. Convolutional neural networks easily overfit on synthetic traits that are not present in the real-world.
This is addressed by using domain adaptation techniques that reduces the disparity between the synthetic and real domains. Some works approach domain adaptation by simply improving the realism aspectÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, or by pushing the randomization and distribution coverage at the sourceÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. These approaches imply additional modeling effort and longer generation times per image, for example by relying on physically based renderers for higher photorealistic results, making the synthetic data better match the real data distribution.</p>
</div>
<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph"></h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">In the context of unsupervised domain adaptation, non-adversarial approaches consist of matching feature distributions in the source and the target domain by transforming the feature space to map the target distributionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Gong et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> found that gradually shifting the domains during training improved the methodâ€™s stability. Recent methods are based on generative adversarial networksÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> because of their unsupervised and unpaired nature. Generative domain adaptation approaches rely on a domain discriminator that distinguishes the source and target domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and updates the generator to produce better images. Our approach improves existing adversarial approaches by optimizing a classifier alongside the generator, producing realistic data that retrain the source category.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Fashion CUT</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our approach has two components: 1) An image translation network that generates realistic images. 2) A classifier that enforces the generated images to keep the class patterns. The overall architecture is shown in Figure <a href="#S3.F2" title="Figure 2 â€£ 3 Fashion CUT â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2305.05580/assets/images/arch.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="485" height="201" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The proposed architecture includes a translation model (CUT) and a classifier model (ResNet50), which are optimized together via a common loss that ensures realistic images with reliable annotations. Pseudo-labeled real images are included in each mini-batch to improve the classifier generalization.</figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Acquiring paired images from both domains can be difficult to achieve in the fashion domain, resulting in high costs. As such, we use Contrastive Unpaired Translation (CUT)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> for the image translation module. Synthetic images donâ€™t have to match the exact position or texture of real images in the dataset because we use an unpaired translation method. CUT learns a mapping that translates unpaired images from the source domain to the target domain. It operates on patches that are mapped to a similar point in learned feature space using an infoNCEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> contrastive loss.
In addition, CUT uses less GPU memory than other two-sided image translation models (e.g. CycleGAN) because it only requires one generator and one discriminator. By reducing memory usage, the joint training of an additional classifier becomes tractable on low cost GPU setups with less than 16GB of memory.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">While CUT produces realistic images, the class patterns can be lost or mixed with other classes since CUT doesnâ€™t enforce that these category features are consistent across the image translation. The generatorâ€™s only objective is to produce realistic images that resemble the real-world domain, but it ignores the nature of each pattern. Any pattern distorted during the translation will impact the performance of a classifier trained on this synthetic data. Figure <a href="#S3.F3" title="Figure 3 â€£ 3 Fashion CUT â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> showcases unsuccessful examples of mixed patterns by the generator, and Figure <a href="#S3.F4" title="Figure 4 â€£ 3 Fashion CUT â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows successful translations using Fashion CUT.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/normal_synth1.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/normal_synth2.png" id="S3.F3.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/normal_synth3.png" id="S3.F3.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/bad_synth1.png" id="S3.F3.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/bad_synth2.png" id="S3.F3.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/bad_synth3.png" id="S3.F3.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> Synthetic images (first row) and unsuccessfully adapted images using CUT (second row) due to shifted patterns by the generator when not imposing class constraints.</figcaption>
</figure>
<figure id="S3.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth_fcut_1.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth_fcut_2.png" id="S3.F4.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/synth_fcut_3.png" id="S3.F4.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img src="/html/2305.05580/assets/images/fcut1.png" id="S3.F4.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/fcut2.png" id="S3.F4.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2305.05580/assets/images/fcut3.png" id="S3.F4.g6" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="125" height="125" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Synthetic images (first row) and adapted domain images using Fashion CUT(second row).</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">In order to enforce stability in the generated patterns, we add a ResNet50 model that predicts the category of the images generated by CUT. The classifier is optimized alongside the CUT generator to fulfill both classification and translation tasks. Figure <a href="#S3.F5" title="Figure 5 â€£ 3 Fashion CUT â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows how the classifier preserves the pattern features in comparison to vanilla CUT. Training both models simultaneously is faster and provides better results than training them separately.
The generator loss function is given by:</p>
</div>
<div id="S3.p5" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.55" class="ltx_Math" alttext="\begin{split}\lambda{g}*\mathcal{L}_{\mathit{GAN}}(\mathit{G,D,X,Y})+\lambda{c}*\mathcal{L}_{\mathit{classifier}}(\mathit{C})+\\
\lambda{ncex}*\mathcal{L}_{\mathit{NCEx}}(\mathit{G,D,X})+\lambda{ncey}*\mathcal{L}_{\mathit{NCEy}}(\mathit{G,D,Y})\end{split}" display="block"><semantics id="S3.E1.m1.55a"><mtable displaystyle="true" rowspacing="0pt" id="S3.E1.m1.55.55" xref="S3.E1.m1.55.56.1.cmml"><mtr id="S3.E1.m1.55.55a" xref="S3.E1.m1.55.56.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.55.55b" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.25" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.25.2" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.25.2.1" xref="S3.E1.m1.55.56.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.24.24.24.25.2.1.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.2.2.2.2.2.2" xref="S3.E1.m1.2.2.2.2.2.2.cmml">g</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.3.3.3.3.3.3" xref="S3.E1.m1.3.3.3.3.3.3.cmml">âˆ—</mo><msub id="S3.E1.m1.24.24.24.24.24.25.2.2" xref="S3.E1.m1.55.56.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.4.4.4.4.4.4" xref="S3.E1.m1.4.4.4.4.4.4.cmml">â„’</mi><mi id="S3.E1.m1.5.5.5.5.5.5.1" xref="S3.E1.m1.5.5.5.5.5.5.1.cmml">ğºğ´ğ‘</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.24.24.24.25.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mrow id="S3.E1.m1.24.24.24.24.24.25.3" xref="S3.E1.m1.55.56.1.cmml"><mo stretchy="false" id="S3.E1.m1.6.6.6.6.6.6" xref="S3.E1.m1.55.56.1.cmml">(</mo><mi id="S3.E1.m1.7.7.7.7.7.7" xref="S3.E1.m1.7.7.7.7.7.7.cmml">G</mi><mo id="S3.E1.m1.8.8.8.8.8.8" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.9.9.9.9.9.9" xref="S3.E1.m1.9.9.9.9.9.9.cmml">D</mi><mo id="S3.E1.m1.10.10.10.10.10.10" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.11.11.11.11.11.11" xref="S3.E1.m1.11.11.11.11.11.11.cmml">X</mi><mo id="S3.E1.m1.12.12.12.12.12.12" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.13.13.13.13.13.13" xref="S3.E1.m1.13.13.13.13.13.13.cmml">Y</mi><mo stretchy="false" id="S3.E1.m1.14.14.14.14.14.14" xref="S3.E1.m1.55.56.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.15.15.15.15.15.15" xref="S3.E1.m1.15.15.15.15.15.15.cmml">+</mo><mrow id="S3.E1.m1.24.24.24.24.24.26" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.26.2" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.26.2.2" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.24.24.24.24.24.26.2.2.1" xref="S3.E1.m1.55.56.1.cmml"><mi id="S3.E1.m1.16.16.16.16.16.16" xref="S3.E1.m1.16.16.16.16.16.16.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.24.24.24.26.2.2.1.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.17.17.17.17.17.17" xref="S3.E1.m1.17.17.17.17.17.17.cmml">c</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.18.18.18.18.18.18" xref="S3.E1.m1.18.18.18.18.18.18.cmml">âˆ—</mo><msub id="S3.E1.m1.24.24.24.24.24.26.2.2.2" xref="S3.E1.m1.55.56.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.19.19.19.19.19.19" xref="S3.E1.m1.19.19.19.19.19.19.cmml">â„’</mi><mi id="S3.E1.m1.20.20.20.20.20.20.1" xref="S3.E1.m1.20.20.20.20.20.20.1.cmml">ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.24.24.24.24.24.26.2.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mrow id="S3.E1.m1.24.24.24.24.24.26.2.3" xref="S3.E1.m1.55.56.1.cmml"><mo stretchy="false" id="S3.E1.m1.21.21.21.21.21.21" xref="S3.E1.m1.55.56.1.cmml">(</mo><mi id="S3.E1.m1.22.22.22.22.22.22" xref="S3.E1.m1.22.22.22.22.22.22.cmml">C</mi><mo stretchy="false" id="S3.E1.m1.23.23.23.23.23.23" xref="S3.E1.m1.55.56.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.24.24.24.24.24.24" xref="S3.E1.m1.55.56.1.cmml">+</mo></mrow></mrow></mtd></mtr><mtr id="S3.E1.m1.55.55c" xref="S3.E1.m1.55.56.1.cmml"><mtd class="ltx_align_right" columnalign="right" id="S3.E1.m1.55.55d" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31.32" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31.32.2" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31.32.2.1" xref="S3.E1.m1.55.56.1.cmml"><mi id="S3.E1.m1.25.25.25.1.1.1" xref="S3.E1.m1.25.25.25.1.1.1.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.32.2.1.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.26.26.26.2.2.2" xref="S3.E1.m1.26.26.26.2.2.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.32.2.1.1a" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.27.27.27.3.3.3" xref="S3.E1.m1.27.27.27.3.3.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.32.2.1.1b" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.28.28.28.4.4.4" xref="S3.E1.m1.28.28.28.4.4.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.32.2.1.1c" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.29.29.29.5.5.5" xref="S3.E1.m1.29.29.29.5.5.5.cmml">x</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.30.30.30.6.6.6" xref="S3.E1.m1.30.30.30.6.6.6.cmml">âˆ—</mo><msub id="S3.E1.m1.55.55.55.31.31.32.2.2" xref="S3.E1.m1.55.56.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.31.31.31.7.7.7" xref="S3.E1.m1.31.31.31.7.7.7.cmml">â„’</mi><mi id="S3.E1.m1.32.32.32.8.8.8.1" xref="S3.E1.m1.32.32.32.8.8.8.1.cmml">ğ‘ğ¶ğ¸ğ‘¥</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.32.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mrow id="S3.E1.m1.55.55.55.31.31.32.3" xref="S3.E1.m1.55.56.1.cmml"><mo stretchy="false" id="S3.E1.m1.33.33.33.9.9.9" xref="S3.E1.m1.55.56.1.cmml">(</mo><mi id="S3.E1.m1.34.34.34.10.10.10" xref="S3.E1.m1.34.34.34.10.10.10.cmml">G</mi><mo id="S3.E1.m1.35.35.35.11.11.11" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.36.36.36.12.12.12" xref="S3.E1.m1.36.36.36.12.12.12.cmml">D</mi><mo id="S3.E1.m1.37.37.37.13.13.13" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.38.38.38.14.14.14" xref="S3.E1.m1.38.38.38.14.14.14.cmml">X</mi><mo stretchy="false" id="S3.E1.m1.39.39.39.15.15.15" xref="S3.E1.m1.55.56.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.40.40.40.16.16.16" xref="S3.E1.m1.55.56.1.cmml">+</mo><mrow id="S3.E1.m1.55.55.55.31.31.33" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31.33.2" xref="S3.E1.m1.55.56.1.cmml"><mrow id="S3.E1.m1.55.55.55.31.31.33.2.1" xref="S3.E1.m1.55.56.1.cmml"><mi id="S3.E1.m1.41.41.41.17.17.17" xref="S3.E1.m1.41.41.41.17.17.17.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.33.2.1.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.42.42.42.18.18.18" xref="S3.E1.m1.42.42.42.18.18.18.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.33.2.1.1a" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.43.43.43.19.19.19" xref="S3.E1.m1.43.43.43.19.19.19.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.33.2.1.1b" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.44.44.44.20.20.20" xref="S3.E1.m1.44.44.44.20.20.20.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.33.2.1.1c" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mi id="S3.E1.m1.45.45.45.21.21.21" xref="S3.E1.m1.45.45.45.21.21.21.cmml">y</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.46.46.46.22.22.22" xref="S3.E1.m1.46.46.46.22.22.22.cmml">âˆ—</mo><msub id="S3.E1.m1.55.55.55.31.31.33.2.2" xref="S3.E1.m1.55.56.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.47.47.47.23.23.23" xref="S3.E1.m1.47.47.47.23.23.23.cmml">â„’</mi><mi id="S3.E1.m1.48.48.48.24.24.24.1" xref="S3.E1.m1.48.48.48.24.24.24.1.cmml">ğ‘ğ¶ğ¸ğ‘¦</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.55.55.55.31.31.33.1" xref="S3.E1.m1.55.56.1.cmml">â€‹</mo><mrow id="S3.E1.m1.55.55.55.31.31.33.3" xref="S3.E1.m1.55.56.1.cmml"><mo stretchy="false" id="S3.E1.m1.49.49.49.25.25.25" xref="S3.E1.m1.55.56.1.cmml">(</mo><mi id="S3.E1.m1.50.50.50.26.26.26" xref="S3.E1.m1.50.50.50.26.26.26.cmml">G</mi><mo id="S3.E1.m1.51.51.51.27.27.27" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.52.52.52.28.28.28" xref="S3.E1.m1.52.52.52.28.28.28.cmml">D</mi><mo id="S3.E1.m1.53.53.53.29.29.29" xref="S3.E1.m1.55.56.1.cmml">,</mo><mi id="S3.E1.m1.54.54.54.30.30.30" xref="S3.E1.m1.54.54.54.30.30.30.cmml">Y</mi><mo stretchy="false" id="S3.E1.m1.55.55.55.31.31.31" xref="S3.E1.m1.55.56.1.cmml">)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.E1.m1.55b"><apply id="S3.E1.m1.55.56.1.cmml" xref="S3.E1.m1.55.55"><plus id="S3.E1.m1.15.15.15.15.15.15.cmml" xref="S3.E1.m1.15.15.15.15.15.15"></plus><apply id="S3.E1.m1.55.56.1.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.2.1.cmml" xref="S3.E1.m1.55.55"></times><apply id="S3.E1.m1.55.56.1.2.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.3.3.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3.3.3"></times><apply id="S3.E1.m1.55.56.1.2.2.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.2.2.2.1.cmml" xref="S3.E1.m1.55.55"></times><ci id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">ğœ†</ci><ci id="S3.E1.m1.2.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.2.2">ğ‘”</ci></apply><apply id="S3.E1.m1.55.56.1.2.2.3.cmml" xref="S3.E1.m1.55.55"><csymbol cd="ambiguous" id="S3.E1.m1.55.56.1.2.2.3.1.cmml" xref="S3.E1.m1.55.55">subscript</csymbol><ci id="S3.E1.m1.4.4.4.4.4.4.cmml" xref="S3.E1.m1.4.4.4.4.4.4">â„’</ci><ci id="S3.E1.m1.5.5.5.5.5.5.1.cmml" xref="S3.E1.m1.5.5.5.5.5.5.1">ğºğ´ğ‘</ci></apply></apply><vector id="S3.E1.m1.55.56.1.2.3.cmml" xref="S3.E1.m1.55.55"><ci id="S3.E1.m1.7.7.7.7.7.7.cmml" xref="S3.E1.m1.7.7.7.7.7.7">ğº</ci><ci id="S3.E1.m1.9.9.9.9.9.9.cmml" xref="S3.E1.m1.9.9.9.9.9.9">ğ·</ci><ci id="S3.E1.m1.11.11.11.11.11.11.cmml" xref="S3.E1.m1.11.11.11.11.11.11">ğ‘‹</ci><ci id="S3.E1.m1.13.13.13.13.13.13.cmml" xref="S3.E1.m1.13.13.13.13.13.13">ğ‘Œ</ci></vector></apply><apply id="S3.E1.m1.55.56.1.3.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.3.1.cmml" xref="S3.E1.m1.55.55"></times><apply id="S3.E1.m1.55.56.1.3.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.18.18.18.18.18.18.cmml" xref="S3.E1.m1.18.18.18.18.18.18"></times><apply id="S3.E1.m1.55.56.1.3.2.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.3.2.2.1.cmml" xref="S3.E1.m1.55.55"></times><ci id="S3.E1.m1.16.16.16.16.16.16.cmml" xref="S3.E1.m1.16.16.16.16.16.16">ğœ†</ci><ci id="S3.E1.m1.17.17.17.17.17.17.cmml" xref="S3.E1.m1.17.17.17.17.17.17">ğ‘</ci></apply><apply id="S3.E1.m1.55.56.1.3.2.3.cmml" xref="S3.E1.m1.55.55"><csymbol cd="ambiguous" id="S3.E1.m1.55.56.1.3.2.3.1.cmml" xref="S3.E1.m1.55.55">subscript</csymbol><ci id="S3.E1.m1.19.19.19.19.19.19.cmml" xref="S3.E1.m1.19.19.19.19.19.19">â„’</ci><ci id="S3.E1.m1.20.20.20.20.20.20.1.cmml" xref="S3.E1.m1.20.20.20.20.20.20.1">ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ</ci></apply></apply><ci id="S3.E1.m1.22.22.22.22.22.22.cmml" xref="S3.E1.m1.22.22.22.22.22.22">ğ¶</ci></apply><apply id="S3.E1.m1.55.56.1.4.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.4.1.cmml" xref="S3.E1.m1.55.55"></times><apply id="S3.E1.m1.55.56.1.4.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.30.30.30.6.6.6.cmml" xref="S3.E1.m1.30.30.30.6.6.6"></times><apply id="S3.E1.m1.55.56.1.4.2.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.4.2.2.1.cmml" xref="S3.E1.m1.55.55"></times><ci id="S3.E1.m1.25.25.25.1.1.1.cmml" xref="S3.E1.m1.25.25.25.1.1.1">ğœ†</ci><ci id="S3.E1.m1.26.26.26.2.2.2.cmml" xref="S3.E1.m1.26.26.26.2.2.2">ğ‘›</ci><ci id="S3.E1.m1.27.27.27.3.3.3.cmml" xref="S3.E1.m1.27.27.27.3.3.3">ğ‘</ci><ci id="S3.E1.m1.28.28.28.4.4.4.cmml" xref="S3.E1.m1.28.28.28.4.4.4">ğ‘’</ci><ci id="S3.E1.m1.29.29.29.5.5.5.cmml" xref="S3.E1.m1.29.29.29.5.5.5">ğ‘¥</ci></apply><apply id="S3.E1.m1.55.56.1.4.2.3.cmml" xref="S3.E1.m1.55.55"><csymbol cd="ambiguous" id="S3.E1.m1.55.56.1.4.2.3.1.cmml" xref="S3.E1.m1.55.55">subscript</csymbol><ci id="S3.E1.m1.31.31.31.7.7.7.cmml" xref="S3.E1.m1.31.31.31.7.7.7">â„’</ci><ci id="S3.E1.m1.32.32.32.8.8.8.1.cmml" xref="S3.E1.m1.32.32.32.8.8.8.1">ğ‘ğ¶ğ¸ğ‘¥</ci></apply></apply><vector id="S3.E1.m1.55.56.1.4.3.cmml" xref="S3.E1.m1.55.55"><ci id="S3.E1.m1.34.34.34.10.10.10.cmml" xref="S3.E1.m1.34.34.34.10.10.10">ğº</ci><ci id="S3.E1.m1.36.36.36.12.12.12.cmml" xref="S3.E1.m1.36.36.36.12.12.12">ğ·</ci><ci id="S3.E1.m1.38.38.38.14.14.14.cmml" xref="S3.E1.m1.38.38.38.14.14.14">ğ‘‹</ci></vector></apply><apply id="S3.E1.m1.55.56.1.5.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.5.1.cmml" xref="S3.E1.m1.55.55"></times><apply id="S3.E1.m1.55.56.1.5.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.46.46.46.22.22.22.cmml" xref="S3.E1.m1.46.46.46.22.22.22"></times><apply id="S3.E1.m1.55.56.1.5.2.2.cmml" xref="S3.E1.m1.55.55"><times id="S3.E1.m1.55.56.1.5.2.2.1.cmml" xref="S3.E1.m1.55.55"></times><ci id="S3.E1.m1.41.41.41.17.17.17.cmml" xref="S3.E1.m1.41.41.41.17.17.17">ğœ†</ci><ci id="S3.E1.m1.42.42.42.18.18.18.cmml" xref="S3.E1.m1.42.42.42.18.18.18">ğ‘›</ci><ci id="S3.E1.m1.43.43.43.19.19.19.cmml" xref="S3.E1.m1.43.43.43.19.19.19">ğ‘</ci><ci id="S3.E1.m1.44.44.44.20.20.20.cmml" xref="S3.E1.m1.44.44.44.20.20.20">ğ‘’</ci><ci id="S3.E1.m1.45.45.45.21.21.21.cmml" xref="S3.E1.m1.45.45.45.21.21.21">ğ‘¦</ci></apply><apply id="S3.E1.m1.55.56.1.5.2.3.cmml" xref="S3.E1.m1.55.55"><csymbol cd="ambiguous" id="S3.E1.m1.55.56.1.5.2.3.1.cmml" xref="S3.E1.m1.55.55">subscript</csymbol><ci id="S3.E1.m1.47.47.47.23.23.23.cmml" xref="S3.E1.m1.47.47.47.23.23.23">â„’</ci><ci id="S3.E1.m1.48.48.48.24.24.24.1.cmml" xref="S3.E1.m1.48.48.48.24.24.24.1">ğ‘ğ¶ğ¸ğ‘¦</ci></apply></apply><vector id="S3.E1.m1.55.56.1.5.3.cmml" xref="S3.E1.m1.55.55"><ci id="S3.E1.m1.50.50.50.26.26.26.cmml" xref="S3.E1.m1.50.50.50.26.26.26">ğº</ci><ci id="S3.E1.m1.52.52.52.28.28.28.cmml" xref="S3.E1.m1.52.52.52.28.28.28">ğ·</ci><ci id="S3.E1.m1.54.54.54.30.30.30.cmml" xref="S3.E1.m1.54.54.54.30.30.30">ğ‘Œ</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.55c">\begin{split}\lambda{g}*\mathcal{L}_{\mathit{GAN}}(\mathit{G,D,X,Y})+\lambda{c}*\mathcal{L}_{\mathit{classifier}}(\mathit{C})+\\
\lambda{ncex}*\mathcal{L}_{\mathit{NCEx}}(\mathit{G,D,X})+\lambda{ncey}*\mathcal{L}_{\mathit{NCEy}}(\mathit{G,D,Y})\end{split}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.13" class="ltx_p">where <math id="S3.p6.1.m1.4" class="ltx_Math" alttext="\mathcal{L}_{\mathit{GAN}}(\mathit{G,D,X,Y})" display="inline"><semantics id="S3.p6.1.m1.4a"><mrow id="S3.p6.1.m1.4.5" xref="S3.p6.1.m1.4.5.cmml"><msub id="S3.p6.1.m1.4.5.2" xref="S3.p6.1.m1.4.5.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.1.m1.4.5.2.2" xref="S3.p6.1.m1.4.5.2.2.cmml">â„’</mi><mi id="S3.p6.1.m1.4.5.2.3" xref="S3.p6.1.m1.4.5.2.3.cmml">ğºğ´ğ‘</mi></msub><mo lspace="0em" rspace="0em" id="S3.p6.1.m1.4.5.1" xref="S3.p6.1.m1.4.5.1.cmml">â€‹</mo><mrow id="S3.p6.1.m1.4.5.3.2" xref="S3.p6.1.m1.4.5.3.1.cmml"><mo stretchy="false" id="S3.p6.1.m1.4.5.3.2.1" xref="S3.p6.1.m1.4.5.3.1.cmml">(</mo><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">G</mi><mo id="S3.p6.1.m1.4.5.3.2.2" xref="S3.p6.1.m1.4.5.3.1.cmml">,</mo><mi id="S3.p6.1.m1.2.2" xref="S3.p6.1.m1.2.2.cmml">D</mi><mo id="S3.p6.1.m1.4.5.3.2.3" xref="S3.p6.1.m1.4.5.3.1.cmml">,</mo><mi id="S3.p6.1.m1.3.3" xref="S3.p6.1.m1.3.3.cmml">X</mi><mo id="S3.p6.1.m1.4.5.3.2.4" xref="S3.p6.1.m1.4.5.3.1.cmml">,</mo><mi id="S3.p6.1.m1.4.4" xref="S3.p6.1.m1.4.4.cmml">Y</mi><mo stretchy="false" id="S3.p6.1.m1.4.5.3.2.5" xref="S3.p6.1.m1.4.5.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.4b"><apply id="S3.p6.1.m1.4.5.cmml" xref="S3.p6.1.m1.4.5"><times id="S3.p6.1.m1.4.5.1.cmml" xref="S3.p6.1.m1.4.5.1"></times><apply id="S3.p6.1.m1.4.5.2.cmml" xref="S3.p6.1.m1.4.5.2"><csymbol cd="ambiguous" id="S3.p6.1.m1.4.5.2.1.cmml" xref="S3.p6.1.m1.4.5.2">subscript</csymbol><ci id="S3.p6.1.m1.4.5.2.2.cmml" xref="S3.p6.1.m1.4.5.2.2">â„’</ci><ci id="S3.p6.1.m1.4.5.2.3.cmml" xref="S3.p6.1.m1.4.5.2.3">ğºğ´ğ‘</ci></apply><vector id="S3.p6.1.m1.4.5.3.1.cmml" xref="S3.p6.1.m1.4.5.3.2"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">ğº</ci><ci id="S3.p6.1.m1.2.2.cmml" xref="S3.p6.1.m1.2.2">ğ·</ci><ci id="S3.p6.1.m1.3.3.cmml" xref="S3.p6.1.m1.3.3">ğ‘‹</ci><ci id="S3.p6.1.m1.4.4.cmml" xref="S3.p6.1.m1.4.4">ğ‘Œ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.4c">\mathcal{L}_{\mathit{GAN}}(\mathit{G,D,X,Y})</annotation></semantics></math> is the generator loss, <math id="S3.p6.2.m2.1" class="ltx_Math" alttext="\mathcal{L}_{\mathit{classifier}}(\mathit{C})" display="inline"><semantics id="S3.p6.2.m2.1a"><mrow id="S3.p6.2.m2.1.2" xref="S3.p6.2.m2.1.2.cmml"><msub id="S3.p6.2.m2.1.2.2" xref="S3.p6.2.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.2.m2.1.2.2.2" xref="S3.p6.2.m2.1.2.2.2.cmml">â„’</mi><mi id="S3.p6.2.m2.1.2.2.3" xref="S3.p6.2.m2.1.2.2.3.cmml">ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ</mi></msub><mo lspace="0em" rspace="0em" id="S3.p6.2.m2.1.2.1" xref="S3.p6.2.m2.1.2.1.cmml">â€‹</mo><mrow id="S3.p6.2.m2.1.2.3.2" xref="S3.p6.2.m2.1.2.cmml"><mo stretchy="false" id="S3.p6.2.m2.1.2.3.2.1" xref="S3.p6.2.m2.1.2.cmml">(</mo><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">C</mi><mo stretchy="false" id="S3.p6.2.m2.1.2.3.2.2" xref="S3.p6.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><apply id="S3.p6.2.m2.1.2.cmml" xref="S3.p6.2.m2.1.2"><times id="S3.p6.2.m2.1.2.1.cmml" xref="S3.p6.2.m2.1.2.1"></times><apply id="S3.p6.2.m2.1.2.2.cmml" xref="S3.p6.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.p6.2.m2.1.2.2.1.cmml" xref="S3.p6.2.m2.1.2.2">subscript</csymbol><ci id="S3.p6.2.m2.1.2.2.2.cmml" xref="S3.p6.2.m2.1.2.2.2">â„’</ci><ci id="S3.p6.2.m2.1.2.2.3.cmml" xref="S3.p6.2.m2.1.2.2.3">ğ‘ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ</ci></apply><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">ğ¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">\mathcal{L}_{\mathit{classifier}}(\mathit{C})</annotation></semantics></math> is the cross-entropy loss on the classifier inferred from the images generated by the generator. <math id="S3.p6.3.m3.3" class="ltx_Math" alttext="\mathcal{L}_{\mathit{NCEx}}(\mathit{G,D,X})" display="inline"><semantics id="S3.p6.3.m3.3a"><mrow id="S3.p6.3.m3.3.4" xref="S3.p6.3.m3.3.4.cmml"><msub id="S3.p6.3.m3.3.4.2" xref="S3.p6.3.m3.3.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.3.m3.3.4.2.2" xref="S3.p6.3.m3.3.4.2.2.cmml">â„’</mi><mi id="S3.p6.3.m3.3.4.2.3" xref="S3.p6.3.m3.3.4.2.3.cmml">ğ‘ğ¶ğ¸ğ‘¥</mi></msub><mo lspace="0em" rspace="0em" id="S3.p6.3.m3.3.4.1" xref="S3.p6.3.m3.3.4.1.cmml">â€‹</mo><mrow id="S3.p6.3.m3.3.4.3.2" xref="S3.p6.3.m3.3.4.3.1.cmml"><mo stretchy="false" id="S3.p6.3.m3.3.4.3.2.1" xref="S3.p6.3.m3.3.4.3.1.cmml">(</mo><mi id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml">G</mi><mo id="S3.p6.3.m3.3.4.3.2.2" xref="S3.p6.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.p6.3.m3.2.2" xref="S3.p6.3.m3.2.2.cmml">D</mi><mo id="S3.p6.3.m3.3.4.3.2.3" xref="S3.p6.3.m3.3.4.3.1.cmml">,</mo><mi id="S3.p6.3.m3.3.3" xref="S3.p6.3.m3.3.3.cmml">X</mi><mo stretchy="false" id="S3.p6.3.m3.3.4.3.2.4" xref="S3.p6.3.m3.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.3b"><apply id="S3.p6.3.m3.3.4.cmml" xref="S3.p6.3.m3.3.4"><times id="S3.p6.3.m3.3.4.1.cmml" xref="S3.p6.3.m3.3.4.1"></times><apply id="S3.p6.3.m3.3.4.2.cmml" xref="S3.p6.3.m3.3.4.2"><csymbol cd="ambiguous" id="S3.p6.3.m3.3.4.2.1.cmml" xref="S3.p6.3.m3.3.4.2">subscript</csymbol><ci id="S3.p6.3.m3.3.4.2.2.cmml" xref="S3.p6.3.m3.3.4.2.2">â„’</ci><ci id="S3.p6.3.m3.3.4.2.3.cmml" xref="S3.p6.3.m3.3.4.2.3">ğ‘ğ¶ğ¸ğ‘¥</ci></apply><vector id="S3.p6.3.m3.3.4.3.1.cmml" xref="S3.p6.3.m3.3.4.3.2"><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">ğº</ci><ci id="S3.p6.3.m3.2.2.cmml" xref="S3.p6.3.m3.2.2">ğ·</ci><ci id="S3.p6.3.m3.3.3.cmml" xref="S3.p6.3.m3.3.3">ğ‘‹</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.3c">\mathcal{L}_{\mathit{NCEx}}(\mathit{G,D,X})</annotation></semantics></math> and <math id="S3.p6.4.m4.3" class="ltx_Math" alttext="\mathcal{L}_{\mathit{NCEy}}(\mathit{G,D,Y})" display="inline"><semantics id="S3.p6.4.m4.3a"><mrow id="S3.p6.4.m4.3.4" xref="S3.p6.4.m4.3.4.cmml"><msub id="S3.p6.4.m4.3.4.2" xref="S3.p6.4.m4.3.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p6.4.m4.3.4.2.2" xref="S3.p6.4.m4.3.4.2.2.cmml">â„’</mi><mi id="S3.p6.4.m4.3.4.2.3" xref="S3.p6.4.m4.3.4.2.3.cmml">ğ‘ğ¶ğ¸ğ‘¦</mi></msub><mo lspace="0em" rspace="0em" id="S3.p6.4.m4.3.4.1" xref="S3.p6.4.m4.3.4.1.cmml">â€‹</mo><mrow id="S3.p6.4.m4.3.4.3.2" xref="S3.p6.4.m4.3.4.3.1.cmml"><mo stretchy="false" id="S3.p6.4.m4.3.4.3.2.1" xref="S3.p6.4.m4.3.4.3.1.cmml">(</mo><mi id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml">G</mi><mo id="S3.p6.4.m4.3.4.3.2.2" xref="S3.p6.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.p6.4.m4.2.2" xref="S3.p6.4.m4.2.2.cmml">D</mi><mo id="S3.p6.4.m4.3.4.3.2.3" xref="S3.p6.4.m4.3.4.3.1.cmml">,</mo><mi id="S3.p6.4.m4.3.3" xref="S3.p6.4.m4.3.3.cmml">Y</mi><mo stretchy="false" id="S3.p6.4.m4.3.4.3.2.4" xref="S3.p6.4.m4.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.3b"><apply id="S3.p6.4.m4.3.4.cmml" xref="S3.p6.4.m4.3.4"><times id="S3.p6.4.m4.3.4.1.cmml" xref="S3.p6.4.m4.3.4.1"></times><apply id="S3.p6.4.m4.3.4.2.cmml" xref="S3.p6.4.m4.3.4.2"><csymbol cd="ambiguous" id="S3.p6.4.m4.3.4.2.1.cmml" xref="S3.p6.4.m4.3.4.2">subscript</csymbol><ci id="S3.p6.4.m4.3.4.2.2.cmml" xref="S3.p6.4.m4.3.4.2.2">â„’</ci><ci id="S3.p6.4.m4.3.4.2.3.cmml" xref="S3.p6.4.m4.3.4.2.3">ğ‘ğ¶ğ¸ğ‘¦</ci></apply><vector id="S3.p6.4.m4.3.4.3.1.cmml" xref="S3.p6.4.m4.3.4.3.2"><ci id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1">ğº</ci><ci id="S3.p6.4.m4.2.2.cmml" xref="S3.p6.4.m4.2.2">ğ·</ci><ci id="S3.p6.4.m4.3.3.cmml" xref="S3.p6.4.m4.3.3">ğ‘Œ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.3c">\mathcal{L}_{\mathit{NCEy}}(\mathit{G,D,Y})</annotation></semantics></math> are the contrastive losses that encourage spatial consistency for the synthetic and real images, respectively. <math id="S3.p6.5.m5.1" class="ltx_Math" alttext="\mathit{G}" display="inline"><semantics id="S3.p6.5.m5.1a"><mi id="S3.p6.5.m5.1.1" xref="S3.p6.5.m5.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.1b"><ci id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1">ğº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">\mathit{G}</annotation></semantics></math> is the generator model, <math id="S3.p6.6.m6.1" class="ltx_Math" alttext="\mathit{D}" display="inline"><semantics id="S3.p6.6.m6.1a"><mi id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><ci id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">\mathit{D}</annotation></semantics></math> the discriminator model, <math id="S3.p6.7.m7.1" class="ltx_Math" alttext="\mathit{X}" display="inline"><semantics id="S3.p6.7.m7.1a"><mi id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">ğ‘‹</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">\mathit{X}</annotation></semantics></math> the real image, <math id="S3.p6.8.m8.1" class="ltx_Math" alttext="\mathit{Y}" display="inline"><semantics id="S3.p6.8.m8.1a"><mi id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><ci id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1">ğ‘Œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">\mathit{Y}</annotation></semantics></math> the synthetic image, and <math id="S3.p6.9.m9.1" class="ltx_Math" alttext="\mathit{C}" display="inline"><semantics id="S3.p6.9.m9.1a"><mi id="S3.p6.9.m9.1.1" xref="S3.p6.9.m9.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.p6.9.m9.1b"><ci id="S3.p6.9.m9.1.1.cmml" xref="S3.p6.9.m9.1.1">ğ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m9.1c">\mathit{C}</annotation></semantics></math> the classification model. <math id="S3.p6.10.m10.1" class="ltx_Math" alttext="\lambda{g}" display="inline"><semantics id="S3.p6.10.m10.1a"><mrow id="S3.p6.10.m10.1.1" xref="S3.p6.10.m10.1.1.cmml"><mi id="S3.p6.10.m10.1.1.2" xref="S3.p6.10.m10.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.p6.10.m10.1.1.1" xref="S3.p6.10.m10.1.1.1.cmml">â€‹</mo><mi id="S3.p6.10.m10.1.1.3" xref="S3.p6.10.m10.1.1.3.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.10.m10.1b"><apply id="S3.p6.10.m10.1.1.cmml" xref="S3.p6.10.m10.1.1"><times id="S3.p6.10.m10.1.1.1.cmml" xref="S3.p6.10.m10.1.1.1"></times><ci id="S3.p6.10.m10.1.1.2.cmml" xref="S3.p6.10.m10.1.1.2">ğœ†</ci><ci id="S3.p6.10.m10.1.1.3.cmml" xref="S3.p6.10.m10.1.1.3">ğ‘”</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.10.m10.1c">\lambda{g}</annotation></semantics></math>, <math id="S3.p6.11.m11.1" class="ltx_Math" alttext="\lambda{c}" display="inline"><semantics id="S3.p6.11.m11.1a"><mrow id="S3.p6.11.m11.1.1" xref="S3.p6.11.m11.1.1.cmml"><mi id="S3.p6.11.m11.1.1.2" xref="S3.p6.11.m11.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.p6.11.m11.1.1.1" xref="S3.p6.11.m11.1.1.1.cmml">â€‹</mo><mi id="S3.p6.11.m11.1.1.3" xref="S3.p6.11.m11.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.11.m11.1b"><apply id="S3.p6.11.m11.1.1.cmml" xref="S3.p6.11.m11.1.1"><times id="S3.p6.11.m11.1.1.1.cmml" xref="S3.p6.11.m11.1.1.1"></times><ci id="S3.p6.11.m11.1.1.2.cmml" xref="S3.p6.11.m11.1.1.2">ğœ†</ci><ci id="S3.p6.11.m11.1.1.3.cmml" xref="S3.p6.11.m11.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.11.m11.1c">\lambda{c}</annotation></semantics></math>, <math id="S3.p6.12.m12.1" class="ltx_Math" alttext="\lambda{ncex}" display="inline"><semantics id="S3.p6.12.m12.1a"><mrow id="S3.p6.12.m12.1.1" xref="S3.p6.12.m12.1.1.cmml"><mi id="S3.p6.12.m12.1.1.2" xref="S3.p6.12.m12.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.p6.12.m12.1.1.1" xref="S3.p6.12.m12.1.1.1.cmml">â€‹</mo><mi id="S3.p6.12.m12.1.1.3" xref="S3.p6.12.m12.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p6.12.m12.1.1.1a" xref="S3.p6.12.m12.1.1.1.cmml">â€‹</mo><mi id="S3.p6.12.m12.1.1.4" xref="S3.p6.12.m12.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.p6.12.m12.1.1.1b" xref="S3.p6.12.m12.1.1.1.cmml">â€‹</mo><mi id="S3.p6.12.m12.1.1.5" xref="S3.p6.12.m12.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p6.12.m12.1.1.1c" xref="S3.p6.12.m12.1.1.1.cmml">â€‹</mo><mi id="S3.p6.12.m12.1.1.6" xref="S3.p6.12.m12.1.1.6.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.12.m12.1b"><apply id="S3.p6.12.m12.1.1.cmml" xref="S3.p6.12.m12.1.1"><times id="S3.p6.12.m12.1.1.1.cmml" xref="S3.p6.12.m12.1.1.1"></times><ci id="S3.p6.12.m12.1.1.2.cmml" xref="S3.p6.12.m12.1.1.2">ğœ†</ci><ci id="S3.p6.12.m12.1.1.3.cmml" xref="S3.p6.12.m12.1.1.3">ğ‘›</ci><ci id="S3.p6.12.m12.1.1.4.cmml" xref="S3.p6.12.m12.1.1.4">ğ‘</ci><ci id="S3.p6.12.m12.1.1.5.cmml" xref="S3.p6.12.m12.1.1.5">ğ‘’</ci><ci id="S3.p6.12.m12.1.1.6.cmml" xref="S3.p6.12.m12.1.1.6">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.12.m12.1c">\lambda{ncex}</annotation></semantics></math>, and <math id="S3.p6.13.m13.1" class="ltx_Math" alttext="\lambda{ncey}" display="inline"><semantics id="S3.p6.13.m13.1a"><mrow id="S3.p6.13.m13.1.1" xref="S3.p6.13.m13.1.1.cmml"><mi id="S3.p6.13.m13.1.1.2" xref="S3.p6.13.m13.1.1.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S3.p6.13.m13.1.1.1" xref="S3.p6.13.m13.1.1.1.cmml">â€‹</mo><mi id="S3.p6.13.m13.1.1.3" xref="S3.p6.13.m13.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.p6.13.m13.1.1.1a" xref="S3.p6.13.m13.1.1.1.cmml">â€‹</mo><mi id="S3.p6.13.m13.1.1.4" xref="S3.p6.13.m13.1.1.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S3.p6.13.m13.1.1.1b" xref="S3.p6.13.m13.1.1.1.cmml">â€‹</mo><mi id="S3.p6.13.m13.1.1.5" xref="S3.p6.13.m13.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.p6.13.m13.1.1.1c" xref="S3.p6.13.m13.1.1.1.cmml">â€‹</mo><mi id="S3.p6.13.m13.1.1.6" xref="S3.p6.13.m13.1.1.6.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.13.m13.1b"><apply id="S3.p6.13.m13.1.1.cmml" xref="S3.p6.13.m13.1.1"><times id="S3.p6.13.m13.1.1.1.cmml" xref="S3.p6.13.m13.1.1.1"></times><ci id="S3.p6.13.m13.1.1.2.cmml" xref="S3.p6.13.m13.1.1.2">ğœ†</ci><ci id="S3.p6.13.m13.1.1.3.cmml" xref="S3.p6.13.m13.1.1.3">ğ‘›</ci><ci id="S3.p6.13.m13.1.1.4.cmml" xref="S3.p6.13.m13.1.1.4">ğ‘</ci><ci id="S3.p6.13.m13.1.1.5.cmml" xref="S3.p6.13.m13.1.1.5">ğ‘’</ci><ci id="S3.p6.13.m13.1.1.6.cmml" xref="S3.p6.13.m13.1.1.6">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.13.m13.1c">\lambda{ncey}</annotation></semantics></math> are hyperparameters that control the weight of the generator, the classifier, and both contrastive losses, respectively.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><svg id="S3.F5.pic1" class="ltx_picture ltx_centering" height="456.79" overflow="visible" version="1.1" width="434.71"><g transform="translate(0,456.79) matrix(1 0 0 -1 0 0) translate(72.27,0) translate(0,362.44)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -69.5 -69.5)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_synth1.png" id="S3.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -69.5 -214.59)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_synth2.png" id="S3.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -69.5 -359.68)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_synth3.png" id="S3.F5.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 75.59 -69.5)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_cut1.png" id="S3.F5.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 75.59 -214.59)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_cut2.png" id="S3.F5.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 75.59 -359.68)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_cut3.png" id="S3.F5.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 220.68 -69.5)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_fcut1.png" id="S3.F5.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 220.68 -214.59)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_fcut2.png" id="S3.F5.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 220.68 -359.68)" fill="#000000" stroke="#000000"><foreignObject width="139" height="139" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2305.05580/assets/images/vs_fcut3.png" id="S3.F5.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="139" height="139" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -28.44 80.12)" fill="#000000" stroke="#000000"><foreignObject width="57.27" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F5.pic1.10.10.10.1.1" class="ltx_text ltx_font_bold">Synthetic</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 129.91 77.43)" fill="#000000" stroke="#000000"><foreignObject width="30.36" height="9.46" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F5.pic1.11.11.11.1.1" class="ltx_text ltx_font_bold">CUT</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 249.49 77.43)" fill="#000000" stroke="#000000"><foreignObject width="81.37" height="9.61" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S3.F5.pic1.12.12.12.1.1" class="ltx_text ltx_font_bold">Fashion CUT</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Comparison of CUT and Fashion CUT image translation. Note that the annotations (gradient, striped, dotted) are preserved when using Fashion CUT.</figcaption>
</figure>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">In our experiments we empirically choose to replace half of the synthetic mini-batch with images from the target domain. As real-world annotations are not available for generated images, we use pseudo-labels predicted by the classifier. The model suffers from the cold start problem when introducing pseudo-labels in the early epochs because the classifier struggles to converge. We found that the classifier requires at least 1 epoch of synthetic samples in order to generate reliable pseudo-labels for real-world images. We obtained the best results when enabling pseudo-labels at the end of epoch 2.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section describes the synthetic dataset we generated and the two experiment setups used to evaluate Fashion CUT.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Zalando SDG dataset</h3>

<figure id="S4.F6" class="ltx_figure"><img src="/html/2305.05580/assets/images/blender.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="114" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>For each render we start with a provided 3D object, add environment and spot lights, apply a procedural material and then randomize its properties (e.g. colors, scale).</figcaption>
</figure>
<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The Zalando SDG dataset is composed of 31,840 images of 7 classes: plain, floral, striped, dotted, camouflage, gradient, and herringbone. The dataset has been generated using Blender, an open-source 3D computer-graphic softwareÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. We relied on a basic set of professionally modeled 3D objects from CGTrader representing a variety of fashion silhouettes (e.g. shirt, dress, trousers) and implemented a procedural material for each of the 7 target classes. Each procedural material is implemented as a Blender shader node, where multiple properties can be exposed and controlled via Blender Python API. Examples of such properties include pattern scale, color or color pairing, orientation and image-texture. This setup allows an arbitrary amount of different images for each 3D object and class pair to be generated programmatically. We randomized background, lighting, and camera position, as seen in Figure <a href="#S4.F6" title="Figure 6 â€£ 4.1 Zalando SDG dataset â€£ 4 Experiments â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. We didnâ€™t use physically based renderers as those are more resource intensive, instead we traded off rendering accuracy for speed and adopted the real-time Blender Eevee render engineÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">The procedural materials can be applied to any new 3D objects. As such they provide a powerful generalized approach to data creation, and the generated images do not require any manual human validation as long as the procedural randomization guarantees that each possible output belongs to the expected target domain class.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation on Zalando SDG dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.4" class="ltx_p">In our experiments we train end-user pattern classification models using datasets from both
31,840 synthetic fashion imagery (the source domain, which includes groundtruth labels), and 334,165 real-world fashion imagery (the target domain, which has no groundtruth labels and is used solely to train our domain adaptation transformation).
We evaluate the performance of the algorithms using a validation set and a test set composed of 41,667 annotated real images each. The metric used is accuracy and all algorithms use a ResNet50Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> as the classifier. Fashion CUT is optimized using Adam with learning rate <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="10^{-5}" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><msup id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml"><mn id="S4.SS2.p1.1.m1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.2.cmml">10</mn><mrow id="S4.SS2.p1.1.m1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.3.cmml"><mo id="S4.SS2.p1.1.m1.1.1.3a" xref="S4.SS2.p1.1.m1.1.1.3.cmml">âˆ’</mo><mn id="S4.SS2.p1.1.m1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.3.2.cmml">5</mn></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><apply id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S4.SS2.p1.1.m1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.2">10</cn><apply id="S4.SS2.p1.1.m1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.3"><minus id="S4.SS2.p1.1.m1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.3"></minus><cn type="integer" id="S4.SS2.p1.1.m1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.3.2">5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">10^{-5}</annotation></semantics></math> and <math id="S4.SS2.p1.2.m2.1" class="ltx_Math" alttext="\lambda{g}=0.1" display="inline"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><mrow id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.2" xref="S4.SS2.p1.2.m2.1.1.2.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.2.m2.1.1.2.1" xref="S4.SS2.p1.2.m2.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3" xref="S4.SS2.p1.2.m2.1.1.2.3.cmml">g</mi></mrow><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><eq id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></eq><apply id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><times id="S4.SS2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.2">ğœ†</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3">ğ‘”</ci></apply><cn type="float" id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\lambda{g}=0.1</annotation></semantics></math>, and <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="\lambda{classifier}=0.1" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mrow id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml"><mrow id="S4.SS2.p1.3.m3.1.1.2" xref="S4.SS2.p1.3.m3.1.1.2.cmml"><mi id="S4.SS2.p1.3.m3.1.1.2.2" xref="S4.SS2.p1.3.m3.1.1.2.2.cmml">Î»</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.3" xref="S4.SS2.p1.3.m3.1.1.2.3.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1a" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.4" xref="S4.SS2.p1.3.m3.1.1.2.4.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1b" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.5" xref="S4.SS2.p1.3.m3.1.1.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1c" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.6" xref="S4.SS2.p1.3.m3.1.1.2.6.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1d" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.7" xref="S4.SS2.p1.3.m3.1.1.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1e" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.8" xref="S4.SS2.p1.3.m3.1.1.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1f" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.9" xref="S4.SS2.p1.3.m3.1.1.2.9.cmml">f</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1g" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.10" xref="S4.SS2.p1.3.m3.1.1.2.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1h" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.11" xref="S4.SS2.p1.3.m3.1.1.2.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p1.3.m3.1.1.2.1i" xref="S4.SS2.p1.3.m3.1.1.2.1.cmml">â€‹</mo><mi id="S4.SS2.p1.3.m3.1.1.2.12" xref="S4.SS2.p1.3.m3.1.1.2.12.cmml">r</mi></mrow><mo id="S4.SS2.p1.3.m3.1.1.1" xref="S4.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.3.m3.1.1.3" xref="S4.SS2.p1.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><apply id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1"><eq id="S4.SS2.p1.3.m3.1.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1.1"></eq><apply id="S4.SS2.p1.3.m3.1.1.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2"><times id="S4.SS2.p1.3.m3.1.1.2.1.cmml" xref="S4.SS2.p1.3.m3.1.1.2.1"></times><ci id="S4.SS2.p1.3.m3.1.1.2.2.cmml" xref="S4.SS2.p1.3.m3.1.1.2.2">ğœ†</ci><ci id="S4.SS2.p1.3.m3.1.1.2.3.cmml" xref="S4.SS2.p1.3.m3.1.1.2.3">ğ‘</ci><ci id="S4.SS2.p1.3.m3.1.1.2.4.cmml" xref="S4.SS2.p1.3.m3.1.1.2.4">ğ‘™</ci><ci id="S4.SS2.p1.3.m3.1.1.2.5.cmml" xref="S4.SS2.p1.3.m3.1.1.2.5">ğ‘</ci><ci id="S4.SS2.p1.3.m3.1.1.2.6.cmml" xref="S4.SS2.p1.3.m3.1.1.2.6">ğ‘ </ci><ci id="S4.SS2.p1.3.m3.1.1.2.7.cmml" xref="S4.SS2.p1.3.m3.1.1.2.7">ğ‘ </ci><ci id="S4.SS2.p1.3.m3.1.1.2.8.cmml" xref="S4.SS2.p1.3.m3.1.1.2.8">ğ‘–</ci><ci id="S4.SS2.p1.3.m3.1.1.2.9.cmml" xref="S4.SS2.p1.3.m3.1.1.2.9">ğ‘“</ci><ci id="S4.SS2.p1.3.m3.1.1.2.10.cmml" xref="S4.SS2.p1.3.m3.1.1.2.10">ğ‘–</ci><ci id="S4.SS2.p1.3.m3.1.1.2.11.cmml" xref="S4.SS2.p1.3.m3.1.1.2.11">ğ‘’</ci><ci id="S4.SS2.p1.3.m3.1.1.2.12.cmml" xref="S4.SS2.p1.3.m3.1.1.2.12">ğ‘Ÿ</ci></apply><cn type="float" id="S4.SS2.p1.3.m3.1.1.3.cmml" xref="S4.SS2.p1.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">\lambda{classifier}=0.1</annotation></semantics></math> for <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="\mathit{N}=5" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><mrow id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">N</mi><mo id="S4.SS2.p1.4.m4.1.1.1" xref="S4.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><eq id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1.1"></eq><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">ğ‘</ci><cn type="integer" id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">\mathit{N}=5</annotation></semantics></math> epochs.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">In Table <a href="#S4.T1" title="Table 1 â€£ 4.2 Evaluation on Zalando SDG dataset â€£ 4 Experiments â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we compare the performance of domain adaptation algorithms trained only on our 31,840 synthetically generated dataset and evaluated on the 41,667 real fashion images.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Accuracy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">No adaptation</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">0.441</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">BSPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>
</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center">0.499</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MDDÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>
</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center">0.540</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">AFNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center">0.578</td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Fashion CUT (ours)</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center">0.613</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Fashion CUT with pseudo-labels (ours)</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.7.6.2.1" class="ltx_text ltx_font_bold">0.628</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of unsupervised domain adaptation algorithms on our Zalando SDG dataset. The metric used is accuracy.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">First, we measured the performance of training without domain adaptation. In other words, the classifier was trained only on synthetic images. The performance was poor because the model didnâ€™t have information about real world images.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">Second, we evaluate Zalando SDG on other domain adaptation algorithms in the fashion domain. All experiments were performed in the environment provided by Jiang et al.Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. Our approach outperforms the other algorithms for the pattern classification task. Finally, we found that using pseudo-labels improves the results with minor changes in the training.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Synthetic dataset size</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We explore the required number of synthetic images to successfully train our unsupervised domain adaptation algorithm. For this experiment we train our model using 10,000 unlabeled real images and changing the number of synthetic images. Figure <a href="#S4.F7" title="Figure 7 â€£ 4.3 Synthetic dataset size â€£ 4 Experiments â€£ Fashion CUT: Unsupervised domain adaptation for visual pattern classification in clothes using synthetic data and pseudo-labels" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows that Fashion CUT performance benefits from large synthetic datasets. We found that at least 5,000 synthetic images are required to outperform other algorithms in visual pattern classification.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2305.05580/assets/images/lineplot.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="277" height="273" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Evaluation of Fashion Cut with varying amounts of the Zalando SDG dataset and 10.000 unlabeled real images. The performance is measured in accuracy.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Combining synthetic data generation with unsupervised domain adaptation can successfully classify patterns in clothes without real-world annotations. Furthermore, we found that attaching a classifier to an image translation model can enforce label stability, thus improving performance. Our experiments confirm that Fashion CUT outperforms other domain adaptation algorithms in the fashion domain. In addition, pseudo-labels proved to be beneficial for domain adaptation in the advanced stages of the training. As future work, we will explore the impact of fashion synthetic data in a semi-supervised setup. We hope this study will help enforce 3D rendering as a replacement for human annotations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Lone, S., Harboul, N., Weltevreden, J.: 2021 european e-commerce report

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Liang, X., Lin, L., Yang, W., Luo, P., Huang, J., Yan, S.: Clothes co-parsing
via joint image segmentation and labeling with application to clothing
retrieval. IEEE Transactions on Multimedia <span id="bib.bib2.1.1" class="ltx_text ltx_font_bold">18</span>(6), 1175â€“1186
(2016)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Liu, Z., Yan, S., Luo, P., Wang, X., Tang, X.: Fashion landmark detection in
the wild. In: European Conference on Computer Vision. pp. 229â€“245. Springer
(2016)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jagadeesh, V., Piramuthu, R., Bhardwaj, A., Di, W., Sundaresan, N.: Large scale
visual recommendations from street fashion images. In: Proceedings of the
20th ACM SIGKDD international conference on Knowledge discovery and data
mining. pp. 1925â€“1934 (2014)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Liu, Z., Luo, P., Qiu, S., Wang, X., Tang, X.: Deepfashion: Powering robust
clothes recognition and retrieval with rich annotations. In: Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2016)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Rostamzadeh, N., Hosseini, S., Boquet, T., Stokowiec, W., Zhang, Y., Jauvin,
C., Pal, C.: Fashion-gen: The generative fashion dataset and challenge. arXiv
preprint arXiv:1806.08317 (2018)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Wu, H., Gao, Y., Guo, X., Al-Halah, Z., Rennie, S., Grauman, K., Feris, R.: The
fashion iq dataset: Retrieving images by combining side information and
relative natural language feedback. CVPR (2021)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Sankaranarayanan, S., Balaji, Y., Jain, A., Lim, S.N., Chellappa, R.: Learning
from synthetic data: Addressing domain shift for semantic segmentation. In:
Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 3752â€“3761 (2018)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Moreu, E., Arazo, E., McGuinness, K., Oâ€™Connor, N.E.: Joint one-sided synthetic
unpaired image translation and segmentation for colorectal cancer prevention.
Expert Systems p. e13137 (2022)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Nam, H., Lee, H., Park, J., Yoon, W., Yoo, D.: Reducing domain gap by reducing
style bias. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. pp. 8690â€“8699 (2021)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Wang, M., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing
<span id="bib.bib11.1.1" class="ltx_text ltx_font_bold">312</span>, 135â€“153 (2018)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Rahnemoonfar, M., Sheppard, C.: Deep count: fruit counting based on deep
simulated learning. Sensors <span id="bib.bib12.1.1" class="ltx_text ltx_font_bold">17</span>(4), Â 905 (2017)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Wang, Q., Gao, J., Lin, W., Yuan, Y.: Learning from synthetic data for crowd
counting in the wild. In: Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). pp. 8198â€“8207 (2019)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia
dataset: A large collection of synthetic images for semantic segmentation of
urban scenes. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 3234â€“3243 (2016)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Moreu, E., McGuinness, K., Ortego, D., Oâ€™Connor, N.E.: Domain randomization for
object counting. arXiv preprint arXiv:2202.08670 (2022)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Xu, R., Li, G., Yang, J., Lin, L.: Larger norm more transferable: An adaptive
feature norm approach for unsupervised domain adaptation. In: The IEEE
International Conference on Computer Vision (ICCV) (October 2019)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Gong, B., Shi, Y., Sha, F., Grauman, K.: Geodesic flow kernel for unsupervised
domain adaptation. In: 2012 IEEE conference on computer vision and pattern
recognition. pp. 2066â€“2073. IEEE (2012)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., Bengio, Y.: Generative adversarial networks.
Communications of the ACM <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">63</span>(11), 139â€“144 (2020)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette,
F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural
networks. The journal of machine learning research <span id="bib.bib19.1.1" class="ltx_text ltx_font_bold">17</span>(1),
2096â€“2030 (2016)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Park, T., Efros, A.A., Zhang, R., Zhu, J.Y.: Contrastive learning for unpaired
image-to-image translation. In: European Conference on Computer Vision (2020)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Gutmann, M., HyvÃ¤rinen, A.: Noise-contrastive estimation: A new estimation
principle for unnormalized statistical models. In: Proceedings of the
thirteenth international conference on artificial intelligence and
statistics. pp. 297â€“304. JMLR Workshop and Conference Proceedings (2010)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Community, B.O.: Blender - a 3D modelling and rendering package. Blender
Foundation, Stichting Blender Foundation, Amsterdam (2018),
<a target="_blank" href="http://www.blender.org" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://www.blender.org</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Guevarra, E.T.M.: Modeling and Animation Using Blender: Blender 2.80: The Rise
of Eevee. Apress (2019)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Proceedings of the IEEE conference on computer vision and
pattern recognition. pp. 770â€“778 (2016)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Chen, X., Wang, S., Long, M., Wang, J.: Transferability vs. discriminability:
Batch spectral penalization for adversarial domain adaptation. In:
International conference on machine learning. pp. 1081â€“1090. PMLR (2019)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Zhang, Y., Liu, T., Long, M., Jordan, M.: Bridging theory and algorithm for
domain adaptation. In: International Conference on Machine Learning. pp.
7404â€“7413. PMLR (2019)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Junguang, J., Baixu, C., Bo, F., Mingsheng, L.: Transfer-learning-library.
<a target="_blank" href="https://github.com/thuml/Transfer-Learning-Library" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/thuml/Transfer-Learning-Library</a> (2020)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.05579" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.05580" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.05580">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.05580" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.05581" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 09:04:10 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
