<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2207.08187] Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR</title><meta property="og:description" content="Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smar…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2207.08187">

<!--Generated on Wed Mar 13 13:20:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_text ltx_font_bold">Sannara Ek</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France   
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id2.1.id1" class="ltx_text ltx_font_bold">Romain Rombourg</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France   
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_text ltx_font_bold">François Portet</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France   
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id4.1.id1" class="ltx_text ltx_font_bold">Philippe Lalanda</span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, 38000 Grenoble, France   
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Federated Learning is a new machine learning paradigm dealing with distributed model learning on independent devices. One of the many advantages of federated learning is that training data stay on devices (such as smartphones), and only learned models are shared with a centralized server. In the case of supervised learning, labeling is entrusted to the clients. However, acquiring such labels can be prohibitively expensive and error-prone for many tasks, such as human activity recognition. Hence, a wealth of data remains unlabelled and unexploited. Most existing federated learning approaches that focus mainly on supervised learning have mostly ignored this mass of unlabelled data.
Furthermore, it is unclear whether standard federated Learning approaches are suited to self-supervised learning. The few studies that have dealt with the problem have limited themselves to the favorable situation of homogeneous datasets. This work lays the groundwork for a reference evaluation of federated Learning with Semi-Supervised Learning in a realistic setting. We show that standard lightweight autoencoder and standard Federated Averaging fail to learn a robust representation for Human Activity Recognition with several realistic heterogeneous datasets. These findings advocate for a more intensive research effort in Federated Self Supervised Learning to exploit the mass of heterogeneous unlabelled data present on mobile devices.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) was proposed by Google back in 2016 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> as a novel form of distributed meta-learning that allows specialized machine learning models to be trained directly on user devices without ever communicating user data. The baseline FL works as so: the server sends a model to multiple clients to individually train on their local data, then all trained personalized models are sent back to the server and aggregated to create a single generalized model that summarises the individual models acquired by every client to conclude a communication round. The process is then repeated until convergence. The method offers an efficient and flexible distributed learning machine learning paradigm that preserves user data privacy and seems well suited to pervasive computing applications.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2207.08187/assets/FedAE2.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="248" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of the federated learning process for the autoencoder. </figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This new paradigm has been adopted into some real-world applications, as exemplified by Google keyboard <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, the applicability of FL with uncontrolled environments remains an open challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Adopting FL into a pervasive computing domain, where connected smart devices are embedded into our environment, raises multiple challenges <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. The most complex challenges are the scarcity of labeled data, the heterogeneity of users’ data on mobile devices, and the low computation/memory constraints.
Most FL approaches have been studied with a supervised learning target. However, supervised learning relies on a large amount of accurately labeled data. Such labeling is a tedious process that is highly time-consuming for human experts. Furthermore, since data stays on users’ devices in an FL setting, this would require the users to label their own data, which is not feasible most of the time. Hence, a large amount of data stays unlabelled on the users’ devices. One way to leverage this wealth of unlabelled client data is to develop unsupervised learning methods that only require unlabelled raw data to learn useful representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Another constraint when dealing with learning on devices is the often low computational resources available at the client level. This constraint implies that certain machine learning techniques or models that are too complex cannot be used in such FL scenarios, and lightweight learning techniques must be specifically designed. Finally, a major challenge throughout most environments of FL is data heterogeneity
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
These constraints have been under-studied in the FL domain.
Most recently proposed methods have either used a high resource-consuming architecture/schema, assumed the presence of large quantities of labeled data, or did not consider heterogeneity in their studies.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we present the results of a first attempt to leverage the high amount of unlabelled user data for FL in a realistic heterogeneous setting. We use Human Activity Recognition on mobile devices as a representative pervasive computing application (low resource devices, heterogeneity, users daily self-labeling highly challenging). We implemented self-supervised feature learning using an AutoEncoder (AE) architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> for representation learning due to its low complexity. Such feature learning was performed through FL using the original FedAvg algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> which is also of low complexity. For the human activity classification task,
we defined a realistic learning environment where clients have only unlabelled data while there is a small amount of non-private labeled data at the server level.
The clients are created using multiple publicly available datasets that were collected from a range of different devices and distinct participants. Our study tries to answer the following research question: “Can Autoencoders be used in a federated learning setting to learn efficient representations for human activity recognition from a heterogeneous set of unlabelled data?”</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">There have been only a few studies on the notion of Self-supervised learning and federated learning. For instance, the study by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> has shown that a simple AE in federated learning can indeed provide performance improvements compared to a conventional learning approach. Additionally, the study has also shown that using more advanced feature-learning architecture allows even more profit in performance gains. While the study done in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> that has as well investigated AEs applications onto a federated learning context shows that the AE does not supersede supervised learning.
Another study by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, by designing a novel self-supervised contrasting learning network, has raised that self-supervised learning on a large scale can be well used for sensor modeling tasks. However, the method is based on a modified SimCLR for federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, which might be too complex for real mobile devices.
Indeed, these study does not take into account the computing capabilities of the devices <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> since they propose big model architectures and complex configurations, e.g., dual-stream architecture, reliance on public datasets, and using large training batch-sizes. Furthermore, the evaluations were performed on isolated datasets that do not fully represent the challenges imposed by heterogeneity in the wild. To the best of our knowledge, learning with different datasets in FL has only been achieved by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. However, this was solely for supervised learning. In our paper, we want to evaluate the capability of a lightweight auto-supervised model, namely Autoencoder, to be trained via federated learning on a large heterogeneous set of data. We could not find any work in the literature that has previously achieved this objective.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In order to best replicate a realistic non-iid environment, where system and statistical heterogeneity along with class/data imbalance plays a large role between clients, we combined four different publicly available datasets to have 80 diverse clients. We selected the following datasets: the <span id="S3.p1.1.1" class="ltx_text ltx_font_bold">UCI dataset</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> – the standard benchmark dataset in the HAR community – which, in our experiments, represents five homogeneous clients, the <span id="S3.p1.1.2" class="ltx_text ltx_font_bold">Heterogeneity Human Activity Recognition (HHAR)</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> which comprises 51 heterogeneous clients, the <span id="S3.p1.1.3" class="ltx_text ltx_font_bold">REALWORLD</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> – a large dataset with a high diversity of device positioning – which adds 15 clients and finally the very large <span id="S3.p1.1.4" class="ltx_text ltx_font_bold">Sussex-Huawei Locomotion (SHL)</span> dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> that adds further nine clients. Each dataset brings sets of clients with unique properties to the combined dataset. More so, each of the datasets has its own set of activities, as shown in table <a href="#S3.T1" title="Table 1 ‣ 3 Method ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> with only some overlapping. The combined dataset deals with a total of 13 unique activities: Walk (W), Upstairs (U), Downstairs (D), Sit (ST), Stand (SD), Lay (L), Jump (J), Run (R), Bike (BK), Car (C), Bus (BS), Train (T), Subway (SW). In terms of class distribution, shown in figure <a href="#S3.F2" title="Figure 2 ‣ 3 Method ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, “Jump” is the minority class while there is a large number of samples for the “Stand” and “Walk” classes.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">From all the datasets, we used 3-axis accelerometer data with the 3-axis gyroscope data resampled at 50hz when necessary. We used a window-frame size of 128 (2.56s) with an overlap of 50% (1.28s) for the 6 channels of each axis. No feature extraction was performed, and the data was preprocessed using channel-wise z-normalization.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2207.08187/assets/distri5.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="234" height="79" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Class distribution for the four datasets</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Datasets properties</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Dataset</th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Clients</th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Data Samples</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Activities</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<th id="S3.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">UCI</th>
<th id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">5</th>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10,299</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6 (ST,SD,W,U,D,L)</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<th id="S3.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HHAR</th>
<th id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">51</th>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">85,567</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">6 (ST,SD,W,U,D,BK)</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<th id="S3.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">REALWORLD</th>
<th id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">15</th>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">356,427</td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">8 (ST,SD,W,U,D,J,L,R)</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<th id="S3.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">SHL</th>
<th id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">9</th>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">640,144</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">8 (ST,W,R,BK,C,BS,T,SW)</td>
</tr>
</tbody>
</table>
</figure>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">The data partitioning was done as illustrated by Figure <a href="#S3.F3" title="Figure 3 ‣ 3 Method ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For each dataset, 20% is left for testing the classifier at the server (Test-set), while the remaining 80% is used for training (Train-set). To simulate an in-the-wild training environment, where clients’ data are commonly unlabeled, we disregard 80% of the labels of the train-set for unsupervised feature learning on client devices. On the other hand, the remaining 20% train-set with labels are pooled together for supervised fine-tuning at the server. The final learning environment is then where we have 80 clients performing unsupervised feature learning in a federated manner and the server conducting supervised fine-tuning and testing.
We adapted the AE model architecture from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. The chosen model comprises four 1d convolution layers for the encoder, each consisting of 32 filters with a kernel size of 5, leading to the latent space. The decoder uses the same settings but employs transposed convolution layer instead. Finally, 128 neurons were set as the size of the latent space. The AE model is trained in a federated learning way for 200 communication rounds with 5 local epoch for each client on a learning rate of 0.01 with a Stochastic Gradient Descent (SGD) optimizer. At the end of every local training, the clients evaluate the AE loss on their respective test-set. Additionally, after every server model aggregation, we evaluate the server model on the combined client test-set.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">After the 200 communication rounds of training in a federated method, we take only the encoder of the AE and attach a dense layer of shape 32 that leads to the soft-max layer of shape 13 for classification. We used a learning rate of 0.00005 with an ADAM optimizer to train the model. The classifier is fine-tuned on the labeled data train-set for 200 epochs with class-weighted learning as a measure against the class imbalance. The classifier model evaluates on the combined test-set of all clients, where we then have the model evaluate separately on each set of the dataset’s client and the combined test-set of all clients. The entire learning process is summarized in figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">For comparative reasons in our study, we trained the model with two different settings using the same architecture and shape. First, in order to evaluate the benefits of the feature pre-training process of the AE, we train the AE entirely at the server level on an aggregated clients dataset (The combination of all clients unlabelled data). Secondly, intending to obtain quantitative analysis on the performance change between the FL way and the conventional way, we train a structurally identical model (four convolutional layers plus the dense and softmax layers) solely on the server labeled data with class-weighted learning.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p">The experiments were run on a Debian 4.19.132-1 version 10 having 256GB of RAM, 4x NVIDIA Quadro RTX 8000 48 GB GPU, and an Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz for the CPU. For the federated learning experiment, 4 GPUs were used in parallel, while only 1 of the 4 was used for all the other cases.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2207.08187/assets/dataPartition.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="230" height="107" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Data Partitioning Methodology</figcaption>
</figure>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">The overall results of the study are reported in Table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
The AE learned using the FL method (<span id="S4.p1.1.1" class="ltx_text ltx_font_bold">FL+AE</span>) exhibits an F-score of 71.29% on the combined test sets. On the classifier using <span id="S4.p1.1.2" class="ltx_text ltx_font_bold">conventional</span> learning – which consisted in classical centralized supervised learning on the labeled data only. The conventional approach shows an F-score of 71.96% on the combined test sets. It seems thus that either the FL learning or the AE are not able to benefit from the large set of unlabeled data. When the AE is trained in a centralized way (<span id="S4.p1.1.3" class="ltx_text ltx_font_bold">Conventional + AutoEncoder</span>), no improvement on the combined dataset is observed (69.04%) while there is instead a slight decrease. It seems thus that the AE does not learn a representation helpful for the classification task.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The performance of experiments on the combined datasets</figcaption>
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:99.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(21.0pt,-4.8pt) scale(1.10727007622814,1.10727007622814) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" rowspan="2"><span id="S4.T2.1.1.1.1.1.1" class="ltx_text">Learning Method</span></th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" colspan="5">Datasets (Macro F-score)</th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t">Combined</th>
<td id="S4.T2.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UCI</td>
<td id="S4.T2.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">HHAR</td>
<td id="S4.T2.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">REALWORLD</td>
<td id="S4.T2.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">SHL</td>
</tr>
<tr id="S4.T2.1.1.3.3" class="ltx_tr">
<th id="S4.T2.1.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">FL + Autoencoder</th>
<th id="S4.T2.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t">71.29</th>
<td id="S4.T2.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">73.51</td>
<td id="S4.T2.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.84</td>
<td id="S4.T2.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.3.3.5.1" class="ltx_text ltx_font_bold">77.98</span></td>
<td id="S4.T2.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">68.81</td>
</tr>
<tr id="S4.T2.1.1.4.4" class="ltx_tr">
<th id="S4.T2.1.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t">Conventional</th>
<th id="S4.T2.1.1.4.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_rr ltx_border_t"><span id="S4.T2.1.1.4.4.2.1" class="ltx_text ltx_font_bold">71.96</span></th>
<td id="S4.T2.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.4.3.1" class="ltx_text ltx_font_bold">78.92</span></td>
<td id="S4.T2.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.4.4.1" class="ltx_text ltx_font_bold">82.92</span></td>
<td id="S4.T2.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">76.84</td>
<td id="S4.T2.1.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.1.4.4.6.1" class="ltx_text ltx_font_bold">70.44</span></td>
</tr>
<tr id="S4.T2.1.1.5.5" class="ltx_tr">
<th id="S4.T2.1.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr">Conventional + Autoencoder</th>
<th id="S4.T2.1.1.5.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_rr">69.04</th>
<td id="S4.T2.1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">73.06</td>
<td id="S4.T2.1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">78.58</td>
<td id="S4.T2.1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">75.48</td>
<td id="S4.T2.1.1.5.5.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">66.52</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">On the individual dataset, it is further apparent that <span id="S4.p2.1.1" class="ltx_text ltx_font_bold">conventional</span> approach overall (with the exception of the REALWORLD dataset) has a slight performance edge overall, 73.51%, 82.84%, 77.98% and 68.81% respectively on the UCI, HHAR, REALWORLD, and SHL dataset. Herewith the <span id="S4.p2.1.2" class="ltx_text ltx_font_bold"> FL + Autoencoder</span> approach, the scores are 73.51% on UCI, 82.84% on HHAR, 77.98% on REALWORLD (The best amongst the three emulations), and 68.81% on SHL dataset. Finally, on the <span id="S4.p2.1.3" class="ltx_text ltx_font_bold">Conventional + Autoencoder</span> approach, we relatively see the lowest performance across all the datasets where we obtained 73.06%, 78.58%, 75.48%, and 66.52% respectively on the UCI, HHAR, REALWORLD, and SHL datasets. Overall, with marginal differences, no method seems superior to others.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2207.08187/assets/HeatMapCorrect.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="560" height="433" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Confusion Matrix of Classifier With Federated Learning Feature Learning</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Average storage footprint of local data and model</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Dataset/Model</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Size (MBs)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">UCI</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"> 3.87</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<th id="S4.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">HHAR</th>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r"> 3.15</td>
</tr>
<tr id="S4.T3.1.4.3" class="ltx_tr">
<th id="S4.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">REALWORLD</th>
<td id="S4.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r"> 89.11</td>
</tr>
<tr id="S4.T3.1.5.4" class="ltx_tr">
<th id="S4.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">SHL</th>
<td id="S4.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r"> 266.73</td>
</tr>
<tr id="S4.T3.1.6.5" class="ltx_tr">
<th id="S4.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Autoencoder</th>
<td id="S4.T3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"> 0.38</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Regarding the FL learning, it can be seen from the learning curve presented by Figure <a href="#S4.F5" title="Figure 5 ‣ 4 Results ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> that the training was regular. The averaged loss of the clients learning over 200 communication rounds shows that the learning is indeed progressing and able to converge. More so, a significant standard deviation can be observed on the client-side. This trait is likely due to the large number of dissimilar clients and the heterogeneity where the results show that such training with heterogeneous data is indeed challenging.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Details relating to memory footprint, where the data are stored in an HDF5 format<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, is shown in table <a href="#S4.T3" title="Table 3 ‣ 4 Results ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> where the average local data size of clients from the four different datasets vary significantly. Specifically, clients of UCI and HHAR contains around 3MB of data on average, while clients of REALWORLD have an average of 89.11 MBs, and SHL clients have an average size of 266.73 MBs. On the other hand, the communicated AE has a size of 0.38 MBs. Thus, we can establish that the communication cost for each client after 200 rounds of training in FL would be 76 MBs.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2207.08187/assets/Loss.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="213" height="153" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Autoencoder Loss with FedAvg</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2207.08187/assets/learningAcc3.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="302" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Classifier Fine-Tuning on the Server</figcaption>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">The presented combination of several distinctly dissimilar datasets has indeed presented a challenging learning problem. The results presented above have shown that even the model trained by the conventional/centralised learning method is unable to obtain good results compared to the studies that had trained individually and separately trained on only one dataset at a time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">We attribute the mitigated performance with the federated learning approach to several reasons. First, the nature of FedAvg’s weighted averaging does not favor clients with little data. Clients with little data (from UCI and HHAR datasets) have a lower weight in the federated learning than clients with large data (REALWORLD and SHL datasets). This property explains why the performance of the UCI and HHAR datasets is much lower than the conventional approach. Hence, the global/server model is heavily driven mainly by the clients of the task-challenging SHL dataset (classification of transport mode) and partially by the heterogeneous REALWORLD dataset. To support this interpretation, a confusion matrix presented by Figure <a href="#S4.F4" title="Figure 4 ‣ 4 Results ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reveals that the classifier is able to correctly predict non-locomotion related activities e.g. “Upstair”, “Downstair”, “Run”, “Jump”. The model, however, was often confused in classifying between the different locomotion classes, namely, “Bus”, “Train” and “Subway”. This can be due to the signal z-normalization and the lack of context in the model representation.
Additionally, the mentioned locomotion classes are often confused with the “Stand” activity. This behavior of the model is quite understandable since, in public transport, a person is often standing while waiting at a stop. The classification task thus becomes reasonably tricky as the model must uniquely learn the subtle motion changes applied by the transportation vehicle to the “stand” activity.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Second, collaborative feature learning, as we have done by training the AE using FedAvg, presents an orientation and scaling issue when the server model is aggregated. Given the same data samples to multiple client models that have just finished local training to embed/project the data into their respective latent space, we would see that the projections across all the clients would be different. The problem has been well raised in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and the study shows that a global set of rules of directions are needed in order to aggregate a suitable feature extractor that represents all client’s data. The aggregation by FedAvg as done in our experiments, however, is a naive approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and does not handle the mentioned problems. As shown in Figure <a href="#S5.F7" title="Figure 7 ‣ 5 Discussion ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> where the T-SNE dimensions reduced embeddings of the different classes are mostly intertwined with each other without distinct separations. The aggregation method would thus require more complex policies to properly merge the learned feature extractors of all the 80 clients in this study, favorably an aggregator that would preserve specialization and increase generalization.
On the other hand, the AE architecture is lightweight but very much decrepit. This claim can be further supported by the results seen in table <a href="#S4.T2" title="Table 2 ‣ 4 Results ‣ Federated Self-Supervised Learning in Heterogeneous Settings: Limits of a Baseline Approach on HAR" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, where the conventional approach was able to outperform the conventional with AE approach. The feature-learning done with the AE is an expensive training process, in both the federated and conventional means, yet did not bring improvements. This lacking thus calls for newer feature learning architectures such as CPC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, SimCLR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, and Moco <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> which has exhibited improved feature extraction and may be the route for improved performance if hardware constraints are properly taken into account.</p>
</div>
<figure id="S5.F7" class="ltx_figure"><img src="/html/2207.08187/assets/AEServerEmebd2.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="306" height="314" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Aggregated Autoencoder Embeddings on a REALWORLD client</figcaption>
</figure>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this study, we leveraged the high amount of unlabeled data in mobile devices by studying how federated learning and lightweight self-supervised learning can be combined. We evaluated the approach on the Human Activity Recognition task using a realistic learning environment through the combination of several public datasets. For the models, we used an AE due to its low complexity and used FedAvg as the baseline server model aggregation technique. We showed that a simple auto-encoder is not an effective way to represent the input in neither a centralized nor a federated setting. However, federated learning can better compensate for this ineffective representation than centralized learning. This behavior shows that studying coupling FL and more advanced self-supervised learning method exhibits the potential to be very promising to handle unlabelled data, which ultimately allows more practical FL assimilation into pervasive applications.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Acknowledgements</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work has been partially funded by Naval Group, France and by MIAI@Grenoble Alpes (ANR-19-P3IA-0003) funded by the French program Investissement d’avenir.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-Efficient Learning of Deep Networks from Decentralized
Data,” in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 20th International Conference on
Artificial Intelligence and Statistics</span>, vol. 54, (Fort Lauderdale, USA),
pp. 1273–1282, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Hard, C. M. Kiddon, D. Ramage, F. Beaufays, H. Eichner, K. Rao, R. Mathews,
and S. Augenstein, “Federated learning for mobile keyboard prediction,”
2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, and et al., “Advances and open problems in
federated learning,” <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine Learning</span>,
vol. 14, no. 1–2, pp. 1–210, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
C. Becker, C. Julien, P. Lalanda, and F. Zambonelli, “Pervasive computing
middleware: current trends and emerging challenges,” <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">CCF Transactions
on Pervasive Computing and Interaction</span>, vol. 1, 02 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
F. Zhang, K. Kuang, Z. You, T. Shen, J. Xiao, Y. Zhang, C. Wu, Y. Zhuang, and
X. Li, “Federated unsupervised representation learning,” <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CoRR</span>,
vol. abs/2010.08982, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Saeed, F. D. Salim, T. Ozcelebi, and J. Lukkien, “Federated self-supervised
learning of multisensor representations for embedded intelligence,” <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, vol. 8, p. 1030–1040, Jan 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith,
“Federated optimization in heterogeneous networks,” 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Wang, M. Yurochkin, Y. Sun, D. Papailiopoulos, and Y. Khazaeni, “Federated
learning with matched averaging,” in <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">International Conference on
Learning Representations</span>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
P. Baldi, “Autoencoders, unsupervised learning, and deep architectures,” in
<span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</span>
(I. Guyon, G. Dror, V. Lemaire, G. Taylor, and D. Silver, eds.), vol. 27 of
<span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, (Bellevue, Washington, USA),
pp. 37–49, PMLR, 02 Jul 2012.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. van Berlo, A. Saeed, and T. Ozcelebi, “Towards federated unsupervised
representation learning,” in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of the Third ACM International
Workshop on Edge Systems, Analytics and Networking</span>, EdgeSys ’20, (New York,
NY, USA), p. 31–36, Association for Computing Machinery, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q. Li, Z. Wen, Z. Wu, S. Hu, N. Wang, Y. Li, X. Liu, and B. He, “A survey on
federated learning systems: Vision, hype and reality for data privacy and
protection,” <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge &amp; Data Engineering</span>,
pp. 1–1, nov 5555.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Anguita, A. Ghio, L. Oneto, X. Parra, and J. L. Reyes-Ortiz, “A public
domain dataset for human activity recognition using smartphones,” in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">21st European Symposium on Artificial Neural Networks, ESANN 2013, Bruges,
Belgium, April 24-26, 2013</span>, 2013.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Stisen, H. Blunck, S. Bhattacharya, T. S. Prentow, M. B. Kjærgaard,
A. Dey, T. Sonne, and M. M. Jensen, “Smart devices are different: Assessing
and mitigating mobile sensing heterogeneities for activity recognition,” in
<span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of the 13th ACM Conference on Embedded Networked Sensor
Systems</span>, (New York, NY, USA), p. 127–140, 2015.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. Sztyler and H. Stuckenschmidt, “On-body localization of wearable
devices: An investigation of position-aware activity recognition,” in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">2016 IEEE International Conference on Pervasive Computing and Communications
(PerCom)</span>, pp. 1–9, 2016.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
H. Gjoreski, M. Ciliberto, L. Wang, F. J. Ordonez Morales, S. Mekki,
S. Valentin, and D. Roggen, “The university of sussex-huawei locomotion and
transportation dataset for multimodal analytics with mobile devices,” <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 6, pp. 42592–42604, 2018.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
A. Varamin, E. Abbasnejad, Q. Shi, D. Ranasinghe, and H. Rezatofighi, “Deep
auto-set: a deep auto-encoder-set network for activity recognition using
wearables,” in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of the 15th EAI International Conference on
Mobile and Ubiquitous Systems: Computing, Networking and Services</span>
(C. Borcea, S. Mao, and J. Tang, eds.), (United States of America),
pp. 246–253, Association for Computing Machinery (ACM), Nov. 2018.

</span>
<span class="ltx_bibblock">International Conference on Mobile and Ubiquitous Systems: Networks
and Services 2018, MobiQuitous 2018 ; Conference date: 05-11-2018 Through
07-11-2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
D. Price, S. Celles, P. Eendebak, M. McKerns, E. Olson, C. Raffel, and B. Yi,
“Hickle: A hdf5-based python pickle replacement,” <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Journal of Open
Source Software</span>, vol. 3, p. 1115, 12 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A. Ignatov, “Real-time human activity recognition from accelerometer data
using convolutional neural networks,” <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Applied Soft Computing</span>, vol. 62,
pp. 915 – 922, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
S. Ek, F. Portet, P. Lalanda, and G. Vega, “A Federated Learning Aggregation
Algorithm for Pervasive Computing: Evaluation and Comparison,” in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">19th IEEE International Conference on Pervasive Computing and Communications
PerCom 2021</span>, (Kassel, Germany), 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Yao, S. Hu, Y. Zhao, A. Zhang, and T. Abdelzaher, “Deepsense: A unified
deep learning framework for time-series mobile sensing data processing,” in
<span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th International Conference on World Wide Web</span>,
pp. 351–360, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
L. Wang, H. Gjoreski, M. Ciliberto, P. Lago, K. Murao, T. Okita, and D. Roggen,
“Summary of the sussex-huawei locomotion-transportation recognition
challenge 2019,” in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Adjunct Proceedings of the 2019 ACM International
Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the
2019 ACM International Symposium on Wearable Computers</span>, UbiComp/ISWC ’19
Adjunct, (New York, NY, USA), p. 849–856, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. van den Oord, Y. Li, and O. Vinyals, “Representation learning with
contrastive predictive coding,” <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">CoRR</span>, vol. abs/1807.03748, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for
contrastive learning of visual representations,” in <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the
37th International Conference on Machine Learning</span> (H. D. III and A. Singh,
eds.), vol. 119 of <span id="bib.bib23.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>,
pp. 1597–1607, PMLR, 13–18 Jul 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick, “Momentum contrast for
unsupervised visual representation learning,” in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</span>,
pp. 9726–9735, 2020.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2207.08185" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2207.08187" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2207.08187">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2207.08187" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2207.08188" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar 13 13:20:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
