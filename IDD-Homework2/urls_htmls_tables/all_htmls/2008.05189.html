<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2008.05189] Dispersed Federated Learning: Vision, Taxonomy, and Future Directions</title><meta property="og:description" content="The ongoing deployments of the Internet of Things (IoT)-based smart applications are spurring the adoption of machine learning as a key technology enabler. To overcome the privacy and overhead challenges of centralized…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Dispersed Federated Learning: Vision, Taxonomy, and Future Directions">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Dispersed Federated Learning: Vision, Taxonomy, and Future Directions">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2008.05189">

<!--Generated on Thu Mar  7 06:03:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Federated learning,  machine learning,  resource optimization,  matching theory.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Dispersed Federated Learning: Vision, Taxonomy, and Future Directions</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Latif U. Khan, Walid Saad, , Zhu Han, , and Choong Seon Hong
</span><span class="ltx_author_notes">
L. U. Khan and C. S. Hong are with the Department of Computer Science &amp; Engineering, Kyung Hee University, Yongin-si 17104, South Korea.
Walid Saad is with the Wireless@VT, Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA 24061 USA
Zhu Han is with the Electrical and Computer Engineering Department, University of Houston, Houston, TX 77004 USA, and also with the Computer Science Department, University of Houston, Houston, TX 77004 USA, and the Department of Computer Science and Engineering, Kyung Hee University, South Korea.
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">The ongoing deployments of the Internet of Things (IoT)-based smart applications are spurring the adoption of machine learning as a key technology enabler. To overcome the privacy and overhead challenges of centralized machine learning, there has been a significant recent interest in the concept of federated learning. Federated learning offers on-device machine learning without the need to transfer end-devices data to a third party location. However, federated learning has robustness concerns because it might stop working due to a failure of the aggregation server (e.g., due to a malicious attack or physical defect). Furthermore, federated learning over IoT networks requires a significant amount of communication resources for training. To cope with these issues, we propose a novel framework of dispersed federated learning (DFL) that is based on the true decentralization. We opine that DFL will serve as a practical implementation of federated learning for various IoT-based smart applications such as smart industries and intelligent transportation systems. First, the fundamentals of the DFL are presented. Second, a taxonomy is devised with a qualitative analysis of various DFL schemes. Third, a DFL framework for IoT networks is proposed with a matching theory-based solution. Finally, an outlook on future research directions is presented.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Federated learning, machine learning, resource optimization, matching theory.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Recent years have revealed a significant rise in the number of Internet of Things (IoT) devices to enable various applications, such as smart health-care, augmented reality, industry 4.0, autonomous driving cars, among others. These applications use emerging communication and computing technologies along with machine learning to offer smart services. In order to deploy machine learning in large-scale, heterogeneous systems such as the IoT, it is necessary to preserve the privacy of the data and reduce the communication overhead. As a result, centralized machine learning techniques may not be suitable. Instead, <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">federated learning (FL)</span>  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a distributed machine learning solution that can be amenable to deployment in an IoT. Although FL enables on-device machine learning, it faces a few challenges.</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">Traditional FL based on a centralized aggregation server might suffer from a malicious user attack or failure due to a physical damage, which significantly degrades the performance of FL. The aggregation server can be attacked by (a) an outsider that is not participating the learning process or (b) one of end-devices participating in the learning process.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A malicious aggregation server can infer the end-devices sensitive information from their learning model parameters <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Therefore, there is a need to address the privacy leakage challenge of FL.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">FL requires a significant amount of communication resources for the iterative exchange of learning model parameters between massive number of devices and the aggregation server.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To truly benefit from the deployment of FL in IoT networks, we must resolve the aforementioned challenges. To address the privacy concerns of FL, a user-level differential privacy protection obtained by adding noise to local learning models was proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Although the differential privacy scheme in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> can offer privacy preservation, it may suffer from prolonging the global FL model convergence time. Other works in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> proposed privacy preserving schemes based on over-the-air-computation. The works in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> can preserve the privacy but at the cost of additional design complexity. On the other hand, a number of recent works (e.g., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>) considered resource optimization in FL. However, these works (i.e., <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>) did not discuss FL privacy issues. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, a novel framework, called collaborative FL (CFL) was proposed to enable the participation of devices with insufficient communication resources in the learning process for performance enhancement. In CFL, the devices send their local learning models to nearby devices with sufficient communication resources for local aggregation. Subsequently, the receiving devices send the locally aggregated models to the base station (BS) for global aggregation. The BS after performing global aggregation sends back the global model updates to the end-devices for local models update. On the other hand, hierarchical FL proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> has multiple iterative sub-global aggregations at various small cell base stations (SBSs) and global aggregation at macrocell BS. All of three works, such as traditional FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, hierarchical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and CFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> suffer from a robustness issue. In contrast, the main contribution of this paper is a novel framework, dubbed <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">dispersed FL (DFL)</span> that offers learning of a global FL model in a fully distributed manner. The DFL framework uses a distributed fashion of learning to jointly offer efficient communication resources reuse, robustness, and enhanced privacy for FL. In our proposed DFL, first, sub-global models are computed within different groups consisting of closely located end-devices. The sub-global models are then aggregated to yield a global model. Aggregation, as discussed in Section <a href="#S2" title="II DFL: Fundamentals and Taxonomy ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, can be either centralized or distributed depending on the type of DFL. The distributed aggregations of sub-global models will result in a robust operation in contrast to traditional FL that is based on a centralized aggregation. Finally, the global model updates are sent back to the end-devices. The two-stage aggregation of learning models in DFL can offer a better privacy protection compared to the traditional FL. Inferring end-devices sensitive information from sub-global models (e.g., at global server) is very difficult compared to inferring of information at sub-global model computation server using local learning model updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. Thus, we can say that DFL can offer a better privacy preservation. Furthermore, one can reuse the communication resources occupied by other cellular users within small groups used for sub-global model computation to offer efficient communication resource usage. The summary of our contributions are as follows:</p>
</div>
<div id="S1.p3" class="ltx_para">
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">We present a framework of DFL and proposed a taxonomy using the approach of aggregating of sub-global models to yield a global model, as a parameter.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">We propose a cost function for DFL that captures loss in global model accuracy due to packet error rate and local learning model accuracy. To minimize the cost of DFL, we propose an iterative scheme that performs joint association and resource allocation. For association, we use a one-sided one-to-many matching game, whereas a one-sided one-to-one matching game is used for resource allocation.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">To validate our proposed iterative-matching-game-enabled solution, we provide numerical results. We show the fast convergence of the proposed scheme for a fewer number of global DFL rounds. Furthermore, we evaluate the performance of DFL using the MNIST dataset for image classification tasks, which shows promising results.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">Finally, we conclude the paper and provide an outlook on future research directions.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2008.05189/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="363" height="363" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Taxonomy of DFL</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">DFL: Fundamentals and Taxonomy</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic">Fundamentals</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In DFL, a sub-global model is first iteratively computed for different groups similar to traditional FL. Next, the sub-global model updates are aggregated to yield to a global model. The aggregation of the sub-global models can be performed either in a centralized similar to hierarchical FL or a distributed way. Finally, the global model updates are sent back to all the devices involved in learning as shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Here, learning takes place in an iterative manner until convergence of the global FL model to a desirable value. When using a centralized global aggregation server, DFL can be considered to be hierarchical FL with sub-global aggregation at small cell base stations (SBSs) and global at a macrocell base station (MBS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> considered a hierarchical FL scheme that performs global aggregation of sub-global models at the MBS. Hierarchical FL generally follows the leader-follower concept, where cloud/MBS acts as a leader and leads the learning process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The leader will announce to the followers about participation by broadcasting the messages. After receiving the acknowledgment from the followers, the leader will decide on the participation of devices in the learning process along with initialization parameters (e.g., fixed sub-global iterations for all groups). In contrast, DFL will have multiple groups without following the single leader with multiple followers trend. Within every group, the sub-global aggregator can act as a leader and control the learning within that group. To learn a global model, different groups can coordinate with each other about the model initialization and sharing of sub-global model updates. DFL provides more freedom for performing sub-global model learning by using different selection criteria of end-devices with other parameters (e.g., sub-global iterations). Additionally, the trust verification criteria for participation of devices in learning can also be distinct for every group. On the other hand, the sub-global model iterations can also be different depending on the computational and communication resource constraints of end-devices within groups used for sub-global model computation. Performing more sub-global iterations yields better performance in terms of learning model accuracy. Such a fashion of variable sub-global iterations can offer an additional advantage of fairness. For fixed sub-global model iterations, some of the groups might not perform well in terms of accuracy due to data and system heterogeneity. The sub-global model computing groups with high accuracy will affect the global model more compared to the poor-performing sub-global model computing groups. Therefore, groups with high computing resources can perform more sub-global iterations to improve their performance to enable fairness-based DFL. Overall, DFL can better scale with the number of devices.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">For a fixed global FL model accuracy, there is a tradeoff between the number of sub-global iterations and the number of global rounds. An increase in sub-global model iterations requires few global rounds to train the FL model, and vice versa. The selection of sub-global model iterations and global rounds strongly depend on the application and settings used. For instance, consider an intelligent transportation setting in which the sub-global model computation takes place at the level of an autonomous vehicle and the global model aggregation at the core network. For such scenarios, it is preferable to use more sub-global iterations than global iterations due to the high mobility of vehicles and the potentially high number of handovers between the roadside units (RSUs). On the other hand, consider the scenario where different smart industries seek to train a DFL model for a certain application. First, we can train a sub-global model within every industry. Next, the sub-global models are shared between the industries. Finally, global aggregation takes place within every industry and global model updates are sent back to end-devices. In such a scenario of smart industries, we can use a few sub-global iterations and more global rounds compared to the later scenario of autonomous vehicles.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">Now we discuss the advantages of DFL. First, DFL provides better privacy preservation than the traditional FL. In DFL, there are two aggregations, such as sub-global aggregation and global aggregation. A sub-global aggregator can infer the end-devices sensitive information using their local updates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, whereas it is very difficult for a global aggregation server to infer the end-devices sensitive information from the sub-global model updates. On the other hand, in traditional FL there is only one aggregation that can easily infer the end-devices information from the learning model updates. Therefore, we can say that DFL can preserve privacy better than the traditional FL. Furthermore, DFL can offer more robust operation than the FL. In contrast to FL, DFL performs multiple sub-global aggregations. These multiple aggregations are followed by global aggregation that can be either centralized or distributed. A distributed aggregation of sub-global model updates will offer the highest robustness as given in Table <a href="#S2.T1" title="TABLE I ‣ II-B Taxonomy ‣ II DFL: Fundamentals and Taxonomy ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. Other than robustness and privacy preservation, the DFL trains multiple sub-global models in distributed groups. Therefore, one can efficiently reuse the channel resources that are already in use by other cellular users by keeping the interference level below the maximum allowed limit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. Collectively, we can say that DFL can provide robust operation, better privacy preservation, and efficient reuse of communication resources than the FL.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic">Taxonomy</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We can categorize DFL into two main types (shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) depending on the fashion of global model aggregation: CDFL and DDFL, as discussed next.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.2.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S2.T1.3.2" class="ltx_text" style="font-size:90%;">Comparison of FL and various DFL schemes.</span></figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S2.T1.4.1" class="ltx_tr">
<td id="S2.T1.4.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></span>
</span>
</td>
<td id="S2.T1.4.1.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.2.1.1" class="ltx_p" style="width:159.3pt;"><span id="S2.T1.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</td>
<td id="S2.T1.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.3.1.1" class="ltx_p" style="width:42.7pt;"><span id="S2.T1.4.1.3.1.1.1" class="ltx_text ltx_font_bold">Edge-based CDFL</span></span>
</span>
</td>
<td id="S2.T1.4.1.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.4.1.1" class="ltx_p" style="width:42.7pt;"><span id="S2.T1.4.1.4.1.1.1" class="ltx_text ltx_font_bold">Cloud-based CDFL</span></span>
</span>
</td>
<td id="S2.T1.4.1.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.5.1.1" class="ltx_p" style="width:42.7pt;"><span id="S2.T1.4.1.5.1.1.1" class="ltx_text ltx_font_bold">Edge-based DDFL</span></span>
</span>
</td>
<td id="S2.T1.4.1.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.6.1.1" class="ltx_p" style="width:42.7pt;"><span id="S2.T1.4.1.6.1.1.1" class="ltx_text ltx_font_bold">Blockchain-based DDFL</span></span>
</span>
</td>
<td id="S2.T1.4.1.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.1.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.1.7.1.1" class="ltx_p" style="width:42.7pt;"><span id="S2.T1.4.1.7.1.1.1" class="ltx_text ltx_font_bold">Traditional cloud-based FL</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.2" class="ltx_tr">
<td id="S2.T1.4.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.2.1.1.1.1" class="ltx_text ltx_font_bold">Global model computation delay</span></span>
</span>
</td>
<td id="S2.T1.4.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.2.1.1" class="ltx_p" style="width:159.3pt;">This is related to the delay in computing global model during for one global round.</span>
</span>
</td>
<td id="S2.T1.4.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.3.1.1" class="ltx_p" style="width:42.7pt;">Lowest</span>
</span>
</td>
<td id="S2.T1.4.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.4.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
<td id="S2.T1.4.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.5.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
<td id="S2.T1.4.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.6.1.1" class="ltx_p" style="width:42.7pt;">Highest</span>
</span>
</td>
<td id="S2.T1.4.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="S2.T1.4.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.2.7.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.3" class="ltx_tr">
<td id="S2.T1.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.3.1.1.1.1" class="ltx_text ltx_font_bold">Robustness</span></span>
</span>
</td>
<td id="S2.T1.4.3.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.2.1.1" class="ltx_p" style="width:159.3pt;">It refers to a successful operation of DFL in case of edge server/cloud server/miners physical damage. Additionally, robustness is also related to the interruption of DFL due to malicious attack on the aggregation server.</span>
</span>
</td>
<td id="S2.T1.4.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.3.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
<td id="S2.T1.4.3.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.4.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
<td id="S2.T1.4.3.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.5.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
<td id="S2.T1.4.3.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.6.1.1" class="ltx_p" style="width:42.7pt;">Highest</span>
</span>
</td>
<td id="S2.T1.4.3.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.3.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.3.7.1.1" class="ltx_p" style="width:42.7pt;">Lowest</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.4.1.1.1.1" class="ltx_text ltx_font_bold">Communication resources usage</span></span>
</span>
</td>
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.2.1.1" class="ltx_p" style="width:159.3pt;">This refers to the communication resources used for computing global DFL model.</span>
</span>
</td>
<td id="S2.T1.4.4.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.3.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
<td id="S2.T1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.4.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
<td id="S2.T1.4.4.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.5.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
<td id="S2.T1.4.4.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.6.1.1" class="ltx_p" style="width:42.7pt;">Highest</span>
</span>
</td>
<td id="S2.T1.4.4.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="S2.T1.4.4.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.7.1.1" class="ltx_p" style="width:42.7pt;">Higher</span>
</span>
</td>
</tr>
<tr id="S2.T1.4.5" class="ltx_tr">
<td id="S2.T1.4.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.1.1.1" class="ltx_p" style="width:56.9pt;"><span id="S2.T1.4.5.1.1.1.1" class="ltx_text ltx_font_bold">Implementation complexity</span></span>
</span>
</td>
<td id="S2.T1.4.5.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.2.1.1" class="ltx_p" style="width:159.3pt;">This is the measure of the complexity in the computation of a global DFL model.</span>
</span>
</td>
<td id="S2.T1.4.5.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.3.1.1" class="ltx_p" style="width:42.7pt;">Lowest</span>
</span>
</td>
<td id="S2.T1.4.5.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.4.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.4.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
<td id="S2.T1.4.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.5.1.1" class="ltx_p" style="width:42.7pt;">High</span>
</span>
</td>
<td id="S2.T1.4.5.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.6.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.6.1.1" class="ltx_p" style="width:42.7pt;">Highest</span>
</span>
</td>
<td id="S2.T1.4.5.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t">
<span id="S2.T1.4.5.7.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.7.1.1" class="ltx_p" style="width:42.7pt;">Low</span>
</span>
</td>
</tr>
</table>
</figure>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS1.4.1.1" class="ltx_text">II-B</span>1 </span>Centralized-Aggregation-Enabled DFL</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">In CDFL, a global FL model is obtained by aggregation of the sub-global models at a centralized server. In contrast to CFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, CDFL iteratively computes sub-global models for all groups of closely located devices prior to global aggregation. Meanwhile, in CFL, few single iteration local aggregations at a few devices take place prior to global aggregation at the BS. CDFL can be either edge-based or cloud-based depending on the context of the machine learning model. Edge-based CDFL is more suitable for the local context-aware, specialized machine learning model for devices located within a small geographical region. For instance, we can train a local context-aware, specialized machine learning model (e.g., a smart keyboard search suggestion for a regional language) using edge-based CDFL. To do so, we can consider an edge-based SBS and a device-to-device (D2D) communication network reusing the already occupied frequency bands by other cellular users. Multiple clusters with their own cluster heads can be formed using some criteria (e.g., throughput enhancement). First, sub-global models can be trained in a fashion similar to traditional FL for different clusters with the cluster head acting for sub-global model aggregation. Second, sub-global models are transferred to the edge servers for global model aggregation by cluster heads. Finally, the global model updates are transmitted back to the cluster heads which disseminate them to all the devices. On the other hand, cloud-based DFL can train global context-aware, generalized machine learning models for devices located within several geographically distributed regions. The process of cloud-based DFL is similar to edge-based DFL, but with the aggregation of sub-global models taking place at the remote cloud. Although cloud-based DFL enable more generalized learning of a model, it suffers from the high-latency issue. Furthermore, comparative analysis with DFL schemes are also provided in Table <a href="#S2.T1" title="TABLE I ‣ II-B Taxonomy ‣ II DFL: Fundamentals and Taxonomy ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S2.SS2.SSS2.4.1.1" class="ltx_text">II-B</span>2 </span>Distributed-Aggregation-Enabled DFL</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">DDFL aggregation of the sub-global model updates is performed at distributed nodes in contrast to CDFL. CDFL is based on centralized aggregation of sub-global models which can be suffered from centralized server crash or a malicious attack. DDFL overcomes these issues by using distributed nodes-based aggregation. However, it suffers from high latency and extra communication resources for transfer of sub-global model updates among distributed nodes, as given in Table <a href="#S2.T1" title="TABLE I ‣ II-B Taxonomy ‣ II DFL: Fundamentals and Taxonomy ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. To implement DDFL, there can be two possible ways depending on the way in which the sub-global model updates are exchanged. The sharing of sub-global model updates can be either using blockchain-based miners or direct exchange between edge servers. Blockchain-based DFL can offer the most trustful transfer of sub-global model updates. However, it faces the inherent challenge of high latency associated with a blockchain consensus algorithm (e.g., proof of work) that is not desirable in FL model computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Therefore, we must design novel consensus algorithms with low latency for blockchain-based DFL.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">DDFL Framework for IoT Systems</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">For an IoT system-enabled by FL using a massive number of end-devices, we need a significant amount of communication resources for training. To enable communication resource-efficient and robust FL for such an IoT network, we can either use CDFL or DDFL. In contrast to CDFL, DDFL performs sub-global model aggregations at multiple locations and thus offers a more robust operation.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">System Model</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Consider an IoT network which consists of IoT devices and edge server-enabled SBSs. The IoT devices, each having its own local datasets seek to train FL model. The IoT devices show significant heterogeneity in terms of computational power (CPU-cycles/sec), local dataset size, and data distribution. To account for IoT devices heterogeneity, we use the notion of relative local accuracy. The lower values of relative accuracy reflect better local learning model accuracy, and thus less global communication rounds are needed to achieve a desirable global FL accuracy. To enable edge-based DDFL in IoT network, first, a sub-global model is computed for SBS in an iterative fashion via interaction with its associated IoT devices. After the computation of the sub-global model updates by all the SBSs, the sub-global model updates are transferred between all the SBSs via fast backhaul links. Finally, all the SBSs perform global model aggregation and sent the global model updates to their associated IoT devices. The process for edge-based DDFL uses wireless channel resources. The packet error rate due to channel uncertainties might cause severe degradation effect on the performance of the DDFL process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We define a cost function that jointly accounts for relative local accuracy and the degradation effect due to the packet error rate on the performance of DDFL. The cost function is given by taking the product of the relative accuracy term (i.e., <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">1</annotation></semantics></math>+relative accuracy) and packet error rate term (i.e., <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">1</annotation></semantics></math>-exponential function with an exponent of <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="(-1/SINR)" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.1a" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml">−</mo><mrow id="S3.SS1.p1.3.m3.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.cmml"><mn id="S3.SS1.p1.3.m3.1.1.1.1.2.2.2" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.2.cmml">1</mn><mo id="S3.SS1.p1.3.m3.1.1.1.1.2.2.1" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.1.cmml">/</mo><mi id="S3.SS1.p1.3.m3.1.1.1.1.2.2.3" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.3.cmml">S</mi></mrow><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.2.1" xref="S3.SS1.p1.3.m3.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.1.1.2.3.cmml">I</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.2.1a" xref="S3.SS1.p1.3.m3.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.1.1.2.4" xref="S3.SS1.p1.3.m3.1.1.1.1.2.4.cmml">N</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.1.1.2.1b" xref="S3.SS1.p1.3.m3.1.1.1.1.2.1.cmml">​</mo><mi id="S3.SS1.p1.3.m3.1.1.1.1.2.5" xref="S3.SS1.p1.3.m3.1.1.1.1.2.5.cmml">R</mi></mrow></mrow><mo stretchy="false" id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><minus id="S3.SS1.p1.3.m3.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></minus><apply id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2"><times id="S3.SS1.p1.3.m3.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.1"></times><apply id="S3.SS1.p1.3.m3.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2"><divide id="S3.SS1.p1.3.m3.1.1.1.1.2.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.1"></divide><cn type="integer" id="S3.SS1.p1.3.m3.1.1.1.1.2.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.2">1</cn><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.2.3">𝑆</ci></apply><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.3">𝐼</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.4.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.4">𝑁</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.2.5.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.2.5">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">(-1/SINR)</annotation></semantics></math> multiplied by a waterfall threshold). For high relative local accuracy, the local learning model accuracy will be low and vice versa. Therefore, lower values of relative local accuracy are desirable. The packet error rate strongly depends on the device’s signal-to-interference-plus-noise ratio (SINR). Therefore, to enable edge-based DDFL for IoT networks while minimizing the cost function, there is a need to address the two following challenges.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">An IoT network is characterized by a dynamic topology due to the free mobility of IoT devices. Furthermore, the seamless connectivity of IoT devices with the SBSs is desirable during the exchange of learning model parameters. Considering the aforementioned challenges, we must appropriately associate IoT devices with corresponding SBSs so as to optimize FL performance by increasing the overall throughput that consequently minimizes the cost.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">To effectively use the limited available communication resources, we can reuse the already occupied resource blocks by cellular users for IoT-networks. Therefore, we must propose an effective resource allocation strategy for FL-based IoT networks.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Iterative-Matching-Based Solution</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">In this subsection, we present an iterative scheme for cost optimization of proposed edge-based DDFL. First, we consider several constraints for our DDFL framework:</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_italic">Constraint 1:</span> The size of the packet used to transfer the local learning model of end-devices to the aggregation server strictly depends on the architecture of the local learning model. The architecture of local learning model is dependent on the application and dataset. Therefore, we assume that every device can be allocated a maximum of one resource block.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_italic">Constraint 2:</span> Every resource block must not be allocated to more than one device.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">Constraint 3:</span> The total number of resource blocks allocated to all devices must not be greater than the maximum available resource blocks.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_italic">Constraint 4:</span> Every device be associated to a maximum of one SBS.</p>
</div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.1" class="ltx_p"><span id="S3.I2.i5.p1.1.1" class="ltx_text ltx_font_italic">Constraint 5:</span> Every SBS can be associated with maximum number of devices determined by a threshold (fixed in this paper for all SBSs).</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We consider a cost function which jointly considers devices relative local accuracy and effect of packet error rate on global DDFL model accuracy. The cost function jointly depends on both resource allocation and devices association. Due to the NP-hard nature of the joint optimization problem for optimizing both resource allocation and device association, we decompose the main problem into two sub-problems: resource allocation and association sub-problems. Our proposed algorithm solves one problem (i.e., resource allocation) by fixing the other problem (i.e., association) in an iterative manner until the convergence.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.4.1.1" class="ltx_text">III-B</span>1 </span>One-to-One Matching-Based Resource Allocation</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.6" class="ltx_p">To minimize the DDFL cost (i.e., improving FL accuracy) for IoT networks by improving SINR, we can use a low-complexity matching theory-based wireless resource allocation scheme. Our resource allocation problem is similar to house allocation problem that can be represented by a tuple <math id="S3.SS2.SSS1.p1.1.m1.3" class="ltx_Math" alttext="(\mathcal{A},\mathcal{H},\mathcal{P})" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.3a"><mrow id="S3.SS2.SSS1.p1.1.m1.3.4.2" xref="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml"><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.3.4.2.1" xref="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">𝒜</mi><mo id="S3.SS2.SSS1.p1.1.m1.3.4.2.2" xref="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.1.m1.2.2" xref="S3.SS2.SSS1.p1.1.m1.2.2.cmml">ℋ</mi><mo id="S3.SS2.SSS1.p1.1.m1.3.4.2.3" xref="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.1.m1.3.3" xref="S3.SS2.SSS1.p1.1.m1.3.3.cmml">𝒫</mi><mo stretchy="false" id="S3.SS2.SSS1.p1.1.m1.3.4.2.4" xref="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.3b"><vector id="S3.SS2.SSS1.p1.1.m1.3.4.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.4.2"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝒜</ci><ci id="S3.SS2.SSS1.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.2.2">ℋ</ci><ci id="S3.SS2.SSS1.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.3.3">𝒫</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.3c">(\mathcal{A},\mathcal{H},\mathcal{P})</annotation></semantics></math>, where <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">\mathcal{A}</annotation></semantics></math>, <math id="S3.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{H}" display="inline"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml">ℋ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><ci id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">ℋ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">\mathcal{H}</annotation></semantics></math>, and <math id="S3.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{P}" display="inline"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">𝒫</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝒫</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">\mathcal{P}</annotation></semantics></math> represent the agents set, houses set, and preference profile of agents over houses, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Similar to the house allocation problem, we can define a preference profile <math id="S3.SS2.SSS1.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{P}_{r}" display="inline"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><msub id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.5.m5.1.1.2" xref="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS1.p1.5.m5.1.1.3" xref="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><apply id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.2">𝒫</ci><ci id="S3.SS2.SSS1.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">\mathcal{P}_{r}</annotation></semantics></math> for a set of resource blocks over devices. To allocate resource blocks to devices, we use a one-sided one-to-one matching game. All the resource blocks rank devices based on the values of the cost function. For a particular resource block, the device with low cost is preferred over the car with a high cost. Using preference profile <math id="S3.SS2.SSS1.p1.6.m6.1" class="ltx_Math" alttext="\mathcal{P}_{r}" display="inline"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><msub id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.6.m6.1.1.2" xref="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS1.p1.6.m6.1.1.3" xref="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><apply id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.6.m6.1.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.6.m6.1.1.2.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.2">𝒫</ci><ci id="S3.SS2.SSS1.p1.6.m6.1.1.3.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">\mathcal{P}_{r}</annotation></semantics></math>, all the resource blocks are allocated to devices iteratively until no blocking pair is left, which shows stable matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.4.1.1" class="ltx_text">III-B</span>2 </span>One-to-Many Matching-Based Devices Association</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.3" class="ltx_p">For a fixed resource block allocation, the DDFL cost strongly depends on the association of devices with SBSs. Therefore, we must associate the devices with SBSs so as to minimize the DDFL cost by increasing the throughput. Moreover, it will decrease DDFL convergence time. We can associate devices to SBSs using a brute force algorithm, it suffers from high computational complexity. In contrast to the house allocation problem, our device association problem involves the assignment of a single SBS to multiple devices. Therefore, we use one-sided one-to-many matching for the association of SBSs to devices. First, we define a preference profile <math id="S3.SS2.SSS2.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{P}_{a}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><msub id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\mathcal{P}_{a}</annotation></semantics></math> for RSUs. The preference profile <math id="S3.SS2.SSS2.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{P}_{a}" display="inline"><semantics id="S3.SS2.SSS2.p1.2.m2.1a"><msub id="S3.SS2.SSS2.p1.2.m2.1.1" xref="S3.SS2.SSS2.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.2.m2.1.1.2" xref="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p1.2.m2.1.1.3" xref="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.2.m2.1b"><apply id="S3.SS2.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS2.p1.2.m2.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.2.m2.1c">\mathcal{P}_{a}</annotation></semantics></math> ranks all the devices based on the values of the cost function. Similar to <math id="S3.SS2.SSS2.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{P}_{r}" display="inline"><semantics id="S3.SS2.SSS2.p1.3.m3.1a"><msub id="S3.SS2.SSS2.p1.3.m3.1.1" xref="S3.SS2.SSS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS2.p1.3.m3.1.1.2" xref="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml">𝒫</mi><mi id="S3.SS2.SSS2.p1.3.m3.1.1.3" xref="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml">r</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.3.m3.1b"><apply id="S3.SS2.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.SSS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.2">𝒫</ci><ci id="S3.SS2.SSS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS2.p1.3.m3.1.1.3">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.3.m3.1c">\mathcal{P}_{r}</annotation></semantics></math>, the device with a high cost is given less preference than the device with a low cost. Once the preference profile is obtained for all devices, the association is performed in an iterative manner until no blocking par is left to achieve stable matching <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2008.05189/assets/x2.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="153" height="121" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2008.05189/assets/x3.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="153" height="118" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2008.05189/assets/x4.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="146" height="102" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.4.2.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.2.1" class="ltx_text" style="font-size:90%;">(a) Variations in DDFL cost for SINR and relative local accuracy,(b) Proposed DDFL cost vs. iterations, (c) Accuracy vs. communication rounds for non-IID data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and <math id="S3.F2.2.1.m1.1" class="ltx_Math" alttext="T=8" display="inline"><semantics id="S3.F2.2.1.m1.1b"><mrow id="S3.F2.2.1.m1.1.1" xref="S3.F2.2.1.m1.1.1.cmml"><mi id="S3.F2.2.1.m1.1.1.2" xref="S3.F2.2.1.m1.1.1.2.cmml">T</mi><mo id="S3.F2.2.1.m1.1.1.1" xref="S3.F2.2.1.m1.1.1.1.cmml">=</mo><mn id="S3.F2.2.1.m1.1.1.3" xref="S3.F2.2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.2.1.m1.1c"><apply id="S3.F2.2.1.m1.1.1.cmml" xref="S3.F2.2.1.m1.1.1"><eq id="S3.F2.2.1.m1.1.1.1.cmml" xref="S3.F2.2.1.m1.1.1.1"></eq><ci id="S3.F2.2.1.m1.1.1.2.cmml" xref="S3.F2.2.1.m1.1.1.2">𝑇</ci><cn type="integer" id="S3.F2.2.1.m1.1.1.3.cmml" xref="S3.F2.2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.1.m1.1d">T=8</annotation></semantics></math>.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Performance Evaluation</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.6" class="ltx_p">We present numerical results to evaluate the performance of the proposed framework for IoT networks. Moreover, we evaluate the performance of the proposed DFFL framework for an image classification task using the MNIST dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Our simulation scenario consists of <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="54" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mn id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">54</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><cn type="integer" id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">54</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">54</annotation></semantics></math> devices and <math id="S3.SS3.p1.2.m2.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn type="integer" id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">6</annotation></semantics></math> SBSs which are deployed in an area of <math id="S3.SS3.p1.3.m3.1" class="ltx_Math" alttext="1000\times 1000~{}m^{2}" display="inline"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mrow id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2.2" xref="S3.SS3.p1.3.m3.1.1.2.2.cmml">1000</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS3.p1.3.m3.1.1.2.1" xref="S3.SS3.p1.3.m3.1.1.2.1.cmml">×</mo><mn id="S3.SS3.p1.3.m3.1.1.2.3" xref="S3.SS3.p1.3.m3.1.1.2.3.cmml">1000</mn></mrow><mo lspace="0.330em" rspace="0em" id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">​</mo><msup id="S3.SS3.p1.3.m3.1.1.3" xref="S3.SS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS3.p1.3.m3.1.1.3.2" xref="S3.SS3.p1.3.m3.1.1.3.2.cmml">m</mi><mn id="S3.SS3.p1.3.m3.1.1.3.3" xref="S3.SS3.p1.3.m3.1.1.3.3.cmml">2</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><times id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1"></times><apply id="S3.SS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2"><times id="S3.SS3.p1.3.m3.1.1.2.1.cmml" xref="S3.SS3.p1.3.m3.1.1.2.1"></times><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.2.cmml" xref="S3.SS3.p1.3.m3.1.1.2.2">1000</cn><cn type="integer" id="S3.SS3.p1.3.m3.1.1.2.3.cmml" xref="S3.SS3.p1.3.m3.1.1.2.3">1000</cn></apply><apply id="S3.SS3.p1.3.m3.1.1.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS3.p1.3.m3.1.1.3.2">𝑚</ci><cn type="integer" id="S3.SS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS3.p1.3.m3.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">1000\times 1000~{}m^{2}</annotation></semantics></math>. The position of the SBS is taken fixed whereas devices are positioned randomly. All the values are generated using an average of <math id="S3.SS3.p1.4.m4.1" class="ltx_Math" alttext="500" display="inline"><semantics id="S3.SS3.p1.4.m4.1a"><mn id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">500</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><cn type="integer" id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">500</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">500</annotation></semantics></math> different runs. Other parameters such as carrier frequency and number of sub-carriers per resource block are taken <math id="S3.SS3.p1.5.m5.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p1.5.m5.1a"><mn id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><cn type="integer" id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">2</annotation></semantics></math>GHz and <math id="S3.SS3.p1.6.m6.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S3.SS3.p1.6.m6.1a"><mn id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><cn type="integer" id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">12</annotation></semantics></math>, respectively. Furthermore, the free space path loss model is considered in our work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The position of the SBS is taken fixed whereas devices are deployed randomly. All statistical results are generated using an average of 500 different runs. We compare the performance of our proposed iterative-matching-based scheme with two baselines schemes. One-to-many matching-based association and random resource allocation are used for baseline-1, whereas random association and one-to-one matching-based resource allocation are used for baseline-2. We use the term ”iterations” to refer to one-time execution of both one-to-one matching-based resource allocation and one-to-many matching-based association. Furthermore, the keyword ”communication round” denotes global iteration for DDFL.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Fig. <a href="#S3.F2" title="Figure 2 ‣ III-B2 One-to-Many Matching-Based Devices Association ‣ III-B Iterative-Matching-Based Solution ‣ III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>a shows the variations in DDFL cost for signal-to-interference-plus-noise-ratio (SINR) and relative local accuracy. For a fixed relative local accuracy, the DDFL cost decreases exponentially with an increase in SINR and vice versa. For lower values of SINR, the decrease in DFL cost is very less, but for higher values of SINR, the cost decreases significantly. On the other hand, a decrease in relative local accuracy shows a decrease in DDFL cost. The DDFL cost shows lower values for higher and lower values of SNIR and relative local accuracy, respectively. This shows that the cost of training DDFL global model is low for devices having high local learning model accuracy (low relative local accuracy) and high throughput and vice versa. Fig. <a href="#S3.F2" title="Figure 2 ‣ III-B2 One-to-Many Matching-Based Devices Association ‣ III-B Iterative-Matching-Based Solution ‣ III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>b shows superior performance of our proposed DDFL scheme over baseline-1, and baseline-2 schemes. Moreover, all three schemes showed fast convergence for a lower number of iterations. Baseline-1 has better performance than baseline-2, which shows that DDFL cost depends more on devices association than resource block allocation.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.5" class="ltx_p">Finally, Fig. <a href="#S3.F2" title="Figure 2 ‣ III-B2 One-to-Many Matching-Based Devices Association ‣ III-B Iterative-Matching-Based Solution ‣ III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c evaluates the performance of DDFL using the MNIST dataset for image classification tasks using a convolutional neural network at the local devices. We consider <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">T</annotation></semantics></math> as the product of local iterations and sub-global iterations for a single global round. For analysis, we use Non-IID distribution of data that is based on sorting and dividing the entire MNIST dataset into <math id="S3.SS3.p3.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S3.SS3.p3.2.m2.1a"><mn id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><cn type="integer" id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">200</annotation></semantics></math> shards of <math id="S3.SS3.p3.3.m3.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S3.SS3.p3.3.m3.1a"><mn id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><cn type="integer" id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">300</annotation></semantics></math> images each and assigns <math id="S3.SS3.p3.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S3.SS3.p3.4.m4.1a"><mn id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><cn type="integer" id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">2</annotation></semantics></math> shards to every device <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. FL here refers to traditional cloud-based FL using an equal number of end-devices and local iterations (i.e., 8) at every device, as DDFL. From Fig. <a href="#S3.F2" title="Figure 2 ‣ III-B2 One-to-Many Matching-Based Devices Association ‣ III-B Iterative-Matching-Based Solution ‣ III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c, we can clearly observe that DDFL outperforms traditional FL at all sub-global iterations. Fig. <a href="#S3.F2" title="Figure 2 ‣ III-B2 One-to-Many Matching-Based Devices Association ‣ III-B Iterative-Matching-Based Solution ‣ III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>c shows that DDFL exhibits a convergence time that is much faster than traditional FL when using different numbers of sub-global iterations. For a fixed <math id="S3.SS3.p3.5.m5.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS3.p3.5.m5.1a"><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">T</annotation></semantics></math>, faster convergence is observed for higher number of sub-global iterations, and thus requiring few global rounds to reach a certain accuracy level. From the above discussion, we observe a tradeoff between the number of sub-global iterations and the number of global communication rounds for achieving a certain global FL accuracy.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and Future Directions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we have presented a novel idea of DFL. A taxonomy is devised based on the fashion of global model aggregation for DFL. We have found that DFL can be adopted widely to achieve resource-efficient and privacy-aware implementation of FL for several IoT scenarios. Furthermore, DFL offers a tradeoff between convergence rate and the number of sub-global iterations. Finally, we present several future research directions regarding DFL.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic">Data Heterogeneity-Aware DFL</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">How do we enable FL for a massive number of devices with heterogeneous datasets of non-IID nature? In contrast to FedAvg, FedProx was developed to account for data heterogeneity in the training of a global FL model. FedProx is based on the addition of a weighted proximal term to end-device loss function to handle data heterogeneity. However, choosing the weight of the proximal term might be challenging. Furthermore, it is unclear whether FedProx can provably improve the convergence rate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. To cope with these challenges, we can propose novel heterogeneity-aware DFL for devices with heterogeneous datasets. A set of devices can be divided into different clusters based on datasets homogeneity, with their own cluster heads acting for sub-global aggregation. The sub-global models are obtained in an iterative manner via the exchange of learning model parameters between the end-devices and their cluster heads. Then, the sub-global models are aggregated to yield a global model which is then sent back to all the cluster heads. Finally, the cluster heads disseminate the global model updates to all the devices for updating their local learning models. This process of global model computation takes place iteratively until convergence. It must be noted that homogeneous clustering-based DFL will converge faster than the FL because of two reasons. DFL converges faster than the FL as revealed by Section <a href="#S3" title="III DDFL Framework for IoT Systems ‣ Dispersed Federated Learning: Vision, Taxonomy, and Future Directions" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. Moreover, learning of a sub-global model for a homogeneous set of devices will minimize the convergence time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.4.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.5.2" class="ltx_text ltx_font_italic">Enhanced Distributed Privacy-Aware DFL</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">How do we enable DFL with enhanced privacy preservation? Although the works in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> can be used to enable privacy preservation in FL, these algorithms may not be able to perform well for our proposed DFL framework. Different than FL, two kinds of weight divergences will occur in DFL: one between the end-devices and sub-global servers, and the other between sub-global servers and global aggregation server. In contrast to global aggregation server in DFL, the server used for sub-global aggregation can infer the end-devices sensitive information from their local learning models. Therefore, new effective privacy-preserving schemes within groups used for sub-global model computation must be designed. One way is to add artificial noise to local learning models before sending for sub-global aggregation. Other way can be use of wireless channel noise as a privacy preserving scheme for DFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.4.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.5.2" class="ltx_text ltx_font_italic">Mobility-Aware DFL</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">How do we enable efficient DFL for mobile nodes? An end-device involved in DFL process requires seamless connectivity with the sub-global aggregation server during sub-global model computation. Therefore, it is necessary to propose novel protocols that accounts for end-devices mobility. Several ways can be used for mobility management such as deep learning-enabled mobility prediction, hidden Markov model, Bayesian network, and data mining. Deep learning models (e.g., Long Short Term Memory) can be used to learn devices mobility pattern using their previous traces. Hidden Markov model can also be used for devices mobility pattern because devices’ mobility posses Markov property. Additionally, Bayesian network uses directed acyclic graph for mobility management of devices.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas,
“Communication-efficient learning of deep networks from decentralized
data,” pp. 1273–1282, 2017.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
W. Y. B. Lim, N. C. Luong, D. T. Hoang, Y. Jiao, Y.-C. Liang, Q. Yang,
D. Niyato, and C. Miao, “Federated learning in mobile edge networks: A
comprehensive survey,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</em>,
vol. 22, no. 3, pp. 2031–2063, third quarter 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. B. McMahan, D. Ramage, K. Talwar, and L. Zhang, “Learning differentially
private recurrent language models,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06963</em>,
2017.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Koda, K. Yamamoto, T. Nishio, and M. Morikura, “Differentially private
aircomp federated learning with power adaptation harnessing receiver noise,”
<em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.06337</em>, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
D. Liu and O. Simeone, “Privacy for free: Wireless federated learning via
uncoded transmission with adaptive power control,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on
Selected Areas in Communications</em>, vol. 39, no. 1, pp. 170–185, January
2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M. Chen, Z. Yang, W. Saad, C. Yin, H. V. Poor, and S. Cui, “A joint learning
and communications framework for federated learning over wireless networks,”
<em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Early access, IEEE Transactions on Wireless Communications</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
S. Wang, T. Tuor, T. Salonidis, K. K. Leung, C. Makaya, T. He, and
K. Chan, “Adaptive federated learning in resource constrained edge
computing systems,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">IEEE Journal on Selected Areas in Communications</em>,
vol. 37, no. 6, pp. 1205–1221, June 2019.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
M. S. H. Abad, E. Ozfatura, D. Gunduz, and O. Ercetin, “Hierarchical federated
learning across heterogeneous cellular networks,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">IEEE
International Conference on Acoustics, Speech and Signal Processing</em>,
Barcelona, Spain, May 2020, pp. 8866–8870.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
M. Chen, H. V. Poor, W. Saad, and S. Cui, “Wireless communications for
collaborative federated learning,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>,
vol. 58, no. 12, pp. 48–54, December 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Nasr, R. Shokri, and A. Houmansadr, “Comprehensive privacy analysis of deep
learning: Passive and active white-box inference attacks against centralized
and federated learning,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">IEEE Symposium on Security and Privacy</em>,
USA, May 2019, pp. 739–753.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Zhao, C. Feng, H. H. Yang, and X. Luo, “Federated-learning-enabled
intelligent fog radio access networks: Fundamental theory, key techniques,
and future trends,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE wireless communications</em>, vol. 27, no. 2, pp.
22–28, April 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Hosseinalipour, C. G. Brinton, V. Aggarwal, H. Dai, and M. Chiang, “From
federated to fog learning: Distributed machine learning over heterogeneous
wireless networks,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Communications Magazine</em>, vol. 58, no. 12,
pp. 41–47, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. A. Kazmi, N. H. Tran, W. Saad, Z. Han, T. M. Ho, T. Z. Oo, and C. S. Hong,
“Mode selection and resource allocation in device-to-device communications:
A matching game approach,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Mobile Computing</em>,
vol. 16, no. 11, pp. 3126–3141, March 2017.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
G. Lee, J. Park, W. Saad, and M. Bennis, “Performance analysis of blockchain
systems with wireless mobile miners,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Networking Letters</em>,
vol. 2, no. 3, pp. 111–115, September 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
L. Zhou, “On a conjecture by gale about one-sided matching problems,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Journal of Economic Theory</em>, vol. 52, no. 1, pp. 123–135, October
1990.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Z. Han, D. Niyato, W. Saad, T. Başar, and A. Hjørungnes, <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Game
theory in wireless and communication networks: theory, models, and
applications</em>.   Cambridge university
press, 2012.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
K. Bonawitz, Z. Charles, G. Cormode, R. Cummings <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Advances
and open problems in federated learning,” <em id="bib.bib17.2.2" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1912.04977</em>, 2019.

</span>
</li>
</ul>
</section>
<figure id="id1" class="ltx_float biography">
<table id="id1.1" class="ltx_tabular">
<tr id="id1.1.1" class="ltx_tr">
<td id="id1.1.1.1" class="ltx_td"><img src="/html/2008.05189/assets/latif.jpg" id="id1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="id1.1.1.2" class="ltx_td">
<span id="id1.1.1.2.1" class="ltx_inline-block">
<span id="id1.1.1.2.1.1" class="ltx_p"><span id="id1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Latif U. Khan</span>  is currently pursuing his Ph.D. degree in Computer Engineering at Kyung Hee University (KHU), South Korea. He received his MS (Electrical Engineering) degree with distinction from University of Engineering and Technology, Peshawar, Pakistan in 2017. His research interests include analytical techniques of optimization and game theory to edge computing and end-to-end network slicing.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id2" class="ltx_float biography">
<table id="id2.1" class="ltx_tabular">
<tr id="id2.1.1" class="ltx_tr">
<td id="id2.1.1.1" class="ltx_td"><img src="/html/2008.05189/assets/walid_saad.jpg" id="id2.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="81" height="125" alt="[Uncaptioned image]"></td>
<td id="id2.1.1.2" class="ltx_td">
<span id="id2.1.1.2.1" class="ltx_inline-block">
<span id="id2.1.1.2.1.1" class="ltx_p"><span id="id2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Walid Saad </span>  (S’07, M’10, SM’15, F’19) received his Ph.D. degree from the University of Oslo in 2010. Currently, he is a professor in the Department of Electrical and Computer Engineering at Virginia Tech. His research interests include wireless networks, machine learning, game theory, cybersecurity, unmanned aerial vehicles, and cyber-physical systems. He is the author/co-author of eight conference best paper awards and of the 2015 IEEE ComSoc Fred W. Ellersick Prize.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id3" class="ltx_float biography">
<table id="id3.1" class="ltx_tabular">
<tr id="id3.1.1" class="ltx_tr">
<td id="id3.1.1.1" class="ltx_td"><img src="/html/2008.05189/assets/Zhu_han.jpg" id="id3.1.1.1.g1" class="ltx_graphics ltx_img_portrait" width="94" height="125" alt="[Uncaptioned image]"></td>
<td id="id3.1.1.2" class="ltx_td">
<span id="id3.1.1.2.1" class="ltx_inline-block">
<span id="id3.1.1.2.1.1" class="ltx_p"><span id="id3.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Zhu Han</span> (S’01, M’04, SM’09, F’14) received his Ph.D. degree in electrical and computer engineering from the University of Maryland, College Park. Currently, he is a professor in the Electrical and Computer Engineering Department as well as in the Computer Science Department at the University of Houston, Texas. Dr. Han is an AAAS fellow since 2019. Dr. Han is 1% highly cited researcher since 2017 according to Web of Science.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure id="id4" class="ltx_float biography">
<table id="id4.1" class="ltx_tabular">
<tr id="id4.1.1" class="ltx_tr">
<td id="id4.1.1.1" class="ltx_td"><img src="/html/2008.05189/assets/CS_Hong.jpg" id="id4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="100" height="100" alt="[Uncaptioned image]"></td>
<td id="id4.1.1.2" class="ltx_td">
<span id="id4.1.1.2.1" class="ltx_inline-block">
<span id="id4.1.1.2.1.1" class="ltx_p"><span id="id4.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Choong Seon Hong</span>  (S’95-M’97-SM’11) is working as a professor with the Department of Computer Science and Engineering, Kyung Hee University. His research interests include future Internet, ad hoc networks, network management, and network security. He is currently an Associate Editor of the IEEE Transactions on Network and Service Management, International Journal of Network Management, and Journal of Communications and Networks and an Associate Technical Editor of the IEEE Communications Magazine.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2008.05188" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2008.05189" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2008.05189">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2008.05189" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2008.05190" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 06:03:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
