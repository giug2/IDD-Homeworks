<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Retrieval Augmented Spelling Correction for E-Commerce Applications</title>
<!--Generated on Tue Oct 15 04:21:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.11655v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S1" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S2" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S2.SS0.SSS0.Px1" title="In 2 Related Work ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title">Retrieval Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S2.SS0.SSS0.Px2" title="In 2 Related Work ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title">Retrieval Augmented Fine-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S2.SS0.SSS0.Px3" title="In 2 Related Work ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title">Contextualized Spelling Correction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS1" title="In 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS2" title="In 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retrieval Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS3" title="In 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Prompt Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS4" title="In 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>LLM Fine-Tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS1" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS2" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS3" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Zero-Shot LLM Performance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS4" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>RAG with a Frozen LLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS5" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>LLM Fine-Tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS6" title="In 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Latency Considerations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S5" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S6" title="In Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Ethics Statements</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document" lang="en">
<h1 class="ltx_title ltx_title_document">Retrieval Augmented Spelling Correction for E-Commerce Applications</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuan Guo 
<br class="ltx_break"/>Amazon.com Inc
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">xuangu@amazon.com</span>
<br class="ltx_break"/>&amp;Rohit Patki 
<br class="ltx_break"/>Amazon.com Inc
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">patkir@amazon.com</span>
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="id3.3.id3">\AND</span>Dante Everaert 
<br class="ltx_break"/>Amazon.com Inc
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.4.id4">danteev@amazon.com</span>
<br class="ltx_break"/>&amp;Christopher Potts 
<br class="ltx_break"/>Stanford University 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id5.5.id5">cgpotts@stanford.edu</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id6.id1"><span class="ltx_text" id="id6.id1.1">The rapid introduction of new brand names into everyday language poses a unique challenge for e-commerce spelling correction services, which must distinguish genuine misspellings from novel brand names that use unconventional spelling. We seek to address this challenge via Retrieval Augmented Generation (RAG). On this approach, product names are retrieved from a catalog and incorporated into the context used by a large language model (LLM) that has been fine-tuned to do contextual spelling correction. Through quantitative evaluation and qualitative error analyses, we find improvements in spelling correction utilizing the RAG framework beyond a stand-alone LLM. We also demonstrate the value of additional finetuning of the LLM to incorporate retrieved context.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Retrieval Augmented Spelling Correction for E-Commerce Applications</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tr" id="p1.1.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1">Xuan Guo</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.1">Amazon.com Inc</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.3.1.1">xuangu@amazon.com</span></span></span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tr" id="p1.1.2.2.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1">Rohit Patki</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.1">Amazon.com Inc</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.3.1.1">patkir@amazon.com</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.3" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.3.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.1.1">
<span class="ltx_tr" id="p1.1.3.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.1.1.1.1.1">Dante Everaert</span></span></span>
<span class="ltx_tr" id="p1.1.3.1.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.2.1">Amazon.com Inc</span></span>
<span class="ltx_tr" id="p1.1.3.1.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.1.1.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.3.1.1.3.1.1">danteev@amazon.com</span></span></span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.3.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.3.2.1">
<span class="ltx_tr" id="p1.1.3.2.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.3.2.1.1.1.1">Christopher Potts</span></span></span>
<span class="ltx_tr" id="p1.1.3.2.1.2">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.2.1">Stanford University</span></span>
<span class="ltx_tr" id="p1.1.3.2.1.3">
<span class="ltx_td ltx_align_center" id="p1.1.3.2.1.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.3.2.1.3.1.1">cgpotts@stanford.edu</span></span></span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">New brand names are continuously introduced, and many of them use unconventional spelling to create specific associations while still ensuring that the brand is unique and memorable. Prominent examples include “playgro” vs. “playground”, “biotanicals” vs. “botanicals”, and “hygeeni” vs. “hygiene”. Such cases pose a significant challenge for e-commerce spelling correction services, which are prone to over-correcting such terms, especially completely novel ones.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we seek to address this challenge by leveraging Retrieval Augmented Generation (RAG). On this approach, the user’s query is passed to a retrieval module that seeks to find relevant items from a product catalog. The retrieved items are then incorporated into a prompt to a large language model (LLM) that predicts a correct spelling for the user’s query.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">We report on a wide range of experiments with different retrieval models and LLMs. We find that the RAG-based approach consistently leads to large performance improvements with only minor latency increases. In addition, we explore methods for fine-tuning the LLM to make better use of retrieved contexts, and we find that this leads to very substantial improvements, with the largest gains coming from queries that contain brand names.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Retrieval Augmentation</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Retrieval has proven highly effective across a wide range of knowledge intensive tasks. Early works such as DrQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib3" title="">2017</a>)</cite> and Dense Passage Retrieval (DPR; <cite class="ltx_cite ltx_citemacro_citet">Karpukhin et al. <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib8" title="">2020</a></cite>) laid important groundwork by integrating retrieval with neural models to enhance question-answering accuracy and facilitate knowledge access. REALM <cite class="ltx_cite ltx_citemacro_cite">Guu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib4" title="">2020</a>)</cite> introduced unsupervised retrieval for language model pre-training, and <cite class="ltx_cite ltx_citemacro_citet">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib11" title="">2020</a>)</cite> combined retrieval with generation to tackle knowledge intensive tasks. RETRO <cite class="ltx_cite ltx_citemacro_cite">Borgeaud et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib2" title="">2022</a>)</cite> scaled these ideas by accessing a vast database of tokens for contextual relevance across large datasets. <cite class="ltx_cite ltx_citemacro_citet">Khattab et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib9" title="">2021</a>)</cite> weakly supervise the ColBERT neural retrieval model to improve performance on open-domain question answering. Overall, these approaches seem to help systems provide up-to-date knowledge and reduce hallucinations <cite class="ltx_cite ltx_citemacro_cite">Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib1" title="">2023</a>)</cite>. Despite these advances, the specific challenges of contextual spelling correction in e-commerce settings, particularly for dynamically evolving brand names, remain underexplored.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Retrieval Augmented Fine-Tuning</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Retrieval Augmented Fine-Tuning (RAFT) adapts models by fine-tuning them for specific tasks like question answering, with a strong focus on handling irrelevant documents to boost accuracy <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib20" title="">2024</a>)</cite>. Similarly, Atlas <cite class="ltx_cite ltx_citemacro_cite">Izacard et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib6" title="">2023</a>)</cite> demonstrates the effectiveness of retrieval-augmented models for few-shot learning by integrating retrieved content during both pre-training and fine-tuning phases. While other models like RPT <cite class="ltx_cite ltx_citemacro_cite">Rubin and Berant (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib13" title="">2023</a>)</cite> and REPLUG <cite class="ltx_cite ltx_citemacro_cite">Shi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib16" title="">2023</a>)</cite> also combine retrieval with generative capabilities, they either focus on black-box integration or training from scratch without fine-tuning on external context. Our approach, akin to RAFT, fine-tunes the LLM with task-specific data, distinguishing between relevant and irrelevant contexts, especially for complex non-standard brand names. This underscores the value of targeted retrieval in specialized tasks like e-commerce spelling correction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Contextualized Spelling Correction</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Related studies in character-level and multilingual spelling correction highlight the importance of context and fine-grained adjustments. For example, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib19" title="">2023</a>)</cite> show that using multiple teacher models improves spelling correction across diverse languages, underscoring the value of context-specific training. <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib5" title="">2023</a>)</cite> introduce structural interventions at the subword level to improve spelling correction, particularly for morphologically complex terms. Unlike these methods, which are tailored to specific contexts, our RAG-based approach generalizes to evolving and unconventional brand names by retrieving up-to-date context on demand, enhancing adaptability and robustness in real-world e-commerce applications.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p2.1">Contextualized spell-checking methods <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib17" title="">2023</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib18" title="">2023</a>)</cite> emphasize the value of integrating external knowledge to handle domain-specific terms, demonstrating the benefits of user-specific data for improved spelling accuracy. Unlike these methods, which use prompt conditioning and attention mechanisms to incorporate context, our approach employs RAG to meet the unique demands of an evolving e-commerce catalog. By leveraging RAG, we dynamically integrate new and modified brand names directly from the catalog, enabling adaptation to non-standard lexicons unique to brand names. This retrieval-based method helps address the continuously updated nature of brand catalogs in ways that prompt-tuning or attention-based techniques alone may not fully resolve, particularly for ambiguous or unconventional spellings.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We now present our approach to spelling correction, which leverages Retrieval Augmented Generation (RAG) and optionally includes a phase of fine-tuning the LLM to make better use of context.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Language Models</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We experiment primarily with <span class="ltx_text" id="S3.SS1.p1.1.1">Mistral-7B</span> <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib7" title="">2023</a>)</cite> (specifically, <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.2">open-mistral-7b</span> v0.1) and Claude-3-sonnet (<span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.3">claude-3-sonnet-20240229</span> v1:0). We chose Mistral-7B because it is highly effective for its size (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.SS3" title="4.3 Zero-Shot LLM Performance ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">4.3</span></a>), and we chose Claude-3-sonnet as a representative of a much larger class of LLMs. We would expect to see similar results for other LLMs, even larger and more capable ones, because our central challenge is making accurate predictions about novel brand names.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Retrieval Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We evaluate three main retrieval methods:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">BM25</span> <cite class="ltx_cite ltx_citemacro_citep">(Robertson et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib12" title="">2009</a>)</cite> is a traditional, time-tested n-gram-based retrieval model. We expect it to excel where exact matching suffices but may struggle where fuzzy or semantic matching is called for.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Fuzzy BM25</span> combines traditional BM25 with fuzzy matching. This allows for minor spelling errors that are common in e-commerce queries. For instance, it can match “air fryer cusinart” to both “air fryer” and “cuisinart”.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i3.p1.1.1">ColBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Khattab and Zaharia, <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib10" title="">2020</a>; Santhanam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib14" title="">2022a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib15" title="">b</a>)</cite> is a neural retrieval model that represents queries and documents with token-level vectors. We expect this model to excel at retrieving semantically relevant terms.</p>
</div>
</li>
</ol>
<p class="ltx_p" id="S3.SS2.p1.2">In all cases, we index our product catalog. For ColBERT, this is done using a pretrained ColBERTv2 model checkpoint released by the ColBERT team.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="https://huggingface.co/colbert-ir/colbertv2.0" title="">huggingface.co/colbert-ir/colbertv2.0</a></span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Prompt Design</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The following is the LLM prompt template that we use where the retrieved items are provided as context:</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<pre class="ltx_verbatim ltx_font_typewriter" id="S3.SS3.p2.1">
### Instruction:
Provide spelling correction for given query
if necessary, referring to the provided context
if it’s relevant.
### Context:
{context}
### Query:
{input}
### Correction:
</pre>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">Retrieved items are formatted as a comma-separated list and placed in the <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p3.1.1">context</span> field. We use the top 3 or 4 retrieved items, with the precise number controlled by the maximum prompt length we permit in our fine-tuning runs. (Future work could explore the effects of including more context items at inference time.)</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>LLM Fine-Tuning</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Our approach includes an optional step of fine-tuning the LLM to make more effective use of context. We consider two variants.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">For <span class="ltx_text ltx_font_bold" id="S3.SS4.p2.1.1">Basic Fine-Tuning</span>, the training dataset consists of 50K <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.2">&lt;input query, label query&gt;</span> pairs derived from search logs to create a training dataset. Here, <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.3">label query</span> reflects user-validated corrections. For this fine-tuning, we remove the “referring to the provided context if it’s relevant” wording from <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.4">Instruction</span> of the prompt in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS3" title="3.3 Prompt Design ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3.3</span></a>, with the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.5">context</span> field also removed and the desired output appended to the end followed by <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p2.1.6">###</span> on a newline.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">For <span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">Contextual Fine-Tuning</span>, we begin with the same dataset of 50K pairs, but now we retrieve context for each <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.1.2">&lt;input query&gt;</span>, forming <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.1.3">&lt;input query, context, label query&gt;</span> triples. This added context, derived through the same retrieval mechanism used in RAG, helps teach the model how to make use of the <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.1.4">context</span> field. The prompt has the same format as the one in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS3" title="3.3 Prompt Design ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3.3</span></a> but with the desired output appended to the end followed by <span class="ltx_text ltx_font_typewriter" id="S3.SS4.p3.1.5">###</span> on a newline.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">For both variants, the fine-tuning process is simply additional language model training using strings formatted from our prompt templates. Further evaluation details, including metrics, are covered in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4" title="4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.2.1">LLM (no finetune)</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.2.2">Size</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S3.T1.1.2.3">F1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.1">Flan UL2</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.2">20B</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S3.T1.1.3.3">13.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.1">mT0</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.2">13B</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.4.3">19.3</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.1">Flan T5</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.2">11B</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.5.3">20.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.1">mGPT</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.2">13B</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.6.3">21.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.1">Mistral</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.2">7B</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.7.3">28.1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.2">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_left" id="S3.T1.1.1.1">70B<sup class="ltx_sup" id="S3.T1.1.1.1.1"><span class="ltx_text ltx_font_italic" id="S3.T1.1.1.1.1.1">∗</span></sup>
</td>
<td class="ltx_td ltx_align_right" id="S3.T1.1.1.3">34.7</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.8.1">Mixtral</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.8.2">47B</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S3.T1.1.8.3">57.4</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results for zero-shot LLMs. The largest models achieve the best results, and Mistral-7B achieves excellent results for its size. <sup class="ltx_sup" id="S3.T1.5.1"><span class="ltx_text ltx_font_italic" id="S3.T1.5.1.1">∗</span></sup>The size given for Claude-3-sonnet is a guess based on the comparative performance of the model relative to others of known size.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Data</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our evaluation dataset is sourced from search logs collected between 2021 and 2023. Each data point consists of an <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.1">&lt;input query, label query&gt;</span> pair. The <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.2">input query</span> refers to the user’s original search query, while the <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.3">label query</span> is obtained from annotators. We conducted stratified sampling to arrive at a 10K <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p1.1.4">input query</span> set, which was designed to promote the diversity of the query population, particularly with regard to the presence of misspellings (roughly 1/4) and brand names (roughly 1/3 cases with a brand name). To ensure label quality, we applied a “2+1” annotation method. That is, two annotators initially labeled each query; if they disagreed, an auditor made the final determination. This process included specific instructions to guide annotators on the e-commerce context. We use this dataset to evaluate all experiments in this paper.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Metrics</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our primary metric for evaluating spelling correction quality is the F1 score, which is the harmonic mean of precision and recall. Precision reflects the proportion of model-predicted corrections that match the gold standard annotations, while recall measures the proportion of required corrections that the model identifies correctly. We rely on exact match criteria, where two strings are considered equal after punctuation removal. All F1 scores are reported as percentages for clarity.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Zero-Shot LLM Performance</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.T1" title="Table 1 ‣ 3.4 LLM Fine-Tuning ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">1</span></a> reports baseline results for a range of different LLMs used without any retrieval. The prompt template used for this is the same one as in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS3" title="3.3 Prompt Design ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3.3</span></a> without the <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.1">context</span> field and its mentions in the <span class="ltx_text ltx_font_typewriter" id="S4.SS3.p1.1.2">Instruction</span> section. The top-performing model by a large margin is Mixtral-47B, followed by Claude-3-sonnet (<math alttext="\approx" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mo id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">≈</mo><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><approx id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"></approx></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">\approx</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">≈</annotation></semantics></math>70B, estimated). The much smaller Mistral-7B model <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#bib.bib7" title="">2023</a>)</cite> is reasonably competitive, though, and it represents a better balance of costs and performance, especially for high-volume services like spelling correction.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>RAG with a Frozen LLM</h3>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:380.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(104.0pt,-91.2pt) scale(1.92126689931051,1.92126689931051) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Retriever</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T2.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">LM</td>
<td class="ltx_td ltx_align_right ltx_border_tt" id="S4.T2.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">Doc index</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_tt" id="S4.T2.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">F1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">BM25</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T2.1.1.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">25.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">Fuzzy BM25</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">60K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">31.7</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">Fuzzy BM25</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">34.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">ColBERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">60K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">35.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">ColBERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">Mistral-7B</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">35.9</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">BM25</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.1.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_right ltx_border_t" id="S4.T2.1.1.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" id="S4.T2.1.1.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">26.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">Fuzzy BM25</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">60K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">30.4</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">Fuzzy BM25</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">34.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10">
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">ColBERT</td>
<td class="ltx_td ltx_align_left" id="S4.T2.1.1.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_right" id="S4.T2.1.1.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">60K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right" id="S4.T2.1.1.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">29.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.11">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.1.11.1" style="padding-left:4.0pt;padding-right:4.0pt;">ColBERT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T2.1.1.11.2" style="padding-left:4.0pt;padding-right:4.0pt;">Claude-3-sonnet</td>
<td class="ltx_td ltx_align_right ltx_border_bb" id="S4.T2.1.1.11.3" style="padding-left:4.0pt;padding-right:4.0pt;">572K</td>
<td class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" id="S4.T2.1.1.11.4" style="padding-left:4.0pt;padding-right:4.0pt;">39.3</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results for RAG with frozen LLMs. The best configurations use ColBERT and a larger document index. Both LLMs are substantially improved by RAG.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:235.2pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.1pt,11.4pt) scale(0.911161138148882,0.911161138148882) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.1.1">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.1.1">
<span class="ltx_p" id="S4.T3.1.1.1.1.1.1" style="width:8.5pt;">#</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.2.1">
<span class="ltx_p" id="S4.T3.1.1.1.2.1.1" style="width:71.1pt;">User query</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.3.1">
<span class="ltx_p" id="S4.T3.1.1.1.3.1.1" style="width:190.6pt;">Retrieved items</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.4.1">
<span class="ltx_p" id="S4.T3.1.1.1.4.1.1" style="width:71.1pt;">RAG generation</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S4.T3.1.1.1.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.1.5.1">
<span class="ltx_p" id="S4.T3.1.1.1.5.1.1" style="width:71.1pt;">Ground truth</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.1.2.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.1.1">
<span class="ltx_p" id="S4.T3.1.1.2.1.1.1" style="width:8.5pt;">1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.1.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.2.1">
<span class="ltx_p" id="S4.T3.1.1.2.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.2.2.1.1.1" style="background-color:#FFB3B3;">sumbrella</span> umbrella outdoor patio</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.1.2.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.3.1">
<span class="ltx_p" id="S4.T3.1.1.2.3.1.1" style="width:190.6pt;">abba patio umbrella, patio umbrella <span class="ltx_text" id="S4.T3.1.1.2.3.1.1.1" style="background-color:#B3FFB3;">sunbrella</span>, <span class="ltx_text" id="S4.T3.1.1.2.3.1.1.2" style="background-color:#B3FFB3;">sunbrella</span> umbrellas, hampton bay patio umbrella</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.1.2.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.4.1">
<span class="ltx_p" id="S4.T3.1.1.2.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.2.4.1.1.1" style="background-color:#B3FFB3;">sunbrella</span> umbrella outdoor patio</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S4.T3.1.1.2.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.2.5.1">
<span class="ltx_p" id="S4.T3.1.1.2.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.2.5.1.1.1" style="background-color:#B3FFB3;">sunbrella</span> umbrella outdoor patio</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.3.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.1.1">
<span class="ltx_p" id="S4.T3.1.1.3.1.1.1" style="width:8.5pt;">2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.2.1">
<span class="ltx_p" id="S4.T3.1.1.3.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.3.2.1.1.1" style="background-color:#FFB3B3;">niwax</span> ne tx direct</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.3.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.3.1">
<span class="ltx_p" id="S4.T3.1.1.3.3.1.1" style="width:190.6pt;"><span class="ltx_text" id="S4.T3.1.1.3.3.1.1.1" style="background-color:#B3FFB3;">nikwax</span> tx direct,tx direct spray on,tx direct</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.3.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.4.1">
<span class="ltx_p" id="S4.T3.1.1.3.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.3.4.1.1.1" style="background-color:#B3FFB3;">nikwax</span> ne tx direct</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.3.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.3.5.1">
<span class="ltx_p" id="S4.T3.1.1.3.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.3.5.1.1.1" style="background-color:#B3FFB3;">nikwax</span> ne tx direct</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.4.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.4.1.1">
<span class="ltx_p" id="S4.T3.1.1.4.1.1.1" style="width:8.5pt;">3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.4.2.1">
<span class="ltx_p" id="S4.T3.1.1.4.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.4.2.1.1.1" style="background-color:#FFB3B3;">dubman</span> brush</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.4.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.4.3.1">
<span class="ltx_p" id="S4.T3.1.1.4.3.1.1" style="width:190.6pt;"><span class="ltx_text" id="S4.T3.1.1.4.3.1.1.1" style="background-color:#B3FFB3;">denman</span> brush natural hair, <span class="ltx_text" id="S4.T3.1.1.4.3.1.1.2" style="background-color:#B3FFB3;">denman</span> brush, <span class="ltx_text" id="S4.T3.1.1.4.3.1.1.3" style="background-color:#B3FFB3;">denman</span> nylon brush, <span class="ltx_text" id="S4.T3.1.1.4.3.1.1.4" style="background-color:#B3FFB3;">denman</span> brush</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.4.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.4.4.1">
<span class="ltx_p" id="S4.T3.1.1.4.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.4.4.1.1.1" style="background-color:#B3FFB3;">denman</span> brush</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.4.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.4.5.1">
<span class="ltx_p" id="S4.T3.1.1.4.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.4.5.1.1.1" style="background-color:#B3FFB3;">denman</span> brush</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.5.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.5.1.1">
<span class="ltx_p" id="S4.T3.1.1.5.1.1.1" style="width:8.5pt;">4</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.5.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.5.2.1">
<span class="ltx_p" id="S4.T3.1.1.5.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.5.2.1.1.1" style="background-color:#FFB3B3;">salamin</span> boots</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.5.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.5.3.1">
<span class="ltx_p" id="S4.T3.1.1.5.3.1.1" style="width:190.6pt;">mens <span class="ltx_text" id="S4.T3.1.1.5.3.1.1.1" style="background-color:#B3FFB3;">salomon</span> boots, <span class="ltx_text" id="S4.T3.1.1.5.3.1.1.2" style="background-color:#B3FFB3;">salomon</span> snowboard boots, <span class="ltx_text" id="S4.T3.1.1.5.3.1.1.3" style="background-color:#B3FFB3;">salomon</span> womens winter boots</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.5.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.5.4.1">
<span class="ltx_p" id="S4.T3.1.1.5.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.5.4.1.1.1" style="background-color:#B3FFB3;">salomon</span> boots</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.5.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.5.5.1">
<span class="ltx_p" id="S4.T3.1.1.5.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.5.5.1.1.1" style="background-color:#B3FFB3;">salomon</span> boots</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.6.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.6.1.1">
<span class="ltx_p" id="S4.T3.1.1.6.1.1.1" style="width:8.5pt;">5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.6.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.6.2.1">
<span class="ltx_p" id="S4.T3.1.1.6.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.6.2.1.1.1" style="background-color:#FFB3B3;">tumeric</span> soap bars</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.6.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.6.3.1">
<span class="ltx_p" id="S4.T3.1.1.6.3.1.1" style="width:190.6pt;">bali soap bars, camay soap bars, dettol soap bars, himalaya soap bars</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.6.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.6.4.1">
<span class="ltx_p" id="S4.T3.1.1.6.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.6.4.1.1.1" style="background-color:#B3FFB3;">turmeric</span> soap bars</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.6.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.6.5.1">
<span class="ltx_p" id="S4.T3.1.1.6.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.6.5.1.1.1" style="background-color:#B3FFB3;">turmeric</span> soap bars</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.7.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.7.1.1">
<span class="ltx_p" id="S4.T3.1.1.7.1.1.1" style="width:8.5pt;">6</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.7.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.7.2.1">
<span class="ltx_p" id="S4.T3.1.1.7.2.1.1" style="width:71.1pt;">doom eternal <span class="ltx_text" id="S4.T3.1.1.7.2.1.1.1" style="background-color:#FFB3B3;">puns</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.7.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.7.3.1">
<span class="ltx_p" id="S4.T3.1.1.7.3.1.1" style="width:190.6pt;">doom eternal juguete,doom,doom eternal</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.7.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.7.4.1">
<span class="ltx_p" id="S4.T3.1.1.7.4.1.1" style="width:71.1pt;">doom eternal <span class="ltx_text" id="S4.T3.1.1.7.4.1.1.1" style="background-color:#FFB3B3;">puns</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.7.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.7.5.1">
<span class="ltx_p" id="S4.T3.1.1.7.5.1.1" style="width:71.1pt;">doom eternal <span class="ltx_text" id="S4.T3.1.1.7.5.1.1.1" style="background-color:#B3FFB3;">pins</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.8.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.8.1.1">
<span class="ltx_p" id="S4.T3.1.1.8.1.1.1" style="width:8.5pt;">7</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.8.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.8.2.1">
<span class="ltx_p" id="S4.T3.1.1.8.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.8.2.1.1.1" style="background-color:#FFB3B3;">tonkatsu</span> ramen noodles</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.8.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.8.3.1">
<span class="ltx_p" id="S4.T3.1.1.8.3.1.1" style="width:190.6pt;"><span class="ltx_text" id="S4.T3.1.1.8.3.1.1.1" style="background-color:#FFB3B3;">tonkatsu</span> ramen noodles, <span class="ltx_text" id="S4.T3.1.1.8.3.1.1.2" style="background-color:#FFB3B3;">tonkatsu</span> ramen bowl, buldak ramen noodles, immi ramen noodles</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.8.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.8.4.1">
<span class="ltx_p" id="S4.T3.1.1.8.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.8.4.1.1.1" style="background-color:#B3FFB3;">tonkotsu</span> ramen noodles</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.8.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.8.5.1">
<span class="ltx_p" id="S4.T3.1.1.8.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.8.5.1.1.1" style="background-color:#B3FFB3;">tonkotsu</span> ramen noodles</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.9.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.9.1.1">
<span class="ltx_p" id="S4.T3.1.1.9.1.1.1" style="width:8.5pt;">8</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.9.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.9.2.1">
<span class="ltx_p" id="S4.T3.1.1.9.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.9.2.1.1.1" style="background-color:#FFB3B3;">correlle</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.9.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.9.3.1">
<span class="ltx_p" id="S4.T3.1.1.9.3.1.1" style="width:190.6pt;"><span class="ltx_text" id="S4.T3.1.1.9.3.1.1.1" style="background-color:#FFB3B3;">correlle</span> cafe red bowl,<span class="ltx_text" id="S4.T3.1.1.9.3.1.1.2" style="background-color:#FFB3B3;">correlle</span> plates white,<span class="ltx_text" id="S4.T3.1.1.9.3.1.1.3" style="background-color:#FFB3B3;">correlle</span>,<span class="ltx_text" id="S4.T3.1.1.9.3.1.1.4" style="background-color:#FFB3B3;">correlle</span> red bowl</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.9.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.9.4.1">
<span class="ltx_p" id="S4.T3.1.1.9.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.9.4.1.1.1" style="background-color:#FFB3B3;">correlle</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S4.T3.1.1.9.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.9.5.1">
<span class="ltx_p" id="S4.T3.1.1.9.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.9.5.1.1.1" style="background-color:#B3FFB3;">corelle</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.1.10.1">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.10.1.1">
<span class="ltx_p" id="S4.T3.1.1.10.1.1.1" style="width:8.5pt;">9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.1.10.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.10.2.1">
<span class="ltx_p" id="S4.T3.1.1.10.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.10.2.1.1.1" style="background-color:#FFB3B3;">laroche b5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.1.10.3">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.10.3.1">
<span class="ltx_p" id="S4.T3.1.1.10.3.1.1" style="width:190.6pt;">guy <span class="ltx_text" id="S4.T3.1.1.10.3.1.1.1" style="background-color:#B3FFB3;">laroche</span>,flydigi b5,<span class="ltx_text" id="S4.T3.1.1.10.3.1.1.2" style="background-color:#B3FFB3;">laroche</span>,<span class="ltx_text" id="S4.T3.1.1.10.3.1.1.3" style="background-color:#B3FFB3;">laroche</span> set</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.1.10.4">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.10.4.1">
<span class="ltx_p" id="S4.T3.1.1.10.4.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.10.4.1.1.1" style="background-color:#FFB3B3;">laroche b5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S4.T3.1.1.10.5">
<span class="ltx_inline-block ltx_align_top" id="S4.T3.1.1.10.5.1">
<span class="ltx_p" id="S4.T3.1.1.10.5.1.1" style="width:71.1pt;"><span class="ltx_text" id="S4.T3.1.1.10.5.1.1.1" style="background-color:#B3FFB3;">la roche b5</span></span>
</span>
</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Qualitative evaluation of RAG-generated spelling corrections across various examples. In 1–4, correctly spelled retrieved items lead to accurate corrections. In 5–6, the retrieved items do not involve misspelled spans of the input query, leading RAG generation to rely on the LLM’s internal knowledge. In 7, the LLM generates the correct spelling despite misspelled retrieved items, while in 8, the model is misled by the incorrect retrieval. Finally, in 9, “la roche” and “laroche” are both real brands. The retriever does not correctly consider the context “b5” to distinguish the brands (“b5” is a specific item that is only associated with brand “la roche”).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T2" title="Table 2 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">2</span></a> provides our primary results. We consider Mistral-7B and Clause-3-sonnet as the base LLMs. For each LLM, we evaluate our three different retrieval models. We also explore the role of the size of the product catalog by considering two different pools of documents: one with 572K documents (132K unique brands) and one with 60K documents (29K unique brands).</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The overall best-performing setting is with ColBERT indexing 572K documents and providing retrieval results to Claude-3-sonnet. This configuration results in 39.3 F1 vs. 34.7 for Clause-3-sonnet used without retrieval (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.T1" title="Table 1 ‣ 3.4 LLM Fine-Tuning ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">1</span></a>): a 4.6 point increase. Strikingly, the next best system is one that uses Mistral-7B, again with ColBERT indexing the larger document collection. This configuration achieves 35.9 vs. 28.1 for Mistral-7B used without retrieval: a 7.8 point increase.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T3" title="Table 3 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3</span></a> provides a more qualitative analysis that reveals nuanced interactions between retrieved context and LLM responses. When relevant context is retrieved (Examples 1–4), the LLM provides accurate corrections aligned with expected outputs. Conversely, in instances where context is absent (Examples 5–6), incorrect (Examples 7–8), or misleading (Example 9), the LLM’s performance varied, highlighting the complex balance between the LLM’s parameterized knowledge and the information retrieved. These examples illustrate that, while retrievers enrich context, they can also introduce noise or irrelevant data that might detract from accuracy if not carefully managed.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.1">
<tr class="ltx_tr" id="S4.T4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.1">Retriever</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S4.T4.1.1.2">LLM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3">Precision</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.4">Recall</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.5">F1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.1" rowspan="2"><span class="ltx_text" id="S4.T4.1.2.1.1">None</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.2.2">Mistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.3">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.4">26.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.5">28.1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3">
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.1">Mistral-7B with Basic Fine-Tuning</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.2">70.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.3">59.0</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.3.4">64.1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.4.1" rowspan="3"><span class="ltx_text" id="S4.T4.1.4.1.1">Fuzzy BM25</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.4.2">Mistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.4.3">42.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.4.4">29.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.4.5">34.6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.1">Mistral-7B with Basic Fine-Tuning</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.2">49.7</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.3">40.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.5.4">44.5</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.6">
<td class="ltx_td ltx_align_left" id="S4.T4.1.6.1">Mistral-7B with Contextual Fine-Tuning</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.2">77.4</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.3">59.5</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.6.4">67.3</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S4.T4.1.7.1" rowspan="3"><span class="ltx_text" id="S4.T4.1.7.1.1">ColBERT</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T4.1.7.2">Mistral-7B</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.7.3">43.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.7.4">30.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.7.5">35.9</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.8">
<td class="ltx_td ltx_align_left" id="S4.T4.1.8.1">Mistral-7B with Basic Fine-Tuning</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.2">52.3</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.3">42.1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.8.4">46.6</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.9">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T4.1.9.1">Mistral-7B with Contextual Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.9.2">77.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.9.3">65.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.9.4">71.0</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance comparison across different retrieval configurations and fine-tuning setups. All experiments used an indexed pool of 572K documents (132K unique brands). The highest F1 score of 71.0 was achieved with ColBERT in RAG, demonstrating the added benefit of Contextual Fine-Tuning. These results indicate that fine-tuning with context-specific instructions is extremely effective.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>LLM Fine-Tuning</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T4" title="Table 4 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes our experiments involving LLM fine-tuning, using both Basic and Contextual variants of this method (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS4" title="3.4 LLM Fine-Tuning ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3.4</span></a>). For these experiments, we adopt Mistral-7B as our base LLM. To facilitate comparisons, the top row in the first section of the table is repeated from Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.T1" title="Table 1 ‣ 3.4 LLM Fine-Tuning ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">1</span></a>, and the top rows from the middle and bottom sections are repeated from Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T2" title="Table 2 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">2</span></a>. We include the precision/recall breakdown here to support further analysis of the trade-offs.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Across all three panels we see substantial gains from fine-tuning, with similar precision/recall ratios across all settings. The largest gains come from Contextual Fine-Tuning. The best performing configuration uses ColBERT and Contextual Fine-Tuning, leading to 70.1 F1, a 34.2 point increase over the system that employs only RAG with ColBERT. Thus, the overall message is very clear: if it is feasible to fine-tune the LLM with context, that is likely to lead to very substantial performance improvements.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Our primary goal is to improve performance on brands that are novel from the perspective of the LLM. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T5" title="Table 5 ‣ 4.5 LLM Fine-Tuning ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">5</span></a> seeks to quantify the extent to which Contextual Fine-Tuning marks progress in this area, as compared to Basic Fine-Tuning. In both cases, we train on the same set of examples (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S3.SS4" title="3.4 LLM Fine-Tuning ‣ 3 Approach ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3.4</span></a>). The table shows that Contextual Fine-Tuning leads to a 6.9 point increase in overall F1 and a 16.7 point increase in queries containing brands. While brands remain very challenging, our approach certainly alleviates the challenge.</p>
</div>
<div class="ltx_para" id="S4.SS5.p4">
<p class="ltx_p" id="S4.SS5.p4.1">These gains are supported by qualitative analysis as well. For example, in cases like “snowflake necklace for women”, where the context includes varied necklaces (“swarovski snowflake necklace for women, efytal necklace for women, birthstone necklace for women, baguette necklace for women pavori”), Contextual Fine-Tuning helps the model produce the correct label “swarovski snowflake necklace for women,” ensuring that it makes thoughtful corrections rather than echoing the context.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T5.1" style="width:433.6pt;height:130.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(96.9pt,-29.1pt) scale(1.80810738449072,1.80810738449072) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1">
<tr class="ltx_tr" id="S4.T5.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T5.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T5.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">F1</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.2">
<td class="ltx_td" id="S4.T5.1.1.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">All queries</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">Brands</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T5.1.1.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">Basic Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">64.1</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.1.1.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">44.1</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T5.1.1.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">RAG + Contextual Fine-Tuning</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">71.0</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">60.8</td>
</tr>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Performance improvements from Contextual Fine-Tuning and RAG as compared to Basic Fine-Tuning without RAG, broken down by overall performance and performance on queries containing brands. The LLM is Mistral-7B and the retriever used for RAG is ColBERT over the 572K document collection.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS5.p5">
<p class="ltx_p" id="S4.SS5.p5.1">Ideally, we would be able to home in on a set of definitely new brands and measure performance on them. However, this set is challenging to define, since multiple sources of knowledge are in play, including the pretrained knowledge parameterized in LLM. However, we are able to identify a set of 525 brands that are in our evaluation set but absent from our fine-tuning dataset. For this set, we actually get an F1 score of 78.4 using Contextual Fine-Tuning and a ColBERT retriever. For future work, we will snapshot new brands by LLM release date as the cutoff, to present a more controlled experiment.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Latency Considerations</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">Incorporating RAG leads to a slight latency increase; however, it remains within acceptable limits for real-time applications. Using the Mistral-7B model as a baseline, retrieval from a pool of 60K candidate documents adds only 2.42% to the overall generation time, while expanding the pool to 572K documents results in a 2.79% increase. These changes are minimal, and they enable substantial gains in accuracy.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduced a fine-tuned Retrieval Augmented Generation (RAG) framework tailored for e-commerce spelling correction, specifically addressing the complexities posed by brand names and other non-standard lexicons. We showed that this approach is highly effective even with a frozen retriever and frozen large language model (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T2" title="Table 2 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">2</span></a>). In addition, we showed that fine-tuning the LLM with retrieved context leads to even larger gains (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T4" title="Table 4 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">4</span></a>), particularly for spelling corrections involving evolving brand names (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T5" title="Table 5 ‣ 4.5 LLM Fine-Tuning ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">5</span></a>). These results underscore the value of incorporating retrieval and allowing the model to dynamically adapt to context in a way that standalone LLMs or RAG with frozen components cannot achieve.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Our qualitative analysis further revealed challenges inherent in using real-world data, such as the presence of misspellings in indexed documents, which can mislead the LLM during generation (Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11655v1#S4.T3" title="Table 3 ‣ 4.4 RAG with a Frozen LLM ‣ 4 Experiments ‣ Retrieval Augmented Spelling Correction for E-Commerce Applications"><span class="ltx_text ltx_ref_tag">3</span></a>). This highlights the importance of ensuring the quality of retrieved contexts. Practical improvements include refining the contextual data through heuristic signals, like user interactions and engagement metrics, to enhance relevance and accuracy. Another promising avenue is to diversify the styles and noise levels within the retrieved context to bolster the model’s robustness.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">For long-term directions, we propose exploring mechanisms that enable LLMs to assess and selectively integrate context based on relevance and quality. Such advancements could pave the way for smarter, more context-aware LLMs that distinguish valuable insights from noise, ultimately enhancing their adaptability in real-world applications. Additionally, evaluating models with context on emerging entities could provide a more dynamic measure of RAG’s effectiveness as new content enters the dataset. These lines of inquiry contribute insights into optimizing LLMs within RAG frameworks, driving advancements in the broader field of adaptive language models and their application in context-sensitive domains.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Ethics Statements</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study uses anonymized, user-generated data to enhance the model’s ability to do contextual spelling correction in e-commerce. We acknowledge that user-generated data may reflect inherent biases, such as regional or demographic linguistic preferences, which could affect spelling correction accuracy for certain user groups. We are committed to monitoring these issues and improving the fairness of the model over time, aiming to make spelling correction equitable, inclusive, and accurate. Future efforts will focus on refining our methodology to address these concerns, especially as the model encounters new data and evolves to handle a broader range of brand-specific terminology and user inputs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-tutorials.6" title="">Retrieval-based language models and applications</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 61st Annual Meeting of the Association
for Computational Linguistics (Volume 6: Tutorial Abstracts)</em>, pages 41–46,
Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. (2022)</span>
<span class="ltx_bibblock">
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf" title="">Improving
language models by retrieving from trillions of tokens</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">International conference on machine learning</em>, pages
2206–2240, Baltimore, Maryland. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P17-1171" title="">Reading Wikipedia to
answer open-domain questions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 55th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1870–1879,
Vancouver, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlr.press/v119/guu20a/guu20a.pdf" title="">Retrieval augmented language model pre-training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">International Conference on Machine Learning</em>, pages
3929–3938, Vienna, Austria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Jing Huang, Zhengxuan Wu, Kyle Mahowald, and Christopher Potts. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.770" title="">Inducing
character-level structure in subword-based language models with type-level
interchange intervention training</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Findings of the Association for Computational Linguistics:
ACL 2023</em>, pages 12163–12180, Toronto, Canada. Association for Computational
Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard et al. (2023)</span>
<span class="ltx_bibblock">
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni,
Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://jmlr.org/papers/volume24/23-0037/23-0037.pdf" title="">Atlas:
Few-shot learning with retrieval augmented language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Journal of Machine Learning Research</em>, 24(251):1–43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et al. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2310.06825" title="">Mistral 7B</a>.

</span>
<span class="ltx_bibblock">arXiv:2310.06825.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.550" title="">Dense
passage retrieval for open-domain question answering</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP)</em>, pages 6769–6781, Online. Association
for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab et al. (2021)</span>
<span class="ltx_bibblock">
Omar Khattab, Christopher Potts, and Matei Zaharia. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00405" title="">Relevance-guided
supervision for OpenQA with ColBERT</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics</em>,
9:929–944.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab and Zaharia (2020)</span>
<span class="ltx_bibblock">
Omar Khattab and Matei Zaharia. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2004.12832" title="">ColBERT: Efficient and
effective passage search via contextualized late interaction over bert</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 43rd International ACM SIGIR conference
on research and development in Information Retrieval</em>, pages 39–48, New
York, NY. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf" title="">Retrieval-augmented generation for knowledge-intensive NLP tasks</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Advances in Neural Information Processing Systems</em>,
33:9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson et al. (2009)</span>
<span class="ltx_bibblock">
Stephen Robertson, Hugo Zaragoza, et al. 2009.

</span>
<span class="ltx_bibblock">The probabilistic relevance framework: BM25 and beyond.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Foundations and Trends® in Information
Retrieval</em>, 3(4):333–389.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rubin and Berant (2023)</span>
<span class="ltx_bibblock">
Ohad Rubin and Jonathan Berant. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2306.13421v2" title="">Retrieval-pretrained
transformer: Long-range language modeling with self-retrieval</a>.

</span>
<span class="ltx_bibblock">arXiv:2306.13421.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et al. (2022a)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia.
2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3511808.3557325" title="">PLAID: An
efficient engine for late interaction retrieval</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 31st ACM International Conference on
Information &amp; Knowledge Management</em>, pages 1747–1756, New York, NY, USA.
Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santhanam et al. (2022b)</span>
<span class="ltx_bibblock">
Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei
Zaharia. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.naacl-main.272" title="">ColBERTv2:
Effective and efficient retrieval via lightweight late interaction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 3715–3734, Seattle, United States. Association for
Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. (2023)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2301.12652" title="">REPLUG:
Retrieval-augmented black-box language models</a>.

</span>
<span class="ltx_bibblock">arXiv:2301.12652.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023)</span>
<span class="ltx_bibblock">
Gan Song, Zelin Wu, Golan Pundak, Angad Chandorkar, Kandarp Joshi, Xavier
Velez, Diamantino Caseiro, Ben Haynor, Weiran Wang, Nikhil Siddhartha, et al.
2023.

</span>
<span class="ltx_bibblock">Contextual spelling correction with large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2023 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU)</em>, pages 1–8. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Xiaoqiang Wang, Yanqing Liu, Jinyu Li, and Sheng Zhao. 2023.

</span>
<span class="ltx_bibblock">Improving contextual spelling correction by external acoustics
attention and semantic aware data augmentation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICASSP 2023-2023 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 1–5. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Jingfen Zhang, Xuan Guo, Sravan Bodapati, and Christopher Potts. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.emnlp-industry.15" title="">Multi-teacher distillation for multilingual spelling correction</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing: Industry Track</em>, pages 142–151, Singapore.
Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Tianjun Zhang, Shishir G Patil, Naman Jain, Sheng Shen, Matei Zaharia, Ion
Stoica, and Joseph E Gonzalez. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.10131" title="">RAFT: Adapting language
model to domain specific RAG</a>.

</span>
<span class="ltx_bibblock">arXiv:2403.10131.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Oct 15 04:21:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
