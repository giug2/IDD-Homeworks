<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.08500] CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving</title><meta property="og:description" content="Conventional frame camera is the mainstream sensor of the autonomous driving scene perception, while it is limited in adverse conditions, such as low light. Event camera with high dynamic range has been applied in assi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.08500">

<!--Generated on Thu Sep  5 17:46:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">
<span id="id4.id1" class="ltx_text ltx_font_medium" style="font-size:58%;">
</span>CoSEC: A Coaxial Stereo Event Camera Dataset 
<br class="ltx_break">for Autonomous Driving
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Shihan Peng<sup id="id5.4.id1" class="ltx_sup"><span id="id5.4.id1.1" class="ltx_text ltx_font_italic">†</span></sup>, Hanyu Zhou<sup id="id6.5.id2" class="ltx_sup"><span id="id6.5.id2.1" class="ltx_text ltx_font_italic">†</span></sup>, Hao Dong, Zhiwei Shi, Haoyue Liu, Yuxing Duan, Yi Chang* and Luxin Yan
</span><span class="ltx_author_notes">*Corresponding author.<sup id="id7.6.id1" class="ltx_sup"><span id="id7.6.id1.1" class="ltx_text ltx_font_italic">†</span></sup>These authors contributed equally to this work.
Shihan Peng, Hanyu Zhou, Hao Dong, Zhiwei Shi, Haoyue Liu, Yuxing Duan, Yi Chang and Luxin Yan are with National Key Lab of Multispectral Information Intelligent Processing Technology, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China. Email: <span id="id8.7.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{pengshihan, hyzhou, donghao0205, shizhiwei, liuhy, duanyuxing, yichang, yanluxin}@hust.edu.cn</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">Conventional frame camera is the mainstream sensor of the autonomous driving scene perception, while it is limited in adverse conditions, such as low light. Event camera with high dynamic range has been applied in assisting frame camera for the multimodal fusion, which relies heavily on the pixel-level spatial alignment between various modalities. Typically, existing multimodal datasets mainly place event and frame cameras in parallel and directly align them spatially via warping operation. However, this parallel strategy is less effective for multimodal fusion, since the large disparity exacerbates spatial misalignment due to the large event-frame baseline. We argue that baseline minimization can reduce alignment error between event and frame cameras. In this work, we introduce hybrid coaxial event-frame devices to build the multimodal system, and propose a coaxial stereo event camera (CoSEC) dataset for autonomous driving. As for the multimodal system, we first utilize the microcontroller to achieve time synchronization, and then spatially calibrate different sensors, where we perform intra- and inter-calibration of stereo coaxial devices. As for the multimodal dataset, we filter LiDAR point clouds to generate depth and optical flow labels using reference depth, which is further improved by fusing aligned event and frame data in nighttime conditions. With the help of the coaxial device, the proposed dataset can promote the all-day pixel-level multimodal fusion. Moreover, we also conduct experiments to demonstrate that the proposed dataset can improve the performance and generalization of the multimodal fusion.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2408.08500/assets/x1.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="612" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Illustration of the multimodal system and dataset. As for the multimodal system, we introduce the beam splitter to design the coaxial event-frame device for the pixel-level spatial alignment, and then build the coaxial stereo multimodal imaging system with the LiDAR and INS. As for the multimodal dataset, we fuse the aligned event-image data and the LiDAR point cloud to generate the ground truth depth and optical flow. In this work, we utilize the multimodal system to collect the coaxial stereo event camera dataset for autonomous driving.
</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">INTRODUCTION</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Multimodal fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is the key to autonomous driving scene perception, aiming to fuse the complementary knowledge between frame, LiDAR, and inertial measurement unit (IMU) to perceive the 3D dynamic scene. Conventional frame camera performs well in ideal imaging conditions, while suffering degradation in adverse imaging conditions, such as low light <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and fast motion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The main reason is that, the frame camera is a fixed-exposure imaging mechanism, which is limited by its low dynamic range and low frame rate. In contrast, the event camera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> with the advantages of high dynamic range and high temporal resolution has been applied in assisting frame camera for multimodal fusion, which relies heavily on the pixel-level spatial alignment between various modal data. In this work, our purpose is to propose a spatiotemporal-aligned multimodal dataset for autonomous driving in Fig. <a href="#S0.F1" title="Figure 1 ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Existing event-based datasets for autonomous driving are mainly divided into three categories: event-only unimodal, event-frame multimodal and stereo event-frame multimodal datasets. As shown in Table <a href="#S1.T1" title="TABLE I ‣ I INTRODUCTION ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, event-only unimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> only focus on relatively simple vision tasks, while being difficult to model ego-motion and 3D dynamic scene. Event-frame multimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> introduce IMU to model the ego-motion of vehicles, but still fail for 3D scene understanding. Stereo event-frame multimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> further introduce LiDAR and stereo cameras to improve 3D dynamic scene perception. These multimodal datasets mainly place conventional frame camera and event camera in parallel, and spatially align them by warping operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, this parallel strategy cannot guarantee the accurate spatial alignment between various modalities, which is less effective for multimodal fusion. Therefore, how to ensure the <em id="S1.p2.1.1" class="ltx_emph ltx_font_italic">event-frame pixel-level alignment</em> is crucial for multimodal fusion.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">As for the pixel-level spatial alignment, we explore the impact of the baseline between event and frame cameras on it in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Event-only Unimodal Datasets ‣ II RELATED WORK ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, illustrating that the large baseline deteriorates the event-frame spatial alignment. The main reason is that, the large baseline between event and frame cameras brings in the large disparity, increasing the pixel shift of the corresponding points between the two cameras, thus restricting the spatial alignment. Therefore, we suggest that <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">baseline minimization reduces the event-frame pixel-level spatial alignment error</em>.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>
Comparison of different event camera datasets for autonomous driving. Event-only unimodal datasets provide single event data for relatively simple vision tasks. Event-frame multimodal datasets introduce frame cameras and INS sensors for motion perception. Stereo event-frame multimodal datasets further introduce LiDAR for 3D scene motion perception. In this work, we further design a coaxial event-frame strategy to improve the pixel-level alignment of the multimodal dataset.
</figcaption>
<table id="S1.T1.24" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.24.25" class="ltx_tr">
<td id="S1.T1.24.25.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.1.1" class="ltx_text ltx_font_bold">Type</span></td>
<td id="S1.T1.24.25.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.2.1" class="ltx_text ltx_font_bold">Dataset</span></td>
<td id="S1.T1.24.25.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.3.1" class="ltx_text ltx_font_bold">LiDAR</span></td>
<td id="S1.T1.24.25.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.4.1" class="ltx_text ltx_font_bold">IMU</span></td>
<td id="S1.T1.24.25.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.5.1" class="ltx_text ltx_font_bold">GNSS</span></td>
<td id="S1.T1.24.25.6" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" colspan="2"><span id="S1.T1.24.25.6.1" class="ltx_text ltx_font_bold">Frame Camera</span></td>
<td id="S1.T1.24.25.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" colspan="2"><span id="S1.T1.24.25.7.1" class="ltx_text ltx_font_bold">Event Camera</span></td>
<td id="S1.T1.24.25.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.24.25.8.1" class="ltx_text ltx_font_bold">Groundtruth</span></td>
</tr>
<tr id="S1.T1.24.26" class="ltx_tr">
<td id="S1.T1.24.26.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.26.1.1" class="ltx_text ltx_font_bold">Resolution</span></td>
<td id="S1.T1.24.26.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.26.2.1" class="ltx_text ltx_font_bold">Color</span></td>
<td id="S1.T1.24.26.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.26.3.1" class="ltx_text ltx_font_bold">Resolution</span></td>
<td id="S1.T1.24.26.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.26.4.1" class="ltx_text ltx_font_bold">Aligned with Frame</span></td>
</tr>
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;" rowspan="4"><span id="S1.T1.1.1.2.1" class="ltx_text"><span id="S1.T1.1.1.2.1.1" class="ltx_text"></span> <span id="S1.T1.1.1.2.1.2" class="ltx_text">
<span id="S1.T1.1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.1.1.2.1.2.1.1" class="ltx_tr">
<span id="S1.T1.1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.1.1.2.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Event-only</span></span></span>
<span id="S1.T1.1.1.2.1.2.1.2" class="ltx_tr">
<span id="S1.T1.1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.1.1.2.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Unimodal</span></span></span>
</span></span> <span id="S1.T1.1.1.2.1.3" class="ltx_text"></span></span></td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.75pt 3.5pt;">N-CARS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>
</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.1.1.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.1.1.5.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.1.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.1.1.6.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.1.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;" colspan="2">
<span id="S1.T1.1.1.7.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"> 304 <math id="S1.T1.1.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.1.1.1.m1.1a"><mo id="S1.T1.1.1.1.m1.1.1" xref="S1.T1.1.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.1.1.1.m1.1b"><times id="S1.T1.1.1.1.m1.1.1.cmml" xref="S1.T1.1.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.1.1.1.m1.1c">\times</annotation></semantics></math> 240</td>
<td id="S1.T1.1.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.1.1.8.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.1.1.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">Classification</td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">ADD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>
</td>
<td id="S1.T1.2.2.3" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.2.2.3.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.2.2.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.2.2.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.2.2.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.2.2.5.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.2.2.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;" colspan="2">
<span id="S1.T1.2.2.6.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.2.2.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 304 <math id="S1.T1.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.2.2.1.m1.1a"><mo id="S1.T1.2.2.1.m1.1.1" xref="S1.T1.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.2.2.1.m1.1b"><times id="S1.T1.2.2.1.m1.1.1.cmml" xref="S1.T1.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.2.2.1.m1.1c">\times</annotation></semantics></math> 240</td>
<td id="S1.T1.2.2.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.2.2.7.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.2.2.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Detection</td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">1MP Detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>
</td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.3.3.3.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.3.3.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.3.3.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.3.3.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.3.3.5.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.3.3.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;" colspan="2">
<span id="S1.T1.3.3.6.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.3.3.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">1280 <math id="S1.T1.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.3.3.1.m1.1a"><mo id="S1.T1.3.3.1.m1.1.1" xref="S1.T1.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.3.3.1.m1.1b"><times id="S1.T1.3.3.1.m1.1.1.cmml" xref="S1.T1.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.3.1.m1.1c">\times</annotation></semantics></math> 720</td>
<td id="S1.T1.3.3.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.3.3.7.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.3.3.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Detection</td>
</tr>
<tr id="S1.T1.4.4" class="ltx_tr">
<td id="S1.T1.4.4.2" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">DET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</td>
<td id="S1.T1.4.4.3" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.4.4.3.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.4.4.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.4.4.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.4.4.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.4.4.5.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.4.4.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;" colspan="2">
<span id="S1.T1.4.4.6.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.4.4.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">1280 <math id="S1.T1.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.4.4.1.m1.1a"><mo id="S1.T1.4.4.1.m1.1.1" xref="S1.T1.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.4.4.1.m1.1b"><times id="S1.T1.4.4.1.m1.1.1.cmml" xref="S1.T1.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.4.1.m1.1c">\times</annotation></semantics></math> 800</td>
<td id="S1.T1.4.4.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.4.4.7.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.4.4.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Lane Extraction</td>
</tr>
<tr id="S1.T1.6.6" class="ltx_tr">
<td id="S1.T1.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;" rowspan="4"><span id="S1.T1.6.6.3.1" class="ltx_text"><span id="S1.T1.6.6.3.1.1" class="ltx_text"></span> <span id="S1.T1.6.6.3.1.2" class="ltx_text">
<span id="S1.T1.6.6.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.6.6.3.1.2.1.1" class="ltx_tr">
<span id="S1.T1.6.6.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.6.6.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Event-Frame</span></span></span>
<span id="S1.T1.6.6.3.1.2.1.2" class="ltx_tr">
<span id="S1.T1.6.6.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.6.6.3.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Multimodal</span></span></span>
</span></span> <span id="S1.T1.6.6.3.1.3" class="ltx_text"></span></span></td>
<td id="S1.T1.6.6.4" class="ltx_td ltx_align_left ltx_border_t" style="padding:0.75pt 3.5pt;">DDD17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S1.T1.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.6.6.5.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.6.6.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.6.6.6.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.6.6.7" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.6.6.7.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.5.5.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.5.5.1.m1.1a"><mo id="S1.T1.5.5.1.m1.1.1" xref="S1.T1.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.5.5.1.m1.1b"><times id="S1.T1.5.5.1.m1.1.1.cmml" xref="S1.T1.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.5.1.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.6.6.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.6.6.8.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.6.6.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.6.6.2.m1.1a"><mo id="S1.T1.6.6.2.m1.1.1" xref="S1.T1.6.6.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.6.6.2.m1.1b"><times id="S1.T1.6.6.2.m1.1.1.cmml" xref="S1.T1.6.6.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.6.2.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.6.6.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.6.6.9.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.6.6.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:0.75pt 3.5pt;">Vehicle Control</td>
</tr>
<tr id="S1.T1.8.8" class="ltx_tr">
<td id="S1.T1.8.8.3" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">DDD20 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S1.T1.8.8.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.8.8.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.8.8.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.8.8.5.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.8.8.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.8.8.6.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.7.7.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.7.7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.7.7.1.m1.1a"><mo id="S1.T1.7.7.1.m1.1.1" xref="S1.T1.7.7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.7.7.1.m1.1b"><times id="S1.T1.7.7.1.m1.1.1.cmml" xref="S1.T1.7.7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.7.7.1.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.8.8.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.8.8.7.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.8.8.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.8.8.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.8.8.2.m1.1a"><mo id="S1.T1.8.8.2.m1.1.1" xref="S1.T1.8.8.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.8.8.2.m1.1b"><times id="S1.T1.8.8.2.m1.1.1.cmml" xref="S1.T1.8.8.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.8.8.2.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.8.8.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.8.8.8.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.8.8.9" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Vehicle Control</td>
</tr>
<tr id="S1.T1.10.10" class="ltx_tr">
<td id="S1.T1.10.10.3" class="ltx_td ltx_align_left" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.3.1" class="ltx_text">Brisbane-Event-VPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite></span></td>
<td id="S1.T1.10.10.4" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.4.1" class="ltx_text"><span id="S1.T1.10.10.4.1.1" class="ltx_ERROR undefined">\usym</span>2613</span></td>
<td id="S1.T1.10.10.5" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.5.1" class="ltx_text"><span id="S1.T1.10.10.5.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.10.10.6" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.6.1" class="ltx_text"><span id="S1.T1.10.10.6.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.9.9.1" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.9.9.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.9.9.1.m1.1a"><mo id="S1.T1.9.9.1.m1.1.1" xref="S1.T1.9.9.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.9.9.1.m1.1b"><times id="S1.T1.9.9.1.m1.1.1.cmml" xref="S1.T1.9.9.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.9.9.1.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.10.10.7" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"><span id="S1.T1.10.10.7.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.10.10.2" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.2.1" class="ltx_text"> 346 <math id="S1.T1.10.10.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.10.10.2.1.m1.1a"><mo id="S1.T1.10.10.2.1.m1.1.1" xref="S1.T1.10.10.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.10.10.2.1.m1.1b"><times id="S1.T1.10.10.2.1.m1.1.1.cmml" xref="S1.T1.10.10.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.10.10.2.1.m1.1c">\times</annotation></semantics></math> 260</span></td>
<td id="S1.T1.10.10.8" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"><span id="S1.T1.10.10.8.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.10.10.9" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.10.10.9.1" class="ltx_text">Place Recognition</span></td>
</tr>
<tr id="S1.T1.11.11" class="ltx_tr">
<td id="S1.T1.11.11.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 1920 <math id="S1.T1.11.11.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.11.11.1.m1.1a"><mo id="S1.T1.11.11.1.m1.1.1" xref="S1.T1.11.11.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.11.11.1.m1.1b"><times id="S1.T1.11.11.1.m1.1.1.cmml" xref="S1.T1.11.11.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.11.11.1.m1.1c">\times</annotation></semantics></math> 1080</td>
<td id="S1.T1.11.11.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.11.11.2.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.11.11.3" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.11.11.3.1" class="ltx_ERROR undefined">\usym</span>2613</td>
</tr>
<tr id="S1.T1.13.13" class="ltx_tr">
<td id="S1.T1.13.13.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="7"><span id="S1.T1.13.13.3.1" class="ltx_text"><span id="S1.T1.13.13.3.1.1" class="ltx_text"></span> <span id="S1.T1.13.13.3.1.2" class="ltx_text">
<span id="S1.T1.13.13.3.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S1.T1.13.13.3.1.2.1.1" class="ltx_tr">
<span id="S1.T1.13.13.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.13.13.3.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Stereo</span></span></span>
<span id="S1.T1.13.13.3.1.2.1.2" class="ltx_tr">
<span id="S1.T1.13.13.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.13.13.3.1.2.1.2.1.1" class="ltx_text ltx_font_bold">Event-Frame</span></span></span>
<span id="S1.T1.13.13.3.1.2.1.3" class="ltx_tr">
<span id="S1.T1.13.13.3.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.13.13.3.1.2.1.3.1.1" class="ltx_text ltx_font_bold">Multimodal</span></span></span>
</span></span> <span id="S1.T1.13.13.3.1.3" class="ltx_text"></span></span></td>
<td id="S1.T1.13.13.4" class="ltx_td ltx_align_left ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.4.1" class="ltx_text">MVSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite></span></td>
<td id="S1.T1.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.5.1" class="ltx_text"><span id="S1.T1.13.13.5.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.13.13.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.6.1" class="ltx_text"><span id="S1.T1.13.13.6.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.13.13.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.7.1" class="ltx_text"><span id="S1.T1.13.13.7.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.12.12.1" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.12.12.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.12.12.1.m1.1a"><mo id="S1.T1.12.12.1.m1.1.1" xref="S1.T1.12.12.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.12.12.1.m1.1b"><times id="S1.T1.12.12.1.m1.1.1.cmml" xref="S1.T1.12.12.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.12.12.1.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.13.13.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.8.1" class="ltx_text"><span id="S1.T1.13.13.8.1.1" class="ltx_ERROR undefined">\usym</span>2613</span></td>
<td id="S1.T1.13.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.2.1" class="ltx_text"> 346 <math id="S1.T1.13.13.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.13.13.2.1.m1.1a"><mo id="S1.T1.13.13.2.1.m1.1.1" xref="S1.T1.13.13.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.13.13.2.1.m1.1b"><times id="S1.T1.13.13.2.1.m1.1.1.cmml" xref="S1.T1.13.13.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.13.13.2.1.m1.1c">\times</annotation></semantics></math> 260</span></td>
<td id="S1.T1.13.13.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"><span id="S1.T1.13.13.9.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.13.13.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.13.13.10.1" class="ltx_text">Depth, Optical Flow</span></td>
</tr>
<tr id="S1.T1.14.14" class="ltx_tr">
<td id="S1.T1.14.14.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 752 <math id="S1.T1.14.14.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.14.14.1.m1.1a"><mo id="S1.T1.14.14.1.m1.1.1" xref="S1.T1.14.14.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.14.14.1.m1.1b"><times id="S1.T1.14.14.1.m1.1.1.cmml" xref="S1.T1.14.14.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.14.14.1.m1.1c">\times</annotation></semantics></math> 480</td>
<td id="S1.T1.14.14.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.14.14.2.1" class="ltx_ERROR undefined">\usym</span>2613</td>
</tr>
<tr id="S1.T1.16.16" class="ltx_tr">
<td id="S1.T1.16.16.3" class="ltx_td ltx_align_left" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.3.1" class="ltx_text">FusionPortableV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite></span></td>
<td id="S1.T1.16.16.4" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.4.1" class="ltx_text"><span id="S1.T1.16.16.4.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.16.16.5" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.5.1" class="ltx_text"><span id="S1.T1.16.16.5.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.16.16.6" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.6.1" class="ltx_text"><span id="S1.T1.16.16.6.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.15.15.1" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"> 346 <math id="S1.T1.15.15.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.15.15.1.m1.1a"><mo id="S1.T1.15.15.1.m1.1.1" xref="S1.T1.15.15.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.15.15.1.m1.1b"><times id="S1.T1.15.15.1.m1.1.1.cmml" xref="S1.T1.15.15.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.15.15.1.m1.1c">\times</annotation></semantics></math> 260</td>
<td id="S1.T1.16.16.7" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.7.1" class="ltx_text"><span id="S1.T1.16.16.7.1.1" class="ltx_ERROR undefined">\faCheck</span></span></td>
<td id="S1.T1.16.16.2" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.2.1" class="ltx_text"> 346 <math id="S1.T1.16.16.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.16.16.2.1.m1.1a"><mo id="S1.T1.16.16.2.1.m1.1.1" xref="S1.T1.16.16.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.16.16.2.1.m1.1b"><times id="S1.T1.16.16.2.1.m1.1.1.cmml" xref="S1.T1.16.16.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.16.16.2.1.m1.1c">\times</annotation></semantics></math> 260</span></td>
<td id="S1.T1.16.16.8" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;"><span id="S1.T1.16.16.8.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.16.16.9" class="ltx_td ltx_align_center" style="padding-bottom:-4.30554pt;padding:0.75pt 3.5pt;" rowspan="2"><span id="S1.T1.16.16.9.1" class="ltx_text">SLAM</span></td>
</tr>
<tr id="S1.T1.17.17" class="ltx_tr">
<td id="S1.T1.17.17.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">1024 <math id="S1.T1.17.17.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.17.17.1.m1.1a"><mo id="S1.T1.17.17.1.m1.1.1" xref="S1.T1.17.17.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.17.17.1.m1.1b"><times id="S1.T1.17.17.1.m1.1.1.cmml" xref="S1.T1.17.17.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.17.17.1.m1.1c">\times</annotation></semantics></math> 768</td>
<td id="S1.T1.17.17.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.17.17.2.1" class="ltx_ERROR undefined">\usym</span>2613</td>
</tr>
<tr id="S1.T1.19.19" class="ltx_tr">
<td id="S1.T1.19.19.3" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S1.T1.19.19.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.19.19.4.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.19.19.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.19.19.5.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.19.19.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.19.19.6.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.18.18.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 1440 <math id="S1.T1.18.18.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.18.18.1.m1.1a"><mo id="S1.T1.18.18.1.m1.1.1" xref="S1.T1.18.18.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.18.18.1.m1.1b"><times id="S1.T1.18.18.1.m1.1.1.cmml" xref="S1.T1.18.18.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.18.18.1.m1.1c">\times</annotation></semantics></math> 1080</td>
<td id="S1.T1.19.19.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.19.19.7.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.19.19.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"> 640 <math id="S1.T1.19.19.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.19.19.2.m1.1a"><mo id="S1.T1.19.19.2.m1.1.1" xref="S1.T1.19.19.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.19.19.2.m1.1b"><times id="S1.T1.19.19.2.m1.1.1.cmml" xref="S1.T1.19.19.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.19.19.2.m1.1c">\times</annotation></semantics></math> 480</td>
<td id="S1.T1.19.19.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.19.19.8.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.19.19.9" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Depth, Optical Flow</td>
</tr>
<tr id="S1.T1.21.21" class="ltx_tr">
<td id="S1.T1.21.21.3" class="ltx_td ltx_align_left" style="padding:0.75pt 3.5pt;">M3ED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S1.T1.21.21.4" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.21.21.4.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.21.21.5" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.21.21.5.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.21.21.6" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.21.21.6.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.20.20.1" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">1280 <math id="S1.T1.20.20.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.20.20.1.m1.1a"><mo id="S1.T1.20.20.1.m1.1.1" xref="S1.T1.20.20.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.20.20.1.m1.1b"><times id="S1.T1.20.20.1.m1.1.1.cmml" xref="S1.T1.20.20.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.20.20.1.m1.1c">\times</annotation></semantics></math> 800</td>
<td id="S1.T1.21.21.7" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;"><span id="S1.T1.21.21.7.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.21.21.2" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">1280 <math id="S1.T1.21.21.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.21.21.2.m1.1a"><mo id="S1.T1.21.21.2.m1.1.1" xref="S1.T1.21.21.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.21.21.2.m1.1b"><times id="S1.T1.21.21.2.m1.1.1.cmml" xref="S1.T1.21.21.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.21.21.2.m1.1c">\times</annotation></semantics></math> 720</td>
<td id="S1.T1.21.21.8" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.21.21.8.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S1.T1.21.21.9" class="ltx_td ltx_align_center" style="padding:0.75pt 3.5pt;">Depth, Optical Flow</td>
</tr>
<tr id="S1.T1.24.24" class="ltx_tr">
<td id="S1.T1.24.24.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;">CoSEC (Ours)</td>
<td id="S1.T1.24.24.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.24.5.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.24.24.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.24.6.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.24.24.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.24.7.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.22.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;"> 2048 <math id="S1.T1.22.22.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.22.22.1.m1.1a"><mo id="S1.T1.22.22.1.m1.1.1" xref="S1.T1.22.22.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.22.22.1.m1.1b"><times id="S1.T1.22.22.1.m1.1.1.cmml" xref="S1.T1.22.22.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.22.22.1.m1.1c">\times</annotation></semantics></math> 1536</td>
<td id="S1.T1.24.24.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;"><span id="S1.T1.24.24.8.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S1.T1.23.23.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;">1280 <math id="S1.T1.23.23.2.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.23.23.2.m1.1a"><mo id="S1.T1.23.23.2.m1.1.1" xref="S1.T1.23.23.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.23.23.2.m1.1b"><times id="S1.T1.23.23.2.m1.1.1.cmml" xref="S1.T1.23.23.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.23.23.2.m1.1c">\times</annotation></semantics></math> 720</td>
<td id="S1.T1.24.24.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;">
<span id="S1.T1.24.24.3.1" class="ltx_ERROR undefined">\faCheck</span> (1200 <math id="S1.T1.24.24.3.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S1.T1.24.24.3.m1.1a"><mo id="S1.T1.24.24.3.m1.1.1" xref="S1.T1.24.24.3.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.T1.24.24.3.m1.1b"><times id="S1.T1.24.24.3.m1.1.1.cmml" xref="S1.T1.24.24.3.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.24.24.3.m1.1c">\times</annotation></semantics></math> 624)</td>
<td id="S1.T1.24.24.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="padding:0.75pt 3.5pt;">Depth, Optical Flow</td>
</tr>
</table>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we introduce a hybrid coaxial event-frame device to build the multimodal system in Fig. <a href="#S0.F1" title="Figure 1 ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, and propose a coaxial stereo event camera (CoSEC) dataset for autonomous driving, including frame, event, LiDAR, IMU and RTK GNSS. As for the system, we realize time synchronization and spatial calibration between various modalities. During time synchronization, we utilize the microcontroller and GNSS to jointly calibrate the timestamp. During spatial calibration, we first perform intra-calibration within single coaxial device and inter-calibration between stereo coaxial devices, and then further calibrate other sensors. As for the dataset, we collect multimodal data covering all-day scenes. To generate ground truth depth and optical flow, we use SLAM algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to fuse LiDAR point clouds, and filter them with reference depth estimated by a foundation model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Moreover, we additionally fuse the aligned event-frame data to enhance the nighttime imaging performance, thus improving the accuracy of the ground truth. With the help of the coaxial device, the proposed dataset can promote all-day pixel-level multimodal fusion. Overall, our main contributions
can be summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We introduce coaxial event-frame devices to build a multimodal system, which realizes the spatiotemporal alignment between various modalities, thus promoting multimodal fusion perception.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We propose a coaxial stereo event camera dataset covering all-day scenes, which fuses the aligned event and frame data to improve the accuracy of depth and optical flow labels in nighttime conditions.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We conduct experiments to demonstrate that pixel-level aligned multimodal data can improve the performance and generalization of multimodal fusion.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">RELATED WORK</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Event camera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> has garnered increasing attention due to its advantages of high dynamic range and high temporal resolution. It has been applied in different autonomous driving datasets in Table <a href="#S1.T1" title="TABLE I ‣ I INTRODUCTION ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>, including event-only unimodal, event-frame multimodal and stereo event-frame multimodal datasets. In this section, we will describe the development of these event-based datasets in detail.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Event-only Unimodal Datasets</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">Event-only unimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> just utilize the event camera to perform relatively simple vision tasks in autonomous driving. For example, N-CARS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and ADD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> datasets took Gen1 event camera with 304<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mo id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><times id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\times</annotation></semantics></math>240 resolution to classify and detect objects. 1MP Detection dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> further introduced EVK4 event camera with a higher resolution of 1280<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mo id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><times id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\times</annotation></semantics></math>720 for object detection. DET dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> focused on lane extraction with an event camera. However, these unimodal datasets are not sufficiently suitable for the challenging task of motion perception, which is the key to understanding the dynamic scene for autonomous driving. Therefore, our dataset focuses on motion perception in driving scenarios, such as optical flow.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.08500/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Difference between different event-frame placement strategies. Left: The large baseline leads to the large disparity, resulting in the misalignment between event and frame camera. Middle: The small baseline brings in a small but non-negligible disparity. Right: Baseline minimization can reduce the disparity. Therefore, we introduce a coaxial strategy to relieve the spatial alignment error between event and frame camera.
</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Event-Frame Multimodal Datasets</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Event-frame multimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> integrate frame camera and sensors of vehicle control (<em id="S2.SS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g.</em>, IMU ) to fuse cross-modal complementary knowledge for motion perception in autonomous driving. For example, DDD17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and DDD20 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> datasets fused appearance information of road direction from event and frame data to predict all-day vehicle steering motion. Brisbane-Event-VPR dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> maximizes the similarity between features extracted from event and frame data for dynamic scene understanding, such as place recognition for autonomous driving in challenging lighting conditions. These datasets demonstrate the advantages of multimodal fusion for robust motion perception under various illumination. However, these datasets are inadequate for 3D scene perception in autonomous driving due to the absence of distance sensors, such as LiDAR. In this work, we further focus on the multimodal dataset for motion perception of the 3D dynamic scene in autonomous driving.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Stereo Event-Frame Multimodal Datasets</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Stereo event-frame multimodal datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> improve 3D dynamic scene motion perception using the distance sensors. For example, the MVSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and FusionPortableV2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> datasets introduced the LiDAR and stereo DAVIS event camera <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to fuse the multimodal knowledge for 3D perception, such as depth estimation and SLAM. The DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and M3ED <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> datasets further utilized the Prophesee event camera with a higher resolution for more fine-grained stereo matching in driving scenarios. Nevertheless, these datasets commonly place event and frame cameras in parallel, with a small but non-negligible baseline between them as shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Event-only Unimodal Datasets ‣ II RELATED WORK ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This parallel strategy introduces a disparity that cannot be ignored, making it impossible to ensure accurate spatial alignment between event and frame cameras. Therefore, our CoSEC dataset introduces the beam splitter to minimize the baseline between event and frame cameras, building a coaxial event-frame device to collect spatiotemporal-aligned event-frame data with a resolution of 1200<math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mo id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><times id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\times</annotation></semantics></math>624.</p>
</div>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2408.08500/assets/x3.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="323" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Comparison between parallel and coaxial strategies. Parallel placement strategy brings in spatial alignment error between the event and frame data in local regions. In contrast, we introduce a coaxial strategy to improve the pixel-level spatial alignment between the event and frame data.
</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">MOTIVATION OF COAXIAL DEVICE</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The prerequisite for multimodal fusion is the pixel-level spatial alignment between different modalities. Existing multimodal datasets mainly place frame and event cameras in parallel, and directly spatially align the two cameras via warping operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. However, this parallel strategy cannot guarantee the accurate spatial alignment between various modalities, which is less effective for multimodal fusion. As shown in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-A Event-only Unimodal Datasets ‣ II RELATED WORK ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we explore the impact of different baselines on spatial alignment between event and frame cameras. We can observe that, the large baseline brings in the large disparity, which is indeed beneficial for scene measurement of stereo cameras, but restricts the cross-modal spatial alignment due to the increased pixel shift of the corresponding points. Conversely, the smaller the baseline, the smaller the disparity and the higher the degree of pixel-level spatial alignment between event and frame cameras.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Motivated by this, we minimize the event-frame baseline using the coaxial strategy, which uses a beam splitter to ensure that the event and frame cameras share the same optical axis for physically reducing the spatial alignment error. Moreover, we further compare the alignment performance of the parallel strategy from DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> dataset and our coaxial strategy in Fig. <a href="#S2.F3" title="Figure 3 ‣ II-C Stereo Event-Frame Multimodal Datasets ‣ II RELATED WORK ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, where we use the warping operation similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. We can observe that, there exists obvious pixel-level misalignment between event data and frame data aligned through the parallel strategy, while the coaxial strategy can significantly improve the accuracy of global alignment. In this work, we will introduce a coaxial event-frame device to build the multimodal system for pixel-level spatial alignment.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">MULTIMODAL SYSTEM</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Different sensors of the multimodal system have their own specific imaging characteristics, and it is difficult to directly obtain all modal data in the same scene. How to ensure the pixel-level alignment of different sensors in the temporal and spatial dimensions is the key to the multimodal system.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Sensors and Time Synchronization</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p"><span id="S4.SS1.p1.2.1" class="ltx_text ltx_font_bold">Sensors.</span> We first design the 3D structure of the multimodal system consisting of several sensors in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>, including stereo coaxial devices, one LiDAR and one integrated navigation system (INS). As for the stereo coaxial devices, we first choose the Prophesee EVK4 event camera with a resolution of 1280 <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mo id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><times id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\times</annotation></semantics></math> 720 and the FLIR Blackfly S USB3 color frame camera with a resolution of 2048 <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mo id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><times id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\times</annotation></semantics></math> 1536 to construct the single coaxial device, which share the same optical axis via a beam splitter for the physical spatial alignment. Then, we build the stereo camera system with the two coaxial devices. As for the LiDAR, we choose the Velodyne VLP-32C to generate the point clouds for measuring the 3D scene distance. As for the INS, we choose the InertialLabs IMU to measure the angular velocity and acceleration of ego-motion, and the Novatel OEM7 GNSS receiver to measure the latitude and longitude position. LiDAR point clouds and INS data can be fused to generate depth and optical flow labels for autonomous driving.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Sensors of the multimodal system</figcaption>
<table id="S4.T2.9" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.9.10" class="ltx_tr">
<td id="S4.T2.9.10.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.9.10.1.1" class="ltx_text ltx_font_bold">Sensor</span></td>
<td id="S4.T2.9.10.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T2.9.10.2.1" class="ltx_text ltx_font_bold">Characteristics</span></td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T2.1.1.2.1" class="ltx_text ltx_font_bold">Prophesee EVK4-HD</span></td>
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_t">Latency: 220 <math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\mathrm{\SIUnitSymbolMicro s}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mrow class="ltx_unit" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.cmml">µ</mi><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">​</mo><mi mathvariant="normal" id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><csymbol cd="latexml" id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">microsecond</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\mathrm{\SIUnitSymbolMicro s}</annotation></semantics></math>
</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_left">Resolution: 1280 <math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mo id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><times id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\times</annotation></semantics></math> 720</td>
</tr>
<tr id="S4.T2.9.11" class="ltx_tr">
<td id="S4.T2.9.11.1" class="ltx_td ltx_align_left">Dynamic Range: &gt;120 dB</td>
</tr>
<tr id="S4.T2.9.12" class="ltx_tr">
<td id="S4.T2.9.12.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="3"><span id="S4.T2.9.12.1.1" class="ltx_text ltx_font_bold">FLIR BFS-U3-32S4C</span></td>
<td id="S4.T2.9.12.2" class="ltx_td ltx_align_left ltx_border_t">Chroma: Color</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_left">Resolution: 2048 <math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><mo id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><times id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\times</annotation></semantics></math> 1536</td>
</tr>
<tr id="S4.T2.9.13" class="ltx_tr">
<td id="S4.T2.9.13.1" class="ltx_td ltx_align_left">Dynamic Range: 71.62 dB</td>
</tr>
<tr id="S4.T2.9.14" class="ltx_tr">
<td id="S4.T2.9.14.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T2.9.14.1.1" class="ltx_text ltx_font_bold">Velodyne VLP-32C</span></td>
<td id="S4.T2.9.14.2" class="ltx_td ltx_align_left ltx_border_t">Channels: 32</td>
</tr>
<tr id="S4.T2.9.15" class="ltx_tr">
<td id="S4.T2.9.15.1" class="ltx_td ltx_align_left">Measurement Range: 200 m</td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_left">Range Accuracy: <math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mo id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><csymbol cd="latexml" id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">\pm</annotation></semantics></math>3 cm</td>
</tr>
<tr id="S4.T2.5.5" class="ltx_tr">
<td id="S4.T2.5.5.1" class="ltx_td ltx_align_left">Horizontal Field of View: 360<sup id="S4.T2.5.5.1.1" class="ltx_sup">∘</sup>
</td>
</tr>
<tr id="S4.T2.8.8" class="ltx_tr">
<td id="S4.T2.8.8.3" class="ltx_td ltx_align_left">Vertical Field of View: 40<sup id="S4.T2.8.8.3.1" class="ltx_sup">∘</sup> (-25<sup id="S4.T2.8.8.3.2" class="ltx_sup">∘</sup> to +15<sup id="S4.T2.8.8.3.3" class="ltx_sup">∘</sup>)</td>
</tr>
<tr id="S4.T2.9.16" class="ltx_tr">
<td id="S4.T2.9.16.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" rowspan="3"><span id="S4.T2.9.16.1.1" class="ltx_text ltx_font_bold">InertialLabs INS-D-E1</span></td>
<td id="S4.T2.9.16.2" class="ltx_td ltx_align_left ltx_border_t">9-Axis IMU</td>
</tr>
<tr id="S4.T2.9.17" class="ltx_tr">
<td id="S4.T2.9.17.1" class="ltx_td ltx_align_left">Novatel OEM7 GNSS receiver</td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.1" class="ltx_td ltx_align_left ltx_border_bb">Position Accuracy (RTK): <math id="S4.T2.9.9.1.m1.1" class="ltx_Math" alttext="\pm" display="inline"><semantics id="S4.T2.9.9.1.m1.1a"><mo id="S4.T2.9.9.1.m1.1.1" xref="S4.T2.9.9.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.1.m1.1b"><csymbol cd="latexml" id="S4.T2.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.1.m1.1c">\pm</annotation></semantics></math>2 cm</td>
</tr>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Time Synchronization.</span> Since all the sensors have different data acquisition frequency, it is necessary to achieve time synchronization, which ensures that all the sensors capture information from the same scene at the same time. Time synchronization is divided into two steps, including external trigger and absolute time calibration. During external trigger stage, we utilize microcontroller to generate four synchronous pulses, where two are the pulses with high frequency (<em id="S4.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, 1M Hz) for event cameras and two are the pulses with low frequency (<em id="S4.SS1.p2.1.3" class="ltx_emph ltx_font_italic">e.g.</em>, 30 Hz) for frame cameras, thus achieving time synchronization between event and frame cameras. During absolute time calibration stage, we use the microcontroller to receive the pps signals from the GNSS receiver, which is further parsed into absolute timestamps for LiDAR, IMU and cameras. In this way, all the sensors work under GNSS timestamps, achieving time synchronization.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2408.08500/assets/x4.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="461" height="618" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
Calibration of the stereo coaxial devices. We perform intra-calibration within the single coaxial device and inter-calibration between stereo coaxial devices. During intra-calibration, we first reconstruct events into event frames for standard calibration, and then align the event and image data via warping operation. During inter-calibration, we further take stereo rectification to obtain the paired rectified event-image data.
</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2408.08500/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Pipeline of ground truth generation. We first fuse single clouds within a time window into a local cloud via SLAM, and then project the local fused cloud into the camera coordinate system for coarse depth. Next, we design an outlier removal module, which estimates reference depth from the input image to filter the coarse depth for ground truth depth and optical flow. In addition, we introduce an event-frame fusion strategy to enhance the nighttime low-light image for achieving better reference depth, thus improving the accuracy of ground truth generation.
</figcaption>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>The overview of the CoSEC dataset.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.1.1" class="ltx_text ltx_font_bold">Split</span></td>
<td id="S4.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.2.1" class="ltx_text ltx_font_bold">Time</span></td>
<td id="S4.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T3.1.1.3.1" class="ltx_text ltx_font_bold">Area</span></td>
<td id="S4.T3.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T3.1.1.4.1" class="ltx_text ltx_font_bold">Sequences</span></td>
<td id="S4.T3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S4.T3.1.1.5.1" class="ltx_text ltx_font_bold">Duration (s)</span></td>
</tr>
<tr id="S4.T3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.2.1.1" class="ltx_text ltx_font_bold">Train</span></td>
<td id="S4.T3.1.2.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.2.2.1" class="ltx_text">Day</span></td>
<td id="S4.T3.1.2.3" class="ltx_td ltx_align_center ltx_border_t">City</td>
<td id="S4.T3.1.2.4" class="ltx_td ltx_align_center ltx_border_t">11</td>
<td id="S4.T3.1.2.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.2.5.1" class="ltx_text">58</span></td>
<td id="S4.T3.1.2.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.2.6.1" class="ltx_text ltx_font_bold">102</span></td>
<td id="S4.T3.1.2.7" class="ltx_td ltx_align_center ltx_border_t">320</td>
<td id="S4.T3.1.2.8" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.2.8.1" class="ltx_text">1652</span></td>
<td id="S4.T3.1.2.9" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.2.9.1" class="ltx_text ltx_font_bold">2905</span></td>
</tr>
<tr id="S4.T3.1.3" class="ltx_tr">
<td id="S4.T3.1.3.1" class="ltx_td ltx_align_center">Campus</td>
<td id="S4.T3.1.3.2" class="ltx_td ltx_align_center">12</td>
<td id="S4.T3.1.3.3" class="ltx_td ltx_align_center">338</td>
</tr>
<tr id="S4.T3.1.4" class="ltx_tr">
<td id="S4.T3.1.4.1" class="ltx_td ltx_align_center">Park</td>
<td id="S4.T3.1.4.2" class="ltx_td ltx_align_center">11</td>
<td id="S4.T3.1.4.3" class="ltx_td ltx_align_center">314</td>
</tr>
<tr id="S4.T3.1.5" class="ltx_tr">
<td id="S4.T3.1.5.1" class="ltx_td ltx_align_center">Suburbs</td>
<td id="S4.T3.1.5.2" class="ltx_td ltx_align_center">15</td>
<td id="S4.T3.1.5.3" class="ltx_td ltx_align_center">411</td>
</tr>
<tr id="S4.T3.1.6" class="ltx_tr">
<td id="S4.T3.1.6.1" class="ltx_td ltx_align_center">Village</td>
<td id="S4.T3.1.6.2" class="ltx_td ltx_align_center">9</td>
<td id="S4.T3.1.6.3" class="ltx_td ltx_align_center">269</td>
</tr>
<tr id="S4.T3.1.7" class="ltx_tr">
<td id="S4.T3.1.7.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.7.1.1" class="ltx_text">Night</span></td>
<td id="S4.T3.1.7.2" class="ltx_td ltx_align_center ltx_border_t">City</td>
<td id="S4.T3.1.7.3" class="ltx_td ltx_align_center ltx_border_t">9</td>
<td id="S4.T3.1.7.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.7.4.1" class="ltx_text">44</span></td>
<td id="S4.T3.1.7.5" class="ltx_td ltx_align_center ltx_border_t">253</td>
<td id="S4.T3.1.7.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.7.6.1" class="ltx_text">1253</span></td>
</tr>
<tr id="S4.T3.1.8" class="ltx_tr">
<td id="S4.T3.1.8.1" class="ltx_td ltx_align_center">Campus</td>
<td id="S4.T3.1.8.2" class="ltx_td ltx_align_center">10</td>
<td id="S4.T3.1.8.3" class="ltx_td ltx_align_center">287</td>
</tr>
<tr id="S4.T3.1.9" class="ltx_tr">
<td id="S4.T3.1.9.1" class="ltx_td ltx_align_center">Park</td>
<td id="S4.T3.1.9.2" class="ltx_td ltx_align_center">8</td>
<td id="S4.T3.1.9.3" class="ltx_td ltx_align_center">231</td>
</tr>
<tr id="S4.T3.1.10" class="ltx_tr">
<td id="S4.T3.1.10.1" class="ltx_td ltx_align_center">Suburbs</td>
<td id="S4.T3.1.10.2" class="ltx_td ltx_align_center">12</td>
<td id="S4.T3.1.10.3" class="ltx_td ltx_align_center">334</td>
</tr>
<tr id="S4.T3.1.11" class="ltx_tr">
<td id="S4.T3.1.11.1" class="ltx_td ltx_align_center">Village</td>
<td id="S4.T3.1.11.2" class="ltx_td ltx_align_center">5</td>
<td id="S4.T3.1.11.3" class="ltx_td ltx_align_center">148</td>
</tr>
<tr id="S4.T3.1.12" class="ltx_tr">
<td id="S4.T3.1.12.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.12.1.1" class="ltx_text ltx_font_bold">Test</span></td>
<td id="S4.T3.1.12.2" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.12.2.1" class="ltx_text">Day</span></td>
<td id="S4.T3.1.12.3" class="ltx_td ltx_align_center ltx_border_t">City</td>
<td id="S4.T3.1.12.4" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T3.1.12.5" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.12.5.1" class="ltx_text">15</span></td>
<td id="S4.T3.1.12.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.12.6.1" class="ltx_text ltx_font_bold">26</span></td>
<td id="S4.T3.1.12.7" class="ltx_td ltx_align_center ltx_border_t">58</td>
<td id="S4.T3.1.12.8" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.12.8.1" class="ltx_text">431</span></td>
<td id="S4.T3.1.12.9" class="ltx_td ltx_align_center ltx_border_t" rowspan="10"><span id="S4.T3.1.12.9.1" class="ltx_text ltx_font_bold">753</span></td>
</tr>
<tr id="S4.T3.1.13" class="ltx_tr">
<td id="S4.T3.1.13.1" class="ltx_td ltx_align_center">Campus</td>
<td id="S4.T3.1.13.2" class="ltx_td ltx_align_center">3</td>
<td id="S4.T3.1.13.3" class="ltx_td ltx_align_center">94</td>
</tr>
<tr id="S4.T3.1.14" class="ltx_tr">
<td id="S4.T3.1.14.1" class="ltx_td ltx_align_center">Park</td>
<td id="S4.T3.1.14.2" class="ltx_td ltx_align_center">3</td>
<td id="S4.T3.1.14.3" class="ltx_td ltx_align_center">84</td>
</tr>
<tr id="S4.T3.1.15" class="ltx_tr">
<td id="S4.T3.1.15.1" class="ltx_td ltx_align_center">Suburbs</td>
<td id="S4.T3.1.15.2" class="ltx_td ltx_align_center">5</td>
<td id="S4.T3.1.15.3" class="ltx_td ltx_align_center">138</td>
</tr>
<tr id="S4.T3.1.16" class="ltx_tr">
<td id="S4.T3.1.16.1" class="ltx_td ltx_align_center">Village</td>
<td id="S4.T3.1.16.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T3.1.16.3" class="ltx_td ltx_align_center">57</td>
</tr>
<tr id="S4.T3.1.17" class="ltx_tr">
<td id="S4.T3.1.17.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.17.1.1" class="ltx_text">Night</span></td>
<td id="S4.T3.1.17.2" class="ltx_td ltx_align_center ltx_border_t">City</td>
<td id="S4.T3.1.17.3" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T3.1.17.4" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.17.4.1" class="ltx_text">11</span></td>
<td id="S4.T3.1.17.5" class="ltx_td ltx_align_center ltx_border_t">61</td>
<td id="S4.T3.1.17.6" class="ltx_td ltx_align_center ltx_border_t" rowspan="5"><span id="S4.T3.1.17.6.1" class="ltx_text">322</span></td>
</tr>
<tr id="S4.T3.1.18" class="ltx_tr">
<td id="S4.T3.1.18.1" class="ltx_td ltx_align_center">Campus</td>
<td id="S4.T3.1.18.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T3.1.18.3" class="ltx_td ltx_align_center">66</td>
</tr>
<tr id="S4.T3.1.19" class="ltx_tr">
<td id="S4.T3.1.19.1" class="ltx_td ltx_align_center">Park</td>
<td id="S4.T3.1.19.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T3.1.19.3" class="ltx_td ltx_align_center">56</td>
</tr>
<tr id="S4.T3.1.20" class="ltx_tr">
<td id="S4.T3.1.20.1" class="ltx_td ltx_align_center">Suburbs</td>
<td id="S4.T3.1.20.2" class="ltx_td ltx_align_center">3</td>
<td id="S4.T3.1.20.3" class="ltx_td ltx_align_center">85</td>
</tr>
<tr id="S4.T3.1.21" class="ltx_tr">
<td id="S4.T3.1.21.1" class="ltx_td ltx_align_center">Village</td>
<td id="S4.T3.1.21.2" class="ltx_td ltx_align_center">2</td>
<td id="S4.T3.1.21.3" class="ltx_td ltx_align_center">54</td>
</tr>
<tr id="S4.T3.1.22" class="ltx_tr">
<td id="S4.T3.1.22.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.22.1.1" class="ltx_text ltx_font_bold">Total</span></td>
<td id="S4.T3.1.22.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.3" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.4" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.5" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.22.6.1" class="ltx_text ltx_font_bold">128</span></td>
<td id="S4.T3.1.22.7" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.8" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S4.T3.1.22.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T3.1.22.9.1" class="ltx_text ltx_font_bold">3658</span></td>
</tr>
</table>
</figure>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Spatial Calibration</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_bold">Intra-Calibration within Coaxial Device.</span> Since event camera and frame camera have different spatial resolutions, we need to calibrate the two camera into the same imaging coordinate system within the single coaxial device, which guarantees the pixel-level spatial alignment. As shown in Fig. <a href="#S4.F4" title="Figure 4 ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, first, we simultaneously capture event stream and color frames for the same checkerboard pattern. Then, considering that event data cannot be directly used for the standard calibration, we reconstruct the event stream into the event frame via E2VID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Finally, we take Kalibr toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> to calibrate the event and frame cameras using the color image and the reconstructed event frame, thus obtaining the intrinsics and extrinsics of the two cameras. Note that these calibration parameters can be used to generate aligned event-frame data by using the warping operation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Inter-Calibration between Coaxial Devices.</span> Stereo coaxial devices consist of two coaxial event-frame devices, similar to the stereo camera system, which could be further performed inter-calibration between the two coaxial devices to obtain the paired rectified event-frame data for stereo matching. First, we calibrate the stereo frame cameras, achieving rectified stereo images with the original resolution (2048 <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mo id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><times id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\times</annotation></semantics></math> 1536). Next, the calibrated extrinsics between the two frame cameras are used to compute the extrinsics of the stereo coaxial devices as follows:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.1" class="ltx_Math" alttext="T_{oRL}=T_{LL_{E}}T_{RL}T_{RR_{E}}^{-1}" display="block"><semantics id="S4.E1.m1.1a"><mrow id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml"><msub id="S4.E1.m1.1.1.2" xref="S4.E1.m1.1.1.2.cmml"><mi id="S4.E1.m1.1.1.2.2" xref="S4.E1.m1.1.1.2.2.cmml">T</mi><mrow id="S4.E1.m1.1.1.2.3" xref="S4.E1.m1.1.1.2.3.cmml"><mi id="S4.E1.m1.1.1.2.3.2" xref="S4.E1.m1.1.1.2.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.3.1" xref="S4.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.3.3" xref="S4.E1.m1.1.1.2.3.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.2.3.1a" xref="S4.E1.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.2.3.4" xref="S4.E1.m1.1.1.2.3.4.cmml">L</mi></mrow></msub><mo id="S4.E1.m1.1.1.1" xref="S4.E1.m1.1.1.1.cmml">=</mo><mrow id="S4.E1.m1.1.1.3" xref="S4.E1.m1.1.1.3.cmml"><msub id="S4.E1.m1.1.1.3.2" xref="S4.E1.m1.1.1.3.2.cmml"><mi id="S4.E1.m1.1.1.3.2.2" xref="S4.E1.m1.1.1.3.2.2.cmml">T</mi><mrow id="S4.E1.m1.1.1.3.2.3" xref="S4.E1.m1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.2" xref="S4.E1.m1.1.1.3.2.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.2.3.1" xref="S4.E1.m1.1.1.3.2.3.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.2.3.3" xref="S4.E1.m1.1.1.3.2.3.3.cmml"><mi id="S4.E1.m1.1.1.3.2.3.3.2" xref="S4.E1.m1.1.1.3.2.3.3.2.cmml">L</mi><mi id="S4.E1.m1.1.1.3.2.3.3.3" xref="S4.E1.m1.1.1.3.2.3.3.3.cmml">E</mi></msub></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1" xref="S4.E1.m1.1.1.3.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.3" xref="S4.E1.m1.1.1.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.2" xref="S4.E1.m1.1.1.3.3.2.cmml">T</mi><mrow id="S4.E1.m1.1.1.3.3.3" xref="S4.E1.m1.1.1.3.3.3.cmml"><mi id="S4.E1.m1.1.1.3.3.3.2" xref="S4.E1.m1.1.1.3.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.3.3.1" xref="S4.E1.m1.1.1.3.3.3.1.cmml">​</mo><mi id="S4.E1.m1.1.1.3.3.3.3" xref="S4.E1.m1.1.1.3.3.3.3.cmml">L</mi></mrow></msub><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.1a" xref="S4.E1.m1.1.1.3.1.cmml">​</mo><msubsup id="S4.E1.m1.1.1.3.4" xref="S4.E1.m1.1.1.3.4.cmml"><mi id="S4.E1.m1.1.1.3.4.2.2" xref="S4.E1.m1.1.1.3.4.2.2.cmml">T</mi><mrow id="S4.E1.m1.1.1.3.4.2.3" xref="S4.E1.m1.1.1.3.4.2.3.cmml"><mi id="S4.E1.m1.1.1.3.4.2.3.2" xref="S4.E1.m1.1.1.3.4.2.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.1.1.3.4.2.3.1" xref="S4.E1.m1.1.1.3.4.2.3.1.cmml">​</mo><msub id="S4.E1.m1.1.1.3.4.2.3.3" xref="S4.E1.m1.1.1.3.4.2.3.3.cmml"><mi id="S4.E1.m1.1.1.3.4.2.3.3.2" xref="S4.E1.m1.1.1.3.4.2.3.3.2.cmml">R</mi><mi id="S4.E1.m1.1.1.3.4.2.3.3.3" xref="S4.E1.m1.1.1.3.4.2.3.3.3.cmml">E</mi></msub></mrow><mrow id="S4.E1.m1.1.1.3.4.3" xref="S4.E1.m1.1.1.3.4.3.cmml"><mo id="S4.E1.m1.1.1.3.4.3a" xref="S4.E1.m1.1.1.3.4.3.cmml">−</mo><mn id="S4.E1.m1.1.1.3.4.3.2" xref="S4.E1.m1.1.1.3.4.3.2.cmml">1</mn></mrow></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.1b"><apply id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1"><eq id="S4.E1.m1.1.1.1.cmml" xref="S4.E1.m1.1.1.1"></eq><apply id="S4.E1.m1.1.1.2.cmml" xref="S4.E1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.2.2">𝑇</ci><apply id="S4.E1.m1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.2.3"><times id="S4.E1.m1.1.1.2.3.1.cmml" xref="S4.E1.m1.1.1.2.3.1"></times><ci id="S4.E1.m1.1.1.2.3.2.cmml" xref="S4.E1.m1.1.1.2.3.2">𝑜</ci><ci id="S4.E1.m1.1.1.2.3.3.cmml" xref="S4.E1.m1.1.1.2.3.3">𝑅</ci><ci id="S4.E1.m1.1.1.2.3.4.cmml" xref="S4.E1.m1.1.1.2.3.4">𝐿</ci></apply></apply><apply id="S4.E1.m1.1.1.3.cmml" xref="S4.E1.m1.1.1.3"><times id="S4.E1.m1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.3.1"></times><apply id="S4.E1.m1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.3.2">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.3.2.2">𝑇</ci><apply id="S4.E1.m1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.3.2.3"><times id="S4.E1.m1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.2">𝐿</ci><apply id="S4.E1.m1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.2.3.3.1.cmml" xref="S4.E1.m1.1.1.3.2.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.2.3.3.2.cmml" xref="S4.E1.m1.1.1.3.2.3.3.2">𝐿</ci><ci id="S4.E1.m1.1.1.3.2.3.3.3.cmml" xref="S4.E1.m1.1.1.3.2.3.3.3">𝐸</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.2">𝑇</ci><apply id="S4.E1.m1.1.1.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3"><times id="S4.E1.m1.1.1.3.3.3.1.cmml" xref="S4.E1.m1.1.1.3.3.3.1"></times><ci id="S4.E1.m1.1.1.3.3.3.2.cmml" xref="S4.E1.m1.1.1.3.3.3.2">𝑅</ci><ci id="S4.E1.m1.1.1.3.3.3.3.cmml" xref="S4.E1.m1.1.1.3.3.3.3">𝐿</ci></apply></apply><apply id="S4.E1.m1.1.1.3.4.cmml" xref="S4.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.4.1.cmml" xref="S4.E1.m1.1.1.3.4">superscript</csymbol><apply id="S4.E1.m1.1.1.3.4.2.cmml" xref="S4.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.4.2.1.cmml" xref="S4.E1.m1.1.1.3.4">subscript</csymbol><ci id="S4.E1.m1.1.1.3.4.2.2.cmml" xref="S4.E1.m1.1.1.3.4.2.2">𝑇</ci><apply id="S4.E1.m1.1.1.3.4.2.3.cmml" xref="S4.E1.m1.1.1.3.4.2.3"><times id="S4.E1.m1.1.1.3.4.2.3.1.cmml" xref="S4.E1.m1.1.1.3.4.2.3.1"></times><ci id="S4.E1.m1.1.1.3.4.2.3.2.cmml" xref="S4.E1.m1.1.1.3.4.2.3.2">𝑅</ci><apply id="S4.E1.m1.1.1.3.4.2.3.3.cmml" xref="S4.E1.m1.1.1.3.4.2.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.3.4.2.3.3.1.cmml" xref="S4.E1.m1.1.1.3.4.2.3.3">subscript</csymbol><ci id="S4.E1.m1.1.1.3.4.2.3.3.2.cmml" xref="S4.E1.m1.1.1.3.4.2.3.3.2">𝑅</ci><ci id="S4.E1.m1.1.1.3.4.2.3.3.3.cmml" xref="S4.E1.m1.1.1.3.4.2.3.3.3">𝐸</ci></apply></apply></apply><apply id="S4.E1.m1.1.1.3.4.3.cmml" xref="S4.E1.m1.1.1.3.4.3"><minus id="S4.E1.m1.1.1.3.4.3.1.cmml" xref="S4.E1.m1.1.1.3.4.3"></minus><cn type="integer" id="S4.E1.m1.1.1.3.4.3.2.cmml" xref="S4.E1.m1.1.1.3.4.3.2">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.1c">T_{oRL}=T_{LL_{E}}T_{RL}T_{RR_{E}}^{-1}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS2.p2.10" class="ltx_p">where <math id="S4.SS2.p2.2.m1.1" class="ltx_Math" alttext="T_{oRL}" display="inline"><semantics id="S4.SS2.p2.2.m1.1a"><msub id="S4.SS2.p2.2.m1.1.1" xref="S4.SS2.p2.2.m1.1.1.cmml"><mi id="S4.SS2.p2.2.m1.1.1.2" xref="S4.SS2.p2.2.m1.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.2.m1.1.1.3" xref="S4.SS2.p2.2.m1.1.1.3.cmml"><mi id="S4.SS2.p2.2.m1.1.1.3.2" xref="S4.SS2.p2.2.m1.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m1.1.1.3.1" xref="S4.SS2.p2.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.2.m1.1.1.3.3" xref="S4.SS2.p2.2.m1.1.1.3.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.2.m1.1.1.3.1a" xref="S4.SS2.p2.2.m1.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.2.m1.1.1.3.4" xref="S4.SS2.p2.2.m1.1.1.3.4.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m1.1b"><apply id="S4.SS2.p2.2.m1.1.1.cmml" xref="S4.SS2.p2.2.m1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.2.m1.1.1.1.cmml" xref="S4.SS2.p2.2.m1.1.1">subscript</csymbol><ci id="S4.SS2.p2.2.m1.1.1.2.cmml" xref="S4.SS2.p2.2.m1.1.1.2">𝑇</ci><apply id="S4.SS2.p2.2.m1.1.1.3.cmml" xref="S4.SS2.p2.2.m1.1.1.3"><times id="S4.SS2.p2.2.m1.1.1.3.1.cmml" xref="S4.SS2.p2.2.m1.1.1.3.1"></times><ci id="S4.SS2.p2.2.m1.1.1.3.2.cmml" xref="S4.SS2.p2.2.m1.1.1.3.2">𝑜</ci><ci id="S4.SS2.p2.2.m1.1.1.3.3.cmml" xref="S4.SS2.p2.2.m1.1.1.3.3">𝑅</ci><ci id="S4.SS2.p2.2.m1.1.1.3.4.cmml" xref="S4.SS2.p2.2.m1.1.1.3.4">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m1.1c">T_{oRL}</annotation></semantics></math> is the transformation matrix from right to left coaxial device, and <math id="S4.SS2.p2.3.m2.1" class="ltx_Math" alttext="T_{RL}" display="inline"><semantics id="S4.SS2.p2.3.m2.1a"><msub id="S4.SS2.p2.3.m2.1.1" xref="S4.SS2.p2.3.m2.1.1.cmml"><mi id="S4.SS2.p2.3.m2.1.1.2" xref="S4.SS2.p2.3.m2.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.3.m2.1.1.3" xref="S4.SS2.p2.3.m2.1.1.3.cmml"><mi id="S4.SS2.p2.3.m2.1.1.3.2" xref="S4.SS2.p2.3.m2.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.3.m2.1.1.3.1" xref="S4.SS2.p2.3.m2.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.3.m2.1.1.3.3" xref="S4.SS2.p2.3.m2.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m2.1b"><apply id="S4.SS2.p2.3.m2.1.1.cmml" xref="S4.SS2.p2.3.m2.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.3.m2.1.1.1.cmml" xref="S4.SS2.p2.3.m2.1.1">subscript</csymbol><ci id="S4.SS2.p2.3.m2.1.1.2.cmml" xref="S4.SS2.p2.3.m2.1.1.2">𝑇</ci><apply id="S4.SS2.p2.3.m2.1.1.3.cmml" xref="S4.SS2.p2.3.m2.1.1.3"><times id="S4.SS2.p2.3.m2.1.1.3.1.cmml" xref="S4.SS2.p2.3.m2.1.1.3.1"></times><ci id="S4.SS2.p2.3.m2.1.1.3.2.cmml" xref="S4.SS2.p2.3.m2.1.1.3.2">𝑅</ci><ci id="S4.SS2.p2.3.m2.1.1.3.3.cmml" xref="S4.SS2.p2.3.m2.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m2.1c">T_{RL}</annotation></semantics></math> is the transformation matrix from right to left frame camera. <math id="S4.SS2.p2.4.m3.1" class="ltx_Math" alttext="T_{LL_{E}}" display="inline"><semantics id="S4.SS2.p2.4.m3.1a"><msub id="S4.SS2.p2.4.m3.1.1" xref="S4.SS2.p2.4.m3.1.1.cmml"><mi id="S4.SS2.p2.4.m3.1.1.2" xref="S4.SS2.p2.4.m3.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.4.m3.1.1.3" xref="S4.SS2.p2.4.m3.1.1.3.cmml"><mi id="S4.SS2.p2.4.m3.1.1.3.2" xref="S4.SS2.p2.4.m3.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m3.1.1.3.1" xref="S4.SS2.p2.4.m3.1.1.3.1.cmml">​</mo><msub id="S4.SS2.p2.4.m3.1.1.3.3" xref="S4.SS2.p2.4.m3.1.1.3.3.cmml"><mi id="S4.SS2.p2.4.m3.1.1.3.3.2" xref="S4.SS2.p2.4.m3.1.1.3.3.2.cmml">L</mi><mi id="S4.SS2.p2.4.m3.1.1.3.3.3" xref="S4.SS2.p2.4.m3.1.1.3.3.3.cmml">E</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m3.1b"><apply id="S4.SS2.p2.4.m3.1.1.cmml" xref="S4.SS2.p2.4.m3.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m3.1.1.1.cmml" xref="S4.SS2.p2.4.m3.1.1">subscript</csymbol><ci id="S4.SS2.p2.4.m3.1.1.2.cmml" xref="S4.SS2.p2.4.m3.1.1.2">𝑇</ci><apply id="S4.SS2.p2.4.m3.1.1.3.cmml" xref="S4.SS2.p2.4.m3.1.1.3"><times id="S4.SS2.p2.4.m3.1.1.3.1.cmml" xref="S4.SS2.p2.4.m3.1.1.3.1"></times><ci id="S4.SS2.p2.4.m3.1.1.3.2.cmml" xref="S4.SS2.p2.4.m3.1.1.3.2">𝐿</ci><apply id="S4.SS2.p2.4.m3.1.1.3.3.cmml" xref="S4.SS2.p2.4.m3.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m3.1.1.3.3.1.cmml" xref="S4.SS2.p2.4.m3.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p2.4.m3.1.1.3.3.2.cmml" xref="S4.SS2.p2.4.m3.1.1.3.3.2">𝐿</ci><ci id="S4.SS2.p2.4.m3.1.1.3.3.3.cmml" xref="S4.SS2.p2.4.m3.1.1.3.3.3">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m3.1c">T_{LL_{E}}</annotation></semantics></math> is the transformation matrix from frame to event camera within the left coaxial device, and <math id="S4.SS2.p2.5.m4.1" class="ltx_Math" alttext="T_{RR_{E}}" display="inline"><semantics id="S4.SS2.p2.5.m4.1a"><msub id="S4.SS2.p2.5.m4.1.1" xref="S4.SS2.p2.5.m4.1.1.cmml"><mi id="S4.SS2.p2.5.m4.1.1.2" xref="S4.SS2.p2.5.m4.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.5.m4.1.1.3" xref="S4.SS2.p2.5.m4.1.1.3.cmml"><mi id="S4.SS2.p2.5.m4.1.1.3.2" xref="S4.SS2.p2.5.m4.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m4.1.1.3.1" xref="S4.SS2.p2.5.m4.1.1.3.1.cmml">​</mo><msub id="S4.SS2.p2.5.m4.1.1.3.3" xref="S4.SS2.p2.5.m4.1.1.3.3.cmml"><mi id="S4.SS2.p2.5.m4.1.1.3.3.2" xref="S4.SS2.p2.5.m4.1.1.3.3.2.cmml">R</mi><mi id="S4.SS2.p2.5.m4.1.1.3.3.3" xref="S4.SS2.p2.5.m4.1.1.3.3.3.cmml">E</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m4.1b"><apply id="S4.SS2.p2.5.m4.1.1.cmml" xref="S4.SS2.p2.5.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m4.1.1.1.cmml" xref="S4.SS2.p2.5.m4.1.1">subscript</csymbol><ci id="S4.SS2.p2.5.m4.1.1.2.cmml" xref="S4.SS2.p2.5.m4.1.1.2">𝑇</ci><apply id="S4.SS2.p2.5.m4.1.1.3.cmml" xref="S4.SS2.p2.5.m4.1.1.3"><times id="S4.SS2.p2.5.m4.1.1.3.1.cmml" xref="S4.SS2.p2.5.m4.1.1.3.1"></times><ci id="S4.SS2.p2.5.m4.1.1.3.2.cmml" xref="S4.SS2.p2.5.m4.1.1.3.2">𝑅</ci><apply id="S4.SS2.p2.5.m4.1.1.3.3.cmml" xref="S4.SS2.p2.5.m4.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m4.1.1.3.3.1.cmml" xref="S4.SS2.p2.5.m4.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p2.5.m4.1.1.3.3.2.cmml" xref="S4.SS2.p2.5.m4.1.1.3.3.2">𝑅</ci><ci id="S4.SS2.p2.5.m4.1.1.3.3.3.cmml" xref="S4.SS2.p2.5.m4.1.1.3.3.3">𝐸</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m4.1c">T_{RR_{E}}</annotation></semantics></math> is similar for the right. Note that <math id="S4.SS2.p2.6.m5.1" class="ltx_Math" alttext="T_{RL}" display="inline"><semantics id="S4.SS2.p2.6.m5.1a"><msub id="S4.SS2.p2.6.m5.1.1" xref="S4.SS2.p2.6.m5.1.1.cmml"><mi id="S4.SS2.p2.6.m5.1.1.2" xref="S4.SS2.p2.6.m5.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.6.m5.1.1.3" xref="S4.SS2.p2.6.m5.1.1.3.cmml"><mi id="S4.SS2.p2.6.m5.1.1.3.2" xref="S4.SS2.p2.6.m5.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.6.m5.1.1.3.1" xref="S4.SS2.p2.6.m5.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.6.m5.1.1.3.3" xref="S4.SS2.p2.6.m5.1.1.3.3.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m5.1b"><apply id="S4.SS2.p2.6.m5.1.1.cmml" xref="S4.SS2.p2.6.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.6.m5.1.1.1.cmml" xref="S4.SS2.p2.6.m5.1.1">subscript</csymbol><ci id="S4.SS2.p2.6.m5.1.1.2.cmml" xref="S4.SS2.p2.6.m5.1.1.2">𝑇</ci><apply id="S4.SS2.p2.6.m5.1.1.3.cmml" xref="S4.SS2.p2.6.m5.1.1.3"><times id="S4.SS2.p2.6.m5.1.1.3.1.cmml" xref="S4.SS2.p2.6.m5.1.1.3.1"></times><ci id="S4.SS2.p2.6.m5.1.1.3.2.cmml" xref="S4.SS2.p2.6.m5.1.1.3.2">𝑅</ci><ci id="S4.SS2.p2.6.m5.1.1.3.3.cmml" xref="S4.SS2.p2.6.m5.1.1.3.3">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m5.1c">T_{RL}</annotation></semantics></math> is obtained from the calibration of stereo frame cameras, and <math id="S4.SS2.p2.7.m6.1" class="ltx_Math" alttext="T_{LL_{o}}" display="inline"><semantics id="S4.SS2.p2.7.m6.1a"><msub id="S4.SS2.p2.7.m6.1.1" xref="S4.SS2.p2.7.m6.1.1.cmml"><mi id="S4.SS2.p2.7.m6.1.1.2" xref="S4.SS2.p2.7.m6.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.7.m6.1.1.3" xref="S4.SS2.p2.7.m6.1.1.3.cmml"><mi id="S4.SS2.p2.7.m6.1.1.3.2" xref="S4.SS2.p2.7.m6.1.1.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.7.m6.1.1.3.1" xref="S4.SS2.p2.7.m6.1.1.3.1.cmml">​</mo><msub id="S4.SS2.p2.7.m6.1.1.3.3" xref="S4.SS2.p2.7.m6.1.1.3.3.cmml"><mi id="S4.SS2.p2.7.m6.1.1.3.3.2" xref="S4.SS2.p2.7.m6.1.1.3.3.2.cmml">L</mi><mi id="S4.SS2.p2.7.m6.1.1.3.3.3" xref="S4.SS2.p2.7.m6.1.1.3.3.3.cmml">o</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m6.1b"><apply id="S4.SS2.p2.7.m6.1.1.cmml" xref="S4.SS2.p2.7.m6.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m6.1.1.1.cmml" xref="S4.SS2.p2.7.m6.1.1">subscript</csymbol><ci id="S4.SS2.p2.7.m6.1.1.2.cmml" xref="S4.SS2.p2.7.m6.1.1.2">𝑇</ci><apply id="S4.SS2.p2.7.m6.1.1.3.cmml" xref="S4.SS2.p2.7.m6.1.1.3"><times id="S4.SS2.p2.7.m6.1.1.3.1.cmml" xref="S4.SS2.p2.7.m6.1.1.3.1"></times><ci id="S4.SS2.p2.7.m6.1.1.3.2.cmml" xref="S4.SS2.p2.7.m6.1.1.3.2">𝐿</ci><apply id="S4.SS2.p2.7.m6.1.1.3.3.cmml" xref="S4.SS2.p2.7.m6.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m6.1.1.3.3.1.cmml" xref="S4.SS2.p2.7.m6.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p2.7.m6.1.1.3.3.2.cmml" xref="S4.SS2.p2.7.m6.1.1.3.3.2">𝐿</ci><ci id="S4.SS2.p2.7.m6.1.1.3.3.3.cmml" xref="S4.SS2.p2.7.m6.1.1.3.3.3">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m6.1c">T_{LL_{o}}</annotation></semantics></math>, <math id="S4.SS2.p2.8.m7.1" class="ltx_Math" alttext="T_{RR_{o}}" display="inline"><semantics id="S4.SS2.p2.8.m7.1a"><msub id="S4.SS2.p2.8.m7.1.1" xref="S4.SS2.p2.8.m7.1.1.cmml"><mi id="S4.SS2.p2.8.m7.1.1.2" xref="S4.SS2.p2.8.m7.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.8.m7.1.1.3" xref="S4.SS2.p2.8.m7.1.1.3.cmml"><mi id="S4.SS2.p2.8.m7.1.1.3.2" xref="S4.SS2.p2.8.m7.1.1.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m7.1.1.3.1" xref="S4.SS2.p2.8.m7.1.1.3.1.cmml">​</mo><msub id="S4.SS2.p2.8.m7.1.1.3.3" xref="S4.SS2.p2.8.m7.1.1.3.3.cmml"><mi id="S4.SS2.p2.8.m7.1.1.3.3.2" xref="S4.SS2.p2.8.m7.1.1.3.3.2.cmml">R</mi><mi id="S4.SS2.p2.8.m7.1.1.3.3.3" xref="S4.SS2.p2.8.m7.1.1.3.3.3.cmml">o</mi></msub></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m7.1b"><apply id="S4.SS2.p2.8.m7.1.1.cmml" xref="S4.SS2.p2.8.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m7.1.1.1.cmml" xref="S4.SS2.p2.8.m7.1.1">subscript</csymbol><ci id="S4.SS2.p2.8.m7.1.1.2.cmml" xref="S4.SS2.p2.8.m7.1.1.2">𝑇</ci><apply id="S4.SS2.p2.8.m7.1.1.3.cmml" xref="S4.SS2.p2.8.m7.1.1.3"><times id="S4.SS2.p2.8.m7.1.1.3.1.cmml" xref="S4.SS2.p2.8.m7.1.1.3.1"></times><ci id="S4.SS2.p2.8.m7.1.1.3.2.cmml" xref="S4.SS2.p2.8.m7.1.1.3.2">𝑅</ci><apply id="S4.SS2.p2.8.m7.1.1.3.3.cmml" xref="S4.SS2.p2.8.m7.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m7.1.1.3.3.1.cmml" xref="S4.SS2.p2.8.m7.1.1.3.3">subscript</csymbol><ci id="S4.SS2.p2.8.m7.1.1.3.3.2.cmml" xref="S4.SS2.p2.8.m7.1.1.3.3.2">𝑅</ci><ci id="S4.SS2.p2.8.m7.1.1.3.3.3.cmml" xref="S4.SS2.p2.8.m7.1.1.3.3.3">𝑜</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m7.1c">T_{RR_{o}}</annotation></semantics></math> are achieved from the intra-calibration of the coaxial devices. Finally, we use <math id="S4.SS2.p2.9.m8.1" class="ltx_Math" alttext="T_{oRL}" display="inline"><semantics id="S4.SS2.p2.9.m8.1a"><msub id="S4.SS2.p2.9.m8.1.1" xref="S4.SS2.p2.9.m8.1.1.cmml"><mi id="S4.SS2.p2.9.m8.1.1.2" xref="S4.SS2.p2.9.m8.1.1.2.cmml">T</mi><mrow id="S4.SS2.p2.9.m8.1.1.3" xref="S4.SS2.p2.9.m8.1.1.3.cmml"><mi id="S4.SS2.p2.9.m8.1.1.3.2" xref="S4.SS2.p2.9.m8.1.1.3.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m8.1.1.3.1" xref="S4.SS2.p2.9.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.9.m8.1.1.3.3" xref="S4.SS2.p2.9.m8.1.1.3.3.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.9.m8.1.1.3.1a" xref="S4.SS2.p2.9.m8.1.1.3.1.cmml">​</mo><mi id="S4.SS2.p2.9.m8.1.1.3.4" xref="S4.SS2.p2.9.m8.1.1.3.4.cmml">L</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m8.1b"><apply id="S4.SS2.p2.9.m8.1.1.cmml" xref="S4.SS2.p2.9.m8.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.9.m8.1.1.1.cmml" xref="S4.SS2.p2.9.m8.1.1">subscript</csymbol><ci id="S4.SS2.p2.9.m8.1.1.2.cmml" xref="S4.SS2.p2.9.m8.1.1.2">𝑇</ci><apply id="S4.SS2.p2.9.m8.1.1.3.cmml" xref="S4.SS2.p2.9.m8.1.1.3"><times id="S4.SS2.p2.9.m8.1.1.3.1.cmml" xref="S4.SS2.p2.9.m8.1.1.3.1"></times><ci id="S4.SS2.p2.9.m8.1.1.3.2.cmml" xref="S4.SS2.p2.9.m8.1.1.3.2">𝑜</ci><ci id="S4.SS2.p2.9.m8.1.1.3.3.cmml" xref="S4.SS2.p2.9.m8.1.1.3.3">𝑅</ci><ci id="S4.SS2.p2.9.m8.1.1.3.4.cmml" xref="S4.SS2.p2.9.m8.1.1.3.4">𝐿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m8.1c">T_{oRL}</annotation></semantics></math> and the camera intrinsics to rectify the aligned events and frames, obtaining paired rectified event-frame data with a resolution of 1200<math id="S4.SS2.p2.10.m9.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.SS2.p2.10.m9.1a"><mo id="S4.SS2.p2.10.m9.1.1" xref="S4.SS2.p2.10.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m9.1b"><times id="S4.SS2.p2.10.m9.1.1.cmml" xref="S4.SS2.p2.10.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m9.1c">\times</annotation></semantics></math>624.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In addition, we also use the Kalibr toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for the calibration between the IMU and camera, where we keep the multimodal system moving towards the AprilTag. As for the calibration between the LiDAR and camera, we first obtain initial extrinsics between the LiDAR and camera from the CAD model. And then, we take the LiDAR-to-camera calibration tool from the SensorsCalibration toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, to refine these initial extrinsics.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2408.08500/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="254" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Effect of event-frame fusion on reference depth in the nighttime scene. Reference depth obtained from the low-light image shows local smoothness due to the lost texture. In contrast, event-frame fusion enhances the imaging quality, thus improving structure details of reference depth.
</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2408.08500/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>
Ground truth examples of the proposed dataset CoSEC under various illumination conditions. CoSEC can provide the pixel-level spatiotemporal-aligned event and frame data with corresponding ground truth under ideal light and extremely low-light conditions.
</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">MULTIMODAL DATASET</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Multimodal dataset consists of input data and high-quality ground truth. The ground truths of autonomous driving datasets mainly include detection, semantics, depth, optical flow, etc. In this section, we focus on the ground truth generation techniques of the depth and optical flow labels based on physical calculation process. In addition, we also introduce the cross-modal fusion strategy for the reliability of labels in nighttime conditions.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.5.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.6.2" class="ltx_text ltx_font_italic">Sequences</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The proposed multimodal dataset CoSEC covers various scenes under all-day conditions with total 128 sequences of 3658 seconds. As shown in Table <a href="#S4.T3" title="TABLE III ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, the sequences of CoSEC are collected in city, campus, park, suburbs and village under daytime and nighttime conditions. These sequences are split into 102 for training and 26 for testing. Moreover, each sequence provides the stereo spatiotemporal-aligned event and frame data with their corresponding ground truth depth and optical flow. The procedure for ground truth generation will be presented in the following section.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.5.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.6.2" class="ltx_text ltx_font_italic">Ground Truth Generation</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Autonomous driving scene perception usually focuses on the 3D semantic and motion information of dynamic scenes, where the former is obtained through manual annotation while the latter is obtained through data calculation. In this part, we mainly describe the ground truth depth and optical flow generation in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Ground Truth Depth.</span> Given LiDAR data, intrinsics and extrinsics, we first compute the poses of LiDAR using the FAST-LIO algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Then, considering the sparsity of a single LiDAR point cloud, we utilize the poses to fuse the LiDAR point clouds within a certain period centered at current timestamp into a local dense cloud. Finally, we use calibration parameters of cameras and LiDAR to project the local fused point cloud into the camera coordinate system for ground truth depth. However, due to the sparsity of LiDAR point clouds, directly projecting them into the camera coordinate system may cause some non-visible points to appear. To this end, we propose to perform outlier removal with reference depth. Specifically, we introduce a foundation depth model, namely Metric3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, to estimate the reference depth for filtering the projected coarse depth. After the outlier removal, we obtain the final ground truth depth.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.5" class="ltx_p"><span id="S5.SS2.p3.5.1" class="ltx_text ltx_font_bold">Ground Truth Optical Flow.</span> In Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we further illustrate the generation of ground truth optical flow, which is calculated by ground truth depth with the poses and calibration parameters of the camera as follows:</p>
<table id="S5.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S5.E2.m1.1" class="ltx_Math" alttext="F=KTD(X)K^{-1}X-X" display="block"><semantics id="S5.E2.m1.1a"><mrow id="S5.E2.m1.1.2" xref="S5.E2.m1.1.2.cmml"><mi id="S5.E2.m1.1.2.2" xref="S5.E2.m1.1.2.2.cmml">F</mi><mo id="S5.E2.m1.1.2.1" xref="S5.E2.m1.1.2.1.cmml">=</mo><mrow id="S5.E2.m1.1.2.3" xref="S5.E2.m1.1.2.3.cmml"><mrow id="S5.E2.m1.1.2.3.2" xref="S5.E2.m1.1.2.3.2.cmml"><mi id="S5.E2.m1.1.2.3.2.2" xref="S5.E2.m1.1.2.3.2.2.cmml">K</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.2.3.2.1" xref="S5.E2.m1.1.2.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.2.3.2.3" xref="S5.E2.m1.1.2.3.2.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.2.3.2.1a" xref="S5.E2.m1.1.2.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.2.3.2.4" xref="S5.E2.m1.1.2.3.2.4.cmml">D</mi><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.2.3.2.1b" xref="S5.E2.m1.1.2.3.2.1.cmml">​</mo><mrow id="S5.E2.m1.1.2.3.2.5.2" xref="S5.E2.m1.1.2.3.2.cmml"><mo stretchy="false" id="S5.E2.m1.1.2.3.2.5.2.1" xref="S5.E2.m1.1.2.3.2.cmml">(</mo><mi id="S5.E2.m1.1.1" xref="S5.E2.m1.1.1.cmml">X</mi><mo stretchy="false" id="S5.E2.m1.1.2.3.2.5.2.2" xref="S5.E2.m1.1.2.3.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.2.3.2.1c" xref="S5.E2.m1.1.2.3.2.1.cmml">​</mo><msup id="S5.E2.m1.1.2.3.2.6" xref="S5.E2.m1.1.2.3.2.6.cmml"><mi id="S5.E2.m1.1.2.3.2.6.2" xref="S5.E2.m1.1.2.3.2.6.2.cmml">K</mi><mrow id="S5.E2.m1.1.2.3.2.6.3" xref="S5.E2.m1.1.2.3.2.6.3.cmml"><mo id="S5.E2.m1.1.2.3.2.6.3a" xref="S5.E2.m1.1.2.3.2.6.3.cmml">−</mo><mn id="S5.E2.m1.1.2.3.2.6.3.2" xref="S5.E2.m1.1.2.3.2.6.3.2.cmml">1</mn></mrow></msup><mo lspace="0em" rspace="0em" id="S5.E2.m1.1.2.3.2.1d" xref="S5.E2.m1.1.2.3.2.1.cmml">​</mo><mi id="S5.E2.m1.1.2.3.2.7" xref="S5.E2.m1.1.2.3.2.7.cmml">X</mi></mrow><mo id="S5.E2.m1.1.2.3.1" xref="S5.E2.m1.1.2.3.1.cmml">−</mo><mi id="S5.E2.m1.1.2.3.3" xref="S5.E2.m1.1.2.3.3.cmml">X</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.E2.m1.1b"><apply id="S5.E2.m1.1.2.cmml" xref="S5.E2.m1.1.2"><eq id="S5.E2.m1.1.2.1.cmml" xref="S5.E2.m1.1.2.1"></eq><ci id="S5.E2.m1.1.2.2.cmml" xref="S5.E2.m1.1.2.2">𝐹</ci><apply id="S5.E2.m1.1.2.3.cmml" xref="S5.E2.m1.1.2.3"><minus id="S5.E2.m1.1.2.3.1.cmml" xref="S5.E2.m1.1.2.3.1"></minus><apply id="S5.E2.m1.1.2.3.2.cmml" xref="S5.E2.m1.1.2.3.2"><times id="S5.E2.m1.1.2.3.2.1.cmml" xref="S5.E2.m1.1.2.3.2.1"></times><ci id="S5.E2.m1.1.2.3.2.2.cmml" xref="S5.E2.m1.1.2.3.2.2">𝐾</ci><ci id="S5.E2.m1.1.2.3.2.3.cmml" xref="S5.E2.m1.1.2.3.2.3">𝑇</ci><ci id="S5.E2.m1.1.2.3.2.4.cmml" xref="S5.E2.m1.1.2.3.2.4">𝐷</ci><ci id="S5.E2.m1.1.1.cmml" xref="S5.E2.m1.1.1">𝑋</ci><apply id="S5.E2.m1.1.2.3.2.6.cmml" xref="S5.E2.m1.1.2.3.2.6"><csymbol cd="ambiguous" id="S5.E2.m1.1.2.3.2.6.1.cmml" xref="S5.E2.m1.1.2.3.2.6">superscript</csymbol><ci id="S5.E2.m1.1.2.3.2.6.2.cmml" xref="S5.E2.m1.1.2.3.2.6.2">𝐾</ci><apply id="S5.E2.m1.1.2.3.2.6.3.cmml" xref="S5.E2.m1.1.2.3.2.6.3"><minus id="S5.E2.m1.1.2.3.2.6.3.1.cmml" xref="S5.E2.m1.1.2.3.2.6.3"></minus><cn type="integer" id="S5.E2.m1.1.2.3.2.6.3.2.cmml" xref="S5.E2.m1.1.2.3.2.6.3.2">1</cn></apply></apply><ci id="S5.E2.m1.1.2.3.2.7.cmml" xref="S5.E2.m1.1.2.3.2.7">𝑋</ci></apply><ci id="S5.E2.m1.1.2.3.3.cmml" xref="S5.E2.m1.1.2.3.3">𝑋</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E2.m1.1c">F=KTD(X)K^{-1}X-X</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S5.SS2.p3.4" class="ltx_p">where <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="X" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">X</annotation></semantics></math> is the pixel coordinate, <math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mi id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><ci id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">K</annotation></semantics></math> is the camera intrinsics, <math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mi id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><ci id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">D</annotation></semantics></math> is the depth map, and <math id="S5.SS2.p3.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S5.SS2.p3.4.m4.1a"><mi id="S5.SS2.p3.4.m4.1.1" xref="S5.SS2.p3.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.4.m4.1b"><ci id="S5.SS2.p3.4.m4.1.1.cmml" xref="S5.SS2.p3.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.4.m4.1c">T</annotation></semantics></math> is the camera transformation, which is computed from the LiDAR poses and the extrinsics between the LiDAR and camera.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">Event-Frame Fusion for Nighttime Scene.</span> Nighttime label generation is a challenging problem, since low light weakens the texture captured by the frame camera due to its low dynamic range. In contrast, event camera has the advantage of high dynamic range, motivating us to introduce an event-frame fusion for improving the accuracy of nighttime label generation. Specifically, in Fig. <a href="#S4.F5" title="Figure 5 ‣ IV-A Sensors and Time Synchronization ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we take the low-light enhancement method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to fuse the event and frame data for the visibility of the nighttime frame, and then estimate the reference depth from the enhanced image using Metric3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. As shown in Fig. <a href="#S4.F6" title="Figure 6 ‣ IV-B Spatial Calibration ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the event-frame fusion can well enhance the low-light image, thus improving the details of the corresponding reference depth. Therefore, this fusion approach leverages the advantages of the coaxial devices, making it possible to generate accurate ground truth depth and flow under all-day conditions in Fig. <a href="#S4.F7" title="Figure 7 ‣ IV-B Spatial Calibration ‣ IV MULTIMODAL SYSTEM ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">EXPERIMENTS</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we conduct experiments to demonstrate that the proposed dataset can improve the performance and generalization of multimodal fusion. The results are shown in Table <a href="#S6.T4" title="TABLE IV ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a href="#S6.T5" title="TABLE V ‣ VI-C Improvement of Generalization for Nighttime Scenes ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, where F, E, and EF indicate training on the frame, event, and event-frame data respectively.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>
Discussion on effect of parallel and coaxial event-frame placement strategies on multimodal fusion.
</figcaption>
<table id="S6.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T4.1.1" class="ltx_tr">
<td id="S6.T4.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.5pt;" colspan="2">
<span id="S6.T4.1.1.1.1" class="ltx_text"></span> <span id="S6.T4.1.1.1.2" class="ltx_text">
<span id="S6.T4.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T4.1.1.1.2.1.1" class="ltx_tr">
<span id="S6.T4.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Parallel</span></span></span>
<span id="S6.T4.1.1.1.2.1.2" class="ltx_tr">
<span id="S6.T4.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">(DSEC)</span></span></span>
</span></span><span id="S6.T4.1.1.1.3" class="ltx_text"></span></td>
<td id="S6.T4.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 3.5pt;" colspan="2">
<span id="S6.T4.1.1.2.1" class="ltx_text"></span> <span id="S6.T4.1.1.2.2" class="ltx_text">
<span id="S6.T4.1.1.2.2.1" class="ltx_tabular ltx_align_middle">
<span id="S6.T4.1.1.2.2.1.1" class="ltx_tr">
<span id="S6.T4.1.1.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.1.2.2.1.1.1.1" class="ltx_text ltx_font_bold">Coaxial</span></span></span>
<span id="S6.T4.1.1.2.2.1.2" class="ltx_tr">
<span id="S6.T4.1.1.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.1.2.2.1.2.1.1" class="ltx_text ltx_font_bold">(CoSEC)</span></span></span>
</span></span><span id="S6.T4.1.1.2.3" class="ltx_text"></span></td>
<td id="S6.T4.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding:1.5pt 3.5pt;" colspan="3"><span id="S6.T4.1.1.3.1" class="ltx_text ltx_font_bold">DSEC</span></td>
<td id="S6.T4.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding:1.5pt 3.5pt;" colspan="3"><span id="S6.T4.1.1.4.1" class="ltx_text ltx_font_bold">CoSEC</span></td>
</tr>
<tr id="S6.T4.1.2" class="ltx_tr">
<td id="S6.T4.1.2.1" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">Frame</td>
<td id="S6.T4.1.2.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">Event</td>
<td id="S6.T4.1.2.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">Frame</td>
<td id="S6.T4.1.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;">Event</td>
<td id="S6.T4.1.2.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">10m</td>
<td id="S6.T4.1.2.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">20m</td>
<td id="S6.T4.1.2.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;">30m</td>
<td id="S6.T4.1.2.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">10m</td>
<td id="S6.T4.1.2.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">20m</td>
<td id="S6.T4.1.2.10" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">30m</td>
</tr>
<tr id="S6.T4.1.3" class="ltx_tr">
<td id="S6.T4.1.3.1" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.3.1.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">
<span id="S6.T4.1.3.2.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S6.T4.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.3.3.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 3.5pt;">
<span id="S6.T4.1.3.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S6.T4.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">2.56</td>
<td id="S6.T4.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">2.82</td>
<td id="S6.T4.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding:1.5pt 3.5pt;">3.86</td>
<td id="S6.T4.1.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">2.57</td>
<td id="S6.T4.1.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">2.79</td>
<td id="S6.T4.1.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding:1.5pt 3.5pt;">5.61</td>
</tr>
<tr id="S6.T4.1.4" class="ltx_tr">
<td id="S6.T4.1.4.1" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.4.1.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.4.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.4.2.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.4.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.4.3.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;">
<span id="S6.T4.1.4.4.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S6.T4.1.4.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.18</td>
<td id="S6.T4.1.4.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.56</td>
<td id="S6.T4.1.4.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;">2.81</td>
<td id="S6.T4.1.4.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.75</td>
<td id="S6.T4.1.4.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">2.00</td>
<td id="S6.T4.1.4.10" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">5.15</td>
</tr>
<tr id="S6.T4.1.5" class="ltx_tr">
<td id="S6.T4.1.5.1" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.5.1.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.5.2" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.5.2.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.5.3" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.5.3.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.5.4" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.5.4.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.5.5" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.14</td>
<td id="S6.T4.1.5.6" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.52</td>
<td id="S6.T4.1.5.7" class="ltx_td ltx_align_center ltx_border_r" style="padding:1.5pt 3.5pt;">2.79</td>
<td id="S6.T4.1.5.8" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.68</td>
<td id="S6.T4.1.5.9" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">1.92</td>
<td id="S6.T4.1.5.10" class="ltx_td ltx_align_center" style="padding:1.5pt 3.5pt;">5.01</td>
</tr>
<tr id="S6.T4.1.6" class="ltx_tr">
<td id="S6.T4.1.6.1" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.1.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.6.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;">
<span id="S6.T4.1.6.2.1" class="ltx_ERROR undefined">\usym</span>2613</td>
<td id="S6.T4.1.6.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.3.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.4.1" class="ltx_ERROR undefined">\faCheck</span></td>
<td id="S6.T4.1.6.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.5.1" class="ltx_text ltx_font_bold">1.11</span></td>
<td id="S6.T4.1.6.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.6.1" class="ltx_text ltx_font_bold">1.46</span></td>
<td id="S6.T4.1.6.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.7.1" class="ltx_text ltx_font_bold">2.65</span></td>
<td id="S6.T4.1.6.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.8.1" class="ltx_text ltx_font_bold">1.66</span></td>
<td id="S6.T4.1.6.9" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.9.1" class="ltx_text ltx_font_bold">1.90</span></td>
<td id="S6.T4.1.6.10" class="ltx_td ltx_align_center ltx_border_bb" style="padding:1.5pt 3.5pt;"><span id="S6.T4.1.6.10.1" class="ltx_text ltx_font_bold">4.94</span></td>
</tr>
</table>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.5.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.6.2" class="ltx_text ltx_font_italic">Experiment Setup</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p"><span id="S6.SS1.p1.1.1" class="ltx_text ltx_font_bold">Dataset.</span> We conduct experiments on DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and CoSEC datasets. The event-frame data of the DSEC dataset is spatially unaligned due to the parallel strategy, while CoSEC dataset implements the coaxial strategy to capture aligned data. Note that the training and testing sets from CoSEC are selected to match the size of those from DSEC, and the comparisons should be performed within each dataset due to differences in scene scale caused by different capturing locations and camera intrinsics.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p"><span id="S6.SS1.p2.1.1" class="ltx_text ltx_font_bold">Method.</span> To demonstrate the effectiveness of the pixel-level spatial alignment between different modalities on multimodal fusion, we take depth estimation as the example task, where we introduce the multimodal-based model RAMNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> to conduct experiments. We pre-train the depth model for weight initialization on dense depth maps from the EventScape <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> dataset, which is a synthetic dataset collected from the CARLA simulator <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. For the comparison experiment of different event-frame placement strategies in Table <a href="#S6.T4" title="TABLE IV ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we first train the frame branch of the RAMNet individually on the mixed frame data of DSEC and CoSEC. Then, we load the obtained weights of the frame branch into RAMNet, which is fine-tuned on the event-frame data of DSEC, CoSEC, and their mixture respectively. For the generalization experiment for nighttime scenes in Table <a href="#S6.T5" title="TABLE V ‣ VI-C Improvement of Generalization for Nighttime Scenes ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we train RAMNet and its unimodal-based baselines on nighttime data of DSEC and CoSEC.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Metric.</span> We choose the average absolute depth error for ground truth depth up to 10 m, 20 m and 30 m as quantitative metric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, which is lower for better performance.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.5.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.6.2" class="ltx_text ltx_font_italic">Effectiveness of Coaxial Device on Multimodal Fusion</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">To validate the effectiveness of the coaxial device on event-frame fusion in Table <a href="#S6.T4" title="TABLE IV ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we compare unimodal-based and multimodal-based depth estimation models trained on DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and CoSEC datasets. Note that the event-frame data of DSEC is captured via parallel strategy, and that of CoSEC is captured via coaxial strategy. First, multimodal fusion significantly improves the performance of depth estimation over the unimodal approach. This is because the complementary knowledge fusion between frame and event cameras can compensate for the limitation of frame-based imaging mechanism, thus improving the performance of depth estimation. Second, the depth models trained on the multimodal data of CoSEC perform better than those trained on the DSEC. The main reason is that the coaxial strategy used in CoSEC enables better pixel-level spatial alignment between the event and frame data, facilitating the multimodal-based depth estimation. Therefore, the proposed coaxial device ensures pixel-level alignment between event and frame, promoting the multimodal fusion for vision tasks.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.5.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.6.2" class="ltx_text ltx_font_italic">Improvement of Generalization for Nighttime Scenes</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">To demonstrate the impact of the proposed dataset CoSEC on model generalization for nighttime scenes in Table <a href="#S6.T5" title="TABLE V ‣ VI-C Improvement of Generalization for Nighttime Scenes ‣ VI EXPERIMENTS ‣ CoSEC: A Coaxial Stereo Event Camera Dataset for Autonomous Driving" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we evaluate the depth models trained on unimodal and multimodal nighttime data of DSEC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and CoSEC. First, the multimodal-based model trained on CoSEC outperforms the one trained on DSEC. The main reason is that the pixel-level spatially aligned event-frame data of CoSEC enhances multimodal fusion for nighttime scenes more effectively than the unaligned data of DSEC. Second, the models trained on nighttime data of CoSEC perform better than those trained on DSEC. This is because our nighttime enhancement strategy, namely event-frame fusion, can effectively improve the quality of ground truth depth in nighttime scenes. Therefore, the proposed CoSEC dataset can not only enhance the performance of multimodal fusion, but also facilitate the improvement of model generalization for nighttime scenes.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>
Discussion on improvement of the proposed dataset on the generalization of depth model for nighttime scenes.
</figcaption>
<table id="S6.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S6.T5.1.1" class="ltx_tr">
<td id="S6.T5.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" rowspan="2"><span id="S6.T5.1.1.1.1" class="ltx_text ltx_font_bold">Strategy</span></td>
<td id="S6.T5.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3"><span id="S6.T5.1.1.2.1" class="ltx_text ltx_font_bold">DSEC</span></td>
<td id="S6.T5.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;" colspan="3"><span id="S6.T5.1.1.3.1" class="ltx_text ltx_font_bold">CoSEC</span></td>
</tr>
<tr id="S6.T5.1.2" class="ltx_tr">
<td id="S6.T5.1.2.1" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">10m</td>
<td id="S6.T5.1.2.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">20m</td>
<td id="S6.T5.1.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">30m</td>
<td id="S6.T5.1.2.4" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">10m</td>
<td id="S6.T5.1.2.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">20m</td>
<td id="S6.T5.1.2.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;">30m</td>
</tr>
<tr id="S6.T5.1.3" class="ltx_tr">
<td id="S6.T5.1.3.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.3.1.1" class="ltx_text ltx_font_bold">F<sub id="S6.T5.1.3.1.1.1" class="ltx_sub">DSEC</sub></span></td>
<td id="S6.T5.1.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.46</td>
<td id="S6.T5.1.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.56</td>
<td id="S6.T5.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.02</td>
<td id="S6.T5.1.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.16</td>
<td id="S6.T5.1.3.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.27</td>
<td id="S6.T5.1.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">7.20</td>
</tr>
<tr id="S6.T5.1.4" class="ltx_tr">
<td id="S6.T5.1.4.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.1.1" class="ltx_text ltx_font_bold">F<sub id="S6.T5.1.4.1.1.1" class="ltx_sub">CoSEC</sub></span></td>
<td id="S6.T5.1.4.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.2.1" class="ltx_text ltx_font_bold">3.17</span></td>
<td id="S6.T5.1.4.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.3.1" class="ltx_text ltx_font_bold">3.29</span></td>
<td id="S6.T5.1.4.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.4.1" class="ltx_text ltx_font_bold">3.82</span></td>
<td id="S6.T5.1.4.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.5.1" class="ltx_text ltx_font_bold">3.93</span></td>
<td id="S6.T5.1.4.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.6.1" class="ltx_text ltx_font_bold">4.07</span></td>
<td id="S6.T5.1.4.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.4.7.1" class="ltx_text ltx_font_bold">5.21</span></td>
</tr>
<tr id="S6.T5.1.5" class="ltx_tr">
<td id="S6.T5.1.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.5.1.1" class="ltx_text ltx_font_bold">E<sub id="S6.T5.1.5.1.1.1" class="ltx_sub">DSEC</sub></span></td>
<td id="S6.T5.1.5.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.74</td>
<td id="S6.T5.1.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">5.81</td>
<td id="S6.T5.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">6.13</td>
<td id="S6.T5.1.5.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.30</td>
<td id="S6.T5.1.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">2.46</td>
<td id="S6.T5.1.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.29</td>
</tr>
<tr id="S6.T5.1.6" class="ltx_tr">
<td id="S6.T5.1.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.1.1" class="ltx_text ltx_font_bold">E<sub id="S6.T5.1.6.1.1.1" class="ltx_sub">CoSEC</sub></span></td>
<td id="S6.T5.1.6.2" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.2.1" class="ltx_text ltx_font_bold">3.29</span></td>
<td id="S6.T5.1.6.3" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.3.1" class="ltx_text ltx_font_bold">3.40</span></td>
<td id="S6.T5.1.6.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.4.1" class="ltx_text ltx_font_bold">3.92</span></td>
<td id="S6.T5.1.6.5" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.5.1" class="ltx_text ltx_font_bold">2.11</span></td>
<td id="S6.T5.1.6.6" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.6.1" class="ltx_text ltx_font_bold">2.29</span></td>
<td id="S6.T5.1.6.7" class="ltx_td ltx_align_center" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.6.7.1" class="ltx_text ltx_font_bold">4.05</span></td>
</tr>
<tr id="S6.T5.1.7" class="ltx_tr">
<td id="S6.T5.1.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.7.1.1" class="ltx_text ltx_font_bold">EF<sub id="S6.T5.1.7.1.1.1" class="ltx_sub">DSEC</sub></span></td>
<td id="S6.T5.1.7.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.34</td>
<td id="S6.T5.1.7.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.45</td>
<td id="S6.T5.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4.85</td>
<td id="S6.T5.1.7.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.47</td>
<td id="S6.T5.1.7.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">1.65</td>
<td id="S6.T5.1.7.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">3.59</td>
</tr>
<tr id="S6.T5.1.8" class="ltx_tr">
<td id="S6.T5.1.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.1.1" class="ltx_text ltx_font_bold">EF<sub id="S6.T5.1.8.1.1.1" class="ltx_sub">CoSEC</sub></span></td>
<td id="S6.T5.1.8.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.2.1" class="ltx_text ltx_font_bold">1.72</span></td>
<td id="S6.T5.1.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.3.1" class="ltx_text ltx_font_bold">1.85</span></td>
<td id="S6.T5.1.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.4.1" class="ltx_text ltx_font_bold">2.45</span></td>
<td id="S6.T5.1.8.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.5.1" class="ltx_text ltx_font_bold">0.77</span></td>
<td id="S6.T5.1.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.6.1" class="ltx_text ltx_font_bold">0.97</span></td>
<td id="S6.T5.1.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span id="S6.T5.1.8.7.1" class="ltx_text ltx_font_bold">3.03</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">CONCLUSION</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we propose a coaxial stereo event camera dataset for autonomous driving, which is collected by the proposed multimodal system, including frame, event, LiDAR and INS. As for the system, we design the coaxial event-frame device to build the multimodal system, promoting the pixel-level spatial alignment between different sensors. As for the dataset, we introduce the aligned event-frame fusion approach to improve the accuracy of ground truth depth and optical flow generation in all-day scenes. Moreover, we conduct experiments to verify the superiority of the proposed multimodal dataset on improving the performance and generalization of multimodal fusion. In the future, we will extend the proposed dataset to more challenging scenes, such as adverse weather. We believe that the proposed dataset can facilitate the relevant research on multimodal fusion for the 3D dynamic scene perception.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">ACKNOWLEDGMENT</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported in part by the National Natural Science Foundation of China under Grant 62371203. The computation is completed in the HPC Platform of Huazhong University of Science and Technology.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Prakash, K. Chitta, and A. Geiger, “Multi-modal fusion transformer for end-to-end autonomous driving,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2021, pp. 7077–7087.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Liang, Y. Yang, B. Li, P. Duan, Y. Xu, and B. Shi, “Coherent event guided low-light video enhancement,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 10 615–10 625.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
H. Liu, S. Peng, L. Zhu, Y. Chang, H. Zhou, and L. Yan, “Seeing motion at nighttime with an event camera,” in <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 25 648–25 658.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
G. Gallego, T. Delbrück, G. Orchard, C. Bartolozzi, B. Taba, A. Censi, S. Leutenegger, A. J. Davison, J. Conradt, K. Daniilidis, <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Event-based vision: A survey,” <em id="bib.bib4.2.2" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 44, no. 1, pp. 154–180, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Sironi, M. Brambilla, N. Bourdis, X. Lagorce, and R. Benosman, “Hats: Histograms of averaged time surfaces for robust event-based object classification,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2018, pp. 1731–1740.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
P. De Tournemire, D. Nitti, E. Perot, D. Migliore, and A. Sironi, “A large scale event-based detection dataset for automotive,” <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2001.08499</em>, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
E. Perot, P. De Tournemire, D. Nitti, J. Masci, and A. Sironi, “Learning to detect objects with a 1 megapixel event camera,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, vol. 33, pp. 16 639–16 652, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. Cheng, H. Luo, W. Yang, L. Yu, S. Chen, and W. Li, “Det: A high-resolution dvs dataset for lane extraction,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops</em>, 2019, pp. 0–0.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, “Ddd17: End-to-end davis driving dataset,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.01458</em>, 2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Hu, J. Binas, D. Neil, S.-C. Liu, and T. Delbruck, “Ddd20 end-to-end event camera driving dataset: Fusing frames and events with deep learning for improved steering prediction,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2020, pp. 1–6.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
T. Fischer and M. Milford, “Event-based visual place recognition with ensembles of temporal windows,” <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 5, no. 4, pp. 6924–6931, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. Z. Zhu, D. Thakur, T. Özaslan, B. Pfrommer, V. Kumar, and K. Daniilidis, “The multivehicle stereo event camera dataset: An event camera dataset for 3d perception,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 3, no. 3, pp. 2032–2039, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
M. Gehrig, W. Aarents, D. Gehrig, and D. Scaramuzza, “Dsec: A stereo event camera dataset for driving scenarios,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
H. Wei, J. Jiao, X. Hu, J. Yu, X. Xie, J. Wu, Y. Zhu, Y. Liu, L. Wang, and M. Liu, “Fusionportablev2: A unified multi-sensor dataset for generalized slam across diverse platforms and scalable environments,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.08563</em>, 2024.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K. Chaney, F. Cladera, Z. Wang, A. Bisulco, M. A. Hsieh, C. Korpela, V. Kumar, C. J. Taylor, and K. Daniilidis, “M3ed: Multi-robot, multi-sensor, multi-environment event dataset,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2023, pp. 4016–4023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Gehrig and D. Scaramuzza, “Low-latency automotive vision with event cameras,” <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Nature</em>, vol. 629, no. 8014, pp. 1034–1040, 2024.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
W. Xu and F. Zhang, “Fast-lio: A fast, robust lidar-inertial odometry package by tightly-coupled iterated kalman filter,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 6, no. 2, pp. 3317–3324, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
W. Yin, C. Zhang, H. Chen, Z. Cai, G. Yu, K. Wang, X. Chen, and C. Shen, “Metric3d: Towards zero-shot metric 3d prediction from a single image,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 9043–9053.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
M. Hu, W. Yin, C. Zhang, Z. Cai, X. Long, H. Chen, K. Wang, G. Yu, C. Shen, and S. Shen, “Metric3d v2: A versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2404.15506</em>, 2024.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
C. Brandli, R. Berner, M. Yang, S.-C. Liu, and T. Delbruck, “A 240<math id="bib.bib20.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="bib.bib20.1.m1.1a"><mo id="bib.bib20.1.m1.1.1" xref="bib.bib20.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="bib.bib20.1.m1.1b"><times id="bib.bib20.1.m1.1.1.cmml" xref="bib.bib20.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.1.m1.1c">\times</annotation></semantics></math> 180 130 db 3 <math id="bib.bib20.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="bib.bib20.2.m2.1a"><mi id="bib.bib20.2.m2.1.1" xref="bib.bib20.2.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="bib.bib20.2.m2.1b"><ci id="bib.bib20.2.m2.1.1.cmml" xref="bib.bib20.2.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib20.2.m2.1c">\mu</annotation></semantics></math>s latency global shutter spatiotemporal vision sensor,” <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">IEEE Journal of Solid-State Circuits</em>, vol. 49, no. 10, pp. 2333–2341, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. Li, C. Brandli, R. Berner, H. Liu, M. Yang, S.-C. Liu, and T. Delbruck, “Design of an rgbw color vga rolling and global shutter dynamic and active-pixel vision sensor,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Symposium on Circuits and Systems (ISCAS)</em>.   IEEE, 2015, pp. 718–721.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
G. Taverni, D. P. Moeys, C. Li, C. Cavaco, V. Motsnyi, D. S. S. Bello, and T. Delbruck, “Front and back illuminated dynamic and active pixel vision sensors comparison,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Circuits and Systems II: Express Briefs</em>, vol. 65, no. 5, pp. 677–681, 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H. Rebecq, R. Ranftl, V. Koltun, and D. Scaramuzza, “Events-to-video: Bringing modern computer vision to event cameras,” in <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2019, pp. 3857–3866.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
——, “High speed and high dynamic range video with an event camera,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 43, no. 6, pp. 1964–1980, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
P. Furgale, J. Rehder, and R. Siegwart, “Unified temporal and spatial calibration for multi-sensor systems,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2013 IEEE/RSJ International Conference on Intelligent Robots and Systems</em>.   IEEE, 2013, pp. 1280–1286.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
G. Yan, Z. Liu, C. Wang, C. Shi, P. Wei, X. Cai, T. Ma, Z. Liu, Z. Zhong, Y. Liu, <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Opencalib: A multi-sensor calibration toolbox for autonomous driving,” <em id="bib.bib26.2.2" class="ltx_emph ltx_font_italic">Software Impacts</em>, vol. 14, p. 100393, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Gehrig, M. Rüegg, M. Gehrig, J. Hidalgo-Carrió, and D. Scaramuzza, “Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>, vol. 6, no. 2, pp. 2822–2829, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open urban driving simulator,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Conference on robot learning</em>.   PMLR, 2017, pp. 1–16.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.08499" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.08500" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.08500">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.08500" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.08501" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:46:16 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
