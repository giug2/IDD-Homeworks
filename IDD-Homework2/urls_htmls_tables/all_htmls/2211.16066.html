<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.16066] Analysis of Training Object Detection Models with Synthetic Data</title><meta property="og:description" content="Recently, the use of synthetic training data has been on the rise as it offers correctly labelled datasets at a lower cost. The downside of this technique is that the so-called domain gap between the real target images…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Analysis of Training Object Detection Models with Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Analysis of Training Object Detection Models with Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.16066">

<!--Generated on Thu Mar 14 08:23:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\addauthor</span>
<p id="p1.2" class="ltx_p">Bram Vanherlebram.vanherle@uhasselt.be1
<span id="p1.2.1" class="ltx_ERROR undefined">\addauthor</span>Steven Moonensteven.moonen@uhasselt.be1
<span id="p1.2.2" class="ltx_ERROR undefined">\addauthor</span>Frank Van Reethfrank.vanreeth@uhasselt.be1
<span id="p1.2.3" class="ltx_ERROR undefined">\addauthor</span>Nick Michielsnick.michiels@uhasselt.be1
<span id="p1.2.4" class="ltx_ERROR undefined">\addinstitution</span>
Hasselt University - tUL 
<br class="ltx_break">Flanders Make, 
<br class="ltx_break">Expertise Centre for Digital Media

Analysis of Object Detection with Synthetic Data</p>
</div>
<h1 class="ltx_title ltx_title_document">Analysis of Training Object Detection Models with Synthetic Data</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, the use of synthetic training data has been on the rise as it offers correctly labelled datasets at a lower cost. The downside of this technique is that the so-called domain gap between the real target images and synthetic training data leads to a decrease in performance. In this paper, we attempt to provide a holistic overview of how to use synthetic data for object detection. We analyse aspects of generating the data as well as techniques used to train the models. We do so by devising a number of experiments, training models on the Dataset of Industrial Metal Objects (DIMO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">De Roovere et al.(2022)De Roovere, Moonen, Michiels, and
wyffels</a>]</cite>. This dataset contains both real and synthetic images. The synthetic part has different subsets that are either exact synthetic copies of the real data or are copies with certain aspects randomised. This allows us to analyse what types of variation are good for synthetic training data and which aspects should be modelled to closely match the target data. Furthermore, we investigate what types of training techniques are beneficial towards generalisation to real data, and how to use them. Additionally, we analyse how real images can be leveraged when training on synthetic images. All these experiments are validated on real data and benchmarked to models trained on real data. The results offer a number of interesting takeaways that can serve as basic guidelines for using synthetic data for object detection. Code to reproduce results is available at <a target="_blank" href="https://github.com/EDM-Research/DIMO_ObjectDetection" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/EDM-Research/DIMO_ObjectDetection</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Deep learning and its applications have advanced tremendously over the last couple of years. These powerful machine learning models require a large amount of labelled training data however. The more complex and the better these models get, the more training data they require. But good training data is not easy to come by. Manually creating photographs and labeling them is a slow and costly process. Additionally, humans are prone to introducing errors and bias in datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">Tommasi et al.(2017)Tommasi, Patricia, Caputo, and
Tuytelaars</a>, <a href="#bib.bibx13" title="" class="ltx_ref">Northcutt et al.(2021)Northcutt, Athalye, and Mueller</a>]</cite>, which is bad for model performance. Furthermore, some forms of annotations are very difficult for a human to create, such as depth maps, segmentation maps or object poses. 
<br class="ltx_break">Due to these problems with datasets created by humans, synthetic training data has become more popular over recent years. With modern rendering technology it is easy to render thousands of images fairly quickly and at a low cost when 3D models are provided. Since the 3D composition of the depicted scene is known, the accompanying labels for the machine learning task can easily be generated. Additionally, these labels are pixel correct and the dataset contains less bias, since a computer is way better at randomising than a human. There is however a big disadvantage to using synthetic data. Although looking very realistic, there still is a difference in appearance between real and rendered images. This causes a model that is trained on synthetic images, to perform worse on real images. This phenomenon is called the domain gap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Tobin et al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
Abbeel</a>]</cite> and it hinders synthetic training data from being widely adopted. 
<br class="ltx_break">Object Detection is one of the most prominent fields of computer vision. This is due to the fact that it has many applications and is often the first step in vision pipelines for more complex tasks. Some of these applications include robot control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">Bai et al.(2020)Bai, Li, Yang, Song, Li, and Zhang</a>]</cite>, product inspection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">Yang et al.(2020)Yang, Li, Wang, Dong, Wang, and
Tang</a>]</cite>, surveillance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">Sreenu and Durai(2019)</a>]</cite> and many more. If a company wants to apply deep learning to their specific tasks, they need high quality training data that is specific to them. Manually labelled data is often too expensive or sometimes even too difficult to come by, especially so for smaller companies. These companies sometimes resolve to using synthetic data to train their models, often with unsatisfactory results, due to the aforementioned problems with synthetic data. While synthetic data is cheaper then manually created data, it is not for free. When rendering thousands of images, costs can accumulate to a large number as well. 
<br class="ltx_break">In this paper we offer a number of insights on how to generate and how to use synthetic training data. The goal is to generate knowledge on how to create training data that offers good performance on real images whilst keeping the total cost of rendering as low as possible. When using this synthetic data we use only basic deep learning mechanisms that are available in most toolkits. We deliberately stray away from more complex methods of domain adaptation and generalisation to make our findings as widely applicable as possible. To provide these insights, we perform a number of experiments using the Dataset of Industrial Metal Objects (DIMO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">De Roovere et al.(2022)De Roovere, Moonen, Michiels, and
wyffels</a>]</cite>. This dataset contains a set of real images, exact synthetically rendered copies of those real images and sets of synthetic images with variations in different aspects. This unique dataset allows us to study the exact impact of those variations towards the generalisation on real images. However, data alone is only half the picture. Additionally, we study the impact of a number of deep learning techniques towards the generalisation on real test sets. The impact is measured by training a number of object detection models on different datasets and configurations while measuring the performance on a real test set.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Models trained on synthetic data often suffer from a decrease in performance on real data. This is due to the domain gap, a term introduced by Tobin <em id="S2.p1.1.1" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.2" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Tobin et al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
Abbeel</a>]</cite>. They argue that it is impossible to perfectly simulate all aspects of a camera and that there will always be a difference between synthetic training data and real test data. They solve this for the task of object localisation by using domain randomisation. This technique randomises as many aspects of the rendering as possible as opposed to trying to accurately simulate the data. Trembley <em id="S2.p1.1.3" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.4" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield</a>]</cite> applied this technique for object detection. Their domain randomised car detection dataset leads to great performance on the KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">Geiger et al.(2012)Geiger, Lenz, and Urtasun</a>]</cite>, even better than the Virtual KITTI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">Gaidon et al.(2016)Gaidon, Wang, Cabon, and Vig</a>]</cite> that was modelled to be similar. This has shown that randomisation can be a substitute for realism.
<br class="ltx_break">A different approach to randomisation is attempting to make the datasets as realistic as possible. Movshovitz-Attias <em id="S2.p1.1.5" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.6" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">Movshovitz-Attias et al.(2016)Movshovitz-Attias, Kanade, and
Sheikh</a>]</cite> investigated how useful photorealism is and what parameters are the most important, for the task of viewpoint estimation. They show that a more complex rendering process is beneficial and that adding synthetic images to a real dataset offers a boost in performance. Additionally, they conclude that randomising lighting parameters leads to better generalisation. Hodan <em id="S2.p1.1.7" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.8" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">Hodaň et al.(2019)Hodaň, Vineet, Gal, Shalev, Hanzelka, Connell,
Urbina, Sinha, and Guenter</a>]</cite> developed a method for generating object detection datasets using physically based rendering (PBR). They show that models trained on PBR datasets perform better than ones trained on datasets created by simpler rendering techniques and that increasing the quality of the PBR leads to better models. Additionally, they show that taking into account the context (gist, geometric, semantic, and illumination contextual aspects) in which the object will be placed improves the performance of the trained network. 
<br class="ltx_break">There are other techniques to improve performance as well. Hinterstoisser <em id="S2.p1.1.9" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.10" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Hinterstoisser et al.(2019)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite> use transfer learning to improve generalisation of models trained on synthetic data. They initialise a network with weights trained on real data and freeze the layers of the feature detector during training. Using this technique, they train a model on a simple synthetic dataset and manage to get performance close to that of a model trained on real data. Nowruzi <em id="S2.p1.1.11" class="ltx_emph ltx_font_italic">et al</em><span id="S2.p1.1.12" class="ltx_ERROR undefined">\bmvaOneDot</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Nowruzi et al.(2019)Nowruzi, Kapoor, Kolhatkar, Hassanat,
Laganière, and Rebut</a>]</cite> investigated the use of real images when training on synthetic data. They show that adding a small amount of real images can be useful and that fine-tuning is better than mixing the data. 
<br class="ltx_break">In our research we use the DIMO dataset. This dataset contains a real dataset, an exact synthetic copy and a number of different variations of the synthetic copy. We can thus investigate what variations are beneficial for generalisation but also research if it is useful to put effort in copying some aspects of the target dataset. This allows us to offer some unique insights that other papers have not yet investigated. Additionally, we attempt to offer a holistic analysis, investigating a number of important concepts for synthetic training data.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>The Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In our experiments we use five subsets of the DIMO datasets. The first subset contains real RGB images, captured with a JAI GO-5000 camera (subset denoted as <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">real</span>). Additionally, we used the four synthetic datasets that have two types of variations. The first is an exact digital twin dataset, where both the poses of the objects as well as the light of the environment match with the real images (<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">synth</span>). The second and third are datasets for which either the poses (<span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_italic">synth, rand pose</span>) or the lighting conditions (<span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_italic">synth, rand light</span>) are randomised, with the non-randomised component matching the real images. The fourth and last dataset of DIMO that we use in our experiments varies both the poses and the lighting conditions (<span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_italic">synth, rand all</span>). The poses of the objects for the real images were manually set in representative and interesting positions. These poses were then manually annotated and used in the generation of the synthetic data. The randomization of the poses in the synthetic data is done by spawning the objects in a uniform random location 30 cm above the carrier. Subsequently, a physics engine simulates the dropping of the objects on the carrier, until they advance into a stable state. This provides a variation in the distribution of poses between the synthetic and real datasets. The light variations were created by iterating over a list of 31 environment maps of indoor scenes. The real light was captured with a HDR 360 image of the environment where the real images where recorded. More details on the creation of this dataset can be found in the original DIMO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">De Roovere et al.(2022)De Roovere, Moonen, Michiels, and
wyffels</a>]</cite> paper. Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 The Dataset ‣ 3 Experimental Setup ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows an example scene from the DIMO dataset, and images from the four different synthetic datasets.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2211.16066/assets/figures/dimo_sample/real.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="114" height="91" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2211.16066/assets/figures/dimo_sample/synth.png" id="S3.F1.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="114" height="91" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2211.16066/assets/figures/dimo_sample/synth_rand_pose.png" id="S3.F1.g3" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="114" height="91" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2211.16066/assets/figures/dimo_sample/synth_rand_light.png" id="S3.F1.g4" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="114" height="91" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_many"><img src="/html/2211.16066/assets/figures/dimo_sample/synth_rand_all.png" id="S3.F1.g5" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="114" height="91" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of an scene in the DIMO dataset to illustrate the different variations. From left to right: Real image, synthetic copy, randomised poses, randomised lighting, both randomised.</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">For the experiments in this paper, we use the first 150 scenes of the DIMO dataset for each of the subsets. Since there is more variation introduced in some datasets, they do not have an equal amount of images. The real and its synthetic twin have around 2k images each, the synthetic datasets with random light or random poses contain around 29k images each and the fully random synthetic dataset contains 78k images. We split each dataset in a training (90%), validation (5%) and test (5%) set. Since the datsets are subdivided in scenes, we ensure that all images from a specific scene belong to the same set. The models are thus tested on unseen scenes.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>The Model</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">Although recently transformers have surpassed convolutional neural networks in object detection performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">Zhang et al.(2022)Zhang, Li, Liu, Zhang, Su, Zhu, Ni, and Shum</a>]</cite>, CNN’s still remain the most popular type of neural networks for vision. This is especially true outside of research. Additionally, a lot of research is still being done in the field of CNN’s, pushing their performance closer to that of transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">Liu et al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
Xie</a>, <a href="#bib.bibx19" title="" class="ltx_ref">Wang et al.(2022)Wang, Bochkovskiy, and Liao</a>]</cite>. We therefore opt to focus our analysis on CNN based feature detectors.
<br class="ltx_break">In our experiments we use the Mask R-CNN model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">He et al.(2017)He, Gkioxari, Dollár, and Girshick</a>, <a href="#bib.bibx1" title="" class="ltx_ref">Abdulla(2017)</a>]</cite>, a widely used object detection and instance segmentation model. It is a two stage architecture consisting of a convolutional feature detection network and detection heads. In this work ResNet101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">He et al.(2016)He, Zhang, Ren, and Sun</a>]</cite> is used as a feature detector. When transfer learning is used throughout this paper, the feature detector is initialised with weights trained on COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
Dollár, and Zitnick</a>]</cite>. Unless mentioned otherwise, the layers of the feature detector are frozen when using transfer learning. In each experiment we train the model for 100 epochs with Stochastic Gradient Descent using a learning rate of <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="float" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">0.001</annotation></semantics></math> and a momentum of <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="float" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">0.9</annotation></semantics></math>. A batch size of four is used and each epoch <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="1.000" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">1.000</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="float" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">1.000</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">1.000</annotation></semantics></math> images are used to train the model. This is done to be able to consistently compare per-epoch model performance between models trained on datasets with different amounts of images. If data augmentation is used this is a combination of zero to two color modifying augmentations and zero to one translating augmentations. The color augmentations include: add, multiply, Gaussian blur, Gaussian noise, motion blur and grayscale; the translating augmentations include: rotation, translation, shear, scale, horizontal flip and perspective transform. More details are provided in the supplementary material.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section we describe and execute a number of experiments using the setup described in Section <a href="#S3" title="3 Experimental Setup ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. To test the performance on the real target domain for each of these experiments, we test the trained models on an unseen set of real images. We compute the AP, AP<sub id="S4.p1.1.1" class="ltx_sub">50</sub> and AP<sub id="S4.p1.1.2" class="ltx_sub">75</sub> values as described for the COCO challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
Dollár, and Zitnick</a>]</cite>. In this section we only report on the AP value as the other metrics follow the same trends. For completeness, the other metrics are provided in the supplementary material.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Scene Composition</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this first experiment we attempt to determine whether it is important to create a synthetic dataset that closely matches the target data in terms of scene compositions and what type of variations are beneficial for generalisation. Should you use lighting conditions and object poses that are plausible for the real test dataset, or can you just make these parameters random? To find out, we trained a Mask R-CNN model from scratch on each of five datasets described in Section <a href="#S3.SS1" title="3.1 The Dataset ‣ 3 Experimental Setup ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Since the datasets have different amounts of images, we also repeated the experiment equalising the sizes of the datasets. Each of the datasets were reduced to the size of the real dataset, which is 1775 images. We sampled random images from the training set. The results are shown in Figure <a href="#S4.F2" title="Figure 2 ‣ 4.1 Scene Composition ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2211.16066/assets/figures/plots/exp1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="210" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Results of the Mask R-CNN model trained on each of the five datasets, with variable and equal dataset sizes, on the real test dataset.</figcaption>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">When looking at the results from the variable size experiment, we can see that the model trained on the real images performs the best on the real test set, this is as expected. The rendered copy of that dataset, with the same poses and lighting conditions, performs the worst and fails to generalise towards the real images. The model trained on the dataset with the same poses but with extra images under varying lighting conditions generalises reasonably well, falling 13 AP points under the model trained on real data. The model trained on the dataset with randomised poses and real lighting conditions performs way worse, only achieving 23.77 AP. This shows that variation in the form of lighting conditions is more beneficial toward generalisation on real data than variation in object poses. The model trained on the fully randomised dataset performs the best of all the synthetic datasets. This dataset is however way larger than the other datasets. 
<br class="ltx_break">We therefore also compare the performance of the models trained on datasets of equal size. Here we see that the model trained on the dataset with randomised light and real poses performs the best of the synthetic datasets. It performs slightly better than the fully randomised dataset. Both of these models still outperform the non randomised synthetic dataset. The model trained on the dataset with real lighting and randomised poses performs the worst with only 5.22 AP. This leads us to conclude that there is a slight benefit to modelling object poses. We confirm our suspicion that varying lighting conditions help toward generalisation on real data and trying to model real light conditions hurts performance. We argue this is due to the fact that it is easier to make higher level features such as shapes and poses match between real and rendered images, it therefore makes sense to try and accurately simulate these features in the synthetic dataset. Lower level features – such as color, lighting and texture – are more difficult to accurately render in a synthetic dataset. We therefore believe it is better to try and randomise these features of the synthetic dataset, as this leads to better generalisation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Training Techniques</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">In our previous experiment, we trained the model from scratch and did not augment our data. This is however not a realistic scenario. It has been shown that transfer learning and data augmentation help improve performance on real data, when trained on synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Hinterstoisser et al.(2019)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite>. We therefore repeat the previous experiment, but now we include transfer learning and data augmentation to analyse their effects on generalisation. For this experiment, we are more focused on the effects of certain training techniques on the different datasets as opposed to comparing the datasets amongst each other. We therefore use all images of each dataset, the models have thus been trained on different sizes of datasets. We include one extra experiment where we restricted each dataset to have the same size as the real dataset, being 1.7k images.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2211.16066/assets/figures/plots/exp2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="200" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Results of training the model on each of the five datasets with data augmentation and/or transfer learning. The horizontal lines indicate the performance of the model trained without these techniques.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 ‣ 4.2 Training Techniques ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the results of these experiments. The horizontal lines represent the performance of the models trained without any of these techniques. For the models trained with only data augmentation, we see that the model trained on real images gets a slight performance boost compared to the model without (3.7 AP). The models trained on the two datasets with randomised lighting experience only a small difference in accuracy and the model trained on the dataset with real poses and random lighting even suffers a small decrease in performance. The two models trained on the datasets with the real lighting conditions experience a large boost in performance. When training the models with transfer learning we observe a large boost in performance for all models. Interestingly, the worst performer of the previous experiment – the synthetic copy dataset – now becomes the best performing synthetic dataset, while having much less images than all the other datasets. When using both techniques we see very similar performance to when only transfer learning is used. Some models even suffer a slight decrease in accuracy. Finally, when we train the model on equally sized datasets using both training techniques, we notice only a very small decrease in performance. This is notable, since the dataset sizes decrease from 26k and 70k to only 1.7k.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2211.16066/assets/figures/plots/exp2_epochs.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="171" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The evolution of the AP on the real test set for models trained without any techniques, with data augmentation and transfer learning.</figcaption>
</figure>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">To further analyse the impact of data augmentation and transfer learning, we compute the AP on the real test set for every two epochs. This allows us to investigate the evolution of the training process. Figure <a href="#S4.F4" title="Figure 4 ‣ 4.2 Training Techniques ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the training process for the model trained without any techniques and for the models trained with either data augmentation or transfer learning. When using no techniques we see that learning flattens of very quickly, especially for the datasets with no variation in lighting. When using data augmentation the model is able to learn for longer and at a better rate. This has a way larger impact on the datasets with no variation in lighting. When using transfer learning, the models for all of the datasets show good performance after one epoch, the learning flattens off very quickly however.
<br class="ltx_break">From these results we conclude that data augmentation and especially transfer learning help overcome the difference in low level features between real and synthetic data. Data augmentation does so by introducing more variation in these features, leading to a more robust model. Transfer learning achieves this by initialising the model with weights that are already capable of detecting low level features from real world images. Thus when using these techniques, it is beneficial to accurately simulate the poses and lighting conditions of the target domain in the synthetic dataset. When it is not possible to model the target domain, one should try to maximise variation in lighting conditions to achieve the best generalisation. Additionally, it is not necessary to use a large amount of images, even when using a randomised dataset. To confirm this, we trained models on a number of subsequently smaller subsets of the full random synthetic dataset. The results, shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.2 Training Techniques ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, indicate that adding more images only helps until a certain amount as we see a peak at 20k images. The differences in AP are not big, showing that only a few thousand images can already produce a decent model.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Image Count</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">1755</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">69.42</td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">4387</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">72.23</td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">8775</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">72.58</td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">17550</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S4.T1.1.5.4.2.1" class="ltx_text ltx_font_bold">73.01</span></td>
</tr>
<tr id="S4.T1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">35100</th>
<td id="S4.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">72.01</td>
</tr>
<tr id="S4.T1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">70200</th>
<td id="S4.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">71.52</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Results of training the model on different amounts of images sampled from the fully random dataset.</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Transfer Learning</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">So far we have shown that transfer learning is very helpful when training on synthetic data. Transfer learning can be done in multiple ways however. In our previous experiments we initialised the feature detector with weights trained on COCO, froze those layers and only trained the network heads. This forces the network to make predictions based on features learned from the COCO dataset, possibly leading to a decrease in performance. It is also possible to retrain parts of the feature detector, allowing the network to learn new features from the dataset. To investigate which layers we can retrain without losing the benefits of transfer learning, we train a number of models with different parts of the feature detector frozen. We train a model starting from the 3rd, 4th and 5th ResNet stage and we perform an experiment where we retrain all layers. For this experiment, we only use the fully random synthetic dataset. Networks are again initialised with a model pre-trained on COCO and data augmentation is used.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Layers Retrained</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">AP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt">All</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold">81.26</span></td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Stage 3+</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">76.71</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Stage 4+</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">80.77</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<th id="S4.T2.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r">Stage 5+</th>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">77.13</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<th id="S4.T2.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r">Heads</th>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">71.52</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance in AP on the real test set of models trained with transfer learning, but with different layers retrained.</figcaption>
</figure>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">The results for this experiment are shown in Table <a href="#S4.T2" title="Table 2 ‣ 4.2.1 Transfer Learning ‣ 4.2 Training Techniques ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The best performing model is the model where all layers were retrained. The model where only the detection heads were retrained has the worst performance, falling at least five AP points below the other models. This shows us that while it is useful to initialise a network with transfer learning, it is important to let the network learn new features from the synthetic dataset as well.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Leveraging Real Images</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">So far we have only considered using strictly synthetic images. It is however sometimes possible that some real images with their labels are available as well. In the following experiments we try to examine whether it is useful to use real images besides synthetic images and how to best use the real images. This can be done in many ways, the two most straightforward and widely used are: mixing the real images with the synthetic training images during training, and fine-tuning a network trained on synthetic images with real images afterwards. We will be testing both these methods. Furthermore, we analyse how many real images to use. We perform tests with different ratios of real to synthetic images, keeping the total amount of images constant at 3510. For these experiments we use the real and fully randomised datasets and train with transfer learning and data augmentation. For the initial training stage we only retrain the heads of the network. When fine-tuning, we retrain the entire network with a lower learning rate of <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="float" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">0.0001</annotation></semantics></math>.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2211.16066/assets/figures/plots/exp5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance in AP on the real test set of models trained with both synthetic and real data. The real data is either mixed in with the synthetic or used for fine tuning. The synthetic and real data is used in different ratios. The horizontal lines represent the performance of models trained on purely synthetic and real data, taken from previous experiments.</figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">In Figure <a href="#S4.F5" title="Figure 5 ‣ 4.3 Leveraging Real Images ‣ 4 Results ‣ Analysis of Training Object Detection Models with Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> the results of the different ratios of synthetic to real data are shown for the different techniques. The horizontal lines represent the performance of the models trained on the real and fully random datasets. When mixing real images with the synthetic dataset we see that adding only a small amount already gives a performance boost. Adding a larger ratio of real images improves the performance even more, until the five to one ratio. The best performing model in this strategy achieves an AP of 80.13, which is an improvement over the model trained on real data. The models trained by fine-tuning on real data see a large benefit from this technique. Even when using only a small amount of real images the performance increases to 82.05 AP, this is 10 AP points above the model trained on the large random synthetic dataset and almost five AP points above the model trained on real data. Increasing the ratio of real images slightly increases the performance for the fine-tuning, reaching 83.7 AP for the one to one ratio.
<br class="ltx_break">From these results we can conclude that combining real and synthetic images can lead to an increase in performance compared to training on only one of the two. Additionally, we find that fine-tuning is the best way to use real images and that only a small amount of real images can already make a significant difference.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Throughout this paper we defined a number of experiments, training object detection models using different techniques on the DIMO dataset. We evaluated all these models, trained on synthetic data, on real data from the same problem domain. The goal was to acquire useful guidelines on how to generate data for deep learning and how to properly use this data. 
<br class="ltx_break">Our experiments offer unique insights in how different variations of synthetic datasets perform on real data, using different training techniques. We show that modelling the lighting conditions and poses of a synthetic dataset to match the real target domain is beneficial towards generalisation, but only if transfer learning is used. When using transfer learning, we show that it is not beneficial to freeze the layers of the feature detector. It is better to retrain the entire network. This is contrary to some current research <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">Hinterstoisser et al.(2019)Hinterstoisser, Lepetit, Wohlhart, and
Konolige</a>]</cite>, so we argue this should be considered on a per-problem basis. Additionally, we investigated how to leverage real images. In our experiments we find that adding a small amount of real images is beneficial and that fine-tuning is the best method to do so. This is in line with the current state of the art <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">Nowruzi et al.(2019)Nowruzi, Kapoor, Kolhatkar, Hassanat,
Laganière, and Rebut</a>]</cite>.
<br class="ltx_break">The Dataset of Industrial Metal Objects is a fairly simple dataset in terms of scene composition, as it contains no unknown objects and covers only a limited range of camera positions. Yet, it is the only dataset that includes the kind of controlled variations needed for these experiments and the scenes depicted in the dataset are highly relevant for industrial applications. We therefore believe that the recommendations made in this paper could serve as guidelines to generate data and train models for new problem domains. Future research should try to extrapolate these findings to different and more complex domains by generating other datasets with these controlled variations.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This study was supported by the Special Research Fund (BOF) of Hasselt University. The mandate ID is BOF20OWB24. Research was done in alignment with Flanders Make’s PILS SBO project (R-9874).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Abdulla(2017)]</span>
<span class="ltx_bibblock">
Waleed Abdulla.

</span>
<span class="ltx_bibblock">Mask r-cnn for object detection and instance segmentation on keras
and tensorflow.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/matterport/Mask_RCNN" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/matterport/Mask_RCNN</a>, 2017.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Bai et al.(2020)Bai, Li, Yang, Song, Li, and Zhang]</span>
<span class="ltx_bibblock">
Qiang Bai, Shaobo Li, Jing Yang, Qisong Song, Zhiang Li, and Xingxing Zhang.

</span>
<span class="ltx_bibblock">Object detection recognition and robot grasping based on machine
learning: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bibx2.1.1" class="ltx_emph ltx_font_italic">IEEE Access</em>, 8:181855–181879, 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ACCESS.2020.3028740" title="" class="ltx_ref">10.1109/ACCESS.2020.3028740</a>.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[De Roovere et al.(2022)De Roovere, Moonen, Michiels, and
wyffels]</span>
<span class="ltx_bibblock">
Peter De Roovere, Steven Moonen, Nick Michiels, and Francis wyffels.

</span>
<span class="ltx_bibblock">Dataset of industrial metal objects, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2208.04052" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2208.04052</a>.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Gaidon et al.(2016)Gaidon, Wang, Cabon, and Vig]</span>
<span class="ltx_bibblock">
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig.

</span>
<span class="ltx_bibblock">Virtualworlds as proxy for multi-object tracking analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 4340–4349, 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/CVPR.2016.470" title="" class="ltx_ref">10.1109/CVPR.2016.470</a>.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Geiger et al.(2012)Geiger, Lenz, and Urtasun]</span>
<span class="ltx_bibblock">
Andreas Geiger, Philip Lenz, and Raquel Urtasun.

</span>
<span class="ltx_bibblock">Are we ready for autonomous driving? the kitti vision benchmark
suite.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, 2012.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[He et al.(2016)He, Zhang, Ren, and Sun]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun.

</span>
<span class="ltx_bibblock">Deep residual learning for image recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx6.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, pages 770–778, Los Alamitos, CA, USA, jun 2016. IEEE
Computer Society.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/CVPR.2016.90" title="" class="ltx_ref">10.1109/CVPR.2016.90</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90</a>.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[He et al.(2017)He, Gkioxari, Dollár, and Girshick]</span>
<span class="ltx_bibblock">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Mask r-cnn.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.1.1" class="ltx_emph ltx_font_italic">2017 IEEE International Conference on Computer Vision
(ICCV)</em>, pages 2980–2988, 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICCV.2017.322" title="" class="ltx_ref">10.1109/ICCV.2017.322</a>.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hinterstoisser et al.(2019)Hinterstoisser, Lepetit, Wohlhart, and
Konolige]</span>
<span class="ltx_bibblock">
Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, and Kurt Konolige.

</span>
<span class="ltx_bibblock">On pre-trained image features and synthetic images for deep learning.

</span>
<span class="ltx_bibblock">In Laura Leal-Taixé and Stefan Roth, editors, <em id="bib.bibx8.1.1" class="ltx_emph ltx_font_italic">Computer
Vision – ECCV 2018 Workshops</em>, pages 682–697, Cham, 2019. Springer
International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-030-11009-3.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Hodaň et al.(2019)Hodaň, Vineet, Gal, Shalev, Hanzelka, Connell,
Urbina, Sinha, and Guenter]</span>
<span class="ltx_bibblock">
Tomáš Hodaň, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon Hanzelka, Treb
Connell, Pedro Urbina, Sudipta N. Sinha, and Brian Guenter.

</span>
<span class="ltx_bibblock">Photorealistic image synthesis for object instance detection.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx9.1.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on Image Processing
(ICIP)</em>, pages 66–70, 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/ICIP.2019.8803821" title="" class="ltx_ref">10.1109/ICIP.2019.8803821</a>.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Lin et al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
Dollár, and Zitnick]</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars,
editors, <em id="bib.bibx10.1.1" class="ltx_emph ltx_font_italic">Computer Vision – ECCV 2014</em>, pages 740–755, Cham, 2014.
Springer International Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-319-10602-1.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Liu et al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
Xie]</span>
<span class="ltx_bibblock">
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
and Saining Xie.

</span>
<span class="ltx_bibblock">A convnet for the 2020s, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2201.03545" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2201.03545</a>.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Movshovitz-Attias et al.(2016)Movshovitz-Attias, Kanade, and
Sheikh]</span>
<span class="ltx_bibblock">
Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh.

</span>
<span class="ltx_bibblock">How useful is photo-realistic rendering for visual learning?

</span>
<span class="ltx_bibblock">In Gang Hua and Hervé Jégou, editors, <em id="bib.bibx12.1.1" class="ltx_emph ltx_font_italic">Computer Vision
– ECCV 2016 Workshops</em>, pages 202–217, Cham, 2016. Springer International
Publishing.

</span>
<span class="ltx_bibblock">ISBN 978-3-319-49409-8.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Northcutt et al.(2021)Northcutt, Athalye, and Mueller]</span>
<span class="ltx_bibblock">
Curtis G Northcutt, Anish Athalye, and Jonas Mueller.

</span>
<span class="ltx_bibblock">Pervasive label errors in test sets destabilize machine learning
benchmarks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx13.1.1" class="ltx_emph ltx_font_italic">Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1)</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=XccDXrDNLek" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=XccDXrDNLek</a>.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Nowruzi et al.(2019)Nowruzi, Kapoor, Kolhatkar, Hassanat,
Laganière, and Rebut]</span>
<span class="ltx_bibblock">
Farzan Erlik Nowruzi, Prince Kapoor, Dhanvin Kolhatkar, Fahed Al Hassanat,
Robert Laganière, and Julien Rebut.

</span>
<span class="ltx_bibblock">How much real data do we actually need: Analyzing object detection
performance using synthetic and real data.

</span>
<span class="ltx_bibblock"><em id="bib.bibx14.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/1907.07061, 2019.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Sreenu and Durai(2019)]</span>
<span class="ltx_bibblock">
G. Sreenu and M A Durai.

</span>
<span class="ltx_bibblock">Intelligent video surveillance: a review through deep learning
techniques for crowd analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bibx15.1.1" class="ltx_emph ltx_font_italic">Journal of Big Data</em>, 6:48, 06 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1186/s40537-019-0212-5" title="" class="ltx_ref">10.1186/s40537-019-0212-5</a>.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tobin et al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
Abbeel]</span>
<span class="ltx_bibblock">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural networks from
simulation to the real world.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx16.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</em>, pages 23–30, 2017.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.1109/IROS.2017.8202133" title="" class="ltx_ref">10.1109/IROS.2017.8202133</a>.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tommasi et al.(2017)Tommasi, Patricia, Caputo, and
Tuytelaars]</span>
<span class="ltx_bibblock">
Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars.

</span>
<span class="ltx_bibblock">A deeper look at dataset bias.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.1.1" class="ltx_emph ltx_font_italic">Domain adaptation in computer vision applications</em>, pages
37–55. Springer, 2017.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Tremblay et al.(2018)Tremblay, Prakash, Acuna, Brophy, Jampani, Anil,
To, Cameracci, Boochoon, and Birchfield]</span>
<span class="ltx_bibblock">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition workshops</em>, pages 969–977, 2018.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Wang et al.(2022)Wang, Bochkovskiy, and Liao]</span>
<span class="ltx_bibblock">
Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao.

</span>
<span class="ltx_bibblock">Yolov7: Trainable bag-of-freebies sets new state-of-the-art for
real-time object detectors, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2207.02696" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2207.02696</a>.

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Yang et al.(2020)Yang, Li, Wang, Dong, Wang, and
Tang]</span>
<span class="ltx_bibblock">
Jing Yang, Shaobo Li, Zheng Wang, Hao Dong, Jun Wang, and Shihao Tang.

</span>
<span class="ltx_bibblock">Using deep learning to detect defects in manufacturing: A
comprehensive survey and current challenges.

</span>
<span class="ltx_bibblock"><em id="bib.bibx20.1.1" class="ltx_emph ltx_font_italic">Materials</em>, 13(24), 2020.

</span>
<span class="ltx_bibblock">ISSN 1996-1944.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https:/doi.org/10.3390/ma13245755" title="" class="ltx_ref">10.3390/ma13245755</a>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.mdpi.com/1996-1944/13/24/5755" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/1996-1944/13/24/5755</a>.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Zhang et al.(2022)Zhang, Li, Liu, Zhang, Su, Zhu, Ni, and Shum]</span>
<span class="ltx_bibblock">
Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and
Heung-Yeung Shum.

</span>
<span class="ltx_bibblock">Dino: Detr with improved denoising anchor boxes for end-to-end object
detection, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2203.03605" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2203.03605</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.16065" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.16066" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.16066">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.16066" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.16067" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 08:23:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
