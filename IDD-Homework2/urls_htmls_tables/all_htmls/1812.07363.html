<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1812.07363] Improving Face Detection Performance with 3D-Rendered Synthetic Data</title><meta property="og:description" content="In this paper, we provide a synthetic data generator methodology with fully controlled, multifaceted variations based on a new 3D face dataset (3DU-Face). We customized synthetic datasets to address specific types of v…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Improving Face Detection Performance with 3D-Rendered Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Improving Face Detection Performance with 3D-Rendered Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1812.07363">

<!--Generated on Fri Mar  1 17:27:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Improving Face Detection Performance with 3D-Rendered Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Han<sup id="id7.2.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Sezer Karaoglu<sup id="id8.2.id1" class="ltx_sup"><span id="id8.2.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Hoang-An Le<sup id="id9.2.id1" class="ltx_sup">1</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Theo Gevers<sup id="id10.3.id1" class="ltx_sup"><span id="id10.3.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>
<br class="ltx_break"><sup id="id11.4.id2" class="ltx_sup">1</sup>Computer Vision Lab
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> University of Amsterdam
<br class="ltx_break"><sup id="id12.2.id1" class="ltx_sup">2</sup>3DUniversum
<br class="ltx_break"><span id="id13.3.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{j.han, h.a.le, s.karaoglu, th.gevers}@uva.nl
<br class="ltx_break"></span>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id14.id1" class="ltx_p">In this paper, we provide a synthetic data generator methodology with fully controlled, multifaceted variations based on a new 3D face dataset (3DU-Face). We customized synthetic datasets to address specific types of variations (scale, pose, occlusion, blur, etc.), and systematically investigate the influence of different variations on face detection performances. We examine whether and how these factors contribute to better face detection performances. We validate our synthetic data augmentation for different face detectors (Faster RCNN, SSH and HR) on various face datasets (MAFA, UFDD and Wider Face).</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Face detection is one of the important topics in the field of computer vision. It plays a fundamental role in basically all face related applications. Face detection is the problem of determining the presence of faces in images and their precise locations. Face detection is confronted with different challenges such as variations in scale, pose, expression, occlusion and illumination which all may have a negative influence on the performance of face detection methods. In Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we summarized the different characteristics of various face detection benchmarks. From the table, it can be derived that many datasets are limited in representing extreme poses, different scales and heavy occlusions. However, datasets containing face images under a wide variety of imaging conditions are required to develop face detectors which are robust to all variations of image formation process.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Face detectors are designed to address only a limited set of variations in real-world situations. For example, FAN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> uses an attention based structure and data augmentation to cope with facial occlusion. PCN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> proposes rotation-invariant face detection in a coarse-to-fine manner by dividing the calibration process into several progressive steps. The HR detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> combines both features and image pyramids to make the algorithm robustness against extreme face scales. In Table <a href="#S1.T2" title="Table 2 ‣ 1 Introduction ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show that different face detectors are designed to cope with different imaging conditions. These face detectors heavily rely on the availability of large-scale annotated datasets. Collecting and annotating real-world datasets with different imaging conditions is tedious, time consuming and in some cases even unfeasible. Furthermore, it is difficult to systematically vary the imaging parameters and to avoid errors during the annotation process. Errors in ground truth may lead to far-reaching impact on training and testing of the networks. Therefore, our contribution is to generate synthetic data, as complementary to real data, to create fully controlled datasets by means of automatic and error-less annotation. To validate our methodology, we train different face detectors on a combination of real data and fully controlled synthetic dataset, to systematically address the imaging variations. Our synthetic images are rendered versions of real 3D faces with changes in viewpoint, scale, illumination, occlusion and background. Hence, the variation of imaging conditions is performed in 3D space.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our contributions are (1) we provide a new face dataset (3DU-Face) with a large variety of imaging conditions such as scale, pose, occlusion, blur, etc., (2) large scale experiments to systematically study the impact of data augmentation on the performance of face detection, (3) a comparative study of state-of-the-art face detectors (Faster RCNN, SSH and HR) on different face benchmarks (MAFA, UFDD and Wider Face).</p>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><svg version="1.1" height="18.92" width="104.46" overflow="visible"><g transform="translate(0,18.92) scale(1,-1)"><path d="M 0,18.92 104.46,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="45.59" height="9.46" overflow="visible">
<span id="S1.T1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S1.T1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S1.T1.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Feature</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(52.23,9.46)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="52.23" height="9.46" overflow="visible">
<span id="S1.T1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S1.T1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S1.T1.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Datasets</span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">MAFA</th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">UFDD</th>
<th id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Wider</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">landmark occlusion</td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">complex background</td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td"></td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td"></td>
<td id="S1.T1.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme pose</td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td"></td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td"></td>
<td id="S1.T1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme scale</td>
<td id="S1.T1.1.1.5.4.2" class="ltx_td"></td>
<td id="S1.T1.1.1.5.4.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">heavy occlusion</td>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.6.5.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">blur</td>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.7.6.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme illumination</td>
<td id="S1.T1.1.1.8.7.2" class="ltx_td"></td>
<td id="S1.T1.1.1.8.7.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T1.1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">misleading objects</td>
<td id="S1.T1.1.1.9.8.2" class="ltx_td ltx_border_b"></td>
<td id="S1.T1.1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_b">✓</td>
<td id="S1.T1.1.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">✓</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Three face detection benchmarks and their characteristics.
</figcaption>
</figure>
<figure id="S1.T2" class="ltx_table">
<table id="S1.T2.1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T2.1.1.1" class="ltx_tr">
<th id="S1.T2.1.1.1.1" class="ltx_td ltx_nopad ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><svg version="1.1" height="18.92" width="104.24" overflow="visible"><g transform="translate(0,18.92) scale(1,-1)"><path d="M 0,18.92 104.24,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="45.59" height="9.46" overflow="visible">
<span id="S1.T2.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S1.T2.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S1.T2.1.1.1.1.pic1.1.1.1.1" class="ltx_p">Feature</span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(52.12,9.46)"><g transform="translate(0,9.46) scale(1, -1)"><foreignObject width="52.12" height="9.46" overflow="visible">
<span id="S1.T2.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S1.T2.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S1.T2.1.1.1.1.pic1.2.1.1.1" class="ltx_p">Detector</span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S1.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Faster RCNN</th>
<th id="S1.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">SSH</th>
<th id="S1.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">HR</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T2.1.1.2.1" class="ltx_tr">
<td id="S1.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">landmark occlusion</td>
<td id="S1.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt">✓</td>
<td id="S1.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
</tr>
<tr id="S1.T2.1.1.3.2" class="ltx_tr">
<td id="S1.T2.1.1.3.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">complex background</td>
<td id="S1.T2.1.1.3.2.2" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T2.1.1.3.2.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T2.1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.4.3" class="ltx_tr">
<td id="S1.T2.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme pose</td>
<td id="S1.T2.1.1.4.3.2" class="ltx_td"></td>
<td id="S1.T2.1.1.4.3.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T2.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.5.4" class="ltx_tr">
<td id="S1.T2.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme scale</td>
<td id="S1.T2.1.1.5.4.2" class="ltx_td"></td>
<td id="S1.T2.1.1.5.4.3" class="ltx_td ltx_align_center">✓</td>
<td id="S1.T2.1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.6.5" class="ltx_tr">
<td id="S1.T2.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">heavy occlusion</td>
<td id="S1.T2.1.1.6.5.2" class="ltx_td"></td>
<td id="S1.T2.1.1.6.5.3" class="ltx_td"></td>
<td id="S1.T2.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.7.6" class="ltx_tr">
<td id="S1.T2.1.1.7.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">blur</td>
<td id="S1.T2.1.1.7.6.2" class="ltx_td"></td>
<td id="S1.T2.1.1.7.6.3" class="ltx_td"></td>
<td id="S1.T2.1.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.8.7" class="ltx_tr">
<td id="S1.T2.1.1.8.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">extreme illumination</td>
<td id="S1.T2.1.1.8.7.2" class="ltx_td"></td>
<td id="S1.T2.1.1.8.7.3" class="ltx_td"></td>
<td id="S1.T2.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">✓</td>
</tr>
<tr id="S1.T2.1.1.9.8" class="ltx_tr">
<td id="S1.T2.1.1.9.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">misleading objects</td>
<td id="S1.T2.1.1.9.8.2" class="ltx_td ltx_border_b"></td>
<td id="S1.T2.1.1.9.8.3" class="ltx_td ltx_border_b"></td>
<td id="S1.T2.1.1.9.8.4" class="ltx_td ltx_border_b ltx_border_r"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Three advanced face detectors and their characteristics.
</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Face Detection</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Often face detection is considered as a special case of object detection. Object detectors are, in general, categorized as one-step (e.g. SSD, YOLO) and two-step (e.g. Faster R-CNN) detectors. Two-step detectors mostly use region proposals and classification, while one-step detectors only rely on single feed-forward convolutional networks (without classification).</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Most of the face detectors are designed to address specific variations in real-world scenarios e.g. scale<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, occlusion<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, pose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> or lighting condition<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>. Therefore, face detectors are mostly suitable for datasets with corresponding characteristics and may lack generalization power (cross datasets).
For example, a single face detector may be restricted in handling a wide range of face scales (e.g., 10 px vs. 1000 px tall faces). Therefore, HR uses extremely large receptive fields to locate tiny faces. It also applies multi-scale testing by using an image pyramid to capture extreme scale features<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. To detect occluded faces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, a local linear embedding method is used to reduce noise and recover the lost cues. For blurry scenes, Bai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> propose a GAN based super-resolution and refinement network framework. It restores high-resolution faces from blurry ones. For face detection, blurry and low-resolution faces only have few features to extract from. Also, the boundary between the object and background is often difficult to distinguish.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Face Synthesis</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Synthetic data is useful to improve the performance in face related applications<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. The generation of synthetic face images can be achieved by face editing methods including shape morphing<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, relighting<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, pose normalization<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, and expression modification<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Recently, GAN-based methods provide realistic results of facial attribute manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> but they are bounded by the limitations of the training images. Training images merely cover a narrow range of variations, and may cause artifacts during generation. Face synthesis methods are widely used in face recognition tasks<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, because such tasks rely on extensive face attribute information. However, in our paper, we focus on generating face images for face detection also considering extreme variations such as large scale and heavy blur. Existing datasets mostly contain subtle face attribute manipulation. Further, existing methods directly manipulate faces in 2D. This may hinder their application in handling extreme imaging conditions. Our synthetic images are rendered versions of real 3D faces. Data generation in 3D space enables the process of including extreme changes in viewpoint, scale, illumination, occlusion and background.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Influence of detection characteristics</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">The performance of detectors are affected by both network architecture and object characteristics. Hoiem et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> provides an extensive analysis on the influence of different variations on different detectors. Another comparison study<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> focuses on the trade-off between speed and performance of meta-architecture detectors.
Karaoglu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> exploits the correlation between different object detectors by means of high-level contextual information. In this paper, we focus on faces and their characteristics. The aim is to study the influence of different data augmentation methods on face detection performance.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/1812.07363/assets/x1.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="217" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of rendering pipeline. It shows the generation of real-synthetic images. First, pose, background and occlusion are manipulated on the original 3D models. Then, the 3D models are converted into 2D images. The figure of the pose variation is taken from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Challenges for Face Detection</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text ltx_font_bold">Pose</span> can significantly change the appearance of a face. Extreme poses may result in heavy occlusion or skewed aspect ratio of the face bounding box. Most (deep) face detectors use augmented data by rotating faces over different angles, or jointly estimate the pose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to obtain robustness against pose variations<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. PCN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> calibrates the orientation of faces to upright at different stages progressively.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Scale</span> changes may have a negative influence on the performance of face detection. For example, the image features for a 10px face are essentially different than of a 1000px sized face. Combining feature pyramids and multi-scale testing is used to detect faces of extreme scales<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Context</span> information can play a crucial role in determining the precise location of faces. Faces in unconstrained settings may be surrounded or occluded by different objects. Round-shaped, background objects may result in false positives. For example, HR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> uses large-scale context information to locate tiny faces. SSH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> applies a context module to effectively use background features.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Facial occlusion</span> may obstruct the presence of valuable information for detection<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Facial occlusion can be divided into two different categories: landmark occlusion and heavy occlusion. Landmark occlusion means that only a few landmarks like eyes or mouths are occluded, while most of the face is still visible. In contrast, heavy occlusion means that more than half of the face is missing due to occlusion, image border or extreme pose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Or that a face is occluded by another face.</p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Illumination</span> changes may substantially influence the appearance of faces. For example, it is difficult to distinguish faces, under extreme lighting conditions, from the background. Zhou et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> uses multi-spectrum sensing to detect faces under low lighting conditions.</p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Blur and Low resolution</span> usually impede face detectors from retrieving available information. For example, images may be distorted in collection, storage, or transmission, leading to degraded quality of images<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>. In some extreme cases, only the outline of the faces can be identified. Bai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> use GANs to refine blurry faces to improve performance. Refinement network or multi-scale testing are feasible solutions to detect blurry or low-resolution faces.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Face Detection: Datasets</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we discuss a number of well-known face detection benchmarks and their characteristics.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">MAFA</span>
is a representative dataset of face images with occlusion. The dataset is mainly composed of occluded samples using different types of occlusions<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. To cope with the interference of pose, MAFA only includes a narrow range of head poses. MAFA has three types of annotations in the dataset: masked, unmasked and ignored. Faces are extremely blurry or tiny where faces with a side length of less than 32 pixels are labeled as ’Ignored’.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">UFDD</span> contains faces in different weather conditions and other challenging variations concerning lens impediments, motion blur and defocus blur<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Additionally, it has a collection of distracting images. For the UFDD dataset, the most challenging part is the extreme lighting and blur.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Wider Face</span> is the most challenging benchmark for face detection  <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. It includes various events (e.g., basketball, football) with a variety of backgrounds. The large number of faces contain extreme poses, exaggerated expressions, heavy occlusion and extreme lighting conditions. The most challenging part of Wider Face is the extreme scale. Wider Face has three categories of difficulty: easy, medium, and hard. The criteria to categorize faces into these different categories are vague. Our Table <a href="#S4.T3" title="Table 3 ‣ 4 Face Detection: Datasets ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the basic characteristics of faces, irrespective of invalid faces, in the Wider Face validation partition.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Partition</th>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Large</td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Medium</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Tiny</td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<th id="S4.T3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">Height</th>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">50-400(96.6%)</td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">30-50(99%)</td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">10-30(99%)</td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<th id="S4.T3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Width</th>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">20-300(96.3%)</td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">10-70(99.7%)</td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">8-20(95%)</td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<th id="S4.T3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">Number</th>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">7211</td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">6108</td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">18636</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Face scale information of the validation set in Wider Face. We distinguish three face categories based on height and width. Proportion information represents the percentage of faces that fits within the scale interval.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Face Detectors</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">In this section, we outline the different face detection algorithms used for comparison.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">Faster RCNN</span> is one of the mostly used object detectors in the literature. It is not designed to be robust against challenging variations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> for face detection.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">SSH</span> is an extremely fast one-step face detector. It is designed to be scale invariant <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. To accelerate the inference process, it removes selectively a number of parameters from the structure. This strategy has a negative influence on the detector’s performance. SSH requires additional multi-scale processing to detect faces with extreme scales.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">HR</span> face detector performs well on tiny faces by using wide-range contextual information and using testing on multiple resolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. Its architecture resembles RPN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and uses both feature pyramids and image pyramids. However, HR face detector trained on Wider Face is extremely sensitive to tiny, round objects in the background. HR heavily relies on contextual information to locate faces. For faces with limited information (e.g., heavily occluded, extremely small or blurry), complex background may hinder precise detection.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1812.07363/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="260" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Performance comparison on the basic settings of our training data. It includes the effect of training data size on both (a) Wider Face validation set and (b) MAFA test set. (c) and (d) respectively show the effects of objects and background on Wider Face validation with HR.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Data Augmentation</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Data augmentation is based on our new 3D face dataset (3DU-Face). It contains 700 3D face mesh models with high-resolution texture of 435 different individuals. Some people may have multiple recordings taken at different times and imaging conditions. Most of the 3D models are taken in the wild under uncontrolled conditions. The 3D models contain 50 facial landmarks annotated by human experts.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Using the 3D models, the 2D images (projections) are rendered. The rendering pipeline is built in Blender 2.78. To change the viewpoint, the model is rotated over different Euler angles. The camera is kept in the same position. The parameters of pitch, yaw and roll are selected randomly using different ranges. For face scale variation, we change the distance between camera and the face models within a fixed range. The ground truth for face detection is generated from the 3D landmarks. The bounding box is generated to tightly encompass the forehead, chin, and cheek. The size of the faces is larger than <math id="S5.p2.1.m1.1" class="ltx_Math" alttext="10\times 8" display="inline"><semantics id="S5.p2.1.m1.1a"><mrow id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml"><mn id="S5.p2.1.m1.1.1.2" xref="S5.p2.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S5.p2.1.m1.1.1.1" xref="S5.p2.1.m1.1.1.1.cmml">×</mo><mn id="S5.p2.1.m1.1.1.3" xref="S5.p2.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><apply id="S5.p2.1.m1.1.1.cmml" xref="S5.p2.1.m1.1.1"><times id="S5.p2.1.m1.1.1.1.cmml" xref="S5.p2.1.m1.1.1.1"></times><cn type="integer" id="S5.p2.1.m1.1.1.2.cmml" xref="S5.p2.1.m1.1.1.2">10</cn><cn type="integer" id="S5.p2.1.m1.1.1.3.cmml" xref="S5.p2.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">10\times 8</annotation></semantics></math> pixels.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">In this paper, data is created that include face occlusion. Current approaches focus on (1) cropping face images or (2) using GAN to generate. However, both approaches have drawbacks. Cropping will reduce the information of face and may not provide sufficient generalization in case of real occlusion samples. GAN-based methods are able to generate subtle face attributes. However, for more global image/face changes, GAN may generate blurry results and artifacts. Therefore, in this paper, our aim is to generate face images in 3D space. We randomly add different 3D objects like sunglasses, hats, and helmets in the 3D space before rendering. All objects are placed at a selective locations to simulate landmark occlusion. To simulate occlusion, face regions are divided into three different face parts: head, eye and mouth. More than 1000 different combinations of occlusion are generated for each model.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Experiments are conducted to systematically study the influence of data augmentation on the performance of face detection. We compare three face detection methods on the following face detection benchmarks: MAFA, UFDD and Wider Face. The face detection methods are Faster RCNN<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, SSH<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and HR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.</p>
</div>
<figure id="S6.F3" class="ltx_figure"><img src="/html/1812.07363/assets/x3.png" id="S6.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Performance comparison on pose variation. Across pitch, yaw, and roll, different colors represent the rotated degree, as labeled at the top right (e.g., ”15” means ”-15” to ”15”.) </figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Implementation Details</h3>

<section id="S6.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Settings of rendering process</h4>

<div id="S6.SS1.SSS1.p1" class="ltx_para">
<p id="S6.SS1.SSS1.p1.1" class="ltx_p">This section is to demonstrate the basic settings of our rendering process . The 3D models are not changed in terms of shape or texture in experiments. We choose 100 fixed 3D models as experimental subjects and set the default for pitch, roll and yaw randomly in ranges of (-15, 15), (-15, 15), (-60, 60), respectively. For each face model, the rotation origin is the center of its landmarks, irrespective of invalid landmarks. All the face models are aligned by using landmarks with an anchor model. The anchor model is aligned to the global axis in Blender. Within a fixed range (from 1 to 48 faces), we randomize the number of faces in each image. The distance between each model and camera is randomly selected within the range of 0 to 20 meters. The size of the faces is larger than <math id="S6.SS1.SSS1.p1.1.m1.1" class="ltx_Math" alttext="10\times 8" display="inline"><semantics id="S6.SS1.SSS1.p1.1.m1.1a"><mrow id="S6.SS1.SSS1.p1.1.m1.1.1" xref="S6.SS1.SSS1.p1.1.m1.1.1.cmml"><mn id="S6.SS1.SSS1.p1.1.m1.1.1.2" xref="S6.SS1.SSS1.p1.1.m1.1.1.2.cmml">10</mn><mo lspace="0.222em" rspace="0.222em" id="S6.SS1.SSS1.p1.1.m1.1.1.1" xref="S6.SS1.SSS1.p1.1.m1.1.1.1.cmml">×</mo><mn id="S6.SS1.SSS1.p1.1.m1.1.1.3" xref="S6.SS1.SSS1.p1.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p1.1.m1.1b"><apply id="S6.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1"><times id="S6.SS1.SSS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1.1"></times><cn type="integer" id="S6.SS1.SSS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1.2">10</cn><cn type="integer" id="S6.SS1.SSS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p1.1.m1.1c">10\times 8</annotation></semantics></math> pixels. 50 HDR images (no humans) are taken from Shape Net <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and used as backgrounds. These images provide environment lighting and background variations for the synthetic images. The background dataset includes both indoor and outdoor scenes. Back face culling is applied to avoid artifacts in the rendered results.</p>
</div>
</section>
<section id="S6.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Settings of the face detectors</h4>

<div id="S6.SS1.SSS2.p1" class="ltx_para">
<p id="S6.SS1.SSS2.p1.1" class="ltx_p">In the following experiments, we test our data augmentation methods with different face detectors on three benchmarks. We first train the face detectors on synthetic data and test them on real data. We also validate the methods on a subset of real data for the training part. Every dataset has its own domain. There are many different parameters in our rendering pipeline. It is unlikely to find the optimal setting for a specific real dataset. However, after comparing the performance on real data with different rendering parameters, we attain suitable and effective configurations for testing. We use the augmented synthetic data to improve the performance for real data. Different face detectors have their own approaches to augment data, like flipping, cropping, or transforming images. For fair comparison, we keep their original operations and hyper-parameters. For SSH and HR, we deploy their algorithms on one single GPU<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. For Faster RCNN, we use the implementation from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>The influence of data augmentation</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In this section, we study the influence of various data augmentation formats. For this purpose, HR is considered. We do not employ Faster RCNN or SSH for extensive analysis but we rather use them to test performance of data augmentation. Faster RCNN is a generic detector without multi-scale testing, of which the performance may not reflect all the changes in variations. SSH is designed to be an scale-invariant one-step detector. For all experiments, we study one variation at the time. The other variations are kept the same.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p"><span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_bold">The influence of basic training data settings</span>:
We change the number of images to test the influence of training dataset size. As shown in Figure <a href="#S6.F4" title="Figure 4 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.(a), the increase of synthetic images continuously improves the performance on hard, but not easy and medium, levels. This is because more than half of the faces, for the hard level, are tiny faces. In the training process, large faces generate much more positive samples than tiny faces. Hence, larger training samples are more useful for tiny faces. Simply increasing the number of synthetic images may lead to over-fitting. Because most of faces from the Wider Face dataset contain extreme scales, a test is performed on MAFA to examine our conclusion about the (training) dataset size in Figure <a href="#S6.F4" title="Figure 4 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.(b). It shows that the performance on both occluded faces and full datasets (including occluded and unoccluded faces) is saturated after adding more training images. Moreover, background is crucial. HDR images provide higher resolution and less sharper results than real images. HDR images have their own bias in respect to other images; the increase of the number of HDR images does not improve the performance constantly (see Figure <a href="#S6.F4" title="Figure 4 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.(d)). In Figure <a href="#S6.F4" title="Figure 4 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.(c), we study the influence of the number of 3D objects. It shows that the number of 3D models does not strongly influence the performance. This may be because the face detection task does not heavily rely on the attribute or identity information of the faces. Also, our 3D models may have their own bias (e.g. annotation bias, mesh corruption or scanner noise).</p>
</div>
<figure id="S6.F4" class="ltx_figure"><img src="/html/1812.07363/assets/x4.png" id="S6.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="264" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Performance comparison on the basic settings of our render pipeline. It includes the effect of training data size on both (a) Wider Face validation set and (b) MAFA test set. (c) and (d) respectively show the effects of objects and background on Wider Face validation with HR.</figcaption>
</figure>
<div id="S6.SS2.p3" class="ltx_para">
<p id="S6.SS2.p3.1" class="ltx_p"><span id="S6.SS2.p3.1.1" class="ltx_text ltx_font_bold">The influence of pose</span>:
To investigate the effect of head pose, we render faces with different ranges of pitch, roll and yaw, and test them on the Wider Face validation dataset. As shown in Figure <a href="#S6.F3" title="Figure 3 ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, for pitch, roll or yaw, the minimal range gives better performance than others. The reason is that the majority of faces in the dataset do not contain extreme poses. Also, the face detectors need sufficient data to learn the representation of faces. Then, the different portions of extreme orientations are added to the training dataset, see Figure <a href="#S6.F5" title="Figure 5 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.(b). A small range of extreme pose boosts the performance of large and medium faces because most of the tiny faces (hard level) are extremely blurry and pose-agnostic. For the HR detector, most of the tiny faces are detected by multi-scale testing. Features from generic faces are more useful for detecting tiny faces.</p>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p"><span id="S6.SS2.p4.1.1" class="ltx_text ltx_font_bold">The influence of occlusion</span>:
We test two different types of face occlusion. The first type of occlusion is from other objects in the scene. MAFA focuses on the occlusion of face images. We test our augmentation methods for this first type of occlusion on the MAFA test set for three different occlusion settings: the baseline condition with no extra occlusion, landmark occlusion, and mixed occlusion (both landmark and heavy occlusion). For all occluded cases, none of the 2D faces is occluded by parts of the other 3D face models. As shown in Figure <a href="#S6.F5" title="Figure 5 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.(c), the performance on the MAFA test set improves drastically after adding occlusion in the synthetic training dataset. HR becomes more robust after training on synthetic faces with landmark and heavy occlusions. Some occlusion examples from our synthetic data are shown in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.3 Influence of detection characteristics ‣ 2 Related Work ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S6.F5" class="ltx_figure"><img src="/html/1812.07363/assets/x5.png" id="S6.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Performance comparison on other variations. Only (c) tests on MAFA test set, while the remaining are on Wider Face validation set. (a) shows the results of adding different noise levels from the down-sampling or up-sampling process; (b) shows the results after adding small-portions of extreme poses to the training dataset; (c) shows the results of adding different types of occlusion; (d) shows the results of adding occlusion from other faces.</figcaption>
</figure>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p">The second type of occlusion is from other faces or human body parts. We choose the Wider Face validation set, to study how this second type of occlusion in our synthetic training data influences the performance of HR. This is because most images in the Wider Face are acquired using unconstrained settings. Many of the images are group pictures, and each image may have hundreds of tiny faces. We set a threshold for overlapping faces. This is to avoid other large faces to cover the tiny faces. As shown in Figure <a href="#S6.F5" title="Figure 5 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.(d), after adding occlusion from other faces in the training dataset, the performance of HR for hard level in Wider Face validation set improves substantially. The results on the first type of occlusion demonstrates the effectiveness of our occlusion augmentation. Our synthetic data provides suitable noise to simulate the patterns of occlusion. As for the second kind of occlusion, the samples in training dataset are needed for detectors to learn to distinguish the boundary between different faces.</p>
</div>
<div id="S6.SS2.p6" class="ltx_para">
<p id="S6.SS2.p6.1" class="ltx_p"><span id="S6.SS2.p6.1.1" class="ltx_text ltx_font_bold">The influence of noise</span>:
Each benchmark has its own type of configuration. For the Wider Face dataset, the original high-resolution images are downloaded using a search engine and resized to a predetermined width of 1024 pixel. This process introduces noise caused down-sampling or up-sampling. Therefore, we first render images with multiple resolutions (as Set A, B and C as below), and then re-size them to one fixed resolution (1024<math id="S6.SS2.p6.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S6.SS2.p6.1.m1.1a"><mo id="S6.SS2.p6.1.m1.1.1" xref="S6.SS2.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p6.1.m1.1b"><times id="S6.SS2.p6.1.m1.1.1.cmml" xref="S6.SS2.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p6.1.m1.1c">\times</annotation></semantics></math>768). Set A includes multiple high-resolution images (4096, 3072, 2048). Set B has multiple high- and low-resolution images (4096, 3072, 2048, 512, 256, 128). And Set C has multiple low-resolution images(512, 256, 128). We demonstrate the influence of noise for the different difficulty levels of Wider Face in Figure <a href="#S6.F5" title="Figure 5 ‣ 6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.(a). The performance for all the difficulty levels of Wider Face has been improved, especially for tiny faces. Set A achieves the best performance on Wider Face. This is because sampling process is also changing the size of the original faces in the rendered images. The tiny faces for real data are resized from large faces as our operation in Set A. As for Set B and C, a part of the large and medium faces are resized from the original tiny faces in the rendered images.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Performance comparison on synthetic data</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">In Table <a href="#S6.T4" title="Table 4 ‣ 6.3 Performance comparison on synthetic data ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we show the performance comparison about three synthetic data sets on Wider Face validation set. These three synthetic datasets <math id="S6.SS3.p1.1.m1.3" class="ltx_Math" alttext="s_{1},s_{2},s_{3}" display="inline"><semantics id="S6.SS3.p1.1.m1.3a"><mrow id="S6.SS3.p1.1.m1.3.3.3" xref="S6.SS3.p1.1.m1.3.3.4.cmml"><msub id="S6.SS3.p1.1.m1.1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.1.1.2.cmml">s</mi><mn id="S6.SS3.p1.1.m1.1.1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.SS3.p1.1.m1.3.3.3.4" xref="S6.SS3.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS3.p1.1.m1.2.2.2.2" xref="S6.SS3.p1.1.m1.2.2.2.2.cmml"><mi id="S6.SS3.p1.1.m1.2.2.2.2.2" xref="S6.SS3.p1.1.m1.2.2.2.2.2.cmml">s</mi><mn id="S6.SS3.p1.1.m1.2.2.2.2.3" xref="S6.SS3.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S6.SS3.p1.1.m1.3.3.3.5" xref="S6.SS3.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS3.p1.1.m1.3.3.3.3" xref="S6.SS3.p1.1.m1.3.3.3.3.cmml"><mi id="S6.SS3.p1.1.m1.3.3.3.3.2" xref="S6.SS3.p1.1.m1.3.3.3.3.2.cmml">s</mi><mn id="S6.SS3.p1.1.m1.3.3.3.3.3" xref="S6.SS3.p1.1.m1.3.3.3.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.3b"><list id="S6.SS3.p1.1.m1.3.3.4.cmml" xref="S6.SS3.p1.1.m1.3.3.3"><apply id="S6.SS3.p1.1.m1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S6.SS3.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.2">𝑠</ci><cn type="integer" id="S6.SS3.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S6.SS3.p1.1.m1.2.2.2.2.cmml" xref="S6.SS3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.2.2.2.2.1.cmml" xref="S6.SS3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S6.SS3.p1.1.m1.2.2.2.2.2.cmml" xref="S6.SS3.p1.1.m1.2.2.2.2.2">𝑠</ci><cn type="integer" id="S6.SS3.p1.1.m1.2.2.2.2.3.cmml" xref="S6.SS3.p1.1.m1.2.2.2.2.3">2</cn></apply><apply id="S6.SS3.p1.1.m1.3.3.3.3.cmml" xref="S6.SS3.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S6.SS3.p1.1.m1.3.3.3.3.1.cmml" xref="S6.SS3.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S6.SS3.p1.1.m1.3.3.3.3.2.cmml" xref="S6.SS3.p1.1.m1.3.3.3.3.2">𝑠</ci><cn type="integer" id="S6.SS3.p1.1.m1.3.3.3.3.3.cmml" xref="S6.SS3.p1.1.m1.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.3c">s_{1},s_{2},s_{3}</annotation></semantics></math> are combined with real data to improve detection performance for UFDD<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> and Wider Face respectively in Section <a href="#S6.SS4.SSS2" title="6.4.2 UFDD ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.2</span></a> and <a href="#S6.SS4.SSS3" title="6.4.3 Wider Face ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.3</span></a>.</p>
</div>
<figure id="S6.T4" class="ltx_table">
<table id="S6.T4.3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T4.3.3.4.1" class="ltx_tr">
<th id="S6.T4.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Set</th>
<th id="S6.T4.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Easy</th>
<th id="S6.T4.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Medium</th>
<th id="S6.T4.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:4.0pt;padding-right:4.0pt;">Hard</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T4.1.1.1" class="ltx_tr">
<th id="S6.T4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S6.T4.1.1.1.1.m1.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="S6.T4.1.1.1.1.m1.1a"><msub id="S6.T4.1.1.1.1.m1.1.1" xref="S6.T4.1.1.1.1.m1.1.1.cmml"><mi id="S6.T4.1.1.1.1.m1.1.1.2" xref="S6.T4.1.1.1.1.m1.1.1.2.cmml">s</mi><mn id="S6.T4.1.1.1.1.m1.1.1.3" xref="S6.T4.1.1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.1.1.1.1.m1.1b"><apply id="S6.T4.1.1.1.1.m1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.1.1.1.1.m1.1.1.1.cmml" xref="S6.T4.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.T4.1.1.1.1.m1.1.1.2.cmml" xref="S6.T4.1.1.1.1.m1.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.1.1.1.1.m1.1.1.3.cmml" xref="S6.T4.1.1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.1.1.1.1.m1.1c">s_{1}</annotation></semantics></math></th>
<td id="S6.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">0.795</td>
<td id="S6.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">0.742</td>
<td id="S6.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.0pt;padding-right:4.0pt;">0.502</td>
</tr>
<tr id="S6.T4.2.2.2" class="ltx_tr">
<th id="S6.T4.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S6.T4.2.2.2.1.m1.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="S6.T4.2.2.2.1.m1.1a"><msub id="S6.T4.2.2.2.1.m1.1.1" xref="S6.T4.2.2.2.1.m1.1.1.cmml"><mi id="S6.T4.2.2.2.1.m1.1.1.2" xref="S6.T4.2.2.2.1.m1.1.1.2.cmml">s</mi><mn id="S6.T4.2.2.2.1.m1.1.1.3" xref="S6.T4.2.2.2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.2.2.2.1.m1.1b"><apply id="S6.T4.2.2.2.1.m1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.2.2.2.1.m1.1.1.1.cmml" xref="S6.T4.2.2.2.1.m1.1.1">subscript</csymbol><ci id="S6.T4.2.2.2.1.m1.1.1.2.cmml" xref="S6.T4.2.2.2.1.m1.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.2.2.2.1.m1.1.1.3.cmml" xref="S6.T4.2.2.2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.2.2.2.1.m1.1c">s_{2}</annotation></semantics></math></th>
<td id="S6.T4.2.2.2.2" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">0.818</td>
<td id="S6.T4.2.2.2.3" class="ltx_td ltx_align_center" style="padding-left:4.0pt;padding-right:4.0pt;">0.774</td>
<td id="S6.T4.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">0.53</td>
</tr>
<tr id="S6.T4.3.3.3" class="ltx_tr">
<th id="S6.T4.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;"><math id="S6.T4.3.3.3.1.m1.1" class="ltx_Math" alttext="s_{3}" display="inline"><semantics id="S6.T4.3.3.3.1.m1.1a"><msub id="S6.T4.3.3.3.1.m1.1.1" xref="S6.T4.3.3.3.1.m1.1.1.cmml"><mi id="S6.T4.3.3.3.1.m1.1.1.2" xref="S6.T4.3.3.3.1.m1.1.1.2.cmml">s</mi><mn id="S6.T4.3.3.3.1.m1.1.1.3" xref="S6.T4.3.3.3.1.m1.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.3.3.3.1.m1.1b"><apply id="S6.T4.3.3.3.1.m1.1.1.cmml" xref="S6.T4.3.3.3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.3.3.3.1.m1.1.1.1.cmml" xref="S6.T4.3.3.3.1.m1.1.1">subscript</csymbol><ci id="S6.T4.3.3.3.1.m1.1.1.2.cmml" xref="S6.T4.3.3.3.1.m1.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.3.3.3.1.m1.1.1.3.cmml" xref="S6.T4.3.3.3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.3.3.3.1.m1.1c">s_{3}</annotation></semantics></math></th>
<td id="S6.T4.3.3.3.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">0.828</td>
<td id="S6.T4.3.3.3.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:4.0pt;padding-right:4.0pt;">0.796</td>
<td id="S6.T4.3.3.3.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:4.0pt;padding-right:4.0pt;">0.627</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Average precision from HR<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> trained on different sets of synthetic data. These three sets are combined with real data to improve detectors’ performance. Different settings: <math id="S6.T4.9.m1.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="S6.T4.9.m1.1b"><msub id="S6.T4.9.m1.1.1" xref="S6.T4.9.m1.1.1.cmml"><mi id="S6.T4.9.m1.1.1.2" xref="S6.T4.9.m1.1.1.2.cmml">s</mi><mn id="S6.T4.9.m1.1.1.3" xref="S6.T4.9.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.9.m1.1c"><apply id="S6.T4.9.m1.1.1.cmml" xref="S6.T4.9.m1.1.1"><csymbol cd="ambiguous" id="S6.T4.9.m1.1.1.1.cmml" xref="S6.T4.9.m1.1.1">subscript</csymbol><ci id="S6.T4.9.m1.1.1.2.cmml" xref="S6.T4.9.m1.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.9.m1.1.1.3.cmml" xref="S6.T4.9.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.9.m1.1d">s_{1}</annotation></semantics></math> is our basic settings for rendering with light occlusion. <math id="S6.T4.10.m2.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="S6.T4.10.m2.1b"><msub id="S6.T4.10.m2.1.1" xref="S6.T4.10.m2.1.1.cmml"><mi id="S6.T4.10.m2.1.1.2" xref="S6.T4.10.m2.1.1.2.cmml">s</mi><mn id="S6.T4.10.m2.1.1.3" xref="S6.T4.10.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.10.m2.1c"><apply id="S6.T4.10.m2.1.1.cmml" xref="S6.T4.10.m2.1.1"><csymbol cd="ambiguous" id="S6.T4.10.m2.1.1.1.cmml" xref="S6.T4.10.m2.1.1">subscript</csymbol><ci id="S6.T4.10.m2.1.1.2.cmml" xref="S6.T4.10.m2.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.10.m2.1.1.3.cmml" xref="S6.T4.10.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.10.m2.1d">s_{2}</annotation></semantics></math> combines <math id="S6.T4.11.m3.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="S6.T4.11.m3.1b"><msub id="S6.T4.11.m3.1.1" xref="S6.T4.11.m3.1.1.cmml"><mi id="S6.T4.11.m3.1.1.2" xref="S6.T4.11.m3.1.1.2.cmml">s</mi><mn id="S6.T4.11.m3.1.1.3" xref="S6.T4.11.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.11.m3.1c"><apply id="S6.T4.11.m3.1.1.cmml" xref="S6.T4.11.m3.1.1"><csymbol cd="ambiguous" id="S6.T4.11.m3.1.1.1.cmml" xref="S6.T4.11.m3.1.1">subscript</csymbol><ci id="S6.T4.11.m3.1.1.2.cmml" xref="S6.T4.11.m3.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.11.m3.1.1.3.cmml" xref="S6.T4.11.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.11.m3.1d">s_{1}</annotation></semantics></math> with extra occlusion from other faces in the render process. <math id="S6.T4.12.m4.1" class="ltx_Math" alttext="s_{3}" display="inline"><semantics id="S6.T4.12.m4.1b"><msub id="S6.T4.12.m4.1.1" xref="S6.T4.12.m4.1.1.cmml"><mi id="S6.T4.12.m4.1.1.2" xref="S6.T4.12.m4.1.1.2.cmml">s</mi><mn id="S6.T4.12.m4.1.1.3" xref="S6.T4.12.m4.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.12.m4.1c"><apply id="S6.T4.12.m4.1.1.cmml" xref="S6.T4.12.m4.1.1"><csymbol cd="ambiguous" id="S6.T4.12.m4.1.1.1.cmml" xref="S6.T4.12.m4.1.1">subscript</csymbol><ci id="S6.T4.12.m4.1.1.2.cmml" xref="S6.T4.12.m4.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.12.m4.1.1.3.cmml" xref="S6.T4.12.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.12.m4.1d">s_{3}</annotation></semantics></math> adds additional blurry results from down-sampled high resolution images into <math id="S6.T4.13.m5.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="S6.T4.13.m5.1b"><msub id="S6.T4.13.m5.1.1" xref="S6.T4.13.m5.1.1.cmml"><mi id="S6.T4.13.m5.1.1.2" xref="S6.T4.13.m5.1.1.2.cmml">s</mi><mn id="S6.T4.13.m5.1.1.3" xref="S6.T4.13.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.T4.13.m5.1c"><apply id="S6.T4.13.m5.1.1.cmml" xref="S6.T4.13.m5.1.1"><csymbol cd="ambiguous" id="S6.T4.13.m5.1.1.1.cmml" xref="S6.T4.13.m5.1.1">subscript</csymbol><ci id="S6.T4.13.m5.1.1.2.cmml" xref="S6.T4.13.m5.1.1.2">𝑠</ci><cn type="integer" id="S6.T4.13.m5.1.1.3.cmml" xref="S6.T4.13.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T4.13.m5.1d">s_{2}</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4 </span>Improving face detector performance by data augmentation</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">In this section, we study how to use our synthetic data to improve the performance of the face detectors (i.e., Faster RCNN, SSH and HR) on real datasets. We train on a combination of Wider Face and synthetic data and then test on MAFA, UFDD and Wider Face. Visualization of our detection results are shown in Figure <a href="#S6.F9" title="Figure 9 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
<figure id="S6.F6" class="ltx_figure"><img src="/html/1812.07363/assets/x6.png" id="S6.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Performance comparison on different sizes of synthetic data in training on MAFA test set with different detectors. </figcaption>
</figure>
<figure id="S6.F7" class="ltx_figure"><img src="/html/1812.07363/assets/x7.png" id="S6.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Performance comparison on different data augmentations on UFDD test set with different detectors.the table is not visible and not clear.</figcaption>
</figure>
<figure id="S6.F8" class="ltx_figure"><img src="/html/1812.07363/assets/x8.png" id="S6.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="211" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Performance comparison on different data augmentations on Wider Face validation set with different detectors.</figcaption>
</figure>
<figure id="S6.F9" class="ltx_figure"><img src="/html/1812.07363/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="423" height="190" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Qualitative results on different variations of real dataset. We visualize examples of each variation.</figcaption>
</figure>
<figure id="S6.F10" class="ltx_figure"><img src="/html/1812.07363/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="422" height="237" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>False positive examples derived from our detection results. In both Figure <a href="#S6.F9" title="Figure 9 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> and Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, the green bounding boxes are the ground truth boxes; the bounding boxes with other colors are predictions with different confidence intervals; and the red bounding boxes are false negative examples. (a) is a square annotation example from the MAFA test set. (b) and (c) are occlusion annotations of, respectively, UFDD and Wider Face. (f) only has three annotations for ground truth but actually has more unlabeled faces.(d), (e) and (g) are a number of examples of the real dataset.</figcaption>
</figure>
<section id="S6.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.1 </span>MAFA</h4>

<div id="S6.SS4.SSS1.p1" class="ltx_para">
<p id="S6.SS4.SSS1.p1.1" class="ltx_p">We only use mixed face occlusion (that is landmark and heavy occlusion as in <a href="#S6.SS2" title="6.2 The influence of data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.2</span></a> for data augmentation for MAFA. This is because during the collection and annotation process, MAFA has constraints on the different types of variations. The synthetic images for data augmentation follow the same setting of MAFA training set. Our hypothesis is that the training data size will have an effect on the performance. As shown in Figure <a href="#S6.F6" title="Figure 6 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, the performance of different detectors improves with the increase of the number of synthetic images. However, after adding more and more data, the performance saturates and drops. The reason could be that there is a domain gap between the synthetic and real data. Our synthetic data may not be as complex as real data. Also, our synthetic data inherits biases (e.g. annotation bias or mesh corruption) from the original (real) 3D models. Therefore, there is a inverted U-shape relationship between the increase of synthetic data in training and performance.</p>
</div>
</section>
<section id="S6.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.2 </span>UFDD</h4>

<div id="S6.SS4.SSS2.p1" class="ltx_para">
<p id="S6.SS4.SSS2.p1.9" class="ltx_p">In this section, our data augmentation is used with the Wider Face only. This is because (1) the training dataset of UFDD is not made public, and (2) UFDD itself is trained on Wider Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Three synthetic datasets <math id="S6.SS4.SSS2.p1.1.m1.3" class="ltx_Math" alttext="s_{1},s_{2},s_{3}" display="inline"><semantics id="S6.SS4.SSS2.p1.1.m1.3a"><mrow id="S6.SS4.SSS2.p1.1.m1.3.3.3" xref="S6.SS4.SSS2.p1.1.m1.3.3.4.cmml"><msub id="S6.SS4.SSS2.p1.1.m1.1.1.1.1" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1.cmml"><mi id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.2" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.3" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.SS4.SSS2.p1.1.m1.3.3.3.4" xref="S6.SS4.SSS2.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS4.SSS2.p1.1.m1.2.2.2.2" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2.cmml"><mi id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.2" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.3" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S6.SS4.SSS2.p1.1.m1.3.3.3.5" xref="S6.SS4.SSS2.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS4.SSS2.p1.1.m1.3.3.3.3" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3.cmml"><mi id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.2" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.3" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.1.m1.3b"><list id="S6.SS4.SSS2.p1.1.m1.3.3.4.cmml" xref="S6.SS4.SSS2.p1.1.m1.3.3.3"><apply id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.1.cmml" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS4.SSS2.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.cmml" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.1.cmml" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.2.cmml" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.1.m1.2.2.2.2.3.cmml" xref="S6.SS4.SSS2.p1.1.m1.2.2.2.2.3">2</cn></apply><apply id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.cmml" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.1.cmml" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.2.cmml" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.1.m1.3.3.3.3.3.cmml" xref="S6.SS4.SSS2.p1.1.m1.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.1.m1.3c">s_{1},s_{2},s_{3}</annotation></semantics></math> are combined with real data to improve detectors’ performance. Different settings: <math id="S6.SS4.SSS2.p1.2.m2.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="S6.SS4.SSS2.p1.2.m2.1a"><msub id="S6.SS4.SSS2.p1.2.m2.1.1" xref="S6.SS4.SSS2.p1.2.m2.1.1.cmml"><mi id="S6.SS4.SSS2.p1.2.m2.1.1.2" xref="S6.SS4.SSS2.p1.2.m2.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.2.m2.1.1.3" xref="S6.SS4.SSS2.p1.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.2.m2.1b"><apply id="S6.SS4.SSS2.p1.2.m2.1.1.cmml" xref="S6.SS4.SSS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.2.m2.1.1.1.cmml" xref="S6.SS4.SSS2.p1.2.m2.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.2.m2.1.1.2.cmml" xref="S6.SS4.SSS2.p1.2.m2.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.2.m2.1.1.3.cmml" xref="S6.SS4.SSS2.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.2.m2.1c">s_{1}</annotation></semantics></math> is our basic settings for rendering with light occlusion. <math id="S6.SS4.SSS2.p1.3.m3.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="S6.SS4.SSS2.p1.3.m3.1a"><msub id="S6.SS4.SSS2.p1.3.m3.1.1" xref="S6.SS4.SSS2.p1.3.m3.1.1.cmml"><mi id="S6.SS4.SSS2.p1.3.m3.1.1.2" xref="S6.SS4.SSS2.p1.3.m3.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.3.m3.1.1.3" xref="S6.SS4.SSS2.p1.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.3.m3.1b"><apply id="S6.SS4.SSS2.p1.3.m3.1.1.cmml" xref="S6.SS4.SSS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.3.m3.1.1.1.cmml" xref="S6.SS4.SSS2.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.3.m3.1.1.2.cmml" xref="S6.SS4.SSS2.p1.3.m3.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.3.m3.1.1.3.cmml" xref="S6.SS4.SSS2.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.3.m3.1c">s_{2}</annotation></semantics></math> combines <math id="S6.SS4.SSS2.p1.4.m4.1" class="ltx_Math" alttext="s_{1}" display="inline"><semantics id="S6.SS4.SSS2.p1.4.m4.1a"><msub id="S6.SS4.SSS2.p1.4.m4.1.1" xref="S6.SS4.SSS2.p1.4.m4.1.1.cmml"><mi id="S6.SS4.SSS2.p1.4.m4.1.1.2" xref="S6.SS4.SSS2.p1.4.m4.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.4.m4.1.1.3" xref="S6.SS4.SSS2.p1.4.m4.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.4.m4.1b"><apply id="S6.SS4.SSS2.p1.4.m4.1.1.cmml" xref="S6.SS4.SSS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.4.m4.1.1.1.cmml" xref="S6.SS4.SSS2.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.4.m4.1.1.2.cmml" xref="S6.SS4.SSS2.p1.4.m4.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.4.m4.1.1.3.cmml" xref="S6.SS4.SSS2.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.4.m4.1c">s_{1}</annotation></semantics></math> with extra occlusion from other faces in the render process. <math id="S6.SS4.SSS2.p1.5.m5.1" class="ltx_Math" alttext="s_{3}" display="inline"><semantics id="S6.SS4.SSS2.p1.5.m5.1a"><msub id="S6.SS4.SSS2.p1.5.m5.1.1" xref="S6.SS4.SSS2.p1.5.m5.1.1.cmml"><mi id="S6.SS4.SSS2.p1.5.m5.1.1.2" xref="S6.SS4.SSS2.p1.5.m5.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.5.m5.1.1.3" xref="S6.SS4.SSS2.p1.5.m5.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.5.m5.1b"><apply id="S6.SS4.SSS2.p1.5.m5.1.1.cmml" xref="S6.SS4.SSS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.5.m5.1.1.1.cmml" xref="S6.SS4.SSS2.p1.5.m5.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.5.m5.1.1.2.cmml" xref="S6.SS4.SSS2.p1.5.m5.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.5.m5.1.1.3.cmml" xref="S6.SS4.SSS2.p1.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.5.m5.1c">s_{3}</annotation></semantics></math> adds additional blurry results from down-sampled high resolution images into <math id="S6.SS4.SSS2.p1.6.m6.1" class="ltx_Math" alttext="s_{2}" display="inline"><semantics id="S6.SS4.SSS2.p1.6.m6.1a"><msub id="S6.SS4.SSS2.p1.6.m6.1.1" xref="S6.SS4.SSS2.p1.6.m6.1.1.cmml"><mi id="S6.SS4.SSS2.p1.6.m6.1.1.2" xref="S6.SS4.SSS2.p1.6.m6.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.6.m6.1.1.3" xref="S6.SS4.SSS2.p1.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.6.m6.1b"><apply id="S6.SS4.SSS2.p1.6.m6.1.1.cmml" xref="S6.SS4.SSS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.6.m6.1.1.1.cmml" xref="S6.SS4.SSS2.p1.6.m6.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.6.m6.1.1.2.cmml" xref="S6.SS4.SSS2.p1.6.m6.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.6.m6.1.1.3.cmml" xref="S6.SS4.SSS2.p1.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.6.m6.1c">s_{2}</annotation></semantics></math>. The performance of data augmentation is shown in Table <a href="#S6.T4" title="Table 4 ‣ 6.3 Performance comparison on synthetic data ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. The influence of our data augmentation is shown in Figure <a href="#S6.F7" title="Figure 7 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. After merging synthetic data and real data together, the performance of Faster RCNN, which is trained on real data, improves significantly on <math id="S6.SS4.SSS2.p1.7.m7.1" class="ltx_Math" alttext="r+s_{1}" display="inline"><semantics id="S6.SS4.SSS2.p1.7.m7.1a"><mrow id="S6.SS4.SSS2.p1.7.m7.1.1" xref="S6.SS4.SSS2.p1.7.m7.1.1.cmml"><mi id="S6.SS4.SSS2.p1.7.m7.1.1.2" xref="S6.SS4.SSS2.p1.7.m7.1.1.2.cmml">r</mi><mo id="S6.SS4.SSS2.p1.7.m7.1.1.1" xref="S6.SS4.SSS2.p1.7.m7.1.1.1.cmml">+</mo><msub id="S6.SS4.SSS2.p1.7.m7.1.1.3" xref="S6.SS4.SSS2.p1.7.m7.1.1.3.cmml"><mi id="S6.SS4.SSS2.p1.7.m7.1.1.3.2" xref="S6.SS4.SSS2.p1.7.m7.1.1.3.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.7.m7.1.1.3.3" xref="S6.SS4.SSS2.p1.7.m7.1.1.3.3.cmml">1</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.7.m7.1b"><apply id="S6.SS4.SSS2.p1.7.m7.1.1.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1"><plus id="S6.SS4.SSS2.p1.7.m7.1.1.1.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.1"></plus><ci id="S6.SS4.SSS2.p1.7.m7.1.1.2.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.2">𝑟</ci><apply id="S6.SS4.SSS2.p1.7.m7.1.1.3.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.7.m7.1.1.3.1.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.3">subscript</csymbol><ci id="S6.SS4.SSS2.p1.7.m7.1.1.3.2.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.3.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.7.m7.1.1.3.3.cmml" xref="S6.SS4.SSS2.p1.7.m7.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.7.m7.1c">r+s_{1}</annotation></semantics></math> and <math id="S6.SS4.SSS2.p1.8.m8.1" class="ltx_Math" alttext="r+s_{2}" display="inline"><semantics id="S6.SS4.SSS2.p1.8.m8.1a"><mrow id="S6.SS4.SSS2.p1.8.m8.1.1" xref="S6.SS4.SSS2.p1.8.m8.1.1.cmml"><mi id="S6.SS4.SSS2.p1.8.m8.1.1.2" xref="S6.SS4.SSS2.p1.8.m8.1.1.2.cmml">r</mi><mo id="S6.SS4.SSS2.p1.8.m8.1.1.1" xref="S6.SS4.SSS2.p1.8.m8.1.1.1.cmml">+</mo><msub id="S6.SS4.SSS2.p1.8.m8.1.1.3" xref="S6.SS4.SSS2.p1.8.m8.1.1.3.cmml"><mi id="S6.SS4.SSS2.p1.8.m8.1.1.3.2" xref="S6.SS4.SSS2.p1.8.m8.1.1.3.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.8.m8.1.1.3.3" xref="S6.SS4.SSS2.p1.8.m8.1.1.3.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.8.m8.1b"><apply id="S6.SS4.SSS2.p1.8.m8.1.1.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1"><plus id="S6.SS4.SSS2.p1.8.m8.1.1.1.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.1"></plus><ci id="S6.SS4.SSS2.p1.8.m8.1.1.2.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.2">𝑟</ci><apply id="S6.SS4.SSS2.p1.8.m8.1.1.3.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.3"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.8.m8.1.1.3.1.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.3">subscript</csymbol><ci id="S6.SS4.SSS2.p1.8.m8.1.1.3.2.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.3.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.8.m8.1.1.3.3.cmml" xref="S6.SS4.SSS2.p1.8.m8.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.8.m8.1c">r+s_{2}</annotation></semantics></math>. Given that Faster RCNN is not trained for different scales, the noise of <math id="S6.SS4.SSS2.p1.9.m9.1" class="ltx_Math" alttext="s_{3}" display="inline"><semantics id="S6.SS4.SSS2.p1.9.m9.1a"><msub id="S6.SS4.SSS2.p1.9.m9.1.1" xref="S6.SS4.SSS2.p1.9.m9.1.1.cmml"><mi id="S6.SS4.SSS2.p1.9.m9.1.1.2" xref="S6.SS4.SSS2.p1.9.m9.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS2.p1.9.m9.1.1.3" xref="S6.SS4.SSS2.p1.9.m9.1.1.3.cmml">3</mn></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS2.p1.9.m9.1b"><apply id="S6.SS4.SSS2.p1.9.m9.1.1.cmml" xref="S6.SS4.SSS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS2.p1.9.m9.1.1.1.cmml" xref="S6.SS4.SSS2.p1.9.m9.1.1">subscript</csymbol><ci id="S6.SS4.SSS2.p1.9.m9.1.1.2.cmml" xref="S6.SS4.SSS2.p1.9.m9.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS2.p1.9.m9.1.1.3.cmml" xref="S6.SS4.SSS2.p1.9.m9.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS2.p1.9.m9.1c">s_{3}</annotation></semantics></math> impedes its performance. As for SSH, its architecture and parameters heavily rely on Wider Face. For UFDD, its performance becomes saturated after being trained on real data. After adding synthetic data, its performance is even worse than Faster RCNN. Faces in UFDD are not very challenging to HR; the performance therefore only changes slightly after using our data augmentation.</p>
</div>
</section>
<section id="S6.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.4.3 </span>Wider Face</h4>

<div id="S6.SS4.SSS3.p1" class="ltx_para">
<p id="S6.SS4.SSS3.p1.1" class="ltx_p">We still use the same setting <math id="S6.SS4.SSS3.p1.1.m1.3" class="ltx_Math" alttext="s_{1},s_{2},s_{3}" display="inline"><semantics id="S6.SS4.SSS3.p1.1.m1.3a"><mrow id="S6.SS4.SSS3.p1.1.m1.3.3.3" xref="S6.SS4.SSS3.p1.1.m1.3.3.4.cmml"><msub id="S6.SS4.SSS3.p1.1.m1.1.1.1.1" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1.cmml"><mi id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.2" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1.2.cmml">s</mi><mn id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.3" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S6.SS4.SSS3.p1.1.m1.3.3.3.4" xref="S6.SS4.SSS3.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS4.SSS3.p1.1.m1.2.2.2.2" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2.cmml"><mi id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.2" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2.2.cmml">s</mi><mn id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.3" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S6.SS4.SSS3.p1.1.m1.3.3.3.5" xref="S6.SS4.SSS3.p1.1.m1.3.3.4.cmml">,</mo><msub id="S6.SS4.SSS3.p1.1.m1.3.3.3.3" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3.cmml"><mi id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.2" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3.2.cmml">s</mi><mn id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.3" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3.3.cmml">3</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.SSS3.p1.1.m1.3b"><list id="S6.SS4.SSS3.p1.1.m1.3.3.4.cmml" xref="S6.SS4.SSS3.p1.1.m1.3.3.3"><apply id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.cmml" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.1.cmml" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.2.cmml" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS3.p1.1.m1.1.1.1.1.3.cmml" xref="S6.SS4.SSS3.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.cmml" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.1.cmml" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.2.cmml" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS3.p1.1.m1.2.2.2.2.3.cmml" xref="S6.SS4.SSS3.p1.1.m1.2.2.2.2.3">2</cn></apply><apply id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.cmml" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.1.cmml" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3">subscript</csymbol><ci id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.2.cmml" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3.2">𝑠</ci><cn type="integer" id="S6.SS4.SSS3.p1.1.m1.3.3.3.3.3.cmml" xref="S6.SS4.SSS3.p1.1.m1.3.3.3.3.3">3</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.SSS3.p1.1.m1.3c">s_{1},s_{2},s_{3}</annotation></semantics></math> as in <a href="#S6.SS4.SSS2" title="6.4.2 UFDD ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6.4.2</span></a> to perform data augmentation for Wider Face. The performance comparison is shown in Figure <a href="#S6.F8" title="Figure 8 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. Faster RCNN is a generic object detector without multi-scale testing. Hence, it is supposed to generate fewer predictions than HR and SSH. After we add synthetic data, the performance substantially improves for all levels. The performance of HR and SSH nearly saturate after being trained on real data.</p>
</div>
</section>
</section>
<section id="S6.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.5 </span>Analysis</h3>

<section id="S6.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.1 </span>Analysis of synthetic data</h4>

<div id="S6.SS5.SSS1.p1" class="ltx_para">
<p id="S6.SS5.SSS1.p1.1" class="ltx_p">The advantage of synthetic data is that the variations in dataset can be fully controlled. Although, there is always a domain gap between synthetic data and real data, this paper shows that synthetic augmentation can provide large-scale datasets with annotations conveniently and precisely. Our results show the applicability of synthetic data as an alternative to real data.</p>
</div>
</section>
<section id="S6.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.2 </span>Analysis of false positives</h4>

<div id="S6.SS5.SSS2.p1" class="ltx_para">
<p id="S6.SS5.SSS2.p1.1" class="ltx_p">False positives are the primary factor that decrease the performance. In Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we plot a number of false positive examples. There are two major sources of false positive in our detection results.</p>
</div>
<div id="S6.SS5.SSS2.p2" class="ltx_para">
<p id="S6.SS5.SSS2.p2.1" class="ltx_p">The first source is annotation. Different datasets have their own annotation process. This process may have a negative influence on our predictions. For example, the annotation of occluded faces in Wider Face is based on the region of the entire face (Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.(c)). In comparison, UFDD often annotates the visible part of occluded faces ((Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.(b)). MAFA uses square annotation, which may contain background information of surrounding faces (Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.(a)). Moreover, human annotators may not be able to annotate all the tiny and blurry faces in the background, see Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.(f).</p>
</div>
<div id="S6.SS5.SSS2.p3" class="ltx_para">
<p id="S6.SS5.SSS2.p3.1" class="ltx_p">The second source of false positives is due to confusing objects, such as round-shaped objects and human body parts. Because real data has much more diverse and complex background than synthetic data. Our synthetic images are rendered from 3D face models. Most of these 3D face models only depict the upper part of the human body. Therefore, our rendering results are inherently unrepresentative for other human body parts. As a result, other human body parts (see Figure <a href="#S6.F10" title="Figure 10 ‣ 6.4 Improving face detector performance by data augmentation ‣ 6 Results ‣ Improving Face Detection Performance with 3D-Rendered Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.(e)) and accessories can be a source of false positives.</p>
</div>
</section>
<section id="S6.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.5.3 </span>Analysis of face detectors</h4>

<div id="S6.SS5.SSS3.p1" class="ltx_para">
<p id="S6.SS5.SSS3.p1.1" class="ltx_p">Based on our detection results, we analyze the characteristics of three detectors respectively (1) Faster RCNN is an object, instead of face, detector. It does not adjust its settings of anchors for the face detection benchmarks. And it has fewer predictions without multi-scale testing. Despite that, our synthetic data augmentation substantially improves its performance on multiple challenging datasets. (2) SSH is a face-targeted detector. However, our synthetic data augmentation is not able to outperform Faster RCNN in most of the detection tasks except for the hard level of Wider Face. Designed to cope with scale, SSH results in worse performance when other variations are encountered. Detectors have a trade-off between speed and performance<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. SSH pursues fast speed in inference process such that its light weight architecture is equipped to handle other variations. (3) Although the HR face detector already has excellent performance for different kinds of variations, our synthetic data still improves its performance. However, HR has a drawback that is extremely sensitive to small round-shape objects given its tiny-face-targeted architecture. HR generates more false positives than other detectors. It restricts the generalization on regular faces.</p>
</div>
</section>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we provide a synthetic data generator methodology with fully controlled, multifaceted variations based on a new 3D face dataset (3DU-Face). We customized synthetic datasets to address specific types of variations (scale, pose, occlusion, blur, etc.), and systematically investigate the influence of different variations on face detection performances. We validate our synthetic data augmentation for different face detectors (Faster RCNN, SSH and HR) on various face datasets (MAFA, UFDD and Wider Face).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
I. Abbasnejad, S. Sridharan, D. Nguyen, S. Denman, C. Fookes, and S. Lucey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Using synthetic data to improve facial expression analysis with 3d
convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 1609–1618, 2017.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
M. Ariz, J. J. Bengoechea, A. Villanueva, and R. Cabeza.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">A novel 2d/3d database with automatic face annotation for head
tracking and pose estimation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 148:201–210, 2016.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Y. Bai, Y. Zhang, M. Ding, and B. Ghanem.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Finding tiny faces in the wild with generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR. IEEE</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
H. Bal, D. Epema, C. de Laat, R. van Nieuwpoort, J. Romein, F. Seinstra,
C. Snoek, and H. Wijshoff.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">A medium-scale distributed system for computer science research:
Infrastructure for the long term.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 49(5):54–63, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
V. Blanz and T. Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">A morphable model for the synthesis of 3d faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 26th annual conference on Computer
graphics and interactive techniques</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 187–194. ACM
Press/Addison-Wesley Publishing Co., 1999.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">ShapeNet: An Information-Rich 3D Model Repository.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">Technical Report arXiv:1512.03012 [cs.GR], Stanford University —
Princeton University — Toyota Technological Institute at Chicago, 2015.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Y. Chen, L. Song, and R. He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Masquer hunter: Adversarial occlusion-aware face detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1709.05188</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Y. Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Stargan: Unified generative adversarial networks for multi-domain
image-to-image translation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 1711, 2017.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
cydonia999.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">A pytorch implementation of detectron.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/roytseng-tw/Detectron.pytorch" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/roytseng-tw/Detectron.pytorch</a><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
S. S. Farfade, M. J. Saberian, and L.-J. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Multi-view face detection using deep convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 5th ACM on International Conference on
Multimedia Retrieval</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 643–650. ACM, 2015.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
S. Ge, J. Li, Q. Ye, and Z. Luo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Detecting masked faces in the wild with lle-cnns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE CVPR</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Z. Hao, Y. Liu, H. Qin, J. Yan, X. Li, and X. Hu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Scale-aware face detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, volume 3, 2017.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
T. Hassner, S. Harel, E. Paz, and R. Enbar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Effective face frontalization in unconstrained images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 4295–4304, 2015.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
D. Hoiem, Y. Chodpathumwan, and Q. Dai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Diagnosing error in object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 340–353.
Springer, 2012.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
P. Hu and D. Ramanan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Finding tiny faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1612.04402</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer,
Z. Wojna, Y. Song, S. Guadarrama, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Speed/accuracy trade-offs for modern convolutional object detectors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
G. Y. S. J. Jianfeng Wang, Ye Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Sface: An efficient network for face detection in large scale
variations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.06559</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
S. Karaoglu, Y. Liu, and T. Gevers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Detect2rank: Combining object detectors using learning to rank.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 25(1):233–248, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
A. Kortylewski, A. Schneider, T. Gerig, B. Egger, A. Morel-Forster, and
T. Vetter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Training deep face recognition systems with synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.05891</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Z. Lu, Z. Li, J. Cao, R. He, and Z. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Recent progress of face image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.04717</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
I. Masi, A. T. Trần, T. Hassner, J. T. Leksut, and G. Medioni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Do we really need to collect millions of faces for effective face
recognition?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 579–596.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
H. Nada, V. A. Sindagi, H. Zhang, and V. M. Patel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Pushing the limits of unconstrained face detection: a challenge
dataset and baseline results.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.10275</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
M. Najibi, P. Samangouei, R. Chellappa, and L. Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Ssh: Single stage headless face detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE ICCV</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
M. Osadchy, Y. L. Cun, and M. L. Miller.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Synergistic face detection and pose estimation with energy-based
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 8(May):1197–1215, 2007.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
M. Osadchy, Y. Wang, O. Dunkelman, S. Gibson, J. Hernandez-Castro, and
C. Solomon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Genface: Improving cyber security using realistic synthetic face
generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Cyber Security Cryptography and
Machine Learning</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 19–33. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
A. Pumarola, A. Agudo, A. M. Martinez, A. Sanfeliu, and F. Moreno-Noguer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Ganimation: Anatomically-aware facial animation from a single image.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 818–833, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
S. Ren, K. He, R. Girshick, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Faster r-cnn: Towards real-time object detection with region proposal
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages
91–99, 2015.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
W. Shen and R. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Learning residual images for face attribute manipulation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 1225–1233. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
X. Shi, S. Shan, M. Kan, S. Wu, and X. Chen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Real-time rotation-invariant face detection with progressive
calibration networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, pages 2295–2303, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, and D. Samaras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Neural face editing with intrinsic image disentangling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE
Conference on</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 5444–5453. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
X. Tang, D. K. Du, Z. He, and J. Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Pyramidbox: A context-assisted single shot face detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1803.07737</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
J. Thies, M. Zollhofer, M. Stamminger, C. Theobalt, and M. Nießner.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Face2face: Real-time face capture and reenactment of rgb videos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 2387–2395, 2016.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
J. Wang, Y. Yuan, and G. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Face attention network: An effective face detector for the occluded
faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1711.07246</span><span id="bib.bib33.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Y. Wang, L. Zhang, Z. Liu, G. Hua, Z. Wen, Z. Zhang, and D. Samaras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Face relighting from a single image under arbitrary unknown lighting
conditions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">,
31(11):1968–1984, 2009.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
S. Yang, P. Luo, C.-C. Loy, and X. Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Wider face: A face detection benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 5525–5533, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
S. Yang, Y. Xiong, C. C. Loy, and X. Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Face detection through scale-friendly deep convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1706.02863</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Z. Yang and R. Nevatia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">A multi-scale cascade fully convolutional network face detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition (ICPR), 2016 23rd International
Conference on</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 633–638. IEEE, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
J. Yim, H. Jung, B. Yoo, C. Choi, D. Park, and J. Kim.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Rotating your face using multi-task deep neural network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 676–684, 2015.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
T. H. Yuqian Zhou, Ding Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Survey of face detection on low-quality images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1804.07362</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">
S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li.
</span>
</span>
<span class="ltx_bibblock"><math id="bib.bib40.1.m1.1" class="ltx_Math" alttext="s^{3}" display="inline"><semantics id="bib.bib40.1.m1.1a"><msup id="bib.bib40.1.m1.1.1" xref="bib.bib40.1.m1.1.1.cmml"><mi mathsize="90%" id="bib.bib40.1.m1.1.1.2" xref="bib.bib40.1.m1.1.1.2.cmml">s</mi><mn mathsize="90%" id="bib.bib40.1.m1.1.1.3" xref="bib.bib40.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="bib.bib40.1.m1.1b"><apply id="bib.bib40.1.m1.1.1.cmml" xref="bib.bib40.1.m1.1.1"><csymbol cd="ambiguous" id="bib.bib40.1.m1.1.1.1.cmml" xref="bib.bib40.1.m1.1.1">superscript</csymbol><ci id="bib.bib40.1.m1.1.1.2.cmml" xref="bib.bib40.1.m1.1.1.2">𝑠</ci><cn type="integer" id="bib.bib40.1.m1.1.1.3.cmml" xref="bib.bib40.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="bib.bib40.1.m1.1c">s^{3}</annotation></semantics></math><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">fd: Single shot scale-invariant face detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.4.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1708.05237</span><span id="bib.bib40.5.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
M. Zhou, H. Lin, S. S. Young, and J. Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Hybrid sensing face detection and registration for low-light and
unconstrained conditions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Applied optics</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, 57(1):69–78, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
C. Zhu, R. Tao, K. Luu, and M. Savvides.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Seeing small faces from robust anchor’s perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1802.09058</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
C. Zhu, Y. Zheng, K. Luu, and M. Savvides.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Cms-rcnn: contextual multi-scale region-based cnn for unconstrained
face detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Deep Learning for Biometrics</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 57–79. Springer, 2017.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
X. Zhu, Z. Lei, J. Yan, D. Yi, and S. Z. Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">High-fidelity pose and expression normalization for face recognition
in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 787–796, 2015.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1812.07362" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1812.07363" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1812.07363">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1812.07363" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1812.07364" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 17:27:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
